[I 2024-01-05 23:27:21,822] A new study created in memory with name: cnn_mseloss_data2
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:27:53,480] Trial 0 finished with value: 11.193101761651027 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 11.193101761651027.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:29:38,451] Trial 1 finished with value: 17.761088055216497 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}. Best is trial 0 with value: 11.193101761651027.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:32:45,698] Trial 2 finished with value: 9.24283342470562 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 9.24283342470562.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:33:06,039] Trial 3 finished with value: 9.206228245006722 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}. Best is trial 3 with value: 9.206228245006722.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:33:48,771] Trial 4 finished with value: 9.156381909751245 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 4 with value: 9.156381909751245.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:34:25,018] Trial 5 finished with value: 9.150025231244292 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 5 with value: 9.150025231244292.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:35:25,954] Trial 6 finished with value: 9.250487832347376 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 5 with value: 9.150025231244292.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:36:37,870] Trial 7 finished with value: 9.640857198820447 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 5 with value: 9.150025231244292.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:37:14,923] Trial 8 finished with value: 27.42504839513976 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 5 with value: 9.150025231244292.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:40:12,587] Trial 9 finished with value: 9.301107275769613 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 5 with value: 9.150025231244292.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:41:32,624] Trial 10 finished with value: 9.226582234595224 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.003637276799843544, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 5 with value: 9.150025231244292.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:43:42,474] Trial 11 finished with value: 9.1078756667652 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.009747616659132213, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:45:45,145] Trial 12 finished with value: 9.176854284105781 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00747394189831267, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:47:33,590] Trial 13 finished with value: 9.177014011424749 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.009915964092801072, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:49:22,154] Trial 14 finished with value: 9.182309912698754 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0028723484743078923, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:49:58,081] Trial 15 finished with value: 9.235620569049818 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.006769602303110451, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:51:50,112] Trial 16 finished with value: 9.183258229468212 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.009753589025733351, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:53:18,900] Trial 17 finished with value: 9.186498353721861 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0031079134026941553, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:55:37,074] Trial 18 finished with value: 9.120705962144195 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008754824227657044, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:57:32,368] Trial 19 finished with value: 9.219334943061915 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.008500159134657992, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:58:07,191] Trial 20 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:00:13,517] Trial 21 finished with value: 9.153034932822258 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.007144766198238507, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:01:38,008] Trial 22 finished with value: 9.191229856019204 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005709933591083406, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:03:34,214] Trial 23 finished with value: 9.17835324964336 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.007915188809301588, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:04:06,325] Trial 24 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:04:48,295] Trial 25 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:04:54,187] Trial 26 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:06:09,861] Trial 27 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:07:06,570] Trial 28 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:07:12,459] Trial 29 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:07:29,567] Trial 30 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:09:25,163] Trial 31 finished with value: 9.162906982089885 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.007198955871355653, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:11:26,773] Trial 32 finished with value: 9.180096773481054 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.006890701370078642, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:12:14,657] Trial 33 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:13:06,778] Trial 34 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:13:27,593] Trial 35 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:13:49,500] Trial 36 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:14:51,670] Trial 37 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:17:16,430] Trial 38 finished with value: 9.173103159398284 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.006364279179238642, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:17:37,253] Trial 39 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:18:04,607] Trial 40 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:18:21,711] Trial 41 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:18:46,724] Trial 42 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:18:52,608] Trial 43 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:19:09,715] Trial 44 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:19:57,048] Trial 45 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:21:06,539] Trial 46 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:21:45,431] Trial 47 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:21:52,657] Trial 48 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:22:48,589] Trial 49 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:25:20,004] Trial 50 finished with value: 9.168858840743066 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.007254187156626804, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:26:39,884] Trial 51 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:27:42,103] Trial 52 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:28:59,333] Trial 53 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:31:00,855] Trial 54 finished with value: 9.165747999991803 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008401701351857927, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:31:48,710] Trial 55 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:32:13,984] Trial 56 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:33:05,138] Trial 57 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:33:53,002] Trial 58 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:34:17,909] Trial 59 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:34:34,290] Trial 60 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:35:15,684] Trial 61 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:36:12,763] Trial 62 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:36:56,886] Trial 63 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:38:20,041] Trial 64 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:39:24,587] Trial 65 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:39:50,815] Trial 66 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:42:07,101] Trial 67 finished with value: 9.176975221663351 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0001480064049452492, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:42:39,720] Trial 68 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:44:26,837] Trial 69 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:45:32,520] Trial 70 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:46:12,401] Trial 71 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:46:58,862] Trial 72 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:47:46,569] Trial 73 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:48:30,069] Trial 74 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:49:18,251] Trial 75 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:49:36,999] Trial 76 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:49:45,065] Trial 77 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:51:30,861] Trial 78 finished with value: 9.221560199652325 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.009330084704754018, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:51:50,408] Trial 79 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:52:24,322] Trial 80 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:54:25,099] Trial 81 finished with value: 9.163608677308854 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.006365702857524511, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:56:10,358] Trial 82 finished with value: 9.185979828803607 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.006330740058187831, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:56:56,371] Trial 83 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:58:05,194] Trial 84 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:58:58,118] Trial 85 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 00:59:21,844] Trial 86 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:00:13,678] Trial 87 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:02:00,869] Trial 88 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:02:52,286] Trial 89 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:03:17,860] Trial 90 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:04:35,385] Trial 91 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:06:04,977] Trial 92 finished with value: 9.164005196650853 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005474892820798748, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:08:11,267] Trial 93 finished with value: 9.143664677953625 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004817729729684827, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 11 with value: 9.1078756667652.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:09:09,905] Trial 94 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:10:58,300] Trial 95 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:12:13,422] Trial 96 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:13:10,976] Trial 97 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:13:47,376] Trial 98 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:14:04,853] Trial 99 pruned. 
GPU CUDA available, using GPU for training the models.
-----------------------------------------------
Input arguments: 
   + data_dir: /home/ra56kop/nnets_genomic_prediction/src
   + model: RNN
   + tuned: 1
   + minmax_scale: 0
   + standa_scale: 0
   + pca_fitting: 0
   + dataset: pheno_2
   + gpucuda: 2
   + data_variants: [0, 0, 0, 2]
-----------------------------------------------

---------------------------------------------------------
Tuning RNN with dataset pheno-2
---------------------------------------------------------


----------------------------------------------
Params for Trial 0
{'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=106.18512 | best_loss=106.18512
Epoch 1/80: current_loss=104.78353 | best_loss=104.78353
Epoch 2/80: current_loss=103.15876 | best_loss=103.15876
Epoch 3/80: current_loss=101.13855 | best_loss=101.13855
Epoch 4/80: current_loss=98.54382 | best_loss=98.54382
Epoch 5/80: current_loss=95.14941 | best_loss=95.14941
Epoch 6/80: current_loss=91.04742 | best_loss=91.04742
Epoch 7/80: current_loss=86.51578 | best_loss=86.51578
Epoch 8/80: current_loss=82.17575 | best_loss=82.17575
Epoch 9/80: current_loss=78.19710 | best_loss=78.19710
Epoch 10/80: current_loss=74.60578 | best_loss=74.60578
Epoch 11/80: current_loss=71.52559 | best_loss=71.52559
Epoch 12/80: current_loss=68.84343 | best_loss=68.84343
Epoch 13/80: current_loss=66.48262 | best_loss=66.48262
Epoch 14/80: current_loss=64.34270 | best_loss=64.34270
Epoch 15/80: current_loss=62.39743 | best_loss=62.39743
Epoch 16/80: current_loss=60.60747 | best_loss=60.60747
Epoch 17/80: current_loss=58.93613 | best_loss=58.93613
Epoch 18/80: current_loss=57.38682 | best_loss=57.38682
Epoch 19/80: current_loss=55.92413 | best_loss=55.92413
Epoch 20/80: current_loss=54.53627 | best_loss=54.53627
Epoch 21/80: current_loss=53.20481 | best_loss=53.20481
Epoch 22/80: current_loss=51.94108 | best_loss=51.94108
Epoch 23/80: current_loss=50.72469 | best_loss=50.72469
Epoch 24/80: current_loss=49.55506 | best_loss=49.55506
Epoch 25/80: current_loss=48.41912 | best_loss=48.41912
Epoch 26/80: current_loss=47.33549 | best_loss=47.33549
Epoch 27/80: current_loss=46.29129 | best_loss=46.29129
Epoch 28/80: current_loss=45.27701 | best_loss=45.27701
Epoch 29/80: current_loss=44.28883 | best_loss=44.28883
Epoch 30/80: current_loss=43.34934 | best_loss=43.34934
Epoch 31/80: current_loss=42.43594 | best_loss=42.43594
Epoch 32/80: current_loss=41.54493 | best_loss=41.54493
Epoch 33/80: current_loss=40.69338 | best_loss=40.69338
Epoch 34/80: current_loss=39.84746 | best_loss=39.84746
Epoch 35/80: current_loss=39.04264 | best_loss=39.04264
Epoch 36/80: current_loss=38.26830 | best_loss=38.26830
Epoch 37/80: current_loss=37.49815 | best_loss=37.49815
Epoch 38/80: current_loss=36.75684 | best_loss=36.75684
Epoch 39/80: current_loss=36.05007 | best_loss=36.05007
Epoch 40/80: current_loss=35.34422 | best_loss=35.34422
Epoch 41/80: current_loss=34.65806 | best_loss=34.65806
Epoch 42/80: current_loss=33.99315 | best_loss=33.99315
Epoch 43/80: current_loss=33.34501 | best_loss=33.34501
Epoch 44/80: current_loss=32.70382 | best_loss=32.70382
Epoch 45/80: current_loss=32.08661 | best_loss=32.08661
Epoch 46/80: current_loss=31.48468 | best_loss=31.48468
Epoch 47/80: current_loss=30.90160 | best_loss=30.90160
Epoch 48/80: current_loss=30.34222 | best_loss=30.34222
Epoch 49/80: current_loss=29.79336 | best_loss=29.79336
Epoch 50/80: current_loss=29.25072 | best_loss=29.25072
Epoch 51/80: current_loss=28.72254 | best_loss=28.72254
Epoch 52/80: current_loss=28.20469 | best_loss=28.20469
Epoch 53/80: current_loss=27.69567 | best_loss=27.69567
Epoch 54/80: current_loss=27.21850 | best_loss=27.21850
Epoch 55/80: current_loss=26.75178 | best_loss=26.75178
Epoch 56/80: current_loss=26.29388 | best_loss=26.29388
Epoch 57/80: current_loss=25.84531 | best_loss=25.84531
Epoch 58/80: current_loss=25.40782 | best_loss=25.40782
Epoch 59/80: current_loss=24.99156 | best_loss=24.99156
Epoch 60/80: current_loss=24.57289 | best_loss=24.57289
Epoch 61/80: current_loss=24.17433 | best_loss=24.17433
Epoch 62/80: current_loss=23.78238 | best_loss=23.78238
Epoch 63/80: current_loss=23.41630 | best_loss=23.41630
Epoch 64/80: current_loss=23.04852 | best_loss=23.04852
Epoch 65/80: current_loss=22.68628 | best_loss=22.68628
Epoch 66/80: current_loss=22.34563 | best_loss=22.34563
Epoch 67/80: current_loss=21.99967 | best_loss=21.99967
Epoch 68/80: current_loss=21.66491 | best_loss=21.66491
Epoch 69/80: current_loss=21.33946 | best_loss=21.33946
Epoch 70/80: current_loss=21.01872 | best_loss=21.01872
Epoch 71/80: current_loss=20.71319 | best_loss=20.71319
Epoch 72/80: current_loss=20.41783 | best_loss=20.41783
Epoch 73/80: current_loss=20.13569 | best_loss=20.13569
Epoch 74/80: current_loss=19.84915 | best_loss=19.84915
Epoch 75/80: current_loss=19.56920 | best_loss=19.56920
Epoch 76/80: current_loss=19.29710 | best_loss=19.29710
Epoch 77/80: current_loss=19.04115 | best_loss=19.04115
Epoch 78/80: current_loss=18.77904 | best_loss=18.77904
Epoch 79/80: current_loss=18.54145 | best_loss=18.54145
      explained_var=-0.10582 | mse_loss=19.10276
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.14939 | best_loss=18.14939
Epoch 1/80: current_loss=17.74733 | best_loss=17.74733
Epoch 2/80: current_loss=17.36733 | best_loss=17.36733
Epoch 3/80: current_loss=17.00339 | best_loss=17.00339
Epoch 4/80: current_loss=16.66000 | best_loss=16.66000
Epoch 5/80: current_loss=16.34889 | best_loss=16.34889
Epoch 6/80: current_loss=16.06574 | best_loss=16.06574
Epoch 7/80: current_loss=15.77916 | best_loss=15.77916
Epoch 8/80: current_loss=15.53528 | best_loss=15.53528
Epoch 9/80: current_loss=15.29914 | best_loss=15.29914
Epoch 10/80: current_loss=15.06705 | best_loss=15.06705
Epoch 11/80: current_loss=14.82979 | best_loss=14.82979
Epoch 12/80: current_loss=14.61378 | best_loss=14.61378
Epoch 13/80: current_loss=14.41916 | best_loss=14.41916
Epoch 14/80: current_loss=14.24306 | best_loss=14.24306
Epoch 15/80: current_loss=14.07459 | best_loss=14.07459
Epoch 16/80: current_loss=13.89939 | best_loss=13.89939
Epoch 17/80: current_loss=13.74666 | best_loss=13.74666
Epoch 18/80: current_loss=13.60537 | best_loss=13.60537
Epoch 19/80: current_loss=13.46524 | best_loss=13.46524
Epoch 20/80: current_loss=13.33857 | best_loss=13.33857
Epoch 21/80: current_loss=13.22136 | best_loss=13.22136
Epoch 22/80: current_loss=13.09810 | best_loss=13.09810
Epoch 23/80: current_loss=12.98116 | best_loss=12.98116
Epoch 24/80: current_loss=12.89171 | best_loss=12.89171
Epoch 25/80: current_loss=12.79766 | best_loss=12.79766
Epoch 26/80: current_loss=12.70630 | best_loss=12.70630
Epoch 27/80: current_loss=12.62238 | best_loss=12.62238
Epoch 28/80: current_loss=12.55023 | best_loss=12.55023
Epoch 29/80: current_loss=12.47792 | best_loss=12.47792
Epoch 30/80: current_loss=12.41598 | best_loss=12.41598
Epoch 31/80: current_loss=12.35125 | best_loss=12.35125
Epoch 32/80: current_loss=12.28132 | best_loss=12.28132
Epoch 33/80: current_loss=12.21526 | best_loss=12.21526
Epoch 34/80: current_loss=12.16055 | best_loss=12.16055
Epoch 35/80: current_loss=12.11425 | best_loss=12.11425
Epoch 36/80: current_loss=12.05703 | best_loss=12.05703
Epoch 37/80: current_loss=12.00660 | best_loss=12.00660
Epoch 38/80: current_loss=11.95663 | best_loss=11.95663
Epoch 39/80: current_loss=11.91593 | best_loss=11.91593
Epoch 40/80: current_loss=11.88436 | best_loss=11.88436
Epoch 41/80: current_loss=11.84712 | best_loss=11.84712
Epoch 42/80: current_loss=11.81511 | best_loss=11.81511
Epoch 43/80: current_loss=11.77985 | best_loss=11.77985
Epoch 44/80: current_loss=11.74930 | best_loss=11.74930
Epoch 45/80: current_loss=11.72410 | best_loss=11.72410
Epoch 46/80: current_loss=11.69017 | best_loss=11.69017
Epoch 47/80: current_loss=11.66042 | best_loss=11.66042
Epoch 48/80: current_loss=11.63410 | best_loss=11.63410
Epoch 49/80: current_loss=11.60592 | best_loss=11.60592
Epoch 50/80: current_loss=11.58187 | best_loss=11.58187
Epoch 51/80: current_loss=11.55787 | best_loss=11.55787
Epoch 52/80: current_loss=11.53946 | best_loss=11.53946
Epoch 53/80: current_loss=11.52154 | best_loss=11.52154
Epoch 54/80: current_loss=11.49987 | best_loss=11.49987
Epoch 55/80: current_loss=11.48080 | best_loss=11.48080
Epoch 56/80: current_loss=11.46278 | best_loss=11.46278
Epoch 57/80: current_loss=11.44886 | best_loss=11.44886
Epoch 58/80: current_loss=11.43405 | best_loss=11.43405
Epoch 59/80: current_loss=11.41769 | best_loss=11.41769
Epoch 60/80: current_loss=11.40264 | best_loss=11.40264
Epoch 61/80: current_loss=11.38894 | best_loss=11.38894
Epoch 62/80: current_loss=11.37928 | best_loss=11.37928
Epoch 63/80: current_loss=11.36715 | best_loss=11.36715
Epoch 64/80: current_loss=11.35094 | best_loss=11.35094
Epoch 65/80: current_loss=11.33896 | best_loss=11.33896
Epoch 66/80: current_loss=11.31913 | best_loss=11.31913
Epoch 67/80: current_loss=11.30890 | best_loss=11.30890
Epoch 68/80: current_loss=11.29971 | best_loss=11.29971
Epoch 69/80: current_loss=11.29065 | best_loss=11.29065
Epoch 70/80: current_loss=11.27844 | best_loss=11.27844
Epoch 71/80: current_loss=11.27047 | best_loss=11.27047
Epoch 72/80: current_loss=11.26276 | best_loss=11.26276
Epoch 73/80: current_loss=11.25819 | best_loss=11.25819
Epoch 74/80: current_loss=11.25460 | best_loss=11.25460
Epoch 75/80: current_loss=11.24512 | best_loss=11.24512
Epoch 76/80: current_loss=11.23610 | best_loss=11.23610
Epoch 77/80: current_loss=11.23108 | best_loss=11.23108
Epoch 78/80: current_loss=11.21896 | best_loss=11.21896
Epoch 79/80: current_loss=11.21130 | best_loss=11.21130
      explained_var=-0.05275 | mse_loss=10.79544
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.16185 | best_loss=8.16185
Epoch 1/80: current_loss=8.15417 | best_loss=8.15417
Epoch 2/80: current_loss=8.14535 | best_loss=8.14535
Epoch 3/80: current_loss=8.13642 | best_loss=8.13642
Epoch 4/80: current_loss=8.12841 | best_loss=8.12841
Epoch 5/80: current_loss=8.12136 | best_loss=8.12136
Epoch 6/80: current_loss=8.11447 | best_loss=8.11447
Epoch 7/80: current_loss=8.10814 | best_loss=8.10814
Epoch 8/80: current_loss=8.10033 | best_loss=8.10033
Epoch 9/80: current_loss=8.09281 | best_loss=8.09281
Epoch 10/80: current_loss=8.08612 | best_loss=8.08612
Epoch 11/80: current_loss=8.07946 | best_loss=8.07946
Epoch 12/80: current_loss=8.07277 | best_loss=8.07277
Epoch 13/80: current_loss=8.06701 | best_loss=8.06701
Epoch 14/80: current_loss=8.05994 | best_loss=8.05994
Epoch 15/80: current_loss=8.05286 | best_loss=8.05286
Epoch 16/80: current_loss=8.04690 | best_loss=8.04690
Epoch 17/80: current_loss=8.04039 | best_loss=8.04039
Epoch 18/80: current_loss=8.03465 | best_loss=8.03465
Epoch 19/80: current_loss=8.02860 | best_loss=8.02860
Epoch 20/80: current_loss=8.02289 | best_loss=8.02289
Epoch 21/80: current_loss=8.01695 | best_loss=8.01695
Epoch 22/80: current_loss=8.01240 | best_loss=8.01240
Epoch 23/80: current_loss=8.00688 | best_loss=8.00688
Epoch 24/80: current_loss=8.00224 | best_loss=8.00224
Epoch 25/80: current_loss=7.99777 | best_loss=7.99777
Epoch 26/80: current_loss=7.99227 | best_loss=7.99227
Epoch 27/80: current_loss=7.98714 | best_loss=7.98714
Epoch 28/80: current_loss=7.98221 | best_loss=7.98221
Epoch 29/80: current_loss=7.97746 | best_loss=7.97746
Epoch 30/80: current_loss=7.97279 | best_loss=7.97279
Epoch 31/80: current_loss=7.96813 | best_loss=7.96813
Epoch 32/80: current_loss=7.96430 | best_loss=7.96430
Epoch 33/80: current_loss=7.95870 | best_loss=7.95870
Epoch 34/80: current_loss=7.95376 | best_loss=7.95376
Epoch 35/80: current_loss=7.94900 | best_loss=7.94900
Epoch 36/80: current_loss=7.94470 | best_loss=7.94470
Epoch 37/80: current_loss=7.93988 | best_loss=7.93988
Epoch 38/80: current_loss=7.93476 | best_loss=7.93476
Epoch 39/80: current_loss=7.92881 | best_loss=7.92881
Epoch 40/80: current_loss=7.92366 | best_loss=7.92366
Epoch 41/80: current_loss=7.91929 | best_loss=7.91929
Epoch 42/80: current_loss=7.91536 | best_loss=7.91536
Epoch 43/80: current_loss=7.91060 | best_loss=7.91060
Epoch 44/80: current_loss=7.90643 | best_loss=7.90643
Epoch 45/80: current_loss=7.90224 | best_loss=7.90224
Epoch 46/80: current_loss=7.89765 | best_loss=7.89765
Epoch 47/80: current_loss=7.89389 | best_loss=7.89389
Epoch 48/80: current_loss=7.88960 | best_loss=7.88960
Epoch 49/80: current_loss=7.88587 | best_loss=7.88587
Epoch 50/80: current_loss=7.88156 | best_loss=7.88156
Epoch 51/80: current_loss=7.87766 | best_loss=7.87766
Epoch 52/80: current_loss=7.87349 | best_loss=7.87349
Epoch 53/80: current_loss=7.87016 | best_loss=7.87016
Epoch 54/80: current_loss=7.86723 | best_loss=7.86723
Epoch 55/80: current_loss=7.86276 | best_loss=7.86276
Epoch 56/80: current_loss=7.85853 | best_loss=7.85853
Epoch 57/80: current_loss=7.85499 | best_loss=7.85499
Epoch 58/80: current_loss=7.85050 | best_loss=7.85050
Epoch 59/80: current_loss=7.84700 | best_loss=7.84700
Epoch 60/80: current_loss=7.84308 | best_loss=7.84308
Epoch 61/80: current_loss=7.83979 | best_loss=7.83979
Epoch 62/80: current_loss=7.83637 | best_loss=7.83637
Epoch 63/80: current_loss=7.83371 | best_loss=7.83371
Epoch 64/80: current_loss=7.82990 | best_loss=7.82990
Epoch 65/80: current_loss=7.82541 | best_loss=7.82541
Epoch 66/80: current_loss=7.82093 | best_loss=7.82093
Epoch 67/80: current_loss=7.81691 | best_loss=7.81691
Epoch 68/80: current_loss=7.81276 | best_loss=7.81276
Epoch 69/80: current_loss=7.80881 | best_loss=7.80881
Epoch 70/80: current_loss=7.80428 | best_loss=7.80428
Epoch 71/80: current_loss=7.80067 | best_loss=7.80067
Epoch 72/80: current_loss=7.79714 | best_loss=7.79714
Epoch 73/80: current_loss=7.79399 | best_loss=7.79399
Epoch 74/80: current_loss=7.78953 | best_loss=7.78953
Epoch 75/80: current_loss=7.78475 | best_loss=7.78475
Epoch 76/80: current_loss=7.78197 | best_loss=7.78197
Epoch 77/80: current_loss=7.77888 | best_loss=7.77888
Epoch 78/80: current_loss=7.77568 | best_loss=7.77568
Epoch 79/80: current_loss=7.77176 | best_loss=7.77176
      explained_var=-0.03348 | mse_loss=7.84037
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.26244 | best_loss=9.26244
Epoch 1/80: current_loss=9.26848 | best_loss=9.26244
Epoch 2/80: current_loss=9.27110 | best_loss=9.26244
Epoch 3/80: current_loss=9.26984 | best_loss=9.26244
Epoch 4/80: current_loss=9.27089 | best_loss=9.26244
Epoch 5/80: current_loss=9.27447 | best_loss=9.26244
Epoch 6/80: current_loss=9.26901 | best_loss=9.26244
Epoch 7/80: current_loss=9.26260 | best_loss=9.26244
Epoch 8/80: current_loss=9.24827 | best_loss=9.24827
Epoch 9/80: current_loss=9.24515 | best_loss=9.24515
Epoch 10/80: current_loss=9.24690 | best_loss=9.24515
Epoch 11/80: current_loss=9.24954 | best_loss=9.24515
Epoch 12/80: current_loss=9.25377 | best_loss=9.24515
Epoch 13/80: current_loss=9.25836 | best_loss=9.24515
Epoch 14/80: current_loss=9.26203 | best_loss=9.24515
Epoch 15/80: current_loss=9.26262 | best_loss=9.24515
Epoch 16/80: current_loss=9.26526 | best_loss=9.24515
Epoch 17/80: current_loss=9.25548 | best_loss=9.24515
Epoch 18/80: current_loss=9.26056 | best_loss=9.24515
Epoch 19/80: current_loss=9.25248 | best_loss=9.24515
Epoch 20/80: current_loss=9.25135 | best_loss=9.24515
Epoch 21/80: current_loss=9.24102 | best_loss=9.24102
Epoch 22/80: current_loss=9.24132 | best_loss=9.24102
Epoch 23/80: current_loss=9.25067 | best_loss=9.24102
Epoch 24/80: current_loss=9.25079 | best_loss=9.24102
Epoch 25/80: current_loss=9.25309 | best_loss=9.24102
Epoch 26/80: current_loss=9.24884 | best_loss=9.24102
Epoch 27/80: current_loss=9.24383 | best_loss=9.24102
Epoch 28/80: current_loss=9.24024 | best_loss=9.24024
Epoch 29/80: current_loss=9.22895 | best_loss=9.22895
Epoch 30/80: current_loss=9.23309 | best_loss=9.22895
Epoch 31/80: current_loss=9.22934 | best_loss=9.22895
Epoch 32/80: current_loss=9.21977 | best_loss=9.21977
Epoch 33/80: current_loss=9.22258 | best_loss=9.21977
Epoch 34/80: current_loss=9.19798 | best_loss=9.19798
Epoch 35/80: current_loss=9.19781 | best_loss=9.19781
Epoch 36/80: current_loss=9.18163 | best_loss=9.18163
Epoch 37/80: current_loss=9.19066 | best_loss=9.18163
Epoch 38/80: current_loss=9.19658 | best_loss=9.18163
Epoch 39/80: current_loss=9.19284 | best_loss=9.18163
Epoch 40/80: current_loss=9.18451 | best_loss=9.18163
Epoch 41/80: current_loss=9.17828 | best_loss=9.17828
Epoch 42/80: current_loss=9.16776 | best_loss=9.16776
Epoch 43/80: current_loss=9.16647 | best_loss=9.16647
Epoch 44/80: current_loss=9.17081 | best_loss=9.16647
Epoch 45/80: current_loss=9.18056 | best_loss=9.16647
Epoch 46/80: current_loss=9.17143 | best_loss=9.16647
Epoch 47/80: current_loss=9.18141 | best_loss=9.16647
Epoch 48/80: current_loss=9.18643 | best_loss=9.16647
Epoch 49/80: current_loss=9.17493 | best_loss=9.16647
Epoch 50/80: current_loss=9.16073 | best_loss=9.16073
Epoch 51/80: current_loss=9.14825 | best_loss=9.14825
Epoch 52/80: current_loss=9.14499 | best_loss=9.14499
Epoch 53/80: current_loss=9.13553 | best_loss=9.13553
Epoch 54/80: current_loss=9.12127 | best_loss=9.12127
Epoch 55/80: current_loss=9.12787 | best_loss=9.12127
Epoch 56/80: current_loss=9.12111 | best_loss=9.12111
Epoch 57/80: current_loss=9.10119 | best_loss=9.10119
Epoch 58/80: current_loss=9.11251 | best_loss=9.10119
Epoch 59/80: current_loss=9.11646 | best_loss=9.10119
Epoch 60/80: current_loss=9.13100 | best_loss=9.10119
Epoch 61/80: current_loss=9.13178 | best_loss=9.10119
Epoch 62/80: current_loss=9.14333 | best_loss=9.10119
Epoch 63/80: current_loss=9.14599 | best_loss=9.10119
Epoch 64/80: current_loss=9.14749 | best_loss=9.10119
Epoch 65/80: current_loss=9.15378 | best_loss=9.10119
Epoch 66/80: current_loss=9.13434 | best_loss=9.10119
Epoch 67/80: current_loss=9.11367 | best_loss=9.10119
Epoch 68/80: current_loss=9.10958 | best_loss=9.10119
Epoch 69/80: current_loss=9.09682 | best_loss=9.09682
Epoch 70/80: current_loss=9.08329 | best_loss=9.08329
Epoch 71/80: current_loss=9.08150 | best_loss=9.08150
Epoch 72/80: current_loss=9.09736 | best_loss=9.08150
Epoch 73/80: current_loss=9.10336 | best_loss=9.08150
Epoch 74/80: current_loss=9.09367 | best_loss=9.08150
Epoch 75/80: current_loss=9.08237 | best_loss=9.08150
Epoch 76/80: current_loss=9.10178 | best_loss=9.08150
Epoch 77/80: current_loss=9.10056 | best_loss=9.08150
Epoch 78/80: current_loss=9.09832 | best_loss=9.08150
Epoch 79/80: current_loss=9.10246 | best_loss=9.08150
      explained_var=-0.01533 | mse_loss=8.97058
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.46575 | best_loss=9.46575
Epoch 1/80: current_loss=9.45740 | best_loss=9.45740
Epoch 2/80: current_loss=9.45030 | best_loss=9.45030
Epoch 3/80: current_loss=9.44146 | best_loss=9.44146
Epoch 4/80: current_loss=9.44014 | best_loss=9.44014
Epoch 5/80: current_loss=9.44377 | best_loss=9.44014
Epoch 6/80: current_loss=9.44702 | best_loss=9.44014
Epoch 7/80: current_loss=9.43875 | best_loss=9.43875
Epoch 8/80: current_loss=9.43211 | best_loss=9.43211
Epoch 9/80: current_loss=9.42855 | best_loss=9.42855
Epoch 10/80: current_loss=9.42658 | best_loss=9.42658
Epoch 11/80: current_loss=9.41788 | best_loss=9.41788
Epoch 12/80: current_loss=9.41735 | best_loss=9.41735
Epoch 13/80: current_loss=9.41389 | best_loss=9.41389
Epoch 14/80: current_loss=9.40960 | best_loss=9.40960
Epoch 15/80: current_loss=9.41182 | best_loss=9.40960
Epoch 16/80: current_loss=9.40412 | best_loss=9.40412
Epoch 17/80: current_loss=9.40570 | best_loss=9.40412
Epoch 18/80: current_loss=9.40571 | best_loss=9.40412
Epoch 19/80: current_loss=9.40253 | best_loss=9.40253
Epoch 20/80: current_loss=9.40044 | best_loss=9.40044
Epoch 21/80: current_loss=9.39876 | best_loss=9.39876
Epoch 22/80: current_loss=9.39655 | best_loss=9.39655
Epoch 23/80: current_loss=9.39374 | best_loss=9.39374
Epoch 24/80: current_loss=9.39608 | best_loss=9.39374
Epoch 25/80: current_loss=9.40122 | best_loss=9.39374
Epoch 26/80: current_loss=9.39683 | best_loss=9.39374
Epoch 27/80: current_loss=9.38122 | best_loss=9.38122
Epoch 28/80: current_loss=9.37937 | best_loss=9.37937
Epoch 29/80: current_loss=9.37428 | best_loss=9.37428
Epoch 30/80: current_loss=9.37141 | best_loss=9.37141
Epoch 31/80: current_loss=9.37236 | best_loss=9.37141
Epoch 32/80: current_loss=9.36680 | best_loss=9.36680
Epoch 33/80: current_loss=9.36806 | best_loss=9.36680
Epoch 34/80: current_loss=9.36662 | best_loss=9.36662
Epoch 35/80: current_loss=9.36653 | best_loss=9.36653
Epoch 36/80: current_loss=9.37273 | best_loss=9.36653
Epoch 37/80: current_loss=9.36692 | best_loss=9.36653
Epoch 38/80: current_loss=9.36367 | best_loss=9.36367
Epoch 39/80: current_loss=9.36334 | best_loss=9.36334
Epoch 40/80: current_loss=9.36046 | best_loss=9.36046
Epoch 41/80: current_loss=9.35610 | best_loss=9.35610
Epoch 42/80: current_loss=9.35656 | best_loss=9.35610
Epoch 43/80: current_loss=9.35814 | best_loss=9.35610
Epoch 44/80: current_loss=9.35238 | best_loss=9.35238
Epoch 45/80: current_loss=9.35384 | best_loss=9.35238
Epoch 46/80: current_loss=9.35651 | best_loss=9.35238
Epoch 47/80: current_loss=9.35821 | best_loss=9.35238
Epoch 48/80: current_loss=9.35485 | best_loss=9.35238
Epoch 49/80: current_loss=9.35904 | best_loss=9.35238
Epoch 50/80: current_loss=9.35303 | best_loss=9.35238
Epoch 51/80: current_loss=9.34345 | best_loss=9.34345
Epoch 52/80: current_loss=9.34295 | best_loss=9.34295
Epoch 53/80: current_loss=9.33780 | best_loss=9.33780
Epoch 54/80: current_loss=9.33806 | best_loss=9.33780
Epoch 55/80: current_loss=9.32978 | best_loss=9.32978
Epoch 56/80: current_loss=9.32595 | best_loss=9.32595
Epoch 57/80: current_loss=9.32535 | best_loss=9.32535
Epoch 58/80: current_loss=9.32871 | best_loss=9.32535
Epoch 59/80: current_loss=9.32894 | best_loss=9.32535
Epoch 60/80: current_loss=9.32807 | best_loss=9.32535
Epoch 61/80: current_loss=9.32841 | best_loss=9.32535
Epoch 62/80: current_loss=9.32775 | best_loss=9.32535
Epoch 63/80: current_loss=9.33112 | best_loss=9.32535
Epoch 64/80: current_loss=9.33906 | best_loss=9.32535
Epoch 65/80: current_loss=9.32382 | best_loss=9.32382
Epoch 66/80: current_loss=9.31632 | best_loss=9.31632
Epoch 67/80: current_loss=9.31863 | best_loss=9.31632
Epoch 68/80: current_loss=9.31032 | best_loss=9.31032
Epoch 69/80: current_loss=9.31722 | best_loss=9.31032
Epoch 70/80: current_loss=9.31993 | best_loss=9.31032
Epoch 71/80: current_loss=9.32583 | best_loss=9.31032
Epoch 72/80: current_loss=9.32036 | best_loss=9.31032
Epoch 73/80: current_loss=9.31723 | best_loss=9.31032
Epoch 74/80: current_loss=9.31788 | best_loss=9.31032
Epoch 75/80: current_loss=9.32748 | best_loss=9.31032
Epoch 76/80: current_loss=9.32614 | best_loss=9.31032
Epoch 77/80: current_loss=9.31324 | best_loss=9.31032
Epoch 78/80: current_loss=9.31084 | best_loss=9.31032
Epoch 79/80: current_loss=9.30840 | best_loss=9.30840
      explained_var=0.01060 | mse_loss=9.25636
----------------------------------------------
Average early_stopping_point: 80| avg_exp_var=-0.03936| avg_loss=11.19310
----------------------------------------------


----------------------------------------------
Params for Trial 1
{'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=104.38921 | best_loss=104.38921
Epoch 1/80: current_loss=103.86015 | best_loss=103.86015
Epoch 2/80: current_loss=103.33953 | best_loss=103.33953
Epoch 3/80: current_loss=102.82085 | best_loss=102.82085
Epoch 4/80: current_loss=102.30317 | best_loss=102.30317
Epoch 5/80: current_loss=101.78188 | best_loss=101.78188
Epoch 6/80: current_loss=101.25601 | best_loss=101.25601
Epoch 7/80: current_loss=100.71938 | best_loss=100.71938
Epoch 8/80: current_loss=100.16327 | best_loss=100.16327
Epoch 9/80: current_loss=99.58215 | best_loss=99.58215
Epoch 10/80: current_loss=98.97751 | best_loss=98.97751
Epoch 11/80: current_loss=98.33426 | best_loss=98.33426
Epoch 12/80: current_loss=97.65452 | best_loss=97.65452
Epoch 13/80: current_loss=96.92714 | best_loss=96.92714
Epoch 14/80: current_loss=96.14570 | best_loss=96.14570
Epoch 15/80: current_loss=95.30988 | best_loss=95.30988
Epoch 16/80: current_loss=94.40113 | best_loss=94.40113
Epoch 17/80: current_loss=93.41560 | best_loss=93.41560
Epoch 18/80: current_loss=92.35287 | best_loss=92.35287
Epoch 19/80: current_loss=91.20031 | best_loss=91.20031
Epoch 20/80: current_loss=89.95080 | best_loss=89.95080
Epoch 21/80: current_loss=88.60715 | best_loss=88.60715
Epoch 22/80: current_loss=87.17404 | best_loss=87.17404
Epoch 23/80: current_loss=85.65237 | best_loss=85.65237
Epoch 24/80: current_loss=84.06456 | best_loss=84.06456
Epoch 25/80: current_loss=82.41525 | best_loss=82.41525
Epoch 26/80: current_loss=80.72767 | best_loss=80.72767
Epoch 27/80: current_loss=79.00109 | best_loss=79.00109
Epoch 28/80: current_loss=77.25558 | best_loss=77.25558
Epoch 29/80: current_loss=75.50607 | best_loss=75.50607
Epoch 30/80: current_loss=73.80452 | best_loss=73.80452
Epoch 31/80: current_loss=72.11583 | best_loss=72.11583
Epoch 32/80: current_loss=70.46338 | best_loss=70.46338
Epoch 33/80: current_loss=68.84427 | best_loss=68.84427
Epoch 34/80: current_loss=67.27346 | best_loss=67.27346
Epoch 35/80: current_loss=65.76637 | best_loss=65.76637
Epoch 36/80: current_loss=64.29768 | best_loss=64.29768
Epoch 37/80: current_loss=62.89313 | best_loss=62.89313
Epoch 38/80: current_loss=61.52702 | best_loss=61.52702
Epoch 39/80: current_loss=60.21849 | best_loss=60.21849
Epoch 40/80: current_loss=58.96314 | best_loss=58.96314
Epoch 41/80: current_loss=57.75250 | best_loss=57.75250
Epoch 42/80: current_loss=56.61672 | best_loss=56.61672
Epoch 43/80: current_loss=55.50645 | best_loss=55.50645
Epoch 44/80: current_loss=54.46670 | best_loss=54.46670
Epoch 45/80: current_loss=53.49921 | best_loss=53.49921
Epoch 46/80: current_loss=52.55674 | best_loss=52.55674
Epoch 47/80: current_loss=51.65936 | best_loss=51.65936
Epoch 48/80: current_loss=50.80201 | best_loss=50.80201
Epoch 49/80: current_loss=49.98656 | best_loss=49.98656
Epoch 50/80: current_loss=49.20790 | best_loss=49.20790
Epoch 51/80: current_loss=48.46610 | best_loss=48.46610
Epoch 52/80: current_loss=47.76039 | best_loss=47.76039
Epoch 53/80: current_loss=47.07705 | best_loss=47.07705
Epoch 54/80: current_loss=46.42505 | best_loss=46.42505
Epoch 55/80: current_loss=45.79931 | best_loss=45.79931
Epoch 56/80: current_loss=45.20693 | best_loss=45.20693
Epoch 57/80: current_loss=44.63885 | best_loss=44.63885
Epoch 58/80: current_loss=44.09048 | best_loss=44.09048
Epoch 59/80: current_loss=43.55897 | best_loss=43.55897
Epoch 60/80: current_loss=43.04047 | best_loss=43.04047
Epoch 61/80: current_loss=42.54552 | best_loss=42.54552
Epoch 62/80: current_loss=42.07672 | best_loss=42.07672
Epoch 63/80: current_loss=41.60502 | best_loss=41.60502
Epoch 64/80: current_loss=41.15980 | best_loss=41.15980
Epoch 65/80: current_loss=40.72840 | best_loss=40.72840
Epoch 66/80: current_loss=40.30826 | best_loss=40.30826
Epoch 67/80: current_loss=39.89739 | best_loss=39.89739
Epoch 68/80: current_loss=39.50580 | best_loss=39.50580
Epoch 69/80: current_loss=39.12436 | best_loss=39.12436
Epoch 70/80: current_loss=38.75167 | best_loss=38.75167
Epoch 71/80: current_loss=38.39492 | best_loss=38.39492
Epoch 72/80: current_loss=38.04250 | best_loss=38.04250
Epoch 73/80: current_loss=37.69532 | best_loss=37.69532
Epoch 74/80: current_loss=37.36716 | best_loss=37.36716
Epoch 75/80: current_loss=37.04179 | best_loss=37.04179
Epoch 76/80: current_loss=36.72702 | best_loss=36.72702
Epoch 77/80: current_loss=36.42222 | best_loss=36.42222
Epoch 78/80: current_loss=36.12431 | best_loss=36.12431
Epoch 79/80: current_loss=35.83232 | best_loss=35.83232
      explained_var=-0.08713 | mse_loss=36.59312
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=35.78982 | best_loss=35.78982
Epoch 1/80: current_loss=35.40257 | best_loss=35.40257
Epoch 2/80: current_loss=35.03496 | best_loss=35.03496
Epoch 3/80: current_loss=34.68338 | best_loss=34.68338
Epoch 4/80: current_loss=34.35111 | best_loss=34.35111
Epoch 5/80: current_loss=34.03040 | best_loss=34.03040
Epoch 6/80: current_loss=33.72467 | best_loss=33.72467
Epoch 7/80: current_loss=33.42783 | best_loss=33.42783
Epoch 8/80: current_loss=33.14527 | best_loss=33.14527
Epoch 9/80: current_loss=32.86968 | best_loss=32.86968
Epoch 10/80: current_loss=32.60903 | best_loss=32.60903
Epoch 11/80: current_loss=32.35137 | best_loss=32.35137
Epoch 12/80: current_loss=32.10309 | best_loss=32.10309
Epoch 13/80: current_loss=31.86585 | best_loss=31.86585
Epoch 14/80: current_loss=31.63190 | best_loss=31.63190
Epoch 15/80: current_loss=31.40081 | best_loss=31.40081
Epoch 16/80: current_loss=31.17741 | best_loss=31.17741
Epoch 17/80: current_loss=30.95663 | best_loss=30.95663
Epoch 18/80: current_loss=30.74194 | best_loss=30.74194
Epoch 19/80: current_loss=30.52994 | best_loss=30.52994
Epoch 20/80: current_loss=30.32030 | best_loss=30.32030
Epoch 21/80: current_loss=30.12052 | best_loss=30.12052
Epoch 22/80: current_loss=29.91990 | best_loss=29.91990
Epoch 23/80: current_loss=29.72534 | best_loss=29.72534
Epoch 24/80: current_loss=29.53263 | best_loss=29.53263
Epoch 25/80: current_loss=29.34353 | best_loss=29.34353
Epoch 26/80: current_loss=29.15293 | best_loss=29.15293
Epoch 27/80: current_loss=28.96694 | best_loss=28.96694
Epoch 28/80: current_loss=28.78667 | best_loss=28.78667
Epoch 29/80: current_loss=28.60463 | best_loss=28.60463
Epoch 30/80: current_loss=28.42669 | best_loss=28.42669
Epoch 31/80: current_loss=28.25463 | best_loss=28.25463
Epoch 32/80: current_loss=28.07706 | best_loss=28.07706
Epoch 33/80: current_loss=27.90566 | best_loss=27.90566
Epoch 34/80: current_loss=27.73326 | best_loss=27.73326
Epoch 35/80: current_loss=27.56425 | best_loss=27.56425
Epoch 36/80: current_loss=27.39306 | best_loss=27.39306
Epoch 37/80: current_loss=27.22950 | best_loss=27.22950
Epoch 38/80: current_loss=27.06457 | best_loss=27.06457
Epoch 39/80: current_loss=26.90454 | best_loss=26.90454
Epoch 40/80: current_loss=26.74641 | best_loss=26.74641
Epoch 41/80: current_loss=26.58869 | best_loss=26.58869
Epoch 42/80: current_loss=26.43376 | best_loss=26.43376
Epoch 43/80: current_loss=26.28363 | best_loss=26.28363
Epoch 44/80: current_loss=26.13278 | best_loss=26.13278
Epoch 45/80: current_loss=25.98280 | best_loss=25.98280
Epoch 46/80: current_loss=25.83725 | best_loss=25.83725
Epoch 47/80: current_loss=25.69104 | best_loss=25.69104
Epoch 48/80: current_loss=25.54929 | best_loss=25.54929
Epoch 49/80: current_loss=25.40926 | best_loss=25.40926
Epoch 50/80: current_loss=25.27203 | best_loss=25.27203
Epoch 51/80: current_loss=25.13471 | best_loss=25.13471
Epoch 52/80: current_loss=24.99972 | best_loss=24.99972
Epoch 53/80: current_loss=24.86450 | best_loss=24.86450
Epoch 54/80: current_loss=24.73387 | best_loss=24.73387
Epoch 55/80: current_loss=24.60336 | best_loss=24.60336
Epoch 56/80: current_loss=24.47201 | best_loss=24.47201
Epoch 57/80: current_loss=24.34593 | best_loss=24.34593
Epoch 58/80: current_loss=24.21922 | best_loss=24.21922
Epoch 59/80: current_loss=24.09529 | best_loss=24.09529
Epoch 60/80: current_loss=23.97289 | best_loss=23.97289
Epoch 61/80: current_loss=23.85288 | best_loss=23.85288
Epoch 62/80: current_loss=23.73412 | best_loss=23.73412
Epoch 63/80: current_loss=23.61760 | best_loss=23.61760
Epoch 64/80: current_loss=23.49855 | best_loss=23.49855
Epoch 65/80: current_loss=23.38377 | best_loss=23.38377
Epoch 66/80: current_loss=23.27056 | best_loss=23.27056
Epoch 67/80: current_loss=23.15720 | best_loss=23.15720
Epoch 68/80: current_loss=23.04195 | best_loss=23.04195
Epoch 69/80: current_loss=22.93191 | best_loss=22.93191
Epoch 70/80: current_loss=22.82137 | best_loss=22.82137
Epoch 71/80: current_loss=22.71278 | best_loss=22.71278
Epoch 72/80: current_loss=22.60455 | best_loss=22.60455
Epoch 73/80: current_loss=22.49719 | best_loss=22.49719
Epoch 74/80: current_loss=22.39124 | best_loss=22.39124
Epoch 75/80: current_loss=22.28948 | best_loss=22.28948
Epoch 76/80: current_loss=22.18668 | best_loss=22.18668
Epoch 77/80: current_loss=22.08194 | best_loss=22.08194
Epoch 78/80: current_loss=21.97911 | best_loss=21.97911
Epoch 79/80: current_loss=21.88134 | best_loss=21.88134
      explained_var=-0.06133 | mse_loss=21.30831
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.70104 | best_loss=16.70104
Epoch 1/80: current_loss=16.58926 | best_loss=16.58926
Epoch 2/80: current_loss=16.48137 | best_loss=16.48137
Epoch 3/80: current_loss=16.37534 | best_loss=16.37534
Epoch 4/80: current_loss=16.26862 | best_loss=16.26862
Epoch 5/80: current_loss=16.16653 | best_loss=16.16653
Epoch 6/80: current_loss=16.06623 | best_loss=16.06623
Epoch 7/80: current_loss=15.96909 | best_loss=15.96909
Epoch 8/80: current_loss=15.86957 | best_loss=15.86957
Epoch 9/80: current_loss=15.77465 | best_loss=15.77465
Epoch 10/80: current_loss=15.67993 | best_loss=15.67993
Epoch 11/80: current_loss=15.58568 | best_loss=15.58568
Epoch 12/80: current_loss=15.49327 | best_loss=15.49327
Epoch 13/80: current_loss=15.40206 | best_loss=15.40206
Epoch 14/80: current_loss=15.31443 | best_loss=15.31443
Epoch 15/80: current_loss=15.22357 | best_loss=15.22357
Epoch 16/80: current_loss=15.13771 | best_loss=15.13771
Epoch 17/80: current_loss=15.05141 | best_loss=15.05141
Epoch 18/80: current_loss=14.96590 | best_loss=14.96590
Epoch 19/80: current_loss=14.88106 | best_loss=14.88106
Epoch 20/80: current_loss=14.80155 | best_loss=14.80155
Epoch 21/80: current_loss=14.71968 | best_loss=14.71968
Epoch 22/80: current_loss=14.64095 | best_loss=14.64095
Epoch 23/80: current_loss=14.56032 | best_loss=14.56032
Epoch 24/80: current_loss=14.48067 | best_loss=14.48067
Epoch 25/80: current_loss=14.40134 | best_loss=14.40134
Epoch 26/80: current_loss=14.32648 | best_loss=14.32648
Epoch 27/80: current_loss=14.24884 | best_loss=14.24884
Epoch 28/80: current_loss=14.17352 | best_loss=14.17352
Epoch 29/80: current_loss=14.09945 | best_loss=14.09945
Epoch 30/80: current_loss=14.02655 | best_loss=14.02655
Epoch 31/80: current_loss=13.95418 | best_loss=13.95418
Epoch 32/80: current_loss=13.87929 | best_loss=13.87929
Epoch 33/80: current_loss=13.80560 | best_loss=13.80560
Epoch 34/80: current_loss=13.73742 | best_loss=13.73742
Epoch 35/80: current_loss=13.66561 | best_loss=13.66561
Epoch 36/80: current_loss=13.59636 | best_loss=13.59636
Epoch 37/80: current_loss=13.52661 | best_loss=13.52661
Epoch 38/80: current_loss=13.45836 | best_loss=13.45836
Epoch 39/80: current_loss=13.38876 | best_loss=13.38876
Epoch 40/80: current_loss=13.32124 | best_loss=13.32124
Epoch 41/80: current_loss=13.25355 | best_loss=13.25355
Epoch 42/80: current_loss=13.18676 | best_loss=13.18676
Epoch 43/80: current_loss=13.12147 | best_loss=13.12147
Epoch 44/80: current_loss=13.05618 | best_loss=13.05618
Epoch 45/80: current_loss=12.99398 | best_loss=12.99398
Epoch 46/80: current_loss=12.92952 | best_loss=12.92952
Epoch 47/80: current_loss=12.86607 | best_loss=12.86607
Epoch 48/80: current_loss=12.80459 | best_loss=12.80459
Epoch 49/80: current_loss=12.74171 | best_loss=12.74171
Epoch 50/80: current_loss=12.68073 | best_loss=12.68073
Epoch 51/80: current_loss=12.62010 | best_loss=12.62010
Epoch 52/80: current_loss=12.55832 | best_loss=12.55832
Epoch 53/80: current_loss=12.49961 | best_loss=12.49961
Epoch 54/80: current_loss=12.44187 | best_loss=12.44187
Epoch 55/80: current_loss=12.38373 | best_loss=12.38373
Epoch 56/80: current_loss=12.32539 | best_loss=12.32539
Epoch 57/80: current_loss=12.26717 | best_loss=12.26717
Epoch 58/80: current_loss=12.20995 | best_loss=12.20995
Epoch 59/80: current_loss=12.15071 | best_loss=12.15071
Epoch 60/80: current_loss=12.09536 | best_loss=12.09536
Epoch 61/80: current_loss=12.03875 | best_loss=12.03875
Epoch 62/80: current_loss=11.98410 | best_loss=11.98410
Epoch 63/80: current_loss=11.93009 | best_loss=11.93009
Epoch 64/80: current_loss=11.87547 | best_loss=11.87547
Epoch 65/80: current_loss=11.82188 | best_loss=11.82188
Epoch 66/80: current_loss=11.76887 | best_loss=11.76887
Epoch 67/80: current_loss=11.71654 | best_loss=11.71654
Epoch 68/80: current_loss=11.66527 | best_loss=11.66527
Epoch 69/80: current_loss=11.61341 | best_loss=11.61341
Epoch 70/80: current_loss=11.56279 | best_loss=11.56279
Epoch 71/80: current_loss=11.51200 | best_loss=11.51200
Epoch 72/80: current_loss=11.46037 | best_loss=11.46037
Epoch 73/80: current_loss=11.41131 | best_loss=11.41131
Epoch 74/80: current_loss=11.35916 | best_loss=11.35916
Epoch 75/80: current_loss=11.31069 | best_loss=11.31069
Epoch 76/80: current_loss=11.26315 | best_loss=11.26315
Epoch 77/80: current_loss=11.21485 | best_loss=11.21485
Epoch 78/80: current_loss=11.16604 | best_loss=11.16604
Epoch 79/80: current_loss=11.12066 | best_loss=11.12066
      explained_var=-0.06074 | mse_loss=11.25956
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=13.71527 | best_loss=13.71527
Epoch 1/80: current_loss=13.65133 | best_loss=13.65133
Epoch 2/80: current_loss=13.58584 | best_loss=13.58584
Epoch 3/80: current_loss=13.52301 | best_loss=13.52301
Epoch 4/80: current_loss=13.46133 | best_loss=13.46133
Epoch 5/80: current_loss=13.39820 | best_loss=13.39820
Epoch 6/80: current_loss=13.33956 | best_loss=13.33956
Epoch 7/80: current_loss=13.27957 | best_loss=13.27957
Epoch 8/80: current_loss=13.21781 | best_loss=13.21781
Epoch 9/80: current_loss=13.16071 | best_loss=13.16071
Epoch 10/80: current_loss=13.10349 | best_loss=13.10349
Epoch 11/80: current_loss=13.04368 | best_loss=13.04368
Epoch 12/80: current_loss=12.98665 | best_loss=12.98665
Epoch 13/80: current_loss=12.93305 | best_loss=12.93305
Epoch 14/80: current_loss=12.87613 | best_loss=12.87613
Epoch 15/80: current_loss=12.82128 | best_loss=12.82128
Epoch 16/80: current_loss=12.76674 | best_loss=12.76674
Epoch 17/80: current_loss=12.71127 | best_loss=12.71127
Epoch 18/80: current_loss=12.65883 | best_loss=12.65883
Epoch 19/80: current_loss=12.60573 | best_loss=12.60573
Epoch 20/80: current_loss=12.55409 | best_loss=12.55409
Epoch 21/80: current_loss=12.50141 | best_loss=12.50141
Epoch 22/80: current_loss=12.45054 | best_loss=12.45054
Epoch 23/80: current_loss=12.39809 | best_loss=12.39809
Epoch 24/80: current_loss=12.34720 | best_loss=12.34720
Epoch 25/80: current_loss=12.29827 | best_loss=12.29827
Epoch 26/80: current_loss=12.25013 | best_loss=12.25013
Epoch 27/80: current_loss=12.20283 | best_loss=12.20283
Epoch 28/80: current_loss=12.15553 | best_loss=12.15553
Epoch 29/80: current_loss=12.10939 | best_loss=12.10939
Epoch 30/80: current_loss=12.06298 | best_loss=12.06298
Epoch 31/80: current_loss=12.01566 | best_loss=12.01566
Epoch 32/80: current_loss=11.96953 | best_loss=11.96953
Epoch 33/80: current_loss=11.92585 | best_loss=11.92585
Epoch 34/80: current_loss=11.87893 | best_loss=11.87893
Epoch 35/80: current_loss=11.83626 | best_loss=11.83626
Epoch 36/80: current_loss=11.79499 | best_loss=11.79499
Epoch 37/80: current_loss=11.75300 | best_loss=11.75300
Epoch 38/80: current_loss=11.71120 | best_loss=11.71120
Epoch 39/80: current_loss=11.67053 | best_loss=11.67053
Epoch 40/80: current_loss=11.63001 | best_loss=11.63001
Epoch 41/80: current_loss=11.58642 | best_loss=11.58642
Epoch 42/80: current_loss=11.54582 | best_loss=11.54582
Epoch 43/80: current_loss=11.50509 | best_loss=11.50509
Epoch 44/80: current_loss=11.46758 | best_loss=11.46758
Epoch 45/80: current_loss=11.42929 | best_loss=11.42929
Epoch 46/80: current_loss=11.39201 | best_loss=11.39201
Epoch 47/80: current_loss=11.35230 | best_loss=11.35230
Epoch 48/80: current_loss=11.31582 | best_loss=11.31582
Epoch 49/80: current_loss=11.28136 | best_loss=11.28136
Epoch 50/80: current_loss=11.24492 | best_loss=11.24492
Epoch 51/80: current_loss=11.20862 | best_loss=11.20862
Epoch 52/80: current_loss=11.17190 | best_loss=11.17190
Epoch 53/80: current_loss=11.13772 | best_loss=11.13772
Epoch 54/80: current_loss=11.10424 | best_loss=11.10424
Epoch 55/80: current_loss=11.06931 | best_loss=11.06931
Epoch 56/80: current_loss=11.03544 | best_loss=11.03544
Epoch 57/80: current_loss=11.00023 | best_loss=11.00023
Epoch 58/80: current_loss=10.96648 | best_loss=10.96648
Epoch 59/80: current_loss=10.93385 | best_loss=10.93385
Epoch 60/80: current_loss=10.90326 | best_loss=10.90326
Epoch 61/80: current_loss=10.87015 | best_loss=10.87015
Epoch 62/80: current_loss=10.83778 | best_loss=10.83778
Epoch 63/80: current_loss=10.80556 | best_loss=10.80556
Epoch 64/80: current_loss=10.77397 | best_loss=10.77397
Epoch 65/80: current_loss=10.74492 | best_loss=10.74492
Epoch 66/80: current_loss=10.71362 | best_loss=10.71362
Epoch 67/80: current_loss=10.68381 | best_loss=10.68381
Epoch 68/80: current_loss=10.65551 | best_loss=10.65551
Epoch 69/80: current_loss=10.62755 | best_loss=10.62755
Epoch 70/80: current_loss=10.59814 | best_loss=10.59814
Epoch 71/80: current_loss=10.56978 | best_loss=10.56978
Epoch 72/80: current_loss=10.54293 | best_loss=10.54293
Epoch 73/80: current_loss=10.51423 | best_loss=10.51423
Epoch 74/80: current_loss=10.48761 | best_loss=10.48761
Epoch 75/80: current_loss=10.46222 | best_loss=10.46222
Epoch 76/80: current_loss=10.43698 | best_loss=10.43698
Epoch 77/80: current_loss=10.41150 | best_loss=10.41150
Epoch 78/80: current_loss=10.38682 | best_loss=10.38682
Epoch 79/80: current_loss=10.36079 | best_loss=10.36079
      explained_var=-0.03842 | mse_loss=10.14860
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.59366 | best_loss=10.59366
Epoch 1/80: current_loss=10.56665 | best_loss=10.56665
Epoch 2/80: current_loss=10.54034 | best_loss=10.54034
Epoch 3/80: current_loss=10.51527 | best_loss=10.51527
Epoch 4/80: current_loss=10.48939 | best_loss=10.48939
Epoch 5/80: current_loss=10.46509 | best_loss=10.46509
Epoch 6/80: current_loss=10.44002 | best_loss=10.44002
Epoch 7/80: current_loss=10.41740 | best_loss=10.41740
Epoch 8/80: current_loss=10.39365 | best_loss=10.39365
Epoch 9/80: current_loss=10.37070 | best_loss=10.37070
Epoch 10/80: current_loss=10.34538 | best_loss=10.34538
Epoch 11/80: current_loss=10.32433 | best_loss=10.32433
Epoch 12/80: current_loss=10.30280 | best_loss=10.30280
Epoch 13/80: current_loss=10.28169 | best_loss=10.28169
Epoch 14/80: current_loss=10.26180 | best_loss=10.26180
Epoch 15/80: current_loss=10.24280 | best_loss=10.24280
Epoch 16/80: current_loss=10.22315 | best_loss=10.22315
Epoch 17/80: current_loss=10.20491 | best_loss=10.20491
Epoch 18/80: current_loss=10.18489 | best_loss=10.18489
Epoch 19/80: current_loss=10.16613 | best_loss=10.16613
Epoch 20/80: current_loss=10.14935 | best_loss=10.14935
Epoch 21/80: current_loss=10.13070 | best_loss=10.13070
Epoch 22/80: current_loss=10.11199 | best_loss=10.11199
Epoch 23/80: current_loss=10.09513 | best_loss=10.09513
Epoch 24/80: current_loss=10.07930 | best_loss=10.07930
Epoch 25/80: current_loss=10.06293 | best_loss=10.06293
Epoch 26/80: current_loss=10.04818 | best_loss=10.04818
Epoch 27/80: current_loss=10.03084 | best_loss=10.03084
Epoch 28/80: current_loss=10.01504 | best_loss=10.01504
Epoch 29/80: current_loss=9.99931 | best_loss=9.99931
Epoch 30/80: current_loss=9.98455 | best_loss=9.98455
Epoch 31/80: current_loss=9.97139 | best_loss=9.97139
Epoch 32/80: current_loss=9.95771 | best_loss=9.95771
Epoch 33/80: current_loss=9.94308 | best_loss=9.94308
Epoch 34/80: current_loss=9.92972 | best_loss=9.92972
Epoch 35/80: current_loss=9.91674 | best_loss=9.91674
Epoch 36/80: current_loss=9.90582 | best_loss=9.90582
Epoch 37/80: current_loss=9.89320 | best_loss=9.89320
Epoch 38/80: current_loss=9.88117 | best_loss=9.88117
Epoch 39/80: current_loss=9.87030 | best_loss=9.87030
Epoch 40/80: current_loss=9.85858 | best_loss=9.85858
Epoch 41/80: current_loss=9.84824 | best_loss=9.84824
Epoch 42/80: current_loss=9.83978 | best_loss=9.83978
Epoch 43/80: current_loss=9.82917 | best_loss=9.82917
Epoch 44/80: current_loss=9.81926 | best_loss=9.81926
Epoch 45/80: current_loss=9.80791 | best_loss=9.80791
Epoch 46/80: current_loss=9.79820 | best_loss=9.79820
Epoch 47/80: current_loss=9.78877 | best_loss=9.78877
Epoch 48/80: current_loss=9.77914 | best_loss=9.77914
Epoch 49/80: current_loss=9.77049 | best_loss=9.77049
Epoch 50/80: current_loss=9.76200 | best_loss=9.76200
Epoch 51/80: current_loss=9.75336 | best_loss=9.75336
Epoch 52/80: current_loss=9.74592 | best_loss=9.74592
Epoch 53/80: current_loss=9.73805 | best_loss=9.73805
Epoch 54/80: current_loss=9.72964 | best_loss=9.72964
Epoch 55/80: current_loss=9.72235 | best_loss=9.72235
Epoch 56/80: current_loss=9.71601 | best_loss=9.71601
Epoch 57/80: current_loss=9.70854 | best_loss=9.70854
Epoch 58/80: current_loss=9.70165 | best_loss=9.70165
Epoch 59/80: current_loss=9.69386 | best_loss=9.69386
Epoch 60/80: current_loss=9.68760 | best_loss=9.68760
Epoch 61/80: current_loss=9.68045 | best_loss=9.68045
Epoch 62/80: current_loss=9.67364 | best_loss=9.67364
Epoch 63/80: current_loss=9.66809 | best_loss=9.66809
Epoch 64/80: current_loss=9.66227 | best_loss=9.66227
Epoch 65/80: current_loss=9.65724 | best_loss=9.65724
Epoch 66/80: current_loss=9.65173 | best_loss=9.65173
Epoch 67/80: current_loss=9.64576 | best_loss=9.64576
Epoch 68/80: current_loss=9.64094 | best_loss=9.64094
Epoch 69/80: current_loss=9.63568 | best_loss=9.63568
Epoch 70/80: current_loss=9.62976 | best_loss=9.62976
Epoch 71/80: current_loss=9.62488 | best_loss=9.62488
Epoch 72/80: current_loss=9.61991 | best_loss=9.61991
Epoch 73/80: current_loss=9.61477 | best_loss=9.61477
Epoch 74/80: current_loss=9.61004 | best_loss=9.61004
Epoch 75/80: current_loss=9.60569 | best_loss=9.60569
Epoch 76/80: current_loss=9.60090 | best_loss=9.60090
Epoch 77/80: current_loss=9.59693 | best_loss=9.59693
Epoch 78/80: current_loss=9.59384 | best_loss=9.59384
Epoch 79/80: current_loss=9.58994 | best_loss=9.58994
      explained_var=-0.01178 | mse_loss=9.49584
----------------------------------------------
Average early_stopping_point: 80| avg_exp_var=-0.05188| avg_loss=17.76109
----------------------------------------------


----------------------------------------------
Params for Trial 2
{'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=22.16225 | best_loss=22.16225
Epoch 1/80: current_loss=15.77821 | best_loss=15.77821
Epoch 2/80: current_loss=14.94366 | best_loss=14.94366
Epoch 3/80: current_loss=14.56558 | best_loss=14.56558
Epoch 4/80: current_loss=14.15047 | best_loss=14.15047
Epoch 5/80: current_loss=13.85047 | best_loss=13.85047
Epoch 6/80: current_loss=13.54871 | best_loss=13.54871
Epoch 7/80: current_loss=13.30258 | best_loss=13.30258
Epoch 8/80: current_loss=13.09796 | best_loss=13.09796
Epoch 9/80: current_loss=12.89296 | best_loss=12.89296
Epoch 10/80: current_loss=12.69439 | best_loss=12.69439
Epoch 11/80: current_loss=12.55318 | best_loss=12.55318
Epoch 12/80: current_loss=12.42362 | best_loss=12.42362
Epoch 13/80: current_loss=12.27296 | best_loss=12.27296
Epoch 14/80: current_loss=12.15103 | best_loss=12.15103
Epoch 15/80: current_loss=12.04136 | best_loss=12.04136
Epoch 16/80: current_loss=11.95756 | best_loss=11.95756
Epoch 17/80: current_loss=11.85976 | best_loss=11.85976
Epoch 18/80: current_loss=11.78905 | best_loss=11.78905
Epoch 19/80: current_loss=11.68139 | best_loss=11.68139
Epoch 20/80: current_loss=11.59570 | best_loss=11.59570
Epoch 21/80: current_loss=11.54215 | best_loss=11.54215
Epoch 22/80: current_loss=11.45784 | best_loss=11.45784
Epoch 23/80: current_loss=11.33439 | best_loss=11.33439
Epoch 24/80: current_loss=11.25813 | best_loss=11.25813
Epoch 25/80: current_loss=11.22417 | best_loss=11.22417
Epoch 26/80: current_loss=11.13226 | best_loss=11.13226
Epoch 27/80: current_loss=11.05442 | best_loss=11.05442
Epoch 28/80: current_loss=10.99484 | best_loss=10.99484
Epoch 29/80: current_loss=10.97799 | best_loss=10.97799
Epoch 30/80: current_loss=10.90819 | best_loss=10.90819
Epoch 31/80: current_loss=10.86614 | best_loss=10.86614
Epoch 32/80: current_loss=10.82392 | best_loss=10.82392
Epoch 33/80: current_loss=10.77675 | best_loss=10.77675
Epoch 34/80: current_loss=10.75470 | best_loss=10.75470
Epoch 35/80: current_loss=10.73907 | best_loss=10.73907
Epoch 36/80: current_loss=10.67828 | best_loss=10.67828
Epoch 37/80: current_loss=10.68269 | best_loss=10.67828
Epoch 38/80: current_loss=10.62359 | best_loss=10.62359
Epoch 39/80: current_loss=10.59808 | best_loss=10.59808
Epoch 40/80: current_loss=10.55879 | best_loss=10.55879
Epoch 41/80: current_loss=10.53462 | best_loss=10.53462
Epoch 42/80: current_loss=10.53798 | best_loss=10.53462
Epoch 43/80: current_loss=10.47819 | best_loss=10.47819
Epoch 44/80: current_loss=10.49424 | best_loss=10.47819
Epoch 45/80: current_loss=10.46846 | best_loss=10.46846
Epoch 46/80: current_loss=10.46427 | best_loss=10.46427
Epoch 47/80: current_loss=10.44379 | best_loss=10.44379
Epoch 48/80: current_loss=10.44373 | best_loss=10.44373
Epoch 49/80: current_loss=10.40484 | best_loss=10.40484
Epoch 50/80: current_loss=10.39918 | best_loss=10.39918
Epoch 51/80: current_loss=10.38629 | best_loss=10.38629
Epoch 52/80: current_loss=10.39232 | best_loss=10.38629
Epoch 53/80: current_loss=10.39718 | best_loss=10.38629
Epoch 54/80: current_loss=10.37984 | best_loss=10.37984
Epoch 55/80: current_loss=10.37354 | best_loss=10.37354
Epoch 56/80: current_loss=10.36447 | best_loss=10.36447
Epoch 57/80: current_loss=10.35126 | best_loss=10.35126
Epoch 58/80: current_loss=10.36783 | best_loss=10.35126
Epoch 59/80: current_loss=10.34588 | best_loss=10.34588
Epoch 60/80: current_loss=10.36110 | best_loss=10.34588
Epoch 61/80: current_loss=10.35665 | best_loss=10.34588
Epoch 62/80: current_loss=10.33553 | best_loss=10.33553
Epoch 63/80: current_loss=10.37675 | best_loss=10.33553
Epoch 64/80: current_loss=10.35082 | best_loss=10.33553
Epoch 65/80: current_loss=10.35199 | best_loss=10.33553
Epoch 66/80: current_loss=10.31506 | best_loss=10.31506
Epoch 67/80: current_loss=10.31650 | best_loss=10.31506
Epoch 68/80: current_loss=10.31635 | best_loss=10.31506
Epoch 69/80: current_loss=10.33398 | best_loss=10.31506
Epoch 70/80: current_loss=10.31612 | best_loss=10.31506
Epoch 71/80: current_loss=10.32815 | best_loss=10.31506
Epoch 72/80: current_loss=10.30309 | best_loss=10.30309
Epoch 73/80: current_loss=10.31467 | best_loss=10.30309
Epoch 74/80: current_loss=10.30969 | best_loss=10.30309
Epoch 75/80: current_loss=10.30040 | best_loss=10.30040
Epoch 76/80: current_loss=10.30564 | best_loss=10.30040
Epoch 77/80: current_loss=10.29908 | best_loss=10.29908
Epoch 78/80: current_loss=10.28604 | best_loss=10.28604
Epoch 79/80: current_loss=10.29720 | best_loss=10.28604
      explained_var=-0.00351 | mse_loss=10.52140
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.42885 | best_loss=10.42885
Epoch 1/80: current_loss=10.42227 | best_loss=10.42227
Epoch 2/80: current_loss=10.41105 | best_loss=10.41105
Epoch 3/80: current_loss=10.42765 | best_loss=10.41105
Epoch 4/80: current_loss=10.41299 | best_loss=10.41105
Epoch 5/80: current_loss=10.43244 | best_loss=10.41105
Epoch 6/80: current_loss=10.41366 | best_loss=10.41105
Epoch 7/80: current_loss=10.43417 | best_loss=10.41105
Epoch 8/80: current_loss=10.42584 | best_loss=10.41105
Epoch 9/80: current_loss=10.45734 | best_loss=10.41105
Epoch 10/80: current_loss=10.42367 | best_loss=10.41105
Epoch 11/80: current_loss=10.41501 | best_loss=10.41105
Epoch 12/80: current_loss=10.41174 | best_loss=10.41105
Epoch 13/80: current_loss=10.41177 | best_loss=10.41105
Epoch 14/80: current_loss=10.41500 | best_loss=10.41105
Epoch 15/80: current_loss=10.45655 | best_loss=10.41105
Epoch 16/80: current_loss=10.43440 | best_loss=10.41105
Epoch 17/80: current_loss=10.41682 | best_loss=10.41105
Epoch 18/80: current_loss=10.41570 | best_loss=10.41105
Epoch 19/80: current_loss=10.42022 | best_loss=10.41105
Epoch 20/80: current_loss=10.41785 | best_loss=10.41105
Epoch 21/80: current_loss=10.42131 | best_loss=10.41105
Epoch 22/80: current_loss=10.41271 | best_loss=10.41105
Early Stopping at epoch 22
      explained_var=0.00024 | mse_loss=10.06247
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.55236 | best_loss=7.55236
Epoch 1/80: current_loss=7.56734 | best_loss=7.55236
Epoch 2/80: current_loss=7.60423 | best_loss=7.55236
Epoch 3/80: current_loss=7.59618 | best_loss=7.55236
Epoch 4/80: current_loss=7.54510 | best_loss=7.54510
Epoch 5/80: current_loss=7.55461 | best_loss=7.54510
Epoch 6/80: current_loss=7.57056 | best_loss=7.54510
Epoch 7/80: current_loss=7.55501 | best_loss=7.54510
Epoch 8/80: current_loss=7.53923 | best_loss=7.53923
Epoch 9/80: current_loss=7.57705 | best_loss=7.53923
Epoch 10/80: current_loss=7.55545 | best_loss=7.53923
Epoch 11/80: current_loss=7.56131 | best_loss=7.53923
Epoch 12/80: current_loss=7.60315 | best_loss=7.53923
Epoch 13/80: current_loss=7.57650 | best_loss=7.53923
Epoch 14/80: current_loss=7.57918 | best_loss=7.53923
Epoch 15/80: current_loss=7.58584 | best_loss=7.53923
Epoch 16/80: current_loss=7.60064 | best_loss=7.53923
Epoch 17/80: current_loss=7.55744 | best_loss=7.53923
Epoch 18/80: current_loss=7.57691 | best_loss=7.53923
Epoch 19/80: current_loss=7.64451 | best_loss=7.53923
Epoch 20/80: current_loss=7.57946 | best_loss=7.53923
Epoch 21/80: current_loss=7.53253 | best_loss=7.53253
Epoch 22/80: current_loss=7.55929 | best_loss=7.53253
Epoch 23/80: current_loss=7.55577 | best_loss=7.53253
Epoch 24/80: current_loss=7.56353 | best_loss=7.53253
Epoch 25/80: current_loss=7.52841 | best_loss=7.52841
Epoch 26/80: current_loss=7.58065 | best_loss=7.52841
Epoch 27/80: current_loss=7.57674 | best_loss=7.52841
Epoch 28/80: current_loss=7.54644 | best_loss=7.52841
Epoch 29/80: current_loss=7.52031 | best_loss=7.52031
Epoch 30/80: current_loss=7.56731 | best_loss=7.52031
Epoch 31/80: current_loss=7.54174 | best_loss=7.52031
Epoch 32/80: current_loss=7.57215 | best_loss=7.52031
Epoch 33/80: current_loss=7.57913 | best_loss=7.52031
Epoch 34/80: current_loss=7.55339 | best_loss=7.52031
Epoch 35/80: current_loss=7.55943 | best_loss=7.52031
Epoch 36/80: current_loss=7.58371 | best_loss=7.52031
Epoch 37/80: current_loss=7.54175 | best_loss=7.52031
Epoch 38/80: current_loss=7.57912 | best_loss=7.52031
Epoch 39/80: current_loss=7.61426 | best_loss=7.52031
Epoch 40/80: current_loss=7.51863 | best_loss=7.51863
Epoch 41/80: current_loss=7.56020 | best_loss=7.51863
Epoch 42/80: current_loss=7.65169 | best_loss=7.51863
Epoch 43/80: current_loss=7.54382 | best_loss=7.51863
Epoch 44/80: current_loss=7.57085 | best_loss=7.51863
Epoch 45/80: current_loss=7.55065 | best_loss=7.51863
Epoch 46/80: current_loss=7.58748 | best_loss=7.51863
Epoch 47/80: current_loss=7.57072 | best_loss=7.51863
Epoch 48/80: current_loss=7.55990 | best_loss=7.51863
Epoch 49/80: current_loss=7.55135 | best_loss=7.51863
Epoch 50/80: current_loss=7.62876 | best_loss=7.51863
Epoch 51/80: current_loss=7.52852 | best_loss=7.51863
Epoch 52/80: current_loss=7.60135 | best_loss=7.51863
Epoch 53/80: current_loss=7.52445 | best_loss=7.51863
Epoch 54/80: current_loss=7.62107 | best_loss=7.51863
Epoch 55/80: current_loss=7.55441 | best_loss=7.51863
Epoch 56/80: current_loss=7.55620 | best_loss=7.51863
Epoch 57/80: current_loss=7.63032 | best_loss=7.51863
Epoch 58/80: current_loss=7.53954 | best_loss=7.51863
Epoch 59/80: current_loss=7.54510 | best_loss=7.51863
Epoch 60/80: current_loss=7.58539 | best_loss=7.51863
Early Stopping at epoch 60
      explained_var=-0.00038 | mse_loss=7.61350
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.79285 | best_loss=8.79285
Epoch 1/80: current_loss=8.79630 | best_loss=8.79285
Epoch 2/80: current_loss=8.79860 | best_loss=8.79285
Epoch 3/80: current_loss=8.80077 | best_loss=8.79285
Epoch 4/80: current_loss=8.79823 | best_loss=8.79285
Epoch 5/80: current_loss=8.83340 | best_loss=8.79285
Epoch 6/80: current_loss=8.80120 | best_loss=8.79285
Epoch 7/80: current_loss=8.80261 | best_loss=8.79285
Epoch 8/80: current_loss=8.78895 | best_loss=8.78895
Epoch 9/80: current_loss=8.78676 | best_loss=8.78676
Epoch 10/80: current_loss=8.77803 | best_loss=8.77803
Epoch 11/80: current_loss=8.75647 | best_loss=8.75647
Epoch 12/80: current_loss=8.76995 | best_loss=8.75647
Epoch 13/80: current_loss=8.77090 | best_loss=8.75647
Epoch 14/80: current_loss=8.78406 | best_loss=8.75647
Epoch 15/80: current_loss=8.77054 | best_loss=8.75647
Epoch 16/80: current_loss=8.77736 | best_loss=8.75647
Epoch 17/80: current_loss=8.78957 | best_loss=8.75647
Epoch 18/80: current_loss=8.77584 | best_loss=8.75647
Epoch 19/80: current_loss=8.77755 | best_loss=8.75647
Epoch 20/80: current_loss=8.77734 | best_loss=8.75647
Epoch 21/80: current_loss=8.78045 | best_loss=8.75647
Epoch 22/80: current_loss=8.77588 | best_loss=8.75647
Epoch 23/80: current_loss=8.77471 | best_loss=8.75647
Epoch 24/80: current_loss=8.79107 | best_loss=8.75647
Epoch 25/80: current_loss=8.77515 | best_loss=8.75647
Epoch 26/80: current_loss=8.78388 | best_loss=8.75647
Epoch 27/80: current_loss=8.78120 | best_loss=8.75647
Epoch 28/80: current_loss=8.78829 | best_loss=8.75647
Epoch 29/80: current_loss=8.79007 | best_loss=8.75647
Epoch 30/80: current_loss=8.80463 | best_loss=8.75647
Epoch 31/80: current_loss=8.80850 | best_loss=8.75647
Early Stopping at epoch 31
      explained_var=0.00114 | mse_loss=8.70660
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.39744 | best_loss=9.39744
Epoch 1/80: current_loss=9.40160 | best_loss=9.39744
Epoch 2/80: current_loss=9.40028 | best_loss=9.39744
Epoch 3/80: current_loss=9.40811 | best_loss=9.39744
Epoch 4/80: current_loss=9.40935 | best_loss=9.39744
Epoch 5/80: current_loss=9.40554 | best_loss=9.39744
Epoch 6/80: current_loss=9.40691 | best_loss=9.39744
Epoch 7/80: current_loss=9.41334 | best_loss=9.39744
Epoch 8/80: current_loss=9.41052 | best_loss=9.39744
Epoch 9/80: current_loss=9.39539 | best_loss=9.39539
Epoch 10/80: current_loss=9.39377 | best_loss=9.39377
Epoch 11/80: current_loss=9.38863 | best_loss=9.38863
Epoch 12/80: current_loss=9.38943 | best_loss=9.38863
Epoch 13/80: current_loss=9.39086 | best_loss=9.38863
Epoch 14/80: current_loss=9.39239 | best_loss=9.38863
Epoch 15/80: current_loss=9.39461 | best_loss=9.38863
Epoch 16/80: current_loss=9.40544 | best_loss=9.38863
Epoch 17/80: current_loss=9.40969 | best_loss=9.38863
Epoch 18/80: current_loss=9.40301 | best_loss=9.38863
Epoch 19/80: current_loss=9.40954 | best_loss=9.38863
Epoch 20/80: current_loss=9.39761 | best_loss=9.38863
Epoch 21/80: current_loss=9.40002 | best_loss=9.38863
Epoch 22/80: current_loss=9.39795 | best_loss=9.38863
Epoch 23/80: current_loss=9.39819 | best_loss=9.38863
Epoch 24/80: current_loss=9.41223 | best_loss=9.38863
Epoch 25/80: current_loss=9.40996 | best_loss=9.38863
Epoch 26/80: current_loss=9.41391 | best_loss=9.38863
Epoch 27/80: current_loss=9.41287 | best_loss=9.38863
Epoch 28/80: current_loss=9.41898 | best_loss=9.38863
Epoch 29/80: current_loss=9.41592 | best_loss=9.38863
Epoch 30/80: current_loss=9.40513 | best_loss=9.38863
Epoch 31/80: current_loss=9.40328 | best_loss=9.38863
Early Stopping at epoch 31
      explained_var=-0.00181 | mse_loss=9.31019
----------------------------------------------
Average early_stopping_point: 28| avg_exp_var=-0.00087| avg_loss=9.24283
----------------------------------------------


----------------------------------------------
Params for Trial 3
{'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=90.09385 | best_loss=90.09385
Epoch 1/80: current_loss=67.20871 | best_loss=67.20871
Epoch 2/80: current_loss=51.04236 | best_loss=51.04236
Epoch 3/80: current_loss=40.38141 | best_loss=40.38141
Epoch 4/80: current_loss=32.73503 | best_loss=32.73503
Epoch 5/80: current_loss=27.05495 | best_loss=27.05495
Epoch 6/80: current_loss=22.93136 | best_loss=22.93136
Epoch 7/80: current_loss=19.80969 | best_loss=19.80969
Epoch 8/80: current_loss=17.38546 | best_loss=17.38546
Epoch 9/80: current_loss=15.71517 | best_loss=15.71517
Epoch 10/80: current_loss=14.48017 | best_loss=14.48017
Epoch 11/80: current_loss=13.61677 | best_loss=13.61677
Epoch 12/80: current_loss=13.02514 | best_loss=13.02514
Epoch 13/80: current_loss=12.56624 | best_loss=12.56624
Epoch 14/80: current_loss=12.27340 | best_loss=12.27340
Epoch 15/80: current_loss=12.02632 | best_loss=12.02632
Epoch 16/80: current_loss=11.85724 | best_loss=11.85724
Epoch 17/80: current_loss=11.73627 | best_loss=11.73627
Epoch 18/80: current_loss=11.62795 | best_loss=11.62795
Epoch 19/80: current_loss=11.54581 | best_loss=11.54581
Epoch 20/80: current_loss=11.49580 | best_loss=11.49580
Epoch 21/80: current_loss=11.43364 | best_loss=11.43364
Epoch 22/80: current_loss=11.37616 | best_loss=11.37616
Epoch 23/80: current_loss=11.31501 | best_loss=11.31501
Epoch 24/80: current_loss=11.28037 | best_loss=11.28037
Epoch 25/80: current_loss=11.25513 | best_loss=11.25513
Epoch 26/80: current_loss=11.22326 | best_loss=11.22326
Epoch 27/80: current_loss=11.20060 | best_loss=11.20060
Epoch 28/80: current_loss=11.15654 | best_loss=11.15654
Epoch 29/80: current_loss=11.12599 | best_loss=11.12599
Epoch 30/80: current_loss=11.10282 | best_loss=11.10282
Epoch 31/80: current_loss=11.08252 | best_loss=11.08252
Epoch 32/80: current_loss=11.04416 | best_loss=11.04416
Epoch 33/80: current_loss=11.01961 | best_loss=11.01961
Epoch 34/80: current_loss=10.99044 | best_loss=10.99044
Epoch 35/80: current_loss=10.95718 | best_loss=10.95718
Epoch 36/80: current_loss=10.94669 | best_loss=10.94669
Epoch 37/80: current_loss=10.94178 | best_loss=10.94178
Epoch 38/80: current_loss=10.91063 | best_loss=10.91063
Epoch 39/80: current_loss=10.89073 | best_loss=10.89073
Epoch 40/80: current_loss=10.88419 | best_loss=10.88419
Epoch 41/80: current_loss=10.86276 | best_loss=10.86276
Epoch 42/80: current_loss=10.84798 | best_loss=10.84798
Epoch 43/80: current_loss=10.83789 | best_loss=10.83789
Epoch 44/80: current_loss=10.82336 | best_loss=10.82336
Epoch 45/80: current_loss=10.79490 | best_loss=10.79490
Epoch 46/80: current_loss=10.78560 | best_loss=10.78560
Epoch 47/80: current_loss=10.77489 | best_loss=10.77489
Epoch 48/80: current_loss=10.75529 | best_loss=10.75529
Epoch 49/80: current_loss=10.74991 | best_loss=10.74991
Epoch 50/80: current_loss=10.72199 | best_loss=10.72199
Epoch 51/80: current_loss=10.72425 | best_loss=10.72199
Epoch 52/80: current_loss=10.69227 | best_loss=10.69227
Epoch 53/80: current_loss=10.68971 | best_loss=10.68971
Epoch 54/80: current_loss=10.67595 | best_loss=10.67595
Epoch 55/80: current_loss=10.66370 | best_loss=10.66370
Epoch 56/80: current_loss=10.65136 | best_loss=10.65136
Epoch 57/80: current_loss=10.65127 | best_loss=10.65127
Epoch 58/80: current_loss=10.65247 | best_loss=10.65127
Epoch 59/80: current_loss=10.63171 | best_loss=10.63171
Epoch 60/80: current_loss=10.62038 | best_loss=10.62038
Epoch 61/80: current_loss=10.61445 | best_loss=10.61445
Epoch 62/80: current_loss=10.59681 | best_loss=10.59681
Epoch 63/80: current_loss=10.58970 | best_loss=10.58970
Epoch 64/80: current_loss=10.59262 | best_loss=10.58970
Epoch 65/80: current_loss=10.58100 | best_loss=10.58100
Epoch 66/80: current_loss=10.57866 | best_loss=10.57866
Epoch 67/80: current_loss=10.57902 | best_loss=10.57866
Epoch 68/80: current_loss=10.57268 | best_loss=10.57268
Epoch 69/80: current_loss=10.57003 | best_loss=10.57003
Epoch 70/80: current_loss=10.57689 | best_loss=10.57003
Epoch 71/80: current_loss=10.56954 | best_loss=10.56954
Epoch 72/80: current_loss=10.56490 | best_loss=10.56490
Epoch 73/80: current_loss=10.54144 | best_loss=10.54144
Epoch 74/80: current_loss=10.55173 | best_loss=10.54144
Epoch 75/80: current_loss=10.54620 | best_loss=10.54144
Epoch 76/80: current_loss=10.53856 | best_loss=10.53856
Epoch 77/80: current_loss=10.49935 | best_loss=10.49935
Epoch 78/80: current_loss=10.51143 | best_loss=10.49935
Epoch 79/80: current_loss=10.51045 | best_loss=10.49935
      explained_var=-0.02254 | mse_loss=10.77067
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.54293 | best_loss=10.54293
Epoch 1/80: current_loss=10.55652 | best_loss=10.54293
Epoch 2/80: current_loss=10.50466 | best_loss=10.50466
Epoch 3/80: current_loss=10.46536 | best_loss=10.46536
Epoch 4/80: current_loss=10.47008 | best_loss=10.46536
Epoch 5/80: current_loss=10.45358 | best_loss=10.45358
Epoch 6/80: current_loss=10.41544 | best_loss=10.41544
Epoch 7/80: current_loss=10.40028 | best_loss=10.40028
Epoch 8/80: current_loss=10.41669 | best_loss=10.40028
Epoch 9/80: current_loss=10.39248 | best_loss=10.39248
Epoch 10/80: current_loss=10.37968 | best_loss=10.37968
Epoch 11/80: current_loss=10.35316 | best_loss=10.35316
Epoch 12/80: current_loss=10.31682 | best_loss=10.31682
Epoch 13/80: current_loss=10.32380 | best_loss=10.31682
Epoch 14/80: current_loss=10.27124 | best_loss=10.27124
Epoch 15/80: current_loss=10.27530 | best_loss=10.27124
Epoch 16/80: current_loss=10.24438 | best_loss=10.24438
Epoch 17/80: current_loss=10.22779 | best_loss=10.22779
Epoch 18/80: current_loss=10.23096 | best_loss=10.22779
Epoch 19/80: current_loss=10.22278 | best_loss=10.22278
Epoch 20/80: current_loss=10.21147 | best_loss=10.21147
Epoch 21/80: current_loss=10.18157 | best_loss=10.18157
Epoch 22/80: current_loss=10.23686 | best_loss=10.18157
Epoch 23/80: current_loss=10.24613 | best_loss=10.18157
Epoch 24/80: current_loss=10.20969 | best_loss=10.18157
Epoch 25/80: current_loss=10.17296 | best_loss=10.17296
Epoch 26/80: current_loss=10.17267 | best_loss=10.17267
Epoch 27/80: current_loss=10.18443 | best_loss=10.17267
Epoch 28/80: current_loss=10.18140 | best_loss=10.17267
Epoch 29/80: current_loss=10.20327 | best_loss=10.17267
Epoch 30/80: current_loss=10.17196 | best_loss=10.17196
Epoch 31/80: current_loss=10.24445 | best_loss=10.17196
Epoch 32/80: current_loss=10.16646 | best_loss=10.16646
Epoch 33/80: current_loss=10.16395 | best_loss=10.16395
Epoch 34/80: current_loss=10.16653 | best_loss=10.16395
Epoch 35/80: current_loss=10.15516 | best_loss=10.15516
Epoch 36/80: current_loss=10.14135 | best_loss=10.14135
Epoch 37/80: current_loss=10.17230 | best_loss=10.14135
Epoch 38/80: current_loss=10.11865 | best_loss=10.11865
Epoch 39/80: current_loss=10.12757 | best_loss=10.11865
Epoch 40/80: current_loss=10.11668 | best_loss=10.11668
Epoch 41/80: current_loss=10.12516 | best_loss=10.11668
Epoch 42/80: current_loss=10.17079 | best_loss=10.11668
Epoch 43/80: current_loss=10.14322 | best_loss=10.11668
Epoch 44/80: current_loss=10.14736 | best_loss=10.11668
Epoch 45/80: current_loss=10.13262 | best_loss=10.11668
Epoch 46/80: current_loss=10.12433 | best_loss=10.11668
Epoch 47/80: current_loss=10.18729 | best_loss=10.11668
Epoch 48/80: current_loss=10.13008 | best_loss=10.11668
Epoch 49/80: current_loss=10.16998 | best_loss=10.11668
Epoch 50/80: current_loss=10.13815 | best_loss=10.11668
Epoch 51/80: current_loss=10.14132 | best_loss=10.11668
Epoch 52/80: current_loss=10.16192 | best_loss=10.11668
Epoch 53/80: current_loss=10.15648 | best_loss=10.11668
Epoch 54/80: current_loss=10.11470 | best_loss=10.11470
Epoch 55/80: current_loss=10.11973 | best_loss=10.11470
Epoch 56/80: current_loss=10.12564 | best_loss=10.11470
Epoch 57/80: current_loss=10.16798 | best_loss=10.11470
Epoch 58/80: current_loss=10.15125 | best_loss=10.11470
Epoch 59/80: current_loss=10.13596 | best_loss=10.11470
Epoch 60/80: current_loss=10.12598 | best_loss=10.11470
Epoch 61/80: current_loss=10.11100 | best_loss=10.11100
Epoch 62/80: current_loss=10.13499 | best_loss=10.11100
Epoch 63/80: current_loss=10.11687 | best_loss=10.11100
Epoch 64/80: current_loss=10.15273 | best_loss=10.11100
Epoch 65/80: current_loss=10.12876 | best_loss=10.11100
Epoch 66/80: current_loss=10.14005 | best_loss=10.11100
Epoch 67/80: current_loss=10.13292 | best_loss=10.11100
Epoch 68/80: current_loss=10.15302 | best_loss=10.11100
Epoch 69/80: current_loss=10.16412 | best_loss=10.11100
Epoch 70/80: current_loss=10.17128 | best_loss=10.11100
Epoch 71/80: current_loss=10.14576 | best_loss=10.11100
Epoch 72/80: current_loss=10.12950 | best_loss=10.11100
Epoch 73/80: current_loss=10.14542 | best_loss=10.11100
Epoch 74/80: current_loss=10.19789 | best_loss=10.11100
Epoch 75/80: current_loss=10.14076 | best_loss=10.11100
Epoch 76/80: current_loss=10.17483 | best_loss=10.11100
Epoch 77/80: current_loss=10.18164 | best_loss=10.11100
Epoch 78/80: current_loss=10.13530 | best_loss=10.11100
Epoch 79/80: current_loss=10.18020 | best_loss=10.11100
      explained_var=0.03200 | mse_loss=9.74674
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.47903 | best_loss=7.47903
Epoch 1/80: current_loss=7.51139 | best_loss=7.47903
Epoch 2/80: current_loss=7.50146 | best_loss=7.47903
Epoch 3/80: current_loss=7.54100 | best_loss=7.47903
Epoch 4/80: current_loss=7.52316 | best_loss=7.47903
Epoch 5/80: current_loss=7.54328 | best_loss=7.47903
Epoch 6/80: current_loss=7.52790 | best_loss=7.47903
Epoch 7/80: current_loss=7.59334 | best_loss=7.47903
Epoch 8/80: current_loss=7.52008 | best_loss=7.47903
Epoch 9/80: current_loss=7.56575 | best_loss=7.47903
Epoch 10/80: current_loss=7.53239 | best_loss=7.47903
Epoch 11/80: current_loss=7.56720 | best_loss=7.47903
Epoch 12/80: current_loss=7.54451 | best_loss=7.47903
Epoch 13/80: current_loss=7.53407 | best_loss=7.47903
Epoch 14/80: current_loss=7.61957 | best_loss=7.47903
Epoch 15/80: current_loss=7.53507 | best_loss=7.47903
Epoch 16/80: current_loss=7.55548 | best_loss=7.47903
Epoch 17/80: current_loss=7.53681 | best_loss=7.47903
Epoch 18/80: current_loss=7.54158 | best_loss=7.47903
Epoch 19/80: current_loss=7.56801 | best_loss=7.47903
Epoch 20/80: current_loss=7.55857 | best_loss=7.47903
Early Stopping at epoch 20
      explained_var=-0.00123 | mse_loss=7.59293
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.92322 | best_loss=8.92322
Epoch 1/80: current_loss=8.85987 | best_loss=8.85987
Epoch 2/80: current_loss=8.87469 | best_loss=8.85987
Epoch 3/80: current_loss=8.93035 | best_loss=8.85987
Epoch 4/80: current_loss=8.87310 | best_loss=8.85987
Epoch 5/80: current_loss=8.87119 | best_loss=8.85987
Epoch 6/80: current_loss=8.87500 | best_loss=8.85987
Epoch 7/80: current_loss=8.90655 | best_loss=8.85987
Epoch 8/80: current_loss=8.89775 | best_loss=8.85987
Epoch 9/80: current_loss=8.90903 | best_loss=8.85987
Epoch 10/80: current_loss=8.97727 | best_loss=8.85987
Epoch 11/80: current_loss=8.89459 | best_loss=8.85987
Epoch 12/80: current_loss=8.89955 | best_loss=8.85987
Epoch 13/80: current_loss=8.90579 | best_loss=8.85987
Epoch 14/80: current_loss=8.89045 | best_loss=8.85987
Epoch 15/80: current_loss=8.88120 | best_loss=8.85987
Epoch 16/80: current_loss=8.87350 | best_loss=8.85987
Epoch 17/80: current_loss=8.87003 | best_loss=8.85987
Epoch 18/80: current_loss=8.85542 | best_loss=8.85542
Epoch 19/80: current_loss=8.95771 | best_loss=8.85542
Epoch 20/80: current_loss=8.91146 | best_loss=8.85542
Epoch 21/80: current_loss=8.86472 | best_loss=8.85542
Epoch 22/80: current_loss=8.86359 | best_loss=8.85542
Epoch 23/80: current_loss=8.93762 | best_loss=8.85542
Epoch 24/80: current_loss=8.97965 | best_loss=8.85542
Epoch 25/80: current_loss=8.84740 | best_loss=8.84740
Epoch 26/80: current_loss=8.89733 | best_loss=8.84740
Epoch 27/80: current_loss=8.86501 | best_loss=8.84740
Epoch 28/80: current_loss=8.91047 | best_loss=8.84740
Epoch 29/80: current_loss=8.87010 | best_loss=8.84740
Epoch 30/80: current_loss=8.88803 | best_loss=8.84740
Epoch 31/80: current_loss=8.95476 | best_loss=8.84740
Epoch 32/80: current_loss=8.88179 | best_loss=8.84740
Epoch 33/80: current_loss=8.84748 | best_loss=8.84740
Epoch 34/80: current_loss=8.83526 | best_loss=8.83526
Epoch 35/80: current_loss=8.85228 | best_loss=8.83526
Epoch 36/80: current_loss=8.90717 | best_loss=8.83526
Epoch 37/80: current_loss=8.88684 | best_loss=8.83526
Epoch 38/80: current_loss=8.88771 | best_loss=8.83526
Epoch 39/80: current_loss=8.90137 | best_loss=8.83526
Epoch 40/80: current_loss=8.94103 | best_loss=8.83526
Epoch 41/80: current_loss=8.88229 | best_loss=8.83526
Epoch 42/80: current_loss=8.85300 | best_loss=8.83526
Epoch 43/80: current_loss=8.87387 | best_loss=8.83526
Epoch 44/80: current_loss=8.87403 | best_loss=8.83526
Epoch 45/80: current_loss=8.84505 | best_loss=8.83526
Epoch 46/80: current_loss=8.86983 | best_loss=8.83526
Epoch 47/80: current_loss=8.91724 | best_loss=8.83526
Epoch 48/80: current_loss=8.90875 | best_loss=8.83526
Epoch 49/80: current_loss=9.00190 | best_loss=8.83526
Epoch 50/80: current_loss=8.90847 | best_loss=8.83526
Epoch 51/80: current_loss=8.95509 | best_loss=8.83526
Epoch 52/80: current_loss=8.87674 | best_loss=8.83526
Epoch 53/80: current_loss=8.85850 | best_loss=8.83526
Epoch 54/80: current_loss=8.90890 | best_loss=8.83526
Early Stopping at epoch 54
      explained_var=-0.00837 | mse_loss=8.79509
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.16008 | best_loss=9.16008
Epoch 1/80: current_loss=9.16098 | best_loss=9.16008
Epoch 2/80: current_loss=9.16237 | best_loss=9.16008
Epoch 3/80: current_loss=9.16172 | best_loss=9.16008
Epoch 4/80: current_loss=9.16778 | best_loss=9.16008
Epoch 5/80: current_loss=9.20151 | best_loss=9.16008
Epoch 6/80: current_loss=9.18835 | best_loss=9.16008
Epoch 7/80: current_loss=9.17048 | best_loss=9.16008
Epoch 8/80: current_loss=9.16723 | best_loss=9.16008
Epoch 9/80: current_loss=9.16183 | best_loss=9.16008
Epoch 10/80: current_loss=9.16194 | best_loss=9.16008
Epoch 11/80: current_loss=9.16428 | best_loss=9.16008
Epoch 12/80: current_loss=9.17179 | best_loss=9.16008
Epoch 13/80: current_loss=9.17332 | best_loss=9.16008
Epoch 14/80: current_loss=9.16835 | best_loss=9.16008
Epoch 15/80: current_loss=9.16835 | best_loss=9.16008
Epoch 16/80: current_loss=9.16232 | best_loss=9.16008
Epoch 17/80: current_loss=9.17043 | best_loss=9.16008
Epoch 18/80: current_loss=9.17488 | best_loss=9.16008
Epoch 19/80: current_loss=9.17128 | best_loss=9.16008
Epoch 20/80: current_loss=9.17219 | best_loss=9.16008
Early Stopping at epoch 20
      explained_var=0.01899 | mse_loss=9.12572
----------------------------------------------
Average early_stopping_point: 38| avg_exp_var=0.00377| avg_loss=9.20623
----------------------------------------------


----------------------------------------------
Params for Trial 4
{'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=75.94319 | best_loss=75.94319
Epoch 1/80: current_loss=47.18833 | best_loss=47.18833
Epoch 2/80: current_loss=36.35163 | best_loss=36.35163
Epoch 3/80: current_loss=29.06657 | best_loss=29.06657
Epoch 4/80: current_loss=23.78660 | best_loss=23.78660
Epoch 5/80: current_loss=19.89859 | best_loss=19.89859
Epoch 6/80: current_loss=17.04311 | best_loss=17.04311
Epoch 7/80: current_loss=14.99877 | best_loss=14.99877
Epoch 8/80: current_loss=13.53823 | best_loss=13.53823
Epoch 9/80: current_loss=12.52632 | best_loss=12.52632
Epoch 10/80: current_loss=11.84901 | best_loss=11.84901
Epoch 11/80: current_loss=11.36668 | best_loss=11.36668
Epoch 12/80: current_loss=11.07770 | best_loss=11.07770
Epoch 13/80: current_loss=10.84801 | best_loss=10.84801
Epoch 14/80: current_loss=10.70030 | best_loss=10.70030
Epoch 15/80: current_loss=10.61821 | best_loss=10.61821
Epoch 16/80: current_loss=10.54241 | best_loss=10.54241
Epoch 17/80: current_loss=10.50474 | best_loss=10.50474
Epoch 18/80: current_loss=10.46511 | best_loss=10.46511
Epoch 19/80: current_loss=10.42870 | best_loss=10.42870
Epoch 20/80: current_loss=10.41199 | best_loss=10.41199
Epoch 21/80: current_loss=10.41401 | best_loss=10.41199
Epoch 22/80: current_loss=10.40967 | best_loss=10.40967
Epoch 23/80: current_loss=10.40207 | best_loss=10.40207
Epoch 24/80: current_loss=10.37725 | best_loss=10.37725
Epoch 25/80: current_loss=10.36936 | best_loss=10.36936
Epoch 26/80: current_loss=10.37130 | best_loss=10.36936
Epoch 27/80: current_loss=10.35557 | best_loss=10.35557
Epoch 28/80: current_loss=10.34272 | best_loss=10.34272
Epoch 29/80: current_loss=10.34745 | best_loss=10.34272
Epoch 30/80: current_loss=10.35209 | best_loss=10.34272
Epoch 31/80: current_loss=10.34367 | best_loss=10.34272
Epoch 32/80: current_loss=10.33747 | best_loss=10.33747
Epoch 33/80: current_loss=10.33324 | best_loss=10.33324
Epoch 34/80: current_loss=10.34544 | best_loss=10.33324
Epoch 35/80: current_loss=10.34782 | best_loss=10.33324
Epoch 36/80: current_loss=10.33031 | best_loss=10.33031
Epoch 37/80: current_loss=10.33496 | best_loss=10.33031
Epoch 38/80: current_loss=10.33900 | best_loss=10.33031
Epoch 39/80: current_loss=10.32211 | best_loss=10.32211
Epoch 40/80: current_loss=10.32461 | best_loss=10.32211
Epoch 41/80: current_loss=10.33533 | best_loss=10.32211
Epoch 42/80: current_loss=10.33733 | best_loss=10.32211
Epoch 43/80: current_loss=10.33339 | best_loss=10.32211
Epoch 44/80: current_loss=10.33348 | best_loss=10.32211
Epoch 45/80: current_loss=10.31695 | best_loss=10.31695
Epoch 46/80: current_loss=10.31724 | best_loss=10.31695
Epoch 47/80: current_loss=10.32431 | best_loss=10.31695
Epoch 48/80: current_loss=10.31770 | best_loss=10.31695
Epoch 49/80: current_loss=10.32791 | best_loss=10.31695
Epoch 50/80: current_loss=10.31514 | best_loss=10.31514
Epoch 51/80: current_loss=10.32781 | best_loss=10.31514
Epoch 52/80: current_loss=10.32497 | best_loss=10.31514
Epoch 53/80: current_loss=10.32998 | best_loss=10.31514
Epoch 54/80: current_loss=10.33784 | best_loss=10.31514
Epoch 55/80: current_loss=10.34110 | best_loss=10.31514
Epoch 56/80: current_loss=10.32911 | best_loss=10.31514
Epoch 57/80: current_loss=10.32855 | best_loss=10.31514
Epoch 58/80: current_loss=10.31651 | best_loss=10.31514
Epoch 59/80: current_loss=10.31109 | best_loss=10.31109
Epoch 60/80: current_loss=10.33139 | best_loss=10.31109
Epoch 61/80: current_loss=10.31794 | best_loss=10.31109
Epoch 62/80: current_loss=10.30386 | best_loss=10.30386
Epoch 63/80: current_loss=10.31927 | best_loss=10.30386
Epoch 64/80: current_loss=10.30575 | best_loss=10.30386
Epoch 65/80: current_loss=10.30910 | best_loss=10.30386
Epoch 66/80: current_loss=10.31180 | best_loss=10.30386
Epoch 67/80: current_loss=10.31020 | best_loss=10.30386
Epoch 68/80: current_loss=10.31618 | best_loss=10.30386
Epoch 69/80: current_loss=10.32248 | best_loss=10.30386
Epoch 70/80: current_loss=10.31692 | best_loss=10.30386
Epoch 71/80: current_loss=10.30059 | best_loss=10.30059
Epoch 72/80: current_loss=10.30819 | best_loss=10.30059
Epoch 73/80: current_loss=10.31133 | best_loss=10.30059
Epoch 74/80: current_loss=10.32603 | best_loss=10.30059
Epoch 75/80: current_loss=10.30127 | best_loss=10.30059
Epoch 76/80: current_loss=10.28776 | best_loss=10.28776
Epoch 77/80: current_loss=10.29338 | best_loss=10.28776
Epoch 78/80: current_loss=10.29689 | best_loss=10.28776
Epoch 79/80: current_loss=10.30595 | best_loss=10.28776
      explained_var=-0.00370 | mse_loss=10.52174
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.42926 | best_loss=10.42926
Epoch 1/80: current_loss=10.42704 | best_loss=10.42704
Epoch 2/80: current_loss=10.42001 | best_loss=10.42001
Epoch 3/80: current_loss=10.42547 | best_loss=10.42001
Epoch 4/80: current_loss=10.41131 | best_loss=10.41131
Epoch 5/80: current_loss=10.39922 | best_loss=10.39922
Epoch 6/80: current_loss=10.39450 | best_loss=10.39450
Epoch 7/80: current_loss=10.36614 | best_loss=10.36614
Epoch 8/80: current_loss=10.34343 | best_loss=10.34343
Epoch 9/80: current_loss=10.33282 | best_loss=10.33282
Epoch 10/80: current_loss=10.34539 | best_loss=10.33282
Epoch 11/80: current_loss=10.35420 | best_loss=10.33282
Epoch 12/80: current_loss=10.29738 | best_loss=10.29738
Epoch 13/80: current_loss=10.28102 | best_loss=10.28102
Epoch 14/80: current_loss=10.23398 | best_loss=10.23398
Epoch 15/80: current_loss=10.20978 | best_loss=10.20978
Epoch 16/80: current_loss=10.25852 | best_loss=10.20978
Epoch 17/80: current_loss=10.24219 | best_loss=10.20978
Epoch 18/80: current_loss=10.21872 | best_loss=10.20978
Epoch 19/80: current_loss=10.20333 | best_loss=10.20333
Epoch 20/80: current_loss=10.18693 | best_loss=10.18693
Epoch 21/80: current_loss=10.19957 | best_loss=10.18693
Epoch 22/80: current_loss=10.14637 | best_loss=10.14637
Epoch 23/80: current_loss=10.17458 | best_loss=10.14637
Epoch 24/80: current_loss=10.20342 | best_loss=10.14637
Epoch 25/80: current_loss=10.22137 | best_loss=10.14637
Epoch 26/80: current_loss=10.20188 | best_loss=10.14637
Epoch 27/80: current_loss=10.21357 | best_loss=10.14637
Epoch 28/80: current_loss=10.22652 | best_loss=10.14637
Epoch 29/80: current_loss=10.21457 | best_loss=10.14637
Epoch 30/80: current_loss=10.20301 | best_loss=10.14637
Epoch 31/80: current_loss=10.21148 | best_loss=10.14637
Epoch 32/80: current_loss=10.19582 | best_loss=10.14637
Epoch 33/80: current_loss=10.20238 | best_loss=10.14637
Epoch 34/80: current_loss=10.20142 | best_loss=10.14637
Epoch 35/80: current_loss=10.18510 | best_loss=10.14637
Epoch 36/80: current_loss=10.18614 | best_loss=10.14637
Epoch 37/80: current_loss=10.19048 | best_loss=10.14637
Epoch 38/80: current_loss=10.19331 | best_loss=10.14637
Epoch 39/80: current_loss=10.18539 | best_loss=10.14637
Epoch 40/80: current_loss=10.17575 | best_loss=10.14637
Epoch 41/80: current_loss=10.18484 | best_loss=10.14637
Epoch 42/80: current_loss=10.17916 | best_loss=10.14637
Early Stopping at epoch 42
      explained_var=0.03082 | mse_loss=9.78204
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.49671 | best_loss=7.49671
Epoch 1/80: current_loss=7.54367 | best_loss=7.49671
Epoch 2/80: current_loss=7.53244 | best_loss=7.49671
Epoch 3/80: current_loss=7.56403 | best_loss=7.49671
Epoch 4/80: current_loss=7.56336 | best_loss=7.49671
Epoch 5/80: current_loss=7.61672 | best_loss=7.49671
Epoch 6/80: current_loss=7.60271 | best_loss=7.49671
Epoch 7/80: current_loss=7.57250 | best_loss=7.49671
Epoch 8/80: current_loss=7.64790 | best_loss=7.49671
Epoch 9/80: current_loss=7.55964 | best_loss=7.49671
Epoch 10/80: current_loss=7.59269 | best_loss=7.49671
Epoch 11/80: current_loss=7.60865 | best_loss=7.49671
Epoch 12/80: current_loss=7.62894 | best_loss=7.49671
Epoch 13/80: current_loss=7.57461 | best_loss=7.49671
Epoch 14/80: current_loss=7.54676 | best_loss=7.49671
Epoch 15/80: current_loss=7.57600 | best_loss=7.49671
Epoch 16/80: current_loss=7.57876 | best_loss=7.49671
Epoch 17/80: current_loss=7.56535 | best_loss=7.49671
Epoch 18/80: current_loss=7.57050 | best_loss=7.49671
Epoch 19/80: current_loss=7.57938 | best_loss=7.49671
Epoch 20/80: current_loss=7.59745 | best_loss=7.49671
Early Stopping at epoch 20
      explained_var=0.00029 | mse_loss=7.60039
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.93098 | best_loss=8.93098
Epoch 1/80: current_loss=8.80887 | best_loss=8.80887
Epoch 2/80: current_loss=8.88106 | best_loss=8.80887
Epoch 3/80: current_loss=8.90018 | best_loss=8.80887
Epoch 4/80: current_loss=8.93630 | best_loss=8.80887
Epoch 5/80: current_loss=8.81929 | best_loss=8.80887
Epoch 6/80: current_loss=8.91833 | best_loss=8.80887
Epoch 7/80: current_loss=8.85690 | best_loss=8.80887
Epoch 8/80: current_loss=8.81135 | best_loss=8.80887
Epoch 9/80: current_loss=8.84822 | best_loss=8.80887
Epoch 10/80: current_loss=8.91036 | best_loss=8.80887
Epoch 11/80: current_loss=8.80076 | best_loss=8.80076
Epoch 12/80: current_loss=8.88386 | best_loss=8.80076
Epoch 13/80: current_loss=8.84614 | best_loss=8.80076
Epoch 14/80: current_loss=8.90036 | best_loss=8.80076
Epoch 15/80: current_loss=8.82913 | best_loss=8.80076
Epoch 16/80: current_loss=8.87668 | best_loss=8.80076
Epoch 17/80: current_loss=8.83914 | best_loss=8.80076
Epoch 18/80: current_loss=8.83975 | best_loss=8.80076
Epoch 19/80: current_loss=8.82488 | best_loss=8.80076
Epoch 20/80: current_loss=8.95106 | best_loss=8.80076
Epoch 21/80: current_loss=8.83840 | best_loss=8.80076
Epoch 22/80: current_loss=8.93624 | best_loss=8.80076
Epoch 23/80: current_loss=8.84081 | best_loss=8.80076
Epoch 24/80: current_loss=8.84126 | best_loss=8.80076
Epoch 25/80: current_loss=8.98111 | best_loss=8.80076
Epoch 26/80: current_loss=8.85017 | best_loss=8.80076
Epoch 27/80: current_loss=8.88446 | best_loss=8.80076
Epoch 28/80: current_loss=8.80348 | best_loss=8.80076
Epoch 29/80: current_loss=8.84945 | best_loss=8.80076
Epoch 30/80: current_loss=8.87730 | best_loss=8.80076
Epoch 31/80: current_loss=8.85425 | best_loss=8.80076
Early Stopping at epoch 31
      explained_var=-0.00546 | mse_loss=8.76401
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.15544 | best_loss=9.15544
Epoch 1/80: current_loss=9.18564 | best_loss=9.15544
Epoch 2/80: current_loss=9.18553 | best_loss=9.15544
Epoch 3/80: current_loss=9.21490 | best_loss=9.15544
Epoch 4/80: current_loss=9.19589 | best_loss=9.15544
Epoch 5/80: current_loss=9.25568 | best_loss=9.15544
Epoch 6/80: current_loss=9.20145 | best_loss=9.15544
Epoch 7/80: current_loss=9.18781 | best_loss=9.15544
Epoch 8/80: current_loss=9.20172 | best_loss=9.15544
Epoch 9/80: current_loss=9.18494 | best_loss=9.15544
Epoch 10/80: current_loss=9.17078 | best_loss=9.15544
Epoch 11/80: current_loss=9.16489 | best_loss=9.15544
Epoch 12/80: current_loss=9.18341 | best_loss=9.15544
Epoch 13/80: current_loss=9.17701 | best_loss=9.15544
Epoch 14/80: current_loss=9.17462 | best_loss=9.15544
Epoch 15/80: current_loss=9.17849 | best_loss=9.15544
Epoch 16/80: current_loss=9.17561 | best_loss=9.15544
Epoch 17/80: current_loss=9.20098 | best_loss=9.15544
Epoch 18/80: current_loss=9.16830 | best_loss=9.15544
Epoch 19/80: current_loss=9.18141 | best_loss=9.15544
Epoch 20/80: current_loss=9.17091 | best_loss=9.15544
Early Stopping at epoch 20
      explained_var=0.01942 | mse_loss=9.11373
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=0.00827| avg_loss=9.15638
----------------------------------------------


----------------------------------------------
Params for Trial 5
{'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=26.67918 | best_loss=26.67918
Epoch 1/80: current_loss=14.73236 | best_loss=14.73236
Epoch 2/80: current_loss=13.78872 | best_loss=13.78872
Epoch 3/80: current_loss=13.50817 | best_loss=13.50817
Epoch 4/80: current_loss=13.26741 | best_loss=13.26741
Epoch 5/80: current_loss=13.10953 | best_loss=13.10953
Epoch 6/80: current_loss=12.92104 | best_loss=12.92104
Epoch 7/80: current_loss=12.79189 | best_loss=12.79189
Epoch 8/80: current_loss=12.66784 | best_loss=12.66784
Epoch 9/80: current_loss=12.55562 | best_loss=12.55562
Epoch 10/80: current_loss=12.43289 | best_loss=12.43289
Epoch 11/80: current_loss=12.35144 | best_loss=12.35144
Epoch 12/80: current_loss=12.28954 | best_loss=12.28954
Epoch 13/80: current_loss=12.17969 | best_loss=12.17969
Epoch 14/80: current_loss=12.13658 | best_loss=12.13658
Epoch 15/80: current_loss=12.03471 | best_loss=12.03471
Epoch 16/80: current_loss=11.98687 | best_loss=11.98687
Epoch 17/80: current_loss=11.89661 | best_loss=11.89661
Epoch 18/80: current_loss=11.84415 | best_loss=11.84415
Epoch 19/80: current_loss=11.79321 | best_loss=11.79321
Epoch 20/80: current_loss=11.72266 | best_loss=11.72266
Epoch 21/80: current_loss=11.66792 | best_loss=11.66792
Epoch 22/80: current_loss=11.61965 | best_loss=11.61965
Epoch 23/80: current_loss=11.58016 | best_loss=11.58016
Epoch 24/80: current_loss=11.52712 | best_loss=11.52712
Epoch 25/80: current_loss=11.48664 | best_loss=11.48664
Epoch 26/80: current_loss=11.41388 | best_loss=11.41388
Epoch 27/80: current_loss=11.36910 | best_loss=11.36910
Epoch 28/80: current_loss=11.35713 | best_loss=11.35713
Epoch 29/80: current_loss=11.27119 | best_loss=11.27119
Epoch 30/80: current_loss=11.24743 | best_loss=11.24743
Epoch 31/80: current_loss=11.21401 | best_loss=11.21401
Epoch 32/80: current_loss=11.16793 | best_loss=11.16793
Epoch 33/80: current_loss=11.13822 | best_loss=11.13822
Epoch 34/80: current_loss=11.10571 | best_loss=11.10571
Epoch 35/80: current_loss=11.05318 | best_loss=11.05318
Epoch 36/80: current_loss=11.03342 | best_loss=11.03342
Epoch 37/80: current_loss=10.97279 | best_loss=10.97279
Epoch 38/80: current_loss=10.97725 | best_loss=10.97279
Epoch 39/80: current_loss=10.91031 | best_loss=10.91031
Epoch 40/80: current_loss=10.88291 | best_loss=10.88291
Epoch 41/80: current_loss=10.85587 | best_loss=10.85587
Epoch 42/80: current_loss=10.90587 | best_loss=10.85587
Epoch 43/80: current_loss=10.79482 | best_loss=10.79482
Epoch 44/80: current_loss=10.80001 | best_loss=10.79482
Epoch 45/80: current_loss=10.75261 | best_loss=10.75261
Epoch 46/80: current_loss=10.72439 | best_loss=10.72439
Epoch 47/80: current_loss=10.69539 | best_loss=10.69539
Epoch 48/80: current_loss=10.72741 | best_loss=10.69539
Epoch 49/80: current_loss=10.64218 | best_loss=10.64218
Epoch 50/80: current_loss=10.70535 | best_loss=10.64218
Epoch 51/80: current_loss=10.59430 | best_loss=10.59430
Epoch 52/80: current_loss=10.60432 | best_loss=10.59430
Epoch 53/80: current_loss=10.60231 | best_loss=10.59430
Epoch 54/80: current_loss=10.58106 | best_loss=10.58106
Epoch 55/80: current_loss=10.61987 | best_loss=10.58106
Epoch 56/80: current_loss=10.49263 | best_loss=10.49263
Epoch 57/80: current_loss=10.50545 | best_loss=10.49263
Epoch 58/80: current_loss=10.44474 | best_loss=10.44474
Epoch 59/80: current_loss=10.44736 | best_loss=10.44474
Epoch 60/80: current_loss=10.40364 | best_loss=10.40364
Epoch 61/80: current_loss=10.41233 | best_loss=10.40364
Epoch 62/80: current_loss=10.34767 | best_loss=10.34767
Epoch 63/80: current_loss=10.47999 | best_loss=10.34767
Epoch 64/80: current_loss=10.53664 | best_loss=10.34767
Epoch 65/80: current_loss=10.52001 | best_loss=10.34767
Epoch 66/80: current_loss=10.64529 | best_loss=10.34767
Epoch 67/80: current_loss=10.34705 | best_loss=10.34705
Epoch 68/80: current_loss=10.33785 | best_loss=10.33785
Epoch 69/80: current_loss=10.38583 | best_loss=10.33785
Epoch 70/80: current_loss=10.29990 | best_loss=10.29990
Epoch 71/80: current_loss=10.28499 | best_loss=10.28499
Epoch 72/80: current_loss=10.22125 | best_loss=10.22125
Epoch 73/80: current_loss=10.28928 | best_loss=10.22125
Epoch 74/80: current_loss=10.24491 | best_loss=10.22125
Epoch 75/80: current_loss=10.32382 | best_loss=10.22125
Epoch 76/80: current_loss=10.20986 | best_loss=10.20986
Epoch 77/80: current_loss=10.18815 | best_loss=10.18815
Epoch 78/80: current_loss=10.19031 | best_loss=10.18815
Epoch 79/80: current_loss=10.25618 | best_loss=10.18815
      explained_var=0.00430 | mse_loss=10.43919
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.31419 | best_loss=10.31419
Epoch 1/80: current_loss=10.29990 | best_loss=10.29990
Epoch 2/80: current_loss=10.40487 | best_loss=10.29990
Epoch 3/80: current_loss=10.30035 | best_loss=10.29990
Epoch 4/80: current_loss=10.36929 | best_loss=10.29990
Epoch 5/80: current_loss=10.30452 | best_loss=10.29990
Epoch 6/80: current_loss=10.31917 | best_loss=10.29990
Epoch 7/80: current_loss=10.59291 | best_loss=10.29990
Epoch 8/80: current_loss=10.26723 | best_loss=10.26723
Epoch 9/80: current_loss=10.36795 | best_loss=10.26723
Epoch 10/80: current_loss=10.24865 | best_loss=10.24865
Epoch 11/80: current_loss=10.60490 | best_loss=10.24865
Epoch 12/80: current_loss=10.25707 | best_loss=10.24865
Epoch 13/80: current_loss=10.27982 | best_loss=10.24865
Epoch 14/80: current_loss=10.34470 | best_loss=10.24865
Epoch 15/80: current_loss=10.22298 | best_loss=10.22298
Epoch 16/80: current_loss=10.22283 | best_loss=10.22283
Epoch 17/80: current_loss=10.21640 | best_loss=10.21640
Epoch 18/80: current_loss=10.34073 | best_loss=10.21640
Epoch 19/80: current_loss=10.28794 | best_loss=10.21640
Epoch 20/80: current_loss=10.22923 | best_loss=10.21640
Epoch 21/80: current_loss=10.21601 | best_loss=10.21601
Epoch 22/80: current_loss=10.22309 | best_loss=10.21601
Epoch 23/80: current_loss=10.20957 | best_loss=10.20957
Epoch 24/80: current_loss=10.20308 | best_loss=10.20308
Epoch 25/80: current_loss=10.53398 | best_loss=10.20308
Epoch 26/80: current_loss=10.26579 | best_loss=10.20308
Epoch 27/80: current_loss=10.30445 | best_loss=10.20308
Epoch 28/80: current_loss=10.23026 | best_loss=10.20308
Epoch 29/80: current_loss=10.22618 | best_loss=10.20308
Epoch 30/80: current_loss=10.38203 | best_loss=10.20308
Epoch 31/80: current_loss=10.22606 | best_loss=10.20308
Epoch 32/80: current_loss=10.19693 | best_loss=10.19693
Epoch 33/80: current_loss=10.24472 | best_loss=10.19693
Epoch 34/80: current_loss=10.26302 | best_loss=10.19693
Epoch 35/80: current_loss=10.36322 | best_loss=10.19693
Epoch 36/80: current_loss=10.20019 | best_loss=10.19693
Epoch 37/80: current_loss=10.21986 | best_loss=10.19693
Epoch 38/80: current_loss=10.26720 | best_loss=10.19693
Epoch 39/80: current_loss=10.32891 | best_loss=10.19693
Epoch 40/80: current_loss=10.24031 | best_loss=10.19693
Epoch 41/80: current_loss=10.27605 | best_loss=10.19693
Epoch 42/80: current_loss=10.25859 | best_loss=10.19693
Epoch 43/80: current_loss=10.19589 | best_loss=10.19589
Epoch 44/80: current_loss=10.21369 | best_loss=10.19589
Epoch 45/80: current_loss=10.25250 | best_loss=10.19589
Epoch 46/80: current_loss=10.21424 | best_loss=10.19589
Epoch 47/80: current_loss=10.20914 | best_loss=10.19589
Epoch 48/80: current_loss=10.25594 | best_loss=10.19589
Epoch 49/80: current_loss=10.18597 | best_loss=10.18597
Epoch 50/80: current_loss=10.21760 | best_loss=10.18597
Epoch 51/80: current_loss=10.46300 | best_loss=10.18597
Epoch 52/80: current_loss=10.25060 | best_loss=10.18597
Epoch 53/80: current_loss=10.21899 | best_loss=10.18597
Epoch 54/80: current_loss=10.24631 | best_loss=10.18597
Epoch 55/80: current_loss=10.24259 | best_loss=10.18597
Epoch 56/80: current_loss=10.30062 | best_loss=10.18597
Epoch 57/80: current_loss=10.21122 | best_loss=10.18597
Epoch 58/80: current_loss=10.22312 | best_loss=10.18597
Epoch 59/80: current_loss=10.24133 | best_loss=10.18597
Epoch 60/80: current_loss=10.19967 | best_loss=10.18597
Epoch 61/80: current_loss=10.49394 | best_loss=10.18597
Epoch 62/80: current_loss=10.23081 | best_loss=10.18597
Epoch 63/80: current_loss=10.25907 | best_loss=10.18597
Epoch 64/80: current_loss=10.22797 | best_loss=10.18597
Epoch 65/80: current_loss=10.33429 | best_loss=10.18597
Epoch 66/80: current_loss=10.20799 | best_loss=10.18597
Epoch 67/80: current_loss=10.67160 | best_loss=10.18597
Epoch 68/80: current_loss=10.20032 | best_loss=10.18597
Epoch 69/80: current_loss=10.21899 | best_loss=10.18597
Early Stopping at epoch 69
      explained_var=0.02423 | mse_loss=9.82330
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.48143 | best_loss=7.48143
Epoch 1/80: current_loss=7.82815 | best_loss=7.48143
Epoch 2/80: current_loss=7.69844 | best_loss=7.48143
Epoch 3/80: current_loss=7.51913 | best_loss=7.48143
Epoch 4/80: current_loss=7.66496 | best_loss=7.48143
Epoch 5/80: current_loss=7.58060 | best_loss=7.48143
Epoch 6/80: current_loss=7.52443 | best_loss=7.48143
Epoch 7/80: current_loss=8.35396 | best_loss=7.48143
Epoch 8/80: current_loss=7.51542 | best_loss=7.48143
Epoch 9/80: current_loss=7.63584 | best_loss=7.48143
Epoch 10/80: current_loss=7.57539 | best_loss=7.48143
Epoch 11/80: current_loss=8.06900 | best_loss=7.48143
Epoch 12/80: current_loss=7.68403 | best_loss=7.48143
Epoch 13/80: current_loss=7.80363 | best_loss=7.48143
Epoch 14/80: current_loss=7.61203 | best_loss=7.48143
Epoch 15/80: current_loss=7.54644 | best_loss=7.48143
Epoch 16/80: current_loss=7.55853 | best_loss=7.48143
Epoch 17/80: current_loss=7.78984 | best_loss=7.48143
Epoch 18/80: current_loss=7.58959 | best_loss=7.48143
Epoch 19/80: current_loss=7.55837 | best_loss=7.48143
Epoch 20/80: current_loss=7.53044 | best_loss=7.48143
Early Stopping at epoch 20
      explained_var=-0.00290 | mse_loss=7.60502
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.81297 | best_loss=8.81297
Epoch 1/80: current_loss=8.80148 | best_loss=8.80148
Epoch 2/80: current_loss=8.79422 | best_loss=8.79422
Epoch 3/80: current_loss=8.84951 | best_loss=8.79422
Epoch 4/80: current_loss=8.77172 | best_loss=8.77172
Epoch 5/80: current_loss=8.81020 | best_loss=8.77172
Epoch 6/80: current_loss=8.83735 | best_loss=8.77172
Epoch 7/80: current_loss=8.90137 | best_loss=8.77172
Epoch 8/80: current_loss=8.79600 | best_loss=8.77172
Epoch 9/80: current_loss=8.80273 | best_loss=8.77172
Epoch 10/80: current_loss=8.91787 | best_loss=8.77172
Epoch 11/80: current_loss=8.83452 | best_loss=8.77172
Epoch 12/80: current_loss=8.82388 | best_loss=8.77172
Epoch 13/80: current_loss=9.15043 | best_loss=8.77172
Epoch 14/80: current_loss=8.81793 | best_loss=8.77172
Epoch 15/80: current_loss=8.80973 | best_loss=8.77172
Epoch 16/80: current_loss=8.83417 | best_loss=8.77172
Epoch 17/80: current_loss=8.86954 | best_loss=8.77172
Epoch 18/80: current_loss=8.87329 | best_loss=8.77172
Epoch 19/80: current_loss=8.83273 | best_loss=8.77172
Epoch 20/80: current_loss=8.87402 | best_loss=8.77172
Epoch 21/80: current_loss=9.12186 | best_loss=8.77172
Epoch 22/80: current_loss=8.82370 | best_loss=8.77172
Epoch 23/80: current_loss=8.82224 | best_loss=8.77172
Epoch 24/80: current_loss=8.81478 | best_loss=8.77172
Early Stopping at epoch 24
      explained_var=-0.00337 | mse_loss=8.75276
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.26489 | best_loss=9.26489
Epoch 1/80: current_loss=9.19944 | best_loss=9.19944
Epoch 2/80: current_loss=9.18707 | best_loss=9.18707
Epoch 3/80: current_loss=9.23808 | best_loss=9.18707
Epoch 4/80: current_loss=9.18645 | best_loss=9.18645
Epoch 5/80: current_loss=9.26877 | best_loss=9.18645
Epoch 6/80: current_loss=9.41037 | best_loss=9.18645
Epoch 7/80: current_loss=9.20301 | best_loss=9.18645
Epoch 8/80: current_loss=9.18838 | best_loss=9.18645
Epoch 9/80: current_loss=9.29610 | best_loss=9.18645
Epoch 10/80: current_loss=9.21629 | best_loss=9.18645
Epoch 11/80: current_loss=9.20746 | best_loss=9.18645
Epoch 12/80: current_loss=9.19776 | best_loss=9.18645
Epoch 13/80: current_loss=9.21264 | best_loss=9.18645
Epoch 14/80: current_loss=9.24919 | best_loss=9.18645
Epoch 15/80: current_loss=9.21884 | best_loss=9.18645
Epoch 16/80: current_loss=9.21305 | best_loss=9.18645
Epoch 17/80: current_loss=9.24416 | best_loss=9.18645
Epoch 18/80: current_loss=9.34121 | best_loss=9.18645
Epoch 19/80: current_loss=9.22397 | best_loss=9.18645
Epoch 20/80: current_loss=9.25236 | best_loss=9.18645
Epoch 21/80: current_loss=9.22232 | best_loss=9.18645
Epoch 22/80: current_loss=9.20896 | best_loss=9.18645
Epoch 23/80: current_loss=9.32021 | best_loss=9.18645
Epoch 24/80: current_loss=9.32150 | best_loss=9.18645
Early Stopping at epoch 24
      explained_var=0.01891 | mse_loss=9.12985
----------------------------------------------
Average early_stopping_point: 27| avg_exp_var=0.00824| avg_loss=9.15003
----------------------------------------------


----------------------------------------------
Params for Trial 6
{'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=100.16991 | best_loss=100.16991
Epoch 1/80: current_loss=93.58270 | best_loss=93.58270
Epoch 2/80: current_loss=81.42063 | best_loss=81.42063
Epoch 3/80: current_loss=66.39378 | best_loss=66.39378
Epoch 4/80: current_loss=56.03159 | best_loss=56.03159
Epoch 5/80: current_loss=49.57144 | best_loss=49.57144
Epoch 6/80: current_loss=45.15525 | best_loss=45.15525
Epoch 7/80: current_loss=41.78849 | best_loss=41.78849
Epoch 8/80: current_loss=39.05426 | best_loss=39.05426
Epoch 9/80: current_loss=36.75388 | best_loss=36.75388
Epoch 10/80: current_loss=34.74984 | best_loss=34.74984
Epoch 11/80: current_loss=32.97989 | best_loss=32.97989
Epoch 12/80: current_loss=31.39960 | best_loss=31.39960
Epoch 13/80: current_loss=29.94213 | best_loss=29.94213
Epoch 14/80: current_loss=28.65151 | best_loss=28.65151
Epoch 15/80: current_loss=27.41013 | best_loss=27.41013
Epoch 16/80: current_loss=26.26311 | best_loss=26.26311
Epoch 17/80: current_loss=25.21742 | best_loss=25.21742
Epoch 18/80: current_loss=24.22698 | best_loss=24.22698
Epoch 19/80: current_loss=23.31122 | best_loss=23.31122
Epoch 20/80: current_loss=22.44289 | best_loss=22.44289
Epoch 21/80: current_loss=21.62651 | best_loss=21.62651
Epoch 22/80: current_loss=20.88513 | best_loss=20.88513
Epoch 23/80: current_loss=20.15474 | best_loss=20.15474
Epoch 24/80: current_loss=19.49711 | best_loss=19.49711
Epoch 25/80: current_loss=18.86473 | best_loss=18.86473
Epoch 26/80: current_loss=18.28014 | best_loss=18.28014
Epoch 27/80: current_loss=17.75295 | best_loss=17.75295
Epoch 28/80: current_loss=17.23403 | best_loss=17.23403
Epoch 29/80: current_loss=16.76164 | best_loss=16.76164
Epoch 30/80: current_loss=16.32848 | best_loss=16.32848
Epoch 31/80: current_loss=15.90677 | best_loss=15.90677
Epoch 32/80: current_loss=15.51720 | best_loss=15.51720
Epoch 33/80: current_loss=15.16355 | best_loss=15.16355
Epoch 34/80: current_loss=14.82786 | best_loss=14.82786
Epoch 35/80: current_loss=14.51232 | best_loss=14.51232
Epoch 36/80: current_loss=14.22367 | best_loss=14.22367
Epoch 37/80: current_loss=13.96161 | best_loss=13.96161
Epoch 38/80: current_loss=13.71651 | best_loss=13.71651
Epoch 39/80: current_loss=13.48798 | best_loss=13.48798
Epoch 40/80: current_loss=13.26805 | best_loss=13.26805
Epoch 41/80: current_loss=13.06314 | best_loss=13.06314
Epoch 42/80: current_loss=12.87098 | best_loss=12.87098
Epoch 43/80: current_loss=12.69204 | best_loss=12.69204
Epoch 44/80: current_loss=12.54638 | best_loss=12.54638
Epoch 45/80: current_loss=12.39968 | best_loss=12.39968
Epoch 46/80: current_loss=12.26958 | best_loss=12.26958
Epoch 47/80: current_loss=12.13337 | best_loss=12.13337
Epoch 48/80: current_loss=12.02625 | best_loss=12.02625
Epoch 49/80: current_loss=11.91492 | best_loss=11.91492
Epoch 50/80: current_loss=11.81505 | best_loss=11.81505
Epoch 51/80: current_loss=11.71610 | best_loss=11.71610
Epoch 52/80: current_loss=11.62673 | best_loss=11.62673
Epoch 53/80: current_loss=11.54750 | best_loss=11.54750
Epoch 54/80: current_loss=11.47574 | best_loss=11.47574
Epoch 55/80: current_loss=11.40570 | best_loss=11.40570
Epoch 56/80: current_loss=11.34667 | best_loss=11.34667
Epoch 57/80: current_loss=11.29260 | best_loss=11.29260
Epoch 58/80: current_loss=11.24421 | best_loss=11.24421
Epoch 59/80: current_loss=11.19378 | best_loss=11.19378
Epoch 60/80: current_loss=11.14745 | best_loss=11.14745
Epoch 61/80: current_loss=11.10237 | best_loss=11.10237
Epoch 62/80: current_loss=11.06625 | best_loss=11.06625
Epoch 63/80: current_loss=11.02481 | best_loss=11.02481
Epoch 64/80: current_loss=10.99020 | best_loss=10.99020
Epoch 65/80: current_loss=10.95967 | best_loss=10.95967
Epoch 66/80: current_loss=10.92925 | best_loss=10.92925
Epoch 67/80: current_loss=10.90025 | best_loss=10.90025
Epoch 68/80: current_loss=10.87063 | best_loss=10.87063
Epoch 69/80: current_loss=10.84751 | best_loss=10.84751
Epoch 70/80: current_loss=10.83096 | best_loss=10.83096
Epoch 71/80: current_loss=10.80664 | best_loss=10.80664
Epoch 72/80: current_loss=10.78737 | best_loss=10.78737
Epoch 73/80: current_loss=10.77251 | best_loss=10.77251
Epoch 74/80: current_loss=10.75349 | best_loss=10.75349
Epoch 75/80: current_loss=10.73686 | best_loss=10.73686
Epoch 76/80: current_loss=10.72490 | best_loss=10.72490
Epoch 77/80: current_loss=10.70834 | best_loss=10.70834
Epoch 78/80: current_loss=10.69347 | best_loss=10.69347
Epoch 79/80: current_loss=10.67737 | best_loss=10.67737
      explained_var=-0.03955 | mse_loss=10.95885
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.69602 | best_loss=10.69602
Epoch 1/80: current_loss=10.65483 | best_loss=10.65483
Epoch 2/80: current_loss=10.62176 | best_loss=10.62176
Epoch 3/80: current_loss=10.59248 | best_loss=10.59248
Epoch 4/80: current_loss=10.57083 | best_loss=10.57083
Epoch 5/80: current_loss=10.54999 | best_loss=10.54999
Epoch 6/80: current_loss=10.53835 | best_loss=10.53835
Epoch 7/80: current_loss=10.52378 | best_loss=10.52378
Epoch 8/80: current_loss=10.51839 | best_loss=10.51839
Epoch 9/80: current_loss=10.50845 | best_loss=10.50845
Epoch 10/80: current_loss=10.49643 | best_loss=10.49643
Epoch 11/80: current_loss=10.49060 | best_loss=10.49060
Epoch 12/80: current_loss=10.48416 | best_loss=10.48416
Epoch 13/80: current_loss=10.47568 | best_loss=10.47568
Epoch 14/80: current_loss=10.47035 | best_loss=10.47035
Epoch 15/80: current_loss=10.46915 | best_loss=10.46915
Epoch 16/80: current_loss=10.46974 | best_loss=10.46915
Epoch 17/80: current_loss=10.45863 | best_loss=10.45863
Epoch 18/80: current_loss=10.45611 | best_loss=10.45611
Epoch 19/80: current_loss=10.45565 | best_loss=10.45565
Epoch 20/80: current_loss=10.44778 | best_loss=10.44778
Epoch 21/80: current_loss=10.44520 | best_loss=10.44520
Epoch 22/80: current_loss=10.44403 | best_loss=10.44403
Epoch 23/80: current_loss=10.44310 | best_loss=10.44310
Epoch 24/80: current_loss=10.44596 | best_loss=10.44310
Epoch 25/80: current_loss=10.44329 | best_loss=10.44310
Epoch 26/80: current_loss=10.44532 | best_loss=10.44310
Epoch 27/80: current_loss=10.44365 | best_loss=10.44310
Epoch 28/80: current_loss=10.44359 | best_loss=10.44310
Epoch 29/80: current_loss=10.44148 | best_loss=10.44148
Epoch 30/80: current_loss=10.44270 | best_loss=10.44148
Epoch 31/80: current_loss=10.43874 | best_loss=10.43874
Epoch 32/80: current_loss=10.43559 | best_loss=10.43559
Epoch 33/80: current_loss=10.43345 | best_loss=10.43345
Epoch 34/80: current_loss=10.43682 | best_loss=10.43345
Epoch 35/80: current_loss=10.43505 | best_loss=10.43345
Epoch 36/80: current_loss=10.42998 | best_loss=10.42998
Epoch 37/80: current_loss=10.43006 | best_loss=10.42998
Epoch 38/80: current_loss=10.43098 | best_loss=10.42998
Epoch 39/80: current_loss=10.42465 | best_loss=10.42465
Epoch 40/80: current_loss=10.42550 | best_loss=10.42465
Epoch 41/80: current_loss=10.42258 | best_loss=10.42258
Epoch 42/80: current_loss=10.42109 | best_loss=10.42109
Epoch 43/80: current_loss=10.41857 | best_loss=10.41857
Epoch 44/80: current_loss=10.41652 | best_loss=10.41652
Epoch 45/80: current_loss=10.41851 | best_loss=10.41652
Epoch 46/80: current_loss=10.41617 | best_loss=10.41617
Epoch 47/80: current_loss=10.41589 | best_loss=10.41589
Epoch 48/80: current_loss=10.41624 | best_loss=10.41589
Epoch 49/80: current_loss=10.41387 | best_loss=10.41387
Epoch 50/80: current_loss=10.40869 | best_loss=10.40869
Epoch 51/80: current_loss=10.40570 | best_loss=10.40570
Epoch 52/80: current_loss=10.40060 | best_loss=10.40060
Epoch 53/80: current_loss=10.39204 | best_loss=10.39204
Epoch 54/80: current_loss=10.38891 | best_loss=10.38891
Epoch 55/80: current_loss=10.37948 | best_loss=10.37948
Epoch 56/80: current_loss=10.37660 | best_loss=10.37660
Epoch 57/80: current_loss=10.36944 | best_loss=10.36944
Epoch 58/80: current_loss=10.36296 | best_loss=10.36296
Epoch 59/80: current_loss=10.35317 | best_loss=10.35317
Epoch 60/80: current_loss=10.34644 | best_loss=10.34644
Epoch 61/80: current_loss=10.34233 | best_loss=10.34233
Epoch 62/80: current_loss=10.33455 | best_loss=10.33455
Epoch 63/80: current_loss=10.32832 | best_loss=10.32832
Epoch 64/80: current_loss=10.31937 | best_loss=10.31937
Epoch 65/80: current_loss=10.31266 | best_loss=10.31266
Epoch 66/80: current_loss=10.30218 | best_loss=10.30218
Epoch 67/80: current_loss=10.29608 | best_loss=10.29608
Epoch 68/80: current_loss=10.28615 | best_loss=10.28615
Epoch 69/80: current_loss=10.28034 | best_loss=10.28034
Epoch 70/80: current_loss=10.26808 | best_loss=10.26808
Epoch 71/80: current_loss=10.26253 | best_loss=10.26253
Epoch 72/80: current_loss=10.25302 | best_loss=10.25302
Epoch 73/80: current_loss=10.25096 | best_loss=10.25096
Epoch 74/80: current_loss=10.23750 | best_loss=10.23750
Epoch 75/80: current_loss=10.22839 | best_loss=10.22839
Epoch 76/80: current_loss=10.21919 | best_loss=10.21919
Epoch 77/80: current_loss=10.21381 | best_loss=10.21381
Epoch 78/80: current_loss=10.20592 | best_loss=10.20592
Epoch 79/80: current_loss=10.20932 | best_loss=10.20592
      explained_var=0.02380 | mse_loss=9.84479
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.49469 | best_loss=7.49469
Epoch 1/80: current_loss=7.48184 | best_loss=7.48184
Epoch 2/80: current_loss=7.50266 | best_loss=7.48184
Epoch 3/80: current_loss=7.49414 | best_loss=7.48184
Epoch 4/80: current_loss=7.50171 | best_loss=7.48184
Epoch 5/80: current_loss=7.51778 | best_loss=7.48184
Epoch 6/80: current_loss=7.50820 | best_loss=7.48184
Epoch 7/80: current_loss=7.52262 | best_loss=7.48184
Epoch 8/80: current_loss=7.51453 | best_loss=7.48184
Epoch 9/80: current_loss=7.52987 | best_loss=7.48184
Epoch 10/80: current_loss=7.52383 | best_loss=7.48184
Epoch 11/80: current_loss=7.51698 | best_loss=7.48184
Epoch 12/80: current_loss=7.52156 | best_loss=7.48184
Epoch 13/80: current_loss=7.54223 | best_loss=7.48184
Epoch 14/80: current_loss=7.53684 | best_loss=7.48184
Epoch 15/80: current_loss=7.53434 | best_loss=7.48184
Epoch 16/80: current_loss=7.55242 | best_loss=7.48184
Epoch 17/80: current_loss=7.54799 | best_loss=7.48184
Epoch 18/80: current_loss=7.55783 | best_loss=7.48184
Epoch 19/80: current_loss=7.56034 | best_loss=7.48184
Epoch 20/80: current_loss=7.58420 | best_loss=7.48184
Epoch 21/80: current_loss=7.57139 | best_loss=7.48184
Early Stopping at epoch 21
      explained_var=0.00327 | mse_loss=7.58138
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.81045 | best_loss=8.81045
Epoch 1/80: current_loss=8.81856 | best_loss=8.81045
Epoch 2/80: current_loss=8.81635 | best_loss=8.81045
Epoch 3/80: current_loss=8.81200 | best_loss=8.81045
Epoch 4/80: current_loss=8.81151 | best_loss=8.81045
Epoch 5/80: current_loss=8.81188 | best_loss=8.81045
Epoch 6/80: current_loss=8.82110 | best_loss=8.81045
Epoch 7/80: current_loss=8.81478 | best_loss=8.81045
Epoch 8/80: current_loss=8.81539 | best_loss=8.81045
Epoch 9/80: current_loss=8.81731 | best_loss=8.81045
Epoch 10/80: current_loss=8.81161 | best_loss=8.81045
Epoch 11/80: current_loss=8.84932 | best_loss=8.81045
Epoch 12/80: current_loss=8.83755 | best_loss=8.81045
Epoch 13/80: current_loss=8.84044 | best_loss=8.81045
Epoch 14/80: current_loss=8.81330 | best_loss=8.81045
Epoch 15/80: current_loss=8.81281 | best_loss=8.81045
Epoch 16/80: current_loss=8.81510 | best_loss=8.81045
Epoch 17/80: current_loss=8.83821 | best_loss=8.81045
Epoch 18/80: current_loss=8.83531 | best_loss=8.81045
Epoch 19/80: current_loss=8.81477 | best_loss=8.81045
Epoch 20/80: current_loss=8.84715 | best_loss=8.81045
Early Stopping at epoch 20
      explained_var=-0.00653 | mse_loss=8.77645
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.15167 | best_loss=9.15167
Epoch 1/80: current_loss=9.14481 | best_loss=9.14481
Epoch 2/80: current_loss=9.13334 | best_loss=9.13334
Epoch 3/80: current_loss=9.15588 | best_loss=9.13334
Epoch 4/80: current_loss=9.14898 | best_loss=9.13334
Epoch 5/80: current_loss=9.13942 | best_loss=9.13334
Epoch 6/80: current_loss=9.13037 | best_loss=9.13037
Epoch 7/80: current_loss=9.14644 | best_loss=9.13037
Epoch 8/80: current_loss=9.14685 | best_loss=9.13037
Epoch 9/80: current_loss=9.13984 | best_loss=9.13037
Epoch 10/80: current_loss=9.14444 | best_loss=9.13037
Epoch 11/80: current_loss=9.14818 | best_loss=9.13037
Epoch 12/80: current_loss=9.14102 | best_loss=9.13037
Epoch 13/80: current_loss=9.14343 | best_loss=9.13037
Epoch 14/80: current_loss=9.13498 | best_loss=9.13037
Epoch 15/80: current_loss=9.13897 | best_loss=9.13037
Epoch 16/80: current_loss=9.15491 | best_loss=9.13037
Epoch 17/80: current_loss=9.15264 | best_loss=9.13037
Epoch 18/80: current_loss=9.13964 | best_loss=9.13037
Epoch 19/80: current_loss=9.13665 | best_loss=9.13037
Epoch 20/80: current_loss=9.14585 | best_loss=9.13037
Epoch 21/80: current_loss=9.14119 | best_loss=9.13037
Epoch 22/80: current_loss=9.14383 | best_loss=9.13037
Epoch 23/80: current_loss=9.15452 | best_loss=9.13037
Epoch 24/80: current_loss=9.14101 | best_loss=9.13037
Epoch 25/80: current_loss=9.13220 | best_loss=9.13037
Epoch 26/80: current_loss=9.16242 | best_loss=9.13037
Early Stopping at epoch 26
      explained_var=0.02178 | mse_loss=9.09096
----------------------------------------------
Average early_stopping_point: 33| avg_exp_var=0.00055| avg_loss=9.25049
----------------------------------------------


----------------------------------------------
Params for Trial 7
{'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=74.41724 | best_loss=74.41724
Epoch 1/80: current_loss=69.21815 | best_loss=69.21815
Epoch 2/80: current_loss=33.33683 | best_loss=33.33683
Epoch 3/80: current_loss=33.19320 | best_loss=33.19320
Epoch 4/80: current_loss=23.97333 | best_loss=23.97333
Epoch 5/80: current_loss=32.08708 | best_loss=23.97333
Epoch 6/80: current_loss=14.24127 | best_loss=14.24127
Epoch 7/80: current_loss=12.01259 | best_loss=12.01259
Epoch 8/80: current_loss=13.21389 | best_loss=12.01259
Epoch 9/80: current_loss=11.74341 | best_loss=11.74341
Epoch 10/80: current_loss=14.06262 | best_loss=11.74341
Epoch 11/80: current_loss=13.10239 | best_loss=11.74341
Epoch 12/80: current_loss=11.66010 | best_loss=11.66010
Epoch 13/80: current_loss=11.25337 | best_loss=11.25337
Epoch 14/80: current_loss=13.02079 | best_loss=11.25337
Epoch 15/80: current_loss=17.01475 | best_loss=11.25337
Epoch 16/80: current_loss=10.61732 | best_loss=10.61732
Epoch 17/80: current_loss=12.28438 | best_loss=10.61732
Epoch 18/80: current_loss=11.41588 | best_loss=10.61732
Epoch 19/80: current_loss=19.89382 | best_loss=10.61732
Epoch 20/80: current_loss=11.83992 | best_loss=10.61732
Epoch 21/80: current_loss=12.58141 | best_loss=10.61732
Epoch 22/80: current_loss=15.18254 | best_loss=10.61732
Epoch 23/80: current_loss=12.65516 | best_loss=10.61732
Epoch 24/80: current_loss=23.54860 | best_loss=10.61732
Epoch 25/80: current_loss=11.01675 | best_loss=10.61732
Epoch 26/80: current_loss=12.87252 | best_loss=10.61732
Epoch 27/80: current_loss=18.76925 | best_loss=10.61732
Epoch 28/80: current_loss=18.43837 | best_loss=10.61732
Epoch 29/80: current_loss=30.76866 | best_loss=10.61732
Epoch 30/80: current_loss=29.99223 | best_loss=10.61732
Epoch 31/80: current_loss=93.23133 | best_loss=10.61732
Epoch 32/80: current_loss=245.74024 | best_loss=10.61732
Epoch 33/80: current_loss=33.04320 | best_loss=10.61732
Epoch 34/80: current_loss=38.29253 | best_loss=10.61732
Epoch 35/80: current_loss=10.82256 | best_loss=10.61732
Epoch 36/80: current_loss=15.40818 | best_loss=10.61732
Early Stopping at epoch 36
      explained_var=-0.02957 | mse_loss=10.85932
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=13.69028 | best_loss=13.69028
Epoch 1/80: current_loss=18.99587 | best_loss=13.69028
Epoch 2/80: current_loss=11.58831 | best_loss=11.58831
Epoch 3/80: current_loss=15.05266 | best_loss=11.58831
Epoch 4/80: current_loss=12.01734 | best_loss=11.58831
Epoch 5/80: current_loss=12.98899 | best_loss=11.58831
Epoch 6/80: current_loss=13.39692 | best_loss=11.58831
Epoch 7/80: current_loss=18.83740 | best_loss=11.58831
Epoch 8/80: current_loss=18.03847 | best_loss=11.58831
Epoch 9/80: current_loss=17.40616 | best_loss=11.58831
Epoch 10/80: current_loss=12.49065 | best_loss=11.58831
Epoch 11/80: current_loss=22.91555 | best_loss=11.58831
Epoch 12/80: current_loss=30.22179 | best_loss=11.58831
Epoch 13/80: current_loss=11.89410 | best_loss=11.58831
Epoch 14/80: current_loss=13.65310 | best_loss=11.58831
Epoch 15/80: current_loss=24.33441 | best_loss=11.58831
Epoch 16/80: current_loss=10.26233 | best_loss=10.26233
Epoch 17/80: current_loss=11.87073 | best_loss=10.26233
Epoch 18/80: current_loss=17.81946 | best_loss=10.26233
Epoch 19/80: current_loss=16.62409 | best_loss=10.26233
Epoch 20/80: current_loss=16.31343 | best_loss=10.26233
Epoch 21/80: current_loss=12.85709 | best_loss=10.26233
Epoch 22/80: current_loss=20.65893 | best_loss=10.26233
Epoch 23/80: current_loss=13.62664 | best_loss=10.26233
Epoch 24/80: current_loss=10.41926 | best_loss=10.26233
Epoch 25/80: current_loss=30.42251 | best_loss=10.26233
Epoch 26/80: current_loss=13.99294 | best_loss=10.26233
Epoch 27/80: current_loss=14.12558 | best_loss=10.26233
Epoch 28/80: current_loss=10.87003 | best_loss=10.26233
Epoch 29/80: current_loss=11.83232 | best_loss=10.26233
Epoch 30/80: current_loss=19.16174 | best_loss=10.26233
Epoch 31/80: current_loss=13.16312 | best_loss=10.26233
Epoch 32/80: current_loss=12.51223 | best_loss=10.26233
Epoch 33/80: current_loss=15.28504 | best_loss=10.26233
Epoch 34/80: current_loss=13.70234 | best_loss=10.26233
Epoch 35/80: current_loss=27.40495 | best_loss=10.26233
Epoch 36/80: current_loss=12.32137 | best_loss=10.26233
Early Stopping at epoch 36
      explained_var=0.01444 | mse_loss=9.92937
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.47724 | best_loss=17.47724
Epoch 1/80: current_loss=9.20694 | best_loss=9.20694
Epoch 2/80: current_loss=12.00909 | best_loss=9.20694
Epoch 3/80: current_loss=11.99975 | best_loss=9.20694
Epoch 4/80: current_loss=10.61348 | best_loss=9.20694
Epoch 5/80: current_loss=12.88448 | best_loss=9.20694
Epoch 6/80: current_loss=12.15317 | best_loss=9.20694
Epoch 7/80: current_loss=21.34812 | best_loss=9.20694
Epoch 8/80: current_loss=10.59888 | best_loss=9.20694
Epoch 9/80: current_loss=14.13237 | best_loss=9.20694
Epoch 10/80: current_loss=29.93139 | best_loss=9.20694
Epoch 11/80: current_loss=21.69119 | best_loss=9.20694
Epoch 12/80: current_loss=10.59595 | best_loss=9.20694
Epoch 13/80: current_loss=9.65089 | best_loss=9.20694
Epoch 14/80: current_loss=10.29010 | best_loss=9.20694
Epoch 15/80: current_loss=17.35939 | best_loss=9.20694
Epoch 16/80: current_loss=10.21490 | best_loss=9.20694
Epoch 17/80: current_loss=10.58930 | best_loss=9.20694
Epoch 18/80: current_loss=10.20294 | best_loss=9.20694
Epoch 19/80: current_loss=15.97789 | best_loss=9.20694
Epoch 20/80: current_loss=8.45764 | best_loss=8.45764
Epoch 21/80: current_loss=41.36581 | best_loss=8.45764
Epoch 22/80: current_loss=9.24530 | best_loss=8.45764
Epoch 23/80: current_loss=14.73249 | best_loss=8.45764
Epoch 24/80: current_loss=14.15731 | best_loss=8.45764
Epoch 25/80: current_loss=11.49190 | best_loss=8.45764
Epoch 26/80: current_loss=12.60916 | best_loss=8.45764
Epoch 27/80: current_loss=14.30496 | best_loss=8.45764
Epoch 28/80: current_loss=16.36574 | best_loss=8.45764
Epoch 29/80: current_loss=9.70882 | best_loss=8.45764
Epoch 30/80: current_loss=26.30066 | best_loss=8.45764
Epoch 31/80: current_loss=20.44636 | best_loss=8.45764
Epoch 32/80: current_loss=9.26815 | best_loss=8.45764
Epoch 33/80: current_loss=20.01017 | best_loss=8.45764
Epoch 34/80: current_loss=15.93736 | best_loss=8.45764
Epoch 35/80: current_loss=10.35757 | best_loss=8.45764
Epoch 36/80: current_loss=18.61434 | best_loss=8.45764
Epoch 37/80: current_loss=8.60993 | best_loss=8.45764
Epoch 38/80: current_loss=11.40439 | best_loss=8.45764
Epoch 39/80: current_loss=17.13845 | best_loss=8.45764
Epoch 40/80: current_loss=14.12836 | best_loss=8.45764
Early Stopping at epoch 40
      explained_var=-0.10359 | mse_loss=8.43833
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=11.35363 | best_loss=11.35363
Epoch 1/80: current_loss=10.98854 | best_loss=10.98854
Epoch 2/80: current_loss=22.18825 | best_loss=10.98854
Epoch 3/80: current_loss=16.27754 | best_loss=10.98854
Epoch 4/80: current_loss=14.75438 | best_loss=10.98854
Epoch 5/80: current_loss=11.15309 | best_loss=10.98854
Epoch 6/80: current_loss=12.84534 | best_loss=10.98854
Epoch 7/80: current_loss=10.01523 | best_loss=10.01523
Epoch 8/80: current_loss=14.51342 | best_loss=10.01523
Epoch 9/80: current_loss=19.85996 | best_loss=10.01523
Epoch 10/80: current_loss=10.41427 | best_loss=10.01523
Epoch 11/80: current_loss=22.84980 | best_loss=10.01523
Epoch 12/80: current_loss=29.01609 | best_loss=10.01523
Epoch 13/80: current_loss=15.03674 | best_loss=10.01523
Epoch 14/80: current_loss=13.22452 | best_loss=10.01523
Epoch 15/80: current_loss=23.44569 | best_loss=10.01523
Epoch 16/80: current_loss=51.84040 | best_loss=10.01523
Epoch 17/80: current_loss=11.39620 | best_loss=10.01523
Epoch 18/80: current_loss=34.25812 | best_loss=10.01523
Epoch 19/80: current_loss=272.82460 | best_loss=10.01523
Epoch 20/80: current_loss=9.86401 | best_loss=9.86401
Epoch 21/80: current_loss=10.00628 | best_loss=9.86401
Epoch 22/80: current_loss=14.99173 | best_loss=9.86401
Epoch 23/80: current_loss=12.85626 | best_loss=9.86401
Epoch 24/80: current_loss=9.30040 | best_loss=9.30040
Epoch 25/80: current_loss=12.46521 | best_loss=9.30040
Epoch 26/80: current_loss=11.03271 | best_loss=9.30040
Epoch 27/80: current_loss=9.14437 | best_loss=9.14437
Epoch 28/80: current_loss=9.54613 | best_loss=9.14437
Epoch 29/80: current_loss=9.63895 | best_loss=9.14437
Epoch 30/80: current_loss=9.25955 | best_loss=9.14437
Epoch 31/80: current_loss=9.62296 | best_loss=9.14437
Epoch 32/80: current_loss=8.95646 | best_loss=8.95646
Epoch 33/80: current_loss=9.36632 | best_loss=8.95646
Epoch 34/80: current_loss=12.84075 | best_loss=8.95646
Epoch 35/80: current_loss=8.96532 | best_loss=8.95646
Epoch 36/80: current_loss=9.10152 | best_loss=8.95646
Epoch 37/80: current_loss=9.01120 | best_loss=8.95646
Epoch 38/80: current_loss=16.90102 | best_loss=8.95646
Epoch 39/80: current_loss=9.88932 | best_loss=8.95646
Epoch 40/80: current_loss=9.37551 | best_loss=8.95646
Epoch 41/80: current_loss=9.76702 | best_loss=8.95646
Epoch 42/80: current_loss=13.76440 | best_loss=8.95646
Epoch 43/80: current_loss=12.42041 | best_loss=8.95646
Epoch 44/80: current_loss=11.57759 | best_loss=8.95646
Epoch 45/80: current_loss=18.39930 | best_loss=8.95646
Epoch 46/80: current_loss=10.48081 | best_loss=8.95646
Epoch 47/80: current_loss=30.48143 | best_loss=8.95646
Epoch 48/80: current_loss=26.60678 | best_loss=8.95646
Epoch 49/80: current_loss=15.50764 | best_loss=8.95646
Epoch 50/80: current_loss=13.62439 | best_loss=8.95646
Epoch 51/80: current_loss=12.41366 | best_loss=8.95646
Epoch 52/80: current_loss=11.20003 | best_loss=8.95646
Early Stopping at epoch 52
      explained_var=-0.02045 | mse_loss=8.90134
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=19.58140 | best_loss=19.58140
Epoch 1/80: current_loss=11.82966 | best_loss=11.82966
Epoch 2/80: current_loss=16.41760 | best_loss=11.82966
Epoch 3/80: current_loss=12.69718 | best_loss=11.82966
Epoch 4/80: current_loss=12.85779 | best_loss=11.82966
Epoch 5/80: current_loss=17.89499 | best_loss=11.82966
Epoch 6/80: current_loss=22.42424 | best_loss=11.82966
Epoch 7/80: current_loss=12.04331 | best_loss=11.82966
Epoch 8/80: current_loss=20.10231 | best_loss=11.82966
Epoch 9/80: current_loss=10.49575 | best_loss=10.49575
Epoch 10/80: current_loss=10.97377 | best_loss=10.49575
Epoch 11/80: current_loss=11.49164 | best_loss=10.49575
Epoch 12/80: current_loss=20.08076 | best_loss=10.49575
Epoch 13/80: current_loss=10.58296 | best_loss=10.49575
Epoch 14/80: current_loss=19.17308 | best_loss=10.49575
Epoch 15/80: current_loss=19.79849 | best_loss=10.49575
Epoch 16/80: current_loss=12.97096 | best_loss=10.49575
Epoch 17/80: current_loss=11.58112 | best_loss=10.49575
Epoch 18/80: current_loss=14.18995 | best_loss=10.49575
Epoch 19/80: current_loss=14.93383 | best_loss=10.49575
Epoch 20/80: current_loss=13.01882 | best_loss=10.49575
Epoch 21/80: current_loss=12.65088 | best_loss=10.49575
Epoch 22/80: current_loss=10.09471 | best_loss=10.09471
Epoch 23/80: current_loss=11.29604 | best_loss=10.09471
Epoch 24/80: current_loss=19.31216 | best_loss=10.09471
Epoch 25/80: current_loss=16.67113 | best_loss=10.09471
Epoch 26/80: current_loss=21.14295 | best_loss=10.09471
Epoch 27/80: current_loss=10.79011 | best_loss=10.09471
Epoch 28/80: current_loss=12.20786 | best_loss=10.09471
Epoch 29/80: current_loss=12.85240 | best_loss=10.09471
Epoch 30/80: current_loss=15.08115 | best_loss=10.09471
Epoch 31/80: current_loss=17.20402 | best_loss=10.09471
Epoch 32/80: current_loss=19.79876 | best_loss=10.09471
Epoch 33/80: current_loss=15.27120 | best_loss=10.09471
Epoch 34/80: current_loss=19.41144 | best_loss=10.09471
Epoch 35/80: current_loss=13.37449 | best_loss=10.09471
Epoch 36/80: current_loss=28.69770 | best_loss=10.09471
Epoch 37/80: current_loss=15.85920 | best_loss=10.09471
Epoch 38/80: current_loss=21.85064 | best_loss=10.09471
Epoch 39/80: current_loss=13.08522 | best_loss=10.09471
Epoch 40/80: current_loss=14.85481 | best_loss=10.09471
Epoch 41/80: current_loss=11.86286 | best_loss=10.09471
Epoch 42/80: current_loss=10.98242 | best_loss=10.09471
Early Stopping at epoch 42
      explained_var=-0.08009 | mse_loss=10.07593
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=-0.04385| avg_loss=9.64086
----------------------------------------------


----------------------------------------------
Params for Trial 8
{'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=106.84951 | best_loss=106.84951
Epoch 1/80: current_loss=106.59030 | best_loss=106.59030
Epoch 2/80: current_loss=106.33248 | best_loss=106.33248
Epoch 3/80: current_loss=106.07715 | best_loss=106.07715
Epoch 4/80: current_loss=105.82213 | best_loss=105.82213
Epoch 5/80: current_loss=105.57048 | best_loss=105.57048
Epoch 6/80: current_loss=105.31683 | best_loss=105.31683
Epoch 7/80: current_loss=105.06718 | best_loss=105.06718
Epoch 8/80: current_loss=104.81300 | best_loss=104.81300
Epoch 9/80: current_loss=104.55911 | best_loss=104.55911
Epoch 10/80: current_loss=104.30201 | best_loss=104.30201
Epoch 11/80: current_loss=104.04059 | best_loss=104.04059
Epoch 12/80: current_loss=103.77449 | best_loss=103.77449
Epoch 13/80: current_loss=103.50609 | best_loss=103.50609
Epoch 14/80: current_loss=103.22846 | best_loss=103.22846
Epoch 15/80: current_loss=102.94370 | best_loss=102.94370
Epoch 16/80: current_loss=102.65152 | best_loss=102.65152
Epoch 17/80: current_loss=102.35293 | best_loss=102.35293
Epoch 18/80: current_loss=102.03492 | best_loss=102.03492
Epoch 19/80: current_loss=101.69882 | best_loss=101.69882
Epoch 20/80: current_loss=101.34726 | best_loss=101.34726
Epoch 21/80: current_loss=100.97338 | best_loss=100.97338
Epoch 22/80: current_loss=100.57544 | best_loss=100.57544
Epoch 23/80: current_loss=100.14551 | best_loss=100.14551
Epoch 24/80: current_loss=99.69990 | best_loss=99.69990
Epoch 25/80: current_loss=99.21784 | best_loss=99.21784
Epoch 26/80: current_loss=98.69634 | best_loss=98.69634
Epoch 27/80: current_loss=98.14393 | best_loss=98.14393
Epoch 28/80: current_loss=97.54630 | best_loss=97.54630
Epoch 29/80: current_loss=96.89663 | best_loss=96.89663
Epoch 30/80: current_loss=96.20908 | best_loss=96.20908
Epoch 31/80: current_loss=95.47940 | best_loss=95.47940
Epoch 32/80: current_loss=94.69179 | best_loss=94.69179
Epoch 33/80: current_loss=93.85412 | best_loss=93.85412
Epoch 34/80: current_loss=92.98180 | best_loss=92.98180
Epoch 35/80: current_loss=92.06586 | best_loss=92.06586
Epoch 36/80: current_loss=91.11212 | best_loss=91.11212
Epoch 37/80: current_loss=90.13685 | best_loss=90.13685
Epoch 38/80: current_loss=89.13621 | best_loss=89.13621
Epoch 39/80: current_loss=88.11199 | best_loss=88.11199
Epoch 40/80: current_loss=87.06669 | best_loss=87.06669
Epoch 41/80: current_loss=86.00468 | best_loss=86.00468
Epoch 42/80: current_loss=84.94255 | best_loss=84.94255
Epoch 43/80: current_loss=83.90506 | best_loss=83.90506
Epoch 44/80: current_loss=82.85050 | best_loss=82.85050
Epoch 45/80: current_loss=81.81266 | best_loss=81.81266
Epoch 46/80: current_loss=80.78984 | best_loss=80.78984
Epoch 47/80: current_loss=79.76417 | best_loss=79.76417
Epoch 48/80: current_loss=78.77278 | best_loss=78.77278
Epoch 49/80: current_loss=77.77840 | best_loss=77.77840
Epoch 50/80: current_loss=76.81336 | best_loss=76.81336
Epoch 51/80: current_loss=75.87351 | best_loss=75.87351
Epoch 52/80: current_loss=74.94305 | best_loss=74.94305
Epoch 53/80: current_loss=74.04706 | best_loss=74.04706
Epoch 54/80: current_loss=73.15625 | best_loss=73.15625
Epoch 55/80: current_loss=72.29031 | best_loss=72.29031
Epoch 56/80: current_loss=71.44027 | best_loss=71.44027
Epoch 57/80: current_loss=70.61110 | best_loss=70.61110
Epoch 58/80: current_loss=69.80489 | best_loss=69.80489
Epoch 59/80: current_loss=69.01404 | best_loss=69.01404
Epoch 60/80: current_loss=68.25236 | best_loss=68.25236
Epoch 61/80: current_loss=67.48922 | best_loss=67.48922
Epoch 62/80: current_loss=66.75335 | best_loss=66.75335
Epoch 63/80: current_loss=66.03302 | best_loss=66.03302
Epoch 64/80: current_loss=65.33293 | best_loss=65.33293
Epoch 65/80: current_loss=64.65071 | best_loss=64.65071
Epoch 66/80: current_loss=63.98569 | best_loss=63.98569
Epoch 67/80: current_loss=63.32987 | best_loss=63.32987
Epoch 68/80: current_loss=62.69381 | best_loss=62.69381
Epoch 69/80: current_loss=62.06999 | best_loss=62.06999
Epoch 70/80: current_loss=61.46387 | best_loss=61.46387
Epoch 71/80: current_loss=60.87489 | best_loss=60.87489
Epoch 72/80: current_loss=60.29553 | best_loss=60.29553
Epoch 73/80: current_loss=59.73133 | best_loss=59.73133
Epoch 74/80: current_loss=59.17573 | best_loss=59.17573
Epoch 75/80: current_loss=58.63644 | best_loss=58.63644
Epoch 76/80: current_loss=58.10166 | best_loss=58.10166
Epoch 77/80: current_loss=57.58705 | best_loss=57.58705
Epoch 78/80: current_loss=57.08170 | best_loss=57.08170
Epoch 79/80: current_loss=56.58568 | best_loss=56.58568
      explained_var=-0.07816 | mse_loss=57.51141
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=56.49421 | best_loss=56.49421
Epoch 1/80: current_loss=55.99473 | best_loss=55.99473
Epoch 2/80: current_loss=55.51209 | best_loss=55.51209
Epoch 3/80: current_loss=55.04539 | best_loss=55.04539
Epoch 4/80: current_loss=54.58686 | best_loss=54.58686
Epoch 5/80: current_loss=54.14276 | best_loss=54.14276
Epoch 6/80: current_loss=53.70715 | best_loss=53.70715
Epoch 7/80: current_loss=53.27725 | best_loss=53.27725
Epoch 8/80: current_loss=52.85813 | best_loss=52.85813
Epoch 9/80: current_loss=52.45252 | best_loss=52.45252
Epoch 10/80: current_loss=52.05473 | best_loss=52.05473
Epoch 11/80: current_loss=51.66540 | best_loss=51.66540
Epoch 12/80: current_loss=51.28227 | best_loss=51.28227
Epoch 13/80: current_loss=50.90849 | best_loss=50.90849
Epoch 14/80: current_loss=50.53920 | best_loss=50.53920
Epoch 15/80: current_loss=50.17451 | best_loss=50.17451
Epoch 16/80: current_loss=49.82002 | best_loss=49.82002
Epoch 17/80: current_loss=49.47114 | best_loss=49.47114
Epoch 18/80: current_loss=49.12761 | best_loss=49.12761
Epoch 19/80: current_loss=48.79008 | best_loss=48.79008
Epoch 20/80: current_loss=48.45266 | best_loss=48.45266
Epoch 21/80: current_loss=48.12724 | best_loss=48.12724
Epoch 22/80: current_loss=47.80527 | best_loss=47.80527
Epoch 23/80: current_loss=47.48666 | best_loss=47.48666
Epoch 24/80: current_loss=47.17717 | best_loss=47.17717
Epoch 25/80: current_loss=46.87127 | best_loss=46.87127
Epoch 26/80: current_loss=46.56683 | best_loss=46.56683
Epoch 27/80: current_loss=46.26694 | best_loss=46.26694
Epoch 28/80: current_loss=45.96778 | best_loss=45.96778
Epoch 29/80: current_loss=45.68013 | best_loss=45.68013
Epoch 30/80: current_loss=45.39536 | best_loss=45.39536
Epoch 31/80: current_loss=45.11007 | best_loss=45.11007
Epoch 32/80: current_loss=44.83119 | best_loss=44.83119
Epoch 33/80: current_loss=44.55947 | best_loss=44.55947
Epoch 34/80: current_loss=44.28968 | best_loss=44.28968
Epoch 35/80: current_loss=44.01969 | best_loss=44.01969
Epoch 36/80: current_loss=43.75509 | best_loss=43.75509
Epoch 37/80: current_loss=43.49528 | best_loss=43.49528
Epoch 38/80: current_loss=43.23681 | best_loss=43.23681
Epoch 39/80: current_loss=42.98113 | best_loss=42.98113
Epoch 40/80: current_loss=42.73637 | best_loss=42.73637
Epoch 41/80: current_loss=42.48547 | best_loss=42.48547
Epoch 42/80: current_loss=42.24076 | best_loss=42.24076
Epoch 43/80: current_loss=41.99256 | best_loss=41.99256
Epoch 44/80: current_loss=41.75333 | best_loss=41.75333
Epoch 45/80: current_loss=41.51319 | best_loss=41.51319
Epoch 46/80: current_loss=41.28437 | best_loss=41.28437
Epoch 47/80: current_loss=41.05639 | best_loss=41.05639
Epoch 48/80: current_loss=40.82947 | best_loss=40.82947
Epoch 49/80: current_loss=40.59944 | best_loss=40.59944
Epoch 50/80: current_loss=40.37542 | best_loss=40.37542
Epoch 51/80: current_loss=40.15497 | best_loss=40.15497
Epoch 52/80: current_loss=39.93077 | best_loss=39.93077
Epoch 53/80: current_loss=39.71717 | best_loss=39.71717
Epoch 54/80: current_loss=39.50269 | best_loss=39.50269
Epoch 55/80: current_loss=39.29001 | best_loss=39.29001
Epoch 56/80: current_loss=39.07944 | best_loss=39.07944
Epoch 57/80: current_loss=38.87039 | best_loss=38.87039
Epoch 58/80: current_loss=38.66011 | best_loss=38.66011
Epoch 59/80: current_loss=38.45573 | best_loss=38.45573
Epoch 60/80: current_loss=38.25118 | best_loss=38.25118
Epoch 61/80: current_loss=38.05127 | best_loss=38.05127
Epoch 62/80: current_loss=37.85091 | best_loss=37.85091
Epoch 63/80: current_loss=37.65375 | best_loss=37.65375
Epoch 64/80: current_loss=37.45476 | best_loss=37.45476
Epoch 65/80: current_loss=37.26074 | best_loss=37.26074
Epoch 66/80: current_loss=37.06895 | best_loss=37.06895
Epoch 67/80: current_loss=36.87885 | best_loss=36.87885
Epoch 68/80: current_loss=36.68933 | best_loss=36.68933
Epoch 69/80: current_loss=36.50281 | best_loss=36.50281
Epoch 70/80: current_loss=36.31431 | best_loss=36.31431
Epoch 71/80: current_loss=36.13055 | best_loss=36.13055
Epoch 72/80: current_loss=35.94921 | best_loss=35.94921
Epoch 73/80: current_loss=35.76572 | best_loss=35.76572
Epoch 74/80: current_loss=35.58874 | best_loss=35.58874
Epoch 75/80: current_loss=35.41264 | best_loss=35.41264
Epoch 76/80: current_loss=35.23845 | best_loss=35.23845
Epoch 77/80: current_loss=35.06513 | best_loss=35.06513
Epoch 78/80: current_loss=34.89167 | best_loss=34.89167
Epoch 79/80: current_loss=34.71638 | best_loss=34.71638
      explained_var=-0.12578 | mse_loss=33.96786
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=28.08070 | best_loss=28.08070
Epoch 1/80: current_loss=27.88547 | best_loss=27.88547
Epoch 2/80: current_loss=27.69720 | best_loss=27.69720
Epoch 3/80: current_loss=27.50770 | best_loss=27.50770
Epoch 4/80: current_loss=27.32730 | best_loss=27.32730
Epoch 5/80: current_loss=27.14546 | best_loss=27.14546
Epoch 6/80: current_loss=26.96691 | best_loss=26.96691
Epoch 7/80: current_loss=26.78938 | best_loss=26.78938
Epoch 8/80: current_loss=26.61586 | best_loss=26.61586
Epoch 9/80: current_loss=26.44512 | best_loss=26.44512
Epoch 10/80: current_loss=26.27352 | best_loss=26.27352
Epoch 11/80: current_loss=26.10691 | best_loss=26.10691
Epoch 12/80: current_loss=25.94349 | best_loss=25.94349
Epoch 13/80: current_loss=25.78445 | best_loss=25.78445
Epoch 14/80: current_loss=25.62205 | best_loss=25.62205
Epoch 15/80: current_loss=25.46353 | best_loss=25.46353
Epoch 16/80: current_loss=25.30743 | best_loss=25.30743
Epoch 17/80: current_loss=25.15382 | best_loss=25.15382
Epoch 18/80: current_loss=25.00359 | best_loss=25.00359
Epoch 19/80: current_loss=24.85291 | best_loss=24.85291
Epoch 20/80: current_loss=24.70425 | best_loss=24.70425
Epoch 21/80: current_loss=24.55704 | best_loss=24.55704
Epoch 22/80: current_loss=24.41505 | best_loss=24.41505
Epoch 23/80: current_loss=24.27026 | best_loss=24.27026
Epoch 24/80: current_loss=24.12602 | best_loss=24.12602
Epoch 25/80: current_loss=23.98753 | best_loss=23.98753
Epoch 26/80: current_loss=23.84414 | best_loss=23.84414
Epoch 27/80: current_loss=23.71100 | best_loss=23.71100
Epoch 28/80: current_loss=23.57498 | best_loss=23.57498
Epoch 29/80: current_loss=23.44141 | best_loss=23.44141
Epoch 30/80: current_loss=23.30910 | best_loss=23.30910
Epoch 31/80: current_loss=23.17876 | best_loss=23.17876
Epoch 32/80: current_loss=23.05196 | best_loss=23.05196
Epoch 33/80: current_loss=22.92111 | best_loss=22.92111
Epoch 34/80: current_loss=22.79505 | best_loss=22.79505
Epoch 35/80: current_loss=22.67093 | best_loss=22.67093
Epoch 36/80: current_loss=22.54445 | best_loss=22.54445
Epoch 37/80: current_loss=22.42397 | best_loss=22.42397
Epoch 38/80: current_loss=22.30266 | best_loss=22.30266
Epoch 39/80: current_loss=22.18168 | best_loss=22.18168
Epoch 40/80: current_loss=22.06054 | best_loss=22.06054
Epoch 41/80: current_loss=21.94367 | best_loss=21.94367
Epoch 42/80: current_loss=21.82700 | best_loss=21.82700
Epoch 43/80: current_loss=21.70938 | best_loss=21.70938
Epoch 44/80: current_loss=21.59558 | best_loss=21.59558
Epoch 45/80: current_loss=21.48169 | best_loss=21.48169
Epoch 46/80: current_loss=21.36983 | best_loss=21.36983
Epoch 47/80: current_loss=21.25756 | best_loss=21.25756
Epoch 48/80: current_loss=21.14762 | best_loss=21.14762
Epoch 49/80: current_loss=21.03581 | best_loss=21.03581
Epoch 50/80: current_loss=20.92861 | best_loss=20.92861
Epoch 51/80: current_loss=20.82158 | best_loss=20.82158
Epoch 52/80: current_loss=20.71563 | best_loss=20.71563
Epoch 53/80: current_loss=20.60899 | best_loss=20.60899
Epoch 54/80: current_loss=20.50543 | best_loss=20.50543
Epoch 55/80: current_loss=20.40185 | best_loss=20.40185
Epoch 56/80: current_loss=20.29689 | best_loss=20.29689
Epoch 57/80: current_loss=20.19272 | best_loss=20.19272
Epoch 58/80: current_loss=20.09064 | best_loss=20.09064
Epoch 59/80: current_loss=19.99018 | best_loss=19.99018
Epoch 60/80: current_loss=19.89102 | best_loss=19.89102
Epoch 61/80: current_loss=19.79518 | best_loss=19.79518
Epoch 62/80: current_loss=19.69671 | best_loss=19.69671
Epoch 63/80: current_loss=19.60166 | best_loss=19.60166
Epoch 64/80: current_loss=19.50588 | best_loss=19.50588
Epoch 65/80: current_loss=19.41155 | best_loss=19.41155
Epoch 66/80: current_loss=19.31871 | best_loss=19.31871
Epoch 67/80: current_loss=19.22431 | best_loss=19.22431
Epoch 68/80: current_loss=19.13124 | best_loss=19.13124
Epoch 69/80: current_loss=19.03804 | best_loss=19.03804
Epoch 70/80: current_loss=18.94898 | best_loss=18.94898
Epoch 71/80: current_loss=18.85714 | best_loss=18.85714
Epoch 72/80: current_loss=18.76694 | best_loss=18.76694
Epoch 73/80: current_loss=18.67818 | best_loss=18.67818
Epoch 74/80: current_loss=18.58746 | best_loss=18.58746
Epoch 75/80: current_loss=18.50187 | best_loss=18.50187
Epoch 76/80: current_loss=18.41390 | best_loss=18.41390
Epoch 77/80: current_loss=18.32943 | best_loss=18.32943
Epoch 78/80: current_loss=18.24399 | best_loss=18.24399
Epoch 79/80: current_loss=18.15934 | best_loss=18.15934
      explained_var=-0.15954 | mse_loss=18.31541
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=21.49012 | best_loss=21.49012
Epoch 1/80: current_loss=21.38304 | best_loss=21.38304
Epoch 2/80: current_loss=21.27741 | best_loss=21.27741
Epoch 3/80: current_loss=21.17159 | best_loss=21.17159
Epoch 4/80: current_loss=21.06604 | best_loss=21.06604
Epoch 5/80: current_loss=20.96215 | best_loss=20.96215
Epoch 6/80: current_loss=20.86151 | best_loss=20.86151
Epoch 7/80: current_loss=20.75789 | best_loss=20.75789
Epoch 8/80: current_loss=20.66051 | best_loss=20.66051
Epoch 9/80: current_loss=20.55954 | best_loss=20.55954
Epoch 10/80: current_loss=20.46328 | best_loss=20.46328
Epoch 11/80: current_loss=20.36590 | best_loss=20.36590
Epoch 12/80: current_loss=20.27116 | best_loss=20.27116
Epoch 13/80: current_loss=20.17607 | best_loss=20.17607
Epoch 14/80: current_loss=20.08430 | best_loss=20.08430
Epoch 15/80: current_loss=19.99164 | best_loss=19.99164
Epoch 16/80: current_loss=19.90041 | best_loss=19.90041
Epoch 17/80: current_loss=19.81110 | best_loss=19.81110
Epoch 18/80: current_loss=19.71768 | best_loss=19.71768
Epoch 19/80: current_loss=19.62611 | best_loss=19.62611
Epoch 20/80: current_loss=19.53946 | best_loss=19.53946
Epoch 21/80: current_loss=19.45097 | best_loss=19.45097
Epoch 22/80: current_loss=19.36433 | best_loss=19.36433
Epoch 23/80: current_loss=19.27909 | best_loss=19.27909
Epoch 24/80: current_loss=19.19358 | best_loss=19.19358
Epoch 25/80: current_loss=19.11171 | best_loss=19.11171
Epoch 26/80: current_loss=19.02657 | best_loss=19.02657
Epoch 27/80: current_loss=18.94347 | best_loss=18.94347
Epoch 28/80: current_loss=18.86337 | best_loss=18.86337
Epoch 29/80: current_loss=18.78500 | best_loss=18.78500
Epoch 30/80: current_loss=18.70283 | best_loss=18.70283
Epoch 31/80: current_loss=18.62449 | best_loss=18.62449
Epoch 32/80: current_loss=18.54630 | best_loss=18.54630
Epoch 33/80: current_loss=18.46701 | best_loss=18.46701
Epoch 34/80: current_loss=18.39015 | best_loss=18.39015
Epoch 35/80: current_loss=18.31164 | best_loss=18.31164
Epoch 36/80: current_loss=18.23359 | best_loss=18.23359
Epoch 37/80: current_loss=18.15756 | best_loss=18.15756
Epoch 38/80: current_loss=18.08303 | best_loss=18.08303
Epoch 39/80: current_loss=18.00755 | best_loss=18.00755
Epoch 40/80: current_loss=17.93140 | best_loss=17.93140
Epoch 41/80: current_loss=17.85553 | best_loss=17.85553
Epoch 42/80: current_loss=17.78244 | best_loss=17.78244
Epoch 43/80: current_loss=17.71163 | best_loss=17.71163
Epoch 44/80: current_loss=17.63796 | best_loss=17.63796
Epoch 45/80: current_loss=17.56641 | best_loss=17.56641
Epoch 46/80: current_loss=17.49690 | best_loss=17.49690
Epoch 47/80: current_loss=17.42710 | best_loss=17.42710
Epoch 48/80: current_loss=17.35726 | best_loss=17.35726
Epoch 49/80: current_loss=17.28820 | best_loss=17.28820
Epoch 50/80: current_loss=17.21829 | best_loss=17.21829
Epoch 51/80: current_loss=17.14809 | best_loss=17.14809
Epoch 52/80: current_loss=17.08130 | best_loss=17.08130
Epoch 53/80: current_loss=17.01229 | best_loss=17.01229
Epoch 54/80: current_loss=16.94389 | best_loss=16.94389
Epoch 55/80: current_loss=16.87724 | best_loss=16.87724
Epoch 56/80: current_loss=16.80879 | best_loss=16.80879
Epoch 57/80: current_loss=16.74307 | best_loss=16.74307
Epoch 58/80: current_loss=16.67593 | best_loss=16.67593
Epoch 59/80: current_loss=16.61225 | best_loss=16.61225
Epoch 60/80: current_loss=16.54640 | best_loss=16.54640
Epoch 61/80: current_loss=16.48462 | best_loss=16.48462
Epoch 62/80: current_loss=16.41955 | best_loss=16.41955
Epoch 63/80: current_loss=16.35623 | best_loss=16.35623
Epoch 64/80: current_loss=16.29349 | best_loss=16.29349
Epoch 65/80: current_loss=16.23148 | best_loss=16.23148
Epoch 66/80: current_loss=16.17024 | best_loss=16.17024
Epoch 67/80: current_loss=16.11001 | best_loss=16.11001
Epoch 68/80: current_loss=16.04979 | best_loss=16.04979
Epoch 69/80: current_loss=15.99077 | best_loss=15.99077
Epoch 70/80: current_loss=15.93241 | best_loss=15.93241
Epoch 71/80: current_loss=15.87525 | best_loss=15.87525
Epoch 72/80: current_loss=15.81652 | best_loss=15.81652
Epoch 73/80: current_loss=15.75938 | best_loss=15.75938
Epoch 74/80: current_loss=15.70237 | best_loss=15.70237
Epoch 75/80: current_loss=15.64840 | best_loss=15.64840
Epoch 76/80: current_loss=15.59219 | best_loss=15.59219
Epoch 77/80: current_loss=15.53585 | best_loss=15.53585
Epoch 78/80: current_loss=15.48158 | best_loss=15.48158
Epoch 79/80: current_loss=15.42565 | best_loss=15.42565
      explained_var=-0.16985 | mse_loss=15.05463
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=15.55876 | best_loss=15.55876
Epoch 1/80: current_loss=15.49757 | best_loss=15.49757
Epoch 2/80: current_loss=15.43783 | best_loss=15.43783
Epoch 3/80: current_loss=15.37788 | best_loss=15.37788
Epoch 4/80: current_loss=15.31977 | best_loss=15.31977
Epoch 5/80: current_loss=15.26099 | best_loss=15.26099
Epoch 6/80: current_loss=15.20197 | best_loss=15.20197
Epoch 7/80: current_loss=15.14528 | best_loss=15.14528
Epoch 8/80: current_loss=15.08770 | best_loss=15.08770
Epoch 9/80: current_loss=15.03298 | best_loss=15.03298
Epoch 10/80: current_loss=14.97836 | best_loss=14.97836
Epoch 11/80: current_loss=14.92245 | best_loss=14.92245
Epoch 12/80: current_loss=14.87022 | best_loss=14.87022
Epoch 13/80: current_loss=14.81970 | best_loss=14.81970
Epoch 14/80: current_loss=14.76858 | best_loss=14.76858
Epoch 15/80: current_loss=14.71376 | best_loss=14.71376
Epoch 16/80: current_loss=14.66530 | best_loss=14.66530
Epoch 17/80: current_loss=14.61558 | best_loss=14.61558
Epoch 18/80: current_loss=14.56331 | best_loss=14.56331
Epoch 19/80: current_loss=14.51328 | best_loss=14.51328
Epoch 20/80: current_loss=14.46540 | best_loss=14.46540
Epoch 21/80: current_loss=14.41765 | best_loss=14.41765
Epoch 22/80: current_loss=14.37052 | best_loss=14.37052
Epoch 23/80: current_loss=14.32375 | best_loss=14.32375
Epoch 24/80: current_loss=14.27632 | best_loss=14.27632
Epoch 25/80: current_loss=14.22989 | best_loss=14.22989
Epoch 26/80: current_loss=14.18572 | best_loss=14.18572
Epoch 27/80: current_loss=14.13992 | best_loss=14.13992
Epoch 28/80: current_loss=14.09518 | best_loss=14.09518
Epoch 29/80: current_loss=14.05124 | best_loss=14.05124
Epoch 30/80: current_loss=14.00545 | best_loss=14.00545
Epoch 31/80: current_loss=13.96314 | best_loss=13.96314
Epoch 32/80: current_loss=13.92054 | best_loss=13.92054
Epoch 33/80: current_loss=13.87869 | best_loss=13.87869
Epoch 34/80: current_loss=13.84007 | best_loss=13.84007
Epoch 35/80: current_loss=13.79849 | best_loss=13.79849
Epoch 36/80: current_loss=13.75926 | best_loss=13.75926
Epoch 37/80: current_loss=13.71875 | best_loss=13.71875
Epoch 38/80: current_loss=13.67845 | best_loss=13.67845
Epoch 39/80: current_loss=13.63979 | best_loss=13.63979
Epoch 40/80: current_loss=13.59919 | best_loss=13.59919
Epoch 41/80: current_loss=13.56265 | best_loss=13.56265
Epoch 42/80: current_loss=13.52421 | best_loss=13.52421
Epoch 43/80: current_loss=13.48613 | best_loss=13.48613
Epoch 44/80: current_loss=13.45196 | best_loss=13.45196
Epoch 45/80: current_loss=13.41518 | best_loss=13.41518
Epoch 46/80: current_loss=13.38075 | best_loss=13.38075
Epoch 47/80: current_loss=13.34659 | best_loss=13.34659
Epoch 48/80: current_loss=13.31113 | best_loss=13.31113
Epoch 49/80: current_loss=13.27833 | best_loss=13.27833
Epoch 50/80: current_loss=13.24366 | best_loss=13.24366
Epoch 51/80: current_loss=13.21057 | best_loss=13.21057
Epoch 52/80: current_loss=13.17837 | best_loss=13.17837
Epoch 53/80: current_loss=13.14604 | best_loss=13.14604
Epoch 54/80: current_loss=13.11350 | best_loss=13.11350
Epoch 55/80: current_loss=13.08409 | best_loss=13.08409
Epoch 56/80: current_loss=13.05197 | best_loss=13.05197
Epoch 57/80: current_loss=13.02060 | best_loss=13.02060
Epoch 58/80: current_loss=12.99225 | best_loss=12.99225
Epoch 59/80: current_loss=12.96134 | best_loss=12.96134
Epoch 60/80: current_loss=12.93198 | best_loss=12.93198
Epoch 61/80: current_loss=12.90418 | best_loss=12.90418
Epoch 62/80: current_loss=12.87563 | best_loss=12.87563
Epoch 63/80: current_loss=12.84640 | best_loss=12.84640
Epoch 64/80: current_loss=12.81809 | best_loss=12.81809
Epoch 65/80: current_loss=12.79051 | best_loss=12.79051
Epoch 66/80: current_loss=12.76285 | best_loss=12.76285
Epoch 67/80: current_loss=12.73617 | best_loss=12.73617
Epoch 68/80: current_loss=12.70858 | best_loss=12.70858
Epoch 69/80: current_loss=12.68347 | best_loss=12.68347
Epoch 70/80: current_loss=12.65525 | best_loss=12.65525
Epoch 71/80: current_loss=12.63106 | best_loss=12.63106
Epoch 72/80: current_loss=12.60472 | best_loss=12.60472
Epoch 73/80: current_loss=12.57881 | best_loss=12.57881
Epoch 74/80: current_loss=12.55737 | best_loss=12.55737
Epoch 75/80: current_loss=12.53388 | best_loss=12.53388
Epoch 76/80: current_loss=12.50907 | best_loss=12.50907
Epoch 77/80: current_loss=12.48456 | best_loss=12.48456
Epoch 78/80: current_loss=12.46228 | best_loss=12.46228
Epoch 79/80: current_loss=12.43955 | best_loss=12.43955
      explained_var=-0.16637 | mse_loss=12.27593
----------------------------------------------
Average early_stopping_point: 80| avg_exp_var=-0.13994| avg_loss=27.42505
----------------------------------------------


----------------------------------------------
Params for Trial 9
{'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=80.95298 | best_loss=80.95298
Epoch 1/80: current_loss=48.23496 | best_loss=48.23496
Epoch 2/80: current_loss=61.31931 | best_loss=48.23496
Epoch 3/80: current_loss=42.70767 | best_loss=42.70767
Epoch 4/80: current_loss=24.11578 | best_loss=24.11578
Epoch 5/80: current_loss=28.88046 | best_loss=24.11578
Epoch 6/80: current_loss=20.71557 | best_loss=20.71557
Epoch 7/80: current_loss=11.98635 | best_loss=11.98635
Epoch 8/80: current_loss=12.55823 | best_loss=11.98635
Epoch 9/80: current_loss=15.09004 | best_loss=11.98635
Epoch 10/80: current_loss=12.40107 | best_loss=11.98635
Epoch 11/80: current_loss=15.84060 | best_loss=11.98635
Epoch 12/80: current_loss=11.66447 | best_loss=11.66447
Epoch 13/80: current_loss=12.19844 | best_loss=11.66447
Epoch 14/80: current_loss=11.74326 | best_loss=11.66447
Epoch 15/80: current_loss=14.03661 | best_loss=11.66447
Epoch 16/80: current_loss=15.01363 | best_loss=11.66447
Epoch 17/80: current_loss=10.74725 | best_loss=10.74725
Epoch 18/80: current_loss=12.25847 | best_loss=10.74725
Epoch 19/80: current_loss=15.46811 | best_loss=10.74725
Epoch 20/80: current_loss=16.15287 | best_loss=10.74725
Epoch 21/80: current_loss=14.00383 | best_loss=10.74725
Epoch 22/80: current_loss=11.46795 | best_loss=10.74725
Epoch 23/80: current_loss=12.76059 | best_loss=10.74725
Epoch 24/80: current_loss=16.38356 | best_loss=10.74725
Epoch 25/80: current_loss=11.82836 | best_loss=10.74725
Epoch 26/80: current_loss=25.91201 | best_loss=10.74725
Epoch 27/80: current_loss=10.49439 | best_loss=10.49439
Epoch 28/80: current_loss=19.44303 | best_loss=10.49439
Epoch 29/80: current_loss=18.75132 | best_loss=10.49439
Epoch 30/80: current_loss=11.93590 | best_loss=10.49439
Epoch 31/80: current_loss=16.43572 | best_loss=10.49439
Epoch 32/80: current_loss=19.72829 | best_loss=10.49439
Epoch 33/80: current_loss=15.02527 | best_loss=10.49439
Epoch 34/80: current_loss=11.69295 | best_loss=10.49439
Epoch 35/80: current_loss=16.16392 | best_loss=10.49439
Epoch 36/80: current_loss=15.14283 | best_loss=10.49439
Epoch 37/80: current_loss=18.07858 | best_loss=10.49439
Epoch 38/80: current_loss=14.76531 | best_loss=10.49439
Epoch 39/80: current_loss=11.32006 | best_loss=10.49439
Epoch 40/80: current_loss=12.92905 | best_loss=10.49439
Epoch 41/80: current_loss=17.78228 | best_loss=10.49439
Epoch 42/80: current_loss=14.18752 | best_loss=10.49439
Epoch 43/80: current_loss=12.06375 | best_loss=10.49439
Epoch 44/80: current_loss=12.33805 | best_loss=10.49439
Epoch 45/80: current_loss=18.37726 | best_loss=10.49439
Epoch 46/80: current_loss=13.36268 | best_loss=10.49439
Epoch 47/80: current_loss=11.35770 | best_loss=10.49439
Early Stopping at epoch 47
      explained_var=-0.02527 | mse_loss=10.78026
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=19.39604 | best_loss=19.39604
Epoch 1/80: current_loss=20.64403 | best_loss=19.39604
Epoch 2/80: current_loss=44.55682 | best_loss=19.39604
Epoch 3/80: current_loss=30.93498 | best_loss=19.39604
Epoch 4/80: current_loss=10.85296 | best_loss=10.85296
Epoch 5/80: current_loss=12.34174 | best_loss=10.85296
Epoch 6/80: current_loss=10.76474 | best_loss=10.76474
Epoch 7/80: current_loss=53.71724 | best_loss=10.76474
Epoch 8/80: current_loss=10.47977 | best_loss=10.47977
Epoch 9/80: current_loss=10.93667 | best_loss=10.47977
Epoch 10/80: current_loss=10.77878 | best_loss=10.47977
Epoch 11/80: current_loss=19.83181 | best_loss=10.47977
Epoch 12/80: current_loss=11.41772 | best_loss=10.47977
Epoch 13/80: current_loss=10.82110 | best_loss=10.47977
Epoch 14/80: current_loss=10.34834 | best_loss=10.34834
Epoch 15/80: current_loss=23.62967 | best_loss=10.34834
Epoch 16/80: current_loss=11.63579 | best_loss=10.34834
Epoch 17/80: current_loss=16.17326 | best_loss=10.34834
Epoch 18/80: current_loss=32.81119 | best_loss=10.34834
Epoch 19/80: current_loss=11.79429 | best_loss=10.34834
Epoch 20/80: current_loss=22.85504 | best_loss=10.34834
Epoch 21/80: current_loss=11.98676 | best_loss=10.34834
Epoch 22/80: current_loss=12.02457 | best_loss=10.34834
Epoch 23/80: current_loss=13.64326 | best_loss=10.34834
Epoch 24/80: current_loss=10.61525 | best_loss=10.34834
Epoch 25/80: current_loss=11.94426 | best_loss=10.34834
Epoch 26/80: current_loss=27.59597 | best_loss=10.34834
Epoch 27/80: current_loss=20.74607 | best_loss=10.34834
Epoch 28/80: current_loss=18.51473 | best_loss=10.34834
Epoch 29/80: current_loss=12.36243 | best_loss=10.34834
Epoch 30/80: current_loss=16.18918 | best_loss=10.34834
Epoch 31/80: current_loss=21.31689 | best_loss=10.34834
Epoch 32/80: current_loss=38.93086 | best_loss=10.34834
Epoch 33/80: current_loss=12.99929 | best_loss=10.34834
Epoch 34/80: current_loss=11.42075 | best_loss=10.34834
Early Stopping at epoch 34
      explained_var=0.00326 | mse_loss=10.05383
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.84458 | best_loss=7.84458
Epoch 1/80: current_loss=16.73997 | best_loss=7.84458
Epoch 2/80: current_loss=11.15800 | best_loss=7.84458
Epoch 3/80: current_loss=12.93261 | best_loss=7.84458
Epoch 4/80: current_loss=14.99010 | best_loss=7.84458
Epoch 5/80: current_loss=8.91176 | best_loss=7.84458
Epoch 6/80: current_loss=19.16295 | best_loss=7.84458
Epoch 7/80: current_loss=10.26726 | best_loss=7.84458
Epoch 8/80: current_loss=22.63734 | best_loss=7.84458
Epoch 9/80: current_loss=12.97520 | best_loss=7.84458
Epoch 10/80: current_loss=8.45935 | best_loss=7.84458
Epoch 11/80: current_loss=7.77885 | best_loss=7.77885
Epoch 12/80: current_loss=21.72092 | best_loss=7.77885
Epoch 13/80: current_loss=9.44496 | best_loss=7.77885
Epoch 14/80: current_loss=12.78369 | best_loss=7.77885
Epoch 15/80: current_loss=30.26348 | best_loss=7.77885
Epoch 16/80: current_loss=82.12170 | best_loss=7.77885
Epoch 17/80: current_loss=14.96543 | best_loss=7.77885
Epoch 18/80: current_loss=9.24093 | best_loss=7.77885
Epoch 19/80: current_loss=27.54874 | best_loss=7.77885
Epoch 20/80: current_loss=8.60407 | best_loss=7.77885
Epoch 21/80: current_loss=7.64732 | best_loss=7.64732
Epoch 22/80: current_loss=7.52975 | best_loss=7.52975
Epoch 23/80: current_loss=10.17491 | best_loss=7.52975
Epoch 24/80: current_loss=8.06547 | best_loss=7.52975
Epoch 25/80: current_loss=8.12023 | best_loss=7.52975
Epoch 26/80: current_loss=7.62459 | best_loss=7.52975
Epoch 27/80: current_loss=9.00870 | best_loss=7.52975
Epoch 28/80: current_loss=68.74356 | best_loss=7.52975
Epoch 29/80: current_loss=11.97750 | best_loss=7.52975
Epoch 30/80: current_loss=7.59657 | best_loss=7.52975
Epoch 31/80: current_loss=7.54068 | best_loss=7.52975
Epoch 32/80: current_loss=8.26471 | best_loss=7.52975
Epoch 33/80: current_loss=8.23931 | best_loss=7.52975
Epoch 34/80: current_loss=7.84623 | best_loss=7.52975
Epoch 35/80: current_loss=7.50533 | best_loss=7.50533
Epoch 36/80: current_loss=12.93878 | best_loss=7.50533
Epoch 37/80: current_loss=8.34418 | best_loss=7.50533
Epoch 38/80: current_loss=11.09928 | best_loss=7.50533
Epoch 39/80: current_loss=7.46812 | best_loss=7.46812
Epoch 40/80: current_loss=8.07024 | best_loss=7.46812
Epoch 41/80: current_loss=8.44922 | best_loss=7.46812
Epoch 42/80: current_loss=8.05924 | best_loss=7.46812
Epoch 43/80: current_loss=9.12868 | best_loss=7.46812
Epoch 44/80: current_loss=10.83485 | best_loss=7.46812
Epoch 45/80: current_loss=15.35012 | best_loss=7.46812
Epoch 46/80: current_loss=32.10505 | best_loss=7.46812
Epoch 47/80: current_loss=8.56718 | best_loss=7.46812
Epoch 48/80: current_loss=7.84791 | best_loss=7.46812
Epoch 49/80: current_loss=12.91241 | best_loss=7.46812
Epoch 50/80: current_loss=20.80870 | best_loss=7.46812
Epoch 51/80: current_loss=29.75853 | best_loss=7.46812
Epoch 52/80: current_loss=8.57093 | best_loss=7.46812
Epoch 53/80: current_loss=10.55557 | best_loss=7.46812
Epoch 54/80: current_loss=13.63973 | best_loss=7.46812
Epoch 55/80: current_loss=8.99262 | best_loss=7.46812
Epoch 56/80: current_loss=8.31088 | best_loss=7.46812
Epoch 57/80: current_loss=44.49368 | best_loss=7.46812
Epoch 58/80: current_loss=7.67621 | best_loss=7.46812
Epoch 59/80: current_loss=10.28298 | best_loss=7.46812
Early Stopping at epoch 59
      explained_var=0.00093 | mse_loss=7.57601
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=26.99235 | best_loss=26.99235
Epoch 1/80: current_loss=10.60782 | best_loss=10.60782
Epoch 2/80: current_loss=14.48266 | best_loss=10.60782
Epoch 3/80: current_loss=11.48934 | best_loss=10.60782
Epoch 4/80: current_loss=15.72771 | best_loss=10.60782
Epoch 5/80: current_loss=13.96156 | best_loss=10.60782
Epoch 6/80: current_loss=9.02087 | best_loss=9.02087
Epoch 7/80: current_loss=17.39193 | best_loss=9.02087
Epoch 8/80: current_loss=14.12263 | best_loss=9.02087
Epoch 9/80: current_loss=16.70055 | best_loss=9.02087
Epoch 10/80: current_loss=13.35540 | best_loss=9.02087
Epoch 11/80: current_loss=10.65052 | best_loss=9.02087
Epoch 12/80: current_loss=8.84030 | best_loss=8.84030
Epoch 13/80: current_loss=35.50801 | best_loss=8.84030
Epoch 14/80: current_loss=17.51269 | best_loss=8.84030
Epoch 15/80: current_loss=25.81263 | best_loss=8.84030
Epoch 16/80: current_loss=20.18668 | best_loss=8.84030
Epoch 17/80: current_loss=19.26642 | best_loss=8.84030
Epoch 18/80: current_loss=8.91987 | best_loss=8.84030
Epoch 19/80: current_loss=17.35892 | best_loss=8.84030
Epoch 20/80: current_loss=10.51461 | best_loss=8.84030
Epoch 21/80: current_loss=29.93084 | best_loss=8.84030
Epoch 22/80: current_loss=14.70166 | best_loss=8.84030
Epoch 23/80: current_loss=11.02493 | best_loss=8.84030
Epoch 24/80: current_loss=8.87672 | best_loss=8.84030
Epoch 25/80: current_loss=41.77748 | best_loss=8.84030
Epoch 26/80: current_loss=9.57537 | best_loss=8.84030
Epoch 27/80: current_loss=8.79055 | best_loss=8.79055
Epoch 28/80: current_loss=8.87641 | best_loss=8.79055
Epoch 29/80: current_loss=13.99469 | best_loss=8.79055
Epoch 30/80: current_loss=47.84521 | best_loss=8.79055
Epoch 31/80: current_loss=15.83508 | best_loss=8.79055
Epoch 32/80: current_loss=10.02569 | best_loss=8.79055
Epoch 33/80: current_loss=9.17712 | best_loss=8.79055
Epoch 34/80: current_loss=19.98216 | best_loss=8.79055
Epoch 35/80: current_loss=10.33662 | best_loss=8.79055
Epoch 36/80: current_loss=8.97401 | best_loss=8.79055
Epoch 37/80: current_loss=16.25203 | best_loss=8.79055
Epoch 38/80: current_loss=12.64112 | best_loss=8.79055
Epoch 39/80: current_loss=11.59878 | best_loss=8.79055
Epoch 40/80: current_loss=10.24321 | best_loss=8.79055
Epoch 41/80: current_loss=25.60673 | best_loss=8.79055
Epoch 42/80: current_loss=9.63064 | best_loss=8.79055
Epoch 43/80: current_loss=11.15944 | best_loss=8.79055
Epoch 44/80: current_loss=12.42689 | best_loss=8.79055
Epoch 45/80: current_loss=31.59941 | best_loss=8.79055
Epoch 46/80: current_loss=18.57568 | best_loss=8.79055
Epoch 47/80: current_loss=9.39979 | best_loss=8.79055
Early Stopping at epoch 47
      explained_var=-0.00562 | mse_loss=8.76567
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=19.25494 | best_loss=19.25494
Epoch 1/80: current_loss=20.29366 | best_loss=19.25494
Epoch 2/80: current_loss=19.88109 | best_loss=19.25494
Epoch 3/80: current_loss=10.53199 | best_loss=10.53199
Epoch 4/80: current_loss=10.95396 | best_loss=10.53199
Epoch 5/80: current_loss=13.55707 | best_loss=10.53199
Epoch 6/80: current_loss=12.13108 | best_loss=10.53199
Epoch 7/80: current_loss=9.44131 | best_loss=9.44131
Epoch 8/80: current_loss=9.68418 | best_loss=9.44131
Epoch 9/80: current_loss=10.15230 | best_loss=9.44131
Epoch 10/80: current_loss=14.06950 | best_loss=9.44131
Epoch 11/80: current_loss=14.28671 | best_loss=9.44131
Epoch 12/80: current_loss=9.60561 | best_loss=9.44131
Epoch 13/80: current_loss=13.41169 | best_loss=9.44131
Epoch 14/80: current_loss=10.24726 | best_loss=9.44131
Epoch 15/80: current_loss=10.78827 | best_loss=9.44131
Epoch 16/80: current_loss=15.36079 | best_loss=9.44131
Epoch 17/80: current_loss=9.85203 | best_loss=9.44131
Epoch 18/80: current_loss=11.73453 | best_loss=9.44131
Epoch 19/80: current_loss=12.45515 | best_loss=9.44131
Epoch 20/80: current_loss=17.50852 | best_loss=9.44131
Epoch 21/80: current_loss=12.76153 | best_loss=9.44131
Epoch 22/80: current_loss=10.43647 | best_loss=9.44131
Epoch 23/80: current_loss=10.47943 | best_loss=9.44131
Epoch 24/80: current_loss=9.97415 | best_loss=9.44131
Epoch 25/80: current_loss=18.03001 | best_loss=9.44131
Epoch 26/80: current_loss=11.97236 | best_loss=9.44131
Epoch 27/80: current_loss=13.44550 | best_loss=9.44131
Early Stopping at epoch 27
      explained_var=-0.00394 | mse_loss=9.32977
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=-0.00613| avg_loss=9.30111
----------------------------------------------


----------------------------------------------
Params for Trial 10
{'learning_rate': 0.01, 'weight_decay': 0.003637276799843544, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=11.05599 | best_loss=11.05599
Epoch 1/80: current_loss=10.25327 | best_loss=10.25327
Epoch 2/80: current_loss=10.50105 | best_loss=10.25327
Epoch 3/80: current_loss=10.31543 | best_loss=10.25327
Epoch 4/80: current_loss=11.01200 | best_loss=10.25327
Epoch 5/80: current_loss=10.76985 | best_loss=10.25327
Epoch 6/80: current_loss=10.31896 | best_loss=10.25327
Epoch 7/80: current_loss=10.85606 | best_loss=10.25327
Epoch 8/80: current_loss=10.34255 | best_loss=10.25327
Epoch 9/80: current_loss=10.25393 | best_loss=10.25327
Epoch 10/80: current_loss=10.40988 | best_loss=10.25327
Epoch 11/80: current_loss=10.50080 | best_loss=10.25327
Epoch 12/80: current_loss=10.93820 | best_loss=10.25327
Epoch 13/80: current_loss=10.39807 | best_loss=10.25327
Epoch 14/80: current_loss=10.38096 | best_loss=10.25327
Epoch 15/80: current_loss=10.40620 | best_loss=10.25327
Epoch 16/80: current_loss=10.26155 | best_loss=10.25327
Epoch 17/80: current_loss=10.39840 | best_loss=10.25327
Epoch 18/80: current_loss=10.29209 | best_loss=10.25327
Epoch 19/80: current_loss=10.85795 | best_loss=10.25327
Epoch 20/80: current_loss=10.29125 | best_loss=10.25327
Epoch 21/80: current_loss=10.28300 | best_loss=10.25327
Early Stopping at epoch 21
      explained_var=-0.00001 | mse_loss=10.47396
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.82677 | best_loss=10.82677
Epoch 1/80: current_loss=10.41414 | best_loss=10.41414
Epoch 2/80: current_loss=10.71934 | best_loss=10.41414
Epoch 3/80: current_loss=11.02086 | best_loss=10.41414
Epoch 4/80: current_loss=10.81750 | best_loss=10.41414
Epoch 5/80: current_loss=10.40415 | best_loss=10.40415
Epoch 6/80: current_loss=10.40533 | best_loss=10.40415
Epoch 7/80: current_loss=10.85950 | best_loss=10.40415
Epoch 8/80: current_loss=10.70935 | best_loss=10.40415
Epoch 9/80: current_loss=10.42696 | best_loss=10.40415
Epoch 10/80: current_loss=10.97120 | best_loss=10.40415
Epoch 11/80: current_loss=11.07047 | best_loss=10.40415
Epoch 12/80: current_loss=10.75073 | best_loss=10.40415
Epoch 13/80: current_loss=10.41059 | best_loss=10.40415
Epoch 14/80: current_loss=10.53982 | best_loss=10.40415
Epoch 15/80: current_loss=10.75192 | best_loss=10.40415
Epoch 16/80: current_loss=10.51495 | best_loss=10.40415
Epoch 17/80: current_loss=11.43310 | best_loss=10.40415
Epoch 18/80: current_loss=11.57738 | best_loss=10.40415
Epoch 19/80: current_loss=10.49219 | best_loss=10.40415
Epoch 20/80: current_loss=11.39643 | best_loss=10.40415
Epoch 21/80: current_loss=10.52642 | best_loss=10.40415
Epoch 22/80: current_loss=10.43889 | best_loss=10.40415
Epoch 23/80: current_loss=10.79336 | best_loss=10.40415
Epoch 24/80: current_loss=10.40914 | best_loss=10.40415
Epoch 25/80: current_loss=10.57115 | best_loss=10.40415
Early Stopping at epoch 25
      explained_var=0.00027 | mse_loss=10.06126
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.08446 | best_loss=8.08446
Epoch 1/80: current_loss=7.46976 | best_loss=7.46976
Epoch 2/80: current_loss=7.78041 | best_loss=7.46976
Epoch 3/80: current_loss=8.66569 | best_loss=7.46976
Epoch 4/80: current_loss=7.86812 | best_loss=7.46976
Epoch 5/80: current_loss=7.93996 | best_loss=7.46976
Epoch 6/80: current_loss=7.93803 | best_loss=7.46976
Epoch 7/80: current_loss=7.76277 | best_loss=7.46976
Epoch 8/80: current_loss=8.11970 | best_loss=7.46976
Epoch 9/80: current_loss=7.47704 | best_loss=7.46976
Epoch 10/80: current_loss=7.57996 | best_loss=7.46976
Epoch 11/80: current_loss=7.52919 | best_loss=7.46976
Epoch 12/80: current_loss=7.48991 | best_loss=7.46976
Epoch 13/80: current_loss=7.48696 | best_loss=7.46976
Epoch 14/80: current_loss=7.64622 | best_loss=7.46976
Epoch 15/80: current_loss=7.51225 | best_loss=7.46976
Epoch 16/80: current_loss=7.50089 | best_loss=7.46976
Epoch 17/80: current_loss=7.49069 | best_loss=7.46976
Epoch 18/80: current_loss=8.80676 | best_loss=7.46976
Epoch 19/80: current_loss=7.82813 | best_loss=7.46976
Epoch 20/80: current_loss=8.24816 | best_loss=7.46976
Epoch 21/80: current_loss=7.93696 | best_loss=7.46976
Early Stopping at epoch 21
      explained_var=0.00105 | mse_loss=7.57558
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.79001 | best_loss=8.79001
Epoch 1/80: current_loss=8.93283 | best_loss=8.79001
Epoch 2/80: current_loss=9.15726 | best_loss=8.79001
Epoch 3/80: current_loss=10.77963 | best_loss=8.79001
Epoch 4/80: current_loss=8.90920 | best_loss=8.79001
Epoch 5/80: current_loss=8.84656 | best_loss=8.79001
Epoch 6/80: current_loss=8.88162 | best_loss=8.79001
Epoch 7/80: current_loss=8.93936 | best_loss=8.79001
Epoch 8/80: current_loss=8.86820 | best_loss=8.79001
Epoch 9/80: current_loss=8.89295 | best_loss=8.79001
Epoch 10/80: current_loss=8.95503 | best_loss=8.79001
Epoch 11/80: current_loss=8.90832 | best_loss=8.79001
Epoch 12/80: current_loss=8.89053 | best_loss=8.79001
Epoch 13/80: current_loss=8.90179 | best_loss=8.79001
Epoch 14/80: current_loss=9.32560 | best_loss=8.79001
Epoch 15/80: current_loss=8.88269 | best_loss=8.79001
Epoch 16/80: current_loss=8.81220 | best_loss=8.79001
Epoch 17/80: current_loss=8.88674 | best_loss=8.79001
Epoch 18/80: current_loss=8.82105 | best_loss=8.79001
Epoch 19/80: current_loss=10.13123 | best_loss=8.79001
Epoch 20/80: current_loss=9.37681 | best_loss=8.79001
Early Stopping at epoch 20
      explained_var=0.00021 | mse_loss=8.72001
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.53249 | best_loss=9.53249
Epoch 1/80: current_loss=9.56208 | best_loss=9.53249
Epoch 2/80: current_loss=9.41025 | best_loss=9.41025
Epoch 3/80: current_loss=10.01853 | best_loss=9.41025
Epoch 4/80: current_loss=9.57884 | best_loss=9.41025
Epoch 5/80: current_loss=9.53687 | best_loss=9.41025
Epoch 6/80: current_loss=9.38386 | best_loss=9.38386
Epoch 7/80: current_loss=9.70688 | best_loss=9.38386
Epoch 8/80: current_loss=9.46395 | best_loss=9.38386
Epoch 9/80: current_loss=9.70273 | best_loss=9.38386
Epoch 10/80: current_loss=9.42246 | best_loss=9.38386
Epoch 11/80: current_loss=9.59871 | best_loss=9.38386
Epoch 12/80: current_loss=9.41165 | best_loss=9.38386
Epoch 13/80: current_loss=9.38707 | best_loss=9.38386
Epoch 14/80: current_loss=9.38999 | best_loss=9.38386
Epoch 15/80: current_loss=9.47683 | best_loss=9.38386
Epoch 16/80: current_loss=9.66979 | best_loss=9.38386
Epoch 17/80: current_loss=10.95315 | best_loss=9.38386
Epoch 18/80: current_loss=9.38333 | best_loss=9.38333
Epoch 19/80: current_loss=10.33307 | best_loss=9.38333
Epoch 20/80: current_loss=9.68709 | best_loss=9.38333
Epoch 21/80: current_loss=9.38128 | best_loss=9.38128
Epoch 22/80: current_loss=9.53901 | best_loss=9.38128
Epoch 23/80: current_loss=9.38554 | best_loss=9.38128
Epoch 24/80: current_loss=9.45953 | best_loss=9.38128
Epoch 25/80: current_loss=9.47617 | best_loss=9.38128
Epoch 26/80: current_loss=9.42780 | best_loss=9.38128
Epoch 27/80: current_loss=9.38622 | best_loss=9.38128
Epoch 28/80: current_loss=9.60011 | best_loss=9.38128
Epoch 29/80: current_loss=9.39608 | best_loss=9.38128
Epoch 30/80: current_loss=9.54330 | best_loss=9.38128
Epoch 31/80: current_loss=9.42666 | best_loss=9.38128
Epoch 32/80: current_loss=9.85504 | best_loss=9.38128
Epoch 33/80: current_loss=9.55259 | best_loss=9.38128
Epoch 34/80: current_loss=9.42870 | best_loss=9.38128
Epoch 35/80: current_loss=9.90674 | best_loss=9.38128
Epoch 36/80: current_loss=9.49025 | best_loss=9.38128
Epoch 37/80: current_loss=9.38468 | best_loss=9.38128
Epoch 38/80: current_loss=9.38924 | best_loss=9.38128
Epoch 39/80: current_loss=9.43216 | best_loss=9.38128
Epoch 40/80: current_loss=9.50975 | best_loss=9.38128
Epoch 41/80: current_loss=9.38181 | best_loss=9.38128
Early Stopping at epoch 41
      explained_var=-0.00093 | mse_loss=9.30209
----------------------------------------------
Average early_stopping_point: 5| avg_exp_var=0.00012| avg_loss=9.22658
----------------------------------------------


----------------------------------------------
Params for Trial 11
{'learning_rate': 0.001, 'weight_decay': 0.009747616659132213, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.41335 | best_loss=16.41335
Epoch 1/80: current_loss=10.71652 | best_loss=10.71652
Epoch 2/80: current_loss=10.47857 | best_loss=10.47857
Epoch 3/80: current_loss=10.43199 | best_loss=10.43199
Epoch 4/80: current_loss=10.38761 | best_loss=10.38761
Epoch 5/80: current_loss=10.36897 | best_loss=10.36897
Epoch 6/80: current_loss=10.33254 | best_loss=10.33254
Epoch 7/80: current_loss=10.31713 | best_loss=10.31713
Epoch 8/80: current_loss=10.32452 | best_loss=10.31713
Epoch 9/80: current_loss=10.31072 | best_loss=10.31072
Epoch 10/80: current_loss=10.29597 | best_loss=10.29597
Epoch 11/80: current_loss=10.32952 | best_loss=10.29597
Epoch 12/80: current_loss=10.31554 | best_loss=10.29597
Epoch 13/80: current_loss=10.29645 | best_loss=10.29597
Epoch 14/80: current_loss=10.31032 | best_loss=10.29597
Epoch 15/80: current_loss=10.30090 | best_loss=10.29597
Epoch 16/80: current_loss=10.31019 | best_loss=10.29597
Epoch 17/80: current_loss=10.30134 | best_loss=10.29597
Epoch 18/80: current_loss=10.34818 | best_loss=10.29597
Epoch 19/80: current_loss=10.32108 | best_loss=10.29597
Epoch 20/80: current_loss=10.30376 | best_loss=10.29597
Epoch 21/80: current_loss=10.31503 | best_loss=10.29597
Epoch 22/80: current_loss=10.31512 | best_loss=10.29597
Epoch 23/80: current_loss=10.29467 | best_loss=10.29467
Epoch 24/80: current_loss=10.30358 | best_loss=10.29467
Epoch 25/80: current_loss=10.29820 | best_loss=10.29467
Epoch 26/80: current_loss=10.29728 | best_loss=10.29467
Epoch 27/80: current_loss=10.28950 | best_loss=10.28950
Epoch 28/80: current_loss=10.29045 | best_loss=10.28950
Epoch 29/80: current_loss=10.33147 | best_loss=10.28950
Epoch 30/80: current_loss=10.29436 | best_loss=10.28950
Epoch 31/80: current_loss=10.29525 | best_loss=10.28950
Epoch 32/80: current_loss=10.29509 | best_loss=10.28950
Epoch 33/80: current_loss=10.28917 | best_loss=10.28917
Epoch 34/80: current_loss=10.29001 | best_loss=10.28917
Epoch 35/80: current_loss=10.30079 | best_loss=10.28917
Epoch 36/80: current_loss=10.31331 | best_loss=10.28917
Epoch 37/80: current_loss=10.31222 | best_loss=10.28917
Epoch 38/80: current_loss=10.31639 | best_loss=10.28917
Epoch 39/80: current_loss=10.27834 | best_loss=10.27834
Epoch 40/80: current_loss=10.33345 | best_loss=10.27834
Epoch 41/80: current_loss=10.32237 | best_loss=10.27834
Epoch 42/80: current_loss=10.31511 | best_loss=10.27834
Epoch 43/80: current_loss=10.30438 | best_loss=10.27834
Epoch 44/80: current_loss=10.30478 | best_loss=10.27834
Epoch 45/80: current_loss=10.33781 | best_loss=10.27834
Epoch 46/80: current_loss=10.28567 | best_loss=10.27834
Epoch 47/80: current_loss=10.28473 | best_loss=10.27834
Epoch 48/80: current_loss=10.28905 | best_loss=10.27834
Epoch 49/80: current_loss=10.29144 | best_loss=10.27834
Epoch 50/80: current_loss=10.28585 | best_loss=10.27834
Epoch 51/80: current_loss=10.26603 | best_loss=10.26603
Epoch 52/80: current_loss=10.33534 | best_loss=10.26603
Epoch 53/80: current_loss=10.26449 | best_loss=10.26449
Epoch 54/80: current_loss=10.26682 | best_loss=10.26449
Epoch 55/80: current_loss=10.15035 | best_loss=10.15035
Epoch 56/80: current_loss=10.23477 | best_loss=10.15035
Epoch 57/80: current_loss=10.29926 | best_loss=10.15035
Epoch 58/80: current_loss=10.27859 | best_loss=10.15035
Epoch 59/80: current_loss=10.34520 | best_loss=10.15035
Epoch 60/80: current_loss=10.27592 | best_loss=10.15035
Epoch 61/80: current_loss=10.24822 | best_loss=10.15035
Epoch 62/80: current_loss=10.59796 | best_loss=10.15035
Epoch 63/80: current_loss=10.27542 | best_loss=10.15035
Epoch 64/80: current_loss=10.25966 | best_loss=10.15035
Epoch 65/80: current_loss=10.19792 | best_loss=10.15035
Epoch 66/80: current_loss=10.02016 | best_loss=10.02016
Epoch 67/80: current_loss=10.06776 | best_loss=10.02016
Epoch 68/80: current_loss=10.21873 | best_loss=10.02016
Epoch 69/80: current_loss=10.29774 | best_loss=10.02016
Epoch 70/80: current_loss=10.25868 | best_loss=10.02016
Epoch 71/80: current_loss=10.43318 | best_loss=10.02016
Epoch 72/80: current_loss=10.27560 | best_loss=10.02016
Epoch 73/80: current_loss=10.26181 | best_loss=10.02016
Epoch 74/80: current_loss=10.16628 | best_loss=10.02016
Epoch 75/80: current_loss=10.18630 | best_loss=10.02016
Epoch 76/80: current_loss=10.14712 | best_loss=10.02016
Epoch 77/80: current_loss=10.23950 | best_loss=10.02016
Epoch 78/80: current_loss=10.22208 | best_loss=10.02016
Epoch 79/80: current_loss=10.12826 | best_loss=10.02016
      explained_var=0.02322 | mse_loss=10.27149
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.24366 | best_loss=10.24366
Epoch 1/80: current_loss=10.32158 | best_loss=10.24366
Epoch 2/80: current_loss=10.35996 | best_loss=10.24366
Epoch 3/80: current_loss=10.24510 | best_loss=10.24366
Epoch 4/80: current_loss=10.31033 | best_loss=10.24366
Epoch 5/80: current_loss=10.25463 | best_loss=10.24366
Epoch 6/80: current_loss=10.31572 | best_loss=10.24366
Epoch 7/80: current_loss=10.28969 | best_loss=10.24366
Epoch 8/80: current_loss=10.21588 | best_loss=10.21588
Epoch 9/80: current_loss=10.26424 | best_loss=10.21588
Epoch 10/80: current_loss=10.28272 | best_loss=10.21588
Epoch 11/80: current_loss=10.22772 | best_loss=10.21588
Epoch 12/80: current_loss=10.31211 | best_loss=10.21588
Epoch 13/80: current_loss=10.31973 | best_loss=10.21588
Epoch 14/80: current_loss=10.38061 | best_loss=10.21588
Epoch 15/80: current_loss=10.34799 | best_loss=10.21588
Epoch 16/80: current_loss=10.19792 | best_loss=10.19792
Epoch 17/80: current_loss=10.31001 | best_loss=10.19792
Epoch 18/80: current_loss=10.27454 | best_loss=10.19792
Epoch 19/80: current_loss=10.31524 | best_loss=10.19792
Epoch 20/80: current_loss=10.38747 | best_loss=10.19792
Epoch 21/80: current_loss=10.32143 | best_loss=10.19792
Epoch 22/80: current_loss=10.22079 | best_loss=10.19792
Epoch 23/80: current_loss=10.23279 | best_loss=10.19792
Epoch 24/80: current_loss=10.41751 | best_loss=10.19792
Epoch 25/80: current_loss=10.34741 | best_loss=10.19792
Epoch 26/80: current_loss=10.19953 | best_loss=10.19792
Epoch 27/80: current_loss=10.42821 | best_loss=10.19792
Epoch 28/80: current_loss=10.50083 | best_loss=10.19792
Epoch 29/80: current_loss=10.40072 | best_loss=10.19792
Epoch 30/80: current_loss=10.51388 | best_loss=10.19792
Epoch 31/80: current_loss=10.41018 | best_loss=10.19792
Epoch 32/80: current_loss=10.40112 | best_loss=10.19792
Epoch 33/80: current_loss=10.40558 | best_loss=10.19792
Epoch 34/80: current_loss=10.43590 | best_loss=10.19792
Epoch 35/80: current_loss=10.35359 | best_loss=10.19792
Epoch 36/80: current_loss=10.35442 | best_loss=10.19792
Early Stopping at epoch 36
      explained_var=0.02232 | mse_loss=9.84272
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.75506 | best_loss=7.75506
Epoch 1/80: current_loss=7.56792 | best_loss=7.56792
Epoch 2/80: current_loss=7.53062 | best_loss=7.53062
Epoch 3/80: current_loss=7.56612 | best_loss=7.53062
Epoch 4/80: current_loss=7.53634 | best_loss=7.53062
Epoch 5/80: current_loss=7.56102 | best_loss=7.53062
Epoch 6/80: current_loss=7.85250 | best_loss=7.53062
Epoch 7/80: current_loss=7.60742 | best_loss=7.53062
Epoch 8/80: current_loss=7.60208 | best_loss=7.53062
Epoch 9/80: current_loss=7.68101 | best_loss=7.53062
Epoch 10/80: current_loss=7.51052 | best_loss=7.51052
Epoch 11/80: current_loss=7.53797 | best_loss=7.51052
Epoch 12/80: current_loss=7.51319 | best_loss=7.51052
Epoch 13/80: current_loss=7.63566 | best_loss=7.51052
Epoch 14/80: current_loss=7.57524 | best_loss=7.51052
Epoch 15/80: current_loss=7.66559 | best_loss=7.51052
Epoch 16/80: current_loss=7.57849 | best_loss=7.51052
Epoch 17/80: current_loss=7.76377 | best_loss=7.51052
Epoch 18/80: current_loss=7.61512 | best_loss=7.51052
Epoch 19/80: current_loss=7.62551 | best_loss=7.51052
Epoch 20/80: current_loss=7.51590 | best_loss=7.51052
Epoch 21/80: current_loss=7.99283 | best_loss=7.51052
Epoch 22/80: current_loss=7.54936 | best_loss=7.51052
Epoch 23/80: current_loss=7.57951 | best_loss=7.51052
Epoch 24/80: current_loss=7.61953 | best_loss=7.51052
Epoch 25/80: current_loss=7.59117 | best_loss=7.51052
Epoch 26/80: current_loss=7.73203 | best_loss=7.51052
Epoch 27/80: current_loss=7.61300 | best_loss=7.51052
Epoch 28/80: current_loss=7.71772 | best_loss=7.51052
Epoch 29/80: current_loss=7.58035 | best_loss=7.51052
Epoch 30/80: current_loss=7.61443 | best_loss=7.51052
Early Stopping at epoch 30
      explained_var=-0.00700 | mse_loss=7.65563
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.71605 | best_loss=8.71605
Epoch 1/80: current_loss=8.75926 | best_loss=8.71605
Epoch 2/80: current_loss=8.73881 | best_loss=8.71605
Epoch 3/80: current_loss=8.85675 | best_loss=8.71605
Epoch 4/80: current_loss=8.73618 | best_loss=8.71605
Epoch 5/80: current_loss=8.78945 | best_loss=8.71605
Epoch 6/80: current_loss=8.75152 | best_loss=8.71605
Epoch 7/80: current_loss=8.75408 | best_loss=8.71605
Epoch 8/80: current_loss=8.73088 | best_loss=8.71605
Epoch 9/80: current_loss=8.71912 | best_loss=8.71605
Epoch 10/80: current_loss=8.91022 | best_loss=8.71605
Epoch 11/80: current_loss=8.73275 | best_loss=8.71605
Epoch 12/80: current_loss=8.85930 | best_loss=8.71605
Epoch 13/80: current_loss=8.76454 | best_loss=8.71605
Epoch 14/80: current_loss=8.97303 | best_loss=8.71605
Epoch 15/80: current_loss=8.87495 | best_loss=8.71605
Epoch 16/80: current_loss=8.90051 | best_loss=8.71605
Epoch 17/80: current_loss=9.36021 | best_loss=8.71605
Epoch 18/80: current_loss=8.88872 | best_loss=8.71605
Epoch 19/80: current_loss=8.76757 | best_loss=8.71605
Epoch 20/80: current_loss=8.74943 | best_loss=8.71605
Early Stopping at epoch 20
      explained_var=0.00532 | mse_loss=8.67258
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.28937 | best_loss=9.28937
Epoch 1/80: current_loss=9.22136 | best_loss=9.22136
Epoch 2/80: current_loss=9.24682 | best_loss=9.22136
Epoch 3/80: current_loss=9.16786 | best_loss=9.16786
Epoch 4/80: current_loss=9.33165 | best_loss=9.16786
Epoch 5/80: current_loss=9.17837 | best_loss=9.16786
Epoch 6/80: current_loss=9.17650 | best_loss=9.16786
Epoch 7/80: current_loss=9.42135 | best_loss=9.16786
Epoch 8/80: current_loss=9.22392 | best_loss=9.16786
Epoch 9/80: current_loss=9.21405 | best_loss=9.16786
Epoch 10/80: current_loss=9.24038 | best_loss=9.16786
Epoch 11/80: current_loss=9.18108 | best_loss=9.16786
Epoch 12/80: current_loss=9.20662 | best_loss=9.16786
Epoch 13/80: current_loss=9.24136 | best_loss=9.16786
Epoch 14/80: current_loss=9.15414 | best_loss=9.15414
Epoch 15/80: current_loss=9.23067 | best_loss=9.15414
Epoch 16/80: current_loss=9.16606 | best_loss=9.15414
Epoch 17/80: current_loss=9.34264 | best_loss=9.15414
Epoch 18/80: current_loss=9.22660 | best_loss=9.15414
Epoch 19/80: current_loss=9.18819 | best_loss=9.15414
Epoch 20/80: current_loss=9.21846 | best_loss=9.15414
Epoch 21/80: current_loss=9.23681 | best_loss=9.15414
Epoch 22/80: current_loss=9.23645 | best_loss=9.15414
Epoch 23/80: current_loss=9.17959 | best_loss=9.15414
Epoch 24/80: current_loss=9.20767 | best_loss=9.15414
Epoch 25/80: current_loss=9.25768 | best_loss=9.15414
Epoch 26/80: current_loss=9.14113 | best_loss=9.14113
Epoch 27/80: current_loss=9.18059 | best_loss=9.14113
Epoch 28/80: current_loss=9.18918 | best_loss=9.14113
Epoch 29/80: current_loss=9.19502 | best_loss=9.14113
Epoch 30/80: current_loss=9.21327 | best_loss=9.14113
Epoch 31/80: current_loss=9.23129 | best_loss=9.14113
Epoch 32/80: current_loss=9.19378 | best_loss=9.14113
Epoch 33/80: current_loss=9.37174 | best_loss=9.14113
Epoch 34/80: current_loss=9.24024 | best_loss=9.14113
Epoch 35/80: current_loss=9.36209 | best_loss=9.14113
Epoch 36/80: current_loss=9.41416 | best_loss=9.14113
Epoch 37/80: current_loss=9.23561 | best_loss=9.14113
Epoch 38/80: current_loss=9.23547 | best_loss=9.14113
Epoch 39/80: current_loss=9.21080 | best_loss=9.14113
Epoch 40/80: current_loss=9.17598 | best_loss=9.14113
Epoch 41/80: current_loss=9.14917 | best_loss=9.14113
Epoch 42/80: current_loss=9.39369 | best_loss=9.14113
Epoch 43/80: current_loss=9.26204 | best_loss=9.14113
Epoch 44/80: current_loss=9.22033 | best_loss=9.14113
Epoch 45/80: current_loss=9.22437 | best_loss=9.14113
Epoch 46/80: current_loss=9.18627 | best_loss=9.14113
Early Stopping at epoch 46
      explained_var=0.02126 | mse_loss=9.09695
----------------------------------------------
Average early_stopping_point: 26| avg_exp_var=0.01303| avg_loss=9.10788
----------------------------------------------


----------------------------------------------
Params for Trial 12
{'learning_rate': 0.001, 'weight_decay': 0.00747394189831267, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.28014 | best_loss=16.28014
Epoch 1/80: current_loss=10.75188 | best_loss=10.75188
Epoch 2/80: current_loss=10.43642 | best_loss=10.43642
Epoch 3/80: current_loss=10.36264 | best_loss=10.36264
Epoch 4/80: current_loss=10.33578 | best_loss=10.33578
Epoch 5/80: current_loss=10.32591 | best_loss=10.32591
Epoch 6/80: current_loss=10.29977 | best_loss=10.29977
Epoch 7/80: current_loss=10.31932 | best_loss=10.29977
Epoch 8/80: current_loss=10.28867 | best_loss=10.28867
Epoch 9/80: current_loss=10.30931 | best_loss=10.28867
Epoch 10/80: current_loss=10.30728 | best_loss=10.28867
Epoch 11/80: current_loss=10.27147 | best_loss=10.27147
Epoch 12/80: current_loss=10.31672 | best_loss=10.27147
Epoch 13/80: current_loss=10.28897 | best_loss=10.27147
Epoch 14/80: current_loss=10.27874 | best_loss=10.27147
Epoch 15/80: current_loss=10.27435 | best_loss=10.27147
Epoch 16/80: current_loss=10.29739 | best_loss=10.27147
Epoch 17/80: current_loss=10.29998 | best_loss=10.27147
Epoch 18/80: current_loss=10.29755 | best_loss=10.27147
Epoch 19/80: current_loss=10.31546 | best_loss=10.27147
Epoch 20/80: current_loss=10.28458 | best_loss=10.27147
Epoch 21/80: current_loss=10.28199 | best_loss=10.27147
Epoch 22/80: current_loss=10.28432 | best_loss=10.27147
Epoch 23/80: current_loss=10.27811 | best_loss=10.27147
Epoch 24/80: current_loss=10.28402 | best_loss=10.27147
Epoch 25/80: current_loss=10.32598 | best_loss=10.27147
Epoch 26/80: current_loss=10.28250 | best_loss=10.27147
Epoch 27/80: current_loss=10.30601 | best_loss=10.27147
Epoch 28/80: current_loss=10.27174 | best_loss=10.27147
Epoch 29/80: current_loss=10.30221 | best_loss=10.27147
Epoch 30/80: current_loss=10.29533 | best_loss=10.27147
Epoch 31/80: current_loss=10.30173 | best_loss=10.27147
Early Stopping at epoch 31
      explained_var=-0.00293 | mse_loss=10.49505
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.41189 | best_loss=10.41189
Epoch 1/80: current_loss=10.55324 | best_loss=10.41189
Epoch 2/80: current_loss=10.45635 | best_loss=10.41189
Epoch 3/80: current_loss=10.40730 | best_loss=10.40730
Epoch 4/80: current_loss=10.44764 | best_loss=10.40730
Epoch 5/80: current_loss=10.45039 | best_loss=10.40730
Epoch 6/80: current_loss=10.41751 | best_loss=10.40730
Epoch 7/80: current_loss=10.52926 | best_loss=10.40730
Epoch 8/80: current_loss=10.41151 | best_loss=10.40730
Epoch 9/80: current_loss=10.42244 | best_loss=10.40730
Epoch 10/80: current_loss=10.40400 | best_loss=10.40400
Epoch 11/80: current_loss=10.42418 | best_loss=10.40400
Epoch 12/80: current_loss=10.50141 | best_loss=10.40400
Epoch 13/80: current_loss=10.40468 | best_loss=10.40400
Epoch 14/80: current_loss=10.42436 | best_loss=10.40400
Epoch 15/80: current_loss=10.45063 | best_loss=10.40400
Epoch 16/80: current_loss=10.40457 | best_loss=10.40400
Epoch 17/80: current_loss=10.41802 | best_loss=10.40400
Epoch 18/80: current_loss=10.42898 | best_loss=10.40400
Epoch 19/80: current_loss=10.40435 | best_loss=10.40400
Epoch 20/80: current_loss=10.41064 | best_loss=10.40400
Epoch 21/80: current_loss=10.41183 | best_loss=10.40400
Epoch 22/80: current_loss=10.47759 | best_loss=10.40400
Epoch 23/80: current_loss=10.40887 | best_loss=10.40400
Epoch 24/80: current_loss=10.41965 | best_loss=10.40400
Epoch 25/80: current_loss=10.48286 | best_loss=10.40400
Epoch 26/80: current_loss=10.41152 | best_loss=10.40400
Epoch 27/80: current_loss=10.41482 | best_loss=10.40400
Epoch 28/80: current_loss=10.40942 | best_loss=10.40400
Epoch 29/80: current_loss=10.42490 | best_loss=10.40400
Epoch 30/80: current_loss=10.39766 | best_loss=10.39766
Epoch 31/80: current_loss=10.41928 | best_loss=10.39766
Epoch 32/80: current_loss=10.48171 | best_loss=10.39766
Epoch 33/80: current_loss=10.40746 | best_loss=10.39766
Epoch 34/80: current_loss=10.43394 | best_loss=10.39766
Epoch 35/80: current_loss=10.40603 | best_loss=10.39766
Epoch 36/80: current_loss=10.41441 | best_loss=10.39766
Epoch 37/80: current_loss=10.45186 | best_loss=10.39766
Epoch 38/80: current_loss=10.42979 | best_loss=10.39766
Epoch 39/80: current_loss=10.45889 | best_loss=10.39766
Epoch 40/80: current_loss=10.41661 | best_loss=10.39766
Epoch 41/80: current_loss=10.41171 | best_loss=10.39766
Epoch 42/80: current_loss=10.41660 | best_loss=10.39766
Epoch 43/80: current_loss=10.39992 | best_loss=10.39766
Epoch 44/80: current_loss=10.42905 | best_loss=10.39766
Epoch 45/80: current_loss=10.41279 | best_loss=10.39766
Epoch 46/80: current_loss=10.40922 | best_loss=10.39766
Epoch 47/80: current_loss=10.42721 | best_loss=10.39766
Epoch 48/80: current_loss=10.40209 | best_loss=10.39766
Epoch 49/80: current_loss=10.41479 | best_loss=10.39766
Epoch 50/80: current_loss=10.41801 | best_loss=10.39766
Early Stopping at epoch 50
      explained_var=0.02193 | mse_loss=10.03247
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.59804 | best_loss=7.59804
Epoch 1/80: current_loss=7.49100 | best_loss=7.49100
Epoch 2/80: current_loss=7.61623 | best_loss=7.49100
Epoch 3/80: current_loss=7.48842 | best_loss=7.48842
Epoch 4/80: current_loss=7.59108 | best_loss=7.48842
Epoch 5/80: current_loss=7.59689 | best_loss=7.48842
Epoch 6/80: current_loss=7.72098 | best_loss=7.48842
Epoch 7/80: current_loss=7.64656 | best_loss=7.48842
Epoch 8/80: current_loss=7.51108 | best_loss=7.48842
Epoch 9/80: current_loss=7.58768 | best_loss=7.48842
Epoch 10/80: current_loss=7.63710 | best_loss=7.48842
Epoch 11/80: current_loss=7.47836 | best_loss=7.47836
Epoch 12/80: current_loss=7.58155 | best_loss=7.47836
Epoch 13/80: current_loss=7.53274 | best_loss=7.47836
Epoch 14/80: current_loss=7.63044 | best_loss=7.47836
Epoch 15/80: current_loss=7.57051 | best_loss=7.47836
Epoch 16/80: current_loss=7.66737 | best_loss=7.47836
Epoch 17/80: current_loss=7.62575 | best_loss=7.47836
Epoch 18/80: current_loss=7.55626 | best_loss=7.47836
Epoch 19/80: current_loss=7.52165 | best_loss=7.47836
Epoch 20/80: current_loss=7.78569 | best_loss=7.47836
Epoch 21/80: current_loss=7.59866 | best_loss=7.47836
Epoch 22/80: current_loss=7.55008 | best_loss=7.47836
Epoch 23/80: current_loss=7.68796 | best_loss=7.47836
Epoch 24/80: current_loss=7.70183 | best_loss=7.47836
Epoch 25/80: current_loss=7.56963 | best_loss=7.47836
Epoch 26/80: current_loss=7.67463 | best_loss=7.47836
Epoch 27/80: current_loss=7.59484 | best_loss=7.47836
Epoch 28/80: current_loss=7.69419 | best_loss=7.47836
Epoch 29/80: current_loss=7.57222 | best_loss=7.47836
Epoch 30/80: current_loss=7.84786 | best_loss=7.47836
Epoch 31/80: current_loss=7.65452 | best_loss=7.47836
Early Stopping at epoch 31
      explained_var=0.00065 | mse_loss=7.58338
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.76004 | best_loss=8.76004
Epoch 1/80: current_loss=8.79663 | best_loss=8.76004
Epoch 2/80: current_loss=8.77602 | best_loss=8.76004
Epoch 3/80: current_loss=8.83671 | best_loss=8.76004
Epoch 4/80: current_loss=8.78314 | best_loss=8.76004
Epoch 5/80: current_loss=8.77545 | best_loss=8.76004
Epoch 6/80: current_loss=8.79297 | best_loss=8.76004
Epoch 7/80: current_loss=8.95393 | best_loss=8.76004
Epoch 8/80: current_loss=8.73059 | best_loss=8.73059
Epoch 9/80: current_loss=8.77412 | best_loss=8.73059
Epoch 10/80: current_loss=8.72626 | best_loss=8.72626
Epoch 11/80: current_loss=8.78434 | best_loss=8.72626
Epoch 12/80: current_loss=8.83780 | best_loss=8.72626
Epoch 13/80: current_loss=8.77026 | best_loss=8.72626
Epoch 14/80: current_loss=8.88340 | best_loss=8.72626
Epoch 15/80: current_loss=8.77097 | best_loss=8.72626
Epoch 16/80: current_loss=8.84273 | best_loss=8.72626
Epoch 17/80: current_loss=8.85509 | best_loss=8.72626
Epoch 18/80: current_loss=8.77772 | best_loss=8.72626
Epoch 19/80: current_loss=8.83314 | best_loss=8.72626
Epoch 20/80: current_loss=8.87216 | best_loss=8.72626
Epoch 21/80: current_loss=8.84563 | best_loss=8.72626
Epoch 22/80: current_loss=8.82837 | best_loss=8.72626
Epoch 23/80: current_loss=8.76937 | best_loss=8.72626
Epoch 24/80: current_loss=8.81425 | best_loss=8.72626
Epoch 25/80: current_loss=9.34013 | best_loss=8.72626
Epoch 26/80: current_loss=9.05729 | best_loss=8.72626
Epoch 27/80: current_loss=8.76368 | best_loss=8.72626
Epoch 28/80: current_loss=8.73301 | best_loss=8.72626
Epoch 29/80: current_loss=8.80700 | best_loss=8.72626
Epoch 30/80: current_loss=8.89363 | best_loss=8.72626
Early Stopping at epoch 30
      explained_var=0.00255 | mse_loss=8.69569
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.22487 | best_loss=9.22487
Epoch 1/80: current_loss=9.18261 | best_loss=9.18261
Epoch 2/80: current_loss=9.18068 | best_loss=9.18068
Epoch 3/80: current_loss=9.21711 | best_loss=9.18068
Epoch 4/80: current_loss=9.14981 | best_loss=9.14981
Epoch 5/80: current_loss=9.14946 | best_loss=9.14946
Epoch 6/80: current_loss=9.36919 | best_loss=9.14946
Epoch 7/80: current_loss=9.20373 | best_loss=9.14946
Epoch 8/80: current_loss=9.21526 | best_loss=9.14946
Epoch 9/80: current_loss=9.15649 | best_loss=9.14946
Epoch 10/80: current_loss=9.44435 | best_loss=9.14946
Epoch 11/80: current_loss=9.23839 | best_loss=9.14946
Epoch 12/80: current_loss=9.16522 | best_loss=9.14946
Epoch 13/80: current_loss=9.22234 | best_loss=9.14946
Epoch 14/80: current_loss=9.44208 | best_loss=9.14946
Epoch 15/80: current_loss=9.18253 | best_loss=9.14946
Epoch 16/80: current_loss=9.17009 | best_loss=9.14946
Epoch 17/80: current_loss=9.25443 | best_loss=9.14946
Epoch 18/80: current_loss=9.19992 | best_loss=9.14946
Epoch 19/80: current_loss=9.19570 | best_loss=9.14946
Epoch 20/80: current_loss=9.17151 | best_loss=9.14946
Epoch 21/80: current_loss=9.14644 | best_loss=9.14644
Epoch 22/80: current_loss=9.18426 | best_loss=9.14644
Epoch 23/80: current_loss=9.15929 | best_loss=9.14644
Epoch 24/80: current_loss=9.16847 | best_loss=9.14644
Epoch 25/80: current_loss=9.18388 | best_loss=9.14644
Epoch 26/80: current_loss=9.30015 | best_loss=9.14644
Epoch 27/80: current_loss=9.17389 | best_loss=9.14644
Epoch 28/80: current_loss=9.19866 | best_loss=9.14644
Epoch 29/80: current_loss=9.15474 | best_loss=9.14644
Epoch 30/80: current_loss=9.23441 | best_loss=9.14644
Epoch 31/80: current_loss=9.27359 | best_loss=9.14644
Epoch 32/80: current_loss=9.22863 | best_loss=9.14644
Epoch 33/80: current_loss=9.16997 | best_loss=9.14644
Epoch 34/80: current_loss=9.16593 | best_loss=9.14644
Epoch 35/80: current_loss=9.17514 | best_loss=9.14644
Epoch 36/80: current_loss=9.15424 | best_loss=9.14644
Epoch 37/80: current_loss=9.15383 | best_loss=9.14644
Epoch 38/80: current_loss=9.12693 | best_loss=9.12693
Epoch 39/80: current_loss=9.20794 | best_loss=9.12693
Epoch 40/80: current_loss=9.21215 | best_loss=9.12693
Epoch 41/80: current_loss=9.16853 | best_loss=9.12693
Epoch 42/80: current_loss=9.19060 | best_loss=9.12693
Epoch 43/80: current_loss=9.26276 | best_loss=9.12693
Epoch 44/80: current_loss=9.18612 | best_loss=9.12693
Epoch 45/80: current_loss=9.21734 | best_loss=9.12693
Epoch 46/80: current_loss=9.20374 | best_loss=9.12693
Epoch 47/80: current_loss=9.17613 | best_loss=9.12693
Epoch 48/80: current_loss=9.17610 | best_loss=9.12693
Epoch 49/80: current_loss=9.41480 | best_loss=9.12693
Epoch 50/80: current_loss=9.21520 | best_loss=9.12693
Epoch 51/80: current_loss=9.18042 | best_loss=9.12693
Epoch 52/80: current_loss=9.17773 | best_loss=9.12693
Epoch 53/80: current_loss=9.15297 | best_loss=9.12693
Epoch 54/80: current_loss=9.34985 | best_loss=9.12693
Epoch 55/80: current_loss=9.21061 | best_loss=9.12693
Epoch 56/80: current_loss=9.18460 | best_loss=9.12693
Epoch 57/80: current_loss=9.18617 | best_loss=9.12693
Epoch 58/80: current_loss=9.19392 | best_loss=9.12693
Early Stopping at epoch 58
      explained_var=0.02433 | mse_loss=9.07768
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00930| avg_loss=9.17685
----------------------------------------------


----------------------------------------------
Params for Trial 13
{'learning_rate': 0.001, 'weight_decay': 0.009915964092801072, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.56653 | best_loss=18.56653
Epoch 1/80: current_loss=10.97819 | best_loss=10.97819
Epoch 2/80: current_loss=10.43832 | best_loss=10.43832
Epoch 3/80: current_loss=10.43091 | best_loss=10.43091
Epoch 4/80: current_loss=10.38459 | best_loss=10.38459
Epoch 5/80: current_loss=10.35255 | best_loss=10.35255
Epoch 6/80: current_loss=10.33509 | best_loss=10.33509
Epoch 7/80: current_loss=10.34554 | best_loss=10.33509
Epoch 8/80: current_loss=10.31291 | best_loss=10.31291
Epoch 9/80: current_loss=10.31220 | best_loss=10.31220
Epoch 10/80: current_loss=10.30135 | best_loss=10.30135
Epoch 11/80: current_loss=10.29240 | best_loss=10.29240
Epoch 12/80: current_loss=10.31599 | best_loss=10.29240
Epoch 13/80: current_loss=10.31217 | best_loss=10.29240
Epoch 14/80: current_loss=10.30272 | best_loss=10.29240
Epoch 15/80: current_loss=10.28791 | best_loss=10.28791
Epoch 16/80: current_loss=10.30603 | best_loss=10.28791
Epoch 17/80: current_loss=10.30457 | best_loss=10.28791
Epoch 18/80: current_loss=10.27996 | best_loss=10.27996
Epoch 19/80: current_loss=10.30702 | best_loss=10.27996
Epoch 20/80: current_loss=10.27939 | best_loss=10.27939
Epoch 21/80: current_loss=10.31218 | best_loss=10.27939
Epoch 22/80: current_loss=10.29634 | best_loss=10.27939
Epoch 23/80: current_loss=10.28877 | best_loss=10.27939
Epoch 24/80: current_loss=10.27824 | best_loss=10.27824
Epoch 25/80: current_loss=10.28712 | best_loss=10.27824
Epoch 26/80: current_loss=10.30004 | best_loss=10.27824
Epoch 27/80: current_loss=10.30166 | best_loss=10.27824
Epoch 28/80: current_loss=10.31267 | best_loss=10.27824
Epoch 29/80: current_loss=10.28960 | best_loss=10.27824
Epoch 30/80: current_loss=10.30567 | best_loss=10.27824
Epoch 31/80: current_loss=10.28494 | best_loss=10.27824
Epoch 32/80: current_loss=10.29406 | best_loss=10.27824
Epoch 33/80: current_loss=10.28877 | best_loss=10.27824
Epoch 34/80: current_loss=10.32299 | best_loss=10.27824
Epoch 35/80: current_loss=10.30580 | best_loss=10.27824
Epoch 36/80: current_loss=10.29584 | best_loss=10.27824
Epoch 37/80: current_loss=10.28214 | best_loss=10.27824
Epoch 38/80: current_loss=10.28390 | best_loss=10.27824
Epoch 39/80: current_loss=10.28944 | best_loss=10.27824
Epoch 40/80: current_loss=10.27905 | best_loss=10.27824
Epoch 41/80: current_loss=10.29440 | best_loss=10.27824
Epoch 42/80: current_loss=10.30686 | best_loss=10.27824
Epoch 43/80: current_loss=10.30725 | best_loss=10.27824
Epoch 44/80: current_loss=10.28017 | best_loss=10.27824
Early Stopping at epoch 44
      explained_var=-0.00352 | mse_loss=10.50573
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.43587 | best_loss=10.43587
Epoch 1/80: current_loss=10.41465 | best_loss=10.41465
Epoch 2/80: current_loss=10.42589 | best_loss=10.41465
Epoch 3/80: current_loss=10.46087 | best_loss=10.41465
Epoch 4/80: current_loss=10.43577 | best_loss=10.41465
Epoch 5/80: current_loss=10.43782 | best_loss=10.41465
Epoch 6/80: current_loss=10.40587 | best_loss=10.40587
Epoch 7/80: current_loss=10.47227 | best_loss=10.40587
Epoch 8/80: current_loss=10.40528 | best_loss=10.40528
Epoch 9/80: current_loss=10.44184 | best_loss=10.40528
Epoch 10/80: current_loss=10.46575 | best_loss=10.40528
Epoch 11/80: current_loss=10.38580 | best_loss=10.38580
Epoch 12/80: current_loss=10.43052 | best_loss=10.38580
Epoch 13/80: current_loss=10.44550 | best_loss=10.38580
Epoch 14/80: current_loss=10.41780 | best_loss=10.38580
Epoch 15/80: current_loss=10.44945 | best_loss=10.38580
Epoch 16/80: current_loss=10.42999 | best_loss=10.38580
Epoch 17/80: current_loss=10.42098 | best_loss=10.38580
Epoch 18/80: current_loss=10.47074 | best_loss=10.38580
Epoch 19/80: current_loss=10.42066 | best_loss=10.38580
Epoch 20/80: current_loss=10.51774 | best_loss=10.38580
Epoch 21/80: current_loss=10.40568 | best_loss=10.38580
Epoch 22/80: current_loss=10.41083 | best_loss=10.38580
Epoch 23/80: current_loss=10.49167 | best_loss=10.38580
Epoch 24/80: current_loss=10.45400 | best_loss=10.38580
Epoch 25/80: current_loss=10.42486 | best_loss=10.38580
Epoch 26/80: current_loss=10.42268 | best_loss=10.38580
Epoch 27/80: current_loss=10.40439 | best_loss=10.38580
Epoch 28/80: current_loss=10.49816 | best_loss=10.38580
Epoch 29/80: current_loss=10.40808 | best_loss=10.38580
Epoch 30/80: current_loss=10.40960 | best_loss=10.38580
Epoch 31/80: current_loss=10.43508 | best_loss=10.38580
Early Stopping at epoch 31
      explained_var=0.00232 | mse_loss=10.04094
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.66866 | best_loss=7.66866
Epoch 1/80: current_loss=7.63367 | best_loss=7.63367
Epoch 2/80: current_loss=7.54555 | best_loss=7.54555
Epoch 3/80: current_loss=7.55766 | best_loss=7.54555
Epoch 4/80: current_loss=7.68352 | best_loss=7.54555
Epoch 5/80: current_loss=7.54740 | best_loss=7.54555
Epoch 6/80: current_loss=7.51992 | best_loss=7.51992
Epoch 7/80: current_loss=7.56263 | best_loss=7.51992
Epoch 8/80: current_loss=7.54209 | best_loss=7.51992
Epoch 9/80: current_loss=7.52889 | best_loss=7.51992
Epoch 10/80: current_loss=7.76114 | best_loss=7.51992
Epoch 11/80: current_loss=7.58145 | best_loss=7.51992
Epoch 12/80: current_loss=7.57198 | best_loss=7.51992
Epoch 13/80: current_loss=7.53778 | best_loss=7.51992
Epoch 14/80: current_loss=7.56532 | best_loss=7.51992
Epoch 15/80: current_loss=7.56138 | best_loss=7.51992
Epoch 16/80: current_loss=7.52021 | best_loss=7.51992
Epoch 17/80: current_loss=7.58628 | best_loss=7.51992
Epoch 18/80: current_loss=7.48073 | best_loss=7.48073
Epoch 19/80: current_loss=7.69119 | best_loss=7.48073
Epoch 20/80: current_loss=7.48209 | best_loss=7.48073
Epoch 21/80: current_loss=7.94292 | best_loss=7.48073
Epoch 22/80: current_loss=7.52059 | best_loss=7.48073
Epoch 23/80: current_loss=7.55344 | best_loss=7.48073
Epoch 24/80: current_loss=7.62183 | best_loss=7.48073
Epoch 25/80: current_loss=7.54140 | best_loss=7.48073
Epoch 26/80: current_loss=7.73368 | best_loss=7.48073
Epoch 27/80: current_loss=7.54532 | best_loss=7.48073
Epoch 28/80: current_loss=8.42050 | best_loss=7.48073
Epoch 29/80: current_loss=7.62528 | best_loss=7.48073
Epoch 30/80: current_loss=7.56053 | best_loss=7.48073
Epoch 31/80: current_loss=7.52530 | best_loss=7.48073
Epoch 32/80: current_loss=7.65777 | best_loss=7.48073
Epoch 33/80: current_loss=7.51090 | best_loss=7.48073
Epoch 34/80: current_loss=7.82143 | best_loss=7.48073
Epoch 35/80: current_loss=7.48352 | best_loss=7.48073
Epoch 36/80: current_loss=7.63758 | best_loss=7.48073
Epoch 37/80: current_loss=7.62748 | best_loss=7.48073
Epoch 38/80: current_loss=7.55166 | best_loss=7.48073
Early Stopping at epoch 38
      explained_var=0.00010 | mse_loss=7.58232
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.79807 | best_loss=8.79807
Epoch 1/80: current_loss=8.75913 | best_loss=8.75913
Epoch 2/80: current_loss=8.82619 | best_loss=8.75913
Epoch 3/80: current_loss=8.94325 | best_loss=8.75913
Epoch 4/80: current_loss=8.90759 | best_loss=8.75913
Epoch 5/80: current_loss=9.48878 | best_loss=8.75913
Epoch 6/80: current_loss=9.14996 | best_loss=8.75913
Epoch 7/80: current_loss=8.76604 | best_loss=8.75913
Epoch 8/80: current_loss=8.76811 | best_loss=8.75913
Epoch 9/80: current_loss=8.76717 | best_loss=8.75913
Epoch 10/80: current_loss=8.77175 | best_loss=8.75913
Epoch 11/80: current_loss=9.02323 | best_loss=8.75913
Epoch 12/80: current_loss=8.81987 | best_loss=8.75913
Epoch 13/80: current_loss=8.80908 | best_loss=8.75913
Epoch 14/80: current_loss=8.80248 | best_loss=8.75913
Epoch 15/80: current_loss=8.75808 | best_loss=8.75808
Epoch 16/80: current_loss=8.81300 | best_loss=8.75808
Epoch 17/80: current_loss=8.73186 | best_loss=8.73186
Epoch 18/80: current_loss=8.75519 | best_loss=8.73186
Epoch 19/80: current_loss=9.38437 | best_loss=8.73186
Epoch 20/80: current_loss=8.76278 | best_loss=8.73186
Epoch 21/80: current_loss=9.06134 | best_loss=8.73186
Epoch 22/80: current_loss=8.69147 | best_loss=8.69147
Epoch 23/80: current_loss=8.89352 | best_loss=8.69147
Epoch 24/80: current_loss=8.90195 | best_loss=8.69147
Epoch 25/80: current_loss=8.88219 | best_loss=8.69147
Epoch 26/80: current_loss=8.72548 | best_loss=8.69147
Epoch 27/80: current_loss=8.74465 | best_loss=8.69147
Epoch 28/80: current_loss=8.73523 | best_loss=8.69147
Epoch 29/80: current_loss=8.74002 | best_loss=8.69147
Epoch 30/80: current_loss=8.94031 | best_loss=8.69147
Epoch 31/80: current_loss=8.86539 | best_loss=8.69147
Epoch 32/80: current_loss=8.96472 | best_loss=8.69147
Epoch 33/80: current_loss=8.84796 | best_loss=8.69147
Epoch 34/80: current_loss=8.91850 | best_loss=8.69147
Epoch 35/80: current_loss=8.78395 | best_loss=8.69147
Epoch 36/80: current_loss=8.81554 | best_loss=8.69147
Epoch 37/80: current_loss=8.77452 | best_loss=8.69147
Epoch 38/80: current_loss=8.77720 | best_loss=8.69147
Epoch 39/80: current_loss=8.87799 | best_loss=8.69147
Epoch 40/80: current_loss=8.75566 | best_loss=8.69147
Epoch 41/80: current_loss=8.79562 | best_loss=8.69147
Epoch 42/80: current_loss=8.88013 | best_loss=8.69147
Early Stopping at epoch 42
      explained_var=0.00711 | mse_loss=8.65684
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.29824 | best_loss=9.29824
Epoch 1/80: current_loss=9.12236 | best_loss=9.12236
Epoch 2/80: current_loss=9.24428 | best_loss=9.12236
Epoch 3/80: current_loss=9.25578 | best_loss=9.12236
Epoch 4/80: current_loss=9.20083 | best_loss=9.12236
Epoch 5/80: current_loss=9.34632 | best_loss=9.12236
Epoch 6/80: current_loss=9.29692 | best_loss=9.12236
Epoch 7/80: current_loss=9.27007 | best_loss=9.12236
Epoch 8/80: current_loss=9.22602 | best_loss=9.12236
Epoch 9/80: current_loss=9.20155 | best_loss=9.12236
Epoch 10/80: current_loss=9.20622 | best_loss=9.12236
Epoch 11/80: current_loss=9.32319 | best_loss=9.12236
Epoch 12/80: current_loss=9.20535 | best_loss=9.12236
Epoch 13/80: current_loss=9.19933 | best_loss=9.12236
Epoch 14/80: current_loss=9.41729 | best_loss=9.12236
Epoch 15/80: current_loss=9.22915 | best_loss=9.12236
Epoch 16/80: current_loss=9.21102 | best_loss=9.12236
Epoch 17/80: current_loss=9.21043 | best_loss=9.12236
Epoch 18/80: current_loss=9.18039 | best_loss=9.12236
Epoch 19/80: current_loss=9.23057 | best_loss=9.12236
Epoch 20/80: current_loss=9.33871 | best_loss=9.12236
Epoch 21/80: current_loss=9.22228 | best_loss=9.12236
Early Stopping at epoch 21
      explained_var=0.02208 | mse_loss=9.09924
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.00562| avg_loss=9.17701
----------------------------------------------


----------------------------------------------
Params for Trial 14
{'learning_rate': 0.001, 'weight_decay': 0.0028723484743078923, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.78627 | best_loss=18.78627
Epoch 1/80: current_loss=10.95983 | best_loss=10.95983
Epoch 2/80: current_loss=10.46221 | best_loss=10.46221
Epoch 3/80: current_loss=10.40045 | best_loss=10.40045
Epoch 4/80: current_loss=10.38200 | best_loss=10.38200
Epoch 5/80: current_loss=10.34591 | best_loss=10.34591
Epoch 6/80: current_loss=10.34271 | best_loss=10.34271
Epoch 7/80: current_loss=10.32607 | best_loss=10.32607
Epoch 8/80: current_loss=10.30827 | best_loss=10.30827
Epoch 9/80: current_loss=10.31028 | best_loss=10.30827
Epoch 10/80: current_loss=10.32503 | best_loss=10.30827
Epoch 11/80: current_loss=10.28950 | best_loss=10.28950
Epoch 12/80: current_loss=10.27707 | best_loss=10.27707
Epoch 13/80: current_loss=10.27195 | best_loss=10.27195
Epoch 14/80: current_loss=10.27341 | best_loss=10.27195
Epoch 15/80: current_loss=10.29212 | best_loss=10.27195
Epoch 16/80: current_loss=10.26382 | best_loss=10.26382
Epoch 17/80: current_loss=10.30905 | best_loss=10.26382
Epoch 18/80: current_loss=10.28407 | best_loss=10.26382
Epoch 19/80: current_loss=10.26975 | best_loss=10.26382
Epoch 20/80: current_loss=10.28449 | best_loss=10.26382
Epoch 21/80: current_loss=10.28599 | best_loss=10.26382
Epoch 22/80: current_loss=10.27997 | best_loss=10.26382
Epoch 23/80: current_loss=10.26292 | best_loss=10.26292
Epoch 24/80: current_loss=10.27912 | best_loss=10.26292
Epoch 25/80: current_loss=10.26230 | best_loss=10.26230
Epoch 26/80: current_loss=10.28087 | best_loss=10.26230
Epoch 27/80: current_loss=10.29235 | best_loss=10.26230
Epoch 28/80: current_loss=10.27463 | best_loss=10.26230
Epoch 29/80: current_loss=10.27131 | best_loss=10.26230
Epoch 30/80: current_loss=10.28199 | best_loss=10.26230
Epoch 31/80: current_loss=10.28791 | best_loss=10.26230
Epoch 32/80: current_loss=10.26759 | best_loss=10.26230
Epoch 33/80: current_loss=10.27691 | best_loss=10.26230
Epoch 34/80: current_loss=10.30981 | best_loss=10.26230
Epoch 35/80: current_loss=10.28431 | best_loss=10.26230
Epoch 36/80: current_loss=10.26813 | best_loss=10.26230
Epoch 37/80: current_loss=10.27904 | best_loss=10.26230
Epoch 38/80: current_loss=10.32215 | best_loss=10.26230
Epoch 39/80: current_loss=10.26074 | best_loss=10.26074
Epoch 40/80: current_loss=10.29178 | best_loss=10.26074
Epoch 41/80: current_loss=10.26623 | best_loss=10.26074
Epoch 42/80: current_loss=10.28477 | best_loss=10.26074
Epoch 43/80: current_loss=10.26806 | best_loss=10.26074
Epoch 44/80: current_loss=10.26552 | best_loss=10.26074
Epoch 45/80: current_loss=10.27558 | best_loss=10.26074
Epoch 46/80: current_loss=10.26536 | best_loss=10.26074
Epoch 47/80: current_loss=10.32424 | best_loss=10.26074
Epoch 48/80: current_loss=10.28603 | best_loss=10.26074
Epoch 49/80: current_loss=10.28287 | best_loss=10.26074
Epoch 50/80: current_loss=10.30473 | best_loss=10.26074
Epoch 51/80: current_loss=10.30819 | best_loss=10.26074
Epoch 52/80: current_loss=10.26880 | best_loss=10.26074
Epoch 53/80: current_loss=10.29425 | best_loss=10.26074
Epoch 54/80: current_loss=10.26313 | best_loss=10.26074
Epoch 55/80: current_loss=10.29515 | best_loss=10.26074
Epoch 56/80: current_loss=10.27112 | best_loss=10.26074
Epoch 57/80: current_loss=10.28756 | best_loss=10.26074
Epoch 58/80: current_loss=10.26828 | best_loss=10.26074
Epoch 59/80: current_loss=10.26702 | best_loss=10.26074
Early Stopping at epoch 59
      explained_var=-0.00137 | mse_loss=10.48216
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.51209 | best_loss=10.51209
Epoch 1/80: current_loss=10.41845 | best_loss=10.41845
Epoch 2/80: current_loss=10.41012 | best_loss=10.41012
Epoch 3/80: current_loss=10.40837 | best_loss=10.40837
Epoch 4/80: current_loss=10.40431 | best_loss=10.40431
Epoch 5/80: current_loss=10.43077 | best_loss=10.40431
Epoch 6/80: current_loss=10.42779 | best_loss=10.40431
Epoch 7/80: current_loss=10.40548 | best_loss=10.40431
Epoch 8/80: current_loss=10.40712 | best_loss=10.40431
Epoch 9/80: current_loss=10.43207 | best_loss=10.40431
Epoch 10/80: current_loss=10.45347 | best_loss=10.40431
Epoch 11/80: current_loss=10.40453 | best_loss=10.40431
Epoch 12/80: current_loss=10.41743 | best_loss=10.40431
Epoch 13/80: current_loss=10.45351 | best_loss=10.40431
Epoch 14/80: current_loss=10.49138 | best_loss=10.40431
Epoch 15/80: current_loss=10.40438 | best_loss=10.40431
Epoch 16/80: current_loss=10.39413 | best_loss=10.39413
Epoch 17/80: current_loss=10.61297 | best_loss=10.39413
Epoch 18/80: current_loss=10.40950 | best_loss=10.39413
Epoch 19/80: current_loss=10.44295 | best_loss=10.39413
Epoch 20/80: current_loss=10.40338 | best_loss=10.39413
Epoch 21/80: current_loss=10.40800 | best_loss=10.39413
Epoch 22/80: current_loss=10.40654 | best_loss=10.39413
Epoch 23/80: current_loss=10.56044 | best_loss=10.39413
Epoch 24/80: current_loss=10.40386 | best_loss=10.39413
Epoch 25/80: current_loss=10.43274 | best_loss=10.39413
Epoch 26/80: current_loss=10.43257 | best_loss=10.39413
Epoch 27/80: current_loss=10.40326 | best_loss=10.39413
Epoch 28/80: current_loss=10.42034 | best_loss=10.39413
Epoch 29/80: current_loss=10.40938 | best_loss=10.39413
Epoch 30/80: current_loss=10.41045 | best_loss=10.39413
Epoch 31/80: current_loss=10.40486 | best_loss=10.39413
Epoch 32/80: current_loss=10.44261 | best_loss=10.39413
Epoch 33/80: current_loss=10.40658 | best_loss=10.39413
Epoch 34/80: current_loss=10.41839 | best_loss=10.39413
Epoch 35/80: current_loss=10.40457 | best_loss=10.39413
Epoch 36/80: current_loss=10.42836 | best_loss=10.39413
Early Stopping at epoch 36
      explained_var=0.00205 | mse_loss=10.04643
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.53554 | best_loss=7.53554
Epoch 1/80: current_loss=7.50746 | best_loss=7.50746
Epoch 2/80: current_loss=7.67360 | best_loss=7.50746
Epoch 3/80: current_loss=7.60051 | best_loss=7.50746
Epoch 4/80: current_loss=7.55740 | best_loss=7.50746
Epoch 5/80: current_loss=7.53898 | best_loss=7.50746
Epoch 6/80: current_loss=7.56970 | best_loss=7.50746
Epoch 7/80: current_loss=7.60742 | best_loss=7.50746
Epoch 8/80: current_loss=7.64080 | best_loss=7.50746
Epoch 9/80: current_loss=7.50105 | best_loss=7.50105
Epoch 10/80: current_loss=7.52803 | best_loss=7.50105
Epoch 11/80: current_loss=7.71518 | best_loss=7.50105
Epoch 12/80: current_loss=7.49308 | best_loss=7.49308
Epoch 13/80: current_loss=7.57689 | best_loss=7.49308
Epoch 14/80: current_loss=7.55932 | best_loss=7.49308
Epoch 15/80: current_loss=7.64660 | best_loss=7.49308
Epoch 16/80: current_loss=7.65168 | best_loss=7.49308
Epoch 17/80: current_loss=7.44653 | best_loss=7.44653
Epoch 18/80: current_loss=7.60247 | best_loss=7.44653
Epoch 19/80: current_loss=7.64801 | best_loss=7.44653
Epoch 20/80: current_loss=7.63242 | best_loss=7.44653
Epoch 21/80: current_loss=7.60144 | best_loss=7.44653
Epoch 22/80: current_loss=7.61953 | best_loss=7.44653
Epoch 23/80: current_loss=7.66184 | best_loss=7.44653
Epoch 24/80: current_loss=7.55346 | best_loss=7.44653
Epoch 25/80: current_loss=7.67768 | best_loss=7.44653
Epoch 26/80: current_loss=7.60802 | best_loss=7.44653
Epoch 27/80: current_loss=7.63296 | best_loss=7.44653
Epoch 28/80: current_loss=7.66939 | best_loss=7.44653
Epoch 29/80: current_loss=7.67544 | best_loss=7.44653
Epoch 30/80: current_loss=7.66147 | best_loss=7.44653
Epoch 31/80: current_loss=7.55103 | best_loss=7.44653
Epoch 32/80: current_loss=7.63922 | best_loss=7.44653
Epoch 33/80: current_loss=7.57617 | best_loss=7.44653
Epoch 34/80: current_loss=7.62329 | best_loss=7.44653
Epoch 35/80: current_loss=7.75467 | best_loss=7.44653
Epoch 36/80: current_loss=7.60194 | best_loss=7.44653
Epoch 37/80: current_loss=7.64661 | best_loss=7.44653
Early Stopping at epoch 37
      explained_var=0.00162 | mse_loss=7.58226
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.79756 | best_loss=8.79756
Epoch 1/80: current_loss=8.86039 | best_loss=8.79756
Epoch 2/80: current_loss=8.82965 | best_loss=8.79756
Epoch 3/80: current_loss=8.78192 | best_loss=8.78192
Epoch 4/80: current_loss=8.73414 | best_loss=8.73414
Epoch 5/80: current_loss=8.80918 | best_loss=8.73414
Epoch 6/80: current_loss=8.84720 | best_loss=8.73414
Epoch 7/80: current_loss=8.93641 | best_loss=8.73414
Epoch 8/80: current_loss=8.91365 | best_loss=8.73414
Epoch 9/80: current_loss=8.87644 | best_loss=8.73414
Epoch 10/80: current_loss=8.98499 | best_loss=8.73414
Epoch 11/80: current_loss=8.85531 | best_loss=8.73414
Epoch 12/80: current_loss=8.92173 | best_loss=8.73414
Epoch 13/80: current_loss=8.84276 | best_loss=8.73414
Epoch 14/80: current_loss=8.92812 | best_loss=8.73414
Epoch 15/80: current_loss=8.86015 | best_loss=8.73414
Epoch 16/80: current_loss=8.87206 | best_loss=8.73414
Epoch 17/80: current_loss=8.87635 | best_loss=8.73414
Epoch 18/80: current_loss=8.90994 | best_loss=8.73414
Epoch 19/80: current_loss=8.85983 | best_loss=8.73414
Epoch 20/80: current_loss=8.83680 | best_loss=8.73414
Epoch 21/80: current_loss=8.86326 | best_loss=8.73414
Epoch 22/80: current_loss=8.84797 | best_loss=8.73414
Epoch 23/80: current_loss=8.89037 | best_loss=8.73414
Epoch 24/80: current_loss=8.86708 | best_loss=8.73414
Early Stopping at epoch 24
      explained_var=0.00181 | mse_loss=8.70159
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.13918 | best_loss=9.13918
Epoch 1/80: current_loss=9.48291 | best_loss=9.13918
Epoch 2/80: current_loss=9.20916 | best_loss=9.13918
Epoch 3/80: current_loss=9.17863 | best_loss=9.13918
Epoch 4/80: current_loss=9.18948 | best_loss=9.13918
Epoch 5/80: current_loss=9.16231 | best_loss=9.13918
Epoch 6/80: current_loss=9.14987 | best_loss=9.13918
Epoch 7/80: current_loss=9.13994 | best_loss=9.13918
Epoch 8/80: current_loss=9.14724 | best_loss=9.13918
Epoch 9/80: current_loss=9.15204 | best_loss=9.13918
Epoch 10/80: current_loss=9.16690 | best_loss=9.13918
Epoch 11/80: current_loss=9.18514 | best_loss=9.13918
Epoch 12/80: current_loss=9.22644 | best_loss=9.13918
Epoch 13/80: current_loss=9.22841 | best_loss=9.13918
Epoch 14/80: current_loss=9.20623 | best_loss=9.13918
Epoch 15/80: current_loss=9.19540 | best_loss=9.13918
Epoch 16/80: current_loss=9.20679 | best_loss=9.13918
Epoch 17/80: current_loss=9.18258 | best_loss=9.13918
Epoch 18/80: current_loss=9.16002 | best_loss=9.13918
Epoch 19/80: current_loss=9.16989 | best_loss=9.13918
Epoch 20/80: current_loss=9.16248 | best_loss=9.13918
Early Stopping at epoch 20
      explained_var=0.02106 | mse_loss=9.09911
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.00503| avg_loss=9.18231
----------------------------------------------


----------------------------------------------
Params for Trial 15
{'learning_rate': 0.01, 'weight_decay': 0.006769602303110451, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=12.97749 | best_loss=12.97749
Epoch 1/80: current_loss=11.99221 | best_loss=11.99221
Epoch 2/80: current_loss=11.46997 | best_loss=11.46997
Epoch 3/80: current_loss=10.75068 | best_loss=10.75068
Epoch 4/80: current_loss=11.56543 | best_loss=10.75068
Epoch 5/80: current_loss=10.52169 | best_loss=10.52169
Epoch 6/80: current_loss=10.56802 | best_loss=10.52169
Epoch 7/80: current_loss=10.73717 | best_loss=10.52169
Epoch 8/80: current_loss=10.36824 | best_loss=10.36824
Epoch 9/80: current_loss=10.34094 | best_loss=10.34094
Epoch 10/80: current_loss=10.38333 | best_loss=10.34094
Epoch 11/80: current_loss=10.45399 | best_loss=10.34094
Epoch 12/80: current_loss=10.32086 | best_loss=10.32086
Epoch 13/80: current_loss=10.31092 | best_loss=10.31092
Epoch 14/80: current_loss=10.79249 | best_loss=10.31092
Epoch 15/80: current_loss=10.37057 | best_loss=10.31092
Epoch 16/80: current_loss=10.31994 | best_loss=10.31092
Epoch 17/80: current_loss=10.31532 | best_loss=10.31092
Epoch 18/80: current_loss=10.34872 | best_loss=10.31092
Epoch 19/80: current_loss=10.52779 | best_loss=10.31092
Epoch 20/80: current_loss=10.50132 | best_loss=10.31092
Epoch 21/80: current_loss=10.53953 | best_loss=10.31092
Epoch 22/80: current_loss=10.33201 | best_loss=10.31092
Epoch 23/80: current_loss=10.42850 | best_loss=10.31092
Epoch 24/80: current_loss=10.43270 | best_loss=10.31092
Epoch 25/80: current_loss=10.77808 | best_loss=10.31092
Epoch 26/80: current_loss=10.31308 | best_loss=10.31092
Epoch 27/80: current_loss=10.50195 | best_loss=10.31092
Epoch 28/80: current_loss=10.87792 | best_loss=10.31092
Epoch 29/80: current_loss=10.30423 | best_loss=10.30423
Epoch 30/80: current_loss=10.65318 | best_loss=10.30423
Epoch 31/80: current_loss=10.30058 | best_loss=10.30058
Epoch 32/80: current_loss=10.33100 | best_loss=10.30058
Epoch 33/80: current_loss=10.32939 | best_loss=10.30058
Epoch 34/80: current_loss=11.40289 | best_loss=10.30058
Epoch 35/80: current_loss=11.41604 | best_loss=10.30058
Epoch 36/80: current_loss=10.72449 | best_loss=10.30058
Epoch 37/80: current_loss=10.60328 | best_loss=10.30058
Epoch 38/80: current_loss=10.50297 | best_loss=10.30058
Epoch 39/80: current_loss=10.44510 | best_loss=10.30058
Epoch 40/80: current_loss=10.33443 | best_loss=10.30058
Epoch 41/80: current_loss=11.48895 | best_loss=10.30058
Epoch 42/80: current_loss=10.43496 | best_loss=10.30058
Epoch 43/80: current_loss=10.43387 | best_loss=10.30058
Epoch 44/80: current_loss=10.71179 | best_loss=10.30058
Epoch 45/80: current_loss=10.52957 | best_loss=10.30058
Epoch 46/80: current_loss=10.69942 | best_loss=10.30058
Epoch 47/80: current_loss=10.70294 | best_loss=10.30058
Epoch 48/80: current_loss=10.71386 | best_loss=10.30058
Epoch 49/80: current_loss=10.76495 | best_loss=10.30058
Epoch 50/80: current_loss=10.28324 | best_loss=10.28324
Epoch 51/80: current_loss=10.29473 | best_loss=10.28324
Epoch 52/80: current_loss=10.35514 | best_loss=10.28324
Epoch 53/80: current_loss=10.56251 | best_loss=10.28324
Epoch 54/80: current_loss=11.82904 | best_loss=10.28324
Epoch 55/80: current_loss=10.57821 | best_loss=10.28324
Epoch 56/80: current_loss=10.29616 | best_loss=10.28324
Epoch 57/80: current_loss=10.44042 | best_loss=10.28324
Epoch 58/80: current_loss=10.52830 | best_loss=10.28324
Epoch 59/80: current_loss=10.28497 | best_loss=10.28324
Epoch 60/80: current_loss=10.31547 | best_loss=10.28324
Epoch 61/80: current_loss=10.35010 | best_loss=10.28324
Epoch 62/80: current_loss=11.25278 | best_loss=10.28324
Epoch 63/80: current_loss=10.28929 | best_loss=10.28324
Epoch 64/80: current_loss=10.28880 | best_loss=10.28324
Epoch 65/80: current_loss=10.31890 | best_loss=10.28324
Epoch 66/80: current_loss=10.67952 | best_loss=10.28324
Epoch 67/80: current_loss=11.83174 | best_loss=10.28324
Epoch 68/80: current_loss=10.34773 | best_loss=10.28324
Epoch 69/80: current_loss=10.38867 | best_loss=10.28324
Epoch 70/80: current_loss=10.48217 | best_loss=10.28324
Early Stopping at epoch 70
      explained_var=-0.00500 | mse_loss=10.51484
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=12.41571 | best_loss=12.41571
Epoch 1/80: current_loss=10.43177 | best_loss=10.43177
Epoch 2/80: current_loss=10.49502 | best_loss=10.43177
Epoch 3/80: current_loss=10.43742 | best_loss=10.43177
Epoch 4/80: current_loss=10.44014 | best_loss=10.43177
Epoch 5/80: current_loss=10.40626 | best_loss=10.40626
Epoch 6/80: current_loss=10.55992 | best_loss=10.40626
Epoch 7/80: current_loss=11.16382 | best_loss=10.40626
Epoch 8/80: current_loss=10.57404 | best_loss=10.40626
Epoch 9/80: current_loss=10.95111 | best_loss=10.40626
Epoch 10/80: current_loss=10.42995 | best_loss=10.40626
Epoch 11/80: current_loss=10.40108 | best_loss=10.40108
Epoch 12/80: current_loss=10.60282 | best_loss=10.40108
Epoch 13/80: current_loss=10.50024 | best_loss=10.40108
Epoch 14/80: current_loss=10.48645 | best_loss=10.40108
Epoch 15/80: current_loss=10.52868 | best_loss=10.40108
Epoch 16/80: current_loss=10.40703 | best_loss=10.40108
Epoch 17/80: current_loss=10.42092 | best_loss=10.40108
Epoch 18/80: current_loss=10.99644 | best_loss=10.40108
Epoch 19/80: current_loss=10.49664 | best_loss=10.40108
Epoch 20/80: current_loss=10.45808 | best_loss=10.40108
Epoch 21/80: current_loss=11.05599 | best_loss=10.40108
Epoch 22/80: current_loss=10.66207 | best_loss=10.40108
Epoch 23/80: current_loss=10.51523 | best_loss=10.40108
Epoch 24/80: current_loss=10.50075 | best_loss=10.40108
Epoch 25/80: current_loss=10.71939 | best_loss=10.40108
Epoch 26/80: current_loss=10.71325 | best_loss=10.40108
Epoch 27/80: current_loss=10.43415 | best_loss=10.40108
Epoch 28/80: current_loss=10.42223 | best_loss=10.40108
Epoch 29/80: current_loss=10.46652 | best_loss=10.40108
Epoch 30/80: current_loss=11.14232 | best_loss=10.40108
Epoch 31/80: current_loss=10.51901 | best_loss=10.40108
Early Stopping at epoch 31
      explained_var=0.00081 | mse_loss=10.05614
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.35356 | best_loss=8.35356
Epoch 1/80: current_loss=8.08403 | best_loss=8.08403
Epoch 2/80: current_loss=7.72255 | best_loss=7.72255
Epoch 3/80: current_loss=7.56661 | best_loss=7.56661
Epoch 4/80: current_loss=7.91991 | best_loss=7.56661
Epoch 5/80: current_loss=7.50968 | best_loss=7.50968
Epoch 6/80: current_loss=7.47769 | best_loss=7.47769
Epoch 7/80: current_loss=7.55140 | best_loss=7.47769
Epoch 8/80: current_loss=7.54169 | best_loss=7.47769
Epoch 9/80: current_loss=7.56988 | best_loss=7.47769
Epoch 10/80: current_loss=7.86192 | best_loss=7.47769
Epoch 11/80: current_loss=7.47752 | best_loss=7.47752
Epoch 12/80: current_loss=8.90679 | best_loss=7.47752
Epoch 13/80: current_loss=7.52714 | best_loss=7.47752
Epoch 14/80: current_loss=7.55352 | best_loss=7.47752
Epoch 15/80: current_loss=7.72232 | best_loss=7.47752
Epoch 16/80: current_loss=7.87080 | best_loss=7.47752
Epoch 17/80: current_loss=7.85035 | best_loss=7.47752
Epoch 18/80: current_loss=7.88623 | best_loss=7.47752
Epoch 19/80: current_loss=9.52448 | best_loss=7.47752
Epoch 20/80: current_loss=7.66940 | best_loss=7.47752
Epoch 21/80: current_loss=8.19245 | best_loss=7.47752
Epoch 22/80: current_loss=7.80867 | best_loss=7.47752
Epoch 23/80: current_loss=7.50284 | best_loss=7.47752
Epoch 24/80: current_loss=7.52847 | best_loss=7.47752
Epoch 25/80: current_loss=7.51413 | best_loss=7.47752
Epoch 26/80: current_loss=7.59911 | best_loss=7.47752
Epoch 27/80: current_loss=8.78963 | best_loss=7.47752
Epoch 28/80: current_loss=7.60679 | best_loss=7.47752
Epoch 29/80: current_loss=7.65682 | best_loss=7.47752
Epoch 30/80: current_loss=8.44663 | best_loss=7.47752
Epoch 31/80: current_loss=7.48692 | best_loss=7.47752
Early Stopping at epoch 31
      explained_var=-0.00009 | mse_loss=7.58379
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.85355 | best_loss=8.85355
Epoch 1/80: current_loss=9.14499 | best_loss=8.85355
Epoch 2/80: current_loss=8.94011 | best_loss=8.85355
Epoch 3/80: current_loss=9.16437 | best_loss=8.85355
Epoch 4/80: current_loss=9.29610 | best_loss=8.85355
Epoch 5/80: current_loss=8.81481 | best_loss=8.81481
Epoch 6/80: current_loss=8.99630 | best_loss=8.81481
Epoch 7/80: current_loss=8.81062 | best_loss=8.81062
Epoch 8/80: current_loss=8.83038 | best_loss=8.81062
Epoch 9/80: current_loss=9.01191 | best_loss=8.81062
Epoch 10/80: current_loss=8.78786 | best_loss=8.78786
Epoch 11/80: current_loss=8.84102 | best_loss=8.78786
Epoch 12/80: current_loss=9.44576 | best_loss=8.78786
Epoch 13/80: current_loss=8.87890 | best_loss=8.78786
Epoch 14/80: current_loss=8.78255 | best_loss=8.78255
Epoch 15/80: current_loss=8.96246 | best_loss=8.78255
Epoch 16/80: current_loss=9.32388 | best_loss=8.78255
Epoch 17/80: current_loss=8.75374 | best_loss=8.75374
Epoch 18/80: current_loss=8.90711 | best_loss=8.75374
Epoch 19/80: current_loss=8.81459 | best_loss=8.75374
Epoch 20/80: current_loss=8.77092 | best_loss=8.75374
Epoch 21/80: current_loss=9.65932 | best_loss=8.75374
Epoch 22/80: current_loss=8.80314 | best_loss=8.75374
Epoch 23/80: current_loss=8.94299 | best_loss=8.75374
Epoch 24/80: current_loss=8.95168 | best_loss=8.75374
Epoch 25/80: current_loss=8.88815 | best_loss=8.75374
Epoch 26/80: current_loss=8.82326 | best_loss=8.75374
Epoch 27/80: current_loss=10.34282 | best_loss=8.75374
Epoch 28/80: current_loss=8.78471 | best_loss=8.75374
Epoch 29/80: current_loss=8.86767 | best_loss=8.75374
Epoch 30/80: current_loss=8.82871 | best_loss=8.75374
Epoch 31/80: current_loss=8.82931 | best_loss=8.75374
Epoch 32/80: current_loss=8.82052 | best_loss=8.75374
Epoch 33/80: current_loss=9.00244 | best_loss=8.75374
Epoch 34/80: current_loss=8.89856 | best_loss=8.75374
Epoch 35/80: current_loss=8.95380 | best_loss=8.75374
Epoch 36/80: current_loss=8.82161 | best_loss=8.75374
Epoch 37/80: current_loss=8.76786 | best_loss=8.75374
Early Stopping at epoch 37
      explained_var=0.00145 | mse_loss=8.71203
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.61435 | best_loss=9.61435
Epoch 1/80: current_loss=9.41654 | best_loss=9.41654
Epoch 2/80: current_loss=9.68484 | best_loss=9.41654
Epoch 3/80: current_loss=9.72851 | best_loss=9.41654
Epoch 4/80: current_loss=9.53848 | best_loss=9.41654
Epoch 5/80: current_loss=9.44581 | best_loss=9.41654
Epoch 6/80: current_loss=9.43036 | best_loss=9.41654
Epoch 7/80: current_loss=9.56839 | best_loss=9.41654
Epoch 8/80: current_loss=9.65597 | best_loss=9.41654
Epoch 9/80: current_loss=9.45648 | best_loss=9.41654
Epoch 10/80: current_loss=9.52477 | best_loss=9.41654
Epoch 11/80: current_loss=9.41540 | best_loss=9.41540
Epoch 12/80: current_loss=9.63631 | best_loss=9.41540
Epoch 13/80: current_loss=9.46833 | best_loss=9.41540
Epoch 14/80: current_loss=9.59077 | best_loss=9.41540
Epoch 15/80: current_loss=9.46566 | best_loss=9.41540
Epoch 16/80: current_loss=9.40819 | best_loss=9.40819
Epoch 17/80: current_loss=9.43374 | best_loss=9.40819
Epoch 18/80: current_loss=9.46596 | best_loss=9.40819
Epoch 19/80: current_loss=9.61047 | best_loss=9.40819
Epoch 20/80: current_loss=9.64332 | best_loss=9.40819
Epoch 21/80: current_loss=9.60642 | best_loss=9.40819
Epoch 22/80: current_loss=9.40951 | best_loss=9.40819
Epoch 23/80: current_loss=9.50368 | best_loss=9.40819
Epoch 24/80: current_loss=9.66013 | best_loss=9.40819
Epoch 25/80: current_loss=9.42320 | best_loss=9.40819
Epoch 26/80: current_loss=9.43849 | best_loss=9.40819
Epoch 27/80: current_loss=9.43291 | best_loss=9.40819
Epoch 28/80: current_loss=9.44597 | best_loss=9.40819
Epoch 29/80: current_loss=9.38375 | best_loss=9.38375
Epoch 30/80: current_loss=10.01714 | best_loss=9.38375
Epoch 31/80: current_loss=9.44012 | best_loss=9.38375
Epoch 32/80: current_loss=9.88885 | best_loss=9.38375
Epoch 33/80: current_loss=9.55044 | best_loss=9.38375
Epoch 34/80: current_loss=9.52765 | best_loss=9.38375
Epoch 35/80: current_loss=10.24970 | best_loss=9.38375
Epoch 36/80: current_loss=9.49585 | best_loss=9.38375
Epoch 37/80: current_loss=9.54564 | best_loss=9.38375
Epoch 38/80: current_loss=9.79120 | best_loss=9.38375
Epoch 39/80: current_loss=9.55519 | best_loss=9.38375
Epoch 40/80: current_loss=9.97641 | best_loss=9.38375
Epoch 41/80: current_loss=9.41050 | best_loss=9.38375
Epoch 42/80: current_loss=9.83534 | best_loss=9.38375
Epoch 43/80: current_loss=9.40462 | best_loss=9.38375
Epoch 44/80: current_loss=10.68203 | best_loss=9.38375
Epoch 45/80: current_loss=9.75926 | best_loss=9.38375
Epoch 46/80: current_loss=9.63062 | best_loss=9.38375
Epoch 47/80: current_loss=9.41631 | best_loss=9.38375
Epoch 48/80: current_loss=9.47189 | best_loss=9.38375
Epoch 49/80: current_loss=9.81808 | best_loss=9.38375
Early Stopping at epoch 49
      explained_var=-0.00099 | mse_loss=9.31130
----------------------------------------------
Average early_stopping_point: 23| avg_exp_var=-0.00077| avg_loss=9.23562
----------------------------------------------


----------------------------------------------
Params for Trial 16
{'learning_rate': 0.001, 'weight_decay': 0.009753589025733351, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=15.88699 | best_loss=15.88699
Epoch 1/80: current_loss=10.67631 | best_loss=10.67631
Epoch 2/80: current_loss=10.39673 | best_loss=10.39673
Epoch 3/80: current_loss=10.36313 | best_loss=10.36313
Epoch 4/80: current_loss=10.34944 | best_loss=10.34944
Epoch 5/80: current_loss=10.32733 | best_loss=10.32733
Epoch 6/80: current_loss=10.33984 | best_loss=10.32733
Epoch 7/80: current_loss=10.30149 | best_loss=10.30149
Epoch 8/80: current_loss=10.29178 | best_loss=10.29178
Epoch 9/80: current_loss=10.34940 | best_loss=10.29178
Epoch 10/80: current_loss=10.29660 | best_loss=10.29178
Epoch 11/80: current_loss=10.30407 | best_loss=10.29178
Epoch 12/80: current_loss=10.28609 | best_loss=10.28609
Epoch 13/80: current_loss=10.29767 | best_loss=10.28609
Epoch 14/80: current_loss=10.29500 | best_loss=10.28609
Epoch 15/80: current_loss=10.28762 | best_loss=10.28609
Epoch 16/80: current_loss=10.30681 | best_loss=10.28609
Epoch 17/80: current_loss=10.32893 | best_loss=10.28609
Epoch 18/80: current_loss=10.29905 | best_loss=10.28609
Epoch 19/80: current_loss=10.30951 | best_loss=10.28609
Epoch 20/80: current_loss=10.28711 | best_loss=10.28609
Epoch 21/80: current_loss=10.27633 | best_loss=10.27633
Epoch 22/80: current_loss=10.32499 | best_loss=10.27633
Epoch 23/80: current_loss=10.31480 | best_loss=10.27633
Epoch 24/80: current_loss=10.27909 | best_loss=10.27633
Epoch 25/80: current_loss=10.30220 | best_loss=10.27633
Epoch 26/80: current_loss=10.31315 | best_loss=10.27633
Epoch 27/80: current_loss=10.30460 | best_loss=10.27633
Epoch 28/80: current_loss=10.27471 | best_loss=10.27471
Epoch 29/80: current_loss=10.28882 | best_loss=10.27471
Epoch 30/80: current_loss=10.27112 | best_loss=10.27112
Epoch 31/80: current_loss=10.28832 | best_loss=10.27112
Epoch 32/80: current_loss=10.31439 | best_loss=10.27112
Epoch 33/80: current_loss=10.30052 | best_loss=10.27112
Epoch 34/80: current_loss=10.32682 | best_loss=10.27112
Epoch 35/80: current_loss=10.29598 | best_loss=10.27112
Epoch 36/80: current_loss=10.28890 | best_loss=10.27112
Epoch 37/80: current_loss=10.33414 | best_loss=10.27112
Epoch 38/80: current_loss=10.27873 | best_loss=10.27112
Epoch 39/80: current_loss=10.33041 | best_loss=10.27112
Epoch 40/80: current_loss=10.33398 | best_loss=10.27112
Epoch 41/80: current_loss=10.29672 | best_loss=10.27112
Epoch 42/80: current_loss=10.27862 | best_loss=10.27112
Epoch 43/80: current_loss=10.37675 | best_loss=10.27112
Epoch 44/80: current_loss=10.27313 | best_loss=10.27112
Epoch 45/80: current_loss=10.29526 | best_loss=10.27112
Epoch 46/80: current_loss=10.30952 | best_loss=10.27112
Epoch 47/80: current_loss=10.33570 | best_loss=10.27112
Epoch 48/80: current_loss=10.28389 | best_loss=10.27112
Epoch 49/80: current_loss=10.27727 | best_loss=10.27112
Epoch 50/80: current_loss=10.31673 | best_loss=10.27112
Early Stopping at epoch 50
      explained_var=-0.00264 | mse_loss=10.49625
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.59464 | best_loss=10.59464
Epoch 1/80: current_loss=10.43873 | best_loss=10.43873
Epoch 2/80: current_loss=10.41519 | best_loss=10.41519
Epoch 3/80: current_loss=10.44233 | best_loss=10.41519
Epoch 4/80: current_loss=10.45911 | best_loss=10.41519
Epoch 5/80: current_loss=10.40694 | best_loss=10.40694
Epoch 6/80: current_loss=10.43940 | best_loss=10.40694
Epoch 7/80: current_loss=10.44393 | best_loss=10.40694
Epoch 8/80: current_loss=10.41699 | best_loss=10.40694
Epoch 9/80: current_loss=10.47916 | best_loss=10.40694
Epoch 10/80: current_loss=10.43214 | best_loss=10.40694
Epoch 11/80: current_loss=10.43856 | best_loss=10.40694
Epoch 12/80: current_loss=10.41473 | best_loss=10.40694
Epoch 13/80: current_loss=10.42078 | best_loss=10.40694
Epoch 14/80: current_loss=10.46344 | best_loss=10.40694
Epoch 15/80: current_loss=10.41595 | best_loss=10.40694
Epoch 16/80: current_loss=10.49711 | best_loss=10.40694
Epoch 17/80: current_loss=10.67099 | best_loss=10.40694
Epoch 18/80: current_loss=10.40865 | best_loss=10.40694
Epoch 19/80: current_loss=10.41947 | best_loss=10.40694
Epoch 20/80: current_loss=10.47899 | best_loss=10.40694
Epoch 21/80: current_loss=10.44760 | best_loss=10.40694
Epoch 22/80: current_loss=10.42782 | best_loss=10.40694
Epoch 23/80: current_loss=10.41104 | best_loss=10.40694
Epoch 24/80: current_loss=10.44215 | best_loss=10.40694
Epoch 25/80: current_loss=10.43996 | best_loss=10.40694
Early Stopping at epoch 25
      explained_var=0.00029 | mse_loss=10.06127
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.54909 | best_loss=7.54909
Epoch 1/80: current_loss=7.90508 | best_loss=7.54909
Epoch 2/80: current_loss=7.50111 | best_loss=7.50111
Epoch 3/80: current_loss=7.57194 | best_loss=7.50111
Epoch 4/80: current_loss=7.53412 | best_loss=7.50111
Epoch 5/80: current_loss=7.61159 | best_loss=7.50111
Epoch 6/80: current_loss=7.51757 | best_loss=7.50111
Epoch 7/80: current_loss=7.53209 | best_loss=7.50111
Epoch 8/80: current_loss=7.56493 | best_loss=7.50111
Epoch 9/80: current_loss=7.58320 | best_loss=7.50111
Epoch 10/80: current_loss=7.69374 | best_loss=7.50111
Epoch 11/80: current_loss=7.53241 | best_loss=7.50111
Epoch 12/80: current_loss=7.67109 | best_loss=7.50111
Epoch 13/80: current_loss=7.62536 | best_loss=7.50111
Epoch 14/80: current_loss=7.46075 | best_loss=7.46075
Epoch 15/80: current_loss=7.62045 | best_loss=7.46075
Epoch 16/80: current_loss=7.62270 | best_loss=7.46075
Epoch 17/80: current_loss=7.56537 | best_loss=7.46075
Epoch 18/80: current_loss=7.46794 | best_loss=7.46075
Epoch 19/80: current_loss=7.63940 | best_loss=7.46075
Epoch 20/80: current_loss=7.53443 | best_loss=7.46075
Epoch 21/80: current_loss=7.53605 | best_loss=7.46075
Epoch 22/80: current_loss=7.53005 | best_loss=7.46075
Epoch 23/80: current_loss=7.60844 | best_loss=7.46075
Epoch 24/80: current_loss=7.52118 | best_loss=7.46075
Epoch 25/80: current_loss=7.56168 | best_loss=7.46075
Epoch 26/80: current_loss=7.49499 | best_loss=7.46075
Epoch 27/80: current_loss=7.49855 | best_loss=7.46075
Epoch 28/80: current_loss=7.63452 | best_loss=7.46075
Epoch 29/80: current_loss=7.45526 | best_loss=7.45526
Epoch 30/80: current_loss=7.65804 | best_loss=7.45526
Epoch 31/80: current_loss=7.53490 | best_loss=7.45526
Epoch 32/80: current_loss=7.74716 | best_loss=7.45526
Epoch 33/80: current_loss=7.48984 | best_loss=7.45526
Epoch 34/80: current_loss=7.55251 | best_loss=7.45526
Epoch 35/80: current_loss=7.52067 | best_loss=7.45526
Epoch 36/80: current_loss=7.67035 | best_loss=7.45526
Epoch 37/80: current_loss=7.46873 | best_loss=7.45526
Epoch 38/80: current_loss=7.71679 | best_loss=7.45526
Epoch 39/80: current_loss=7.50175 | best_loss=7.45526
Epoch 40/80: current_loss=7.54514 | best_loss=7.45526
Epoch 41/80: current_loss=7.53007 | best_loss=7.45526
Epoch 42/80: current_loss=7.60390 | best_loss=7.45526
Epoch 43/80: current_loss=7.57277 | best_loss=7.45526
Epoch 44/80: current_loss=7.55163 | best_loss=7.45526
Epoch 45/80: current_loss=7.54397 | best_loss=7.45526
Epoch 46/80: current_loss=7.55752 | best_loss=7.45526
Epoch 47/80: current_loss=7.55089 | best_loss=7.45526
Epoch 48/80: current_loss=7.61419 | best_loss=7.45526
Epoch 49/80: current_loss=7.50306 | best_loss=7.45526
Early Stopping at epoch 49
      explained_var=0.00171 | mse_loss=7.57023
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.85368 | best_loss=8.85368
Epoch 1/80: current_loss=8.82884 | best_loss=8.82884
Epoch 2/80: current_loss=9.07517 | best_loss=8.82884
Epoch 3/80: current_loss=8.78145 | best_loss=8.78145
Epoch 4/80: current_loss=8.80376 | best_loss=8.78145
Epoch 5/80: current_loss=8.93064 | best_loss=8.78145
Epoch 6/80: current_loss=8.80781 | best_loss=8.78145
Epoch 7/80: current_loss=8.78851 | best_loss=8.78145
Epoch 8/80: current_loss=8.89529 | best_loss=8.78145
Epoch 9/80: current_loss=8.84208 | best_loss=8.78145
Epoch 10/80: current_loss=8.82886 | best_loss=8.78145
Epoch 11/80: current_loss=8.80383 | best_loss=8.78145
Epoch 12/80: current_loss=8.79507 | best_loss=8.78145
Epoch 13/80: current_loss=8.73789 | best_loss=8.73789
Epoch 14/80: current_loss=8.78161 | best_loss=8.73789
Epoch 15/80: current_loss=9.01921 | best_loss=8.73789
Epoch 16/80: current_loss=8.76750 | best_loss=8.73789
Epoch 17/80: current_loss=8.79710 | best_loss=8.73789
Epoch 18/80: current_loss=8.84105 | best_loss=8.73789
Epoch 19/80: current_loss=8.83139 | best_loss=8.73789
Epoch 20/80: current_loss=8.82533 | best_loss=8.73789
Epoch 21/80: current_loss=8.84198 | best_loss=8.73789
Epoch 22/80: current_loss=8.97959 | best_loss=8.73789
Epoch 23/80: current_loss=8.81700 | best_loss=8.73789
Epoch 24/80: current_loss=8.79967 | best_loss=8.73789
Epoch 25/80: current_loss=8.77773 | best_loss=8.73789
Epoch 26/80: current_loss=9.09542 | best_loss=8.73789
Epoch 27/80: current_loss=8.83313 | best_loss=8.73789
Epoch 28/80: current_loss=8.81698 | best_loss=8.73789
Epoch 29/80: current_loss=8.80496 | best_loss=8.73789
Epoch 30/80: current_loss=8.78743 | best_loss=8.73789
Epoch 31/80: current_loss=8.77764 | best_loss=8.73789
Epoch 32/80: current_loss=8.78727 | best_loss=8.73789
Epoch 33/80: current_loss=8.75416 | best_loss=8.73789
Early Stopping at epoch 33
      explained_var=0.00313 | mse_loss=8.69754
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.26967 | best_loss=9.26967
Epoch 1/80: current_loss=9.21956 | best_loss=9.21956
Epoch 2/80: current_loss=9.15123 | best_loss=9.15123
Epoch 3/80: current_loss=9.28772 | best_loss=9.15123
Epoch 4/80: current_loss=9.25501 | best_loss=9.15123
Epoch 5/80: current_loss=9.13182 | best_loss=9.13182
Epoch 6/80: current_loss=9.15805 | best_loss=9.13182
Epoch 7/80: current_loss=9.28070 | best_loss=9.13182
Epoch 8/80: current_loss=9.18788 | best_loss=9.13182
Epoch 9/80: current_loss=9.25305 | best_loss=9.13182
Epoch 10/80: current_loss=9.19564 | best_loss=9.13182
Epoch 11/80: current_loss=9.24013 | best_loss=9.13182
Epoch 12/80: current_loss=9.16775 | best_loss=9.13182
Epoch 13/80: current_loss=9.27515 | best_loss=9.13182
Epoch 14/80: current_loss=9.34533 | best_loss=9.13182
Epoch 15/80: current_loss=9.21809 | best_loss=9.13182
Epoch 16/80: current_loss=9.16763 | best_loss=9.13182
Epoch 17/80: current_loss=9.26294 | best_loss=9.13182
Epoch 18/80: current_loss=9.19118 | best_loss=9.13182
Epoch 19/80: current_loss=9.24892 | best_loss=9.13182
Epoch 20/80: current_loss=9.25429 | best_loss=9.13182
Epoch 21/80: current_loss=9.20699 | best_loss=9.13182
Epoch 22/80: current_loss=9.23659 | best_loss=9.13182
Epoch 23/80: current_loss=9.19174 | best_loss=9.13182
Epoch 24/80: current_loss=9.36388 | best_loss=9.13182
Epoch 25/80: current_loss=9.24075 | best_loss=9.13182
Early Stopping at epoch 25
      explained_var=0.02179 | mse_loss=9.09100
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.00486| avg_loss=9.18326
----------------------------------------------


----------------------------------------------
Params for Trial 17
{'learning_rate': 0.001, 'weight_decay': 0.0031079134026941553, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.83930 | best_loss=16.83930
Epoch 1/80: current_loss=11.88486 | best_loss=11.88486
Epoch 2/80: current_loss=11.54493 | best_loss=11.54493
Epoch 3/80: current_loss=11.29416 | best_loss=11.29416
Epoch 4/80: current_loss=11.06588 | best_loss=11.06588
Epoch 5/80: current_loss=10.88534 | best_loss=10.88534
Epoch 6/80: current_loss=10.76617 | best_loss=10.76617
Epoch 7/80: current_loss=10.68807 | best_loss=10.68807
Epoch 8/80: current_loss=10.58075 | best_loss=10.58075
Epoch 9/80: current_loss=10.51045 | best_loss=10.51045
Epoch 10/80: current_loss=10.46011 | best_loss=10.46011
Epoch 11/80: current_loss=10.41176 | best_loss=10.41176
Epoch 12/80: current_loss=10.40594 | best_loss=10.40594
Epoch 13/80: current_loss=10.37172 | best_loss=10.37172
Epoch 14/80: current_loss=10.39359 | best_loss=10.37172
Epoch 15/80: current_loss=10.36462 | best_loss=10.36462
Epoch 16/80: current_loss=10.34744 | best_loss=10.34744
Epoch 17/80: current_loss=10.35510 | best_loss=10.34744
Epoch 18/80: current_loss=10.32030 | best_loss=10.32030
Epoch 19/80: current_loss=10.33010 | best_loss=10.32030
Epoch 20/80: current_loss=10.32434 | best_loss=10.32030
Epoch 21/80: current_loss=10.31770 | best_loss=10.31770
Epoch 22/80: current_loss=10.33954 | best_loss=10.31770
Epoch 23/80: current_loss=10.30234 | best_loss=10.30234
Epoch 24/80: current_loss=10.31319 | best_loss=10.30234
Epoch 25/80: current_loss=10.34105 | best_loss=10.30234
Epoch 26/80: current_loss=10.29075 | best_loss=10.29075
Epoch 27/80: current_loss=10.29536 | best_loss=10.29075
Epoch 28/80: current_loss=10.32250 | best_loss=10.29075
Epoch 29/80: current_loss=10.29511 | best_loss=10.29075
Epoch 30/80: current_loss=10.31915 | best_loss=10.29075
Epoch 31/80: current_loss=10.30775 | best_loss=10.29075
Epoch 32/80: current_loss=10.29435 | best_loss=10.29075
Epoch 33/80: current_loss=10.32234 | best_loss=10.29075
Epoch 34/80: current_loss=10.30775 | best_loss=10.29075
Epoch 35/80: current_loss=10.32177 | best_loss=10.29075
Epoch 36/80: current_loss=10.29058 | best_loss=10.29058
Epoch 37/80: current_loss=10.29901 | best_loss=10.29058
Epoch 38/80: current_loss=10.28428 | best_loss=10.28428
Epoch 39/80: current_loss=10.28580 | best_loss=10.28428
Epoch 40/80: current_loss=10.28487 | best_loss=10.28428
Epoch 41/80: current_loss=10.28981 | best_loss=10.28428
Epoch 42/80: current_loss=10.30023 | best_loss=10.28428
Epoch 43/80: current_loss=10.28214 | best_loss=10.28214
Epoch 44/80: current_loss=10.36300 | best_loss=10.28214
Epoch 45/80: current_loss=10.29116 | best_loss=10.28214
Epoch 46/80: current_loss=10.34748 | best_loss=10.28214
Epoch 47/80: current_loss=10.28159 | best_loss=10.28159
Epoch 48/80: current_loss=10.30026 | best_loss=10.28159
Epoch 49/80: current_loss=10.28848 | best_loss=10.28159
Epoch 50/80: current_loss=10.29859 | best_loss=10.28159
Epoch 51/80: current_loss=10.29539 | best_loss=10.28159
Epoch 52/80: current_loss=10.29640 | best_loss=10.28159
Epoch 53/80: current_loss=10.28135 | best_loss=10.28135
Epoch 54/80: current_loss=10.29674 | best_loss=10.28135
Epoch 55/80: current_loss=10.32355 | best_loss=10.28135
Epoch 56/80: current_loss=10.31623 | best_loss=10.28135
Epoch 57/80: current_loss=10.29638 | best_loss=10.28135
Epoch 58/80: current_loss=10.28410 | best_loss=10.28135
Epoch 59/80: current_loss=10.33572 | best_loss=10.28135
Epoch 60/80: current_loss=10.28522 | best_loss=10.28135
Epoch 61/80: current_loss=10.28496 | best_loss=10.28135
Epoch 62/80: current_loss=10.28490 | best_loss=10.28135
Epoch 63/80: current_loss=10.31778 | best_loss=10.28135
Epoch 64/80: current_loss=10.27974 | best_loss=10.27974
Epoch 65/80: current_loss=10.27996 | best_loss=10.27974
Epoch 66/80: current_loss=10.27926 | best_loss=10.27926
Epoch 67/80: current_loss=10.29763 | best_loss=10.27926
Epoch 68/80: current_loss=10.32126 | best_loss=10.27926
Epoch 69/80: current_loss=10.28976 | best_loss=10.27926
Epoch 70/80: current_loss=10.31657 | best_loss=10.27926
Epoch 71/80: current_loss=10.28074 | best_loss=10.27926
Epoch 72/80: current_loss=10.32064 | best_loss=10.27926
Epoch 73/80: current_loss=10.29624 | best_loss=10.27926
Epoch 74/80: current_loss=10.27576 | best_loss=10.27576
Epoch 75/80: current_loss=10.29619 | best_loss=10.27576
Epoch 76/80: current_loss=10.28747 | best_loss=10.27576
Epoch 77/80: current_loss=10.30047 | best_loss=10.27576
Epoch 78/80: current_loss=10.27925 | best_loss=10.27576
Epoch 79/80: current_loss=10.28416 | best_loss=10.27576
      explained_var=-0.00301 | mse_loss=10.50278
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.41174 | best_loss=10.41174
Epoch 1/80: current_loss=10.40434 | best_loss=10.40434
Epoch 2/80: current_loss=10.47841 | best_loss=10.40434
Epoch 3/80: current_loss=10.40311 | best_loss=10.40311
Epoch 4/80: current_loss=10.40971 | best_loss=10.40311
Epoch 5/80: current_loss=10.42759 | best_loss=10.40311
Epoch 6/80: current_loss=10.41800 | best_loss=10.40311
Epoch 7/80: current_loss=10.40419 | best_loss=10.40311
Epoch 8/80: current_loss=10.41209 | best_loss=10.40311
Epoch 9/80: current_loss=10.41017 | best_loss=10.40311
Epoch 10/80: current_loss=10.41399 | best_loss=10.40311
Epoch 11/80: current_loss=10.44303 | best_loss=10.40311
Epoch 12/80: current_loss=10.50987 | best_loss=10.40311
Epoch 13/80: current_loss=10.42272 | best_loss=10.40311
Epoch 14/80: current_loss=10.40991 | best_loss=10.40311
Epoch 15/80: current_loss=10.44884 | best_loss=10.40311
Epoch 16/80: current_loss=10.43396 | best_loss=10.40311
Epoch 17/80: current_loss=10.46464 | best_loss=10.40311
Epoch 18/80: current_loss=10.40607 | best_loss=10.40311
Epoch 19/80: current_loss=10.43027 | best_loss=10.40311
Epoch 20/80: current_loss=10.40670 | best_loss=10.40311
Epoch 21/80: current_loss=10.42777 | best_loss=10.40311
Epoch 22/80: current_loss=10.40529 | best_loss=10.40311
Epoch 23/80: current_loss=10.41803 | best_loss=10.40311
Early Stopping at epoch 23
      explained_var=0.00091 | mse_loss=10.06275
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.56739 | best_loss=7.56739
Epoch 1/80: current_loss=7.48745 | best_loss=7.48745
Epoch 2/80: current_loss=7.62045 | best_loss=7.48745
Epoch 3/80: current_loss=7.62424 | best_loss=7.48745
Epoch 4/80: current_loss=7.52557 | best_loss=7.48745
Epoch 5/80: current_loss=7.58680 | best_loss=7.48745
Epoch 6/80: current_loss=7.56080 | best_loss=7.48745
Epoch 7/80: current_loss=7.48104 | best_loss=7.48104
Epoch 8/80: current_loss=7.80539 | best_loss=7.48104
Epoch 9/80: current_loss=7.90000 | best_loss=7.48104
Epoch 10/80: current_loss=7.55715 | best_loss=7.48104
Epoch 11/80: current_loss=7.64213 | best_loss=7.48104
Epoch 12/80: current_loss=7.55251 | best_loss=7.48104
Epoch 13/80: current_loss=7.68625 | best_loss=7.48104
Epoch 14/80: current_loss=7.56546 | best_loss=7.48104
Epoch 15/80: current_loss=7.59291 | best_loss=7.48104
Epoch 16/80: current_loss=7.62368 | best_loss=7.48104
Epoch 17/80: current_loss=7.52979 | best_loss=7.48104
Epoch 18/80: current_loss=7.65535 | best_loss=7.48104
Epoch 19/80: current_loss=7.45879 | best_loss=7.45879
Epoch 20/80: current_loss=7.55076 | best_loss=7.45879
Epoch 21/80: current_loss=7.58334 | best_loss=7.45879
Epoch 22/80: current_loss=7.68318 | best_loss=7.45879
Epoch 23/80: current_loss=7.66167 | best_loss=7.45879
Epoch 24/80: current_loss=7.57974 | best_loss=7.45879
Epoch 25/80: current_loss=7.55804 | best_loss=7.45879
Epoch 26/80: current_loss=7.55775 | best_loss=7.45879
Epoch 27/80: current_loss=7.59582 | best_loss=7.45879
Epoch 28/80: current_loss=7.65644 | best_loss=7.45879
Epoch 29/80: current_loss=8.06650 | best_loss=7.45879
Epoch 30/80: current_loss=7.51092 | best_loss=7.45879
Epoch 31/80: current_loss=7.63632 | best_loss=7.45879
Epoch 32/80: current_loss=7.52768 | best_loss=7.45879
Epoch 33/80: current_loss=7.52288 | best_loss=7.45879
Epoch 34/80: current_loss=7.76542 | best_loss=7.45879
Epoch 35/80: current_loss=7.56054 | best_loss=7.45879
Epoch 36/80: current_loss=7.48168 | best_loss=7.45879
Epoch 37/80: current_loss=7.69882 | best_loss=7.45879
Epoch 38/80: current_loss=7.58297 | best_loss=7.45879
Epoch 39/80: current_loss=7.67239 | best_loss=7.45879
Early Stopping at epoch 39
      explained_var=0.00219 | mse_loss=7.56983
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.78119 | best_loss=8.78119
Epoch 1/80: current_loss=8.75168 | best_loss=8.75168
Epoch 2/80: current_loss=8.76560 | best_loss=8.75168
Epoch 3/80: current_loss=8.78973 | best_loss=8.75168
Epoch 4/80: current_loss=8.83035 | best_loss=8.75168
Epoch 5/80: current_loss=8.74714 | best_loss=8.74714
Epoch 6/80: current_loss=8.85456 | best_loss=8.74714
Epoch 7/80: current_loss=8.78991 | best_loss=8.74714
Epoch 8/80: current_loss=8.93092 | best_loss=8.74714
Epoch 9/80: current_loss=8.88960 | best_loss=8.74714
Epoch 10/80: current_loss=8.82941 | best_loss=8.74714
Epoch 11/80: current_loss=8.78258 | best_loss=8.74714
Epoch 12/80: current_loss=8.82100 | best_loss=8.74714
Epoch 13/80: current_loss=8.81696 | best_loss=8.74714
Epoch 14/80: current_loss=8.93709 | best_loss=8.74714
Epoch 15/80: current_loss=8.96561 | best_loss=8.74714
Epoch 16/80: current_loss=8.86717 | best_loss=8.74714
Epoch 17/80: current_loss=8.82661 | best_loss=8.74714
Epoch 18/80: current_loss=8.72442 | best_loss=8.72442
Epoch 19/80: current_loss=8.77739 | best_loss=8.72442
Epoch 20/80: current_loss=8.96458 | best_loss=8.72442
Epoch 21/80: current_loss=8.85552 | best_loss=8.72442
Epoch 22/80: current_loss=8.84055 | best_loss=8.72442
Epoch 23/80: current_loss=8.84889 | best_loss=8.72442
Epoch 24/80: current_loss=8.91692 | best_loss=8.72442
Epoch 25/80: current_loss=8.82618 | best_loss=8.72442
Epoch 26/80: current_loss=8.80168 | best_loss=8.72442
Epoch 27/80: current_loss=8.79277 | best_loss=8.72442
Epoch 28/80: current_loss=8.93253 | best_loss=8.72442
Epoch 29/80: current_loss=9.07688 | best_loss=8.72442
Epoch 30/80: current_loss=8.79092 | best_loss=8.72442
Epoch 31/80: current_loss=8.79454 | best_loss=8.72442
Epoch 32/80: current_loss=8.86136 | best_loss=8.72442
Epoch 33/80: current_loss=8.82977 | best_loss=8.72442
Epoch 34/80: current_loss=8.81281 | best_loss=8.72442
Epoch 35/80: current_loss=8.82160 | best_loss=8.72442
Epoch 36/80: current_loss=8.90717 | best_loss=8.72442
Epoch 37/80: current_loss=9.01506 | best_loss=8.72442
Epoch 38/80: current_loss=8.79449 | best_loss=8.72442
Early Stopping at epoch 38
      explained_var=0.00329 | mse_loss=8.69541
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.26960 | best_loss=9.26960
Epoch 1/80: current_loss=9.14829 | best_loss=9.14829
Epoch 2/80: current_loss=9.16494 | best_loss=9.14829
Epoch 3/80: current_loss=9.16399 | best_loss=9.14829
Epoch 4/80: current_loss=9.15961 | best_loss=9.14829
Epoch 5/80: current_loss=9.15758 | best_loss=9.14829
Epoch 6/80: current_loss=9.17510 | best_loss=9.14829
Epoch 7/80: current_loss=9.20839 | best_loss=9.14829
Epoch 8/80: current_loss=9.17004 | best_loss=9.14829
Epoch 9/80: current_loss=9.16497 | best_loss=9.14829
Epoch 10/80: current_loss=9.18704 | best_loss=9.14829
Epoch 11/80: current_loss=9.16465 | best_loss=9.14829
Epoch 12/80: current_loss=9.18599 | best_loss=9.14829
Epoch 13/80: current_loss=9.14207 | best_loss=9.14207
Epoch 14/80: current_loss=9.20737 | best_loss=9.14207
Epoch 15/80: current_loss=9.15793 | best_loss=9.14207
Epoch 16/80: current_loss=9.14307 | best_loss=9.14207
Epoch 17/80: current_loss=9.14156 | best_loss=9.14156
Epoch 18/80: current_loss=9.16913 | best_loss=9.14156
Epoch 19/80: current_loss=9.14896 | best_loss=9.14156
Epoch 20/80: current_loss=9.17062 | best_loss=9.14156
Epoch 21/80: current_loss=9.15813 | best_loss=9.14156
Epoch 22/80: current_loss=9.24152 | best_loss=9.14156
Epoch 23/80: current_loss=9.23277 | best_loss=9.14156
Epoch 24/80: current_loss=9.17341 | best_loss=9.14156
Epoch 25/80: current_loss=9.16055 | best_loss=9.14156
Epoch 26/80: current_loss=9.16780 | best_loss=9.14156
Epoch 27/80: current_loss=9.17608 | best_loss=9.14156
Epoch 28/80: current_loss=9.21830 | best_loss=9.14156
Epoch 29/80: current_loss=9.17594 | best_loss=9.14156
Epoch 30/80: current_loss=9.23006 | best_loss=9.14156
Epoch 31/80: current_loss=9.19602 | best_loss=9.14156
Epoch 32/80: current_loss=9.20905 | best_loss=9.14156
Epoch 33/80: current_loss=9.18229 | best_loss=9.14156
Epoch 34/80: current_loss=9.19645 | best_loss=9.14156
Epoch 35/80: current_loss=9.15571 | best_loss=9.14156
Epoch 36/80: current_loss=9.15510 | best_loss=9.14156
Epoch 37/80: current_loss=9.16288 | best_loss=9.14156
Early Stopping at epoch 37
      explained_var=0.02071 | mse_loss=9.10172
----------------------------------------------
Average early_stopping_point: 27| avg_exp_var=0.00482| avg_loss=9.18650
----------------------------------------------


----------------------------------------------
Params for Trial 18
{'learning_rate': 0.001, 'weight_decay': 0.008754824227657044, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.91559 | best_loss=16.91559
Epoch 1/80: current_loss=10.73762 | best_loss=10.73762
Epoch 2/80: current_loss=10.43230 | best_loss=10.43230
Epoch 3/80: current_loss=10.39218 | best_loss=10.39218
Epoch 4/80: current_loss=10.37179 | best_loss=10.37179
Epoch 5/80: current_loss=10.35569 | best_loss=10.35569
Epoch 6/80: current_loss=10.34104 | best_loss=10.34104
Epoch 7/80: current_loss=10.29859 | best_loss=10.29859
Epoch 8/80: current_loss=10.30974 | best_loss=10.29859
Epoch 9/80: current_loss=10.31353 | best_loss=10.29859
Epoch 10/80: current_loss=10.33201 | best_loss=10.29859
Epoch 11/80: current_loss=10.30624 | best_loss=10.29859
Epoch 12/80: current_loss=10.31632 | best_loss=10.29859
Epoch 13/80: current_loss=10.30270 | best_loss=10.29859
Epoch 14/80: current_loss=10.32677 | best_loss=10.29859
Epoch 15/80: current_loss=10.33036 | best_loss=10.29859
Epoch 16/80: current_loss=10.30207 | best_loss=10.29859
Epoch 17/80: current_loss=10.30733 | best_loss=10.29859
Epoch 18/80: current_loss=10.33051 | best_loss=10.29859
Epoch 19/80: current_loss=10.30573 | best_loss=10.29859
Epoch 20/80: current_loss=10.28427 | best_loss=10.28427
Epoch 21/80: current_loss=10.28399 | best_loss=10.28399
Epoch 22/80: current_loss=10.29860 | best_loss=10.28399
Epoch 23/80: current_loss=10.31001 | best_loss=10.28399
Epoch 24/80: current_loss=10.30983 | best_loss=10.28399
Epoch 25/80: current_loss=10.28386 | best_loss=10.28386
Epoch 26/80: current_loss=10.33534 | best_loss=10.28386
Epoch 27/80: current_loss=10.28111 | best_loss=10.28111
Epoch 28/80: current_loss=10.28649 | best_loss=10.28111
Epoch 29/80: current_loss=10.29441 | best_loss=10.28111
Epoch 30/80: current_loss=10.32688 | best_loss=10.28111
Epoch 31/80: current_loss=10.28998 | best_loss=10.28111
Epoch 32/80: current_loss=10.29823 | best_loss=10.28111
Epoch 33/80: current_loss=10.27510 | best_loss=10.27510
Epoch 34/80: current_loss=10.28993 | best_loss=10.27510
Epoch 35/80: current_loss=10.29617 | best_loss=10.27510
Epoch 36/80: current_loss=10.28914 | best_loss=10.27510
Epoch 37/80: current_loss=10.28117 | best_loss=10.27510
Epoch 38/80: current_loss=10.32540 | best_loss=10.27510
Epoch 39/80: current_loss=10.30661 | best_loss=10.27510
Epoch 40/80: current_loss=10.33541 | best_loss=10.27510
Epoch 41/80: current_loss=10.31543 | best_loss=10.27510
Epoch 42/80: current_loss=10.31036 | best_loss=10.27510
Epoch 43/80: current_loss=10.31740 | best_loss=10.27510
Epoch 44/80: current_loss=10.28183 | best_loss=10.27510
Epoch 45/80: current_loss=10.30035 | best_loss=10.27510
Epoch 46/80: current_loss=10.29963 | best_loss=10.27510
Epoch 47/80: current_loss=10.29467 | best_loss=10.27510
Epoch 48/80: current_loss=10.33750 | best_loss=10.27510
Epoch 49/80: current_loss=10.28681 | best_loss=10.27510
Epoch 50/80: current_loss=10.32026 | best_loss=10.27510
Epoch 51/80: current_loss=10.26495 | best_loss=10.26495
Epoch 52/80: current_loss=10.29707 | best_loss=10.26495
Epoch 53/80: current_loss=10.27538 | best_loss=10.26495
Epoch 54/80: current_loss=10.25890 | best_loss=10.25890
Epoch 55/80: current_loss=10.29064 | best_loss=10.25890
Epoch 56/80: current_loss=10.26713 | best_loss=10.25890
Epoch 57/80: current_loss=10.27450 | best_loss=10.25890
Epoch 58/80: current_loss=10.26126 | best_loss=10.25890
Epoch 59/80: current_loss=10.26791 | best_loss=10.25890
Epoch 60/80: current_loss=10.29399 | best_loss=10.25890
Epoch 61/80: current_loss=10.27141 | best_loss=10.25890
Epoch 62/80: current_loss=10.28947 | best_loss=10.25890
Epoch 63/80: current_loss=10.27236 | best_loss=10.25890
Epoch 64/80: current_loss=10.27149 | best_loss=10.25890
Epoch 65/80: current_loss=10.31302 | best_loss=10.25890
Epoch 66/80: current_loss=10.29816 | best_loss=10.25890
Epoch 67/80: current_loss=10.27688 | best_loss=10.25890
Epoch 68/80: current_loss=10.25613 | best_loss=10.25613
Epoch 69/80: current_loss=10.20473 | best_loss=10.20473
Epoch 70/80: current_loss=10.26779 | best_loss=10.20473
Epoch 71/80: current_loss=10.32365 | best_loss=10.20473
Epoch 72/80: current_loss=10.21487 | best_loss=10.20473
Epoch 73/80: current_loss=10.23879 | best_loss=10.20473
Epoch 74/80: current_loss=10.23826 | best_loss=10.20473
Epoch 75/80: current_loss=10.10771 | best_loss=10.10771
Epoch 76/80: current_loss=10.16497 | best_loss=10.10771
Epoch 77/80: current_loss=10.28550 | best_loss=10.10771
Epoch 78/80: current_loss=10.28978 | best_loss=10.10771
Epoch 79/80: current_loss=10.28195 | best_loss=10.10771
      explained_var=0.02538 | mse_loss=10.38488
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.34158 | best_loss=10.34158
Epoch 1/80: current_loss=10.39129 | best_loss=10.34158
Epoch 2/80: current_loss=10.38139 | best_loss=10.34158
Epoch 3/80: current_loss=10.36768 | best_loss=10.34158
Epoch 4/80: current_loss=10.32741 | best_loss=10.32741
Epoch 5/80: current_loss=10.35927 | best_loss=10.32741
Epoch 6/80: current_loss=10.33033 | best_loss=10.32741
Epoch 7/80: current_loss=10.30181 | best_loss=10.30181
Epoch 8/80: current_loss=10.22390 | best_loss=10.22390
Epoch 9/80: current_loss=10.29450 | best_loss=10.22390
Epoch 10/80: current_loss=10.34277 | best_loss=10.22390
Epoch 11/80: current_loss=10.31592 | best_loss=10.22390
Epoch 12/80: current_loss=10.21392 | best_loss=10.21392
Epoch 13/80: current_loss=10.38773 | best_loss=10.21392
Epoch 14/80: current_loss=10.47150 | best_loss=10.21392
Epoch 15/80: current_loss=10.33530 | best_loss=10.21392
Epoch 16/80: current_loss=10.21885 | best_loss=10.21392
Epoch 17/80: current_loss=10.53510 | best_loss=10.21392
Epoch 18/80: current_loss=10.37785 | best_loss=10.21392
Epoch 19/80: current_loss=10.36014 | best_loss=10.21392
Epoch 20/80: current_loss=10.29921 | best_loss=10.21392
Epoch 21/80: current_loss=10.27187 | best_loss=10.21392
Epoch 22/80: current_loss=10.34367 | best_loss=10.21392
Epoch 23/80: current_loss=10.36402 | best_loss=10.21392
Epoch 24/80: current_loss=10.38226 | best_loss=10.21392
Epoch 25/80: current_loss=10.46573 | best_loss=10.21392
Epoch 26/80: current_loss=10.40770 | best_loss=10.21392
Epoch 27/80: current_loss=10.28753 | best_loss=10.21392
Epoch 28/80: current_loss=10.39692 | best_loss=10.21392
Epoch 29/80: current_loss=10.44011 | best_loss=10.21392
Epoch 30/80: current_loss=10.38459 | best_loss=10.21392
Epoch 31/80: current_loss=10.53457 | best_loss=10.21392
Epoch 32/80: current_loss=10.39441 | best_loss=10.21392
Early Stopping at epoch 32
      explained_var=0.02611 | mse_loss=9.84761
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.51153 | best_loss=7.51153
Epoch 1/80: current_loss=7.51493 | best_loss=7.51153
Epoch 2/80: current_loss=7.74489 | best_loss=7.51153
Epoch 3/80: current_loss=7.57223 | best_loss=7.51153
Epoch 4/80: current_loss=7.61428 | best_loss=7.51153
Epoch 5/80: current_loss=7.51424 | best_loss=7.51153
Epoch 6/80: current_loss=7.62460 | best_loss=7.51153
Epoch 7/80: current_loss=7.70011 | best_loss=7.51153
Epoch 8/80: current_loss=7.88043 | best_loss=7.51153
Epoch 9/80: current_loss=7.53019 | best_loss=7.51153
Epoch 10/80: current_loss=7.62362 | best_loss=7.51153
Epoch 11/80: current_loss=7.50668 | best_loss=7.50668
Epoch 12/80: current_loss=7.51194 | best_loss=7.50668
Epoch 13/80: current_loss=7.57595 | best_loss=7.50668
Epoch 14/80: current_loss=7.59538 | best_loss=7.50668
Epoch 15/80: current_loss=7.53967 | best_loss=7.50668
Epoch 16/80: current_loss=7.64959 | best_loss=7.50668
Epoch 17/80: current_loss=7.58207 | best_loss=7.50668
Epoch 18/80: current_loss=7.57330 | best_loss=7.50668
Epoch 19/80: current_loss=7.48870 | best_loss=7.48870
Epoch 20/80: current_loss=7.64565 | best_loss=7.48870
Epoch 21/80: current_loss=7.57437 | best_loss=7.48870
Epoch 22/80: current_loss=7.56223 | best_loss=7.48870
Epoch 23/80: current_loss=7.51552 | best_loss=7.48870
Epoch 24/80: current_loss=7.69112 | best_loss=7.48870
Epoch 25/80: current_loss=7.53597 | best_loss=7.48870
Epoch 26/80: current_loss=7.52770 | best_loss=7.48870
Epoch 27/80: current_loss=7.73130 | best_loss=7.48870
Epoch 28/80: current_loss=7.58184 | best_loss=7.48870
Epoch 29/80: current_loss=7.66254 | best_loss=7.48870
Epoch 30/80: current_loss=7.47529 | best_loss=7.47529
Epoch 31/80: current_loss=7.75297 | best_loss=7.47529
Epoch 32/80: current_loss=7.53210 | best_loss=7.47529
Epoch 33/80: current_loss=7.66674 | best_loss=7.47529
Epoch 34/80: current_loss=7.60358 | best_loss=7.47529
Epoch 35/80: current_loss=7.73535 | best_loss=7.47529
Epoch 36/80: current_loss=7.42783 | best_loss=7.42783
Epoch 37/80: current_loss=7.60241 | best_loss=7.42783
Epoch 38/80: current_loss=7.57337 | best_loss=7.42783
Epoch 39/80: current_loss=7.74458 | best_loss=7.42783
Epoch 40/80: current_loss=7.54865 | best_loss=7.42783
Epoch 41/80: current_loss=7.57864 | best_loss=7.42783
Epoch 42/80: current_loss=7.63996 | best_loss=7.42783
Epoch 43/80: current_loss=7.70713 | best_loss=7.42783
Epoch 44/80: current_loss=7.62082 | best_loss=7.42783
Epoch 45/80: current_loss=7.63826 | best_loss=7.42783
Epoch 46/80: current_loss=7.60569 | best_loss=7.42783
Epoch 47/80: current_loss=7.57710 | best_loss=7.42783
Epoch 48/80: current_loss=7.68641 | best_loss=7.42783
Epoch 49/80: current_loss=7.62055 | best_loss=7.42783
Epoch 50/80: current_loss=7.68161 | best_loss=7.42783
Epoch 51/80: current_loss=7.64186 | best_loss=7.42783
Epoch 52/80: current_loss=7.65301 | best_loss=7.42783
Epoch 53/80: current_loss=7.61557 | best_loss=7.42783
Epoch 54/80: current_loss=7.62410 | best_loss=7.42783
Epoch 55/80: current_loss=7.57384 | best_loss=7.42783
Epoch 56/80: current_loss=7.60069 | best_loss=7.42783
Early Stopping at epoch 56
      explained_var=0.00232 | mse_loss=7.57247
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.76859 | best_loss=8.76859
Epoch 1/80: current_loss=8.73791 | best_loss=8.73791
Epoch 2/80: current_loss=8.84732 | best_loss=8.73791
Epoch 3/80: current_loss=8.76327 | best_loss=8.73791
Epoch 4/80: current_loss=9.00221 | best_loss=8.73791
Epoch 5/80: current_loss=8.83500 | best_loss=8.73791
Epoch 6/80: current_loss=8.88401 | best_loss=8.73791
Epoch 7/80: current_loss=8.94592 | best_loss=8.73791
Epoch 8/80: current_loss=8.83929 | best_loss=8.73791
Epoch 9/80: current_loss=8.80751 | best_loss=8.73791
Epoch 10/80: current_loss=8.80589 | best_loss=8.73791
Epoch 11/80: current_loss=8.91816 | best_loss=8.73791
Epoch 12/80: current_loss=8.89325 | best_loss=8.73791
Epoch 13/80: current_loss=8.84117 | best_loss=8.73791
Epoch 14/80: current_loss=8.85688 | best_loss=8.73791
Epoch 15/80: current_loss=8.82799 | best_loss=8.73791
Epoch 16/80: current_loss=8.77074 | best_loss=8.73791
Epoch 17/80: current_loss=8.85796 | best_loss=8.73791
Epoch 18/80: current_loss=9.04791 | best_loss=8.73791
Epoch 19/80: current_loss=8.99875 | best_loss=8.73791
Epoch 20/80: current_loss=8.75613 | best_loss=8.73791
Epoch 21/80: current_loss=8.77678 | best_loss=8.73791
Early Stopping at epoch 21
      explained_var=0.00442 | mse_loss=8.70084
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.19885 | best_loss=9.19885
Epoch 1/80: current_loss=9.19167 | best_loss=9.19167
Epoch 2/80: current_loss=9.21759 | best_loss=9.19167
Epoch 3/80: current_loss=9.19379 | best_loss=9.19167
Epoch 4/80: current_loss=9.57712 | best_loss=9.19167
Epoch 5/80: current_loss=9.40862 | best_loss=9.19167
Epoch 6/80: current_loss=9.31419 | best_loss=9.19167
Epoch 7/80: current_loss=9.25997 | best_loss=9.19167
Epoch 8/80: current_loss=9.25085 | best_loss=9.19167
Epoch 9/80: current_loss=9.55161 | best_loss=9.19167
Epoch 10/80: current_loss=9.25557 | best_loss=9.19167
Epoch 11/80: current_loss=9.28163 | best_loss=9.19167
Epoch 12/80: current_loss=9.21847 | best_loss=9.19167
Epoch 13/80: current_loss=9.19954 | best_loss=9.19167
Epoch 14/80: current_loss=9.30279 | best_loss=9.19167
Epoch 15/80: current_loss=9.24700 | best_loss=9.19167
Epoch 16/80: current_loss=9.30336 | best_loss=9.19167
Epoch 17/80: current_loss=9.23253 | best_loss=9.19167
Epoch 18/80: current_loss=9.13282 | best_loss=9.13282
Epoch 19/80: current_loss=9.25347 | best_loss=9.13282
Epoch 20/80: current_loss=9.25937 | best_loss=9.13282
Epoch 21/80: current_loss=9.19894 | best_loss=9.13282
Epoch 22/80: current_loss=9.27956 | best_loss=9.13282
Epoch 23/80: current_loss=9.20264 | best_loss=9.13282
Epoch 24/80: current_loss=9.18498 | best_loss=9.13282
Epoch 25/80: current_loss=9.27420 | best_loss=9.13282
Epoch 26/80: current_loss=9.25016 | best_loss=9.13282
Epoch 27/80: current_loss=9.21393 | best_loss=9.13282
Epoch 28/80: current_loss=9.22749 | best_loss=9.13282
Epoch 29/80: current_loss=9.33019 | best_loss=9.13282
Epoch 30/80: current_loss=9.19154 | best_loss=9.13282
Epoch 31/80: current_loss=9.16908 | best_loss=9.13282
Epoch 32/80: current_loss=9.20402 | best_loss=9.13282
Epoch 33/80: current_loss=9.27582 | best_loss=9.13282
Epoch 34/80: current_loss=9.17455 | best_loss=9.13282
Epoch 35/80: current_loss=9.18784 | best_loss=9.13282
Epoch 36/80: current_loss=9.21773 | best_loss=9.13282
Epoch 37/80: current_loss=9.21190 | best_loss=9.13282
Epoch 38/80: current_loss=9.15587 | best_loss=9.13282
Early Stopping at epoch 38
      explained_var=0.02125 | mse_loss=9.09772
----------------------------------------------
Average early_stopping_point: 29| avg_exp_var=0.01590| avg_loss=9.12071
----------------------------------------------


----------------------------------------------
Params for Trial 19
{'learning_rate': 0.01, 'weight_decay': 0.008500159134657992, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=11.72590 | best_loss=11.72590
Epoch 1/80: current_loss=10.38209 | best_loss=10.38209
Epoch 2/80: current_loss=10.45473 | best_loss=10.38209
Epoch 3/80: current_loss=10.37907 | best_loss=10.37907
Epoch 4/80: current_loss=10.25805 | best_loss=10.25805
Epoch 5/80: current_loss=10.28259 | best_loss=10.25805
Epoch 6/80: current_loss=10.37030 | best_loss=10.25805
Epoch 7/80: current_loss=10.28205 | best_loss=10.25805
Epoch 8/80: current_loss=11.51042 | best_loss=10.25805
Epoch 9/80: current_loss=10.26674 | best_loss=10.25805
Epoch 10/80: current_loss=10.43299 | best_loss=10.25805
Epoch 11/80: current_loss=10.27738 | best_loss=10.25805
Epoch 12/80: current_loss=10.35448 | best_loss=10.25805
Epoch 13/80: current_loss=10.26358 | best_loss=10.25805
Epoch 14/80: current_loss=10.40981 | best_loss=10.25805
Epoch 15/80: current_loss=10.42332 | best_loss=10.25805
Epoch 16/80: current_loss=10.61365 | best_loss=10.25805
Epoch 17/80: current_loss=10.28079 | best_loss=10.25805
Epoch 18/80: current_loss=10.30044 | best_loss=10.25805
Epoch 19/80: current_loss=10.24873 | best_loss=10.24873
Epoch 20/80: current_loss=10.42209 | best_loss=10.24873
Epoch 21/80: current_loss=10.37233 | best_loss=10.24873
Epoch 22/80: current_loss=10.87727 | best_loss=10.24873
Epoch 23/80: current_loss=10.49000 | best_loss=10.24873
Epoch 24/80: current_loss=10.50109 | best_loss=10.24873
Epoch 25/80: current_loss=10.50853 | best_loss=10.24873
Epoch 26/80: current_loss=10.27871 | best_loss=10.24873
Epoch 27/80: current_loss=10.26008 | best_loss=10.24873
Epoch 28/80: current_loss=10.32123 | best_loss=10.24873
Epoch 29/80: current_loss=10.56076 | best_loss=10.24873
Epoch 30/80: current_loss=10.25949 | best_loss=10.24873
Epoch 31/80: current_loss=10.33249 | best_loss=10.24873
Epoch 32/80: current_loss=10.45872 | best_loss=10.24873
Epoch 33/80: current_loss=10.64700 | best_loss=10.24873
Epoch 34/80: current_loss=10.28540 | best_loss=10.24873
Epoch 35/80: current_loss=10.26007 | best_loss=10.24873
Epoch 36/80: current_loss=10.27292 | best_loss=10.24873
Epoch 37/80: current_loss=10.35138 | best_loss=10.24873
Epoch 38/80: current_loss=10.45864 | best_loss=10.24873
Epoch 39/80: current_loss=10.52666 | best_loss=10.24873
Early Stopping at epoch 39
      explained_var=0.00053 | mse_loss=10.47317
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.46195 | best_loss=10.46195
Epoch 1/80: current_loss=10.51559 | best_loss=10.46195
Epoch 2/80: current_loss=10.51588 | best_loss=10.46195
Epoch 3/80: current_loss=11.10089 | best_loss=10.46195
Epoch 4/80: current_loss=10.41208 | best_loss=10.41208
Epoch 5/80: current_loss=10.48272 | best_loss=10.41208
Epoch 6/80: current_loss=10.91205 | best_loss=10.41208
Epoch 7/80: current_loss=10.94110 | best_loss=10.41208
Epoch 8/80: current_loss=10.63675 | best_loss=10.41208
Epoch 9/80: current_loss=10.74211 | best_loss=10.41208
Epoch 10/80: current_loss=10.68090 | best_loss=10.41208
Epoch 11/80: current_loss=11.08705 | best_loss=10.41208
Epoch 12/80: current_loss=10.55872 | best_loss=10.41208
Epoch 13/80: current_loss=10.38517 | best_loss=10.38517
Epoch 14/80: current_loss=10.35763 | best_loss=10.35763
Epoch 15/80: current_loss=11.49769 | best_loss=10.35763
Epoch 16/80: current_loss=10.76808 | best_loss=10.35763
Epoch 17/80: current_loss=10.42808 | best_loss=10.35763
Epoch 18/80: current_loss=10.95397 | best_loss=10.35763
Epoch 19/80: current_loss=10.95794 | best_loss=10.35763
Epoch 20/80: current_loss=10.42489 | best_loss=10.35763
Epoch 21/80: current_loss=10.49408 | best_loss=10.35763
Epoch 22/80: current_loss=10.47735 | best_loss=10.35763
Epoch 23/80: current_loss=11.33545 | best_loss=10.35763
Epoch 24/80: current_loss=10.41626 | best_loss=10.35763
Epoch 25/80: current_loss=10.65998 | best_loss=10.35763
Epoch 26/80: current_loss=10.56586 | best_loss=10.35763
Epoch 27/80: current_loss=10.42476 | best_loss=10.35763
Epoch 28/80: current_loss=10.45735 | best_loss=10.35763
Epoch 29/80: current_loss=10.44307 | best_loss=10.35763
Epoch 30/80: current_loss=10.40826 | best_loss=10.35763
Epoch 31/80: current_loss=10.48043 | best_loss=10.35763
Epoch 32/80: current_loss=10.45652 | best_loss=10.35763
Epoch 33/80: current_loss=10.43761 | best_loss=10.35763
Epoch 34/80: current_loss=10.57326 | best_loss=10.35763
Early Stopping at epoch 34
      explained_var=0.00481 | mse_loss=10.02790
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.60404 | best_loss=7.60404
Epoch 1/80: current_loss=7.63803 | best_loss=7.60404
Epoch 2/80: current_loss=7.49522 | best_loss=7.49522
Epoch 3/80: current_loss=8.16122 | best_loss=7.49522
Epoch 4/80: current_loss=7.73888 | best_loss=7.49522
Epoch 5/80: current_loss=7.62783 | best_loss=7.49522
Epoch 6/80: current_loss=7.61192 | best_loss=7.49522
Epoch 7/80: current_loss=7.47511 | best_loss=7.47511
Epoch 8/80: current_loss=9.48039 | best_loss=7.47511
Epoch 9/80: current_loss=8.09056 | best_loss=7.47511
Epoch 10/80: current_loss=7.47837 | best_loss=7.47511
Epoch 11/80: current_loss=8.59866 | best_loss=7.47511
Epoch 12/80: current_loss=7.47569 | best_loss=7.47511
Epoch 13/80: current_loss=9.06565 | best_loss=7.47511
Epoch 14/80: current_loss=7.70650 | best_loss=7.47511
Epoch 15/80: current_loss=7.67512 | best_loss=7.47511
Epoch 16/80: current_loss=9.02785 | best_loss=7.47511
Epoch 17/80: current_loss=7.57651 | best_loss=7.47511
Epoch 18/80: current_loss=8.59230 | best_loss=7.47511
Epoch 19/80: current_loss=7.66821 | best_loss=7.47511
Epoch 20/80: current_loss=8.36591 | best_loss=7.47511
Epoch 21/80: current_loss=7.82467 | best_loss=7.47511
Epoch 22/80: current_loss=8.32555 | best_loss=7.47511
Epoch 23/80: current_loss=7.51482 | best_loss=7.47511
Epoch 24/80: current_loss=7.97090 | best_loss=7.47511
Epoch 25/80: current_loss=9.13280 | best_loss=7.47511
Epoch 26/80: current_loss=7.58515 | best_loss=7.47511
Epoch 27/80: current_loss=7.68782 | best_loss=7.47511
Early Stopping at epoch 27
      explained_var=-0.00013 | mse_loss=7.58496
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.97342 | best_loss=8.97342
Epoch 1/80: current_loss=10.08156 | best_loss=8.97342
Epoch 2/80: current_loss=8.84576 | best_loss=8.84576
Epoch 3/80: current_loss=9.24572 | best_loss=8.84576
Epoch 4/80: current_loss=8.79209 | best_loss=8.79209
Epoch 5/80: current_loss=10.84647 | best_loss=8.79209
Epoch 6/80: current_loss=9.55843 | best_loss=8.79209
Epoch 7/80: current_loss=15.22447 | best_loss=8.79209
Epoch 8/80: current_loss=9.39375 | best_loss=8.79209
Epoch 9/80: current_loss=8.79795 | best_loss=8.79209
Epoch 10/80: current_loss=8.79949 | best_loss=8.79209
Epoch 11/80: current_loss=8.77894 | best_loss=8.77894
Epoch 12/80: current_loss=8.87005 | best_loss=8.77894
Epoch 13/80: current_loss=8.88517 | best_loss=8.77894
Epoch 14/80: current_loss=8.79964 | best_loss=8.77894
Epoch 15/80: current_loss=8.84046 | best_loss=8.77894
Epoch 16/80: current_loss=9.08348 | best_loss=8.77894
Epoch 17/80: current_loss=8.95865 | best_loss=8.77894
Epoch 18/80: current_loss=8.80779 | best_loss=8.77894
Epoch 19/80: current_loss=8.80720 | best_loss=8.77894
Epoch 20/80: current_loss=9.17840 | best_loss=8.77894
Epoch 21/80: current_loss=8.80669 | best_loss=8.77894
Epoch 22/80: current_loss=8.81179 | best_loss=8.77894
Epoch 23/80: current_loss=8.85563 | best_loss=8.77894
Epoch 24/80: current_loss=8.80522 | best_loss=8.77894
Epoch 25/80: current_loss=8.79327 | best_loss=8.77894
Epoch 26/80: current_loss=8.93196 | best_loss=8.77894
Epoch 27/80: current_loss=9.03139 | best_loss=8.77894
Epoch 28/80: current_loss=8.80356 | best_loss=8.77894
Epoch 29/80: current_loss=8.77791 | best_loss=8.77791
Epoch 30/80: current_loss=10.16352 | best_loss=8.77791
Epoch 31/80: current_loss=8.86981 | best_loss=8.77791
Epoch 32/80: current_loss=8.79395 | best_loss=8.77791
Epoch 33/80: current_loss=8.86995 | best_loss=8.77791
Epoch 34/80: current_loss=8.78341 | best_loss=8.77791
Epoch 35/80: current_loss=8.81181 | best_loss=8.77791
Epoch 36/80: current_loss=9.07035 | best_loss=8.77791
Epoch 37/80: current_loss=8.98189 | best_loss=8.77791
Epoch 38/80: current_loss=9.07341 | best_loss=8.77791
Epoch 39/80: current_loss=8.91337 | best_loss=8.77791
Epoch 40/80: current_loss=8.78325 | best_loss=8.77791
Epoch 41/80: current_loss=8.80770 | best_loss=8.77791
Epoch 42/80: current_loss=9.02796 | best_loss=8.77791
Epoch 43/80: current_loss=8.87445 | best_loss=8.77791
Epoch 44/80: current_loss=8.86123 | best_loss=8.77791
Epoch 45/80: current_loss=9.05276 | best_loss=8.77791
Epoch 46/80: current_loss=8.78770 | best_loss=8.77791
Epoch 47/80: current_loss=8.79134 | best_loss=8.77791
Epoch 48/80: current_loss=8.97853 | best_loss=8.77791
Epoch 49/80: current_loss=8.92107 | best_loss=8.77791
Early Stopping at epoch 49
      explained_var=0.00009 | mse_loss=8.71726
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.82092 | best_loss=9.82092
Epoch 1/80: current_loss=9.69685 | best_loss=9.69685
Epoch 2/80: current_loss=10.41662 | best_loss=9.69685
Epoch 3/80: current_loss=9.39297 | best_loss=9.39297
Epoch 4/80: current_loss=10.06754 | best_loss=9.39297
Epoch 5/80: current_loss=9.78027 | best_loss=9.39297
Epoch 6/80: current_loss=13.57117 | best_loss=9.39297
Epoch 7/80: current_loss=9.41969 | best_loss=9.39297
Epoch 8/80: current_loss=9.57363 | best_loss=9.39297
Epoch 9/80: current_loss=9.79377 | best_loss=9.39297
Epoch 10/80: current_loss=9.53542 | best_loss=9.39297
Epoch 11/80: current_loss=10.66226 | best_loss=9.39297
Epoch 12/80: current_loss=9.53325 | best_loss=9.39297
Epoch 13/80: current_loss=9.42893 | best_loss=9.39297
Epoch 14/80: current_loss=9.43538 | best_loss=9.39297
Epoch 15/80: current_loss=10.06068 | best_loss=9.39297
Epoch 16/80: current_loss=9.43415 | best_loss=9.39297
Epoch 17/80: current_loss=10.17139 | best_loss=9.39297
Epoch 18/80: current_loss=9.36224 | best_loss=9.36224
Epoch 19/80: current_loss=9.65923 | best_loss=9.36224
Epoch 20/80: current_loss=9.98042 | best_loss=9.36224
Epoch 21/80: current_loss=9.51527 | best_loss=9.36224
Epoch 22/80: current_loss=9.39356 | best_loss=9.36224
Epoch 23/80: current_loss=9.54858 | best_loss=9.36224
Epoch 24/80: current_loss=9.36456 | best_loss=9.36224
Epoch 25/80: current_loss=9.38028 | best_loss=9.36224
Epoch 26/80: current_loss=9.77433 | best_loss=9.36224
Epoch 27/80: current_loss=9.60313 | best_loss=9.36224
Epoch 28/80: current_loss=9.87162 | best_loss=9.36224
Epoch 29/80: current_loss=9.75411 | best_loss=9.36224
Epoch 30/80: current_loss=9.47936 | best_loss=9.36224
Epoch 31/80: current_loss=9.79669 | best_loss=9.36224
Epoch 32/80: current_loss=9.57600 | best_loss=9.36224
Epoch 33/80: current_loss=9.72437 | best_loss=9.36224
Epoch 34/80: current_loss=10.29440 | best_loss=9.36224
Epoch 35/80: current_loss=9.37158 | best_loss=9.36224
Epoch 36/80: current_loss=9.39947 | best_loss=9.36224
Epoch 37/80: current_loss=9.41695 | best_loss=9.36224
Epoch 38/80: current_loss=9.51513 | best_loss=9.36224
Early Stopping at epoch 38
      explained_var=0.00358 | mse_loss=9.29339
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00178| avg_loss=9.21933
----------------------------------------------


----------------------------------------------
Params for Trial 20
{'learning_rate': 0.1, 'weight_decay': 0.009182147619924847, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.76987 | best_loss=10.76987
Epoch 1/80: current_loss=10.54737 | best_loss=10.54737
Epoch 2/80: current_loss=15.59679 | best_loss=10.54737
Epoch 3/80: current_loss=10.51499 | best_loss=10.51499
Epoch 4/80: current_loss=11.02292 | best_loss=10.51499
Epoch 5/80: current_loss=13.33071 | best_loss=10.51499
Epoch 6/80: current_loss=10.26707 | best_loss=10.26707
Epoch 7/80: current_loss=10.38972 | best_loss=10.26707
Epoch 8/80: current_loss=10.44789 | best_loss=10.26707
Epoch 9/80: current_loss=15.72179 | best_loss=10.26707
Epoch 10/80: current_loss=10.25013 | best_loss=10.25013
Epoch 11/80: current_loss=18.61848 | best_loss=10.25013
Epoch 12/80: current_loss=13.98941 | best_loss=10.25013
Epoch 13/80: current_loss=10.43767 | best_loss=10.25013
Epoch 14/80: current_loss=11.02386 | best_loss=10.25013
Epoch 15/80: current_loss=11.12487 | best_loss=10.25013
Epoch 16/80: current_loss=11.40009 | best_loss=10.25013
Epoch 17/80: current_loss=15.70486 | best_loss=10.25013
Epoch 18/80: current_loss=11.68280 | best_loss=10.25013
Epoch 19/80: current_loss=10.71840 | best_loss=10.25013
Epoch 20/80: current_loss=11.05463 | best_loss=10.25013
Epoch 21/80: current_loss=13.34451 | best_loss=10.25013
Epoch 22/80: current_loss=11.16312 | best_loss=10.25013
Epoch 23/80: current_loss=11.89083 | best_loss=10.25013
Epoch 24/80: current_loss=10.30591 | best_loss=10.25013
Epoch 25/80: current_loss=11.00860 | best_loss=10.25013
Epoch 26/80: current_loss=10.53326 | best_loss=10.25013
Epoch 27/80: current_loss=13.62456 | best_loss=10.25013
Epoch 28/80: current_loss=16.40159 | best_loss=10.25013
Epoch 29/80: current_loss=11.66934 | best_loss=10.25013
Epoch 30/80: current_loss=10.37229 | best_loss=10.25013
Early Stopping at epoch 30
      explained_var=0.00068 | mse_loss=10.44890
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=49.05016 | best_loss=49.05016
Epoch 1/80: current_loss=27.52805 | best_loss=27.52805
Epoch 2/80: current_loss=19.26358 | best_loss=19.26358
Epoch 3/80: current_loss=13.48647 | best_loss=13.48647
Epoch 4/80: current_loss=11.95628 | best_loss=11.95628
Epoch 5/80: current_loss=11.10187 | best_loss=11.10187
Epoch 6/80: current_loss=10.42134 | best_loss=10.42134
Epoch 7/80: current_loss=13.46157 | best_loss=10.42134
Epoch 8/80: current_loss=12.67717 | best_loss=10.42134
Epoch 9/80: current_loss=20.86654 | best_loss=10.42134
Epoch 10/80: current_loss=10.46846 | best_loss=10.42134
Epoch 11/80: current_loss=10.49037 | best_loss=10.42134
Epoch 12/80: current_loss=12.15245 | best_loss=10.42134
Epoch 13/80: current_loss=14.51054 | best_loss=10.42134
Epoch 14/80: current_loss=10.58885 | best_loss=10.42134
Epoch 15/80: current_loss=11.98626 | best_loss=10.42134
Epoch 16/80: current_loss=11.02726 | best_loss=10.42134
Epoch 17/80: current_loss=12.54627 | best_loss=10.42134
Epoch 18/80: current_loss=11.02641 | best_loss=10.42134
Epoch 19/80: current_loss=11.75615 | best_loss=10.42134
Epoch 20/80: current_loss=10.95034 | best_loss=10.42134
Epoch 21/80: current_loss=11.45688 | best_loss=10.42134
Epoch 22/80: current_loss=16.23815 | best_loss=10.42134
Epoch 23/80: current_loss=11.31348 | best_loss=10.42134
Epoch 24/80: current_loss=10.85861 | best_loss=10.42134
Epoch 25/80: current_loss=12.82420 | best_loss=10.42134
Epoch 26/80: current_loss=11.49263 | best_loss=10.42134
Early Stopping at epoch 26
      explained_var=0.00042 | mse_loss=10.06278

----------------------------------------------
Params for Trial 21
{'learning_rate': 0.001, 'weight_decay': 0.007144766198238507, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.80995 | best_loss=17.80995
Epoch 1/80: current_loss=10.76819 | best_loss=10.76819
Epoch 2/80: current_loss=10.42159 | best_loss=10.42159
Epoch 3/80: current_loss=10.36959 | best_loss=10.36959
Epoch 4/80: current_loss=10.38192 | best_loss=10.36959
Epoch 5/80: current_loss=10.33173 | best_loss=10.33173
Epoch 6/80: current_loss=10.33573 | best_loss=10.33173
Epoch 7/80: current_loss=10.32100 | best_loss=10.32100
Epoch 8/80: current_loss=10.31288 | best_loss=10.31288
Epoch 9/80: current_loss=10.29206 | best_loss=10.29206
Epoch 10/80: current_loss=10.28928 | best_loss=10.28928
Epoch 11/80: current_loss=10.28128 | best_loss=10.28128
Epoch 12/80: current_loss=10.30313 | best_loss=10.28128
Epoch 13/80: current_loss=10.29993 | best_loss=10.28128
Epoch 14/80: current_loss=10.28090 | best_loss=10.28090
Epoch 15/80: current_loss=10.33494 | best_loss=10.28090
Epoch 16/80: current_loss=10.29354 | best_loss=10.28090
Epoch 17/80: current_loss=10.28376 | best_loss=10.28090
Epoch 18/80: current_loss=10.34300 | best_loss=10.28090
Epoch 19/80: current_loss=10.28981 | best_loss=10.28090
Epoch 20/80: current_loss=10.29888 | best_loss=10.28090
Epoch 21/80: current_loss=10.28872 | best_loss=10.28090
Epoch 22/80: current_loss=10.29167 | best_loss=10.28090
Epoch 23/80: current_loss=10.28649 | best_loss=10.28090
Epoch 24/80: current_loss=10.27158 | best_loss=10.27158
Epoch 25/80: current_loss=10.29632 | best_loss=10.27158
Epoch 26/80: current_loss=10.27236 | best_loss=10.27158
Epoch 27/80: current_loss=10.27962 | best_loss=10.27158
Epoch 28/80: current_loss=10.28105 | best_loss=10.27158
Epoch 29/80: current_loss=10.30111 | best_loss=10.27158
Epoch 30/80: current_loss=10.28095 | best_loss=10.27158
Epoch 31/80: current_loss=10.29132 | best_loss=10.27158
Epoch 32/80: current_loss=10.28349 | best_loss=10.27158
Epoch 33/80: current_loss=10.30312 | best_loss=10.27158
Epoch 34/80: current_loss=10.33414 | best_loss=10.27158
Epoch 35/80: current_loss=10.27610 | best_loss=10.27158
Epoch 36/80: current_loss=10.27179 | best_loss=10.27158
Epoch 37/80: current_loss=10.28298 | best_loss=10.27158
Epoch 38/80: current_loss=10.27141 | best_loss=10.27141
Epoch 39/80: current_loss=10.29676 | best_loss=10.27141
Epoch 40/80: current_loss=10.29722 | best_loss=10.27141
Epoch 41/80: current_loss=10.31318 | best_loss=10.27141
Epoch 42/80: current_loss=10.28889 | best_loss=10.27141
Epoch 43/80: current_loss=10.29297 | best_loss=10.27141
Epoch 44/80: current_loss=10.31188 | best_loss=10.27141
Epoch 45/80: current_loss=10.27617 | best_loss=10.27141
Epoch 46/80: current_loss=10.29181 | best_loss=10.27141
Epoch 47/80: current_loss=10.28253 | best_loss=10.27141
Epoch 48/80: current_loss=10.27088 | best_loss=10.27088
Epoch 49/80: current_loss=10.29466 | best_loss=10.27088
Epoch 50/80: current_loss=10.27006 | best_loss=10.27006
Epoch 51/80: current_loss=10.31372 | best_loss=10.27006
Epoch 52/80: current_loss=10.29448 | best_loss=10.27006
Epoch 53/80: current_loss=10.33669 | best_loss=10.27006
Epoch 54/80: current_loss=10.28279 | best_loss=10.27006
Epoch 55/80: current_loss=10.38038 | best_loss=10.27006
Epoch 56/80: current_loss=10.27473 | best_loss=10.27006
Epoch 57/80: current_loss=10.29675 | best_loss=10.27006
Epoch 58/80: current_loss=10.30505 | best_loss=10.27006
Epoch 59/80: current_loss=10.27175 | best_loss=10.27006
Epoch 60/80: current_loss=10.27843 | best_loss=10.27006
Epoch 61/80: current_loss=10.30976 | best_loss=10.27006
Epoch 62/80: current_loss=10.27250 | best_loss=10.27006
Epoch 63/80: current_loss=10.28260 | best_loss=10.27006
Epoch 64/80: current_loss=10.29699 | best_loss=10.27006
Epoch 65/80: current_loss=10.37374 | best_loss=10.27006
Epoch 66/80: current_loss=10.30829 | best_loss=10.27006
Epoch 67/80: current_loss=10.30471 | best_loss=10.27006
Epoch 68/80: current_loss=10.29286 | best_loss=10.27006
Epoch 69/80: current_loss=10.29899 | best_loss=10.27006
Epoch 70/80: current_loss=10.25293 | best_loss=10.25293
Epoch 71/80: current_loss=10.22567 | best_loss=10.22567
Epoch 72/80: current_loss=10.22627 | best_loss=10.22567
Epoch 73/80: current_loss=10.25851 | best_loss=10.22567
Epoch 74/80: current_loss=10.29400 | best_loss=10.22567
Epoch 75/80: current_loss=10.29845 | best_loss=10.22567
Epoch 76/80: current_loss=10.32863 | best_loss=10.22567
Epoch 77/80: current_loss=10.31794 | best_loss=10.22567
Epoch 78/80: current_loss=10.27696 | best_loss=10.22567
Epoch 79/80: current_loss=10.28296 | best_loss=10.22567
      explained_var=0.00175 | mse_loss=10.44629
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.40045 | best_loss=10.40045
Epoch 1/80: current_loss=10.43755 | best_loss=10.40045
Epoch 2/80: current_loss=10.42107 | best_loss=10.40045
Epoch 3/80: current_loss=10.49415 | best_loss=10.40045
Epoch 4/80: current_loss=10.40571 | best_loss=10.40045
Epoch 5/80: current_loss=10.41722 | best_loss=10.40045
Epoch 6/80: current_loss=10.40744 | best_loss=10.40045
Epoch 7/80: current_loss=11.18667 | best_loss=10.40045
Epoch 8/80: current_loss=10.40863 | best_loss=10.40045
Epoch 9/80: current_loss=10.41214 | best_loss=10.40045
Epoch 10/80: current_loss=10.42271 | best_loss=10.40045
Epoch 11/80: current_loss=10.41281 | best_loss=10.40045
Epoch 12/80: current_loss=10.40608 | best_loss=10.40045
Epoch 13/80: current_loss=10.44191 | best_loss=10.40045
Epoch 14/80: current_loss=10.39947 | best_loss=10.39947
Epoch 15/80: current_loss=10.42383 | best_loss=10.39947
Epoch 16/80: current_loss=10.42861 | best_loss=10.39947
Epoch 17/80: current_loss=10.40244 | best_loss=10.39947
Epoch 18/80: current_loss=10.41022 | best_loss=10.39947
Epoch 19/80: current_loss=10.44578 | best_loss=10.39947
Epoch 20/80: current_loss=10.45427 | best_loss=10.39947
Epoch 21/80: current_loss=10.39390 | best_loss=10.39390
Epoch 22/80: current_loss=10.39062 | best_loss=10.39062
Epoch 23/80: current_loss=10.36622 | best_loss=10.36622
Epoch 24/80: current_loss=10.30548 | best_loss=10.30548
Epoch 25/80: current_loss=10.36074 | best_loss=10.30548
Epoch 26/80: current_loss=10.41567 | best_loss=10.30548
Epoch 27/80: current_loss=10.49716 | best_loss=10.30548
Epoch 28/80: current_loss=10.44331 | best_loss=10.30548
Epoch 29/80: current_loss=10.40881 | best_loss=10.30548
Epoch 30/80: current_loss=10.44011 | best_loss=10.30548
Epoch 31/80: current_loss=10.42931 | best_loss=10.30548
Epoch 32/80: current_loss=10.40264 | best_loss=10.30548
Epoch 33/80: current_loss=10.43096 | best_loss=10.30548
Epoch 34/80: current_loss=10.39932 | best_loss=10.30548
Epoch 35/80: current_loss=10.42021 | best_loss=10.30548
Epoch 36/80: current_loss=10.43679 | best_loss=10.30548
Epoch 37/80: current_loss=10.40239 | best_loss=10.30548
Epoch 38/80: current_loss=10.56986 | best_loss=10.30548
Epoch 39/80: current_loss=10.41469 | best_loss=10.30548
Epoch 40/80: current_loss=10.43625 | best_loss=10.30548
Epoch 41/80: current_loss=10.41359 | best_loss=10.30548
Epoch 42/80: current_loss=10.44609 | best_loss=10.30548
Epoch 43/80: current_loss=10.43451 | best_loss=10.30548
Epoch 44/80: current_loss=10.41325 | best_loss=10.30548
Early Stopping at epoch 44
      explained_var=0.01167 | mse_loss=9.95263
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.50132 | best_loss=7.50132
Epoch 1/80: current_loss=7.61381 | best_loss=7.50132
Epoch 2/80: current_loss=7.60809 | best_loss=7.50132
Epoch 3/80: current_loss=7.49318 | best_loss=7.49318
Epoch 4/80: current_loss=7.49319 | best_loss=7.49318
Epoch 5/80: current_loss=7.55311 | best_loss=7.49318
Epoch 6/80: current_loss=7.57452 | best_loss=7.49318
Epoch 7/80: current_loss=7.58197 | best_loss=7.49318
Epoch 8/80: current_loss=7.62007 | best_loss=7.49318
Epoch 9/80: current_loss=7.51421 | best_loss=7.49318
Epoch 10/80: current_loss=7.59535 | best_loss=7.49318
Epoch 11/80: current_loss=7.53535 | best_loss=7.49318
Epoch 12/80: current_loss=7.64745 | best_loss=7.49318
Epoch 13/80: current_loss=7.76903 | best_loss=7.49318
Epoch 14/80: current_loss=7.51296 | best_loss=7.49318
Epoch 15/80: current_loss=7.71732 | best_loss=7.49318
Epoch 16/80: current_loss=7.62590 | best_loss=7.49318
Epoch 17/80: current_loss=7.53781 | best_loss=7.49318
Epoch 18/80: current_loss=7.50062 | best_loss=7.49318
Epoch 19/80: current_loss=7.62518 | best_loss=7.49318
Epoch 20/80: current_loss=7.60979 | best_loss=7.49318
Epoch 21/80: current_loss=7.51848 | best_loss=7.49318
Epoch 22/80: current_loss=7.69498 | best_loss=7.49318
Epoch 23/80: current_loss=7.85200 | best_loss=7.49318
Early Stopping at epoch 23
      explained_var=0.00085 | mse_loss=7.59569
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.72642 | best_loss=8.72642
Epoch 1/80: current_loss=8.74352 | best_loss=8.72642
Epoch 2/80: current_loss=8.75459 | best_loss=8.72642
Epoch 3/80: current_loss=8.83206 | best_loss=8.72642
Epoch 4/80: current_loss=8.81550 | best_loss=8.72642
Epoch 5/80: current_loss=8.70681 | best_loss=8.70681
Epoch 6/80: current_loss=8.91213 | best_loss=8.70681
Epoch 7/80: current_loss=8.86963 | best_loss=8.70681
Epoch 8/80: current_loss=8.74649 | best_loss=8.70681
Epoch 9/80: current_loss=8.80006 | best_loss=8.70681
Epoch 10/80: current_loss=8.77673 | best_loss=8.70681
Epoch 11/80: current_loss=8.88286 | best_loss=8.70681
Epoch 12/80: current_loss=8.85011 | best_loss=8.70681
Epoch 13/80: current_loss=8.74171 | best_loss=8.70681
Epoch 14/80: current_loss=8.79557 | best_loss=8.70681
Epoch 15/80: current_loss=8.76293 | best_loss=8.70681
Epoch 16/80: current_loss=8.81645 | best_loss=8.70681
Epoch 17/80: current_loss=8.88035 | best_loss=8.70681
Epoch 18/80: current_loss=8.81158 | best_loss=8.70681
Epoch 19/80: current_loss=8.82379 | best_loss=8.70681
Epoch 20/80: current_loss=8.72761 | best_loss=8.70681
Epoch 21/80: current_loss=8.79715 | best_loss=8.70681
Epoch 22/80: current_loss=8.86062 | best_loss=8.70681
Epoch 23/80: current_loss=8.76956 | best_loss=8.70681
Epoch 24/80: current_loss=8.91429 | best_loss=8.70681
Epoch 25/80: current_loss=9.39850 | best_loss=8.70681
Early Stopping at epoch 25
      explained_var=0.00593 | mse_loss=8.66502
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.40847 | best_loss=9.40847
Epoch 1/80: current_loss=9.36555 | best_loss=9.36555
Epoch 2/80: current_loss=9.30981 | best_loss=9.30981
Epoch 3/80: current_loss=9.31010 | best_loss=9.30981
Epoch 4/80: current_loss=9.16145 | best_loss=9.16145
Epoch 5/80: current_loss=9.22603 | best_loss=9.16145
Epoch 6/80: current_loss=9.21564 | best_loss=9.16145
Epoch 7/80: current_loss=9.29075 | best_loss=9.16145
Epoch 8/80: current_loss=9.21342 | best_loss=9.16145
Epoch 9/80: current_loss=9.20728 | best_loss=9.16145
Epoch 10/80: current_loss=9.19098 | best_loss=9.16145
Epoch 11/80: current_loss=9.19510 | best_loss=9.16145
Epoch 12/80: current_loss=9.16192 | best_loss=9.16145
Epoch 13/80: current_loss=9.18336 | best_loss=9.16145
Epoch 14/80: current_loss=9.22726 | best_loss=9.16145
Epoch 15/80: current_loss=9.14617 | best_loss=9.14617
Epoch 16/80: current_loss=9.31803 | best_loss=9.14617
Epoch 17/80: current_loss=9.15098 | best_loss=9.14617
Epoch 18/80: current_loss=9.19268 | best_loss=9.14617
Epoch 19/80: current_loss=9.19039 | best_loss=9.14617
Epoch 20/80: current_loss=9.17824 | best_loss=9.14617
Epoch 21/80: current_loss=9.28479 | best_loss=9.14617
Epoch 22/80: current_loss=9.18792 | best_loss=9.14617
Epoch 23/80: current_loss=9.26972 | best_loss=9.14617
Epoch 24/80: current_loss=9.25891 | best_loss=9.14617
Epoch 25/80: current_loss=9.26826 | best_loss=9.14617
Epoch 26/80: current_loss=9.29974 | best_loss=9.14617
Epoch 27/80: current_loss=9.20895 | best_loss=9.14617
Epoch 28/80: current_loss=9.16498 | best_loss=9.14617
Epoch 29/80: current_loss=9.23060 | best_loss=9.14617
Epoch 30/80: current_loss=9.22815 | best_loss=9.14617
Epoch 31/80: current_loss=9.21487 | best_loss=9.14617
Epoch 32/80: current_loss=9.18097 | best_loss=9.14617
Epoch 33/80: current_loss=9.21430 | best_loss=9.14617
Epoch 34/80: current_loss=9.18319 | best_loss=9.14617
Epoch 35/80: current_loss=9.26112 | best_loss=9.14617
Early Stopping at epoch 35
      explained_var=0.02152 | mse_loss=9.10553
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=0.00835| avg_loss=9.15303
----------------------------------------------


----------------------------------------------
Params for Trial 22
{'learning_rate': 0.001, 'weight_decay': 0.005709933591083406, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.63408 | best_loss=18.63408
Epoch 1/80: current_loss=11.89329 | best_loss=11.89329
Epoch 2/80: current_loss=11.42253 | best_loss=11.42253
Epoch 3/80: current_loss=11.14830 | best_loss=11.14830
Epoch 4/80: current_loss=10.90512 | best_loss=10.90512
Epoch 5/80: current_loss=10.71294 | best_loss=10.71294
Epoch 6/80: current_loss=10.65218 | best_loss=10.65218
Epoch 7/80: current_loss=10.52225 | best_loss=10.52225
Epoch 8/80: current_loss=10.47740 | best_loss=10.47740
Epoch 9/80: current_loss=10.47272 | best_loss=10.47272
Epoch 10/80: current_loss=10.41844 | best_loss=10.41844
Epoch 11/80: current_loss=10.39815 | best_loss=10.39815
Epoch 12/80: current_loss=10.36266 | best_loss=10.36266
Epoch 13/80: current_loss=10.35997 | best_loss=10.35997
Epoch 14/80: current_loss=10.35516 | best_loss=10.35516
Epoch 15/80: current_loss=10.32824 | best_loss=10.32824
Epoch 16/80: current_loss=10.36861 | best_loss=10.32824
Epoch 17/80: current_loss=10.32274 | best_loss=10.32274
Epoch 18/80: current_loss=10.34356 | best_loss=10.32274
Epoch 19/80: current_loss=10.31872 | best_loss=10.31872
Epoch 20/80: current_loss=10.30875 | best_loss=10.30875
Epoch 21/80: current_loss=10.34754 | best_loss=10.30875
Epoch 22/80: current_loss=10.30612 | best_loss=10.30612
Epoch 23/80: current_loss=10.30601 | best_loss=10.30601
Epoch 24/80: current_loss=10.30118 | best_loss=10.30118
Epoch 25/80: current_loss=10.32657 | best_loss=10.30118
Epoch 26/80: current_loss=10.31183 | best_loss=10.30118
Epoch 27/80: current_loss=10.31575 | best_loss=10.30118
Epoch 28/80: current_loss=10.29903 | best_loss=10.29903
Epoch 29/80: current_loss=10.32691 | best_loss=10.29903
Epoch 30/80: current_loss=10.31168 | best_loss=10.29903
Epoch 31/80: current_loss=10.30522 | best_loss=10.29903
Epoch 32/80: current_loss=10.29409 | best_loss=10.29409
Epoch 33/80: current_loss=10.33334 | best_loss=10.29409
Epoch 34/80: current_loss=10.31553 | best_loss=10.29409
Epoch 35/80: current_loss=10.31043 | best_loss=10.29409
Epoch 36/80: current_loss=10.28580 | best_loss=10.28580
Epoch 37/80: current_loss=10.32350 | best_loss=10.28580
Epoch 38/80: current_loss=10.29282 | best_loss=10.28580
Epoch 39/80: current_loss=10.31714 | best_loss=10.28580
Epoch 40/80: current_loss=10.28472 | best_loss=10.28472
Epoch 41/80: current_loss=10.28566 | best_loss=10.28472
Epoch 42/80: current_loss=10.27877 | best_loss=10.27877
Epoch 43/80: current_loss=10.28796 | best_loss=10.27877
Epoch 44/80: current_loss=10.29994 | best_loss=10.27877
Epoch 45/80: current_loss=10.31443 | best_loss=10.27877
Epoch 46/80: current_loss=10.29531 | best_loss=10.27877
Epoch 47/80: current_loss=10.29277 | best_loss=10.27877
Epoch 48/80: current_loss=10.29150 | best_loss=10.27877
Epoch 49/80: current_loss=10.28667 | best_loss=10.27877
Epoch 50/80: current_loss=10.28777 | best_loss=10.27877
Epoch 51/80: current_loss=10.29337 | best_loss=10.27877
Epoch 52/80: current_loss=10.29096 | best_loss=10.27877
Epoch 53/80: current_loss=10.28773 | best_loss=10.27877
Epoch 54/80: current_loss=10.28825 | best_loss=10.27877
Epoch 55/80: current_loss=10.35317 | best_loss=10.27877
Epoch 56/80: current_loss=10.29711 | best_loss=10.27877
Epoch 57/80: current_loss=10.28466 | best_loss=10.27877
Epoch 58/80: current_loss=10.28977 | best_loss=10.27877
Epoch 59/80: current_loss=10.33030 | best_loss=10.27877
Epoch 60/80: current_loss=10.28094 | best_loss=10.27877
Epoch 61/80: current_loss=10.30918 | best_loss=10.27877
Epoch 62/80: current_loss=10.32564 | best_loss=10.27877
Early Stopping at epoch 62
      explained_var=-0.00359 | mse_loss=10.50548
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.48301 | best_loss=10.48301
Epoch 1/80: current_loss=10.44528 | best_loss=10.44528
Epoch 2/80: current_loss=10.42494 | best_loss=10.42494
Epoch 3/80: current_loss=10.40758 | best_loss=10.40758
Epoch 4/80: current_loss=10.41051 | best_loss=10.40758
Epoch 5/80: current_loss=10.40823 | best_loss=10.40758
Epoch 6/80: current_loss=10.42571 | best_loss=10.40758
Epoch 7/80: current_loss=10.41616 | best_loss=10.40758
Epoch 8/80: current_loss=10.48297 | best_loss=10.40758
Epoch 9/80: current_loss=10.42780 | best_loss=10.40758
Epoch 10/80: current_loss=10.41463 | best_loss=10.40758
Epoch 11/80: current_loss=10.41676 | best_loss=10.40758
Epoch 12/80: current_loss=10.42929 | best_loss=10.40758
Epoch 13/80: current_loss=10.40568 | best_loss=10.40568
Epoch 14/80: current_loss=10.39488 | best_loss=10.39488
Epoch 15/80: current_loss=10.40674 | best_loss=10.39488
Epoch 16/80: current_loss=10.43142 | best_loss=10.39488
Epoch 17/80: current_loss=10.43755 | best_loss=10.39488
Epoch 18/80: current_loss=10.40469 | best_loss=10.39488
Epoch 19/80: current_loss=10.40313 | best_loss=10.39488
Epoch 20/80: current_loss=10.39282 | best_loss=10.39282
Epoch 21/80: current_loss=10.39273 | best_loss=10.39273
Epoch 22/80: current_loss=10.44947 | best_loss=10.39273
Epoch 23/80: current_loss=10.40897 | best_loss=10.39273
Epoch 24/80: current_loss=10.39931 | best_loss=10.39273
Epoch 25/80: current_loss=10.40515 | best_loss=10.39273
Epoch 26/80: current_loss=10.41903 | best_loss=10.39273
Epoch 27/80: current_loss=10.40860 | best_loss=10.39273
Epoch 28/80: current_loss=10.44982 | best_loss=10.39273
Epoch 29/80: current_loss=10.42576 | best_loss=10.39273
Epoch 30/80: current_loss=10.44350 | best_loss=10.39273
Epoch 31/80: current_loss=10.40133 | best_loss=10.39273
Epoch 32/80: current_loss=10.40884 | best_loss=10.39273
Epoch 33/80: current_loss=10.43039 | best_loss=10.39273
Epoch 34/80: current_loss=10.41588 | best_loss=10.39273
Epoch 35/80: current_loss=10.46837 | best_loss=10.39273
Epoch 36/80: current_loss=10.40099 | best_loss=10.39273
Epoch 37/80: current_loss=10.40703 | best_loss=10.39273
Epoch 38/80: current_loss=10.41076 | best_loss=10.39273
Epoch 39/80: current_loss=10.41396 | best_loss=10.39273
Epoch 40/80: current_loss=10.47136 | best_loss=10.39273
Epoch 41/80: current_loss=10.39930 | best_loss=10.39273
Early Stopping at epoch 41
      explained_var=0.00306 | mse_loss=10.04234
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.49135 | best_loss=7.49135
Epoch 1/80: current_loss=7.59700 | best_loss=7.49135
Epoch 2/80: current_loss=7.60468 | best_loss=7.49135
Epoch 3/80: current_loss=7.59768 | best_loss=7.49135
Epoch 4/80: current_loss=7.53126 | best_loss=7.49135
Epoch 5/80: current_loss=7.55304 | best_loss=7.49135
Epoch 6/80: current_loss=7.58941 | best_loss=7.49135
Epoch 7/80: current_loss=7.59723 | best_loss=7.49135
Epoch 8/80: current_loss=7.54001 | best_loss=7.49135
Epoch 9/80: current_loss=7.54388 | best_loss=7.49135
Epoch 10/80: current_loss=7.58061 | best_loss=7.49135
Epoch 11/80: current_loss=7.50497 | best_loss=7.49135
Epoch 12/80: current_loss=7.58691 | best_loss=7.49135
Epoch 13/80: current_loss=7.48804 | best_loss=7.48804
Epoch 14/80: current_loss=7.61060 | best_loss=7.48804
Epoch 15/80: current_loss=7.49409 | best_loss=7.48804
Epoch 16/80: current_loss=7.55088 | best_loss=7.48804
Epoch 17/80: current_loss=7.90243 | best_loss=7.48804
Epoch 18/80: current_loss=7.67203 | best_loss=7.48804
Epoch 19/80: current_loss=7.55066 | best_loss=7.48804
Epoch 20/80: current_loss=7.55197 | best_loss=7.48804
Epoch 21/80: current_loss=7.50914 | best_loss=7.48804
Epoch 22/80: current_loss=7.72585 | best_loss=7.48804
Epoch 23/80: current_loss=7.51846 | best_loss=7.48804
Epoch 24/80: current_loss=7.59129 | best_loss=7.48804
Epoch 25/80: current_loss=7.58011 | best_loss=7.48804
Epoch 26/80: current_loss=7.58578 | best_loss=7.48804
Epoch 27/80: current_loss=7.67609 | best_loss=7.48804
Epoch 28/80: current_loss=7.65074 | best_loss=7.48804
Epoch 29/80: current_loss=7.56649 | best_loss=7.48804
Epoch 30/80: current_loss=7.85959 | best_loss=7.48804
Epoch 31/80: current_loss=7.53441 | best_loss=7.48804
Epoch 32/80: current_loss=7.56577 | best_loss=7.48804
Epoch 33/80: current_loss=7.67227 | best_loss=7.48804
Early Stopping at epoch 33
      explained_var=0.00233 | mse_loss=7.58744
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.76743 | best_loss=8.76743
Epoch 1/80: current_loss=8.79760 | best_loss=8.76743
Epoch 2/80: current_loss=8.86329 | best_loss=8.76743
Epoch 3/80: current_loss=8.78918 | best_loss=8.76743
Epoch 4/80: current_loss=8.82857 | best_loss=8.76743
Epoch 5/80: current_loss=9.10479 | best_loss=8.76743
Epoch 6/80: current_loss=8.74581 | best_loss=8.74581
Epoch 7/80: current_loss=8.75275 | best_loss=8.74581
Epoch 8/80: current_loss=8.79404 | best_loss=8.74581
Epoch 9/80: current_loss=8.87501 | best_loss=8.74581
Epoch 10/80: current_loss=8.77797 | best_loss=8.74581
Epoch 11/80: current_loss=8.85525 | best_loss=8.74581
Epoch 12/80: current_loss=8.78371 | best_loss=8.74581
Epoch 13/80: current_loss=9.27422 | best_loss=8.74581
Epoch 14/80: current_loss=8.83512 | best_loss=8.74581
Epoch 15/80: current_loss=9.03040 | best_loss=8.74581
Epoch 16/80: current_loss=8.77571 | best_loss=8.74581
Epoch 17/80: current_loss=8.76684 | best_loss=8.74581
Epoch 18/80: current_loss=8.85306 | best_loss=8.74581
Epoch 19/80: current_loss=8.91010 | best_loss=8.74581
Epoch 20/80: current_loss=8.79580 | best_loss=8.74581
Epoch 21/80: current_loss=8.87482 | best_loss=8.74581
Epoch 22/80: current_loss=8.84499 | best_loss=8.74581
Epoch 23/80: current_loss=8.77771 | best_loss=8.74581
Epoch 24/80: current_loss=8.76614 | best_loss=8.74581
Epoch 25/80: current_loss=8.90748 | best_loss=8.74581
Epoch 26/80: current_loss=8.78572 | best_loss=8.74581
Early Stopping at epoch 26
      explained_var=0.00140 | mse_loss=8.71158
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.17720 | best_loss=9.17720
Epoch 1/80: current_loss=9.25463 | best_loss=9.17720
Epoch 2/80: current_loss=9.21578 | best_loss=9.17720
Epoch 3/80: current_loss=9.17288 | best_loss=9.17288
Epoch 4/80: current_loss=9.19116 | best_loss=9.17288
Epoch 5/80: current_loss=9.21529 | best_loss=9.17288
Epoch 6/80: current_loss=9.25320 | best_loss=9.17288
Epoch 7/80: current_loss=9.21469 | best_loss=9.17288
Epoch 8/80: current_loss=9.25746 | best_loss=9.17288
Epoch 9/80: current_loss=9.20469 | best_loss=9.17288
Epoch 10/80: current_loss=9.21477 | best_loss=9.17288
Epoch 11/80: current_loss=9.23128 | best_loss=9.17288
Epoch 12/80: current_loss=9.30565 | best_loss=9.17288
Epoch 13/80: current_loss=9.23142 | best_loss=9.17288
Epoch 14/80: current_loss=9.18877 | best_loss=9.17288
Epoch 15/80: current_loss=9.23519 | best_loss=9.17288
Epoch 16/80: current_loss=9.21781 | best_loss=9.17288
Epoch 17/80: current_loss=9.21749 | best_loss=9.17288
Epoch 18/80: current_loss=9.52657 | best_loss=9.17288
Epoch 19/80: current_loss=9.27989 | best_loss=9.17288
Epoch 20/80: current_loss=9.18655 | best_loss=9.17288
Epoch 21/80: current_loss=9.18904 | best_loss=9.17288
Epoch 22/80: current_loss=9.17061 | best_loss=9.17061
Epoch 23/80: current_loss=9.14894 | best_loss=9.14894
Epoch 24/80: current_loss=9.20134 | best_loss=9.14894
Epoch 25/80: current_loss=9.18765 | best_loss=9.14894
Epoch 26/80: current_loss=9.41283 | best_loss=9.14894
Epoch 27/80: current_loss=9.24534 | best_loss=9.14894
Epoch 28/80: current_loss=9.26161 | best_loss=9.14894
Epoch 29/80: current_loss=9.37858 | best_loss=9.14894
Epoch 30/80: current_loss=9.22342 | best_loss=9.14894
Epoch 31/80: current_loss=9.29745 | best_loss=9.14894
Epoch 32/80: current_loss=9.22609 | best_loss=9.14894
Epoch 33/80: current_loss=9.20572 | best_loss=9.14894
Epoch 34/80: current_loss=9.31555 | best_loss=9.14894
Epoch 35/80: current_loss=9.19512 | best_loss=9.14894
Epoch 36/80: current_loss=9.21649 | best_loss=9.14894
Epoch 37/80: current_loss=9.19837 | best_loss=9.14894
Epoch 38/80: current_loss=9.20692 | best_loss=9.14894
Epoch 39/80: current_loss=9.15344 | best_loss=9.14894
Epoch 40/80: current_loss=9.16658 | best_loss=9.14894
Epoch 41/80: current_loss=9.18912 | best_loss=9.14894
Epoch 42/80: current_loss=9.21936 | best_loss=9.14894
Epoch 43/80: current_loss=9.18590 | best_loss=9.14894
Early Stopping at epoch 43
      explained_var=0.02004 | mse_loss=9.10931
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.00465| avg_loss=9.19123
----------------------------------------------


----------------------------------------------
Params for Trial 23
{'learning_rate': 0.001, 'weight_decay': 0.007915188809301588, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=15.71769 | best_loss=15.71769
Epoch 1/80: current_loss=10.72679 | best_loss=10.72679
Epoch 2/80: current_loss=10.48673 | best_loss=10.48673
Epoch 3/80: current_loss=10.42336 | best_loss=10.42336
Epoch 4/80: current_loss=10.37292 | best_loss=10.37292
Epoch 5/80: current_loss=10.35853 | best_loss=10.35853
Epoch 6/80: current_loss=10.29834 | best_loss=10.29834
Epoch 7/80: current_loss=10.31359 | best_loss=10.29834
Epoch 8/80: current_loss=10.29598 | best_loss=10.29598
Epoch 9/80: current_loss=10.28908 | best_loss=10.28908
Epoch 10/80: current_loss=10.28302 | best_loss=10.28302
Epoch 11/80: current_loss=10.30169 | best_loss=10.28302
Epoch 12/80: current_loss=10.30162 | best_loss=10.28302
Epoch 13/80: current_loss=10.29343 | best_loss=10.28302
Epoch 14/80: current_loss=10.29912 | best_loss=10.28302
Epoch 15/80: current_loss=10.28054 | best_loss=10.28054
Epoch 16/80: current_loss=10.28779 | best_loss=10.28054
Epoch 17/80: current_loss=10.32013 | best_loss=10.28054
Epoch 18/80: current_loss=10.28479 | best_loss=10.28054
Epoch 19/80: current_loss=10.30451 | best_loss=10.28054
Epoch 20/80: current_loss=10.31230 | best_loss=10.28054
Epoch 21/80: current_loss=10.27671 | best_loss=10.27671
Epoch 22/80: current_loss=10.28231 | best_loss=10.27671
Epoch 23/80: current_loss=10.28082 | best_loss=10.27671
Epoch 24/80: current_loss=10.27637 | best_loss=10.27637
Epoch 25/80: current_loss=10.28098 | best_loss=10.27637
Epoch 26/80: current_loss=10.30505 | best_loss=10.27637
Epoch 27/80: current_loss=10.28759 | best_loss=10.27637
Epoch 28/80: current_loss=10.29832 | best_loss=10.27637
Epoch 29/80: current_loss=10.29505 | best_loss=10.27637
Epoch 30/80: current_loss=10.27109 | best_loss=10.27109
Epoch 31/80: current_loss=10.27808 | best_loss=10.27109
Epoch 32/80: current_loss=10.27757 | best_loss=10.27109
Epoch 33/80: current_loss=10.28454 | best_loss=10.27109
Epoch 34/80: current_loss=10.27584 | best_loss=10.27109
Epoch 35/80: current_loss=10.26812 | best_loss=10.26812
Epoch 36/80: current_loss=10.31158 | best_loss=10.26812
Epoch 37/80: current_loss=10.28670 | best_loss=10.26812
Epoch 38/80: current_loss=10.29119 | best_loss=10.26812
Epoch 39/80: current_loss=10.28402 | best_loss=10.26812
Epoch 40/80: current_loss=10.28125 | best_loss=10.26812
Epoch 41/80: current_loss=10.32504 | best_loss=10.26812
Epoch 42/80: current_loss=10.27869 | best_loss=10.26812
Epoch 43/80: current_loss=10.27344 | best_loss=10.26812
Epoch 44/80: current_loss=10.29409 | best_loss=10.26812
Epoch 45/80: current_loss=10.27595 | best_loss=10.26812
Epoch 46/80: current_loss=10.27456 | best_loss=10.26812
Epoch 47/80: current_loss=10.28594 | best_loss=10.26812
Epoch 48/80: current_loss=10.34157 | best_loss=10.26812
Epoch 49/80: current_loss=10.28805 | best_loss=10.26812
Epoch 50/80: current_loss=10.34990 | best_loss=10.26812
Epoch 51/80: current_loss=10.31481 | best_loss=10.26812
Epoch 52/80: current_loss=10.28249 | best_loss=10.26812
Epoch 53/80: current_loss=10.28677 | best_loss=10.26812
Epoch 54/80: current_loss=10.29982 | best_loss=10.26812
Epoch 55/80: current_loss=10.28245 | best_loss=10.26812
Early Stopping at epoch 55
      explained_var=-0.00254 | mse_loss=10.48537
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.50575 | best_loss=10.50575
Epoch 1/80: current_loss=10.43511 | best_loss=10.43511
Epoch 2/80: current_loss=10.41358 | best_loss=10.41358
Epoch 3/80: current_loss=10.41146 | best_loss=10.41146
Epoch 4/80: current_loss=10.40548 | best_loss=10.40548
Epoch 5/80: current_loss=10.40986 | best_loss=10.40548
Epoch 6/80: current_loss=10.41683 | best_loss=10.40548
Epoch 7/80: current_loss=10.41888 | best_loss=10.40548
Epoch 8/80: current_loss=10.39618 | best_loss=10.39618
Epoch 9/80: current_loss=10.41232 | best_loss=10.39618
Epoch 10/80: current_loss=10.41433 | best_loss=10.39618
Epoch 11/80: current_loss=10.40430 | best_loss=10.39618
Epoch 12/80: current_loss=10.40670 | best_loss=10.39618
Epoch 13/80: current_loss=10.54440 | best_loss=10.39618
Epoch 14/80: current_loss=10.40995 | best_loss=10.39618
Epoch 15/80: current_loss=10.43736 | best_loss=10.39618
Epoch 16/80: current_loss=10.42971 | best_loss=10.39618
Epoch 17/80: current_loss=10.40522 | best_loss=10.39618
Epoch 18/80: current_loss=10.40440 | best_loss=10.39618
Epoch 19/80: current_loss=10.42674 | best_loss=10.39618
Epoch 20/80: current_loss=10.40987 | best_loss=10.39618
Epoch 21/80: current_loss=10.42598 | best_loss=10.39618
Epoch 22/80: current_loss=10.44180 | best_loss=10.39618
Epoch 23/80: current_loss=10.49475 | best_loss=10.39618
Epoch 24/80: current_loss=10.40480 | best_loss=10.39618
Epoch 25/80: current_loss=10.40602 | best_loss=10.39618
Epoch 26/80: current_loss=10.43068 | best_loss=10.39618
Epoch 27/80: current_loss=10.42556 | best_loss=10.39618
Epoch 28/80: current_loss=10.40633 | best_loss=10.39618
Early Stopping at epoch 28
      explained_var=0.00147 | mse_loss=10.04958
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.61152 | best_loss=7.61152
Epoch 1/80: current_loss=7.83093 | best_loss=7.61152
Epoch 2/80: current_loss=7.57994 | best_loss=7.57994
Epoch 3/80: current_loss=7.56520 | best_loss=7.56520
Epoch 4/80: current_loss=7.57625 | best_loss=7.56520
Epoch 5/80: current_loss=7.57310 | best_loss=7.56520
Epoch 6/80: current_loss=7.57771 | best_loss=7.56520
Epoch 7/80: current_loss=7.58350 | best_loss=7.56520
Epoch 8/80: current_loss=7.48768 | best_loss=7.48768
Epoch 9/80: current_loss=8.05986 | best_loss=7.48768
Epoch 10/80: current_loss=7.50597 | best_loss=7.48768
Epoch 11/80: current_loss=7.79461 | best_loss=7.48768
Epoch 12/80: current_loss=7.61127 | best_loss=7.48768
Epoch 13/80: current_loss=7.61949 | best_loss=7.48768
Epoch 14/80: current_loss=7.67237 | best_loss=7.48768
Epoch 15/80: current_loss=7.54824 | best_loss=7.48768
Epoch 16/80: current_loss=7.60131 | best_loss=7.48768
Epoch 17/80: current_loss=7.62496 | best_loss=7.48768
Epoch 18/80: current_loss=7.62869 | best_loss=7.48768
Epoch 19/80: current_loss=7.66896 | best_loss=7.48768
Epoch 20/80: current_loss=7.54349 | best_loss=7.48768
Epoch 21/80: current_loss=7.52679 | best_loss=7.48768
Epoch 22/80: current_loss=7.70706 | best_loss=7.48768
Epoch 23/80: current_loss=7.59161 | best_loss=7.48768
Epoch 24/80: current_loss=7.51562 | best_loss=7.48768
Epoch 25/80: current_loss=7.46603 | best_loss=7.46603
Epoch 26/80: current_loss=7.65628 | best_loss=7.46603
Epoch 27/80: current_loss=7.61746 | best_loss=7.46603
Epoch 28/80: current_loss=7.76335 | best_loss=7.46603
Epoch 29/80: current_loss=7.53442 | best_loss=7.46603
Epoch 30/80: current_loss=7.80869 | best_loss=7.46603
Epoch 31/80: current_loss=7.57253 | best_loss=7.46603
Epoch 32/80: current_loss=7.59198 | best_loss=7.46603
Epoch 33/80: current_loss=7.58985 | best_loss=7.46603
Epoch 34/80: current_loss=7.68954 | best_loss=7.46603
Epoch 35/80: current_loss=7.61851 | best_loss=7.46603
Epoch 36/80: current_loss=7.94703 | best_loss=7.46603
Epoch 37/80: current_loss=7.47877 | best_loss=7.46603
Epoch 38/80: current_loss=7.61489 | best_loss=7.46603
Epoch 39/80: current_loss=7.54446 | best_loss=7.46603
Epoch 40/80: current_loss=7.50750 | best_loss=7.46603
Epoch 41/80: current_loss=7.57426 | best_loss=7.46603
Epoch 42/80: current_loss=7.63042 | best_loss=7.46603
Epoch 43/80: current_loss=7.68719 | best_loss=7.46603
Epoch 44/80: current_loss=7.46697 | best_loss=7.46603
Epoch 45/80: current_loss=7.49064 | best_loss=7.46603
Early Stopping at epoch 45
      explained_var=0.00175 | mse_loss=7.57006
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.88522 | best_loss=8.88522
Epoch 1/80: current_loss=8.78975 | best_loss=8.78975
Epoch 2/80: current_loss=8.88042 | best_loss=8.78975
Epoch 3/80: current_loss=8.72592 | best_loss=8.72592
Epoch 4/80: current_loss=8.83746 | best_loss=8.72592
Epoch 5/80: current_loss=8.74579 | best_loss=8.72592
Epoch 6/80: current_loss=8.75756 | best_loss=8.72592
Epoch 7/80: current_loss=8.80950 | best_loss=8.72592
Epoch 8/80: current_loss=8.90491 | best_loss=8.72592
Epoch 9/80: current_loss=8.78513 | best_loss=8.72592
Epoch 10/80: current_loss=8.92092 | best_loss=8.72592
Epoch 11/80: current_loss=8.84904 | best_loss=8.72592
Epoch 12/80: current_loss=8.99961 | best_loss=8.72592
Epoch 13/80: current_loss=8.74830 | best_loss=8.72592
Epoch 14/80: current_loss=8.75234 | best_loss=8.72592
Epoch 15/80: current_loss=8.76095 | best_loss=8.72592
Epoch 16/80: current_loss=8.92249 | best_loss=8.72592
Epoch 17/80: current_loss=8.78070 | best_loss=8.72592
Epoch 18/80: current_loss=8.77009 | best_loss=8.72592
Epoch 19/80: current_loss=8.75642 | best_loss=8.72592
Epoch 20/80: current_loss=8.81601 | best_loss=8.72592
Epoch 21/80: current_loss=8.75368 | best_loss=8.72592
Epoch 22/80: current_loss=8.80526 | best_loss=8.72592
Epoch 23/80: current_loss=8.76251 | best_loss=8.72592
Early Stopping at epoch 23
      explained_var=0.00402 | mse_loss=8.68141
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.31962 | best_loss=9.31962
Epoch 1/80: current_loss=9.20075 | best_loss=9.20075
Epoch 2/80: current_loss=9.28180 | best_loss=9.20075
Epoch 3/80: current_loss=9.21271 | best_loss=9.20075
Epoch 4/80: current_loss=9.22830 | best_loss=9.20075
Epoch 5/80: current_loss=9.19297 | best_loss=9.19297
Epoch 6/80: current_loss=9.21219 | best_loss=9.19297
Epoch 7/80: current_loss=9.21578 | best_loss=9.19297
Epoch 8/80: current_loss=9.15160 | best_loss=9.15160
Epoch 9/80: current_loss=9.27185 | best_loss=9.15160
Epoch 10/80: current_loss=9.22132 | best_loss=9.15160
Epoch 11/80: current_loss=9.24714 | best_loss=9.15160
Epoch 12/80: current_loss=9.23091 | best_loss=9.15160
Epoch 13/80: current_loss=9.24199 | best_loss=9.15160
Epoch 14/80: current_loss=9.36653 | best_loss=9.15160
Epoch 15/80: current_loss=9.19990 | best_loss=9.15160
Epoch 16/80: current_loss=9.18378 | best_loss=9.15160
Epoch 17/80: current_loss=9.20664 | best_loss=9.15160
Epoch 18/80: current_loss=9.14565 | best_loss=9.14565
Epoch 19/80: current_loss=9.15465 | best_loss=9.14565
Epoch 20/80: current_loss=9.20726 | best_loss=9.14565
Epoch 21/80: current_loss=9.19648 | best_loss=9.14565
Epoch 22/80: current_loss=9.21538 | best_loss=9.14565
Epoch 23/80: current_loss=9.17304 | best_loss=9.14565
Epoch 24/80: current_loss=9.19382 | best_loss=9.14565
Epoch 25/80: current_loss=9.18399 | best_loss=9.14565
Epoch 26/80: current_loss=9.26158 | best_loss=9.14565
Epoch 27/80: current_loss=9.16646 | best_loss=9.14565
Epoch 28/80: current_loss=9.23623 | best_loss=9.14565
Epoch 29/80: current_loss=9.18435 | best_loss=9.14565
Epoch 30/80: current_loss=9.26489 | best_loss=9.14565
Epoch 31/80: current_loss=9.23961 | best_loss=9.14565
Epoch 32/80: current_loss=9.26420 | best_loss=9.14565
Epoch 33/80: current_loss=9.23546 | best_loss=9.14565
Epoch 34/80: current_loss=9.17411 | best_loss=9.14565
Epoch 35/80: current_loss=9.17181 | best_loss=9.14565
Epoch 36/80: current_loss=9.21873 | best_loss=9.14565
Epoch 37/80: current_loss=9.23437 | best_loss=9.14565
Epoch 38/80: current_loss=9.22865 | best_loss=9.14565
Early Stopping at epoch 38
      explained_var=0.02034 | mse_loss=9.10534
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00501| avg_loss=9.17835
----------------------------------------------


----------------------------------------------
Params for Trial 24
{'learning_rate': 1e-05, 'weight_decay': 0.008858912532230232, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=103.53971 | best_loss=103.53971
Epoch 1/80: current_loss=102.38341 | best_loss=102.38341
Epoch 2/80: current_loss=101.13408 | best_loss=101.13408
Epoch 3/80: current_loss=99.75747 | best_loss=99.75747
Epoch 4/80: current_loss=98.16761 | best_loss=98.16761
Epoch 5/80: current_loss=96.28956 | best_loss=96.28956
Epoch 6/80: current_loss=94.04679 | best_loss=94.04679
Epoch 7/80: current_loss=91.29155 | best_loss=91.29155
Epoch 8/80: current_loss=87.94619 | best_loss=87.94619
Epoch 9/80: current_loss=83.99026 | best_loss=83.99026
Epoch 10/80: current_loss=79.49682 | best_loss=79.49682
Epoch 11/80: current_loss=74.72460 | best_loss=74.72460
Epoch 12/80: current_loss=70.06538 | best_loss=70.06538
Epoch 13/80: current_loss=65.70169 | best_loss=65.70169
Epoch 14/80: current_loss=61.77542 | best_loss=61.77542
Epoch 15/80: current_loss=58.28768 | best_loss=58.28768
Epoch 16/80: current_loss=55.16609 | best_loss=55.16609
Epoch 17/80: current_loss=52.40548 | best_loss=52.40548
Epoch 18/80: current_loss=49.98079 | best_loss=49.98079
Epoch 19/80: current_loss=47.74966 | best_loss=47.74966
Epoch 20/80: current_loss=45.73885 | best_loss=45.73885
Epoch 21/80: current_loss=43.95572 | best_loss=43.95572
Epoch 22/80: current_loss=42.30104 | best_loss=42.30104
Epoch 23/80: current_loss=40.83212 | best_loss=40.83212
Epoch 24/80: current_loss=39.49455 | best_loss=39.49455
Epoch 25/80: current_loss=38.27222 | best_loss=38.27222
Epoch 26/80: current_loss=37.14460 | best_loss=37.14460
Epoch 27/80: current_loss=36.09893 | best_loss=36.09893
Epoch 28/80: current_loss=35.14488 | best_loss=35.14488
Epoch 29/80: current_loss=34.25796 | best_loss=34.25796
Epoch 30/80: current_loss=33.45179 | best_loss=33.45179
Epoch 31/80: current_loss=32.68836 | best_loss=32.68836
Epoch 32/80: current_loss=31.98335 | best_loss=31.98335
Epoch 33/80: current_loss=31.30789 | best_loss=31.30789
Epoch 34/80: current_loss=30.68294 | best_loss=30.68294
Epoch 35/80: current_loss=30.09332 | best_loss=30.09332
Epoch 36/80: current_loss=29.52764 | best_loss=29.52764
Epoch 37/80: current_loss=28.98789 | best_loss=28.98789
Epoch 38/80: current_loss=28.48221 | best_loss=28.48221
Epoch 39/80: current_loss=28.00238 | best_loss=28.00238
Epoch 40/80: current_loss=27.54622 | best_loss=27.54622
Epoch 41/80: current_loss=27.11620 | best_loss=27.11620
Epoch 42/80: current_loss=26.69663 | best_loss=26.69663
Epoch 43/80: current_loss=26.28447 | best_loss=26.28447
Epoch 44/80: current_loss=25.91417 | best_loss=25.91417
Epoch 45/80: current_loss=25.55670 | best_loss=25.55670
Epoch 46/80: current_loss=25.20546 | best_loss=25.20546
Epoch 47/80: current_loss=24.87225 | best_loss=24.87225
Epoch 48/80: current_loss=24.55644 | best_loss=24.55644
Epoch 49/80: current_loss=24.25030 | best_loss=24.25030
Epoch 50/80: current_loss=23.95258 | best_loss=23.95258
Epoch 51/80: current_loss=23.66842 | best_loss=23.66842
Epoch 52/80: current_loss=23.39248 | best_loss=23.39248
Epoch 53/80: current_loss=23.12975 | best_loss=23.12975
Epoch 54/80: current_loss=22.87016 | best_loss=22.87016
Epoch 55/80: current_loss=22.61804 | best_loss=22.61804
Epoch 56/80: current_loss=22.38146 | best_loss=22.38146
Epoch 57/80: current_loss=22.15326 | best_loss=22.15326
Epoch 58/80: current_loss=21.92032 | best_loss=21.92032
Epoch 59/80: current_loss=21.69837 | best_loss=21.69837
Epoch 60/80: current_loss=21.48428 | best_loss=21.48428
Epoch 61/80: current_loss=21.26835 | best_loss=21.26835
Epoch 62/80: current_loss=21.05540 | best_loss=21.05540
Epoch 63/80: current_loss=20.85017 | best_loss=20.85017
Epoch 64/80: current_loss=20.64565 | best_loss=20.64565
Epoch 65/80: current_loss=20.45676 | best_loss=20.45676
Epoch 66/80: current_loss=20.26332 | best_loss=20.26332
Epoch 67/80: current_loss=20.08423 | best_loss=20.08423
Epoch 68/80: current_loss=19.90636 | best_loss=19.90636
Epoch 69/80: current_loss=19.73243 | best_loss=19.73243
Epoch 70/80: current_loss=19.56765 | best_loss=19.56765
Epoch 71/80: current_loss=19.39793 | best_loss=19.39793
Epoch 72/80: current_loss=19.24025 | best_loss=19.24025
Epoch 73/80: current_loss=19.08565 | best_loss=19.08565
Epoch 74/80: current_loss=18.93215 | best_loss=18.93215
Epoch 75/80: current_loss=18.78322 | best_loss=18.78322
Epoch 76/80: current_loss=18.63501 | best_loss=18.63501
Epoch 77/80: current_loss=18.49416 | best_loss=18.49416
Epoch 78/80: current_loss=18.35953 | best_loss=18.35953
Epoch 79/80: current_loss=18.22436 | best_loss=18.22436
      explained_var=-0.20604 | mse_loss=18.65914

----------------------------------------------
Params for Trial 25
{'learning_rate': 0.001, 'weight_decay': 0.0060415855345961385, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.95138 | best_loss=16.95138
Epoch 1/80: current_loss=10.74968 | best_loss=10.74968
Epoch 2/80: current_loss=10.42780 | best_loss=10.42780
Epoch 3/80: current_loss=10.39268 | best_loss=10.39268
Epoch 4/80: current_loss=10.38646 | best_loss=10.38646
Epoch 5/80: current_loss=10.33909 | best_loss=10.33909
Epoch 6/80: current_loss=10.32973 | best_loss=10.32973
Epoch 7/80: current_loss=10.33188 | best_loss=10.32973
Epoch 8/80: current_loss=10.32136 | best_loss=10.32136
Epoch 9/80: current_loss=10.30531 | best_loss=10.30531
Epoch 10/80: current_loss=10.31474 | best_loss=10.30531
Epoch 11/80: current_loss=10.28598 | best_loss=10.28598
Epoch 12/80: current_loss=10.28818 | best_loss=10.28598
Epoch 13/80: current_loss=10.27175 | best_loss=10.27175
Epoch 14/80: current_loss=10.28758 | best_loss=10.27175
Epoch 15/80: current_loss=10.27550 | best_loss=10.27175
Epoch 16/80: current_loss=10.27205 | best_loss=10.27175
Epoch 17/80: current_loss=10.27418 | best_loss=10.27175
Epoch 18/80: current_loss=10.27754 | best_loss=10.27175
Epoch 19/80: current_loss=10.28453 | best_loss=10.27175
Epoch 20/80: current_loss=10.28213 | best_loss=10.27175
Epoch 21/80: current_loss=10.28308 | best_loss=10.27175
Epoch 22/80: current_loss=10.29337 | best_loss=10.27175
Epoch 23/80: current_loss=10.27353 | best_loss=10.27175
Epoch 24/80: current_loss=10.26918 | best_loss=10.26918
Epoch 25/80: current_loss=10.27602 | best_loss=10.26918
Epoch 26/80: current_loss=10.28774 | best_loss=10.26918
Epoch 27/80: current_loss=10.26699 | best_loss=10.26699
Epoch 28/80: current_loss=10.26725 | best_loss=10.26699
Epoch 29/80: current_loss=10.30248 | best_loss=10.26699
Epoch 30/80: current_loss=10.26766 | best_loss=10.26699
Epoch 31/80: current_loss=10.31172 | best_loss=10.26699
Epoch 32/80: current_loss=10.30499 | best_loss=10.26699
Epoch 33/80: current_loss=10.28884 | best_loss=10.26699
Epoch 34/80: current_loss=10.27930 | best_loss=10.26699
Epoch 35/80: current_loss=10.33431 | best_loss=10.26699
Epoch 36/80: current_loss=10.28262 | best_loss=10.26699
Epoch 37/80: current_loss=10.30141 | best_loss=10.26699
Epoch 38/80: current_loss=10.28520 | best_loss=10.26699
Epoch 39/80: current_loss=10.29461 | best_loss=10.26699
Epoch 40/80: current_loss=10.27052 | best_loss=10.26699
Epoch 41/80: current_loss=10.27253 | best_loss=10.26699
Epoch 42/80: current_loss=10.27645 | best_loss=10.26699
Epoch 43/80: current_loss=10.29080 | best_loss=10.26699
Epoch 44/80: current_loss=10.28297 | best_loss=10.26699
Epoch 45/80: current_loss=10.30780 | best_loss=10.26699
Epoch 46/80: current_loss=10.28358 | best_loss=10.26699
Epoch 47/80: current_loss=10.27107 | best_loss=10.26699
Early Stopping at epoch 47
      explained_var=-0.00191 | mse_loss=10.49310
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.40649 | best_loss=10.40649
Epoch 1/80: current_loss=10.40459 | best_loss=10.40459
Epoch 2/80: current_loss=10.44573 | best_loss=10.40459
Epoch 3/80: current_loss=10.45377 | best_loss=10.40459
Epoch 4/80: current_loss=10.41753 | best_loss=10.40459
Epoch 5/80: current_loss=10.46940 | best_loss=10.40459
Epoch 6/80: current_loss=10.43360 | best_loss=10.40459
Epoch 7/80: current_loss=10.46010 | best_loss=10.40459
Epoch 8/80: current_loss=10.40582 | best_loss=10.40459
Epoch 9/80: current_loss=10.43091 | best_loss=10.40459
Epoch 10/80: current_loss=10.40459 | best_loss=10.40459
Epoch 11/80: current_loss=10.49079 | best_loss=10.40459
Epoch 12/80: current_loss=10.40895 | best_loss=10.40459
Epoch 13/80: current_loss=10.51856 | best_loss=10.40459
Epoch 14/80: current_loss=10.43549 | best_loss=10.40459
Epoch 15/80: current_loss=10.41336 | best_loss=10.40459
Epoch 16/80: current_loss=10.40636 | best_loss=10.40459
Epoch 17/80: current_loss=10.40500 | best_loss=10.40459
Epoch 18/80: current_loss=10.50952 | best_loss=10.40459
Epoch 19/80: current_loss=10.44215 | best_loss=10.40459
Epoch 20/80: current_loss=10.42306 | best_loss=10.40459
Epoch 21/80: current_loss=10.40686 | best_loss=10.40459
Early Stopping at epoch 21
      explained_var=0.00019 | mse_loss=10.06156

----------------------------------------------
Params for Trial 26
{'learning_rate': 0.001, 'weight_decay': 0.004040717926401882, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=82.67912 | best_loss=82.67912
Epoch 1/80: current_loss=57.38829 | best_loss=57.38829
Epoch 2/80: current_loss=44.23576 | best_loss=44.23576
Epoch 3/80: current_loss=35.46663 | best_loss=35.46663
Epoch 4/80: current_loss=28.93067 | best_loss=28.93067
Epoch 5/80: current_loss=24.00417 | best_loss=24.00417
Epoch 6/80: current_loss=20.43203 | best_loss=20.43203
Epoch 7/80: current_loss=17.75042 | best_loss=17.75042
Epoch 8/80: current_loss=15.88300 | best_loss=15.88300
Epoch 9/80: current_loss=14.51853 | best_loss=14.51853
Epoch 10/80: current_loss=13.56237 | best_loss=13.56237
Epoch 11/80: current_loss=12.91149 | best_loss=12.91149
Epoch 12/80: current_loss=12.48924 | best_loss=12.48924
Epoch 13/80: current_loss=12.18226 | best_loss=12.18226
Epoch 14/80: current_loss=11.94785 | best_loss=11.94785
Epoch 15/80: current_loss=11.77787 | best_loss=11.77787
Epoch 16/80: current_loss=11.68477 | best_loss=11.68477
Epoch 17/80: current_loss=11.59921 | best_loss=11.59921
Epoch 18/80: current_loss=11.51721 | best_loss=11.51721
Epoch 19/80: current_loss=11.45845 | best_loss=11.45845
Epoch 20/80: current_loss=11.40380 | best_loss=11.40380
Epoch 21/80: current_loss=11.36566 | best_loss=11.36566
Epoch 22/80: current_loss=11.32303 | best_loss=11.32303
Epoch 23/80: current_loss=11.28717 | best_loss=11.28717
Epoch 24/80: current_loss=11.25948 | best_loss=11.25948
Epoch 25/80: current_loss=11.22273 | best_loss=11.22273
Epoch 26/80: current_loss=11.19478 | best_loss=11.19478
Epoch 27/80: current_loss=11.16727 | best_loss=11.16727
Epoch 28/80: current_loss=11.13762 | best_loss=11.13762
Epoch 29/80: current_loss=11.10860 | best_loss=11.10860
Epoch 30/80: current_loss=11.08092 | best_loss=11.08092
Epoch 31/80: current_loss=11.04323 | best_loss=11.04323
Epoch 32/80: current_loss=11.01708 | best_loss=11.01708
Epoch 33/80: current_loss=11.01076 | best_loss=11.01076
Epoch 34/80: current_loss=10.98714 | best_loss=10.98714
Epoch 35/80: current_loss=10.95322 | best_loss=10.95322
Epoch 36/80: current_loss=10.93921 | best_loss=10.93921
Epoch 37/80: current_loss=10.92374 | best_loss=10.92374
Epoch 38/80: current_loss=10.89630 | best_loss=10.89630
Epoch 39/80: current_loss=10.89320 | best_loss=10.89320
Epoch 40/80: current_loss=10.89141 | best_loss=10.89141
Epoch 41/80: current_loss=10.86743 | best_loss=10.86743
Epoch 42/80: current_loss=10.84496 | best_loss=10.84496
Epoch 43/80: current_loss=10.81607 | best_loss=10.81607
Epoch 44/80: current_loss=10.81382 | best_loss=10.81382
Epoch 45/80: current_loss=10.80970 | best_loss=10.80970
Epoch 46/80: current_loss=10.79696 | best_loss=10.79696
Epoch 47/80: current_loss=10.78862 | best_loss=10.78862
Epoch 48/80: current_loss=10.75843 | best_loss=10.75843
Epoch 49/80: current_loss=10.74595 | best_loss=10.74595
Epoch 50/80: current_loss=10.73439 | best_loss=10.73439
Epoch 51/80: current_loss=10.71713 | best_loss=10.71713
Epoch 52/80: current_loss=10.71201 | best_loss=10.71201
Epoch 53/80: current_loss=10.69029 | best_loss=10.69029
Epoch 54/80: current_loss=10.69732 | best_loss=10.69029
Epoch 55/80: current_loss=10.68485 | best_loss=10.68485
Epoch 56/80: current_loss=10.66612 | best_loss=10.66612
Epoch 57/80: current_loss=10.65672 | best_loss=10.65672
Epoch 58/80: current_loss=10.64967 | best_loss=10.64967
Epoch 59/80: current_loss=10.64168 | best_loss=10.64168
Epoch 60/80: current_loss=10.64094 | best_loss=10.64094
Epoch 61/80: current_loss=10.62268 | best_loss=10.62268
Epoch 62/80: current_loss=10.62551 | best_loss=10.62268
Epoch 63/80: current_loss=10.61423 | best_loss=10.61423
Epoch 64/80: current_loss=10.60035 | best_loss=10.60035
Epoch 65/80: current_loss=10.58038 | best_loss=10.58038
Epoch 66/80: current_loss=10.58202 | best_loss=10.58038
Epoch 67/80: current_loss=10.57977 | best_loss=10.57977
Epoch 68/80: current_loss=10.57187 | best_loss=10.57187
Epoch 69/80: current_loss=10.56803 | best_loss=10.56803
Epoch 70/80: current_loss=10.56587 | best_loss=10.56587
Epoch 71/80: current_loss=10.54930 | best_loss=10.54930
Epoch 72/80: current_loss=10.54574 | best_loss=10.54574
Epoch 73/80: current_loss=10.55419 | best_loss=10.54574
Epoch 74/80: current_loss=10.54203 | best_loss=10.54203
Epoch 75/80: current_loss=10.54849 | best_loss=10.54203
Epoch 76/80: current_loss=10.53630 | best_loss=10.53630
Epoch 77/80: current_loss=10.51414 | best_loss=10.51414
Epoch 78/80: current_loss=10.51712 | best_loss=10.51414
Epoch 79/80: current_loss=10.52356 | best_loss=10.51414
      explained_var=-0.02530 | mse_loss=10.78195

----------------------------------------------
Params for Trial 27
{'learning_rate': 0.001, 'weight_decay': 0.009964826816231847, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=12.25455 | best_loss=12.25455
Epoch 1/80: current_loss=11.16373 | best_loss=11.16373
Epoch 2/80: current_loss=10.76598 | best_loss=10.76598
Epoch 3/80: current_loss=10.65151 | best_loss=10.65151
Epoch 4/80: current_loss=10.72434 | best_loss=10.65151
Epoch 5/80: current_loss=10.40621 | best_loss=10.40621
Epoch 6/80: current_loss=10.40728 | best_loss=10.40621
Epoch 7/80: current_loss=10.52053 | best_loss=10.40621
Epoch 8/80: current_loss=10.66466 | best_loss=10.40621
Epoch 9/80: current_loss=10.35536 | best_loss=10.35536
Epoch 10/80: current_loss=10.31719 | best_loss=10.31719
Epoch 11/80: current_loss=10.52896 | best_loss=10.31719
Epoch 12/80: current_loss=10.32654 | best_loss=10.31719
Epoch 13/80: current_loss=10.33153 | best_loss=10.31719
Epoch 14/80: current_loss=10.29016 | best_loss=10.29016
Epoch 15/80: current_loss=10.47466 | best_loss=10.29016
Epoch 16/80: current_loss=11.05445 | best_loss=10.29016
Epoch 17/80: current_loss=10.36729 | best_loss=10.29016
Epoch 18/80: current_loss=10.54722 | best_loss=10.29016
Epoch 19/80: current_loss=10.37212 | best_loss=10.29016
Epoch 20/80: current_loss=10.35454 | best_loss=10.29016
Epoch 21/80: current_loss=10.48626 | best_loss=10.29016
Epoch 22/80: current_loss=10.30089 | best_loss=10.29016
Epoch 23/80: current_loss=10.27848 | best_loss=10.27848
Epoch 24/80: current_loss=10.31263 | best_loss=10.27848
Epoch 25/80: current_loss=10.26448 | best_loss=10.26448
Epoch 26/80: current_loss=10.33574 | best_loss=10.26448
Epoch 27/80: current_loss=10.26379 | best_loss=10.26379
Epoch 28/80: current_loss=10.29233 | best_loss=10.26379
Epoch 29/80: current_loss=10.32934 | best_loss=10.26379
Epoch 30/80: current_loss=10.43706 | best_loss=10.26379
Epoch 31/80: current_loss=10.59269 | best_loss=10.26379
Epoch 32/80: current_loss=10.49016 | best_loss=10.26379
Epoch 33/80: current_loss=10.28612 | best_loss=10.26379
Epoch 34/80: current_loss=10.28555 | best_loss=10.26379
Epoch 35/80: current_loss=10.48171 | best_loss=10.26379
Epoch 36/80: current_loss=10.38105 | best_loss=10.26379
Epoch 37/80: current_loss=10.40845 | best_loss=10.26379
Epoch 38/80: current_loss=10.40577 | best_loss=10.26379
Epoch 39/80: current_loss=10.31343 | best_loss=10.26379
Epoch 40/80: current_loss=10.65212 | best_loss=10.26379
Epoch 41/80: current_loss=10.59237 | best_loss=10.26379
Epoch 42/80: current_loss=10.45995 | best_loss=10.26379
Epoch 43/80: current_loss=10.65203 | best_loss=10.26379
Epoch 44/80: current_loss=10.36646 | best_loss=10.26379
Epoch 45/80: current_loss=10.28958 | best_loss=10.26379
Epoch 46/80: current_loss=10.50618 | best_loss=10.26379
Epoch 47/80: current_loss=10.62392 | best_loss=10.26379
Early Stopping at epoch 47
      explained_var=-0.00063 | mse_loss=10.46949
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.75232 | best_loss=10.75232
Epoch 1/80: current_loss=10.45570 | best_loss=10.45570
Epoch 2/80: current_loss=10.44253 | best_loss=10.44253
Epoch 3/80: current_loss=10.84189 | best_loss=10.44253
Epoch 4/80: current_loss=10.49082 | best_loss=10.44253
Epoch 5/80: current_loss=10.41316 | best_loss=10.41316
Epoch 6/80: current_loss=10.46398 | best_loss=10.41316
Epoch 7/80: current_loss=10.49548 | best_loss=10.41316
Epoch 8/80: current_loss=10.42509 | best_loss=10.41316
Epoch 9/80: current_loss=10.50907 | best_loss=10.41316
Epoch 10/80: current_loss=10.49210 | best_loss=10.41316
Epoch 11/80: current_loss=10.52832 | best_loss=10.41316
Epoch 12/80: current_loss=10.48767 | best_loss=10.41316
Epoch 13/80: current_loss=10.40859 | best_loss=10.40859
Epoch 14/80: current_loss=10.48409 | best_loss=10.40859
Epoch 15/80: current_loss=10.48834 | best_loss=10.40859
Epoch 16/80: current_loss=10.45317 | best_loss=10.40859
Epoch 17/80: current_loss=10.55516 | best_loss=10.40859
Epoch 18/80: current_loss=10.44046 | best_loss=10.40859
Epoch 19/80: current_loss=10.44490 | best_loss=10.40859
Epoch 20/80: current_loss=10.61409 | best_loss=10.40859
Epoch 21/80: current_loss=10.42900 | best_loss=10.40859
Epoch 22/80: current_loss=10.40926 | best_loss=10.40859
Epoch 23/80: current_loss=10.48529 | best_loss=10.40859
Epoch 24/80: current_loss=10.40401 | best_loss=10.40401
Epoch 25/80: current_loss=10.40875 | best_loss=10.40401
Epoch 26/80: current_loss=10.48269 | best_loss=10.40401
Epoch 27/80: current_loss=10.42849 | best_loss=10.40401
Epoch 28/80: current_loss=10.46954 | best_loss=10.40401
Epoch 29/80: current_loss=10.51062 | best_loss=10.40401
Epoch 30/80: current_loss=10.40630 | best_loss=10.40401
Epoch 31/80: current_loss=10.63232 | best_loss=10.40401
Epoch 32/80: current_loss=10.40939 | best_loss=10.40401
Epoch 33/80: current_loss=10.46483 | best_loss=10.40401
Epoch 34/80: current_loss=10.56344 | best_loss=10.40401
Epoch 35/80: current_loss=10.45068 | best_loss=10.40401
Epoch 36/80: current_loss=10.68220 | best_loss=10.40401
Epoch 37/80: current_loss=10.47572 | best_loss=10.40401
Epoch 38/80: current_loss=10.51556 | best_loss=10.40401
Epoch 39/80: current_loss=10.44125 | best_loss=10.40401
Epoch 40/80: current_loss=10.79700 | best_loss=10.40401
Epoch 41/80: current_loss=10.48843 | best_loss=10.40401
Epoch 42/80: current_loss=10.41268 | best_loss=10.40401
Epoch 43/80: current_loss=10.64351 | best_loss=10.40401
Epoch 44/80: current_loss=10.46313 | best_loss=10.40401
Early Stopping at epoch 44
      explained_var=0.00031 | mse_loss=10.06123

----------------------------------------------
Params for Trial 28
{'learning_rate': 0.001, 'weight_decay': 0.007563485077219246, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=35.55519 | best_loss=35.55519
Epoch 1/80: current_loss=21.38789 | best_loss=21.38789
Epoch 2/80: current_loss=14.87118 | best_loss=14.87118
Epoch 3/80: current_loss=11.94699 | best_loss=11.94699
Epoch 4/80: current_loss=10.81755 | best_loss=10.81755
Epoch 5/80: current_loss=10.48207 | best_loss=10.48207
Epoch 6/80: current_loss=10.37389 | best_loss=10.37389
Epoch 7/80: current_loss=10.36740 | best_loss=10.36740
Epoch 8/80: current_loss=10.33294 | best_loss=10.33294
Epoch 9/80: current_loss=10.33623 | best_loss=10.33294
Epoch 10/80: current_loss=10.32368 | best_loss=10.32368
Epoch 11/80: current_loss=10.31878 | best_loss=10.31878
Epoch 12/80: current_loss=10.30956 | best_loss=10.30956
Epoch 13/80: current_loss=10.29810 | best_loss=10.29810
Epoch 14/80: current_loss=10.30186 | best_loss=10.29810
Epoch 15/80: current_loss=10.30667 | best_loss=10.29810
Epoch 16/80: current_loss=10.30357 | best_loss=10.29810
Epoch 17/80: current_loss=10.28352 | best_loss=10.28352
Epoch 18/80: current_loss=10.30299 | best_loss=10.28352
Epoch 19/80: current_loss=10.30689 | best_loss=10.28352
Epoch 20/80: current_loss=10.28407 | best_loss=10.28352
Epoch 21/80: current_loss=10.28160 | best_loss=10.28160
Epoch 22/80: current_loss=10.29033 | best_loss=10.28160
Epoch 23/80: current_loss=10.28492 | best_loss=10.28160
Epoch 24/80: current_loss=10.29713 | best_loss=10.28160
Epoch 25/80: current_loss=10.29616 | best_loss=10.28160
Epoch 26/80: current_loss=10.29962 | best_loss=10.28160
Epoch 27/80: current_loss=10.28723 | best_loss=10.28160
Epoch 28/80: current_loss=10.28711 | best_loss=10.28160
Epoch 29/80: current_loss=10.28332 | best_loss=10.28160
Epoch 30/80: current_loss=10.28488 | best_loss=10.28160
Epoch 31/80: current_loss=10.32587 | best_loss=10.28160
Epoch 32/80: current_loss=10.29581 | best_loss=10.28160
Epoch 33/80: current_loss=10.28752 | best_loss=10.28160
Epoch 34/80: current_loss=10.29593 | best_loss=10.28160
Epoch 35/80: current_loss=10.28776 | best_loss=10.28160
Epoch 36/80: current_loss=10.31133 | best_loss=10.28160
Epoch 37/80: current_loss=10.29683 | best_loss=10.28160
Epoch 38/80: current_loss=10.29672 | best_loss=10.28160
Epoch 39/80: current_loss=10.28237 | best_loss=10.28160
Epoch 40/80: current_loss=10.27722 | best_loss=10.27722
Epoch 41/80: current_loss=10.27213 | best_loss=10.27213
Epoch 42/80: current_loss=10.28543 | best_loss=10.27213
Epoch 43/80: current_loss=10.27877 | best_loss=10.27213
Epoch 44/80: current_loss=10.28785 | best_loss=10.27213
Epoch 45/80: current_loss=10.27484 | best_loss=10.27213
Epoch 46/80: current_loss=10.29483 | best_loss=10.27213
Epoch 47/80: current_loss=10.28024 | best_loss=10.27213
Epoch 48/80: current_loss=10.29057 | best_loss=10.27213
Epoch 49/80: current_loss=10.27946 | best_loss=10.27213
Epoch 50/80: current_loss=10.29967 | best_loss=10.27213
Epoch 51/80: current_loss=10.27828 | best_loss=10.27213
Epoch 52/80: current_loss=10.29083 | best_loss=10.27213
Epoch 53/80: current_loss=10.31134 | best_loss=10.27213
Epoch 54/80: current_loss=10.30347 | best_loss=10.27213
Epoch 55/80: current_loss=10.28709 | best_loss=10.27213
Epoch 56/80: current_loss=10.30244 | best_loss=10.27213
Epoch 57/80: current_loss=10.30116 | best_loss=10.27213
Epoch 58/80: current_loss=10.27967 | best_loss=10.27213
Epoch 59/80: current_loss=10.28842 | best_loss=10.27213
Epoch 60/80: current_loss=10.30330 | best_loss=10.27213
Epoch 61/80: current_loss=10.28372 | best_loss=10.27213
Early Stopping at epoch 61
      explained_var=-0.00251 | mse_loss=10.49957
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.41791 | best_loss=10.41791
Epoch 1/80: current_loss=10.40287 | best_loss=10.40287
Epoch 2/80: current_loss=10.42500 | best_loss=10.40287
Epoch 3/80: current_loss=10.42485 | best_loss=10.40287
Epoch 4/80: current_loss=10.38523 | best_loss=10.38523
Epoch 5/80: current_loss=10.32891 | best_loss=10.32891
Epoch 6/80: current_loss=10.31512 | best_loss=10.31512
Epoch 7/80: current_loss=10.28803 | best_loss=10.28803
Epoch 8/80: current_loss=10.17649 | best_loss=10.17649
Epoch 9/80: current_loss=10.22763 | best_loss=10.17649
Epoch 10/80: current_loss=10.34560 | best_loss=10.17649
Epoch 11/80: current_loss=10.31612 | best_loss=10.17649
Epoch 12/80: current_loss=10.28276 | best_loss=10.17649
Epoch 13/80: current_loss=10.29563 | best_loss=10.17649
Epoch 14/80: current_loss=10.33530 | best_loss=10.17649
Epoch 15/80: current_loss=10.28675 | best_loss=10.17649
Epoch 16/80: current_loss=10.27273 | best_loss=10.17649
Epoch 17/80: current_loss=10.26710 | best_loss=10.17649
Epoch 18/80: current_loss=10.26550 | best_loss=10.17649
Epoch 19/80: current_loss=10.24926 | best_loss=10.17649
Epoch 20/80: current_loss=10.19316 | best_loss=10.17649
Epoch 21/80: current_loss=10.34726 | best_loss=10.17649
Epoch 22/80: current_loss=10.32987 | best_loss=10.17649
Epoch 23/80: current_loss=10.15852 | best_loss=10.15852
Epoch 24/80: current_loss=10.21892 | best_loss=10.15852
Epoch 25/80: current_loss=10.29152 | best_loss=10.15852
Epoch 26/80: current_loss=10.19148 | best_loss=10.15852
Epoch 27/80: current_loss=10.25134 | best_loss=10.15852
Epoch 28/80: current_loss=10.18541 | best_loss=10.15852
Epoch 29/80: current_loss=10.27166 | best_loss=10.15852
Epoch 30/80: current_loss=10.24550 | best_loss=10.15852
Epoch 31/80: current_loss=10.19077 | best_loss=10.15852
Epoch 32/80: current_loss=10.27777 | best_loss=10.15852
Epoch 33/80: current_loss=10.23519 | best_loss=10.15852
Epoch 34/80: current_loss=10.24659 | best_loss=10.15852
Epoch 35/80: current_loss=10.25749 | best_loss=10.15852
Epoch 36/80: current_loss=10.28245 | best_loss=10.15852
Epoch 37/80: current_loss=10.26259 | best_loss=10.15852
Epoch 38/80: current_loss=10.19658 | best_loss=10.15852
Epoch 39/80: current_loss=10.21811 | best_loss=10.15852
Epoch 40/80: current_loss=10.26045 | best_loss=10.15852
Epoch 41/80: current_loss=10.29905 | best_loss=10.15852
Epoch 42/80: current_loss=10.31590 | best_loss=10.15852
Epoch 43/80: current_loss=10.24550 | best_loss=10.15852
Early Stopping at epoch 43
      explained_var=0.02880 | mse_loss=9.79948
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.64329 | best_loss=7.64329
Epoch 1/80: current_loss=7.58844 | best_loss=7.58844
Epoch 2/80: current_loss=7.65877 | best_loss=7.58844
Epoch 3/80: current_loss=7.57639 | best_loss=7.57639
Epoch 4/80: current_loss=7.58153 | best_loss=7.57639
Epoch 5/80: current_loss=7.64843 | best_loss=7.57639
Epoch 6/80: current_loss=7.58701 | best_loss=7.57639
Epoch 7/80: current_loss=7.60162 | best_loss=7.57639
Epoch 8/80: current_loss=7.65204 | best_loss=7.57639
Epoch 9/80: current_loss=7.61049 | best_loss=7.57639
Epoch 10/80: current_loss=7.57867 | best_loss=7.57639
Epoch 11/80: current_loss=7.57416 | best_loss=7.57416
Epoch 12/80: current_loss=7.54182 | best_loss=7.54182
Epoch 13/80: current_loss=7.56162 | best_loss=7.54182
Epoch 14/80: current_loss=7.59916 | best_loss=7.54182
Epoch 15/80: current_loss=7.62264 | best_loss=7.54182
Epoch 16/80: current_loss=7.67449 | best_loss=7.54182
Epoch 17/80: current_loss=7.62796 | best_loss=7.54182
Epoch 18/80: current_loss=7.57142 | best_loss=7.54182
Epoch 19/80: current_loss=7.57256 | best_loss=7.54182
Epoch 20/80: current_loss=7.69570 | best_loss=7.54182
Epoch 21/80: current_loss=7.52417 | best_loss=7.52417
Epoch 22/80: current_loss=7.59268 | best_loss=7.52417
Epoch 23/80: current_loss=7.67397 | best_loss=7.52417
Epoch 24/80: current_loss=7.58881 | best_loss=7.52417
Epoch 25/80: current_loss=7.53169 | best_loss=7.52417
Epoch 26/80: current_loss=7.59135 | best_loss=7.52417
Epoch 27/80: current_loss=7.56711 | best_loss=7.52417
Epoch 28/80: current_loss=7.64316 | best_loss=7.52417
Epoch 29/80: current_loss=7.61640 | best_loss=7.52417
Epoch 30/80: current_loss=7.54659 | best_loss=7.52417
Epoch 31/80: current_loss=7.67401 | best_loss=7.52417
Epoch 32/80: current_loss=7.59820 | best_loss=7.52417
Epoch 33/80: current_loss=7.61588 | best_loss=7.52417
Epoch 34/80: current_loss=7.61820 | best_loss=7.52417
Epoch 35/80: current_loss=7.58830 | best_loss=7.52417
Epoch 36/80: current_loss=7.56605 | best_loss=7.52417
Epoch 37/80: current_loss=7.58179 | best_loss=7.52417
Epoch 38/80: current_loss=7.65889 | best_loss=7.52417
Epoch 39/80: current_loss=7.61071 | best_loss=7.52417
Epoch 40/80: current_loss=7.72843 | best_loss=7.52417
Epoch 41/80: current_loss=7.61131 | best_loss=7.52417
Early Stopping at epoch 41
      explained_var=-0.00453 | mse_loss=7.63107

----------------------------------------------
Params for Trial 29
{'learning_rate': 0.0001, 'weight_decay': 0.0013876994804289905, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=102.42451 | best_loss=102.42451
Epoch 1/80: current_loss=101.24697 | best_loss=101.24697
Epoch 2/80: current_loss=99.99427 | best_loss=99.99427
Epoch 3/80: current_loss=98.58769 | best_loss=98.58769
Epoch 4/80: current_loss=96.92530 | best_loss=96.92530
Epoch 5/80: current_loss=94.85175 | best_loss=94.85175
Epoch 6/80: current_loss=92.16764 | best_loss=92.16764
Epoch 7/80: current_loss=88.61721 | best_loss=88.61721
Epoch 8/80: current_loss=84.26015 | best_loss=84.26015
Epoch 9/80: current_loss=79.62730 | best_loss=79.62730
Epoch 10/80: current_loss=75.29833 | best_loss=75.29833
Epoch 11/80: current_loss=71.49100 | best_loss=71.49100
Epoch 12/80: current_loss=68.15639 | best_loss=68.15639
Epoch 13/80: current_loss=65.24954 | best_loss=65.24954
Epoch 14/80: current_loss=62.66039 | best_loss=62.66039
Epoch 15/80: current_loss=60.31691 | best_loss=60.31691
Epoch 16/80: current_loss=58.18540 | best_loss=58.18540
Epoch 17/80: current_loss=56.21835 | best_loss=56.21835
Epoch 18/80: current_loss=54.38909 | best_loss=54.38909
Epoch 19/80: current_loss=52.72464 | best_loss=52.72464
Epoch 20/80: current_loss=51.16737 | best_loss=51.16737
Epoch 21/80: current_loss=49.71099 | best_loss=49.71099
Epoch 22/80: current_loss=48.33190 | best_loss=48.33190
Epoch 23/80: current_loss=47.03460 | best_loss=47.03460
Epoch 24/80: current_loss=45.82659 | best_loss=45.82659
Epoch 25/80: current_loss=44.66457 | best_loss=44.66457
Epoch 26/80: current_loss=43.57651 | best_loss=43.57651
Epoch 27/80: current_loss=42.53117 | best_loss=42.53117
Epoch 28/80: current_loss=41.54005 | best_loss=41.54005
Epoch 29/80: current_loss=40.58944 | best_loss=40.58944
Epoch 30/80: current_loss=39.67123 | best_loss=39.67123
Epoch 31/80: current_loss=38.79253 | best_loss=38.79253
Epoch 32/80: current_loss=37.94604 | best_loss=37.94604
Epoch 33/80: current_loss=37.13022 | best_loss=37.13022
Epoch 34/80: current_loss=36.34518 | best_loss=36.34518
Epoch 35/80: current_loss=35.58772 | best_loss=35.58772
Epoch 36/80: current_loss=34.84303 | best_loss=34.84303
Epoch 37/80: current_loss=34.12458 | best_loss=34.12458
Epoch 38/80: current_loss=33.43965 | best_loss=33.43965
Epoch 39/80: current_loss=32.77099 | best_loss=32.77099
Epoch 40/80: current_loss=32.11764 | best_loss=32.11764
Epoch 41/80: current_loss=31.49061 | best_loss=31.49061
Epoch 42/80: current_loss=30.87361 | best_loss=30.87361
Epoch 43/80: current_loss=30.27551 | best_loss=30.27551
Epoch 44/80: current_loss=29.69693 | best_loss=29.69693
Epoch 45/80: current_loss=29.12938 | best_loss=29.12938
Epoch 46/80: current_loss=28.58286 | best_loss=28.58286
Epoch 47/80: current_loss=28.05945 | best_loss=28.05945
Epoch 48/80: current_loss=27.54592 | best_loss=27.54592
Epoch 49/80: current_loss=27.04223 | best_loss=27.04223
Epoch 50/80: current_loss=26.55472 | best_loss=26.55472
Epoch 51/80: current_loss=26.07853 | best_loss=26.07853
Epoch 52/80: current_loss=25.61838 | best_loss=25.61838
Epoch 53/80: current_loss=25.17198 | best_loss=25.17198
Epoch 54/80: current_loss=24.73785 | best_loss=24.73785
Epoch 55/80: current_loss=24.32770 | best_loss=24.32770
Epoch 56/80: current_loss=23.90973 | best_loss=23.90973
Epoch 57/80: current_loss=23.50490 | best_loss=23.50490
Epoch 58/80: current_loss=23.11479 | best_loss=23.11479
Epoch 59/80: current_loss=22.73921 | best_loss=22.73921
Epoch 60/80: current_loss=22.36773 | best_loss=22.36773
Epoch 61/80: current_loss=22.01148 | best_loss=22.01148
Epoch 62/80: current_loss=21.66644 | best_loss=21.66644
Epoch 63/80: current_loss=21.31866 | best_loss=21.31866
Epoch 64/80: current_loss=20.99601 | best_loss=20.99601
Epoch 65/80: current_loss=20.68108 | best_loss=20.68108
Epoch 66/80: current_loss=20.37274 | best_loss=20.37274
Epoch 67/80: current_loss=20.06202 | best_loss=20.06202
Epoch 68/80: current_loss=19.76995 | best_loss=19.76995
Epoch 69/80: current_loss=19.48111 | best_loss=19.48111
Epoch 70/80: current_loss=19.20407 | best_loss=19.20407
Epoch 71/80: current_loss=18.94100 | best_loss=18.94100
Epoch 72/80: current_loss=18.68172 | best_loss=18.68172
Epoch 73/80: current_loss=18.42639 | best_loss=18.42639
Epoch 74/80: current_loss=18.17989 | best_loss=18.17989
Epoch 75/80: current_loss=17.94324 | best_loss=17.94324
Epoch 76/80: current_loss=17.71181 | best_loss=17.71181
Epoch 77/80: current_loss=17.48737 | best_loss=17.48737
Epoch 78/80: current_loss=17.26423 | best_loss=17.26423
Epoch 79/80: current_loss=17.05728 | best_loss=17.05728
      explained_var=-0.11690 | mse_loss=17.57039

----------------------------------------------
Params for Trial 30
{'learning_rate': 0.01, 'weight_decay': 0.002046266758736492, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=12.97164 | best_loss=12.97164
Epoch 1/80: current_loss=11.59173 | best_loss=11.59173
Epoch 2/80: current_loss=12.19558 | best_loss=11.59173
Epoch 3/80: current_loss=11.30381 | best_loss=11.30381
Epoch 4/80: current_loss=10.99334 | best_loss=10.99334
Epoch 5/80: current_loss=11.29851 | best_loss=10.99334
Epoch 6/80: current_loss=11.47174 | best_loss=10.99334
Epoch 7/80: current_loss=11.03545 | best_loss=10.99334
Epoch 8/80: current_loss=10.88602 | best_loss=10.88602
Epoch 9/80: current_loss=10.54485 | best_loss=10.54485
Epoch 10/80: current_loss=10.65660 | best_loss=10.54485
Epoch 11/80: current_loss=10.80702 | best_loss=10.54485
Epoch 12/80: current_loss=10.58277 | best_loss=10.54485
Epoch 13/80: current_loss=10.55668 | best_loss=10.54485
Epoch 14/80: current_loss=10.44315 | best_loss=10.44315
Epoch 15/80: current_loss=10.44074 | best_loss=10.44074
Epoch 16/80: current_loss=10.41248 | best_loss=10.41248
Epoch 17/80: current_loss=10.87194 | best_loss=10.41248
Epoch 18/80: current_loss=10.40240 | best_loss=10.40240
Epoch 19/80: current_loss=10.47072 | best_loss=10.40240
Epoch 20/80: current_loss=10.50035 | best_loss=10.40240
Epoch 21/80: current_loss=10.53703 | best_loss=10.40240
Epoch 22/80: current_loss=10.37588 | best_loss=10.37588
Epoch 23/80: current_loss=10.50200 | best_loss=10.37588
Epoch 24/80: current_loss=10.67019 | best_loss=10.37588
Epoch 25/80: current_loss=10.41622 | best_loss=10.37588
Epoch 26/80: current_loss=10.51849 | best_loss=10.37588
Epoch 27/80: current_loss=10.65884 | best_loss=10.37588
Epoch 28/80: current_loss=10.66116 | best_loss=10.37588
Epoch 29/80: current_loss=10.38324 | best_loss=10.37588
Epoch 30/80: current_loss=10.37030 | best_loss=10.37030
Epoch 31/80: current_loss=10.44419 | best_loss=10.37030
Epoch 32/80: current_loss=10.41663 | best_loss=10.37030
Epoch 33/80: current_loss=10.48063 | best_loss=10.37030
Epoch 34/80: current_loss=11.00666 | best_loss=10.37030
Epoch 35/80: current_loss=10.32077 | best_loss=10.32077
Epoch 36/80: current_loss=10.35933 | best_loss=10.32077
Epoch 37/80: current_loss=10.52354 | best_loss=10.32077
Epoch 38/80: current_loss=10.39501 | best_loss=10.32077
Epoch 39/80: current_loss=10.49667 | best_loss=10.32077
Epoch 40/80: current_loss=10.49546 | best_loss=10.32077
Epoch 41/80: current_loss=10.32483 | best_loss=10.32077
Epoch 42/80: current_loss=11.28931 | best_loss=10.32077
Epoch 43/80: current_loss=10.45602 | best_loss=10.32077
Epoch 44/80: current_loss=10.35292 | best_loss=10.32077
Epoch 45/80: current_loss=10.65256 | best_loss=10.32077
Epoch 46/80: current_loss=11.39701 | best_loss=10.32077
Epoch 47/80: current_loss=10.77343 | best_loss=10.32077
Epoch 48/80: current_loss=10.31809 | best_loss=10.31809
Epoch 49/80: current_loss=10.60992 | best_loss=10.31809
Epoch 50/80: current_loss=10.75932 | best_loss=10.31809
Epoch 51/80: current_loss=10.85779 | best_loss=10.31809
Epoch 52/80: current_loss=10.48490 | best_loss=10.31809
Epoch 53/80: current_loss=10.31955 | best_loss=10.31809
Epoch 54/80: current_loss=10.26743 | best_loss=10.26743
Epoch 55/80: current_loss=10.29004 | best_loss=10.26743
Epoch 56/80: current_loss=10.32029 | best_loss=10.26743
Epoch 57/80: current_loss=11.23922 | best_loss=10.26743
Epoch 58/80: current_loss=10.53677 | best_loss=10.26743
Epoch 59/80: current_loss=11.04582 | best_loss=10.26743
Epoch 60/80: current_loss=10.37852 | best_loss=10.26743
Epoch 61/80: current_loss=10.74719 | best_loss=10.26743
Epoch 62/80: current_loss=10.85607 | best_loss=10.26743
Epoch 63/80: current_loss=10.50945 | best_loss=10.26743
Epoch 64/80: current_loss=10.59169 | best_loss=10.26743
Epoch 65/80: current_loss=10.78000 | best_loss=10.26743
Epoch 66/80: current_loss=10.89845 | best_loss=10.26743
Epoch 67/80: current_loss=10.61233 | best_loss=10.26743
Epoch 68/80: current_loss=10.36045 | best_loss=10.26743
Epoch 69/80: current_loss=10.27515 | best_loss=10.26743
Epoch 70/80: current_loss=10.39663 | best_loss=10.26743
Epoch 71/80: current_loss=10.38335 | best_loss=10.26743
Epoch 72/80: current_loss=10.39793 | best_loss=10.26743
Epoch 73/80: current_loss=10.99534 | best_loss=10.26743
Epoch 74/80: current_loss=10.44050 | best_loss=10.26743
Early Stopping at epoch 74
      explained_var=-0.00172 | mse_loss=10.47370
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.65499 | best_loss=10.65499
Epoch 1/80: current_loss=10.46352 | best_loss=10.46352
Epoch 2/80: current_loss=10.45828 | best_loss=10.45828
Epoch 3/80: current_loss=10.58959 | best_loss=10.45828
Epoch 4/80: current_loss=11.02221 | best_loss=10.45828
Epoch 5/80: current_loss=10.76183 | best_loss=10.45828
Epoch 6/80: current_loss=10.43472 | best_loss=10.43472
Epoch 7/80: current_loss=10.43950 | best_loss=10.43472
Epoch 8/80: current_loss=10.48871 | best_loss=10.43472
Epoch 9/80: current_loss=10.59989 | best_loss=10.43472
Epoch 10/80: current_loss=10.43636 | best_loss=10.43472
Epoch 11/80: current_loss=10.42224 | best_loss=10.42224
Epoch 12/80: current_loss=10.47250 | best_loss=10.42224
Epoch 13/80: current_loss=10.86570 | best_loss=10.42224
Epoch 14/80: current_loss=10.58793 | best_loss=10.42224
Epoch 15/80: current_loss=11.54592 | best_loss=10.42224
Epoch 16/80: current_loss=10.43161 | best_loss=10.42224
Epoch 17/80: current_loss=10.46446 | best_loss=10.42224
Epoch 18/80: current_loss=10.44480 | best_loss=10.42224
Epoch 19/80: current_loss=10.80385 | best_loss=10.42224
Epoch 20/80: current_loss=10.84072 | best_loss=10.42224
Epoch 21/80: current_loss=10.97674 | best_loss=10.42224
Epoch 22/80: current_loss=11.04187 | best_loss=10.42224
Epoch 23/80: current_loss=10.89087 | best_loss=10.42224
Epoch 24/80: current_loss=10.44365 | best_loss=10.42224
Epoch 25/80: current_loss=10.73320 | best_loss=10.42224
Epoch 26/80: current_loss=10.73022 | best_loss=10.42224
Epoch 27/80: current_loss=10.60771 | best_loss=10.42224
Epoch 28/80: current_loss=10.69132 | best_loss=10.42224
Epoch 29/80: current_loss=10.51190 | best_loss=10.42224
Epoch 30/80: current_loss=10.64647 | best_loss=10.42224
Epoch 31/80: current_loss=10.58759 | best_loss=10.42224
Early Stopping at epoch 31
      explained_var=-0.00081 | mse_loss=10.07891

----------------------------------------------
Params for Trial 31
{'learning_rate': 0.001, 'weight_decay': 0.007198955871355653, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=19.43653 | best_loss=19.43653
Epoch 1/80: current_loss=11.16184 | best_loss=11.16184
Epoch 2/80: current_loss=10.44123 | best_loss=10.44123
Epoch 3/80: current_loss=10.38945 | best_loss=10.38945
Epoch 4/80: current_loss=10.35287 | best_loss=10.35287
Epoch 5/80: current_loss=10.34626 | best_loss=10.34626
Epoch 6/80: current_loss=10.32487 | best_loss=10.32487
Epoch 7/80: current_loss=10.31500 | best_loss=10.31500
Epoch 8/80: current_loss=10.31589 | best_loss=10.31500
Epoch 9/80: current_loss=10.29122 | best_loss=10.29122
Epoch 10/80: current_loss=10.28673 | best_loss=10.28673
Epoch 11/80: current_loss=10.31135 | best_loss=10.28673
Epoch 12/80: current_loss=10.29334 | best_loss=10.28673
Epoch 13/80: current_loss=10.29827 | best_loss=10.28673
Epoch 14/80: current_loss=10.30157 | best_loss=10.28673
Epoch 15/80: current_loss=10.29077 | best_loss=10.28673
Epoch 16/80: current_loss=10.28753 | best_loss=10.28673
Epoch 17/80: current_loss=10.29172 | best_loss=10.28673
Epoch 18/80: current_loss=10.27591 | best_loss=10.27591
Epoch 19/80: current_loss=10.30883 | best_loss=10.27591
Epoch 20/80: current_loss=10.28809 | best_loss=10.27591
Epoch 21/80: current_loss=10.29283 | best_loss=10.27591
Epoch 22/80: current_loss=10.28315 | best_loss=10.27591
Epoch 23/80: current_loss=10.30601 | best_loss=10.27591
Epoch 24/80: current_loss=10.29440 | best_loss=10.27591
Epoch 25/80: current_loss=10.29558 | best_loss=10.27591
Epoch 26/80: current_loss=10.28317 | best_loss=10.27591
Epoch 27/80: current_loss=10.29687 | best_loss=10.27591
Epoch 28/80: current_loss=10.27482 | best_loss=10.27482
Epoch 29/80: current_loss=10.28927 | best_loss=10.27482
Epoch 30/80: current_loss=10.29452 | best_loss=10.27482
Epoch 31/80: current_loss=10.26687 | best_loss=10.26687
Epoch 32/80: current_loss=10.29737 | best_loss=10.26687
Epoch 33/80: current_loss=10.28909 | best_loss=10.26687
Epoch 34/80: current_loss=10.26885 | best_loss=10.26687
Epoch 35/80: current_loss=10.33411 | best_loss=10.26687
Epoch 36/80: current_loss=10.27970 | best_loss=10.26687
Epoch 37/80: current_loss=10.31308 | best_loss=10.26687
Epoch 38/80: current_loss=10.31807 | best_loss=10.26687
Epoch 39/80: current_loss=10.29892 | best_loss=10.26687
Epoch 40/80: current_loss=10.28485 | best_loss=10.26687
Epoch 41/80: current_loss=10.31909 | best_loss=10.26687
Epoch 42/80: current_loss=10.30204 | best_loss=10.26687
Epoch 43/80: current_loss=10.27525 | best_loss=10.26687
Epoch 44/80: current_loss=10.27514 | best_loss=10.26687
Epoch 45/80: current_loss=10.27422 | best_loss=10.26687
Epoch 46/80: current_loss=10.31598 | best_loss=10.26687
Epoch 47/80: current_loss=10.30822 | best_loss=10.26687
Epoch 48/80: current_loss=10.28246 | best_loss=10.26687
Epoch 49/80: current_loss=10.27432 | best_loss=10.26687
Epoch 50/80: current_loss=10.29294 | best_loss=10.26687
Epoch 51/80: current_loss=10.28814 | best_loss=10.26687
Early Stopping at epoch 51
      explained_var=-0.00240 | mse_loss=10.48476
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.40800 | best_loss=10.40800
Epoch 1/80: current_loss=10.29264 | best_loss=10.29264
Epoch 2/80: current_loss=10.36800 | best_loss=10.29264
Epoch 3/80: current_loss=10.40065 | best_loss=10.29264
Epoch 4/80: current_loss=10.36108 | best_loss=10.29264
Epoch 5/80: current_loss=10.38655 | best_loss=10.29264
Epoch 6/80: current_loss=10.53374 | best_loss=10.29264
Epoch 7/80: current_loss=10.39584 | best_loss=10.29264
Epoch 8/80: current_loss=10.48974 | best_loss=10.29264
Epoch 9/80: current_loss=10.39722 | best_loss=10.29264
Epoch 10/80: current_loss=10.41555 | best_loss=10.29264
Epoch 11/80: current_loss=10.45726 | best_loss=10.29264
Epoch 12/80: current_loss=10.39529 | best_loss=10.29264
Epoch 13/80: current_loss=10.39586 | best_loss=10.29264
Epoch 14/80: current_loss=10.39246 | best_loss=10.29264
Epoch 15/80: current_loss=10.38958 | best_loss=10.29264
Epoch 16/80: current_loss=10.43369 | best_loss=10.29264
Epoch 17/80: current_loss=10.32797 | best_loss=10.29264
Epoch 18/80: current_loss=10.29457 | best_loss=10.29264
Epoch 19/80: current_loss=10.39215 | best_loss=10.29264
Epoch 20/80: current_loss=10.40587 | best_loss=10.29264
Epoch 21/80: current_loss=10.37693 | best_loss=10.29264
Early Stopping at epoch 21
      explained_var=0.01213 | mse_loss=9.94814
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.85939 | best_loss=7.85939
Epoch 1/80: current_loss=7.50877 | best_loss=7.50877
Epoch 2/80: current_loss=7.61856 | best_loss=7.50877
Epoch 3/80: current_loss=7.83480 | best_loss=7.50877
Epoch 4/80: current_loss=7.76055 | best_loss=7.50877
Epoch 5/80: current_loss=7.71920 | best_loss=7.50877
Epoch 6/80: current_loss=7.49101 | best_loss=7.49101
Epoch 7/80: current_loss=7.50135 | best_loss=7.49101
Epoch 8/80: current_loss=7.61945 | best_loss=7.49101
Epoch 9/80: current_loss=7.56927 | best_loss=7.49101
Epoch 10/80: current_loss=7.63815 | best_loss=7.49101
Epoch 11/80: current_loss=7.63066 | best_loss=7.49101
Epoch 12/80: current_loss=7.60888 | best_loss=7.49101
Epoch 13/80: current_loss=7.49115 | best_loss=7.49101
Epoch 14/80: current_loss=7.54612 | best_loss=7.49101
Epoch 15/80: current_loss=7.76635 | best_loss=7.49101
Epoch 16/80: current_loss=7.74259 | best_loss=7.49101
Epoch 17/80: current_loss=7.61546 | best_loss=7.49101
Epoch 18/80: current_loss=7.69079 | best_loss=7.49101
Epoch 19/80: current_loss=7.51583 | best_loss=7.49101
Epoch 20/80: current_loss=7.81290 | best_loss=7.49101
Epoch 21/80: current_loss=7.57989 | best_loss=7.49101
Epoch 22/80: current_loss=7.48758 | best_loss=7.48758
Epoch 23/80: current_loss=7.61926 | best_loss=7.48758
Epoch 24/80: current_loss=7.59977 | best_loss=7.48758
Epoch 25/80: current_loss=7.66346 | best_loss=7.48758
Epoch 26/80: current_loss=7.52043 | best_loss=7.48758
Epoch 27/80: current_loss=7.58755 | best_loss=7.48758
Epoch 28/80: current_loss=7.98641 | best_loss=7.48758
Epoch 29/80: current_loss=7.48727 | best_loss=7.48727
Epoch 30/80: current_loss=7.64604 | best_loss=7.48727
Epoch 31/80: current_loss=7.55146 | best_loss=7.48727
Epoch 32/80: current_loss=7.58902 | best_loss=7.48727
Epoch 33/80: current_loss=7.64484 | best_loss=7.48727
Epoch 34/80: current_loss=7.57722 | best_loss=7.48727
Epoch 35/80: current_loss=7.57143 | best_loss=7.48727
Epoch 36/80: current_loss=7.58252 | best_loss=7.48727
Epoch 37/80: current_loss=7.55938 | best_loss=7.48727
Epoch 38/80: current_loss=7.65874 | best_loss=7.48727
Epoch 39/80: current_loss=7.54834 | best_loss=7.48727
Epoch 40/80: current_loss=7.64476 | best_loss=7.48727
Epoch 41/80: current_loss=7.58028 | best_loss=7.48727
Epoch 42/80: current_loss=7.56673 | best_loss=7.48727
Epoch 43/80: current_loss=7.64922 | best_loss=7.48727
Epoch 44/80: current_loss=7.52844 | best_loss=7.48727
Epoch 45/80: current_loss=7.50975 | best_loss=7.48727
Epoch 46/80: current_loss=7.64189 | best_loss=7.48727
Epoch 47/80: current_loss=7.57539 | best_loss=7.48727
Epoch 48/80: current_loss=7.62686 | best_loss=7.48727
Epoch 49/80: current_loss=7.68333 | best_loss=7.48727
Early Stopping at epoch 49
      explained_var=0.00090 | mse_loss=7.59218
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.00426 | best_loss=9.00426
Epoch 1/80: current_loss=8.78784 | best_loss=8.78784
Epoch 2/80: current_loss=8.97052 | best_loss=8.78784
Epoch 3/80: current_loss=8.79518 | best_loss=8.78784
Epoch 4/80: current_loss=8.81950 | best_loss=8.78784
Epoch 5/80: current_loss=9.01449 | best_loss=8.78784
Epoch 6/80: current_loss=8.78447 | best_loss=8.78447
Epoch 7/80: current_loss=8.76929 | best_loss=8.76929
Epoch 8/80: current_loss=8.77604 | best_loss=8.76929
Epoch 9/80: current_loss=8.83915 | best_loss=8.76929
Epoch 10/80: current_loss=8.73952 | best_loss=8.73952
Epoch 11/80: current_loss=8.90130 | best_loss=8.73952
Epoch 12/80: current_loss=8.76407 | best_loss=8.73952
Epoch 13/80: current_loss=8.79624 | best_loss=8.73952
Epoch 14/80: current_loss=8.83104 | best_loss=8.73952
Epoch 15/80: current_loss=8.77762 | best_loss=8.73952
Epoch 16/80: current_loss=8.81977 | best_loss=8.73952
Epoch 17/80: current_loss=8.87217 | best_loss=8.73952
Epoch 18/80: current_loss=8.78503 | best_loss=8.73952
Epoch 19/80: current_loss=8.81059 | best_loss=8.73952
Epoch 20/80: current_loss=8.82359 | best_loss=8.73952
Epoch 21/80: current_loss=8.76810 | best_loss=8.73952
Epoch 22/80: current_loss=8.90330 | best_loss=8.73952
Epoch 23/80: current_loss=8.81441 | best_loss=8.73952
Epoch 24/80: current_loss=8.82998 | best_loss=8.73952
Epoch 25/80: current_loss=8.91569 | best_loss=8.73952
Epoch 26/80: current_loss=8.95785 | best_loss=8.73952
Epoch 27/80: current_loss=8.98864 | best_loss=8.73952
Epoch 28/80: current_loss=8.82097 | best_loss=8.73952
Epoch 29/80: current_loss=8.80694 | best_loss=8.73952
Epoch 30/80: current_loss=8.86814 | best_loss=8.73952
Early Stopping at epoch 30
      explained_var=0.00197 | mse_loss=8.69963
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.23217 | best_loss=9.23217
Epoch 1/80: current_loss=9.18617 | best_loss=9.18617
Epoch 2/80: current_loss=9.28212 | best_loss=9.18617
Epoch 3/80: current_loss=9.21148 | best_loss=9.18617
Epoch 4/80: current_loss=9.29180 | best_loss=9.18617
Epoch 5/80: current_loss=9.21329 | best_loss=9.18617
Epoch 6/80: current_loss=9.17240 | best_loss=9.17240
Epoch 7/80: current_loss=9.19287 | best_loss=9.17240
Epoch 8/80: current_loss=9.19755 | best_loss=9.17240
Epoch 9/80: current_loss=9.14815 | best_loss=9.14815
Epoch 10/80: current_loss=9.16009 | best_loss=9.14815
Epoch 11/80: current_loss=9.17169 | best_loss=9.14815
Epoch 12/80: current_loss=9.15942 | best_loss=9.14815
Epoch 13/80: current_loss=9.23173 | best_loss=9.14815
Epoch 14/80: current_loss=9.20808 | best_loss=9.14815
Epoch 15/80: current_loss=9.29261 | best_loss=9.14815
Epoch 16/80: current_loss=9.19908 | best_loss=9.14815
Epoch 17/80: current_loss=9.12035 | best_loss=9.12035
Epoch 18/80: current_loss=9.20324 | best_loss=9.12035
Epoch 19/80: current_loss=9.18593 | best_loss=9.12035
Epoch 20/80: current_loss=9.18745 | best_loss=9.12035
Epoch 21/80: current_loss=9.16103 | best_loss=9.12035
Epoch 22/80: current_loss=9.24340 | best_loss=9.12035
Epoch 23/80: current_loss=9.16059 | best_loss=9.12035
Epoch 24/80: current_loss=9.17977 | best_loss=9.12035
Epoch 25/80: current_loss=9.15180 | best_loss=9.12035
Epoch 26/80: current_loss=9.19750 | best_loss=9.12035
Epoch 27/80: current_loss=9.15661 | best_loss=9.12035
Epoch 28/80: current_loss=9.14970 | best_loss=9.12035
Epoch 29/80: current_loss=9.15608 | best_loss=9.12035
Epoch 30/80: current_loss=9.18173 | best_loss=9.12035
Epoch 31/80: current_loss=9.19475 | best_loss=9.12035
Epoch 32/80: current_loss=9.17187 | best_loss=9.12035
Epoch 33/80: current_loss=9.16167 | best_loss=9.12035
Epoch 34/80: current_loss=9.18770 | best_loss=9.12035
Epoch 35/80: current_loss=9.17421 | best_loss=9.12035
Epoch 36/80: current_loss=9.25300 | best_loss=9.12035
Epoch 37/80: current_loss=9.16414 | best_loss=9.12035
Early Stopping at epoch 37
      explained_var=0.02296 | mse_loss=9.08982
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00711| avg_loss=9.16291
----------------------------------------------


----------------------------------------------
Params for Trial 32
{'learning_rate': 0.001, 'weight_decay': 0.006890701370078642, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.38682 | best_loss=16.38682
Epoch 1/80: current_loss=10.69598 | best_loss=10.69598
Epoch 2/80: current_loss=10.46051 | best_loss=10.46051
Epoch 3/80: current_loss=10.39952 | best_loss=10.39952
Epoch 4/80: current_loss=10.36563 | best_loss=10.36563
Epoch 5/80: current_loss=10.35901 | best_loss=10.35901
Epoch 6/80: current_loss=10.31330 | best_loss=10.31330
Epoch 7/80: current_loss=10.32103 | best_loss=10.31330
Epoch 8/80: current_loss=10.32464 | best_loss=10.31330
Epoch 9/80: current_loss=10.28428 | best_loss=10.28428
Epoch 10/80: current_loss=10.31486 | best_loss=10.28428
Epoch 11/80: current_loss=10.29152 | best_loss=10.28428
Epoch 12/80: current_loss=10.29470 | best_loss=10.28428
Epoch 13/80: current_loss=10.32412 | best_loss=10.28428
Epoch 14/80: current_loss=10.28565 | best_loss=10.28428
Epoch 15/80: current_loss=10.31492 | best_loss=10.28428
Epoch 16/80: current_loss=10.33057 | best_loss=10.28428
Epoch 17/80: current_loss=10.30217 | best_loss=10.28428
Epoch 18/80: current_loss=10.34836 | best_loss=10.28428
Epoch 19/80: current_loss=10.30760 | best_loss=10.28428
Epoch 20/80: current_loss=10.31569 | best_loss=10.28428
Epoch 21/80: current_loss=10.33746 | best_loss=10.28428
Epoch 22/80: current_loss=10.28772 | best_loss=10.28428
Epoch 23/80: current_loss=10.32520 | best_loss=10.28428
Epoch 24/80: current_loss=10.28711 | best_loss=10.28428
Epoch 25/80: current_loss=10.29149 | best_loss=10.28428
Epoch 26/80: current_loss=10.28718 | best_loss=10.28428
Epoch 27/80: current_loss=10.32550 | best_loss=10.28428
Epoch 28/80: current_loss=10.29196 | best_loss=10.28428
Epoch 29/80: current_loss=10.29318 | best_loss=10.28428
Early Stopping at epoch 29
      explained_var=-0.00436 | mse_loss=10.51241
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.46844 | best_loss=10.46844
Epoch 1/80: current_loss=10.41893 | best_loss=10.41893
Epoch 2/80: current_loss=10.40705 | best_loss=10.40705
Epoch 3/80: current_loss=10.46048 | best_loss=10.40705
Epoch 4/80: current_loss=10.40584 | best_loss=10.40584
Epoch 5/80: current_loss=10.44732 | best_loss=10.40584
Epoch 6/80: current_loss=10.41185 | best_loss=10.40584
Epoch 7/80: current_loss=10.43269 | best_loss=10.40584
Epoch 8/80: current_loss=10.47024 | best_loss=10.40584
Epoch 9/80: current_loss=10.42028 | best_loss=10.40584
Epoch 10/80: current_loss=10.45099 | best_loss=10.40584
Epoch 11/80: current_loss=10.42342 | best_loss=10.40584
Epoch 12/80: current_loss=10.40748 | best_loss=10.40584
Epoch 13/80: current_loss=10.40469 | best_loss=10.40469
Epoch 14/80: current_loss=10.43820 | best_loss=10.40469
Epoch 15/80: current_loss=10.43514 | best_loss=10.40469
Epoch 16/80: current_loss=10.40486 | best_loss=10.40469
Epoch 17/80: current_loss=10.44752 | best_loss=10.40469
Epoch 18/80: current_loss=10.40399 | best_loss=10.40399
Epoch 19/80: current_loss=10.46244 | best_loss=10.40399
Epoch 20/80: current_loss=10.47870 | best_loss=10.40399
Epoch 21/80: current_loss=10.40463 | best_loss=10.40399
Epoch 22/80: current_loss=10.52772 | best_loss=10.40399
Epoch 23/80: current_loss=10.40494 | best_loss=10.40399
Epoch 24/80: current_loss=10.41946 | best_loss=10.40399
Epoch 25/80: current_loss=10.47349 | best_loss=10.40399
Epoch 26/80: current_loss=10.41049 | best_loss=10.40399
Epoch 27/80: current_loss=10.41573 | best_loss=10.40399
Epoch 28/80: current_loss=10.40286 | best_loss=10.40286
Epoch 29/80: current_loss=10.47604 | best_loss=10.40286
Epoch 30/80: current_loss=10.46178 | best_loss=10.40286
Epoch 31/80: current_loss=10.45534 | best_loss=10.40286
Epoch 32/80: current_loss=10.37716 | best_loss=10.37716
Epoch 33/80: current_loss=10.41055 | best_loss=10.37716
Epoch 34/80: current_loss=10.49319 | best_loss=10.37716
Epoch 35/80: current_loss=10.41609 | best_loss=10.37716
Epoch 36/80: current_loss=10.43106 | best_loss=10.37716
Epoch 37/80: current_loss=10.42747 | best_loss=10.37716
Epoch 38/80: current_loss=10.42372 | best_loss=10.37716
Epoch 39/80: current_loss=10.48996 | best_loss=10.37716
Epoch 40/80: current_loss=10.43361 | best_loss=10.37716
Epoch 41/80: current_loss=10.43532 | best_loss=10.37716
Epoch 42/80: current_loss=10.43034 | best_loss=10.37716
Epoch 43/80: current_loss=10.44253 | best_loss=10.37716
Epoch 44/80: current_loss=10.40433 | best_loss=10.37716
Epoch 45/80: current_loss=10.48093 | best_loss=10.37716
Epoch 46/80: current_loss=10.41456 | best_loss=10.37716
Epoch 47/80: current_loss=10.40549 | best_loss=10.37716
Epoch 48/80: current_loss=10.43106 | best_loss=10.37716
Epoch 49/80: current_loss=10.41558 | best_loss=10.37716
Epoch 50/80: current_loss=10.40763 | best_loss=10.37716
Epoch 51/80: current_loss=10.45165 | best_loss=10.37716
Epoch 52/80: current_loss=10.41923 | best_loss=10.37716
Early Stopping at epoch 52
      explained_var=0.00386 | mse_loss=10.02940
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.72274 | best_loss=7.72274
Epoch 1/80: current_loss=7.49380 | best_loss=7.49380
Epoch 2/80: current_loss=7.60659 | best_loss=7.49380
Epoch 3/80: current_loss=7.55560 | best_loss=7.49380
Epoch 4/80: current_loss=7.71980 | best_loss=7.49380
Epoch 5/80: current_loss=7.57756 | best_loss=7.49380
Epoch 6/80: current_loss=7.60984 | best_loss=7.49380
Epoch 7/80: current_loss=7.56656 | best_loss=7.49380
Epoch 8/80: current_loss=7.51514 | best_loss=7.49380
Epoch 9/80: current_loss=7.58470 | best_loss=7.49380
Epoch 10/80: current_loss=7.53313 | best_loss=7.49380
Epoch 11/80: current_loss=7.57042 | best_loss=7.49380
Epoch 12/80: current_loss=7.59197 | best_loss=7.49380
Epoch 13/80: current_loss=7.61141 | best_loss=7.49380
Epoch 14/80: current_loss=7.58028 | best_loss=7.49380
Epoch 15/80: current_loss=7.46639 | best_loss=7.46639
Epoch 16/80: current_loss=7.61365 | best_loss=7.46639
Epoch 17/80: current_loss=7.83529 | best_loss=7.46639
Epoch 18/80: current_loss=7.50860 | best_loss=7.46639
Epoch 19/80: current_loss=7.54978 | best_loss=7.46639
Epoch 20/80: current_loss=7.56081 | best_loss=7.46639
Epoch 21/80: current_loss=7.62169 | best_loss=7.46639
Epoch 22/80: current_loss=7.51466 | best_loss=7.46639
Epoch 23/80: current_loss=7.61303 | best_loss=7.46639
Epoch 24/80: current_loss=7.57684 | best_loss=7.46639
Epoch 25/80: current_loss=7.58533 | best_loss=7.46639
Epoch 26/80: current_loss=7.64040 | best_loss=7.46639
Epoch 27/80: current_loss=7.55080 | best_loss=7.46639
Epoch 28/80: current_loss=7.57701 | best_loss=7.46639
Epoch 29/80: current_loss=7.53321 | best_loss=7.46639
Epoch 30/80: current_loss=7.52801 | best_loss=7.46639
Epoch 31/80: current_loss=7.67285 | best_loss=7.46639
Epoch 32/80: current_loss=7.45812 | best_loss=7.45812
Epoch 33/80: current_loss=7.46124 | best_loss=7.45812
Epoch 34/80: current_loss=7.53474 | best_loss=7.45812
Epoch 35/80: current_loss=7.49923 | best_loss=7.45812
Epoch 36/80: current_loss=7.53081 | best_loss=7.45812
Epoch 37/80: current_loss=7.47777 | best_loss=7.45812
Epoch 38/80: current_loss=7.64703 | best_loss=7.45812
Epoch 39/80: current_loss=7.49710 | best_loss=7.45812
Epoch 40/80: current_loss=7.63744 | best_loss=7.45812
Epoch 41/80: current_loss=7.54381 | best_loss=7.45812
Epoch 42/80: current_loss=7.52961 | best_loss=7.45812
Epoch 43/80: current_loss=7.67979 | best_loss=7.45812
Epoch 44/80: current_loss=7.49741 | best_loss=7.45812
Epoch 45/80: current_loss=7.63359 | best_loss=7.45812
Epoch 46/80: current_loss=7.52298 | best_loss=7.45812
Epoch 47/80: current_loss=7.58459 | best_loss=7.45812
Epoch 48/80: current_loss=7.48029 | best_loss=7.45812
Epoch 49/80: current_loss=7.48239 | best_loss=7.45812
Epoch 50/80: current_loss=7.79481 | best_loss=7.45812
Epoch 51/80: current_loss=7.57632 | best_loss=7.45812
Epoch 52/80: current_loss=7.58075 | best_loss=7.45812
Early Stopping at epoch 52
      explained_var=0.00114 | mse_loss=7.57662
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.76312 | best_loss=8.76312
Epoch 1/80: current_loss=9.07922 | best_loss=8.76312
Epoch 2/80: current_loss=8.89039 | best_loss=8.76312
Epoch 3/80: current_loss=8.84458 | best_loss=8.76312
Epoch 4/80: current_loss=8.80772 | best_loss=8.76312
Epoch 5/80: current_loss=8.82123 | best_loss=8.76312
Epoch 6/80: current_loss=9.03199 | best_loss=8.76312
Epoch 7/80: current_loss=8.81281 | best_loss=8.76312
Epoch 8/80: current_loss=9.03571 | best_loss=8.76312
Epoch 9/80: current_loss=8.74118 | best_loss=8.74118
Epoch 10/80: current_loss=8.75518 | best_loss=8.74118
Epoch 11/80: current_loss=9.17124 | best_loss=8.74118
Epoch 12/80: current_loss=8.83952 | best_loss=8.74118
Epoch 13/80: current_loss=8.75282 | best_loss=8.74118
Epoch 14/80: current_loss=8.92858 | best_loss=8.74118
Epoch 15/80: current_loss=8.77865 | best_loss=8.74118
Epoch 16/80: current_loss=8.83342 | best_loss=8.74118
Epoch 17/80: current_loss=8.93691 | best_loss=8.74118
Epoch 18/80: current_loss=9.03050 | best_loss=8.74118
Epoch 19/80: current_loss=8.83928 | best_loss=8.74118
Epoch 20/80: current_loss=8.98529 | best_loss=8.74118
Epoch 21/80: current_loss=8.75984 | best_loss=8.74118
Epoch 22/80: current_loss=8.95209 | best_loss=8.74118
Epoch 23/80: current_loss=8.86666 | best_loss=8.74118
Epoch 24/80: current_loss=8.71005 | best_loss=8.71005
Epoch 25/80: current_loss=8.75898 | best_loss=8.71005
Epoch 26/80: current_loss=8.78253 | best_loss=8.71005
Epoch 27/80: current_loss=8.77156 | best_loss=8.71005
Epoch 28/80: current_loss=8.77682 | best_loss=8.71005
Epoch 29/80: current_loss=8.71341 | best_loss=8.71005
Epoch 30/80: current_loss=8.87754 | best_loss=8.71005
Epoch 31/80: current_loss=8.76890 | best_loss=8.71005
Epoch 32/80: current_loss=8.79155 | best_loss=8.71005
Epoch 33/80: current_loss=8.83301 | best_loss=8.71005
Epoch 34/80: current_loss=8.74806 | best_loss=8.71005
Epoch 35/80: current_loss=8.74195 | best_loss=8.71005
Epoch 36/80: current_loss=8.75836 | best_loss=8.71005
Epoch 37/80: current_loss=8.89061 | best_loss=8.71005
Epoch 38/80: current_loss=8.78772 | best_loss=8.71005
Epoch 39/80: current_loss=8.81461 | best_loss=8.71005
Epoch 40/80: current_loss=9.00922 | best_loss=8.71005
Epoch 41/80: current_loss=8.90666 | best_loss=8.71005
Epoch 42/80: current_loss=8.78195 | best_loss=8.71005
Epoch 43/80: current_loss=8.87954 | best_loss=8.71005
Epoch 44/80: current_loss=8.81996 | best_loss=8.71005
Early Stopping at epoch 44
      explained_var=0.00440 | mse_loss=8.68336
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.28471 | best_loss=9.28471
Epoch 1/80: current_loss=9.14360 | best_loss=9.14360
Epoch 2/80: current_loss=9.17398 | best_loss=9.14360
Epoch 3/80: current_loss=9.16265 | best_loss=9.14360
Epoch 4/80: current_loss=9.36798 | best_loss=9.14360
Epoch 5/80: current_loss=9.20407 | best_loss=9.14360
Epoch 6/80: current_loss=9.18398 | best_loss=9.14360
Epoch 7/80: current_loss=9.15587 | best_loss=9.14360
Epoch 8/80: current_loss=9.38762 | best_loss=9.14360
Epoch 9/80: current_loss=9.27172 | best_loss=9.14360
Epoch 10/80: current_loss=9.19398 | best_loss=9.14360
Epoch 11/80: current_loss=9.16891 | best_loss=9.14360
Epoch 12/80: current_loss=9.22037 | best_loss=9.14360
Epoch 13/80: current_loss=9.18059 | best_loss=9.14360
Epoch 14/80: current_loss=9.16385 | best_loss=9.14360
Epoch 15/80: current_loss=9.19341 | best_loss=9.14360
Epoch 16/80: current_loss=9.15116 | best_loss=9.14360
Epoch 17/80: current_loss=9.16278 | best_loss=9.14360
Epoch 18/80: current_loss=9.24785 | best_loss=9.14360
Epoch 19/80: current_loss=9.17324 | best_loss=9.14360
Epoch 20/80: current_loss=9.19347 | best_loss=9.14360
Epoch 21/80: current_loss=9.25954 | best_loss=9.14360
Early Stopping at epoch 21
      explained_var=0.02173 | mse_loss=9.09870
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00535| avg_loss=9.18010
----------------------------------------------


----------------------------------------------
Params for Trial 33
{'learning_rate': 1e-05, 'weight_decay': 0.008134174594248155, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=106.41346 | best_loss=106.41346
Epoch 1/80: current_loss=104.85746 | best_loss=104.85746
Epoch 2/80: current_loss=103.15032 | best_loss=103.15032
Epoch 3/80: current_loss=101.12303 | best_loss=101.12303
Epoch 4/80: current_loss=98.62125 | best_loss=98.62125
Epoch 5/80: current_loss=95.38397 | best_loss=95.38397
Epoch 6/80: current_loss=91.13623 | best_loss=91.13623
Epoch 7/80: current_loss=85.70810 | best_loss=85.70810
Epoch 8/80: current_loss=79.17794 | best_loss=79.17794
Epoch 9/80: current_loss=72.19976 | best_loss=72.19976
Epoch 10/80: current_loss=65.53666 | best_loss=65.53666
Epoch 11/80: current_loss=59.65424 | best_loss=59.65424
Epoch 12/80: current_loss=54.76938 | best_loss=54.76938
Epoch 13/80: current_loss=50.73507 | best_loss=50.73507
Epoch 14/80: current_loss=47.33079 | best_loss=47.33079
Epoch 15/80: current_loss=44.56954 | best_loss=44.56954
Epoch 16/80: current_loss=42.22198 | best_loss=42.22198
Epoch 17/80: current_loss=40.20559 | best_loss=40.20559
Epoch 18/80: current_loss=38.43442 | best_loss=38.43442
Epoch 19/80: current_loss=36.89762 | best_loss=36.89762
Epoch 20/80: current_loss=35.55401 | best_loss=35.55401
Epoch 21/80: current_loss=34.34749 | best_loss=34.34749
Epoch 22/80: current_loss=33.24518 | best_loss=33.24518
Epoch 23/80: current_loss=32.24897 | best_loss=32.24897
Epoch 24/80: current_loss=31.33584 | best_loss=31.33584
Epoch 25/80: current_loss=30.50819 | best_loss=30.50819
Epoch 26/80: current_loss=29.73089 | best_loss=29.73089
Epoch 27/80: current_loss=29.00756 | best_loss=29.00756
Epoch 28/80: current_loss=28.34799 | best_loss=28.34799
Epoch 29/80: current_loss=27.72417 | best_loss=27.72417
Epoch 30/80: current_loss=27.13042 | best_loss=27.13042
Epoch 31/80: current_loss=26.58347 | best_loss=26.58347
Epoch 32/80: current_loss=26.06527 | best_loss=26.06527
Epoch 33/80: current_loss=25.55970 | best_loss=25.55970
Epoch 34/80: current_loss=25.07698 | best_loss=25.07698
Epoch 35/80: current_loss=24.61811 | best_loss=24.61811
Epoch 36/80: current_loss=24.19098 | best_loss=24.19098
Epoch 37/80: current_loss=23.77816 | best_loss=23.77816
Epoch 38/80: current_loss=23.37705 | best_loss=23.37705
Epoch 39/80: current_loss=23.00668 | best_loss=23.00668
Epoch 40/80: current_loss=22.64059 | best_loss=22.64059
Epoch 41/80: current_loss=22.28732 | best_loss=22.28732
Epoch 42/80: current_loss=21.95936 | best_loss=21.95936
Epoch 43/80: current_loss=21.63600 | best_loss=21.63600
Epoch 44/80: current_loss=21.32754 | best_loss=21.32754
Epoch 45/80: current_loss=21.03548 | best_loss=21.03548
Epoch 46/80: current_loss=20.75382 | best_loss=20.75382
Epoch 47/80: current_loss=20.48902 | best_loss=20.48902
Epoch 48/80: current_loss=20.23055 | best_loss=20.23055
Epoch 49/80: current_loss=19.99141 | best_loss=19.99141
Epoch 50/80: current_loss=19.76032 | best_loss=19.76032
Epoch 51/80: current_loss=19.53532 | best_loss=19.53532
Epoch 52/80: current_loss=19.31393 | best_loss=19.31393
Epoch 53/80: current_loss=19.10358 | best_loss=19.10358
Epoch 54/80: current_loss=18.90269 | best_loss=18.90269
Epoch 55/80: current_loss=18.71204 | best_loss=18.71204
Epoch 56/80: current_loss=18.52531 | best_loss=18.52531
Epoch 57/80: current_loss=18.34024 | best_loss=18.34024
Epoch 58/80: current_loss=18.16678 | best_loss=18.16678
Epoch 59/80: current_loss=17.99211 | best_loss=17.99211
Epoch 60/80: current_loss=17.82746 | best_loss=17.82746
Epoch 61/80: current_loss=17.66434 | best_loss=17.66434
Epoch 62/80: current_loss=17.50793 | best_loss=17.50793
Epoch 63/80: current_loss=17.35944 | best_loss=17.35944
Epoch 64/80: current_loss=17.21165 | best_loss=17.21165
Epoch 65/80: current_loss=17.06201 | best_loss=17.06201
Epoch 66/80: current_loss=16.92740 | best_loss=16.92740
Epoch 67/80: current_loss=16.79000 | best_loss=16.79000
Epoch 68/80: current_loss=16.65692 | best_loss=16.65692
Epoch 69/80: current_loss=16.52773 | best_loss=16.52773
Epoch 70/80: current_loss=16.40003 | best_loss=16.40003
Epoch 71/80: current_loss=16.27633 | best_loss=16.27633
Epoch 72/80: current_loss=16.15859 | best_loss=16.15859
Epoch 73/80: current_loss=16.04173 | best_loss=16.04173
Epoch 74/80: current_loss=15.93041 | best_loss=15.93041
Epoch 75/80: current_loss=15.82078 | best_loss=15.82078
Epoch 76/80: current_loss=15.70823 | best_loss=15.70823
Epoch 77/80: current_loss=15.60086 | best_loss=15.60086
Epoch 78/80: current_loss=15.49664 | best_loss=15.49664
Epoch 79/80: current_loss=15.39002 | best_loss=15.39002
      explained_var=-0.16645 | mse_loss=15.82360

----------------------------------------------
Params for Trial 34
{'learning_rate': 0.001, 'weight_decay': 0.005398536634391274, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.19008 | best_loss=16.19008
Epoch 1/80: current_loss=10.68201 | best_loss=10.68201
Epoch 2/80: current_loss=10.40958 | best_loss=10.40958
Epoch 3/80: current_loss=10.36284 | best_loss=10.36284
Epoch 4/80: current_loss=10.35430 | best_loss=10.35430
Epoch 5/80: current_loss=10.31698 | best_loss=10.31698
Epoch 6/80: current_loss=10.32515 | best_loss=10.31698
Epoch 7/80: current_loss=10.29791 | best_loss=10.29791
Epoch 8/80: current_loss=10.31487 | best_loss=10.29791
Epoch 9/80: current_loss=10.30764 | best_loss=10.29791
Epoch 10/80: current_loss=10.28626 | best_loss=10.28626
Epoch 11/80: current_loss=10.29119 | best_loss=10.28626
Epoch 12/80: current_loss=10.27911 | best_loss=10.27911
Epoch 13/80: current_loss=10.28643 | best_loss=10.27911
Epoch 14/80: current_loss=10.28149 | best_loss=10.27911
Epoch 15/80: current_loss=10.27717 | best_loss=10.27717
Epoch 16/80: current_loss=10.28193 | best_loss=10.27717
Epoch 17/80: current_loss=10.26943 | best_loss=10.26943
Epoch 18/80: current_loss=10.28350 | best_loss=10.26943
Epoch 19/80: current_loss=10.28616 | best_loss=10.26943
Epoch 20/80: current_loss=10.28826 | best_loss=10.26943
Epoch 21/80: current_loss=10.29937 | best_loss=10.26943
Epoch 22/80: current_loss=10.27266 | best_loss=10.26943
Epoch 23/80: current_loss=10.26387 | best_loss=10.26387
Epoch 24/80: current_loss=10.29492 | best_loss=10.26387
Epoch 25/80: current_loss=10.30039 | best_loss=10.26387
Epoch 26/80: current_loss=10.33354 | best_loss=10.26387
Epoch 27/80: current_loss=10.36835 | best_loss=10.26387
Epoch 28/80: current_loss=10.27119 | best_loss=10.26387
Epoch 29/80: current_loss=10.28949 | best_loss=10.26387
Epoch 30/80: current_loss=10.26625 | best_loss=10.26387
Epoch 31/80: current_loss=10.29959 | best_loss=10.26387
Epoch 32/80: current_loss=10.26756 | best_loss=10.26387
Epoch 33/80: current_loss=10.31466 | best_loss=10.26387
Epoch 34/80: current_loss=10.29696 | best_loss=10.26387
Epoch 35/80: current_loss=10.28751 | best_loss=10.26387
Epoch 36/80: current_loss=10.29080 | best_loss=10.26387
Epoch 37/80: current_loss=10.28083 | best_loss=10.26387
Epoch 38/80: current_loss=10.27652 | best_loss=10.26387
Epoch 39/80: current_loss=10.27106 | best_loss=10.26387
Epoch 40/80: current_loss=10.34492 | best_loss=10.26387
Epoch 41/80: current_loss=10.26373 | best_loss=10.26373
Epoch 42/80: current_loss=10.28364 | best_loss=10.26373
Epoch 43/80: current_loss=10.27622 | best_loss=10.26373
Epoch 44/80: current_loss=10.28093 | best_loss=10.26373
Epoch 45/80: current_loss=10.29142 | best_loss=10.26373
Epoch 46/80: current_loss=10.27450 | best_loss=10.26373
Epoch 47/80: current_loss=10.30464 | best_loss=10.26373
Epoch 48/80: current_loss=10.27499 | best_loss=10.26373
Epoch 49/80: current_loss=10.30598 | best_loss=10.26373
Epoch 50/80: current_loss=10.27901 | best_loss=10.26373
Epoch 51/80: current_loss=10.30619 | best_loss=10.26373
Epoch 52/80: current_loss=10.30395 | best_loss=10.26373
Epoch 53/80: current_loss=10.26865 | best_loss=10.26373
Epoch 54/80: current_loss=10.29451 | best_loss=10.26373
Epoch 55/80: current_loss=10.29577 | best_loss=10.26373
Epoch 56/80: current_loss=10.28497 | best_loss=10.26373
Epoch 57/80: current_loss=10.27700 | best_loss=10.26373
Epoch 58/80: current_loss=10.26779 | best_loss=10.26373
Epoch 59/80: current_loss=10.29750 | best_loss=10.26373
Epoch 60/80: current_loss=10.29106 | best_loss=10.26373
Epoch 61/80: current_loss=10.27686 | best_loss=10.26373
Early Stopping at epoch 61
      explained_var=-0.00193 | mse_loss=10.48383
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.47557 | best_loss=10.47557
Epoch 1/80: current_loss=10.46583 | best_loss=10.46583
Epoch 2/80: current_loss=10.47016 | best_loss=10.46583
Epoch 3/80: current_loss=10.42107 | best_loss=10.42107
Epoch 4/80: current_loss=10.40383 | best_loss=10.40383
Epoch 5/80: current_loss=10.42689 | best_loss=10.40383
Epoch 6/80: current_loss=10.45619 | best_loss=10.40383
Epoch 7/80: current_loss=10.41481 | best_loss=10.40383
Epoch 8/80: current_loss=10.48385 | best_loss=10.40383
Epoch 9/80: current_loss=10.44350 | best_loss=10.40383
Epoch 10/80: current_loss=10.41278 | best_loss=10.40383
Epoch 11/80: current_loss=10.45766 | best_loss=10.40383
Epoch 12/80: current_loss=10.40937 | best_loss=10.40383
Epoch 13/80: current_loss=10.40666 | best_loss=10.40383
Epoch 14/80: current_loss=10.49923 | best_loss=10.40383
Epoch 15/80: current_loss=10.41175 | best_loss=10.40383
Epoch 16/80: current_loss=10.45546 | best_loss=10.40383
Epoch 17/80: current_loss=10.40652 | best_loss=10.40383
Epoch 18/80: current_loss=10.52892 | best_loss=10.40383
Epoch 19/80: current_loss=10.41070 | best_loss=10.40383
Epoch 20/80: current_loss=10.43183 | best_loss=10.40383
Epoch 21/80: current_loss=10.42328 | best_loss=10.40383
Epoch 22/80: current_loss=10.62216 | best_loss=10.40383
Epoch 23/80: current_loss=10.42236 | best_loss=10.40383
Epoch 24/80: current_loss=10.44371 | best_loss=10.40383
Early Stopping at epoch 24
      explained_var=0.00025 | mse_loss=10.06109

----------------------------------------------
Params for Trial 35
{'learning_rate': 0.0001, 'weight_decay': 0.00931386350850102, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=99.26070 | best_loss=99.26070
Epoch 1/80: current_loss=91.50688 | best_loss=91.50688
Epoch 2/80: current_loss=78.14319 | best_loss=78.14319
Epoch 3/80: current_loss=62.67044 | best_loss=62.67044
Epoch 4/80: current_loss=51.73348 | best_loss=51.73348
Epoch 5/80: current_loss=44.79318 | best_loss=44.79318
Epoch 6/80: current_loss=40.21117 | best_loss=40.21117
Epoch 7/80: current_loss=36.96244 | best_loss=36.96244
Epoch 8/80: current_loss=34.45147 | best_loss=34.45147
Epoch 9/80: current_loss=32.33901 | best_loss=32.33901
Epoch 10/80: current_loss=30.51295 | best_loss=30.51295
Epoch 11/80: current_loss=28.91277 | best_loss=28.91277
Epoch 12/80: current_loss=27.49335 | best_loss=27.49335
Epoch 13/80: current_loss=26.27239 | best_loss=26.27239
Epoch 14/80: current_loss=25.11843 | best_loss=25.11843
Epoch 15/80: current_loss=24.07659 | best_loss=24.07659
Epoch 16/80: current_loss=23.11429 | best_loss=23.11429
Epoch 17/80: current_loss=22.21655 | best_loss=22.21655
Epoch 18/80: current_loss=21.38649 | best_loss=21.38649
Epoch 19/80: current_loss=20.62171 | best_loss=20.62171
Epoch 20/80: current_loss=19.89671 | best_loss=19.89671
Epoch 21/80: current_loss=19.24295 | best_loss=19.24295
Epoch 22/80: current_loss=18.61135 | best_loss=18.61135
Epoch 23/80: current_loss=18.03407 | best_loss=18.03407
Epoch 24/80: current_loss=17.49410 | best_loss=17.49410
Epoch 25/80: current_loss=16.98168 | best_loss=16.98168
Epoch 26/80: current_loss=16.53139 | best_loss=16.53139
Epoch 27/80: current_loss=16.09697 | best_loss=16.09697
Epoch 28/80: current_loss=15.67446 | best_loss=15.67446
Epoch 29/80: current_loss=15.30984 | best_loss=15.30984
Epoch 30/80: current_loss=14.96190 | best_loss=14.96190
Epoch 31/80: current_loss=14.64400 | best_loss=14.64400
Epoch 32/80: current_loss=14.34939 | best_loss=14.34939
Epoch 33/80: current_loss=14.06635 | best_loss=14.06635
Epoch 34/80: current_loss=13.81070 | best_loss=13.81070
Epoch 35/80: current_loss=13.57678 | best_loss=13.57678
Epoch 36/80: current_loss=13.35641 | best_loss=13.35641
Epoch 37/80: current_loss=13.15216 | best_loss=13.15216
Epoch 38/80: current_loss=12.96352 | best_loss=12.96352
Epoch 39/80: current_loss=12.78308 | best_loss=12.78308
Epoch 40/80: current_loss=12.63125 | best_loss=12.63125
Epoch 41/80: current_loss=12.47463 | best_loss=12.47463
Epoch 42/80: current_loss=12.35553 | best_loss=12.35553
Epoch 43/80: current_loss=12.21946 | best_loss=12.21946
Epoch 44/80: current_loss=12.10131 | best_loss=12.10131
Epoch 45/80: current_loss=11.99276 | best_loss=11.99276
Epoch 46/80: current_loss=11.88682 | best_loss=11.88682
Epoch 47/80: current_loss=11.79916 | best_loss=11.79916
Epoch 48/80: current_loss=11.71713 | best_loss=11.71713
Epoch 49/80: current_loss=11.63779 | best_loss=11.63779
Epoch 50/80: current_loss=11.56432 | best_loss=11.56432
Epoch 51/80: current_loss=11.49660 | best_loss=11.49660
Epoch 52/80: current_loss=11.44020 | best_loss=11.44020
Epoch 53/80: current_loss=11.38466 | best_loss=11.38466
Epoch 54/80: current_loss=11.33510 | best_loss=11.33510
Epoch 55/80: current_loss=11.28641 | best_loss=11.28641
Epoch 56/80: current_loss=11.24028 | best_loss=11.24028
Epoch 57/80: current_loss=11.19836 | best_loss=11.19836
Epoch 58/80: current_loss=11.15748 | best_loss=11.15748
Epoch 59/80: current_loss=11.12434 | best_loss=11.12434
Epoch 60/80: current_loss=11.08813 | best_loss=11.08813
Epoch 61/80: current_loss=11.05287 | best_loss=11.05287
Epoch 62/80: current_loss=11.02657 | best_loss=11.02657
Epoch 63/80: current_loss=11.00010 | best_loss=11.00010
Epoch 64/80: current_loss=10.97942 | best_loss=10.97942
Epoch 65/80: current_loss=10.95900 | best_loss=10.95900
Epoch 66/80: current_loss=10.93170 | best_loss=10.93170
Epoch 67/80: current_loss=10.91259 | best_loss=10.91259
Epoch 68/80: current_loss=10.88987 | best_loss=10.88987
Epoch 69/80: current_loss=10.86636 | best_loss=10.86636
Epoch 70/80: current_loss=10.85080 | best_loss=10.85080
Epoch 71/80: current_loss=10.83300 | best_loss=10.83300
Epoch 72/80: current_loss=10.81602 | best_loss=10.81602
Epoch 73/80: current_loss=10.79879 | best_loss=10.79879
Epoch 74/80: current_loss=10.78356 | best_loss=10.78356
Epoch 75/80: current_loss=10.77198 | best_loss=10.77198
Epoch 76/80: current_loss=10.75777 | best_loss=10.75777
Epoch 77/80: current_loss=10.74319 | best_loss=10.74319
Epoch 78/80: current_loss=10.72368 | best_loss=10.72368
Epoch 79/80: current_loss=10.71301 | best_loss=10.71301
      explained_var=-0.04251 | mse_loss=10.99687

----------------------------------------------
Params for Trial 36
{'learning_rate': 0.001, 'weight_decay': 0.008682698228432387, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=68.67875 | best_loss=68.67875
Epoch 1/80: current_loss=45.86194 | best_loss=45.86194
Epoch 2/80: current_loss=35.61356 | best_loss=35.61356
Epoch 3/80: current_loss=28.50130 | best_loss=28.50130
Epoch 4/80: current_loss=23.19719 | best_loss=23.19719
Epoch 5/80: current_loss=19.28356 | best_loss=19.28356
Epoch 6/80: current_loss=16.43846 | best_loss=16.43846
Epoch 7/80: current_loss=14.49560 | best_loss=14.49560
Epoch 8/80: current_loss=13.11318 | best_loss=13.11318
Epoch 9/80: current_loss=12.15426 | best_loss=12.15426
Epoch 10/80: current_loss=11.50189 | best_loss=11.50189
Epoch 11/80: current_loss=11.07334 | best_loss=11.07334
Epoch 12/80: current_loss=10.80011 | best_loss=10.80011
Epoch 13/80: current_loss=10.60738 | best_loss=10.60738
Epoch 14/80: current_loss=10.48016 | best_loss=10.48016
Epoch 15/80: current_loss=10.42526 | best_loss=10.42526
Epoch 16/80: current_loss=10.38575 | best_loss=10.38575
Epoch 17/80: current_loss=10.34855 | best_loss=10.34855
Epoch 18/80: current_loss=10.33333 | best_loss=10.33333
Epoch 19/80: current_loss=10.33315 | best_loss=10.33315
Epoch 20/80: current_loss=10.32535 | best_loss=10.32535
Epoch 21/80: current_loss=10.32143 | best_loss=10.32143
Epoch 22/80: current_loss=10.32049 | best_loss=10.32049
Epoch 23/80: current_loss=10.32063 | best_loss=10.32049
Epoch 24/80: current_loss=10.31925 | best_loss=10.31925
Epoch 25/80: current_loss=10.31743 | best_loss=10.31743
Epoch 26/80: current_loss=10.30198 | best_loss=10.30198
Epoch 27/80: current_loss=10.30218 | best_loss=10.30198
Epoch 28/80: current_loss=10.30538 | best_loss=10.30198
Epoch 29/80: current_loss=10.30479 | best_loss=10.30198
Epoch 30/80: current_loss=10.30968 | best_loss=10.30198
Epoch 31/80: current_loss=10.30738 | best_loss=10.30198
Epoch 32/80: current_loss=10.30155 | best_loss=10.30155
Epoch 33/80: current_loss=10.30170 | best_loss=10.30155
Epoch 34/80: current_loss=10.30109 | best_loss=10.30109
Epoch 35/80: current_loss=10.29779 | best_loss=10.29779
Epoch 36/80: current_loss=10.31428 | best_loss=10.29779
Epoch 37/80: current_loss=10.31645 | best_loss=10.29779
Epoch 38/80: current_loss=10.31547 | best_loss=10.29779
Epoch 39/80: current_loss=10.30579 | best_loss=10.29779
Epoch 40/80: current_loss=10.29936 | best_loss=10.29779
Epoch 41/80: current_loss=10.29687 | best_loss=10.29687
Epoch 42/80: current_loss=10.29554 | best_loss=10.29554
Epoch 43/80: current_loss=10.29440 | best_loss=10.29440
Epoch 44/80: current_loss=10.30228 | best_loss=10.29440
Epoch 45/80: current_loss=10.29528 | best_loss=10.29440
Epoch 46/80: current_loss=10.29327 | best_loss=10.29327
Epoch 47/80: current_loss=10.30211 | best_loss=10.29327
Epoch 48/80: current_loss=10.28512 | best_loss=10.28512
Epoch 49/80: current_loss=10.28512 | best_loss=10.28512
Epoch 50/80: current_loss=10.28737 | best_loss=10.28512
Epoch 51/80: current_loss=10.29391 | best_loss=10.28512
Epoch 52/80: current_loss=10.31407 | best_loss=10.28512
Epoch 53/80: current_loss=10.31983 | best_loss=10.28512
Epoch 54/80: current_loss=10.29564 | best_loss=10.28512
Epoch 55/80: current_loss=10.30772 | best_loss=10.28512
Epoch 56/80: current_loss=10.30337 | best_loss=10.28512
Epoch 57/80: current_loss=10.30032 | best_loss=10.28512
Epoch 58/80: current_loss=10.28848 | best_loss=10.28512
Epoch 59/80: current_loss=10.29694 | best_loss=10.28512
Epoch 60/80: current_loss=10.31486 | best_loss=10.28512
Epoch 61/80: current_loss=10.31386 | best_loss=10.28512
Epoch 62/80: current_loss=10.31352 | best_loss=10.28512
Epoch 63/80: current_loss=10.29193 | best_loss=10.28512
Epoch 64/80: current_loss=10.29649 | best_loss=10.28512
Epoch 65/80: current_loss=10.29989 | best_loss=10.28512
Epoch 66/80: current_loss=10.29760 | best_loss=10.28512
Epoch 67/80: current_loss=10.31460 | best_loss=10.28512
Epoch 68/80: current_loss=10.31775 | best_loss=10.28512
Epoch 69/80: current_loss=10.29972 | best_loss=10.28512
Early Stopping at epoch 69
      explained_var=-0.00264 | mse_loss=10.52190

----------------------------------------------
Params for Trial 37
{'learning_rate': 0.1, 'weight_decay': 0.004405260619117996, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=150.63972 | best_loss=150.63972
Epoch 1/80: current_loss=32.14687 | best_loss=32.14687
Epoch 2/80: current_loss=13.50404 | best_loss=13.50404
Epoch 3/80: current_loss=11.34592 | best_loss=11.34592
Epoch 4/80: current_loss=10.38178 | best_loss=10.38178
Epoch 5/80: current_loss=13.08698 | best_loss=10.38178
Epoch 6/80: current_loss=10.67533 | best_loss=10.38178
Epoch 7/80: current_loss=10.32722 | best_loss=10.32722
Epoch 8/80: current_loss=10.47917 | best_loss=10.32722
Epoch 9/80: current_loss=10.34007 | best_loss=10.32722
Epoch 10/80: current_loss=13.34751 | best_loss=10.32722
Epoch 11/80: current_loss=11.17048 | best_loss=10.32722
Epoch 12/80: current_loss=20.94951 | best_loss=10.32722
Epoch 13/80: current_loss=14.85026 | best_loss=10.32722
Epoch 14/80: current_loss=10.60134 | best_loss=10.32722
Epoch 15/80: current_loss=11.93661 | best_loss=10.32722
Epoch 16/80: current_loss=10.15219 | best_loss=10.15219
Epoch 17/80: current_loss=127.76133 | best_loss=10.15219
Epoch 18/80: current_loss=17.69707 | best_loss=10.15219
Epoch 19/80: current_loss=12.64981 | best_loss=10.15219
Epoch 20/80: current_loss=12.25234 | best_loss=10.15219
Epoch 21/80: current_loss=30.01516 | best_loss=10.15219
Epoch 22/80: current_loss=12.91959 | best_loss=10.15219
Epoch 23/80: current_loss=21.59480 | best_loss=10.15219
Epoch 24/80: current_loss=14.49325 | best_loss=10.15219
Epoch 25/80: current_loss=13.26718 | best_loss=10.15219
Epoch 26/80: current_loss=11.59330 | best_loss=10.15219
Epoch 27/80: current_loss=12.04824 | best_loss=10.15219
Epoch 28/80: current_loss=11.90317 | best_loss=10.15219
Epoch 29/80: current_loss=11.66767 | best_loss=10.15219
Epoch 30/80: current_loss=20.07009 | best_loss=10.15219
Epoch 31/80: current_loss=11.82468 | best_loss=10.15219
Epoch 32/80: current_loss=10.26161 | best_loss=10.15219
Epoch 33/80: current_loss=10.47969 | best_loss=10.15219
Epoch 34/80: current_loss=23.07957 | best_loss=10.15219
Epoch 35/80: current_loss=12.40776 | best_loss=10.15219
Epoch 36/80: current_loss=10.46572 | best_loss=10.15219
Early Stopping at epoch 36
      explained_var=0.02015 | mse_loss=10.33557
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=49.81149 | best_loss=49.81149
Epoch 1/80: current_loss=38.69804 | best_loss=38.69804
Epoch 2/80: current_loss=17.53438 | best_loss=17.53438
Epoch 3/80: current_loss=14.21507 | best_loss=14.21507
Epoch 4/80: current_loss=14.29644 | best_loss=14.21507
Epoch 5/80: current_loss=14.04669 | best_loss=14.04669
Epoch 6/80: current_loss=15.39231 | best_loss=14.04669
Epoch 7/80: current_loss=12.15344 | best_loss=12.15344
Epoch 8/80: current_loss=17.62123 | best_loss=12.15344
Epoch 9/80: current_loss=13.92493 | best_loss=12.15344
Epoch 10/80: current_loss=12.01012 | best_loss=12.01012
Epoch 11/80: current_loss=11.49274 | best_loss=11.49274
Epoch 12/80: current_loss=14.67857 | best_loss=11.49274
Epoch 13/80: current_loss=18.73058 | best_loss=11.49274
Epoch 14/80: current_loss=21.36431 | best_loss=11.49274
Epoch 15/80: current_loss=13.75876 | best_loss=11.49274
Epoch 16/80: current_loss=20.01160 | best_loss=11.49274
Epoch 17/80: current_loss=24.06888 | best_loss=11.49274
Epoch 18/80: current_loss=18.86092 | best_loss=11.49274
Epoch 19/80: current_loss=15.81896 | best_loss=11.49274
Epoch 20/80: current_loss=10.65647 | best_loss=10.65647
Epoch 21/80: current_loss=13.40032 | best_loss=10.65647
Epoch 22/80: current_loss=14.22343 | best_loss=10.65647
Epoch 23/80: current_loss=12.20935 | best_loss=10.65647
Epoch 24/80: current_loss=15.08322 | best_loss=10.65647
Epoch 25/80: current_loss=13.19098 | best_loss=10.65647
Epoch 26/80: current_loss=20.49011 | best_loss=10.65647
Epoch 27/80: current_loss=13.38039 | best_loss=10.65647
Epoch 28/80: current_loss=29.42203 | best_loss=10.65647
Epoch 29/80: current_loss=11.88318 | best_loss=10.65647
Epoch 30/80: current_loss=17.21327 | best_loss=10.65647
Epoch 31/80: current_loss=13.88097 | best_loss=10.65647
Epoch 32/80: current_loss=14.77918 | best_loss=10.65647
Epoch 33/80: current_loss=13.21046 | best_loss=10.65647
Epoch 34/80: current_loss=14.14132 | best_loss=10.65647
Epoch 35/80: current_loss=11.93627 | best_loss=10.65647
Epoch 36/80: current_loss=36.75944 | best_loss=10.65647
Epoch 37/80: current_loss=12.61247 | best_loss=10.65647
Epoch 38/80: current_loss=10.81848 | best_loss=10.65647
Epoch 39/80: current_loss=11.07120 | best_loss=10.65647
Epoch 40/80: current_loss=12.20985 | best_loss=10.65647
Early Stopping at epoch 40
      explained_var=-0.04134 | mse_loss=10.48957

----------------------------------------------
Params for Trial 38
{'learning_rate': 0.001, 'weight_decay': 0.006364279179238642, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.55497 | best_loss=17.55497
Epoch 1/80: current_loss=10.81590 | best_loss=10.81590
Epoch 2/80: current_loss=10.41991 | best_loss=10.41991
Epoch 3/80: current_loss=10.35341 | best_loss=10.35341
Epoch 4/80: current_loss=10.34153 | best_loss=10.34153
Epoch 5/80: current_loss=10.31495 | best_loss=10.31495
Epoch 6/80: current_loss=10.30556 | best_loss=10.30556
Epoch 7/80: current_loss=10.30681 | best_loss=10.30556
Epoch 8/80: current_loss=10.28982 | best_loss=10.28982
Epoch 9/80: current_loss=10.31739 | best_loss=10.28982
Epoch 10/80: current_loss=10.29355 | best_loss=10.28982
Epoch 11/80: current_loss=10.30456 | best_loss=10.28982
Epoch 12/80: current_loss=10.28204 | best_loss=10.28204
Epoch 13/80: current_loss=10.27911 | best_loss=10.27911
Epoch 14/80: current_loss=10.30164 | best_loss=10.27911
Epoch 15/80: current_loss=10.30548 | best_loss=10.27911
Epoch 16/80: current_loss=10.30161 | best_loss=10.27911
Epoch 17/80: current_loss=10.28122 | best_loss=10.27911
Epoch 18/80: current_loss=10.30580 | best_loss=10.27911
Epoch 19/80: current_loss=10.27833 | best_loss=10.27833
Epoch 20/80: current_loss=10.31177 | best_loss=10.27833
Epoch 21/80: current_loss=10.29353 | best_loss=10.27833
Epoch 22/80: current_loss=10.30197 | best_loss=10.27833
Epoch 23/80: current_loss=10.30326 | best_loss=10.27833
Epoch 24/80: current_loss=10.27959 | best_loss=10.27833
Epoch 25/80: current_loss=10.29029 | best_loss=10.27833
Epoch 26/80: current_loss=10.29861 | best_loss=10.27833
Epoch 27/80: current_loss=10.30856 | best_loss=10.27833
Epoch 28/80: current_loss=10.31721 | best_loss=10.27833
Epoch 29/80: current_loss=10.29988 | best_loss=10.27833
Epoch 30/80: current_loss=10.31331 | best_loss=10.27833
Epoch 31/80: current_loss=10.29407 | best_loss=10.27833
Epoch 32/80: current_loss=10.31244 | best_loss=10.27833
Epoch 33/80: current_loss=10.27858 | best_loss=10.27833
Epoch 34/80: current_loss=10.26710 | best_loss=10.26710
Epoch 35/80: current_loss=10.37959 | best_loss=10.26710
Epoch 36/80: current_loss=10.29026 | best_loss=10.26710
Epoch 37/80: current_loss=10.27778 | best_loss=10.26710
Epoch 38/80: current_loss=10.32544 | best_loss=10.26710
Epoch 39/80: current_loss=10.28939 | best_loss=10.26710
Epoch 40/80: current_loss=10.26887 | best_loss=10.26710
Epoch 41/80: current_loss=10.27966 | best_loss=10.26710
Epoch 42/80: current_loss=10.27983 | best_loss=10.26710
Epoch 43/80: current_loss=10.28301 | best_loss=10.26710
Epoch 44/80: current_loss=10.27623 | best_loss=10.26710
Epoch 45/80: current_loss=10.28386 | best_loss=10.26710
Epoch 46/80: current_loss=10.29903 | best_loss=10.26710
Epoch 47/80: current_loss=10.27686 | best_loss=10.26710
Epoch 48/80: current_loss=10.28648 | best_loss=10.26710
Epoch 49/80: current_loss=10.27140 | best_loss=10.26710
Epoch 50/80: current_loss=10.28976 | best_loss=10.26710
Epoch 51/80: current_loss=10.30781 | best_loss=10.26710
Epoch 52/80: current_loss=10.27424 | best_loss=10.26710
Epoch 53/80: current_loss=10.29727 | best_loss=10.26710
Epoch 54/80: current_loss=10.27735 | best_loss=10.26710
Early Stopping at epoch 54
      explained_var=-0.00237 | mse_loss=10.48906
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.45662 | best_loss=10.45662
Epoch 1/80: current_loss=10.49681 | best_loss=10.45662
Epoch 2/80: current_loss=10.42398 | best_loss=10.42398
Epoch 3/80: current_loss=10.43129 | best_loss=10.42398
Epoch 4/80: current_loss=10.40452 | best_loss=10.40452
Epoch 5/80: current_loss=10.42289 | best_loss=10.40452
Epoch 6/80: current_loss=10.42010 | best_loss=10.40452
Epoch 7/80: current_loss=10.42369 | best_loss=10.40452
Epoch 8/80: current_loss=10.40398 | best_loss=10.40398
Epoch 9/80: current_loss=10.44287 | best_loss=10.40398
Epoch 10/80: current_loss=10.51670 | best_loss=10.40398
Epoch 11/80: current_loss=10.42916 | best_loss=10.40398
Epoch 12/80: current_loss=10.51641 | best_loss=10.40398
Epoch 13/80: current_loss=10.40598 | best_loss=10.40398
Epoch 14/80: current_loss=10.44506 | best_loss=10.40398
Epoch 15/80: current_loss=10.40594 | best_loss=10.40398
Epoch 16/80: current_loss=10.45300 | best_loss=10.40398
Epoch 17/80: current_loss=10.41122 | best_loss=10.40398
Epoch 18/80: current_loss=10.45665 | best_loss=10.40398
Epoch 19/80: current_loss=10.41108 | best_loss=10.40398
Epoch 20/80: current_loss=10.42467 | best_loss=10.40398
Epoch 21/80: current_loss=10.40452 | best_loss=10.40398
Epoch 22/80: current_loss=10.40593 | best_loss=10.40398
Epoch 23/80: current_loss=10.41058 | best_loss=10.40398
Epoch 24/80: current_loss=10.48327 | best_loss=10.40398
Epoch 25/80: current_loss=10.40402 | best_loss=10.40398
Epoch 26/80: current_loss=10.40757 | best_loss=10.40398
Epoch 27/80: current_loss=10.42801 | best_loss=10.40398
Epoch 28/80: current_loss=10.40185 | best_loss=10.40185
Epoch 29/80: current_loss=10.42354 | best_loss=10.40185
Epoch 30/80: current_loss=10.37178 | best_loss=10.37178
Epoch 31/80: current_loss=10.40425 | best_loss=10.37178
Epoch 32/80: current_loss=10.48478 | best_loss=10.37178
Epoch 33/80: current_loss=10.40479 | best_loss=10.37178
Epoch 34/80: current_loss=10.42784 | best_loss=10.37178
Epoch 35/80: current_loss=10.40525 | best_loss=10.37178
Epoch 36/80: current_loss=10.48780 | best_loss=10.37178
Epoch 37/80: current_loss=10.43165 | best_loss=10.37178
Epoch 38/80: current_loss=10.40471 | best_loss=10.37178
Epoch 39/80: current_loss=10.42066 | best_loss=10.37178
Epoch 40/80: current_loss=10.40824 | best_loss=10.37178
Epoch 41/80: current_loss=10.41266 | best_loss=10.37178
Epoch 42/80: current_loss=10.40340 | best_loss=10.37178
Epoch 43/80: current_loss=10.40902 | best_loss=10.37178
Epoch 44/80: current_loss=10.50482 | best_loss=10.37178
Epoch 45/80: current_loss=10.40482 | best_loss=10.37178
Epoch 46/80: current_loss=10.40554 | best_loss=10.37178
Epoch 47/80: current_loss=10.42018 | best_loss=10.37178
Epoch 48/80: current_loss=10.40589 | best_loss=10.37178
Epoch 49/80: current_loss=10.40388 | best_loss=10.37178
Epoch 50/80: current_loss=10.46490 | best_loss=10.37178
Early Stopping at epoch 50
      explained_var=0.01241 | mse_loss=10.01123
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.53026 | best_loss=7.53026
Epoch 1/80: current_loss=7.50569 | best_loss=7.50569
Epoch 2/80: current_loss=7.62900 | best_loss=7.50569
Epoch 3/80: current_loss=7.54742 | best_loss=7.50569
Epoch 4/80: current_loss=7.51675 | best_loss=7.50569
Epoch 5/80: current_loss=7.53463 | best_loss=7.50569
Epoch 6/80: current_loss=7.59049 | best_loss=7.50569
Epoch 7/80: current_loss=7.58295 | best_loss=7.50569
Epoch 8/80: current_loss=7.52093 | best_loss=7.50569
Epoch 9/80: current_loss=7.68471 | best_loss=7.50569
Epoch 10/80: current_loss=7.53499 | best_loss=7.50569
Epoch 11/80: current_loss=7.54459 | best_loss=7.50569
Epoch 12/80: current_loss=7.55675 | best_loss=7.50569
Epoch 13/80: current_loss=7.60128 | best_loss=7.50569
Epoch 14/80: current_loss=7.55404 | best_loss=7.50569
Epoch 15/80: current_loss=7.57507 | best_loss=7.50569
Epoch 16/80: current_loss=7.65691 | best_loss=7.50569
Epoch 17/80: current_loss=7.49557 | best_loss=7.49557
Epoch 18/80: current_loss=7.76713 | best_loss=7.49557
Epoch 19/80: current_loss=7.62369 | best_loss=7.49557
Epoch 20/80: current_loss=7.55879 | best_loss=7.49557
Epoch 21/80: current_loss=7.61669 | best_loss=7.49557
Epoch 22/80: current_loss=7.72994 | best_loss=7.49557
Epoch 23/80: current_loss=7.51169 | best_loss=7.49557
Epoch 24/80: current_loss=7.56440 | best_loss=7.49557
Epoch 25/80: current_loss=7.59278 | best_loss=7.49557
Epoch 26/80: current_loss=7.50682 | best_loss=7.49557
Epoch 27/80: current_loss=7.56750 | best_loss=7.49557
Epoch 28/80: current_loss=7.57381 | best_loss=7.49557
Epoch 29/80: current_loss=7.55499 | best_loss=7.49557
Epoch 30/80: current_loss=7.63775 | best_loss=7.49557
Epoch 31/80: current_loss=7.48063 | best_loss=7.48063
Epoch 32/80: current_loss=7.69226 | best_loss=7.48063
Epoch 33/80: current_loss=7.67590 | best_loss=7.48063
Epoch 34/80: current_loss=7.79051 | best_loss=7.48063
Epoch 35/80: current_loss=7.48990 | best_loss=7.48063
Epoch 36/80: current_loss=7.49510 | best_loss=7.48063
Epoch 37/80: current_loss=7.54348 | best_loss=7.48063
Epoch 38/80: current_loss=9.13951 | best_loss=7.48063
Epoch 39/80: current_loss=7.44996 | best_loss=7.44996
Epoch 40/80: current_loss=7.87275 | best_loss=7.44996
Epoch 41/80: current_loss=7.44860 | best_loss=7.44860
Epoch 42/80: current_loss=7.47899 | best_loss=7.44860
Epoch 43/80: current_loss=7.78718 | best_loss=7.44860
Epoch 44/80: current_loss=7.78732 | best_loss=7.44860
Epoch 45/80: current_loss=7.61953 | best_loss=7.44860
Epoch 46/80: current_loss=7.59242 | best_loss=7.44860
Epoch 47/80: current_loss=7.59209 | best_loss=7.44860
Epoch 48/80: current_loss=7.59080 | best_loss=7.44860
Epoch 49/80: current_loss=7.48235 | best_loss=7.44860
Epoch 50/80: current_loss=7.55620 | best_loss=7.44860
Epoch 51/80: current_loss=7.60826 | best_loss=7.44860
Epoch 52/80: current_loss=7.55544 | best_loss=7.44860
Epoch 53/80: current_loss=7.56406 | best_loss=7.44860
Epoch 54/80: current_loss=7.63734 | best_loss=7.44860
Epoch 55/80: current_loss=7.80953 | best_loss=7.44860
Epoch 56/80: current_loss=7.58804 | best_loss=7.44860
Epoch 57/80: current_loss=7.65437 | best_loss=7.44860
Epoch 58/80: current_loss=7.64053 | best_loss=7.44860
Epoch 59/80: current_loss=7.54791 | best_loss=7.44860
Epoch 60/80: current_loss=7.55782 | best_loss=7.44860
Epoch 61/80: current_loss=7.62790 | best_loss=7.44860
Early Stopping at epoch 61
      explained_var=0.00484 | mse_loss=7.55009
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.92582 | best_loss=8.92582
Epoch 1/80: current_loss=8.78421 | best_loss=8.78421
Epoch 2/80: current_loss=8.74055 | best_loss=8.74055
Epoch 3/80: current_loss=8.82882 | best_loss=8.74055
Epoch 4/80: current_loss=8.80164 | best_loss=8.74055
Epoch 5/80: current_loss=8.91279 | best_loss=8.74055
Epoch 6/80: current_loss=8.83843 | best_loss=8.74055
Epoch 7/80: current_loss=8.79599 | best_loss=8.74055
Epoch 8/80: current_loss=8.80517 | best_loss=8.74055
Epoch 9/80: current_loss=8.86215 | best_loss=8.74055
Epoch 10/80: current_loss=8.81567 | best_loss=8.74055
Epoch 11/80: current_loss=8.80738 | best_loss=8.74055
Epoch 12/80: current_loss=8.85565 | best_loss=8.74055
Epoch 13/80: current_loss=8.83232 | best_loss=8.74055
Epoch 14/80: current_loss=8.87579 | best_loss=8.74055
Epoch 15/80: current_loss=8.77093 | best_loss=8.74055
Epoch 16/80: current_loss=8.78998 | best_loss=8.74055
Epoch 17/80: current_loss=8.84530 | best_loss=8.74055
Epoch 18/80: current_loss=8.81024 | best_loss=8.74055
Epoch 19/80: current_loss=8.87775 | best_loss=8.74055
Epoch 20/80: current_loss=8.82551 | best_loss=8.74055
Epoch 21/80: current_loss=8.86198 | best_loss=8.74055
Epoch 22/80: current_loss=8.75247 | best_loss=8.74055
Early Stopping at epoch 22
      explained_var=0.00190 | mse_loss=8.70920
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.21199 | best_loss=9.21199
Epoch 1/80: current_loss=9.21064 | best_loss=9.21064
Epoch 2/80: current_loss=9.20808 | best_loss=9.20808
Epoch 3/80: current_loss=9.26920 | best_loss=9.20808
Epoch 4/80: current_loss=9.18047 | best_loss=9.18047
Epoch 5/80: current_loss=9.28378 | best_loss=9.18047
Epoch 6/80: current_loss=9.18623 | best_loss=9.18047
Epoch 7/80: current_loss=9.20515 | best_loss=9.18047
Epoch 8/80: current_loss=9.21463 | best_loss=9.18047
Epoch 9/80: current_loss=9.17874 | best_loss=9.17874
Epoch 10/80: current_loss=9.24820 | best_loss=9.17874
Epoch 11/80: current_loss=9.20208 | best_loss=9.17874
Epoch 12/80: current_loss=9.17158 | best_loss=9.17158
Epoch 13/80: current_loss=9.16737 | best_loss=9.16737
Epoch 14/80: current_loss=9.17080 | best_loss=9.16737
Epoch 15/80: current_loss=9.19044 | best_loss=9.16737
Epoch 16/80: current_loss=9.25321 | best_loss=9.16737
Epoch 17/80: current_loss=9.21574 | best_loss=9.16737
Epoch 18/80: current_loss=9.29138 | best_loss=9.16737
Epoch 19/80: current_loss=9.18789 | best_loss=9.16737
Epoch 20/80: current_loss=9.28854 | best_loss=9.16737
Epoch 21/80: current_loss=9.28969 | best_loss=9.16737
Epoch 22/80: current_loss=9.19652 | best_loss=9.16737
Epoch 23/80: current_loss=9.19072 | best_loss=9.16737
Epoch 24/80: current_loss=9.19924 | best_loss=9.16737
Epoch 25/80: current_loss=9.23663 | best_loss=9.16737
Epoch 26/80: current_loss=9.19991 | best_loss=9.16737
Epoch 27/80: current_loss=9.17825 | best_loss=9.16737
Epoch 28/80: current_loss=9.21181 | best_loss=9.16737
Epoch 29/80: current_loss=9.16334 | best_loss=9.16334
Epoch 30/80: current_loss=9.14319 | best_loss=9.14319
Epoch 31/80: current_loss=9.22134 | best_loss=9.14319
Epoch 32/80: current_loss=9.82903 | best_loss=9.14319
Epoch 33/80: current_loss=9.34482 | best_loss=9.14319
Epoch 34/80: current_loss=9.24456 | best_loss=9.14319
Epoch 35/80: current_loss=9.17507 | best_loss=9.14319
Epoch 36/80: current_loss=9.19538 | best_loss=9.14319
Epoch 37/80: current_loss=9.19038 | best_loss=9.14319
Epoch 38/80: current_loss=9.36488 | best_loss=9.14319
Epoch 39/80: current_loss=9.19937 | best_loss=9.14319
Epoch 40/80: current_loss=9.26960 | best_loss=9.14319
Epoch 41/80: current_loss=9.21802 | best_loss=9.14319
Epoch 42/80: current_loss=9.19113 | best_loss=9.14319
Epoch 43/80: current_loss=9.17372 | best_loss=9.14319
Epoch 44/80: current_loss=9.18120 | best_loss=9.14319
Epoch 45/80: current_loss=9.17046 | best_loss=9.14319
Epoch 46/80: current_loss=9.29578 | best_loss=9.14319
Epoch 47/80: current_loss=9.17573 | best_loss=9.14319
Epoch 48/80: current_loss=9.36586 | best_loss=9.14319
Epoch 49/80: current_loss=9.26340 | best_loss=9.14319
Epoch 50/80: current_loss=9.16231 | best_loss=9.14319
Early Stopping at epoch 50
      explained_var=0.02064 | mse_loss=9.10593
----------------------------------------------
Average early_stopping_point: 27| avg_exp_var=0.00748| avg_loss=9.17310
----------------------------------------------


----------------------------------------------
Params for Trial 39
{'learning_rate': 0.0001, 'weight_decay': 0.009492479615578628, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=99.06805 | best_loss=99.06805
Epoch 1/80: current_loss=92.14866 | best_loss=92.14866
Epoch 2/80: current_loss=79.78988 | best_loss=79.78988
Epoch 3/80: current_loss=62.63212 | best_loss=62.63212
Epoch 4/80: current_loss=50.41896 | best_loss=50.41896
Epoch 5/80: current_loss=43.44391 | best_loss=43.44391
Epoch 6/80: current_loss=39.00314 | best_loss=39.00314
Epoch 7/80: current_loss=35.88025 | best_loss=35.88025
Epoch 8/80: current_loss=33.48354 | best_loss=33.48354
Epoch 9/80: current_loss=31.50950 | best_loss=31.50950
Epoch 10/80: current_loss=29.80856 | best_loss=29.80856
Epoch 11/80: current_loss=28.29664 | best_loss=28.29664
Epoch 12/80: current_loss=26.94850 | best_loss=26.94850
Epoch 13/80: current_loss=25.72954 | best_loss=25.72954
Epoch 14/80: current_loss=24.62115 | best_loss=24.62115
Epoch 15/80: current_loss=23.59477 | best_loss=23.59477
Epoch 16/80: current_loss=22.64313 | best_loss=22.64313
Epoch 17/80: current_loss=21.78827 | best_loss=21.78827
Epoch 18/80: current_loss=20.95019 | best_loss=20.95019
Epoch 19/80: current_loss=20.21887 | best_loss=20.21887
Epoch 20/80: current_loss=19.53019 | best_loss=19.53019
Epoch 21/80: current_loss=18.87703 | best_loss=18.87703
Epoch 22/80: current_loss=18.26863 | best_loss=18.26863
Epoch 23/80: current_loss=17.70085 | best_loss=17.70085
Epoch 24/80: current_loss=17.17365 | best_loss=17.17365
Epoch 25/80: current_loss=16.68357 | best_loss=16.68357
Epoch 26/80: current_loss=16.22240 | best_loss=16.22240
Epoch 27/80: current_loss=15.80071 | best_loss=15.80071
Epoch 28/80: current_loss=15.42965 | best_loss=15.42965
Epoch 29/80: current_loss=15.06918 | best_loss=15.06918
Epoch 30/80: current_loss=14.74138 | best_loss=14.74138
Epoch 31/80: current_loss=14.43203 | best_loss=14.43203
Epoch 32/80: current_loss=14.14264 | best_loss=14.14264
Epoch 33/80: current_loss=13.87779 | best_loss=13.87779
Epoch 34/80: current_loss=13.64398 | best_loss=13.64398
Epoch 35/80: current_loss=13.42863 | best_loss=13.42863
Epoch 36/80: current_loss=13.21114 | best_loss=13.21114
Epoch 37/80: current_loss=13.01972 | best_loss=13.01972
Epoch 38/80: current_loss=12.84737 | best_loss=12.84737
Epoch 39/80: current_loss=12.69010 | best_loss=12.69010
Epoch 40/80: current_loss=12.55024 | best_loss=12.55024
Epoch 41/80: current_loss=12.41808 | best_loss=12.41808
Epoch 42/80: current_loss=12.29353 | best_loss=12.29353
Epoch 43/80: current_loss=12.17946 | best_loss=12.17946
Epoch 44/80: current_loss=12.07561 | best_loss=12.07561
Epoch 45/80: current_loss=11.97703 | best_loss=11.97703
Epoch 46/80: current_loss=11.89009 | best_loss=11.89009
Epoch 47/80: current_loss=11.80254 | best_loss=11.80254
Epoch 48/80: current_loss=11.71797 | best_loss=11.71797
Epoch 49/80: current_loss=11.65022 | best_loss=11.65022
Epoch 50/80: current_loss=11.57691 | best_loss=11.57691
Epoch 51/80: current_loss=11.51093 | best_loss=11.51093
Epoch 52/80: current_loss=11.46200 | best_loss=11.46200
Epoch 53/80: current_loss=11.40927 | best_loss=11.40927
Epoch 54/80: current_loss=11.36092 | best_loss=11.36092
Epoch 55/80: current_loss=11.31824 | best_loss=11.31824
Epoch 56/80: current_loss=11.27397 | best_loss=11.27397
Epoch 57/80: current_loss=11.23669 | best_loss=11.23669
Epoch 58/80: current_loss=11.20036 | best_loss=11.20036
Epoch 59/80: current_loss=11.16793 | best_loss=11.16793
Epoch 60/80: current_loss=11.13295 | best_loss=11.13295
Epoch 61/80: current_loss=11.10979 | best_loss=11.10979
Epoch 62/80: current_loss=11.08143 | best_loss=11.08143
Epoch 63/80: current_loss=11.05535 | best_loss=11.05535
Epoch 64/80: current_loss=11.02653 | best_loss=11.02653
Epoch 65/80: current_loss=10.99933 | best_loss=10.99933
Epoch 66/80: current_loss=10.97845 | best_loss=10.97845
Epoch 67/80: current_loss=10.95470 | best_loss=10.95470
Epoch 68/80: current_loss=10.93196 | best_loss=10.93196
Epoch 69/80: current_loss=10.91781 | best_loss=10.91781
Epoch 70/80: current_loss=10.89629 | best_loss=10.89629
Epoch 71/80: current_loss=10.87770 | best_loss=10.87770
Epoch 72/80: current_loss=10.86246 | best_loss=10.86246
Epoch 73/80: current_loss=10.84506 | best_loss=10.84506
Epoch 74/80: current_loss=10.82798 | best_loss=10.82798
Epoch 75/80: current_loss=10.81435 | best_loss=10.81435
Epoch 76/80: current_loss=10.79752 | best_loss=10.79752
Epoch 77/80: current_loss=10.78391 | best_loss=10.78391
Epoch 78/80: current_loss=10.77213 | best_loss=10.77213
Epoch 79/80: current_loss=10.75937 | best_loss=10.75937
      explained_var=-0.04750 | mse_loss=11.04389

----------------------------------------------
Params for Trial 40
{'learning_rate': 1e-05, 'weight_decay': 0.008149884568400495, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=103.54812 | best_loss=103.54812
Epoch 1/80: current_loss=101.82355 | best_loss=101.82355
Epoch 2/80: current_loss=99.50743 | best_loss=99.50743
Epoch 3/80: current_loss=95.41635 | best_loss=95.41635
Epoch 4/80: current_loss=84.28347 | best_loss=84.28347
Epoch 5/80: current_loss=61.43719 | best_loss=61.43719
Epoch 6/80: current_loss=46.75083 | best_loss=46.75083
Epoch 7/80: current_loss=38.82363 | best_loss=38.82363
Epoch 8/80: current_loss=33.85831 | best_loss=33.85831
Epoch 9/80: current_loss=30.68275 | best_loss=30.68275
Epoch 10/80: current_loss=28.39798 | best_loss=28.39798
Epoch 11/80: current_loss=26.72615 | best_loss=26.72615
Epoch 12/80: current_loss=25.47984 | best_loss=25.47984
Epoch 13/80: current_loss=24.50996 | best_loss=24.50996
Epoch 14/80: current_loss=23.79645 | best_loss=23.79645
Epoch 15/80: current_loss=23.19203 | best_loss=23.19203
Epoch 16/80: current_loss=22.68826 | best_loss=22.68826
Epoch 17/80: current_loss=22.28061 | best_loss=22.28061
Epoch 18/80: current_loss=21.92909 | best_loss=21.92909
Epoch 19/80: current_loss=21.63246 | best_loss=21.63246
Epoch 20/80: current_loss=21.38136 | best_loss=21.38136
Epoch 21/80: current_loss=21.15861 | best_loss=21.15861
Epoch 22/80: current_loss=20.97812 | best_loss=20.97812
Epoch 23/80: current_loss=20.78328 | best_loss=20.78328
Epoch 24/80: current_loss=20.62783 | best_loss=20.62783
Epoch 25/80: current_loss=20.47938 | best_loss=20.47938
Epoch 26/80: current_loss=20.34684 | best_loss=20.34684
Epoch 27/80: current_loss=20.22254 | best_loss=20.22254
Epoch 28/80: current_loss=20.09646 | best_loss=20.09646
Epoch 29/80: current_loss=19.97337 | best_loss=19.97337
Epoch 30/80: current_loss=19.86540 | best_loss=19.86540
Epoch 31/80: current_loss=19.75708 | best_loss=19.75708
Epoch 32/80: current_loss=19.64053 | best_loss=19.64053
Epoch 33/80: current_loss=19.54223 | best_loss=19.54223
Epoch 34/80: current_loss=19.44213 | best_loss=19.44213
Epoch 35/80: current_loss=19.33060 | best_loss=19.33060
Epoch 36/80: current_loss=19.24409 | best_loss=19.24409
Epoch 37/80: current_loss=19.14781 | best_loss=19.14781
Epoch 38/80: current_loss=19.05930 | best_loss=19.05930
Epoch 39/80: current_loss=18.98025 | best_loss=18.98025
Epoch 40/80: current_loss=18.89779 | best_loss=18.89779
Epoch 41/80: current_loss=18.80819 | best_loss=18.80819
Epoch 42/80: current_loss=18.72510 | best_loss=18.72510
Epoch 43/80: current_loss=18.64416 | best_loss=18.64416
Epoch 44/80: current_loss=18.56822 | best_loss=18.56822
Epoch 45/80: current_loss=18.48526 | best_loss=18.48526
Epoch 46/80: current_loss=18.41406 | best_loss=18.41406
Epoch 47/80: current_loss=18.33765 | best_loss=18.33765
Epoch 48/80: current_loss=18.24981 | best_loss=18.24981
Epoch 49/80: current_loss=18.17508 | best_loss=18.17508
Epoch 50/80: current_loss=18.10318 | best_loss=18.10318
Epoch 51/80: current_loss=18.03463 | best_loss=18.03463
Epoch 52/80: current_loss=17.96475 | best_loss=17.96475
Epoch 53/80: current_loss=17.89496 | best_loss=17.89496
Epoch 54/80: current_loss=17.83655 | best_loss=17.83655
Epoch 55/80: current_loss=17.76942 | best_loss=17.76942
Epoch 56/80: current_loss=17.70961 | best_loss=17.70961
Epoch 57/80: current_loss=17.65336 | best_loss=17.65336
Epoch 58/80: current_loss=17.59201 | best_loss=17.59201
Epoch 59/80: current_loss=17.54342 | best_loss=17.54342
Epoch 60/80: current_loss=17.48723 | best_loss=17.48723
Epoch 61/80: current_loss=17.42803 | best_loss=17.42803
Epoch 62/80: current_loss=17.37439 | best_loss=17.37439
Epoch 63/80: current_loss=17.31758 | best_loss=17.31758
Epoch 64/80: current_loss=17.26837 | best_loss=17.26837
Epoch 65/80: current_loss=17.21464 | best_loss=17.21464
Epoch 66/80: current_loss=17.16131 | best_loss=17.16131
Epoch 67/80: current_loss=17.11312 | best_loss=17.11312
Epoch 68/80: current_loss=17.06264 | best_loss=17.06264
Epoch 69/80: current_loss=17.01049 | best_loss=17.01049
Epoch 70/80: current_loss=16.96067 | best_loss=16.96067
Epoch 71/80: current_loss=16.91878 | best_loss=16.91878
Epoch 72/80: current_loss=16.87326 | best_loss=16.87326
Epoch 73/80: current_loss=16.82512 | best_loss=16.82512
Epoch 74/80: current_loss=16.78469 | best_loss=16.78469
Epoch 75/80: current_loss=16.74564 | best_loss=16.74564
Epoch 76/80: current_loss=16.70431 | best_loss=16.70431
Epoch 77/80: current_loss=16.66621 | best_loss=16.66621
Epoch 78/80: current_loss=16.62149 | best_loss=16.62149
Epoch 79/80: current_loss=16.58223 | best_loss=16.58223
      explained_var=-0.53206 | mse_loss=16.66174

----------------------------------------------
Params for Trial 41
{'learning_rate': 0.001, 'weight_decay': 0.008836440580599172, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=72.67456 | best_loss=72.67456
Epoch 1/80: current_loss=46.49625 | best_loss=46.49625
Epoch 2/80: current_loss=36.09299 | best_loss=36.09299
Epoch 3/80: current_loss=29.05161 | best_loss=29.05161
Epoch 4/80: current_loss=23.79575 | best_loss=23.79575
Epoch 5/80: current_loss=19.91104 | best_loss=19.91104
Epoch 6/80: current_loss=17.05922 | best_loss=17.05922
Epoch 7/80: current_loss=15.01375 | best_loss=15.01375
Epoch 8/80: current_loss=13.51035 | best_loss=13.51035
Epoch 9/80: current_loss=12.51114 | best_loss=12.51114
Epoch 10/80: current_loss=11.79825 | best_loss=11.79825
Epoch 11/80: current_loss=11.31245 | best_loss=11.31245
Epoch 12/80: current_loss=11.01152 | best_loss=11.01152
Epoch 13/80: current_loss=10.80000 | best_loss=10.80000
Epoch 14/80: current_loss=10.65376 | best_loss=10.65376
Epoch 15/80: current_loss=10.57376 | best_loss=10.57376
Epoch 16/80: current_loss=10.51095 | best_loss=10.51095
Epoch 17/80: current_loss=10.47822 | best_loss=10.47822
Epoch 18/80: current_loss=10.43567 | best_loss=10.43567
Epoch 19/80: current_loss=10.41617 | best_loss=10.41617
Epoch 20/80: current_loss=10.40473 | best_loss=10.40473
Epoch 21/80: current_loss=10.39169 | best_loss=10.39169
Epoch 22/80: current_loss=10.37936 | best_loss=10.37936
Epoch 23/80: current_loss=10.38535 | best_loss=10.37936
Epoch 24/80: current_loss=10.39041 | best_loss=10.37936
Epoch 25/80: current_loss=10.39160 | best_loss=10.37936
Epoch 26/80: current_loss=10.38224 | best_loss=10.37936
Epoch 27/80: current_loss=10.37875 | best_loss=10.37875
Epoch 28/80: current_loss=10.36765 | best_loss=10.36765
Epoch 29/80: current_loss=10.36804 | best_loss=10.36765
Epoch 30/80: current_loss=10.35605 | best_loss=10.35605
Epoch 31/80: current_loss=10.35774 | best_loss=10.35605
Epoch 32/80: current_loss=10.34794 | best_loss=10.34794
Epoch 33/80: current_loss=10.34255 | best_loss=10.34255
Epoch 34/80: current_loss=10.34655 | best_loss=10.34255
Epoch 35/80: current_loss=10.33231 | best_loss=10.33231
Epoch 36/80: current_loss=10.32936 | best_loss=10.32936
Epoch 37/80: current_loss=10.33308 | best_loss=10.32936
Epoch 38/80: current_loss=10.33578 | best_loss=10.32936
Epoch 39/80: current_loss=10.34195 | best_loss=10.32936
Epoch 40/80: current_loss=10.33751 | best_loss=10.32936
Epoch 41/80: current_loss=10.33785 | best_loss=10.32936
Epoch 42/80: current_loss=10.33903 | best_loss=10.32936
Epoch 43/80: current_loss=10.33602 | best_loss=10.32936
Epoch 44/80: current_loss=10.32652 | best_loss=10.32652
Epoch 45/80: current_loss=10.33651 | best_loss=10.32652
Epoch 46/80: current_loss=10.32916 | best_loss=10.32652
Epoch 47/80: current_loss=10.32095 | best_loss=10.32095
Epoch 48/80: current_loss=10.32535 | best_loss=10.32095
Epoch 49/80: current_loss=10.33382 | best_loss=10.32095
Epoch 50/80: current_loss=10.31729 | best_loss=10.31729
Epoch 51/80: current_loss=10.32344 | best_loss=10.31729
Epoch 52/80: current_loss=10.32128 | best_loss=10.31729
Epoch 53/80: current_loss=10.33752 | best_loss=10.31729
Epoch 54/80: current_loss=10.32697 | best_loss=10.31729
Epoch 55/80: current_loss=10.32576 | best_loss=10.31729
Epoch 56/80: current_loss=10.32784 | best_loss=10.31729
Epoch 57/80: current_loss=10.31080 | best_loss=10.31080
Epoch 58/80: current_loss=10.30963 | best_loss=10.30963
Epoch 59/80: current_loss=10.31530 | best_loss=10.30963
Epoch 60/80: current_loss=10.31018 | best_loss=10.30963
Epoch 61/80: current_loss=10.30502 | best_loss=10.30502
Epoch 62/80: current_loss=10.32088 | best_loss=10.30502
Epoch 63/80: current_loss=10.31404 | best_loss=10.30502
Epoch 64/80: current_loss=10.31901 | best_loss=10.30502
Epoch 65/80: current_loss=10.32344 | best_loss=10.30502
Epoch 66/80: current_loss=10.31624 | best_loss=10.30502
Epoch 67/80: current_loss=10.30920 | best_loss=10.30502
Epoch 68/80: current_loss=10.31581 | best_loss=10.30502
Epoch 69/80: current_loss=10.31236 | best_loss=10.30502
Epoch 70/80: current_loss=10.31698 | best_loss=10.30502
Epoch 71/80: current_loss=10.31667 | best_loss=10.30502
Epoch 72/80: current_loss=10.31235 | best_loss=10.30502
Epoch 73/80: current_loss=10.31735 | best_loss=10.30502
Epoch 74/80: current_loss=10.31427 | best_loss=10.30502
Epoch 75/80: current_loss=10.30720 | best_loss=10.30502
Epoch 76/80: current_loss=10.28704 | best_loss=10.28704
Epoch 77/80: current_loss=10.30553 | best_loss=10.28704
Epoch 78/80: current_loss=10.30155 | best_loss=10.28704
Epoch 79/80: current_loss=10.30369 | best_loss=10.28704
      explained_var=-0.00329 | mse_loss=10.52125

----------------------------------------------
Params for Trial 42
{'learning_rate': 0.001, 'weight_decay': 0.009123047103858752, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=59.32290 | best_loss=59.32290
Epoch 1/80: current_loss=43.29078 | best_loss=43.29078
Epoch 2/80: current_loss=34.21004 | best_loss=34.21004
Epoch 3/80: current_loss=27.31518 | best_loss=27.31518
Epoch 4/80: current_loss=22.27890 | best_loss=22.27890
Epoch 5/80: current_loss=18.62619 | best_loss=18.62619
Epoch 6/80: current_loss=15.97593 | best_loss=15.97593
Epoch 7/80: current_loss=14.12216 | best_loss=14.12216
Epoch 8/80: current_loss=12.87151 | best_loss=12.87151
Epoch 9/80: current_loss=11.95137 | best_loss=11.95137
Epoch 10/80: current_loss=11.35268 | best_loss=11.35268
Epoch 11/80: current_loss=10.95601 | best_loss=10.95601
Epoch 12/80: current_loss=10.72636 | best_loss=10.72636
Epoch 13/80: current_loss=10.58065 | best_loss=10.58065
Epoch 14/80: current_loss=10.48074 | best_loss=10.48074
Epoch 15/80: current_loss=10.42287 | best_loss=10.42287
Epoch 16/80: current_loss=10.39521 | best_loss=10.39521
Epoch 17/80: current_loss=10.36590 | best_loss=10.36590
Epoch 18/80: current_loss=10.35812 | best_loss=10.35812
Epoch 19/80: current_loss=10.33873 | best_loss=10.33873
Epoch 20/80: current_loss=10.32559 | best_loss=10.32559
Epoch 21/80: current_loss=10.32586 | best_loss=10.32559
Epoch 22/80: current_loss=10.31357 | best_loss=10.31357
Epoch 23/80: current_loss=10.31455 | best_loss=10.31357
Epoch 24/80: current_loss=10.31415 | best_loss=10.31357
Epoch 25/80: current_loss=10.30689 | best_loss=10.30689
Epoch 26/80: current_loss=10.31515 | best_loss=10.30689
Epoch 27/80: current_loss=10.32196 | best_loss=10.30689
Epoch 28/80: current_loss=10.31136 | best_loss=10.30689
Epoch 29/80: current_loss=10.30832 | best_loss=10.30689
Epoch 30/80: current_loss=10.30542 | best_loss=10.30542
Epoch 31/80: current_loss=10.30702 | best_loss=10.30542
Epoch 32/80: current_loss=10.29903 | best_loss=10.29903
Epoch 33/80: current_loss=10.30651 | best_loss=10.29903
Epoch 34/80: current_loss=10.30608 | best_loss=10.29903
Epoch 35/80: current_loss=10.30772 | best_loss=10.29903
Epoch 36/80: current_loss=10.30032 | best_loss=10.29903
Epoch 37/80: current_loss=10.30250 | best_loss=10.29903
Epoch 38/80: current_loss=10.31070 | best_loss=10.29903
Epoch 39/80: current_loss=10.30439 | best_loss=10.29903
Epoch 40/80: current_loss=10.30347 | best_loss=10.29903
Epoch 41/80: current_loss=10.29989 | best_loss=10.29903
Epoch 42/80: current_loss=10.29606 | best_loss=10.29606
Epoch 43/80: current_loss=10.29157 | best_loss=10.29157
Epoch 44/80: current_loss=10.30879 | best_loss=10.29157
Epoch 45/80: current_loss=10.31614 | best_loss=10.29157
Epoch 46/80: current_loss=10.31210 | best_loss=10.29157
Epoch 47/80: current_loss=10.30276 | best_loss=10.29157
Epoch 48/80: current_loss=10.30298 | best_loss=10.29157
Epoch 49/80: current_loss=10.31739 | best_loss=10.29157
Epoch 50/80: current_loss=10.28895 | best_loss=10.28895
Epoch 51/80: current_loss=10.29797 | best_loss=10.28895
Epoch 52/80: current_loss=10.29744 | best_loss=10.28895
Epoch 53/80: current_loss=10.30372 | best_loss=10.28895
Epoch 54/80: current_loss=10.30036 | best_loss=10.28895
Epoch 55/80: current_loss=10.31453 | best_loss=10.28895
Epoch 56/80: current_loss=10.30080 | best_loss=10.28895
Epoch 57/80: current_loss=10.31005 | best_loss=10.28895
Epoch 58/80: current_loss=10.31334 | best_loss=10.28895
Epoch 59/80: current_loss=10.28754 | best_loss=10.28754
Epoch 60/80: current_loss=10.30734 | best_loss=10.28754
Epoch 61/80: current_loss=10.28720 | best_loss=10.28720
Epoch 62/80: current_loss=10.29677 | best_loss=10.28720
Epoch 63/80: current_loss=10.30526 | best_loss=10.28720
Epoch 64/80: current_loss=10.30873 | best_loss=10.28720
Epoch 65/80: current_loss=10.30644 | best_loss=10.28720
Epoch 66/80: current_loss=10.30236 | best_loss=10.28720
Epoch 67/80: current_loss=10.28447 | best_loss=10.28447
Epoch 68/80: current_loss=10.30520 | best_loss=10.28447
Epoch 69/80: current_loss=10.29437 | best_loss=10.28447
Epoch 70/80: current_loss=10.29985 | best_loss=10.28447
Epoch 71/80: current_loss=10.31533 | best_loss=10.28447
Epoch 72/80: current_loss=10.29919 | best_loss=10.28447
Epoch 73/80: current_loss=10.29877 | best_loss=10.28447
Epoch 74/80: current_loss=10.30427 | best_loss=10.28447
Epoch 75/80: current_loss=10.30256 | best_loss=10.28447
Epoch 76/80: current_loss=10.31196 | best_loss=10.28447
Epoch 77/80: current_loss=10.29384 | best_loss=10.28447
Epoch 78/80: current_loss=10.28804 | best_loss=10.28447
Epoch 79/80: current_loss=10.30451 | best_loss=10.28447
      explained_var=-0.00222 | mse_loss=10.52069

----------------------------------------------
Params for Trial 43
{'learning_rate': 0.001, 'weight_decay': 0.007606623782710665, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=93.26949 | best_loss=93.26949
Epoch 1/80: current_loss=66.52510 | best_loss=66.52510
Epoch 2/80: current_loss=49.19163 | best_loss=49.19163
Epoch 3/80: current_loss=38.12818 | best_loss=38.12818
Epoch 4/80: current_loss=30.51090 | best_loss=30.51090
Epoch 5/80: current_loss=24.97588 | best_loss=24.97588
Epoch 6/80: current_loss=21.13202 | best_loss=21.13202
Epoch 7/80: current_loss=18.23676 | best_loss=18.23676
Epoch 8/80: current_loss=16.21236 | best_loss=16.21236
Epoch 9/80: current_loss=14.71602 | best_loss=14.71602
Epoch 10/80: current_loss=13.73196 | best_loss=13.73196
Epoch 11/80: current_loss=13.03970 | best_loss=13.03970
Epoch 12/80: current_loss=12.57284 | best_loss=12.57284
Epoch 13/80: current_loss=12.24194 | best_loss=12.24194
Epoch 14/80: current_loss=11.98178 | best_loss=11.98178
Epoch 15/80: current_loss=11.79838 | best_loss=11.79838
Epoch 16/80: current_loss=11.69614 | best_loss=11.69614
Epoch 17/80: current_loss=11.60633 | best_loss=11.60633
Epoch 18/80: current_loss=11.51881 | best_loss=11.51881
Epoch 19/80: current_loss=11.45786 | best_loss=11.45786
Epoch 20/80: current_loss=11.41761 | best_loss=11.41761
Epoch 21/80: current_loss=11.37629 | best_loss=11.37629
Epoch 22/80: current_loss=11.32723 | best_loss=11.32723
Epoch 23/80: current_loss=11.29127 | best_loss=11.29127
Epoch 24/80: current_loss=11.25069 | best_loss=11.25069
Epoch 25/80: current_loss=11.23036 | best_loss=11.23036
Epoch 26/80: current_loss=11.19756 | best_loss=11.19756
Epoch 27/80: current_loss=11.16646 | best_loss=11.16646
Epoch 28/80: current_loss=11.13854 | best_loss=11.13854
Epoch 29/80: current_loss=11.09990 | best_loss=11.09990
Epoch 30/80: current_loss=11.06822 | best_loss=11.06822
Epoch 31/80: current_loss=11.04435 | best_loss=11.04435
Epoch 32/80: current_loss=11.01735 | best_loss=11.01735
Epoch 33/80: current_loss=10.99560 | best_loss=10.99560
Epoch 34/80: current_loss=10.97894 | best_loss=10.97894
Epoch 35/80: current_loss=10.95523 | best_loss=10.95523
Epoch 36/80: current_loss=10.94529 | best_loss=10.94529
Epoch 37/80: current_loss=10.92169 | best_loss=10.92169
Epoch 38/80: current_loss=10.89727 | best_loss=10.89727
Epoch 39/80: current_loss=10.88719 | best_loss=10.88719
Epoch 40/80: current_loss=10.87194 | best_loss=10.87194
Epoch 41/80: current_loss=10.84863 | best_loss=10.84863
Epoch 42/80: current_loss=10.83755 | best_loss=10.83755
Epoch 43/80: current_loss=10.80962 | best_loss=10.80962
Epoch 44/80: current_loss=10.79312 | best_loss=10.79312
Epoch 45/80: current_loss=10.76913 | best_loss=10.76913
Epoch 46/80: current_loss=10.76544 | best_loss=10.76544
Epoch 47/80: current_loss=10.74956 | best_loss=10.74956
Epoch 48/80: current_loss=10.73427 | best_loss=10.73427
Epoch 49/80: current_loss=10.72737 | best_loss=10.72737
Epoch 50/80: current_loss=10.73431 | best_loss=10.72737
Epoch 51/80: current_loss=10.72108 | best_loss=10.72108
Epoch 52/80: current_loss=10.72329 | best_loss=10.72108
Epoch 53/80: current_loss=10.70023 | best_loss=10.70023
Epoch 54/80: current_loss=10.67326 | best_loss=10.67326
Epoch 55/80: current_loss=10.67584 | best_loss=10.67326
Epoch 56/80: current_loss=10.65956 | best_loss=10.65956
Epoch 57/80: current_loss=10.64621 | best_loss=10.64621
Epoch 58/80: current_loss=10.64618 | best_loss=10.64618
Epoch 59/80: current_loss=10.65262 | best_loss=10.64618
Epoch 60/80: current_loss=10.63728 | best_loss=10.63728
Epoch 61/80: current_loss=10.62374 | best_loss=10.62374
Epoch 62/80: current_loss=10.60926 | best_loss=10.60926
Epoch 63/80: current_loss=10.60593 | best_loss=10.60593
Epoch 64/80: current_loss=10.61069 | best_loss=10.60593
Epoch 65/80: current_loss=10.60662 | best_loss=10.60593
Epoch 66/80: current_loss=10.57574 | best_loss=10.57574
Epoch 67/80: current_loss=10.57387 | best_loss=10.57387
Epoch 68/80: current_loss=10.58300 | best_loss=10.57387
Epoch 69/80: current_loss=10.59737 | best_loss=10.57387
Epoch 70/80: current_loss=10.54808 | best_loss=10.54808
Epoch 71/80: current_loss=10.55181 | best_loss=10.54808
Epoch 72/80: current_loss=10.54009 | best_loss=10.54009
Epoch 73/80: current_loss=10.53397 | best_loss=10.53397
Epoch 74/80: current_loss=10.53111 | best_loss=10.53111
Epoch 75/80: current_loss=10.54041 | best_loss=10.53111
Epoch 76/80: current_loss=10.53520 | best_loss=10.53111
Epoch 77/80: current_loss=10.54303 | best_loss=10.53111
Epoch 78/80: current_loss=10.52953 | best_loss=10.52953
Epoch 79/80: current_loss=10.53672 | best_loss=10.52953
      explained_var=-0.02434 | mse_loss=10.80403

----------------------------------------------
Params for Trial 44
{'learning_rate': 0.001, 'weight_decay': 0.005043595747734001, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=71.71066 | best_loss=71.71066
Epoch 1/80: current_loss=50.14936 | best_loss=50.14936
Epoch 2/80: current_loss=38.31687 | best_loss=38.31687
Epoch 3/80: current_loss=30.57160 | best_loss=30.57160
Epoch 4/80: current_loss=24.92044 | best_loss=24.92044
Epoch 5/80: current_loss=20.71757 | best_loss=20.71757
Epoch 6/80: current_loss=17.67573 | best_loss=17.67573
Epoch 7/80: current_loss=15.46963 | best_loss=15.46963
Epoch 8/80: current_loss=13.84396 | best_loss=13.84396
Epoch 9/80: current_loss=12.75924 | best_loss=12.75924
Epoch 10/80: current_loss=11.93971 | best_loss=11.93971
Epoch 11/80: current_loss=11.42051 | best_loss=11.42051
Epoch 12/80: current_loss=11.07621 | best_loss=11.07621
Epoch 13/80: current_loss=10.81501 | best_loss=10.81501
Epoch 14/80: current_loss=10.65823 | best_loss=10.65823
Epoch 15/80: current_loss=10.55513 | best_loss=10.55513
Epoch 16/80: current_loss=10.50266 | best_loss=10.50266
Epoch 17/80: current_loss=10.45463 | best_loss=10.45463
Epoch 18/80: current_loss=10.42889 | best_loss=10.42889
Epoch 19/80: current_loss=10.39894 | best_loss=10.39894
Epoch 20/80: current_loss=10.39086 | best_loss=10.39086
Epoch 21/80: current_loss=10.37324 | best_loss=10.37324
Epoch 22/80: current_loss=10.35973 | best_loss=10.35973
Epoch 23/80: current_loss=10.35390 | best_loss=10.35390
Epoch 24/80: current_loss=10.35243 | best_loss=10.35243
Epoch 25/80: current_loss=10.34644 | best_loss=10.34644
Epoch 26/80: current_loss=10.33931 | best_loss=10.33931
Epoch 27/80: current_loss=10.33664 | best_loss=10.33664
Epoch 28/80: current_loss=10.33382 | best_loss=10.33382
Epoch 29/80: current_loss=10.33480 | best_loss=10.33382
Epoch 30/80: current_loss=10.33452 | best_loss=10.33382
Epoch 31/80: current_loss=10.33215 | best_loss=10.33215
Epoch 32/80: current_loss=10.33534 | best_loss=10.33215
Epoch 33/80: current_loss=10.33406 | best_loss=10.33215
Epoch 34/80: current_loss=10.33234 | best_loss=10.33215
Epoch 35/80: current_loss=10.33120 | best_loss=10.33120
Epoch 36/80: current_loss=10.32712 | best_loss=10.32712
Epoch 37/80: current_loss=10.32603 | best_loss=10.32603
Epoch 38/80: current_loss=10.32776 | best_loss=10.32603
Epoch 39/80: current_loss=10.32108 | best_loss=10.32108
Epoch 40/80: current_loss=10.32875 | best_loss=10.32108
Epoch 41/80: current_loss=10.31521 | best_loss=10.31521
Epoch 42/80: current_loss=10.32370 | best_loss=10.31521
Epoch 43/80: current_loss=10.31326 | best_loss=10.31326
Epoch 44/80: current_loss=10.30606 | best_loss=10.30606
Epoch 45/80: current_loss=10.31377 | best_loss=10.30606
Epoch 46/80: current_loss=10.30986 | best_loss=10.30606
Epoch 47/80: current_loss=10.31314 | best_loss=10.30606
Epoch 48/80: current_loss=10.30995 | best_loss=10.30606
Epoch 49/80: current_loss=10.31789 | best_loss=10.30606
Epoch 50/80: current_loss=10.31078 | best_loss=10.30606
Epoch 51/80: current_loss=10.31366 | best_loss=10.30606
Epoch 52/80: current_loss=10.30545 | best_loss=10.30545
Epoch 53/80: current_loss=10.31437 | best_loss=10.30545
Epoch 54/80: current_loss=10.31940 | best_loss=10.30545
Epoch 55/80: current_loss=10.31854 | best_loss=10.30545
Epoch 56/80: current_loss=10.30859 | best_loss=10.30545
Epoch 57/80: current_loss=10.31707 | best_loss=10.30545
Epoch 58/80: current_loss=10.30920 | best_loss=10.30545
Epoch 59/80: current_loss=10.30318 | best_loss=10.30318
Epoch 60/80: current_loss=10.30332 | best_loss=10.30318
Epoch 61/80: current_loss=10.31002 | best_loss=10.30318
Epoch 62/80: current_loss=10.30093 | best_loss=10.30093
Epoch 63/80: current_loss=10.29313 | best_loss=10.29313
Epoch 64/80: current_loss=10.30465 | best_loss=10.29313
Epoch 65/80: current_loss=10.30605 | best_loss=10.29313
Epoch 66/80: current_loss=10.29720 | best_loss=10.29313
Epoch 67/80: current_loss=10.30789 | best_loss=10.29313
Epoch 68/80: current_loss=10.29773 | best_loss=10.29313
Epoch 69/80: current_loss=10.30511 | best_loss=10.29313
Epoch 70/80: current_loss=10.30516 | best_loss=10.29313
Epoch 71/80: current_loss=10.31134 | best_loss=10.29313
Epoch 72/80: current_loss=10.30362 | best_loss=10.29313
Epoch 73/80: current_loss=10.29505 | best_loss=10.29313
Epoch 74/80: current_loss=10.30475 | best_loss=10.29313
Epoch 75/80: current_loss=10.32107 | best_loss=10.29313
Epoch 76/80: current_loss=10.29998 | best_loss=10.29313
Epoch 77/80: current_loss=10.29639 | best_loss=10.29313
Epoch 78/80: current_loss=10.29703 | best_loss=10.29313
Epoch 79/80: current_loss=10.28611 | best_loss=10.28611
      explained_var=-0.00271 | mse_loss=10.52191

----------------------------------------------
Params for Trial 45
{'learning_rate': 0.1, 'weight_decay': 0.008363778414444778, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.29065 | best_loss=18.29065
Epoch 1/80: current_loss=16.22622 | best_loss=16.22622
Epoch 2/80: current_loss=12.40787 | best_loss=12.40787
Epoch 3/80: current_loss=10.78969 | best_loss=10.78969
Epoch 4/80: current_loss=11.62201 | best_loss=10.78969
Epoch 5/80: current_loss=10.24922 | best_loss=10.24922
Epoch 6/80: current_loss=10.98346 | best_loss=10.24922
Epoch 7/80: current_loss=10.65664 | best_loss=10.24922
Epoch 8/80: current_loss=10.69081 | best_loss=10.24922
Epoch 9/80: current_loss=10.32531 | best_loss=10.24922
Epoch 10/80: current_loss=10.35388 | best_loss=10.24922
Epoch 11/80: current_loss=10.83073 | best_loss=10.24922
Epoch 12/80: current_loss=11.17833 | best_loss=10.24922
Epoch 13/80: current_loss=10.32108 | best_loss=10.24922
Epoch 14/80: current_loss=10.39110 | best_loss=10.24922
Epoch 15/80: current_loss=10.91342 | best_loss=10.24922
Epoch 16/80: current_loss=10.28101 | best_loss=10.24922
Epoch 17/80: current_loss=10.87026 | best_loss=10.24922
Epoch 18/80: current_loss=11.52648 | best_loss=10.24922
Epoch 19/80: current_loss=13.06837 | best_loss=10.24922
Epoch 20/80: current_loss=11.15393 | best_loss=10.24922
Epoch 21/80: current_loss=10.48550 | best_loss=10.24922
Epoch 22/80: current_loss=13.57377 | best_loss=10.24922
Epoch 23/80: current_loss=13.12113 | best_loss=10.24922
Epoch 24/80: current_loss=10.97741 | best_loss=10.24922
Epoch 25/80: current_loss=10.36833 | best_loss=10.24922
Early Stopping at epoch 25
      explained_var=0.00070 | mse_loss=10.46137
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=58.96265 | best_loss=58.96265
Epoch 1/80: current_loss=23.70718 | best_loss=23.70718
Epoch 2/80: current_loss=16.63520 | best_loss=16.63520
Epoch 3/80: current_loss=13.01150 | best_loss=13.01150
Epoch 4/80: current_loss=11.16994 | best_loss=11.16994
Epoch 5/80: current_loss=12.44539 | best_loss=11.16994
Epoch 6/80: current_loss=11.63428 | best_loss=11.16994
Epoch 7/80: current_loss=10.97398 | best_loss=10.97398
Epoch 8/80: current_loss=11.59768 | best_loss=10.97398
Epoch 9/80: current_loss=13.90863 | best_loss=10.97398
Epoch 10/80: current_loss=11.21636 | best_loss=10.97398
Epoch 11/80: current_loss=11.42281 | best_loss=10.97398
Epoch 12/80: current_loss=10.83330 | best_loss=10.83330
Epoch 13/80: current_loss=10.61307 | best_loss=10.61307
Epoch 14/80: current_loss=12.26032 | best_loss=10.61307
Epoch 15/80: current_loss=11.16279 | best_loss=10.61307
Epoch 16/80: current_loss=12.19647 | best_loss=10.61307
Epoch 17/80: current_loss=10.49893 | best_loss=10.49893
Epoch 18/80: current_loss=11.23219 | best_loss=10.49893
Epoch 19/80: current_loss=12.04017 | best_loss=10.49893
Epoch 20/80: current_loss=13.19745 | best_loss=10.49893
Epoch 21/80: current_loss=12.57310 | best_loss=10.49893
Epoch 22/80: current_loss=10.59573 | best_loss=10.49893
Epoch 23/80: current_loss=15.36711 | best_loss=10.49893
Epoch 24/80: current_loss=15.25933 | best_loss=10.49893
Epoch 25/80: current_loss=13.00927 | best_loss=10.49893
Epoch 26/80: current_loss=15.44953 | best_loss=10.49893
Epoch 27/80: current_loss=12.95360 | best_loss=10.49893
Epoch 28/80: current_loss=10.50730 | best_loss=10.49893
Epoch 29/80: current_loss=12.32607 | best_loss=10.49893
Epoch 30/80: current_loss=11.45118 | best_loss=10.49893
Epoch 31/80: current_loss=13.15468 | best_loss=10.49893
Epoch 32/80: current_loss=10.39105 | best_loss=10.39105
Epoch 33/80: current_loss=11.36854 | best_loss=10.39105
Epoch 34/80: current_loss=11.45293 | best_loss=10.39105
Epoch 35/80: current_loss=13.01751 | best_loss=10.39105
Epoch 36/80: current_loss=11.68249 | best_loss=10.39105
Epoch 37/80: current_loss=11.43444 | best_loss=10.39105
Epoch 38/80: current_loss=10.80441 | best_loss=10.39105
Epoch 39/80: current_loss=10.54020 | best_loss=10.39105
Epoch 40/80: current_loss=10.41698 | best_loss=10.39105
Epoch 41/80: current_loss=10.94041 | best_loss=10.39105
Epoch 42/80: current_loss=10.63742 | best_loss=10.39105
Epoch 43/80: current_loss=13.77136 | best_loss=10.39105
Epoch 44/80: current_loss=11.12353 | best_loss=10.39105
Epoch 45/80: current_loss=14.72649 | best_loss=10.39105
Epoch 46/80: current_loss=10.81469 | best_loss=10.39105
Epoch 47/80: current_loss=11.92029 | best_loss=10.39105
Epoch 48/80: current_loss=14.75433 | best_loss=10.39105
Epoch 49/80: current_loss=10.41629 | best_loss=10.39105
Epoch 50/80: current_loss=16.23516 | best_loss=10.39105
Epoch 51/80: current_loss=16.24107 | best_loss=10.39105
Epoch 52/80: current_loss=10.63880 | best_loss=10.39105
Early Stopping at epoch 52
      explained_var=-0.00481 | mse_loss=10.11194

----------------------------------------------
Params for Trial 46
{'learning_rate': 0.001, 'weight_decay': 0.006851695288974743, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.93893 | best_loss=18.93893
Epoch 1/80: current_loss=12.02628 | best_loss=12.02628
Epoch 2/80: current_loss=11.45758 | best_loss=11.45758
Epoch 3/80: current_loss=11.18473 | best_loss=11.18473
Epoch 4/80: current_loss=10.91093 | best_loss=10.91093
Epoch 5/80: current_loss=10.72846 | best_loss=10.72846
Epoch 6/80: current_loss=10.60736 | best_loss=10.60736
Epoch 7/80: current_loss=10.54316 | best_loss=10.54316
Epoch 8/80: current_loss=10.48269 | best_loss=10.48269
Epoch 9/80: current_loss=10.48296 | best_loss=10.48269
Epoch 10/80: current_loss=10.42026 | best_loss=10.42026
Epoch 11/80: current_loss=10.43476 | best_loss=10.42026
Epoch 12/80: current_loss=10.38801 | best_loss=10.38801
Epoch 13/80: current_loss=10.40934 | best_loss=10.38801
Epoch 14/80: current_loss=10.37298 | best_loss=10.37298
Epoch 15/80: current_loss=10.36744 | best_loss=10.36744
Epoch 16/80: current_loss=10.34584 | best_loss=10.34584
Epoch 17/80: current_loss=10.33604 | best_loss=10.33604
Epoch 18/80: current_loss=10.38537 | best_loss=10.33604
Epoch 19/80: current_loss=10.37716 | best_loss=10.33604
Epoch 20/80: current_loss=10.32905 | best_loss=10.32905
Epoch 21/80: current_loss=10.33938 | best_loss=10.32905
Epoch 22/80: current_loss=10.37827 | best_loss=10.32905
Epoch 23/80: current_loss=10.33861 | best_loss=10.32905
Epoch 24/80: current_loss=10.33827 | best_loss=10.32905
Epoch 25/80: current_loss=10.32554 | best_loss=10.32554
Epoch 26/80: current_loss=10.36715 | best_loss=10.32554
Epoch 27/80: current_loss=10.32145 | best_loss=10.32145
Epoch 28/80: current_loss=10.35534 | best_loss=10.32145
Epoch 29/80: current_loss=10.30832 | best_loss=10.30832
Epoch 30/80: current_loss=10.34369 | best_loss=10.30832
Epoch 31/80: current_loss=10.33255 | best_loss=10.30832
Epoch 32/80: current_loss=10.35012 | best_loss=10.30832
Epoch 33/80: current_loss=10.31262 | best_loss=10.30832
Epoch 34/80: current_loss=10.35733 | best_loss=10.30832
Epoch 35/80: current_loss=10.32429 | best_loss=10.30832
Epoch 36/80: current_loss=10.37754 | best_loss=10.30832
Epoch 37/80: current_loss=10.30227 | best_loss=10.30227
Epoch 38/80: current_loss=10.31324 | best_loss=10.30227
Epoch 39/80: current_loss=10.31655 | best_loss=10.30227
Epoch 40/80: current_loss=10.32141 | best_loss=10.30227
Epoch 41/80: current_loss=10.28851 | best_loss=10.28851
Epoch 42/80: current_loss=10.30798 | best_loss=10.28851
Epoch 43/80: current_loss=10.32485 | best_loss=10.28851
Epoch 44/80: current_loss=10.29437 | best_loss=10.28851
Epoch 45/80: current_loss=10.31401 | best_loss=10.28851
Epoch 46/80: current_loss=10.28110 | best_loss=10.28110
Epoch 47/80: current_loss=10.35802 | best_loss=10.28110
Epoch 48/80: current_loss=10.29378 | best_loss=10.28110
Epoch 49/80: current_loss=10.30344 | best_loss=10.28110
Epoch 50/80: current_loss=10.29404 | best_loss=10.28110
Epoch 51/80: current_loss=10.28552 | best_loss=10.28110
Epoch 52/80: current_loss=10.28898 | best_loss=10.28110
Epoch 53/80: current_loss=10.30951 | best_loss=10.28110
Epoch 54/80: current_loss=10.31292 | best_loss=10.28110
Epoch 55/80: current_loss=10.33619 | best_loss=10.28110
Epoch 56/80: current_loss=10.28958 | best_loss=10.28110
Epoch 57/80: current_loss=10.35162 | best_loss=10.28110
Epoch 58/80: current_loss=10.28595 | best_loss=10.28110
Epoch 59/80: current_loss=10.30986 | best_loss=10.28110
Epoch 60/80: current_loss=10.28238 | best_loss=10.28110
Epoch 61/80: current_loss=10.32439 | best_loss=10.28110
Epoch 62/80: current_loss=10.28423 | best_loss=10.28110
Epoch 63/80: current_loss=10.31322 | best_loss=10.28110
Epoch 64/80: current_loss=10.35671 | best_loss=10.28110
Epoch 65/80: current_loss=10.33588 | best_loss=10.28110
Epoch 66/80: current_loss=10.29889 | best_loss=10.28110
Early Stopping at epoch 66
      explained_var=-0.00403 | mse_loss=10.50585
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.43744 | best_loss=10.43744
Epoch 1/80: current_loss=10.41772 | best_loss=10.41772
Epoch 2/80: current_loss=10.41603 | best_loss=10.41603
Epoch 3/80: current_loss=10.39165 | best_loss=10.39165
Epoch 4/80: current_loss=10.43221 | best_loss=10.39165
Epoch 5/80: current_loss=10.31127 | best_loss=10.31127
Epoch 6/80: current_loss=10.39774 | best_loss=10.31127
Epoch 7/80: current_loss=10.41294 | best_loss=10.31127
Epoch 8/80: current_loss=10.38951 | best_loss=10.31127
Epoch 9/80: current_loss=10.39095 | best_loss=10.31127
Epoch 10/80: current_loss=10.36466 | best_loss=10.31127
Epoch 11/80: current_loss=10.38123 | best_loss=10.31127
Epoch 12/80: current_loss=10.37304 | best_loss=10.31127
Epoch 13/80: current_loss=10.35928 | best_loss=10.31127
Epoch 14/80: current_loss=10.29363 | best_loss=10.29363
Epoch 15/80: current_loss=10.34551 | best_loss=10.29363
Epoch 16/80: current_loss=10.52117 | best_loss=10.29363
Epoch 17/80: current_loss=10.37008 | best_loss=10.29363
Epoch 18/80: current_loss=10.30899 | best_loss=10.29363
Epoch 19/80: current_loss=10.28671 | best_loss=10.28671
Epoch 20/80: current_loss=10.25982 | best_loss=10.25982
Epoch 21/80: current_loss=10.26423 | best_loss=10.25982
Epoch 22/80: current_loss=10.27726 | best_loss=10.25982
Epoch 23/80: current_loss=10.28599 | best_loss=10.25982
Epoch 24/80: current_loss=10.23417 | best_loss=10.23417
Epoch 25/80: current_loss=10.23810 | best_loss=10.23417
Epoch 26/80: current_loss=10.38309 | best_loss=10.23417
Epoch 27/80: current_loss=10.27418 | best_loss=10.23417
Epoch 28/80: current_loss=10.37966 | best_loss=10.23417
Epoch 29/80: current_loss=10.27612 | best_loss=10.23417
Epoch 30/80: current_loss=10.27799 | best_loss=10.23417
Epoch 31/80: current_loss=10.31908 | best_loss=10.23417
Epoch 32/80: current_loss=10.32090 | best_loss=10.23417
Epoch 33/80: current_loss=10.23438 | best_loss=10.23417
Epoch 34/80: current_loss=10.32535 | best_loss=10.23417
Epoch 35/80: current_loss=10.47634 | best_loss=10.23417
Epoch 36/80: current_loss=10.36955 | best_loss=10.23417
Epoch 37/80: current_loss=10.43772 | best_loss=10.23417
Epoch 38/80: current_loss=10.28633 | best_loss=10.23417
Epoch 39/80: current_loss=10.21569 | best_loss=10.21569
Epoch 40/80: current_loss=10.27654 | best_loss=10.21569
Epoch 41/80: current_loss=10.36131 | best_loss=10.21569
Epoch 42/80: current_loss=10.30225 | best_loss=10.21569
Epoch 43/80: current_loss=10.24451 | best_loss=10.21569
Epoch 44/80: current_loss=10.23538 | best_loss=10.21569
Epoch 45/80: current_loss=10.22302 | best_loss=10.21569
Epoch 46/80: current_loss=10.33361 | best_loss=10.21569
Epoch 47/80: current_loss=10.33302 | best_loss=10.21569
Epoch 48/80: current_loss=10.20946 | best_loss=10.20946
Epoch 49/80: current_loss=10.22692 | best_loss=10.20946
Epoch 50/80: current_loss=10.30312 | best_loss=10.20946
Epoch 51/80: current_loss=10.27478 | best_loss=10.20946
Epoch 52/80: current_loss=10.34653 | best_loss=10.20946
Epoch 53/80: current_loss=10.27035 | best_loss=10.20946
Epoch 54/80: current_loss=10.22986 | best_loss=10.20946
Epoch 55/80: current_loss=10.20492 | best_loss=10.20492
Epoch 56/80: current_loss=10.28393 | best_loss=10.20492
Epoch 57/80: current_loss=10.32344 | best_loss=10.20492
Epoch 58/80: current_loss=10.24850 | best_loss=10.20492
Epoch 59/80: current_loss=10.33158 | best_loss=10.20492
Epoch 60/80: current_loss=10.23915 | best_loss=10.20492
Epoch 61/80: current_loss=10.21971 | best_loss=10.20492
Epoch 62/80: current_loss=10.19731 | best_loss=10.19731
Epoch 63/80: current_loss=10.20969 | best_loss=10.19731
Epoch 64/80: current_loss=10.23922 | best_loss=10.19731
Epoch 65/80: current_loss=10.20995 | best_loss=10.19731
Epoch 66/80: current_loss=10.23284 | best_loss=10.19731
Epoch 67/80: current_loss=10.34674 | best_loss=10.19731
Epoch 68/80: current_loss=10.21141 | best_loss=10.19731
Epoch 69/80: current_loss=10.25742 | best_loss=10.19731
Epoch 70/80: current_loss=10.25089 | best_loss=10.19731
Epoch 71/80: current_loss=10.26519 | best_loss=10.19731
Epoch 72/80: current_loss=10.24884 | best_loss=10.19731
Epoch 73/80: current_loss=10.22798 | best_loss=10.19731
Epoch 74/80: current_loss=10.19334 | best_loss=10.19334
Epoch 75/80: current_loss=10.22229 | best_loss=10.19334
Epoch 76/80: current_loss=10.20787 | best_loss=10.19334
Epoch 77/80: current_loss=10.23879 | best_loss=10.19334
Epoch 78/80: current_loss=10.27786 | best_loss=10.19334
Epoch 79/80: current_loss=10.24295 | best_loss=10.19334
      explained_var=0.02305 | mse_loss=9.83461
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.56299 | best_loss=7.56299
Epoch 1/80: current_loss=7.75646 | best_loss=7.56299
Epoch 2/80: current_loss=7.49387 | best_loss=7.49387
Epoch 3/80: current_loss=7.58269 | best_loss=7.49387
Epoch 4/80: current_loss=7.74244 | best_loss=7.49387
Epoch 5/80: current_loss=7.48899 | best_loss=7.48899
Epoch 6/80: current_loss=7.56774 | best_loss=7.48899
Epoch 7/80: current_loss=7.73902 | best_loss=7.48899
Epoch 8/80: current_loss=7.58846 | best_loss=7.48899
Epoch 9/80: current_loss=7.65129 | best_loss=7.48899
Epoch 10/80: current_loss=7.60585 | best_loss=7.48899
Epoch 11/80: current_loss=7.54020 | best_loss=7.48899
Epoch 12/80: current_loss=7.72516 | best_loss=7.48899
Epoch 13/80: current_loss=7.49014 | best_loss=7.48899
Epoch 14/80: current_loss=7.69810 | best_loss=7.48899
Epoch 15/80: current_loss=7.55351 | best_loss=7.48899
Epoch 16/80: current_loss=7.51638 | best_loss=7.48899
Epoch 17/80: current_loss=7.52168 | best_loss=7.48899
Epoch 18/80: current_loss=7.72117 | best_loss=7.48899
Epoch 19/80: current_loss=7.59695 | best_loss=7.48899
Epoch 20/80: current_loss=7.78559 | best_loss=7.48899
Epoch 21/80: current_loss=7.57094 | best_loss=7.48899
Epoch 22/80: current_loss=7.57270 | best_loss=7.48899
Epoch 23/80: current_loss=7.76737 | best_loss=7.48899
Epoch 24/80: current_loss=7.52459 | best_loss=7.48899
Epoch 25/80: current_loss=7.64240 | best_loss=7.48899
Early Stopping at epoch 25
      explained_var=-0.00368 | mse_loss=7.61171

----------------------------------------------
Params for Trial 47
{'learning_rate': 0.001, 'weight_decay': 0.009444364504689622, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=73.57955 | best_loss=73.57955
Epoch 1/80: current_loss=55.30179 | best_loss=55.30179
Epoch 2/80: current_loss=43.93531 | best_loss=43.93531
Epoch 3/80: current_loss=35.53035 | best_loss=35.53035
Epoch 4/80: current_loss=29.12896 | best_loss=29.12896
Epoch 5/80: current_loss=24.19897 | best_loss=24.19897
Epoch 6/80: current_loss=20.35768 | best_loss=20.35768
Epoch 7/80: current_loss=17.48453 | best_loss=17.48453
Epoch 8/80: current_loss=15.38898 | best_loss=15.38898
Epoch 9/80: current_loss=13.84061 | best_loss=13.84061
Epoch 10/80: current_loss=12.72592 | best_loss=12.72592
Epoch 11/80: current_loss=11.94981 | best_loss=11.94981
Epoch 12/80: current_loss=11.40598 | best_loss=11.40598
Epoch 13/80: current_loss=11.01726 | best_loss=11.01726
Epoch 14/80: current_loss=10.75077 | best_loss=10.75077
Epoch 15/80: current_loss=10.59519 | best_loss=10.59519
Epoch 16/80: current_loss=10.50076 | best_loss=10.50076
Epoch 17/80: current_loss=10.41572 | best_loss=10.41572
Epoch 18/80: current_loss=10.36811 | best_loss=10.36811
Epoch 19/80: current_loss=10.34267 | best_loss=10.34267
Epoch 20/80: current_loss=10.32589 | best_loss=10.32589
Epoch 21/80: current_loss=10.31110 | best_loss=10.31110
Epoch 22/80: current_loss=10.30959 | best_loss=10.30959
Epoch 23/80: current_loss=10.29788 | best_loss=10.29788
Epoch 24/80: current_loss=10.29348 | best_loss=10.29348
Epoch 25/80: current_loss=10.29607 | best_loss=10.29348
Epoch 26/80: current_loss=10.28474 | best_loss=10.28474
Epoch 27/80: current_loss=10.28402 | best_loss=10.28402
Epoch 28/80: current_loss=10.28158 | best_loss=10.28158
Epoch 29/80: current_loss=10.28093 | best_loss=10.28093
Epoch 30/80: current_loss=10.28462 | best_loss=10.28093
Epoch 31/80: current_loss=10.27832 | best_loss=10.27832
Epoch 32/80: current_loss=10.28391 | best_loss=10.27832
Epoch 33/80: current_loss=10.28456 | best_loss=10.27832
Epoch 34/80: current_loss=10.28782 | best_loss=10.27832
Epoch 35/80: current_loss=10.28007 | best_loss=10.27832
Epoch 36/80: current_loss=10.28744 | best_loss=10.27832
Epoch 37/80: current_loss=10.29462 | best_loss=10.27832
Epoch 38/80: current_loss=10.29776 | best_loss=10.27832
Epoch 39/80: current_loss=10.29662 | best_loss=10.27832
Epoch 40/80: current_loss=10.28844 | best_loss=10.27832
Epoch 41/80: current_loss=10.28254 | best_loss=10.27832
Epoch 42/80: current_loss=10.28249 | best_loss=10.27832
Epoch 43/80: current_loss=10.28435 | best_loss=10.27832
Epoch 44/80: current_loss=10.28076 | best_loss=10.27832
Epoch 45/80: current_loss=10.28565 | best_loss=10.27832
Epoch 46/80: current_loss=10.28329 | best_loss=10.27832
Epoch 47/80: current_loss=10.28578 | best_loss=10.27832
Epoch 48/80: current_loss=10.28365 | best_loss=10.27832
Epoch 49/80: current_loss=10.28720 | best_loss=10.27832
Epoch 50/80: current_loss=10.28280 | best_loss=10.27832
Epoch 51/80: current_loss=10.28420 | best_loss=10.27832
Early Stopping at epoch 51
      explained_var=-0.00179 | mse_loss=10.51303
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.42030 | best_loss=10.42030
Epoch 1/80: current_loss=10.41352 | best_loss=10.41352
Epoch 2/80: current_loss=10.42677 | best_loss=10.41352
Epoch 3/80: current_loss=10.42621 | best_loss=10.41352
Epoch 4/80: current_loss=10.41138 | best_loss=10.41138
Epoch 5/80: current_loss=10.41555 | best_loss=10.41138
Epoch 6/80: current_loss=10.39307 | best_loss=10.39307
Epoch 7/80: current_loss=10.39647 | best_loss=10.39307
Epoch 8/80: current_loss=10.37979 | best_loss=10.37979
Epoch 9/80: current_loss=10.36150 | best_loss=10.36150
Epoch 10/80: current_loss=10.31963 | best_loss=10.31963
Epoch 11/80: current_loss=10.22921 | best_loss=10.22921
Epoch 12/80: current_loss=10.25860 | best_loss=10.22921
Epoch 13/80: current_loss=10.23834 | best_loss=10.22921
Epoch 14/80: current_loss=10.26710 | best_loss=10.22921
Epoch 15/80: current_loss=10.21126 | best_loss=10.21126
Epoch 16/80: current_loss=10.28045 | best_loss=10.21126
Epoch 17/80: current_loss=10.20874 | best_loss=10.20874
Epoch 18/80: current_loss=10.25886 | best_loss=10.20874
Epoch 19/80: current_loss=10.21352 | best_loss=10.20874
Epoch 20/80: current_loss=10.19112 | best_loss=10.19112
Epoch 21/80: current_loss=10.21255 | best_loss=10.19112
Epoch 22/80: current_loss=10.20485 | best_loss=10.19112
Epoch 23/80: current_loss=10.15994 | best_loss=10.15994
Epoch 24/80: current_loss=10.21707 | best_loss=10.15994
Epoch 25/80: current_loss=10.15063 | best_loss=10.15063
Epoch 26/80: current_loss=10.17713 | best_loss=10.15063
Epoch 27/80: current_loss=10.17500 | best_loss=10.15063
Epoch 28/80: current_loss=10.21966 | best_loss=10.15063
Epoch 29/80: current_loss=10.19068 | best_loss=10.15063
Epoch 30/80: current_loss=10.18440 | best_loss=10.15063
Epoch 31/80: current_loss=10.18315 | best_loss=10.15063
Epoch 32/80: current_loss=10.24004 | best_loss=10.15063
Epoch 33/80: current_loss=10.15866 | best_loss=10.15063
Epoch 34/80: current_loss=10.29156 | best_loss=10.15063
Epoch 35/80: current_loss=10.25418 | best_loss=10.15063
Epoch 36/80: current_loss=10.15493 | best_loss=10.15063
Epoch 37/80: current_loss=10.26000 | best_loss=10.15063
Epoch 38/80: current_loss=10.21366 | best_loss=10.15063
Epoch 39/80: current_loss=10.19169 | best_loss=10.15063
Epoch 40/80: current_loss=10.15856 | best_loss=10.15063
Epoch 41/80: current_loss=10.17535 | best_loss=10.15063
Epoch 42/80: current_loss=10.19947 | best_loss=10.15063
Epoch 43/80: current_loss=10.25738 | best_loss=10.15063
Epoch 44/80: current_loss=10.22663 | best_loss=10.15063
Epoch 45/80: current_loss=10.17979 | best_loss=10.15063
Early Stopping at epoch 45
      explained_var=0.02789 | mse_loss=9.79064
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.54365 | best_loss=7.54365
Epoch 1/80: current_loss=7.55037 | best_loss=7.54365
Epoch 2/80: current_loss=7.58791 | best_loss=7.54365
Epoch 3/80: current_loss=7.60059 | best_loss=7.54365
Epoch 4/80: current_loss=7.71105 | best_loss=7.54365
Epoch 5/80: current_loss=7.52005 | best_loss=7.52005
Epoch 6/80: current_loss=7.55977 | best_loss=7.52005
Epoch 7/80: current_loss=7.56525 | best_loss=7.52005
Epoch 8/80: current_loss=7.55986 | best_loss=7.52005
Epoch 9/80: current_loss=7.58289 | best_loss=7.52005
Epoch 10/80: current_loss=7.58658 | best_loss=7.52005
Epoch 11/80: current_loss=7.62009 | best_loss=7.52005
Epoch 12/80: current_loss=7.60414 | best_loss=7.52005
Epoch 13/80: current_loss=7.62176 | best_loss=7.52005
Epoch 14/80: current_loss=7.63183 | best_loss=7.52005
Epoch 15/80: current_loss=7.68923 | best_loss=7.52005
Epoch 16/80: current_loss=7.60092 | best_loss=7.52005
Epoch 17/80: current_loss=7.61927 | best_loss=7.52005
Epoch 18/80: current_loss=7.55893 | best_loss=7.52005
Epoch 19/80: current_loss=7.55435 | best_loss=7.52005
Epoch 20/80: current_loss=7.57605 | best_loss=7.52005
Epoch 21/80: current_loss=7.65697 | best_loss=7.52005
Epoch 22/80: current_loss=7.59800 | best_loss=7.52005
Epoch 23/80: current_loss=7.57205 | best_loss=7.52005
Epoch 24/80: current_loss=7.60911 | best_loss=7.52005
Epoch 25/80: current_loss=7.63157 | best_loss=7.52005
Early Stopping at epoch 25
      explained_var=-0.00332 | mse_loss=7.62638

----------------------------------------------
Params for Trial 48
{'learning_rate': 0.01, 'weight_decay': 0.006212011274273111, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=13.40617 | best_loss=13.40617
Epoch 1/80: current_loss=11.98137 | best_loss=11.98137
Epoch 2/80: current_loss=11.17276 | best_loss=11.17276
Epoch 3/80: current_loss=11.04180 | best_loss=11.04180
Epoch 4/80: current_loss=10.84925 | best_loss=10.84925
Epoch 5/80: current_loss=10.98351 | best_loss=10.84925
Epoch 6/80: current_loss=11.12754 | best_loss=10.84925
Epoch 7/80: current_loss=10.77615 | best_loss=10.77615
Epoch 8/80: current_loss=10.47415 | best_loss=10.47415
Epoch 9/80: current_loss=10.75536 | best_loss=10.47415
Epoch 10/80: current_loss=11.34874 | best_loss=10.47415
Epoch 11/80: current_loss=10.40741 | best_loss=10.40741
Epoch 12/80: current_loss=10.41232 | best_loss=10.40741
Epoch 13/80: current_loss=10.52107 | best_loss=10.40741
Epoch 14/80: current_loss=10.62285 | best_loss=10.40741
Epoch 15/80: current_loss=10.39362 | best_loss=10.39362
Epoch 16/80: current_loss=10.50339 | best_loss=10.39362
Epoch 17/80: current_loss=10.39508 | best_loss=10.39362
Epoch 18/80: current_loss=10.46275 | best_loss=10.39362
Epoch 19/80: current_loss=10.44867 | best_loss=10.39362
Epoch 20/80: current_loss=10.79811 | best_loss=10.39362
Epoch 21/80: current_loss=10.35426 | best_loss=10.35426
Epoch 22/80: current_loss=10.32837 | best_loss=10.32837
Epoch 23/80: current_loss=10.49488 | best_loss=10.32837
Epoch 24/80: current_loss=10.32429 | best_loss=10.32429
Epoch 25/80: current_loss=10.36338 | best_loss=10.32429
Epoch 26/80: current_loss=10.41057 | best_loss=10.32429
Epoch 27/80: current_loss=10.33985 | best_loss=10.32429
Epoch 28/80: current_loss=10.96731 | best_loss=10.32429
Epoch 29/80: current_loss=10.33223 | best_loss=10.32429
Epoch 30/80: current_loss=11.17169 | best_loss=10.32429
Epoch 31/80: current_loss=10.38142 | best_loss=10.32429
Epoch 32/80: current_loss=10.33762 | best_loss=10.32429
Epoch 33/80: current_loss=10.33978 | best_loss=10.32429
Epoch 34/80: current_loss=10.38843 | best_loss=10.32429
Epoch 35/80: current_loss=11.52564 | best_loss=10.32429
Epoch 36/80: current_loss=11.09184 | best_loss=10.32429
Epoch 37/80: current_loss=10.57318 | best_loss=10.32429
Epoch 38/80: current_loss=10.44415 | best_loss=10.32429
Epoch 39/80: current_loss=12.12176 | best_loss=10.32429
Epoch 40/80: current_loss=10.68346 | best_loss=10.32429
Epoch 41/80: current_loss=10.51319 | best_loss=10.32429
Epoch 42/80: current_loss=10.37651 | best_loss=10.32429
Epoch 43/80: current_loss=10.55060 | best_loss=10.32429
Epoch 44/80: current_loss=10.76721 | best_loss=10.32429
Early Stopping at epoch 44
      explained_var=-0.00855 | mse_loss=10.55620

----------------------------------------------
Params for Trial 49
{'learning_rate': 0.001, 'weight_decay': 0.0077729932235164655, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=36.87297 | best_loss=36.87297
Epoch 1/80: current_loss=22.54583 | best_loss=22.54583
Epoch 2/80: current_loss=15.42725 | best_loss=15.42725
Epoch 3/80: current_loss=12.26247 | best_loss=12.26247
Epoch 4/80: current_loss=10.96325 | best_loss=10.96325
Epoch 5/80: current_loss=10.53723 | best_loss=10.53723
Epoch 6/80: current_loss=10.39781 | best_loss=10.39781
Epoch 7/80: current_loss=10.34848 | best_loss=10.34848
Epoch 8/80: current_loss=10.32778 | best_loss=10.32778
Epoch 9/80: current_loss=10.31879 | best_loss=10.31879
Epoch 10/80: current_loss=10.31955 | best_loss=10.31879
Epoch 11/80: current_loss=10.30776 | best_loss=10.30776
Epoch 12/80: current_loss=10.30588 | best_loss=10.30588
Epoch 13/80: current_loss=10.30751 | best_loss=10.30588
Epoch 14/80: current_loss=10.30317 | best_loss=10.30317
Epoch 15/80: current_loss=10.29450 | best_loss=10.29450
Epoch 16/80: current_loss=10.29728 | best_loss=10.29450
Epoch 17/80: current_loss=10.29273 | best_loss=10.29273
Epoch 18/80: current_loss=10.29398 | best_loss=10.29273
Epoch 19/80: current_loss=10.31050 | best_loss=10.29273
Epoch 20/80: current_loss=10.30557 | best_loss=10.29273
Epoch 21/80: current_loss=10.30555 | best_loss=10.29273
Epoch 22/80: current_loss=10.30673 | best_loss=10.29273
Epoch 23/80: current_loss=10.29008 | best_loss=10.29008
Epoch 24/80: current_loss=10.29129 | best_loss=10.29008
Epoch 25/80: current_loss=10.31422 | best_loss=10.29008
Epoch 26/80: current_loss=10.30340 | best_loss=10.29008
Epoch 27/80: current_loss=10.27794 | best_loss=10.27794
Epoch 28/80: current_loss=10.30511 | best_loss=10.27794
Epoch 29/80: current_loss=10.28754 | best_loss=10.27794
Epoch 30/80: current_loss=10.29594 | best_loss=10.27794
Epoch 31/80: current_loss=10.28877 | best_loss=10.27794
Epoch 32/80: current_loss=10.29085 | best_loss=10.27794
Epoch 33/80: current_loss=10.30155 | best_loss=10.27794
Epoch 34/80: current_loss=10.28238 | best_loss=10.27794
Epoch 35/80: current_loss=10.29294 | best_loss=10.27794
Epoch 36/80: current_loss=10.28102 | best_loss=10.27794
Epoch 37/80: current_loss=10.29728 | best_loss=10.27794
Epoch 38/80: current_loss=10.29730 | best_loss=10.27794
Epoch 39/80: current_loss=10.29322 | best_loss=10.27794
Epoch 40/80: current_loss=10.32043 | best_loss=10.27794
Epoch 41/80: current_loss=10.28836 | best_loss=10.27794
Epoch 42/80: current_loss=10.29256 | best_loss=10.27794
Epoch 43/80: current_loss=10.29093 | best_loss=10.27794
Epoch 44/80: current_loss=10.31613 | best_loss=10.27794
Epoch 45/80: current_loss=10.28828 | best_loss=10.27794
Epoch 46/80: current_loss=10.30515 | best_loss=10.27794
Epoch 47/80: current_loss=10.27662 | best_loss=10.27662
Epoch 48/80: current_loss=10.30488 | best_loss=10.27662
Epoch 49/80: current_loss=10.29696 | best_loss=10.27662
Epoch 50/80: current_loss=10.29836 | best_loss=10.27662
Epoch 51/80: current_loss=10.28310 | best_loss=10.27662
Epoch 52/80: current_loss=10.29570 | best_loss=10.27662
Epoch 53/80: current_loss=10.31256 | best_loss=10.27662
Epoch 54/80: current_loss=10.28869 | best_loss=10.27662
Epoch 55/80: current_loss=10.27861 | best_loss=10.27662
Epoch 56/80: current_loss=10.29271 | best_loss=10.27662
Epoch 57/80: current_loss=10.28829 | best_loss=10.27662
Epoch 58/80: current_loss=10.27669 | best_loss=10.27662
Epoch 59/80: current_loss=10.28693 | best_loss=10.27662
Epoch 60/80: current_loss=10.31124 | best_loss=10.27662
Epoch 61/80: current_loss=10.27171 | best_loss=10.27171
Epoch 62/80: current_loss=10.28257 | best_loss=10.27171
Epoch 63/80: current_loss=10.28458 | best_loss=10.27171
Epoch 64/80: current_loss=10.27133 | best_loss=10.27133
Epoch 65/80: current_loss=10.26873 | best_loss=10.26873
Epoch 66/80: current_loss=10.28519 | best_loss=10.26873
Epoch 67/80: current_loss=10.26394 | best_loss=10.26394
Epoch 68/80: current_loss=10.28749 | best_loss=10.26394
Epoch 69/80: current_loss=10.27019 | best_loss=10.26394
Epoch 70/80: current_loss=10.27683 | best_loss=10.26394
Epoch 71/80: current_loss=10.25912 | best_loss=10.25912
Epoch 72/80: current_loss=10.25606 | best_loss=10.25606
Epoch 73/80: current_loss=10.19180 | best_loss=10.19180
Epoch 74/80: current_loss=10.21386 | best_loss=10.19180
Epoch 75/80: current_loss=10.14061 | best_loss=10.14061
Epoch 76/80: current_loss=10.16673 | best_loss=10.14061
Epoch 77/80: current_loss=10.22739 | best_loss=10.14061
Epoch 78/80: current_loss=10.27396 | best_loss=10.14061
Epoch 79/80: current_loss=10.27051 | best_loss=10.14061
      explained_var=0.02623 | mse_loss=10.44443
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.32692 | best_loss=10.32692
Epoch 1/80: current_loss=10.18244 | best_loss=10.18244
Epoch 2/80: current_loss=10.06112 | best_loss=10.06112
Epoch 3/80: current_loss=10.27057 | best_loss=10.06112
Epoch 4/80: current_loss=10.27080 | best_loss=10.06112
Epoch 5/80: current_loss=10.19811 | best_loss=10.06112
Epoch 6/80: current_loss=10.12639 | best_loss=10.06112
Epoch 7/80: current_loss=10.26007 | best_loss=10.06112
Epoch 8/80: current_loss=10.21409 | best_loss=10.06112
Epoch 9/80: current_loss=10.23840 | best_loss=10.06112
Epoch 10/80: current_loss=10.29493 | best_loss=10.06112
Epoch 11/80: current_loss=10.27328 | best_loss=10.06112
Epoch 12/80: current_loss=10.22989 | best_loss=10.06112
Epoch 13/80: current_loss=10.20357 | best_loss=10.06112
Epoch 14/80: current_loss=10.24717 | best_loss=10.06112
Epoch 15/80: current_loss=10.16308 | best_loss=10.06112
Epoch 16/80: current_loss=10.29041 | best_loss=10.06112
Epoch 17/80: current_loss=10.28280 | best_loss=10.06112
Epoch 18/80: current_loss=10.19833 | best_loss=10.06112
Epoch 19/80: current_loss=10.17489 | best_loss=10.06112
Epoch 20/80: current_loss=10.27219 | best_loss=10.06112
Epoch 21/80: current_loss=10.21526 | best_loss=10.06112
Epoch 22/80: current_loss=10.21071 | best_loss=10.06112
Early Stopping at epoch 22
      explained_var=0.03857 | mse_loss=9.69303
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.59151 | best_loss=7.59151
Epoch 1/80: current_loss=7.62203 | best_loss=7.59151
Epoch 2/80: current_loss=7.62498 | best_loss=7.59151
Epoch 3/80: current_loss=7.70928 | best_loss=7.59151
Epoch 4/80: current_loss=7.65351 | best_loss=7.59151
Epoch 5/80: current_loss=7.52612 | best_loss=7.52612
Epoch 6/80: current_loss=7.62447 | best_loss=7.52612
Epoch 7/80: current_loss=7.59432 | best_loss=7.52612
Epoch 8/80: current_loss=7.56612 | best_loss=7.52612
Epoch 9/80: current_loss=7.62387 | best_loss=7.52612
Epoch 10/80: current_loss=7.60448 | best_loss=7.52612
Epoch 11/80: current_loss=7.58137 | best_loss=7.52612
Epoch 12/80: current_loss=7.65340 | best_loss=7.52612
Epoch 13/80: current_loss=7.58360 | best_loss=7.52612
Epoch 14/80: current_loss=7.67395 | best_loss=7.52612
Epoch 15/80: current_loss=7.64356 | best_loss=7.52612
Epoch 16/80: current_loss=7.53152 | best_loss=7.52612
Epoch 17/80: current_loss=7.55601 | best_loss=7.52612
Epoch 18/80: current_loss=7.56244 | best_loss=7.52612
Epoch 19/80: current_loss=7.68248 | best_loss=7.52612
Epoch 20/80: current_loss=7.65246 | best_loss=7.52612
Epoch 21/80: current_loss=7.60599 | best_loss=7.52612
Epoch 22/80: current_loss=7.52002 | best_loss=7.52002
Epoch 23/80: current_loss=7.55254 | best_loss=7.52002
Epoch 24/80: current_loss=7.58031 | best_loss=7.52002
Epoch 25/80: current_loss=7.60578 | best_loss=7.52002
Epoch 26/80: current_loss=7.57773 | best_loss=7.52002
Epoch 27/80: current_loss=7.64910 | best_loss=7.52002
Epoch 28/80: current_loss=7.66203 | best_loss=7.52002
Epoch 29/80: current_loss=7.63268 | best_loss=7.52002
Epoch 30/80: current_loss=7.59033 | best_loss=7.52002
Epoch 31/80: current_loss=7.62261 | best_loss=7.52002
Epoch 32/80: current_loss=7.62625 | best_loss=7.52002
Epoch 33/80: current_loss=7.65443 | best_loss=7.52002
Epoch 34/80: current_loss=7.54603 | best_loss=7.52002
Epoch 35/80: current_loss=7.57626 | best_loss=7.52002
Epoch 36/80: current_loss=7.58897 | best_loss=7.52002
Epoch 37/80: current_loss=7.58448 | best_loss=7.52002
Epoch 38/80: current_loss=7.62494 | best_loss=7.52002
Epoch 39/80: current_loss=7.60943 | best_loss=7.52002
Epoch 40/80: current_loss=7.61983 | best_loss=7.52002
Epoch 41/80: current_loss=7.61052 | best_loss=7.52002
Epoch 42/80: current_loss=7.64834 | best_loss=7.52002
Early Stopping at epoch 42
      explained_var=-0.00479 | mse_loss=7.63105

----------------------------------------------
Params for Trial 50
{'learning_rate': 0.001, 'weight_decay': 0.007254187156626804, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.83536 | best_loss=17.83536
Epoch 1/80: current_loss=10.86161 | best_loss=10.86161
Epoch 2/80: current_loss=10.44138 | best_loss=10.44138
Epoch 3/80: current_loss=10.36998 | best_loss=10.36998
Epoch 4/80: current_loss=10.35100 | best_loss=10.35100
Epoch 5/80: current_loss=10.33428 | best_loss=10.33428
Epoch 6/80: current_loss=10.32275 | best_loss=10.32275
Epoch 7/80: current_loss=10.34193 | best_loss=10.32275
Epoch 8/80: current_loss=10.29686 | best_loss=10.29686
Epoch 9/80: current_loss=10.30294 | best_loss=10.29686
Epoch 10/80: current_loss=10.30953 | best_loss=10.29686
Epoch 11/80: current_loss=10.28938 | best_loss=10.28938
Epoch 12/80: current_loss=10.33273 | best_loss=10.28938
Epoch 13/80: current_loss=10.31328 | best_loss=10.28938
Epoch 14/80: current_loss=10.28475 | best_loss=10.28475
Epoch 15/80: current_loss=10.29873 | best_loss=10.28475
Epoch 16/80: current_loss=10.30739 | best_loss=10.28475
Epoch 17/80: current_loss=10.27939 | best_loss=10.27939
Epoch 18/80: current_loss=10.30812 | best_loss=10.27939
Epoch 19/80: current_loss=10.31366 | best_loss=10.27939
Epoch 20/80: current_loss=10.30185 | best_loss=10.27939
Epoch 21/80: current_loss=10.32239 | best_loss=10.27939
Epoch 22/80: current_loss=10.31209 | best_loss=10.27939
Epoch 23/80: current_loss=10.30934 | best_loss=10.27939
Epoch 24/80: current_loss=10.31326 | best_loss=10.27939
Epoch 25/80: current_loss=10.33512 | best_loss=10.27939
Epoch 26/80: current_loss=10.29249 | best_loss=10.27939
Epoch 27/80: current_loss=10.27236 | best_loss=10.27236
Epoch 28/80: current_loss=10.27728 | best_loss=10.27236
Epoch 29/80: current_loss=10.27914 | best_loss=10.27236
Epoch 30/80: current_loss=10.28768 | best_loss=10.27236
Epoch 31/80: current_loss=10.28889 | best_loss=10.27236
Epoch 32/80: current_loss=10.34278 | best_loss=10.27236
Epoch 33/80: current_loss=10.28261 | best_loss=10.27236
Epoch 34/80: current_loss=10.29086 | best_loss=10.27236
Epoch 35/80: current_loss=10.29232 | best_loss=10.27236
Epoch 36/80: current_loss=10.27038 | best_loss=10.27038
Epoch 37/80: current_loss=10.30013 | best_loss=10.27038
Epoch 38/80: current_loss=10.30372 | best_loss=10.27038
Epoch 39/80: current_loss=10.27275 | best_loss=10.27038
Epoch 40/80: current_loss=10.27425 | best_loss=10.27038
Epoch 41/80: current_loss=10.32619 | best_loss=10.27038
Epoch 42/80: current_loss=10.29324 | best_loss=10.27038
Epoch 43/80: current_loss=10.28751 | best_loss=10.27038
Epoch 44/80: current_loss=10.29722 | best_loss=10.27038
Epoch 45/80: current_loss=10.27278 | best_loss=10.27038
Epoch 46/80: current_loss=10.28656 | best_loss=10.27038
Epoch 47/80: current_loss=10.29191 | best_loss=10.27038
Epoch 48/80: current_loss=10.31610 | best_loss=10.27038
Epoch 49/80: current_loss=10.28851 | best_loss=10.27038
Epoch 50/80: current_loss=10.28687 | best_loss=10.27038
Epoch 51/80: current_loss=10.32688 | best_loss=10.27038
Epoch 52/80: current_loss=10.28302 | best_loss=10.27038
Epoch 53/80: current_loss=10.28777 | best_loss=10.27038
Epoch 54/80: current_loss=10.30198 | best_loss=10.27038
Epoch 55/80: current_loss=10.27035 | best_loss=10.27035
Epoch 56/80: current_loss=10.28963 | best_loss=10.27035
Epoch 57/80: current_loss=10.29193 | best_loss=10.27035
Epoch 58/80: current_loss=10.32042 | best_loss=10.27035
Epoch 59/80: current_loss=10.27216 | best_loss=10.27035
Epoch 60/80: current_loss=10.33641 | best_loss=10.27035
Epoch 61/80: current_loss=10.30265 | best_loss=10.27035
Epoch 62/80: current_loss=10.35969 | best_loss=10.27035
Epoch 63/80: current_loss=10.30169 | best_loss=10.27035
Epoch 64/80: current_loss=10.32503 | best_loss=10.27035
Epoch 65/80: current_loss=10.26566 | best_loss=10.26566
Epoch 66/80: current_loss=10.27238 | best_loss=10.26566
Epoch 67/80: current_loss=10.21691 | best_loss=10.21691
Epoch 68/80: current_loss=10.27312 | best_loss=10.21691
Epoch 69/80: current_loss=10.31370 | best_loss=10.21691
Epoch 70/80: current_loss=10.30698 | best_loss=10.21691
Epoch 71/80: current_loss=10.29329 | best_loss=10.21691
Epoch 72/80: current_loss=10.27920 | best_loss=10.21691
Epoch 73/80: current_loss=10.29364 | best_loss=10.21691
Epoch 74/80: current_loss=10.28318 | best_loss=10.21691
Epoch 75/80: current_loss=10.30138 | best_loss=10.21691
Epoch 76/80: current_loss=10.27008 | best_loss=10.21691
Epoch 77/80: current_loss=10.29138 | best_loss=10.21691
Epoch 78/80: current_loss=10.35406 | best_loss=10.21691
Epoch 79/80: current_loss=10.27146 | best_loss=10.21691
      explained_var=0.01287 | mse_loss=10.47963
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.91499 | best_loss=10.91499
Epoch 1/80: current_loss=10.40697 | best_loss=10.40697
Epoch 2/80: current_loss=10.40939 | best_loss=10.40697
Epoch 3/80: current_loss=10.40789 | best_loss=10.40697
Epoch 4/80: current_loss=10.42804 | best_loss=10.40697
Epoch 5/80: current_loss=10.42625 | best_loss=10.40697
Epoch 6/80: current_loss=10.45858 | best_loss=10.40697
Epoch 7/80: current_loss=10.39884 | best_loss=10.39884
Epoch 8/80: current_loss=10.40455 | best_loss=10.39884
Epoch 9/80: current_loss=10.41469 | best_loss=10.39884
Epoch 10/80: current_loss=10.42564 | best_loss=10.39884
Epoch 11/80: current_loss=10.38369 | best_loss=10.38369
Epoch 12/80: current_loss=10.40728 | best_loss=10.38369
Epoch 13/80: current_loss=10.42809 | best_loss=10.38369
Epoch 14/80: current_loss=10.40264 | best_loss=10.38369
Epoch 15/80: current_loss=10.49235 | best_loss=10.38369
Epoch 16/80: current_loss=10.40100 | best_loss=10.38369
Epoch 17/80: current_loss=10.41740 | best_loss=10.38369
Epoch 18/80: current_loss=10.41744 | best_loss=10.38369
Epoch 19/80: current_loss=10.40123 | best_loss=10.38369
Epoch 20/80: current_loss=10.33473 | best_loss=10.33473
Epoch 21/80: current_loss=10.40393 | best_loss=10.33473
Epoch 22/80: current_loss=10.46870 | best_loss=10.33473
Epoch 23/80: current_loss=10.41427 | best_loss=10.33473
Epoch 24/80: current_loss=10.42840 | best_loss=10.33473
Epoch 25/80: current_loss=10.46668 | best_loss=10.33473
Epoch 26/80: current_loss=10.40551 | best_loss=10.33473
Epoch 27/80: current_loss=10.45258 | best_loss=10.33473
Epoch 28/80: current_loss=10.42151 | best_loss=10.33473
Epoch 29/80: current_loss=10.42750 | best_loss=10.33473
Epoch 30/80: current_loss=10.40530 | best_loss=10.33473
Epoch 31/80: current_loss=10.43883 | best_loss=10.33473
Epoch 32/80: current_loss=10.43070 | best_loss=10.33473
Epoch 33/80: current_loss=10.39397 | best_loss=10.33473
Epoch 34/80: current_loss=10.38565 | best_loss=10.33473
Epoch 35/80: current_loss=10.46899 | best_loss=10.33473
Epoch 36/80: current_loss=10.49813 | best_loss=10.33473
Epoch 37/80: current_loss=10.41369 | best_loss=10.33473
Epoch 38/80: current_loss=10.40971 | best_loss=10.33473
Epoch 39/80: current_loss=10.41988 | best_loss=10.33473
Epoch 40/80: current_loss=10.40882 | best_loss=10.33473
Early Stopping at epoch 40
      explained_var=0.01326 | mse_loss=9.97648
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.74233 | best_loss=7.74233
Epoch 1/80: current_loss=7.65059 | best_loss=7.65059
Epoch 2/80: current_loss=7.63662 | best_loss=7.63662
Epoch 3/80: current_loss=7.51057 | best_loss=7.51057
Epoch 4/80: current_loss=7.65930 | best_loss=7.51057
Epoch 5/80: current_loss=7.60621 | best_loss=7.51057
Epoch 6/80: current_loss=7.63283 | best_loss=7.51057
Epoch 7/80: current_loss=7.49949 | best_loss=7.49949
Epoch 8/80: current_loss=7.75728 | best_loss=7.49949
Epoch 9/80: current_loss=7.56435 | best_loss=7.49949
Epoch 10/80: current_loss=7.55468 | best_loss=7.49949
Epoch 11/80: current_loss=7.62228 | best_loss=7.49949
Epoch 12/80: current_loss=7.66033 | best_loss=7.49949
Epoch 13/80: current_loss=7.65857 | best_loss=7.49949
Epoch 14/80: current_loss=7.52259 | best_loss=7.49949
Epoch 15/80: current_loss=7.66340 | best_loss=7.49949
Epoch 16/80: current_loss=7.49212 | best_loss=7.49212
Epoch 17/80: current_loss=7.63367 | best_loss=7.49212
Epoch 18/80: current_loss=7.65702 | best_loss=7.49212
Epoch 19/80: current_loss=7.50403 | best_loss=7.49212
Epoch 20/80: current_loss=7.94761 | best_loss=7.49212
Epoch 21/80: current_loss=7.51565 | best_loss=7.49212
Epoch 22/80: current_loss=7.87204 | best_loss=7.49212
Epoch 23/80: current_loss=7.51218 | best_loss=7.49212
Epoch 24/80: current_loss=7.70136 | best_loss=7.49212
Epoch 25/80: current_loss=7.54532 | best_loss=7.49212
Epoch 26/80: current_loss=7.46424 | best_loss=7.46424
Epoch 27/80: current_loss=7.53125 | best_loss=7.46424
Epoch 28/80: current_loss=7.65001 | best_loss=7.46424
Epoch 29/80: current_loss=7.52743 | best_loss=7.46424
Epoch 30/80: current_loss=7.71209 | best_loss=7.46424
Epoch 31/80: current_loss=7.79511 | best_loss=7.46424
Epoch 32/80: current_loss=7.53295 | best_loss=7.46424
Epoch 33/80: current_loss=7.48066 | best_loss=7.46424
Epoch 34/80: current_loss=7.66722 | best_loss=7.46424
Epoch 35/80: current_loss=7.52356 | best_loss=7.46424
Epoch 36/80: current_loss=7.61757 | best_loss=7.46424
Epoch 37/80: current_loss=7.78135 | best_loss=7.46424
Epoch 38/80: current_loss=7.55263 | best_loss=7.46424
Epoch 39/80: current_loss=7.61641 | best_loss=7.46424
Epoch 40/80: current_loss=7.67131 | best_loss=7.46424
Epoch 41/80: current_loss=7.62843 | best_loss=7.46424
Epoch 42/80: current_loss=7.61973 | best_loss=7.46424
Epoch 43/80: current_loss=7.69964 | best_loss=7.46424
Epoch 44/80: current_loss=7.63172 | best_loss=7.46424
Epoch 45/80: current_loss=7.53952 | best_loss=7.46424
Epoch 46/80: current_loss=7.55252 | best_loss=7.46424
Early Stopping at epoch 46
      explained_var=0.00008 | mse_loss=7.58391
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.79416 | best_loss=8.79416
Epoch 1/80: current_loss=8.76286 | best_loss=8.76286
Epoch 2/80: current_loss=8.74825 | best_loss=8.74825
Epoch 3/80: current_loss=8.82307 | best_loss=8.74825
Epoch 4/80: current_loss=8.79537 | best_loss=8.74825
Epoch 5/80: current_loss=8.88475 | best_loss=8.74825
Epoch 6/80: current_loss=8.76617 | best_loss=8.74825
Epoch 7/80: current_loss=8.78449 | best_loss=8.74825
Epoch 8/80: current_loss=8.86477 | best_loss=8.74825
Epoch 9/80: current_loss=8.92243 | best_loss=8.74825
Epoch 10/80: current_loss=8.74992 | best_loss=8.74825
Epoch 11/80: current_loss=8.73457 | best_loss=8.73457
Epoch 12/80: current_loss=8.96313 | best_loss=8.73457
Epoch 13/80: current_loss=8.83195 | best_loss=8.73457
Epoch 14/80: current_loss=8.75794 | best_loss=8.73457
Epoch 15/80: current_loss=8.80179 | best_loss=8.73457
Epoch 16/80: current_loss=8.84581 | best_loss=8.73457
Epoch 17/80: current_loss=8.88788 | best_loss=8.73457
Epoch 18/80: current_loss=8.80598 | best_loss=8.73457
Epoch 19/80: current_loss=8.81438 | best_loss=8.73457
Epoch 20/80: current_loss=9.00000 | best_loss=8.73457
Epoch 21/80: current_loss=8.83124 | best_loss=8.73457
Epoch 22/80: current_loss=8.88527 | best_loss=8.73457
Epoch 23/80: current_loss=8.74193 | best_loss=8.73457
Epoch 24/80: current_loss=8.85054 | best_loss=8.73457
Epoch 25/80: current_loss=8.82505 | best_loss=8.73457
Epoch 26/80: current_loss=8.88335 | best_loss=8.73457
Epoch 27/80: current_loss=8.81153 | best_loss=8.73457
Epoch 28/80: current_loss=8.82665 | best_loss=8.73457
Epoch 29/80: current_loss=8.90925 | best_loss=8.73457
Epoch 30/80: current_loss=8.73174 | best_loss=8.73174
Epoch 31/80: current_loss=8.82990 | best_loss=8.73174
Epoch 32/80: current_loss=8.84442 | best_loss=8.73174
Epoch 33/80: current_loss=8.86498 | best_loss=8.73174
Epoch 34/80: current_loss=8.88318 | best_loss=8.73174
Epoch 35/80: current_loss=8.77671 | best_loss=8.73174
Epoch 36/80: current_loss=8.89758 | best_loss=8.73174
Epoch 37/80: current_loss=8.81283 | best_loss=8.73174
Epoch 38/80: current_loss=8.85856 | best_loss=8.73174
Epoch 39/80: current_loss=8.85316 | best_loss=8.73174
Epoch 40/80: current_loss=8.79354 | best_loss=8.73174
Epoch 41/80: current_loss=8.93304 | best_loss=8.73174
Epoch 42/80: current_loss=8.89665 | best_loss=8.73174
Epoch 43/80: current_loss=8.89572 | best_loss=8.73174
Epoch 44/80: current_loss=8.92826 | best_loss=8.73174
Epoch 45/80: current_loss=8.74473 | best_loss=8.73174
Epoch 46/80: current_loss=8.86531 | best_loss=8.73174
Epoch 47/80: current_loss=8.85769 | best_loss=8.73174
Epoch 48/80: current_loss=8.83678 | best_loss=8.73174
Epoch 49/80: current_loss=8.79447 | best_loss=8.73174
Epoch 50/80: current_loss=8.88307 | best_loss=8.73174
Early Stopping at epoch 50
      explained_var=0.00273 | mse_loss=8.70139
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.18609 | best_loss=9.18609
Epoch 1/80: current_loss=9.16770 | best_loss=9.16770
Epoch 2/80: current_loss=9.19716 | best_loss=9.16770
Epoch 3/80: current_loss=9.34988 | best_loss=9.16770
Epoch 4/80: current_loss=9.31405 | best_loss=9.16770
Epoch 5/80: current_loss=9.31423 | best_loss=9.16770
Epoch 6/80: current_loss=9.25260 | best_loss=9.16770
Epoch 7/80: current_loss=9.17940 | best_loss=9.16770
Epoch 8/80: current_loss=9.37288 | best_loss=9.16770
Epoch 9/80: current_loss=9.46300 | best_loss=9.16770
Epoch 10/80: current_loss=9.22495 | best_loss=9.16770
Epoch 11/80: current_loss=9.21263 | best_loss=9.16770
Epoch 12/80: current_loss=9.19950 | best_loss=9.16770
Epoch 13/80: current_loss=9.14778 | best_loss=9.14778
Epoch 14/80: current_loss=9.16906 | best_loss=9.14778
Epoch 15/80: current_loss=9.20874 | best_loss=9.14778
Epoch 16/80: current_loss=9.20102 | best_loss=9.14778
Epoch 17/80: current_loss=9.21733 | best_loss=9.14778
Epoch 18/80: current_loss=9.18875 | best_loss=9.14778
Epoch 19/80: current_loss=9.17998 | best_loss=9.14778
Epoch 20/80: current_loss=9.18386 | best_loss=9.14778
Epoch 21/80: current_loss=9.15591 | best_loss=9.14778
Epoch 22/80: current_loss=9.28258 | best_loss=9.14778
Epoch 23/80: current_loss=9.25672 | best_loss=9.14778
Epoch 24/80: current_loss=9.31158 | best_loss=9.14778
Epoch 25/80: current_loss=9.21821 | best_loss=9.14778
Epoch 26/80: current_loss=9.23119 | best_loss=9.14778
Epoch 27/80: current_loss=9.25996 | best_loss=9.14778
Epoch 28/80: current_loss=9.22514 | best_loss=9.14778
Epoch 29/80: current_loss=9.17066 | best_loss=9.14778
Epoch 30/80: current_loss=9.18312 | best_loss=9.14778
Epoch 31/80: current_loss=9.15716 | best_loss=9.14778
Epoch 32/80: current_loss=9.50307 | best_loss=9.14778
Epoch 33/80: current_loss=9.19380 | best_loss=9.14778
Early Stopping at epoch 33
      explained_var=0.02067 | mse_loss=9.10288
----------------------------------------------
Average early_stopping_point: 33| avg_exp_var=0.00992| avg_loss=9.16886
----------------------------------------------


----------------------------------------------
Params for Trial 51
{'learning_rate': 0.001, 'weight_decay': 0.0072121061905505626, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.21581 | best_loss=17.21581
Epoch 1/80: current_loss=10.79387 | best_loss=10.79387
Epoch 2/80: current_loss=10.44052 | best_loss=10.44052
Epoch 3/80: current_loss=10.40046 | best_loss=10.40046
Epoch 4/80: current_loss=10.36438 | best_loss=10.36438
Epoch 5/80: current_loss=10.33110 | best_loss=10.33110
Epoch 6/80: current_loss=10.31946 | best_loss=10.31946
Epoch 7/80: current_loss=10.30574 | best_loss=10.30574
Epoch 8/80: current_loss=10.33369 | best_loss=10.30574
Epoch 9/80: current_loss=10.29364 | best_loss=10.29364
Epoch 10/80: current_loss=10.29722 | best_loss=10.29364
Epoch 11/80: current_loss=10.31016 | best_loss=10.29364
Epoch 12/80: current_loss=10.27850 | best_loss=10.27850
Epoch 13/80: current_loss=10.28779 | best_loss=10.27850
Epoch 14/80: current_loss=10.30323 | best_loss=10.27850
Epoch 15/80: current_loss=10.32392 | best_loss=10.27850
Epoch 16/80: current_loss=10.32235 | best_loss=10.27850
Epoch 17/80: current_loss=10.28731 | best_loss=10.27850
Epoch 18/80: current_loss=10.31706 | best_loss=10.27850
Epoch 19/80: current_loss=10.31092 | best_loss=10.27850
Epoch 20/80: current_loss=10.31262 | best_loss=10.27850
Epoch 21/80: current_loss=10.32630 | best_loss=10.27850
Epoch 22/80: current_loss=10.29519 | best_loss=10.27850
Epoch 23/80: current_loss=10.28435 | best_loss=10.27850
Epoch 24/80: current_loss=10.27240 | best_loss=10.27240
Epoch 25/80: current_loss=10.26943 | best_loss=10.26943
Epoch 26/80: current_loss=10.29305 | best_loss=10.26943
Epoch 27/80: current_loss=10.26716 | best_loss=10.26716
Epoch 28/80: current_loss=10.32844 | best_loss=10.26716
Epoch 29/80: current_loss=10.29308 | best_loss=10.26716
Epoch 30/80: current_loss=10.28073 | best_loss=10.26716
Epoch 31/80: current_loss=10.30185 | best_loss=10.26716
Epoch 32/80: current_loss=10.31259 | best_loss=10.26716
Epoch 33/80: current_loss=10.28277 | best_loss=10.26716
Epoch 34/80: current_loss=10.30288 | best_loss=10.26716
Epoch 35/80: current_loss=10.33416 | best_loss=10.26716
Epoch 36/80: current_loss=10.28200 | best_loss=10.26716
Epoch 37/80: current_loss=10.31712 | best_loss=10.26716
Epoch 38/80: current_loss=10.28967 | best_loss=10.26716
Epoch 39/80: current_loss=10.27513 | best_loss=10.26716
Epoch 40/80: current_loss=10.27342 | best_loss=10.26716
Epoch 41/80: current_loss=10.28520 | best_loss=10.26716
Epoch 42/80: current_loss=10.26350 | best_loss=10.26350
Epoch 43/80: current_loss=10.31109 | best_loss=10.26350
Epoch 44/80: current_loss=10.26241 | best_loss=10.26241
Epoch 45/80: current_loss=10.31447 | best_loss=10.26241
Epoch 46/80: current_loss=10.26771 | best_loss=10.26241
Epoch 47/80: current_loss=10.27363 | best_loss=10.26241
Epoch 48/80: current_loss=10.28150 | best_loss=10.26241
Epoch 49/80: current_loss=10.27952 | best_loss=10.26241
Epoch 50/80: current_loss=10.27804 | best_loss=10.26241
Epoch 51/80: current_loss=10.26478 | best_loss=10.26241
Epoch 52/80: current_loss=10.30901 | best_loss=10.26241
Epoch 53/80: current_loss=10.26636 | best_loss=10.26241
Epoch 54/80: current_loss=10.29391 | best_loss=10.26241
Epoch 55/80: current_loss=10.29818 | best_loss=10.26241
Epoch 56/80: current_loss=10.35496 | best_loss=10.26241
Epoch 57/80: current_loss=10.27236 | best_loss=10.26241
Epoch 58/80: current_loss=10.35359 | best_loss=10.26241
Epoch 59/80: current_loss=10.30418 | best_loss=10.26241
Epoch 60/80: current_loss=10.29500 | best_loss=10.26241
Epoch 61/80: current_loss=10.29399 | best_loss=10.26241
Epoch 62/80: current_loss=10.33238 | best_loss=10.26241
Epoch 63/80: current_loss=10.25248 | best_loss=10.25248
Epoch 64/80: current_loss=10.20874 | best_loss=10.20874
Epoch 65/80: current_loss=10.35571 | best_loss=10.20874
Epoch 66/80: current_loss=10.22037 | best_loss=10.20874
Epoch 67/80: current_loss=10.21049 | best_loss=10.20874
Epoch 68/80: current_loss=10.23922 | best_loss=10.20874
Epoch 69/80: current_loss=10.24427 | best_loss=10.20874
Epoch 70/80: current_loss=10.08314 | best_loss=10.08314
Epoch 71/80: current_loss=10.17658 | best_loss=10.08314
Epoch 72/80: current_loss=10.23125 | best_loss=10.08314
Epoch 73/80: current_loss=10.25436 | best_loss=10.08314
Epoch 74/80: current_loss=10.25271 | best_loss=10.08314
Epoch 75/80: current_loss=10.10916 | best_loss=10.08314
Epoch 76/80: current_loss=10.15538 | best_loss=10.08314
Epoch 77/80: current_loss=10.22809 | best_loss=10.08314
Epoch 78/80: current_loss=10.41082 | best_loss=10.08314
Epoch 79/80: current_loss=10.30562 | best_loss=10.08314
      explained_var=0.01582 | mse_loss=10.31878
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.41066 | best_loss=10.41066
Epoch 1/80: current_loss=10.40122 | best_loss=10.40122
Epoch 2/80: current_loss=10.39085 | best_loss=10.39085
Epoch 3/80: current_loss=10.30267 | best_loss=10.30267
Epoch 4/80: current_loss=10.45571 | best_loss=10.30267
Epoch 5/80: current_loss=10.41939 | best_loss=10.30267
Epoch 6/80: current_loss=10.47226 | best_loss=10.30267
Epoch 7/80: current_loss=10.41630 | best_loss=10.30267
Epoch 8/80: current_loss=10.43090 | best_loss=10.30267
Epoch 9/80: current_loss=10.41030 | best_loss=10.30267
Epoch 10/80: current_loss=10.42984 | best_loss=10.30267
Epoch 11/80: current_loss=10.40522 | best_loss=10.30267
Epoch 12/80: current_loss=10.42715 | best_loss=10.30267
Epoch 13/80: current_loss=10.41699 | best_loss=10.30267
Epoch 14/80: current_loss=10.41050 | best_loss=10.30267
Epoch 15/80: current_loss=10.40721 | best_loss=10.30267
Epoch 16/80: current_loss=10.41098 | best_loss=10.30267
Epoch 17/80: current_loss=10.47065 | best_loss=10.30267
Epoch 18/80: current_loss=10.45130 | best_loss=10.30267
Epoch 19/80: current_loss=10.40649 | best_loss=10.30267
Epoch 20/80: current_loss=10.57860 | best_loss=10.30267
Epoch 21/80: current_loss=10.41887 | best_loss=10.30267
Epoch 22/80: current_loss=10.39604 | best_loss=10.30267
Epoch 23/80: current_loss=10.38008 | best_loss=10.30267
Early Stopping at epoch 23
      explained_var=0.02734 | mse_loss=9.93718
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.80061 | best_loss=7.80061
Epoch 1/80: current_loss=7.51286 | best_loss=7.51286
Epoch 2/80: current_loss=7.55057 | best_loss=7.51286
Epoch 3/80: current_loss=7.83008 | best_loss=7.51286
Epoch 4/80: current_loss=7.61456 | best_loss=7.51286
Epoch 5/80: current_loss=7.67367 | best_loss=7.51286
Epoch 6/80: current_loss=7.50929 | best_loss=7.50929
Epoch 7/80: current_loss=7.78292 | best_loss=7.50929
Epoch 8/80: current_loss=7.48720 | best_loss=7.48720
Epoch 9/80: current_loss=7.72993 | best_loss=7.48720
Epoch 10/80: current_loss=7.63462 | best_loss=7.48720
Epoch 11/80: current_loss=7.54886 | best_loss=7.48720
Epoch 12/80: current_loss=7.61664 | best_loss=7.48720
Epoch 13/80: current_loss=7.64381 | best_loss=7.48720
Epoch 14/80: current_loss=7.49754 | best_loss=7.48720
Epoch 15/80: current_loss=7.63178 | best_loss=7.48720
Epoch 16/80: current_loss=7.60345 | best_loss=7.48720
Epoch 17/80: current_loss=7.50966 | best_loss=7.48720
Epoch 18/80: current_loss=7.83398 | best_loss=7.48720
Epoch 19/80: current_loss=7.64332 | best_loss=7.48720
Epoch 20/80: current_loss=7.72982 | best_loss=7.48720
Epoch 21/80: current_loss=7.57825 | best_loss=7.48720
Epoch 22/80: current_loss=7.57335 | best_loss=7.48720
Epoch 23/80: current_loss=7.73925 | best_loss=7.48720
Epoch 24/80: current_loss=7.52083 | best_loss=7.48720
Epoch 25/80: current_loss=7.63148 | best_loss=7.48720
Epoch 26/80: current_loss=7.72095 | best_loss=7.48720
Epoch 27/80: current_loss=7.61344 | best_loss=7.48720
Epoch 28/80: current_loss=7.63403 | best_loss=7.48720
Early Stopping at epoch 28
      explained_var=-0.00209 | mse_loss=7.61292

----------------------------------------------
Params for Trial 52
{'learning_rate': 0.001, 'weight_decay': 0.0058070970849910435, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.23660 | best_loss=16.23660
Epoch 1/80: current_loss=10.68313 | best_loss=10.68313
Epoch 2/80: current_loss=10.42027 | best_loss=10.42027
Epoch 3/80: current_loss=10.36298 | best_loss=10.36298
Epoch 4/80: current_loss=10.36068 | best_loss=10.36068
Epoch 5/80: current_loss=10.35177 | best_loss=10.35177
Epoch 6/80: current_loss=10.31592 | best_loss=10.31592
Epoch 7/80: current_loss=10.32109 | best_loss=10.31592
Epoch 8/80: current_loss=10.32571 | best_loss=10.31592
Epoch 9/80: current_loss=10.29305 | best_loss=10.29305
Epoch 10/80: current_loss=10.28337 | best_loss=10.28337
Epoch 11/80: current_loss=10.31472 | best_loss=10.28337
Epoch 12/80: current_loss=10.28780 | best_loss=10.28337
Epoch 13/80: current_loss=10.31933 | best_loss=10.28337
Epoch 14/80: current_loss=10.27626 | best_loss=10.27626
Epoch 15/80: current_loss=10.29948 | best_loss=10.27626
Epoch 16/80: current_loss=10.29534 | best_loss=10.27626
Epoch 17/80: current_loss=10.30415 | best_loss=10.27626
Epoch 18/80: current_loss=10.30863 | best_loss=10.27626
Epoch 19/80: current_loss=10.28604 | best_loss=10.27626
Epoch 20/80: current_loss=10.30335 | best_loss=10.27626
Epoch 21/80: current_loss=10.30790 | best_loss=10.27626
Epoch 22/80: current_loss=10.30253 | best_loss=10.27626
Epoch 23/80: current_loss=10.28480 | best_loss=10.27626
Epoch 24/80: current_loss=10.29112 | best_loss=10.27626
Epoch 25/80: current_loss=10.31269 | best_loss=10.27626
Epoch 26/80: current_loss=10.30627 | best_loss=10.27626
Epoch 27/80: current_loss=10.27989 | best_loss=10.27626
Epoch 28/80: current_loss=10.32481 | best_loss=10.27626
Epoch 29/80: current_loss=10.28506 | best_loss=10.27626
Epoch 30/80: current_loss=10.32386 | best_loss=10.27626
Epoch 31/80: current_loss=10.31107 | best_loss=10.27626
Epoch 32/80: current_loss=10.28961 | best_loss=10.27626
Epoch 33/80: current_loss=10.29458 | best_loss=10.27626
Epoch 34/80: current_loss=10.30217 | best_loss=10.27626
Early Stopping at epoch 34
      explained_var=-0.00339 | mse_loss=10.50252
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.43448 | best_loss=10.43448
Epoch 1/80: current_loss=10.44087 | best_loss=10.43448
Epoch 2/80: current_loss=10.43088 | best_loss=10.43088
Epoch 3/80: current_loss=10.41945 | best_loss=10.41945
Epoch 4/80: current_loss=10.40474 | best_loss=10.40474
Epoch 5/80: current_loss=10.40658 | best_loss=10.40474
Epoch 6/80: current_loss=10.54043 | best_loss=10.40474
Epoch 7/80: current_loss=10.41884 | best_loss=10.40474
Epoch 8/80: current_loss=10.45243 | best_loss=10.40474
Epoch 9/80: current_loss=10.40574 | best_loss=10.40474
Epoch 10/80: current_loss=10.42224 | best_loss=10.40474
Epoch 11/80: current_loss=10.44660 | best_loss=10.40474
Epoch 12/80: current_loss=10.42988 | best_loss=10.40474
Epoch 13/80: current_loss=10.41975 | best_loss=10.40474
Epoch 14/80: current_loss=10.41547 | best_loss=10.40474
Epoch 15/80: current_loss=10.59964 | best_loss=10.40474
Epoch 16/80: current_loss=10.40975 | best_loss=10.40474
Epoch 17/80: current_loss=10.48042 | best_loss=10.40474
Epoch 18/80: current_loss=10.46378 | best_loss=10.40474
Epoch 19/80: current_loss=10.40498 | best_loss=10.40474
Epoch 20/80: current_loss=10.42368 | best_loss=10.40474
Epoch 21/80: current_loss=10.40973 | best_loss=10.40474
Epoch 22/80: current_loss=10.40418 | best_loss=10.40418
Epoch 23/80: current_loss=10.46596 | best_loss=10.40418
Epoch 24/80: current_loss=10.50832 | best_loss=10.40418
Epoch 25/80: current_loss=10.40857 | best_loss=10.40418
Epoch 26/80: current_loss=10.42407 | best_loss=10.40418
Epoch 27/80: current_loss=10.42419 | best_loss=10.40418
Epoch 28/80: current_loss=10.41313 | best_loss=10.40418
Epoch 29/80: current_loss=10.43825 | best_loss=10.40418
Epoch 30/80: current_loss=10.46463 | best_loss=10.40418
Epoch 31/80: current_loss=10.43419 | best_loss=10.40418
Epoch 32/80: current_loss=10.43694 | best_loss=10.40418
Epoch 33/80: current_loss=10.61795 | best_loss=10.40418
Epoch 34/80: current_loss=10.40692 | best_loss=10.40418
Epoch 35/80: current_loss=10.45694 | best_loss=10.40418
Epoch 36/80: current_loss=10.40811 | best_loss=10.40418
Epoch 37/80: current_loss=10.46076 | best_loss=10.40418
Epoch 38/80: current_loss=10.39832 | best_loss=10.39832
Epoch 39/80: current_loss=10.40830 | best_loss=10.39832
Epoch 40/80: current_loss=10.49841 | best_loss=10.39832
Epoch 41/80: current_loss=10.42191 | best_loss=10.39832
Epoch 42/80: current_loss=10.43758 | best_loss=10.39832
Epoch 43/80: current_loss=10.45079 | best_loss=10.39832
Epoch 44/80: current_loss=10.43340 | best_loss=10.39832
Epoch 45/80: current_loss=10.40737 | best_loss=10.39832
Epoch 46/80: current_loss=10.46377 | best_loss=10.39832
Epoch 47/80: current_loss=10.40252 | best_loss=10.39832
Epoch 48/80: current_loss=10.39796 | best_loss=10.39796
Epoch 49/80: current_loss=10.42663 | best_loss=10.39796
Epoch 50/80: current_loss=10.49275 | best_loss=10.39796
Epoch 51/80: current_loss=10.44184 | best_loss=10.39796
Epoch 52/80: current_loss=10.42401 | best_loss=10.39796
Epoch 53/80: current_loss=10.43111 | best_loss=10.39796
Epoch 54/80: current_loss=10.40594 | best_loss=10.39796
Epoch 55/80: current_loss=10.50005 | best_loss=10.39796
Epoch 56/80: current_loss=10.40249 | best_loss=10.39796
Epoch 57/80: current_loss=10.51533 | best_loss=10.39796
Epoch 58/80: current_loss=10.40521 | best_loss=10.39796
Epoch 59/80: current_loss=10.40397 | best_loss=10.39796
Epoch 60/80: current_loss=10.41713 | best_loss=10.39796
Epoch 61/80: current_loss=10.41315 | best_loss=10.39796
Epoch 62/80: current_loss=10.44349 | best_loss=10.39796
Epoch 63/80: current_loss=10.40920 | best_loss=10.39796
Epoch 64/80: current_loss=10.43573 | best_loss=10.39796
Epoch 65/80: current_loss=10.41474 | best_loss=10.39796
Epoch 66/80: current_loss=10.43949 | best_loss=10.39796
Epoch 67/80: current_loss=10.45848 | best_loss=10.39796
Epoch 68/80: current_loss=10.41828 | best_loss=10.39796
Early Stopping at epoch 68
      explained_var=0.00095 | mse_loss=10.05392

----------------------------------------------
Params for Trial 53
{'learning_rate': 0.001, 'weight_decay': 0.00879595990503162, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=15.93223 | best_loss=15.93223
Epoch 1/80: current_loss=10.64274 | best_loss=10.64274
Epoch 2/80: current_loss=10.42971 | best_loss=10.42971
Epoch 3/80: current_loss=10.37828 | best_loss=10.37828
Epoch 4/80: current_loss=10.36939 | best_loss=10.36939
Epoch 5/80: current_loss=10.34622 | best_loss=10.34622
Epoch 6/80: current_loss=10.34096 | best_loss=10.34096
Epoch 7/80: current_loss=10.31188 | best_loss=10.31188
Epoch 8/80: current_loss=10.31946 | best_loss=10.31188
Epoch 9/80: current_loss=10.32732 | best_loss=10.31188
Epoch 10/80: current_loss=10.31697 | best_loss=10.31188
Epoch 11/80: current_loss=10.30313 | best_loss=10.30313
Epoch 12/80: current_loss=10.28312 | best_loss=10.28312
Epoch 13/80: current_loss=10.30812 | best_loss=10.28312
Epoch 14/80: current_loss=10.29350 | best_loss=10.28312
Epoch 15/80: current_loss=10.28446 | best_loss=10.28312
Epoch 16/80: current_loss=10.31095 | best_loss=10.28312
Epoch 17/80: current_loss=10.27434 | best_loss=10.27434
Epoch 18/80: current_loss=10.31144 | best_loss=10.27434
Epoch 19/80: current_loss=10.31591 | best_loss=10.27434
Epoch 20/80: current_loss=10.29423 | best_loss=10.27434
Epoch 21/80: current_loss=10.32324 | best_loss=10.27434
Epoch 22/80: current_loss=10.28478 | best_loss=10.27434
Epoch 23/80: current_loss=10.28374 | best_loss=10.27434
Epoch 24/80: current_loss=10.28340 | best_loss=10.27434
Epoch 25/80: current_loss=10.27840 | best_loss=10.27434
Epoch 26/80: current_loss=10.28889 | best_loss=10.27434
Epoch 27/80: current_loss=10.29275 | best_loss=10.27434
Epoch 28/80: current_loss=10.31692 | best_loss=10.27434
Epoch 29/80: current_loss=10.31983 | best_loss=10.27434
Epoch 30/80: current_loss=10.28393 | best_loss=10.27434
Epoch 31/80: current_loss=10.29028 | best_loss=10.27434
Epoch 32/80: current_loss=10.28464 | best_loss=10.27434
Epoch 33/80: current_loss=10.27302 | best_loss=10.27302
Epoch 34/80: current_loss=10.30536 | best_loss=10.27302
Epoch 35/80: current_loss=10.30593 | best_loss=10.27302
Epoch 36/80: current_loss=10.28794 | best_loss=10.27302
Epoch 37/80: current_loss=10.28740 | best_loss=10.27302
Epoch 38/80: current_loss=10.27828 | best_loss=10.27302
Epoch 39/80: current_loss=10.27642 | best_loss=10.27302
Epoch 40/80: current_loss=10.29520 | best_loss=10.27302
Epoch 41/80: current_loss=10.34885 | best_loss=10.27302
Epoch 42/80: current_loss=10.31002 | best_loss=10.27302
Epoch 43/80: current_loss=10.27808 | best_loss=10.27302
Epoch 44/80: current_loss=10.34972 | best_loss=10.27302
Epoch 45/80: current_loss=10.28194 | best_loss=10.27302
Epoch 46/80: current_loss=10.26459 | best_loss=10.26459
Epoch 47/80: current_loss=10.28250 | best_loss=10.26459
Epoch 48/80: current_loss=10.27654 | best_loss=10.26459
Epoch 49/80: current_loss=10.27760 | best_loss=10.26459
Epoch 50/80: current_loss=10.29628 | best_loss=10.26459
Epoch 51/80: current_loss=10.29320 | best_loss=10.26459
Epoch 52/80: current_loss=10.33375 | best_loss=10.26459
Epoch 53/80: current_loss=10.28265 | best_loss=10.26459
Epoch 54/80: current_loss=10.27449 | best_loss=10.26459
Epoch 55/80: current_loss=10.28852 | best_loss=10.26459
Epoch 56/80: current_loss=10.32805 | best_loss=10.26459
Epoch 57/80: current_loss=10.29813 | best_loss=10.26459
Epoch 58/80: current_loss=10.25911 | best_loss=10.25911
Epoch 59/80: current_loss=10.28453 | best_loss=10.25911
Epoch 60/80: current_loss=10.27899 | best_loss=10.25911
Epoch 61/80: current_loss=10.31760 | best_loss=10.25911
Epoch 62/80: current_loss=10.32637 | best_loss=10.25911
Epoch 63/80: current_loss=10.32868 | best_loss=10.25911
Epoch 64/80: current_loss=10.26945 | best_loss=10.25911
Epoch 65/80: current_loss=10.26795 | best_loss=10.25911
Epoch 66/80: current_loss=10.20792 | best_loss=10.20792
Epoch 67/80: current_loss=10.22840 | best_loss=10.20792
Epoch 68/80: current_loss=10.21219 | best_loss=10.20792
Epoch 69/80: current_loss=10.28986 | best_loss=10.20792
Epoch 70/80: current_loss=10.31125 | best_loss=10.20792
Epoch 71/80: current_loss=10.29302 | best_loss=10.20792
Epoch 72/80: current_loss=10.20095 | best_loss=10.20095
Epoch 73/80: current_loss=10.23680 | best_loss=10.20095
Epoch 74/80: current_loss=10.32772 | best_loss=10.20095
Epoch 75/80: current_loss=10.25780 | best_loss=10.20095
Epoch 76/80: current_loss=10.28237 | best_loss=10.20095
Epoch 77/80: current_loss=10.32032 | best_loss=10.20095
Epoch 78/80: current_loss=10.33157 | best_loss=10.20095
Epoch 79/80: current_loss=10.29057 | best_loss=10.20095
      explained_var=0.00491 | mse_loss=10.44117
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.39664 | best_loss=10.39664
Epoch 1/80: current_loss=10.35876 | best_loss=10.35876
Epoch 2/80: current_loss=10.36013 | best_loss=10.35876
Epoch 3/80: current_loss=10.32149 | best_loss=10.32149
Epoch 4/80: current_loss=10.31307 | best_loss=10.31307
Epoch 5/80: current_loss=10.33286 | best_loss=10.31307
Epoch 6/80: current_loss=10.21784 | best_loss=10.21784
Epoch 7/80: current_loss=10.27930 | best_loss=10.21784
Epoch 8/80: current_loss=10.48302 | best_loss=10.21784
Epoch 9/80: current_loss=10.50299 | best_loss=10.21784
Epoch 10/80: current_loss=10.34952 | best_loss=10.21784
Epoch 11/80: current_loss=10.39956 | best_loss=10.21784
Epoch 12/80: current_loss=10.41227 | best_loss=10.21784
Epoch 13/80: current_loss=10.37067 | best_loss=10.21784
Epoch 14/80: current_loss=10.30162 | best_loss=10.21784
Epoch 15/80: current_loss=10.38750 | best_loss=10.21784
Epoch 16/80: current_loss=10.35977 | best_loss=10.21784
Epoch 17/80: current_loss=10.36056 | best_loss=10.21784
Epoch 18/80: current_loss=10.38265 | best_loss=10.21784
Epoch 19/80: current_loss=10.36835 | best_loss=10.21784
Epoch 20/80: current_loss=10.41313 | best_loss=10.21784
Epoch 21/80: current_loss=10.44373 | best_loss=10.21784
Epoch 22/80: current_loss=10.31106 | best_loss=10.21784
Epoch 23/80: current_loss=10.35906 | best_loss=10.21784
Epoch 24/80: current_loss=10.32982 | best_loss=10.21784
Epoch 25/80: current_loss=10.38995 | best_loss=10.21784
Epoch 26/80: current_loss=10.39932 | best_loss=10.21784
Early Stopping at epoch 26
      explained_var=0.02784 | mse_loss=9.84341
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.94525 | best_loss=7.94525
Epoch 1/80: current_loss=7.51031 | best_loss=7.51031
Epoch 2/80: current_loss=7.58473 | best_loss=7.51031
Epoch 3/80: current_loss=8.16318 | best_loss=7.51031
Epoch 4/80: current_loss=7.59135 | best_loss=7.51031
Epoch 5/80: current_loss=7.68177 | best_loss=7.51031
Epoch 6/80: current_loss=7.77798 | best_loss=7.51031
Epoch 7/80: current_loss=7.67834 | best_loss=7.51031
Epoch 8/80: current_loss=7.60688 | best_loss=7.51031
Epoch 9/80: current_loss=7.56451 | best_loss=7.51031
Epoch 10/80: current_loss=7.58259 | best_loss=7.51031
Epoch 11/80: current_loss=7.78067 | best_loss=7.51031
Epoch 12/80: current_loss=7.54379 | best_loss=7.51031
Epoch 13/80: current_loss=7.59173 | best_loss=7.51031
Epoch 14/80: current_loss=7.56431 | best_loss=7.51031
Epoch 15/80: current_loss=7.73055 | best_loss=7.51031
Epoch 16/80: current_loss=7.51704 | best_loss=7.51031
Epoch 17/80: current_loss=7.68648 | best_loss=7.51031
Epoch 18/80: current_loss=7.77126 | best_loss=7.51031
Epoch 19/80: current_loss=7.59406 | best_loss=7.51031
Epoch 20/80: current_loss=7.52085 | best_loss=7.51031
Epoch 21/80: current_loss=7.81210 | best_loss=7.51031
Early Stopping at epoch 21
      explained_var=0.00061 | mse_loss=7.60486

----------------------------------------------
Params for Trial 54
{'learning_rate': 0.001, 'weight_decay': 0.008401701351857927, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.23519 | best_loss=17.23519
Epoch 1/80: current_loss=10.72417 | best_loss=10.72417
Epoch 2/80: current_loss=10.43754 | best_loss=10.43754
Epoch 3/80: current_loss=10.39962 | best_loss=10.39962
Epoch 4/80: current_loss=10.37953 | best_loss=10.37953
Epoch 5/80: current_loss=10.32138 | best_loss=10.32138
Epoch 6/80: current_loss=10.31999 | best_loss=10.31999
Epoch 7/80: current_loss=10.28911 | best_loss=10.28911
Epoch 8/80: current_loss=10.30246 | best_loss=10.28911
Epoch 9/80: current_loss=10.31188 | best_loss=10.28911
Epoch 10/80: current_loss=10.30025 | best_loss=10.28911
Epoch 11/80: current_loss=10.29683 | best_loss=10.28911
Epoch 12/80: current_loss=10.29903 | best_loss=10.28911
Epoch 13/80: current_loss=10.27497 | best_loss=10.27497
Epoch 14/80: current_loss=10.29293 | best_loss=10.27497
Epoch 15/80: current_loss=10.28879 | best_loss=10.27497
Epoch 16/80: current_loss=10.28341 | best_loss=10.27497
Epoch 17/80: current_loss=10.28663 | best_loss=10.27497
Epoch 18/80: current_loss=10.29107 | best_loss=10.27497
Epoch 19/80: current_loss=10.30649 | best_loss=10.27497
Epoch 20/80: current_loss=10.33263 | best_loss=10.27497
Epoch 21/80: current_loss=10.30243 | best_loss=10.27497
Epoch 22/80: current_loss=10.29891 | best_loss=10.27497
Epoch 23/80: current_loss=10.27976 | best_loss=10.27497
Epoch 24/80: current_loss=10.30312 | best_loss=10.27497
Epoch 25/80: current_loss=10.32231 | best_loss=10.27497
Epoch 26/80: current_loss=10.28138 | best_loss=10.27497
Epoch 27/80: current_loss=10.28178 | best_loss=10.27497
Epoch 28/80: current_loss=10.30298 | best_loss=10.27497
Epoch 29/80: current_loss=10.27753 | best_loss=10.27497
Epoch 30/80: current_loss=10.27713 | best_loss=10.27497
Epoch 31/80: current_loss=10.27164 | best_loss=10.27164
Epoch 32/80: current_loss=10.29665 | best_loss=10.27164
Epoch 33/80: current_loss=10.28268 | best_loss=10.27164
Epoch 34/80: current_loss=10.30324 | best_loss=10.27164
Epoch 35/80: current_loss=10.29792 | best_loss=10.27164
Epoch 36/80: current_loss=10.30077 | best_loss=10.27164
Epoch 37/80: current_loss=10.27012 | best_loss=10.27012
Epoch 38/80: current_loss=10.28926 | best_loss=10.27012
Epoch 39/80: current_loss=10.30331 | best_loss=10.27012
Epoch 40/80: current_loss=10.27151 | best_loss=10.27012
Epoch 41/80: current_loss=10.26635 | best_loss=10.26635
Epoch 42/80: current_loss=10.27597 | best_loss=10.26635
Epoch 43/80: current_loss=10.26249 | best_loss=10.26249
Epoch 44/80: current_loss=10.26311 | best_loss=10.26249
Epoch 45/80: current_loss=10.27528 | best_loss=10.26249
Epoch 46/80: current_loss=10.27849 | best_loss=10.26249
Epoch 47/80: current_loss=10.28053 | best_loss=10.26249
Epoch 48/80: current_loss=10.27968 | best_loss=10.26249
Epoch 49/80: current_loss=10.28650 | best_loss=10.26249
Epoch 50/80: current_loss=10.29811 | best_loss=10.26249
Epoch 51/80: current_loss=10.28885 | best_loss=10.26249
Epoch 52/80: current_loss=10.27438 | best_loss=10.26249
Epoch 53/80: current_loss=10.28648 | best_loss=10.26249
Epoch 54/80: current_loss=10.29143 | best_loss=10.26249
Epoch 55/80: current_loss=10.26733 | best_loss=10.26249
Epoch 56/80: current_loss=10.28905 | best_loss=10.26249
Epoch 57/80: current_loss=10.26857 | best_loss=10.26249
Epoch 58/80: current_loss=10.28782 | best_loss=10.26249
Epoch 59/80: current_loss=10.27506 | best_loss=10.26249
Epoch 60/80: current_loss=10.29257 | best_loss=10.26249
Epoch 61/80: current_loss=10.28494 | best_loss=10.26249
Epoch 62/80: current_loss=10.29287 | best_loss=10.26249
Epoch 63/80: current_loss=10.32772 | best_loss=10.26249
Early Stopping at epoch 63
      explained_var=-0.00121 | mse_loss=10.48782
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.39984 | best_loss=10.39984
Epoch 1/80: current_loss=10.46955 | best_loss=10.39984
Epoch 2/80: current_loss=10.39970 | best_loss=10.39970
Epoch 3/80: current_loss=10.41200 | best_loss=10.39970
Epoch 4/80: current_loss=10.38256 | best_loss=10.38256
Epoch 5/80: current_loss=10.47179 | best_loss=10.38256
Epoch 6/80: current_loss=10.41608 | best_loss=10.38256
Epoch 7/80: current_loss=10.42726 | best_loss=10.38256
Epoch 8/80: current_loss=10.39777 | best_loss=10.38256
Epoch 9/80: current_loss=10.46894 | best_loss=10.38256
Epoch 10/80: current_loss=10.44581 | best_loss=10.38256
Epoch 11/80: current_loss=10.34262 | best_loss=10.34262
Epoch 12/80: current_loss=10.46034 | best_loss=10.34262
Epoch 13/80: current_loss=10.38837 | best_loss=10.34262
Epoch 14/80: current_loss=10.71765 | best_loss=10.34262
Epoch 15/80: current_loss=10.45201 | best_loss=10.34262
Epoch 16/80: current_loss=10.41160 | best_loss=10.34262
Epoch 17/80: current_loss=10.40541 | best_loss=10.34262
Epoch 18/80: current_loss=10.40075 | best_loss=10.34262
Epoch 19/80: current_loss=10.46746 | best_loss=10.34262
Epoch 20/80: current_loss=10.39555 | best_loss=10.34262
Epoch 21/80: current_loss=10.41155 | best_loss=10.34262
Epoch 22/80: current_loss=10.42271 | best_loss=10.34262
Epoch 23/80: current_loss=10.36344 | best_loss=10.34262
Epoch 24/80: current_loss=10.39082 | best_loss=10.34262
Epoch 25/80: current_loss=10.59921 | best_loss=10.34262
Epoch 26/80: current_loss=10.41264 | best_loss=10.34262
Epoch 27/80: current_loss=10.42762 | best_loss=10.34262
Epoch 28/80: current_loss=10.42298 | best_loss=10.34262
Epoch 29/80: current_loss=10.42721 | best_loss=10.34262
Epoch 30/80: current_loss=10.43864 | best_loss=10.34262
Epoch 31/80: current_loss=10.43630 | best_loss=10.34262
Early Stopping at epoch 31
      explained_var=0.00661 | mse_loss=9.99695
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.51956 | best_loss=7.51956
Epoch 1/80: current_loss=7.66829 | best_loss=7.51956
Epoch 2/80: current_loss=7.63359 | best_loss=7.51956
Epoch 3/80: current_loss=7.62519 | best_loss=7.51956
Epoch 4/80: current_loss=7.49069 | best_loss=7.49069
Epoch 5/80: current_loss=7.55302 | best_loss=7.49069
Epoch 6/80: current_loss=7.50765 | best_loss=7.49069
Epoch 7/80: current_loss=7.52086 | best_loss=7.49069
Epoch 8/80: current_loss=7.79427 | best_loss=7.49069
Epoch 9/80: current_loss=7.79152 | best_loss=7.49069
Epoch 10/80: current_loss=7.53345 | best_loss=7.49069
Epoch 11/80: current_loss=7.74638 | best_loss=7.49069
Epoch 12/80: current_loss=7.48446 | best_loss=7.48446
Epoch 13/80: current_loss=7.56620 | best_loss=7.48446
Epoch 14/80: current_loss=7.63163 | best_loss=7.48446
Epoch 15/80: current_loss=7.46827 | best_loss=7.46827
Epoch 16/80: current_loss=7.65428 | best_loss=7.46827
Epoch 17/80: current_loss=7.71396 | best_loss=7.46827
Epoch 18/80: current_loss=7.50343 | best_loss=7.46827
Epoch 19/80: current_loss=7.71777 | best_loss=7.46827
Epoch 20/80: current_loss=7.85406 | best_loss=7.46827
Epoch 21/80: current_loss=7.54034 | best_loss=7.46827
Epoch 22/80: current_loss=7.63851 | best_loss=7.46827
Epoch 23/80: current_loss=7.79035 | best_loss=7.46827
Epoch 24/80: current_loss=7.51512 | best_loss=7.46827
Epoch 25/80: current_loss=7.72778 | best_loss=7.46827
Epoch 26/80: current_loss=7.59218 | best_loss=7.46827
Epoch 27/80: current_loss=7.63292 | best_loss=7.46827
Epoch 28/80: current_loss=7.54225 | best_loss=7.46827
Epoch 29/80: current_loss=7.71798 | best_loss=7.46827
Epoch 30/80: current_loss=7.69824 | best_loss=7.46827
Epoch 31/80: current_loss=7.71926 | best_loss=7.46827
Epoch 32/80: current_loss=7.65881 | best_loss=7.46827
Epoch 33/80: current_loss=7.64251 | best_loss=7.46827
Epoch 34/80: current_loss=7.52210 | best_loss=7.46827
Epoch 35/80: current_loss=7.53044 | best_loss=7.46827
Early Stopping at epoch 35
      explained_var=0.00211 | mse_loss=7.57065
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.88891 | best_loss=8.88891
Epoch 1/80: current_loss=8.81011 | best_loss=8.81011
Epoch 2/80: current_loss=9.02663 | best_loss=8.81011
Epoch 3/80: current_loss=8.80085 | best_loss=8.80085
Epoch 4/80: current_loss=8.81434 | best_loss=8.80085
Epoch 5/80: current_loss=8.78532 | best_loss=8.78532
Epoch 6/80: current_loss=8.72885 | best_loss=8.72885
Epoch 7/80: current_loss=8.91821 | best_loss=8.72885
Epoch 8/80: current_loss=8.75992 | best_loss=8.72885
Epoch 9/80: current_loss=8.78088 | best_loss=8.72885
Epoch 10/80: current_loss=8.84025 | best_loss=8.72885
Epoch 11/80: current_loss=8.76883 | best_loss=8.72885
Epoch 12/80: current_loss=9.18200 | best_loss=8.72885
Epoch 13/80: current_loss=8.87537 | best_loss=8.72885
Epoch 14/80: current_loss=8.94302 | best_loss=8.72885
Epoch 15/80: current_loss=8.82082 | best_loss=8.72885
Epoch 16/80: current_loss=8.82557 | best_loss=8.72885
Epoch 17/80: current_loss=8.71203 | best_loss=8.71203
Epoch 18/80: current_loss=8.75293 | best_loss=8.71203
Epoch 19/80: current_loss=8.81337 | best_loss=8.71203
Epoch 20/80: current_loss=8.77776 | best_loss=8.71203
Epoch 21/80: current_loss=8.80437 | best_loss=8.71203
Epoch 22/80: current_loss=8.77889 | best_loss=8.71203
Epoch 23/80: current_loss=8.80346 | best_loss=8.71203
Epoch 24/80: current_loss=8.82564 | best_loss=8.71203
Epoch 25/80: current_loss=8.77250 | best_loss=8.71203
Epoch 26/80: current_loss=8.86103 | best_loss=8.71203
Epoch 27/80: current_loss=8.77778 | best_loss=8.71203
Epoch 28/80: current_loss=8.85022 | best_loss=8.71203
Epoch 29/80: current_loss=8.78860 | best_loss=8.71203
Epoch 30/80: current_loss=8.83667 | best_loss=8.71203
Epoch 31/80: current_loss=8.84068 | best_loss=8.71203
Epoch 32/80: current_loss=8.79431 | best_loss=8.71203
Epoch 33/80: current_loss=9.07541 | best_loss=8.71203
Epoch 34/80: current_loss=8.83128 | best_loss=8.71203
Epoch 35/80: current_loss=8.87546 | best_loss=8.71203
Epoch 36/80: current_loss=8.79983 | best_loss=8.71203
Epoch 37/80: current_loss=8.86742 | best_loss=8.71203
Early Stopping at epoch 37
      explained_var=0.00572 | mse_loss=8.66669
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.32176 | best_loss=9.32176
Epoch 1/80: current_loss=9.19961 | best_loss=9.19961
Epoch 2/80: current_loss=9.22838 | best_loss=9.19961
Epoch 3/80: current_loss=9.16709 | best_loss=9.16709
Epoch 4/80: current_loss=9.27973 | best_loss=9.16709
Epoch 5/80: current_loss=9.19237 | best_loss=9.16709
Epoch 6/80: current_loss=9.25960 | best_loss=9.16709
Epoch 7/80: current_loss=9.16274 | best_loss=9.16274
Epoch 8/80: current_loss=9.23583 | best_loss=9.16274
Epoch 9/80: current_loss=9.31776 | best_loss=9.16274
Epoch 10/80: current_loss=9.22160 | best_loss=9.16274
Epoch 11/80: current_loss=9.16690 | best_loss=9.16274
Epoch 12/80: current_loss=9.15484 | best_loss=9.15484
Epoch 13/80: current_loss=9.21267 | best_loss=9.15484
Epoch 14/80: current_loss=9.16540 | best_loss=9.15484
Epoch 15/80: current_loss=9.25998 | best_loss=9.15484
Epoch 16/80: current_loss=9.21469 | best_loss=9.15484
Epoch 17/80: current_loss=9.20895 | best_loss=9.15484
Epoch 18/80: current_loss=9.20030 | best_loss=9.15484
Epoch 19/80: current_loss=9.16444 | best_loss=9.15484
Epoch 20/80: current_loss=9.19471 | best_loss=9.15484
Epoch 21/80: current_loss=9.22109 | best_loss=9.15484
Epoch 22/80: current_loss=9.27125 | best_loss=9.15484
Epoch 23/80: current_loss=9.20762 | best_loss=9.15484
Epoch 24/80: current_loss=9.21018 | best_loss=9.15484
Epoch 25/80: current_loss=9.15750 | best_loss=9.15484
Epoch 26/80: current_loss=9.28936 | best_loss=9.15484
Epoch 27/80: current_loss=9.16331 | best_loss=9.15484
Epoch 28/80: current_loss=9.15871 | best_loss=9.15484
Epoch 29/80: current_loss=9.23937 | best_loss=9.15484
Epoch 30/80: current_loss=9.19100 | best_loss=9.15484
Epoch 31/80: current_loss=9.25536 | best_loss=9.15484
Epoch 32/80: current_loss=9.18354 | best_loss=9.15484
Early Stopping at epoch 32
      explained_var=0.02070 | mse_loss=9.10663
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00679| avg_loss=9.16575
----------------------------------------------


----------------------------------------------
Params for Trial 55
{'learning_rate': 1e-05, 'weight_decay': 0.00964997076737159, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=102.79954 | best_loss=102.79954
Epoch 1/80: current_loss=101.43238 | best_loss=101.43238
Epoch 2/80: current_loss=99.91247 | best_loss=99.91247
Epoch 3/80: current_loss=98.12813 | best_loss=98.12813
Epoch 4/80: current_loss=95.92996 | best_loss=95.92996
Epoch 5/80: current_loss=93.13803 | best_loss=93.13803
Epoch 6/80: current_loss=89.52889 | best_loss=89.52889
Epoch 7/80: current_loss=84.96158 | best_loss=84.96158
Epoch 8/80: current_loss=79.50323 | best_loss=79.50323
Epoch 9/80: current_loss=73.69319 | best_loss=73.69319
Epoch 10/80: current_loss=68.00713 | best_loss=68.00713
Epoch 11/80: current_loss=62.98348 | best_loss=62.98348
Epoch 12/80: current_loss=58.59639 | best_loss=58.59639
Epoch 13/80: current_loss=54.93675 | best_loss=54.93675
Epoch 14/80: current_loss=51.75948 | best_loss=51.75948
Epoch 15/80: current_loss=49.09088 | best_loss=49.09088
Epoch 16/80: current_loss=46.71939 | best_loss=46.71939
Epoch 17/80: current_loss=44.64610 | best_loss=44.64610
Epoch 18/80: current_loss=42.83097 | best_loss=42.83097
Epoch 19/80: current_loss=41.21611 | best_loss=41.21611
Epoch 20/80: current_loss=39.75492 | best_loss=39.75492
Epoch 21/80: current_loss=38.43483 | best_loss=38.43483
Epoch 22/80: current_loss=37.24569 | best_loss=37.24569
Epoch 23/80: current_loss=36.15673 | best_loss=36.15673
Epoch 24/80: current_loss=35.16326 | best_loss=35.16326
Epoch 25/80: current_loss=34.24739 | best_loss=34.24739
Epoch 26/80: current_loss=33.41682 | best_loss=33.41682
Epoch 27/80: current_loss=32.65899 | best_loss=32.65899
Epoch 28/80: current_loss=31.94925 | best_loss=31.94925
Epoch 29/80: current_loss=31.29326 | best_loss=31.29326
Epoch 30/80: current_loss=30.67718 | best_loss=30.67718
Epoch 31/80: current_loss=30.09499 | best_loss=30.09499
Epoch 32/80: current_loss=29.55300 | best_loss=29.55300
Epoch 33/80: current_loss=29.04020 | best_loss=29.04020
Epoch 34/80: current_loss=28.55152 | best_loss=28.55152
Epoch 35/80: current_loss=28.09049 | best_loss=28.09049
Epoch 36/80: current_loss=27.65503 | best_loss=27.65503
Epoch 37/80: current_loss=27.24147 | best_loss=27.24147
Epoch 38/80: current_loss=26.84616 | best_loss=26.84616
Epoch 39/80: current_loss=26.47047 | best_loss=26.47047
Epoch 40/80: current_loss=26.10893 | best_loss=26.10893
Epoch 41/80: current_loss=25.76430 | best_loss=25.76430
Epoch 42/80: current_loss=25.42739 | best_loss=25.42739
Epoch 43/80: current_loss=25.10293 | best_loss=25.10293
Epoch 44/80: current_loss=24.79706 | best_loss=24.79706
Epoch 45/80: current_loss=24.49811 | best_loss=24.49811
Epoch 46/80: current_loss=24.20358 | best_loss=24.20358
Epoch 47/80: current_loss=23.92475 | best_loss=23.92475
Epoch 48/80: current_loss=23.65647 | best_loss=23.65647
Epoch 49/80: current_loss=23.39184 | best_loss=23.39184
Epoch 50/80: current_loss=23.12962 | best_loss=23.12962
Epoch 51/80: current_loss=22.87504 | best_loss=22.87504
Epoch 52/80: current_loss=22.62927 | best_loss=22.62927
Epoch 53/80: current_loss=22.39517 | best_loss=22.39517
Epoch 54/80: current_loss=22.16408 | best_loss=22.16408
Epoch 55/80: current_loss=21.93563 | best_loss=21.93563
Epoch 56/80: current_loss=21.71869 | best_loss=21.71869
Epoch 57/80: current_loss=21.50055 | best_loss=21.50055
Epoch 58/80: current_loss=21.29545 | best_loss=21.29545
Epoch 59/80: current_loss=21.08996 | best_loss=21.08996
Epoch 60/80: current_loss=20.88651 | best_loss=20.88651
Epoch 61/80: current_loss=20.69328 | best_loss=20.69328
Epoch 62/80: current_loss=20.50224 | best_loss=20.50224
Epoch 63/80: current_loss=20.31210 | best_loss=20.31210
Epoch 64/80: current_loss=20.12620 | best_loss=20.12620
Epoch 65/80: current_loss=19.94953 | best_loss=19.94953
Epoch 66/80: current_loss=19.77565 | best_loss=19.77565
Epoch 67/80: current_loss=19.60544 | best_loss=19.60544
Epoch 68/80: current_loss=19.43350 | best_loss=19.43350
Epoch 69/80: current_loss=19.26737 | best_loss=19.26737
Epoch 70/80: current_loss=19.10557 | best_loss=19.10557
Epoch 71/80: current_loss=18.94008 | best_loss=18.94008
Epoch 72/80: current_loss=18.78458 | best_loss=18.78458
Epoch 73/80: current_loss=18.62934 | best_loss=18.62934
Epoch 74/80: current_loss=18.48060 | best_loss=18.48060
Epoch 75/80: current_loss=18.32716 | best_loss=18.32716
Epoch 76/80: current_loss=18.18428 | best_loss=18.18428
Epoch 77/80: current_loss=18.04025 | best_loss=18.04025
Epoch 78/80: current_loss=17.89519 | best_loss=17.89519
Epoch 79/80: current_loss=17.75825 | best_loss=17.75825
      explained_var=-0.13609 | mse_loss=18.27102

----------------------------------------------
Params for Trial 56
{'learning_rate': 0.001, 'weight_decay': 0.006631842465431322, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=19.39343 | best_loss=19.39343
Epoch 1/80: current_loss=11.91745 | best_loss=11.91745
Epoch 2/80: current_loss=11.44387 | best_loss=11.44387
Epoch 3/80: current_loss=11.23922 | best_loss=11.23922
Epoch 4/80: current_loss=11.00711 | best_loss=11.00711
Epoch 5/80: current_loss=10.84258 | best_loss=10.84258
Epoch 6/80: current_loss=10.70742 | best_loss=10.70742
Epoch 7/80: current_loss=10.64332 | best_loss=10.64332
Epoch 8/80: current_loss=10.58352 | best_loss=10.58352
Epoch 9/80: current_loss=10.50825 | best_loss=10.50825
Epoch 10/80: current_loss=10.47151 | best_loss=10.47151
Epoch 11/80: current_loss=10.45821 | best_loss=10.45821
Epoch 12/80: current_loss=10.41774 | best_loss=10.41774
Epoch 13/80: current_loss=10.46076 | best_loss=10.41774
Epoch 14/80: current_loss=10.38437 | best_loss=10.38437
Epoch 15/80: current_loss=10.37330 | best_loss=10.37330
Epoch 16/80: current_loss=10.37203 | best_loss=10.37203
Epoch 17/80: current_loss=10.40755 | best_loss=10.37203
Epoch 18/80: current_loss=10.34987 | best_loss=10.34987
Epoch 19/80: current_loss=10.35928 | best_loss=10.34987
Epoch 20/80: current_loss=10.37737 | best_loss=10.34987
Epoch 21/80: current_loss=10.34454 | best_loss=10.34454
Epoch 22/80: current_loss=10.35068 | best_loss=10.34454
Epoch 23/80: current_loss=10.32181 | best_loss=10.32181
Epoch 24/80: current_loss=10.34742 | best_loss=10.32181
Epoch 25/80: current_loss=10.32423 | best_loss=10.32181
Epoch 26/80: current_loss=10.36357 | best_loss=10.32181
Epoch 27/80: current_loss=10.33471 | best_loss=10.32181
Epoch 28/80: current_loss=10.35470 | best_loss=10.32181
Epoch 29/80: current_loss=10.36953 | best_loss=10.32181
Epoch 30/80: current_loss=10.33982 | best_loss=10.32181
Epoch 31/80: current_loss=10.35810 | best_loss=10.32181
Epoch 32/80: current_loss=10.33289 | best_loss=10.32181
Epoch 33/80: current_loss=10.40466 | best_loss=10.32181
Epoch 34/80: current_loss=10.33954 | best_loss=10.32181
Epoch 35/80: current_loss=10.34699 | best_loss=10.32181
Epoch 36/80: current_loss=10.35190 | best_loss=10.32181
Epoch 37/80: current_loss=10.37129 | best_loss=10.32181
Epoch 38/80: current_loss=10.32168 | best_loss=10.32168
Epoch 39/80: current_loss=10.36955 | best_loss=10.32168
Epoch 40/80: current_loss=10.30936 | best_loss=10.30936
Epoch 41/80: current_loss=10.38579 | best_loss=10.30936
Epoch 42/80: current_loss=10.29787 | best_loss=10.29787
Epoch 43/80: current_loss=10.32660 | best_loss=10.29787
Epoch 44/80: current_loss=10.30838 | best_loss=10.29787
Epoch 45/80: current_loss=10.33307 | best_loss=10.29787
Epoch 46/80: current_loss=10.34414 | best_loss=10.29787
Epoch 47/80: current_loss=10.31139 | best_loss=10.29787
Epoch 48/80: current_loss=10.30262 | best_loss=10.29787
Epoch 49/80: current_loss=10.36372 | best_loss=10.29787
Epoch 50/80: current_loss=10.31028 | best_loss=10.29787
Epoch 51/80: current_loss=10.33557 | best_loss=10.29787
Epoch 52/80: current_loss=10.36344 | best_loss=10.29787
Epoch 53/80: current_loss=10.32227 | best_loss=10.29787
Epoch 54/80: current_loss=10.32424 | best_loss=10.29787
Epoch 55/80: current_loss=10.34870 | best_loss=10.29787
Epoch 56/80: current_loss=10.31764 | best_loss=10.29787
Epoch 57/80: current_loss=10.38941 | best_loss=10.29787
Epoch 58/80: current_loss=10.30753 | best_loss=10.29787
Epoch 59/80: current_loss=10.30385 | best_loss=10.29787
Epoch 60/80: current_loss=10.34908 | best_loss=10.29787
Epoch 61/80: current_loss=10.34702 | best_loss=10.29787
Epoch 62/80: current_loss=10.33311 | best_loss=10.29787
Early Stopping at epoch 62
      explained_var=-0.00553 | mse_loss=10.53051

----------------------------------------------
Params for Trial 57
{'learning_rate': 0.1, 'weight_decay': 0.009098894933542124, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=96.08663 | best_loss=96.08663
Epoch 1/80: current_loss=43.64039 | best_loss=43.64039
Epoch 2/80: current_loss=15.27558 | best_loss=15.27558
Epoch 3/80: current_loss=22.33585 | best_loss=15.27558
Epoch 4/80: current_loss=13.75574 | best_loss=13.75574
Epoch 5/80: current_loss=13.46361 | best_loss=13.46361
Epoch 6/80: current_loss=35.50810 | best_loss=13.46361
Epoch 7/80: current_loss=15.33107 | best_loss=13.46361
Epoch 8/80: current_loss=14.19429 | best_loss=13.46361
Epoch 9/80: current_loss=38.99761 | best_loss=13.46361
Epoch 10/80: current_loss=23.96472 | best_loss=13.46361
Epoch 11/80: current_loss=16.92874 | best_loss=13.46361
Epoch 12/80: current_loss=11.43465 | best_loss=11.43465
Epoch 13/80: current_loss=11.26338 | best_loss=11.26338
Epoch 14/80: current_loss=11.75693 | best_loss=11.26338
Epoch 15/80: current_loss=15.93409 | best_loss=11.26338
Epoch 16/80: current_loss=35.52622 | best_loss=11.26338
Epoch 17/80: current_loss=19.02972 | best_loss=11.26338
Epoch 18/80: current_loss=10.83747 | best_loss=10.83747
Epoch 19/80: current_loss=25.70627 | best_loss=10.83747
Epoch 20/80: current_loss=14.71972 | best_loss=10.83747
Epoch 21/80: current_loss=13.09784 | best_loss=10.83747
Epoch 22/80: current_loss=21.56842 | best_loss=10.83747
Epoch 23/80: current_loss=16.03885 | best_loss=10.83747
Epoch 24/80: current_loss=13.89055 | best_loss=10.83747
Epoch 25/80: current_loss=14.31777 | best_loss=10.83747
Epoch 26/80: current_loss=11.06123 | best_loss=10.83747
Epoch 27/80: current_loss=11.48435 | best_loss=10.83747
Epoch 28/80: current_loss=18.60328 | best_loss=10.83747
Epoch 29/80: current_loss=13.68294 | best_loss=10.83747
Epoch 30/80: current_loss=24.13375 | best_loss=10.83747
Epoch 31/80: current_loss=16.75503 | best_loss=10.83747
Epoch 32/80: current_loss=15.87315 | best_loss=10.83747
Epoch 33/80: current_loss=17.05658 | best_loss=10.83747
Epoch 34/80: current_loss=17.34508 | best_loss=10.83747
Epoch 35/80: current_loss=16.74436 | best_loss=10.83747
Epoch 36/80: current_loss=19.40162 | best_loss=10.83747
Epoch 37/80: current_loss=33.95116 | best_loss=10.83747
Epoch 38/80: current_loss=39.18915 | best_loss=10.83747
Early Stopping at epoch 38
      explained_var=-0.05567 | mse_loss=11.05577

----------------------------------------------
Params for Trial 58
{'learning_rate': 0.0001, 'weight_decay': 0.0055500554590051186, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=87.77643 | best_loss=87.77643
Epoch 1/80: current_loss=56.89441 | best_loss=56.89441
Epoch 2/80: current_loss=40.62555 | best_loss=40.62555
Epoch 3/80: current_loss=33.22157 | best_loss=33.22157
Epoch 4/80: current_loss=28.64397 | best_loss=28.64397
Epoch 5/80: current_loss=25.30643 | best_loss=25.30643
Epoch 6/80: current_loss=22.84372 | best_loss=22.84372
Epoch 7/80: current_loss=20.80561 | best_loss=20.80561
Epoch 8/80: current_loss=19.14940 | best_loss=19.14940
Epoch 9/80: current_loss=17.78434 | best_loss=17.78434
Epoch 10/80: current_loss=16.63497 | best_loss=16.63497
Epoch 11/80: current_loss=15.68634 | best_loss=15.68634
Epoch 12/80: current_loss=14.85048 | best_loss=14.85048
Epoch 13/80: current_loss=14.17046 | best_loss=14.17046
Epoch 14/80: current_loss=13.58589 | best_loss=13.58589
Epoch 15/80: current_loss=13.11920 | best_loss=13.11920
Epoch 16/80: current_loss=12.70894 | best_loss=12.70894
Epoch 17/80: current_loss=12.35589 | best_loss=12.35589
Epoch 18/80: current_loss=12.06260 | best_loss=12.06260
Epoch 19/80: current_loss=11.81989 | best_loss=11.81989
Epoch 20/80: current_loss=11.60645 | best_loss=11.60645
Epoch 21/80: current_loss=11.41911 | best_loss=11.41911
Epoch 22/80: current_loss=11.29064 | best_loss=11.29064
Epoch 23/80: current_loss=11.17115 | best_loss=11.17115
Epoch 24/80: current_loss=11.06374 | best_loss=11.06374
Epoch 25/80: current_loss=10.96555 | best_loss=10.96555
Epoch 26/80: current_loss=10.89504 | best_loss=10.89504
Epoch 27/80: current_loss=10.83821 | best_loss=10.83821
Epoch 28/80: current_loss=10.78832 | best_loss=10.78832
Epoch 29/80: current_loss=10.73231 | best_loss=10.73231
Epoch 30/80: current_loss=10.69986 | best_loss=10.69986
Epoch 31/80: current_loss=10.65580 | best_loss=10.65580
Epoch 32/80: current_loss=10.63206 | best_loss=10.63206
Epoch 33/80: current_loss=10.60246 | best_loss=10.60246
Epoch 34/80: current_loss=10.57542 | best_loss=10.57542
Epoch 35/80: current_loss=10.55056 | best_loss=10.55056
Epoch 36/80: current_loss=10.52405 | best_loss=10.52405
Epoch 37/80: current_loss=10.50004 | best_loss=10.50004
Epoch 38/80: current_loss=10.49164 | best_loss=10.49164
Epoch 39/80: current_loss=10.47592 | best_loss=10.47592
Epoch 40/80: current_loss=10.46254 | best_loss=10.46254
Epoch 41/80: current_loss=10.44846 | best_loss=10.44846
Epoch 42/80: current_loss=10.44100 | best_loss=10.44100
Epoch 43/80: current_loss=10.43329 | best_loss=10.43329
Epoch 44/80: current_loss=10.42650 | best_loss=10.42650
Epoch 45/80: current_loss=10.42083 | best_loss=10.42083
Epoch 46/80: current_loss=10.41019 | best_loss=10.41019
Epoch 47/80: current_loss=10.40306 | best_loss=10.40306
Epoch 48/80: current_loss=10.39771 | best_loss=10.39771
Epoch 49/80: current_loss=10.39502 | best_loss=10.39502
Epoch 50/80: current_loss=10.38919 | best_loss=10.38919
Epoch 51/80: current_loss=10.38528 | best_loss=10.38528
Epoch 52/80: current_loss=10.38210 | best_loss=10.38210
Epoch 53/80: current_loss=10.37793 | best_loss=10.37793
Epoch 54/80: current_loss=10.37764 | best_loss=10.37764
Epoch 55/80: current_loss=10.36680 | best_loss=10.36680
Epoch 56/80: current_loss=10.36572 | best_loss=10.36572
Epoch 57/80: current_loss=10.35587 | best_loss=10.35587
Epoch 58/80: current_loss=10.35258 | best_loss=10.35258
Epoch 59/80: current_loss=10.34820 | best_loss=10.34820
Epoch 60/80: current_loss=10.34439 | best_loss=10.34439
Epoch 61/80: current_loss=10.33843 | best_loss=10.33843
Epoch 62/80: current_loss=10.34282 | best_loss=10.33843
Epoch 63/80: current_loss=10.33741 | best_loss=10.33741
Epoch 64/80: current_loss=10.33706 | best_loss=10.33706
Epoch 65/80: current_loss=10.33606 | best_loss=10.33606
Epoch 66/80: current_loss=10.33139 | best_loss=10.33139
Epoch 67/80: current_loss=10.33023 | best_loss=10.33023
Epoch 68/80: current_loss=10.32774 | best_loss=10.32774
Epoch 69/80: current_loss=10.32493 | best_loss=10.32493
Epoch 70/80: current_loss=10.32703 | best_loss=10.32493
Epoch 71/80: current_loss=10.32503 | best_loss=10.32493
Epoch 72/80: current_loss=10.32472 | best_loss=10.32472
Epoch 73/80: current_loss=10.32319 | best_loss=10.32319
Epoch 74/80: current_loss=10.32114 | best_loss=10.32114
Epoch 75/80: current_loss=10.31667 | best_loss=10.31667
Epoch 76/80: current_loss=10.31548 | best_loss=10.31548
Epoch 77/80: current_loss=10.31543 | best_loss=10.31543
Epoch 78/80: current_loss=10.31216 | best_loss=10.31216
Epoch 79/80: current_loss=10.31230 | best_loss=10.31216
      explained_var=-0.00590 | mse_loss=10.55391

----------------------------------------------
Params for Trial 59
{'learning_rate': 0.001, 'weight_decay': 0.0099510797490921, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=19.77012 | best_loss=19.77012
Epoch 1/80: current_loss=12.01458 | best_loss=12.01458
Epoch 2/80: current_loss=11.46302 | best_loss=11.46302
Epoch 3/80: current_loss=11.23435 | best_loss=11.23435
Epoch 4/80: current_loss=11.02968 | best_loss=11.02968
Epoch 5/80: current_loss=10.84594 | best_loss=10.84594
Epoch 6/80: current_loss=10.74023 | best_loss=10.74023
Epoch 7/80: current_loss=10.61821 | best_loss=10.61821
Epoch 8/80: current_loss=10.57358 | best_loss=10.57358
Epoch 9/80: current_loss=10.49387 | best_loss=10.49387
Epoch 10/80: current_loss=10.48662 | best_loss=10.48662
Epoch 11/80: current_loss=10.42082 | best_loss=10.42082
Epoch 12/80: current_loss=10.44750 | best_loss=10.42082
Epoch 13/80: current_loss=10.39523 | best_loss=10.39523
Epoch 14/80: current_loss=10.38545 | best_loss=10.38545
Epoch 15/80: current_loss=10.35859 | best_loss=10.35859
Epoch 16/80: current_loss=10.41648 | best_loss=10.35859
Epoch 17/80: current_loss=10.35889 | best_loss=10.35859
Epoch 18/80: current_loss=10.33313 | best_loss=10.33313
Epoch 19/80: current_loss=10.36279 | best_loss=10.33313
Epoch 20/80: current_loss=10.34992 | best_loss=10.33313
Epoch 21/80: current_loss=10.33973 | best_loss=10.33313
Epoch 22/80: current_loss=10.37706 | best_loss=10.33313
Epoch 23/80: current_loss=10.33011 | best_loss=10.33011
Epoch 24/80: current_loss=10.34350 | best_loss=10.33011
Epoch 25/80: current_loss=10.34619 | best_loss=10.33011
Epoch 26/80: current_loss=10.31589 | best_loss=10.31589
Epoch 27/80: current_loss=10.37727 | best_loss=10.31589
Epoch 28/80: current_loss=10.31400 | best_loss=10.31400
Epoch 29/80: current_loss=10.34443 | best_loss=10.31400
Epoch 30/80: current_loss=10.32625 | best_loss=10.31400
Epoch 31/80: current_loss=10.30318 | best_loss=10.30318
Epoch 32/80: current_loss=10.30707 | best_loss=10.30318
Epoch 33/80: current_loss=10.31471 | best_loss=10.30318
Epoch 34/80: current_loss=10.31580 | best_loss=10.30318
Epoch 35/80: current_loss=10.31565 | best_loss=10.30318
Epoch 36/80: current_loss=10.30492 | best_loss=10.30318
Epoch 37/80: current_loss=10.31077 | best_loss=10.30318
Epoch 38/80: current_loss=10.31317 | best_loss=10.30318
Epoch 39/80: current_loss=10.33763 | best_loss=10.30318
Epoch 40/80: current_loss=10.36450 | best_loss=10.30318
Epoch 41/80: current_loss=10.29493 | best_loss=10.29493
Epoch 42/80: current_loss=10.31489 | best_loss=10.29493
Epoch 43/80: current_loss=10.32959 | best_loss=10.29493
Epoch 44/80: current_loss=10.31306 | best_loss=10.29493
Epoch 45/80: current_loss=10.33009 | best_loss=10.29493
Epoch 46/80: current_loss=10.32739 | best_loss=10.29493
Epoch 47/80: current_loss=10.30652 | best_loss=10.29493
Epoch 48/80: current_loss=10.32862 | best_loss=10.29493
Epoch 49/80: current_loss=10.30708 | best_loss=10.29493
Epoch 50/80: current_loss=10.35726 | best_loss=10.29493
Epoch 51/80: current_loss=10.31502 | best_loss=10.29493
Epoch 52/80: current_loss=10.34112 | best_loss=10.29493
Epoch 53/80: current_loss=10.30819 | best_loss=10.29493
Epoch 54/80: current_loss=10.31829 | best_loss=10.29493
Epoch 55/80: current_loss=10.32957 | best_loss=10.29493
Epoch 56/80: current_loss=10.31489 | best_loss=10.29493
Epoch 57/80: current_loss=10.31018 | best_loss=10.29493
Epoch 58/80: current_loss=10.33831 | best_loss=10.29493
Epoch 59/80: current_loss=10.31992 | best_loss=10.29493
Epoch 60/80: current_loss=10.31013 | best_loss=10.29493
Epoch 61/80: current_loss=10.29748 | best_loss=10.29493
Early Stopping at epoch 61
      explained_var=-0.00515 | mse_loss=10.52742

----------------------------------------------
Params for Trial 60
{'learning_rate': 0.01, 'weight_decay': 0.007247655692005512, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=11.70826 | best_loss=11.70826
Epoch 1/80: current_loss=10.50323 | best_loss=10.50323
Epoch 2/80: current_loss=10.35138 | best_loss=10.35138
Epoch 3/80: current_loss=10.29285 | best_loss=10.29285
Epoch 4/80: current_loss=10.27777 | best_loss=10.27777
Epoch 5/80: current_loss=10.28212 | best_loss=10.27777
Epoch 6/80: current_loss=10.31343 | best_loss=10.27777
Epoch 7/80: current_loss=10.31218 | best_loss=10.27777
Epoch 8/80: current_loss=10.26192 | best_loss=10.26192
Epoch 9/80: current_loss=10.41578 | best_loss=10.26192
Epoch 10/80: current_loss=10.29053 | best_loss=10.26192
Epoch 11/80: current_loss=10.27540 | best_loss=10.26192
Epoch 12/80: current_loss=10.27813 | best_loss=10.26192
Epoch 13/80: current_loss=10.26288 | best_loss=10.26192
Epoch 14/80: current_loss=10.29321 | best_loss=10.26192
Epoch 15/80: current_loss=10.39912 | best_loss=10.26192
Epoch 16/80: current_loss=10.32390 | best_loss=10.26192
Epoch 17/80: current_loss=10.26600 | best_loss=10.26192
Epoch 18/80: current_loss=10.27556 | best_loss=10.26192
Epoch 19/80: current_loss=10.26821 | best_loss=10.26192
Epoch 20/80: current_loss=10.28408 | best_loss=10.26192
Epoch 21/80: current_loss=10.26540 | best_loss=10.26192
Epoch 22/80: current_loss=10.32243 | best_loss=10.26192
Epoch 23/80: current_loss=10.31146 | best_loss=10.26192
Epoch 24/80: current_loss=10.40815 | best_loss=10.26192
Epoch 25/80: current_loss=10.46260 | best_loss=10.26192
Epoch 26/80: current_loss=10.26460 | best_loss=10.26192
Epoch 27/80: current_loss=10.33163 | best_loss=10.26192
Epoch 28/80: current_loss=10.29241 | best_loss=10.26192
Early Stopping at epoch 28
      explained_var=-0.00068 | mse_loss=10.48880
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.66254 | best_loss=10.66254
Epoch 1/80: current_loss=10.42203 | best_loss=10.42203
Epoch 2/80: current_loss=10.40416 | best_loss=10.40416
Epoch 3/80: current_loss=10.63563 | best_loss=10.40416
Epoch 4/80: current_loss=10.45954 | best_loss=10.40416
Epoch 5/80: current_loss=10.49267 | best_loss=10.40416
Epoch 6/80: current_loss=10.66695 | best_loss=10.40416
Epoch 7/80: current_loss=10.46943 | best_loss=10.40416
Epoch 8/80: current_loss=10.40659 | best_loss=10.40416
Epoch 9/80: current_loss=10.58245 | best_loss=10.40416
Epoch 10/80: current_loss=10.46490 | best_loss=10.40416
Epoch 11/80: current_loss=10.45048 | best_loss=10.40416
Epoch 12/80: current_loss=10.44115 | best_loss=10.40416
Epoch 13/80: current_loss=10.42247 | best_loss=10.40416
Epoch 14/80: current_loss=10.51154 | best_loss=10.40416
Epoch 15/80: current_loss=10.42110 | best_loss=10.40416
Epoch 16/80: current_loss=10.42907 | best_loss=10.40416
Epoch 17/80: current_loss=10.52614 | best_loss=10.40416
Epoch 18/80: current_loss=10.41276 | best_loss=10.40416
Epoch 19/80: current_loss=10.89244 | best_loss=10.40416
Epoch 20/80: current_loss=10.42535 | best_loss=10.40416
Epoch 21/80: current_loss=10.42134 | best_loss=10.40416
Epoch 22/80: current_loss=10.40694 | best_loss=10.40416
Early Stopping at epoch 22
      explained_var=0.00026 | mse_loss=10.06089

----------------------------------------------
Params for Trial 61
{'learning_rate': 0.001, 'weight_decay': 0.008412746068045606, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.41763 | best_loss=16.41763
Epoch 1/80: current_loss=10.67661 | best_loss=10.67661
Epoch 2/80: current_loss=10.39669 | best_loss=10.39669
Epoch 3/80: current_loss=10.35188 | best_loss=10.35188
Epoch 4/80: current_loss=10.31423 | best_loss=10.31423
Epoch 5/80: current_loss=10.30819 | best_loss=10.30819
Epoch 6/80: current_loss=10.36308 | best_loss=10.30819
Epoch 7/80: current_loss=10.29950 | best_loss=10.29950
Epoch 8/80: current_loss=10.30606 | best_loss=10.29950
Epoch 9/80: current_loss=10.30737 | best_loss=10.29950
Epoch 10/80: current_loss=10.29699 | best_loss=10.29699
Epoch 11/80: current_loss=10.29981 | best_loss=10.29699
Epoch 12/80: current_loss=10.32933 | best_loss=10.29699
Epoch 13/80: current_loss=10.28960 | best_loss=10.28960
Epoch 14/80: current_loss=10.30427 | best_loss=10.28960
Epoch 15/80: current_loss=10.28318 | best_loss=10.28318
Epoch 16/80: current_loss=10.31044 | best_loss=10.28318
Epoch 17/80: current_loss=10.29029 | best_loss=10.28318
Epoch 18/80: current_loss=10.28879 | best_loss=10.28318
Epoch 19/80: current_loss=10.28355 | best_loss=10.28318
Epoch 20/80: current_loss=10.27083 | best_loss=10.27083
Epoch 21/80: current_loss=10.32641 | best_loss=10.27083
Epoch 22/80: current_loss=10.27897 | best_loss=10.27083
Epoch 23/80: current_loss=10.30220 | best_loss=10.27083
Epoch 24/80: current_loss=10.28802 | best_loss=10.27083
Epoch 25/80: current_loss=10.31381 | best_loss=10.27083
Epoch 26/80: current_loss=10.30177 | best_loss=10.27083
Epoch 27/80: current_loss=10.31440 | best_loss=10.27083
Epoch 28/80: current_loss=10.30167 | best_loss=10.27083
Epoch 29/80: current_loss=10.32017 | best_loss=10.27083
Epoch 30/80: current_loss=10.31344 | best_loss=10.27083
Epoch 31/80: current_loss=10.36093 | best_loss=10.27083
Epoch 32/80: current_loss=10.29060 | best_loss=10.27083
Epoch 33/80: current_loss=10.28517 | best_loss=10.27083
Epoch 34/80: current_loss=10.28885 | best_loss=10.27083
Epoch 35/80: current_loss=10.32765 | best_loss=10.27083
Epoch 36/80: current_loss=10.30972 | best_loss=10.27083
Epoch 37/80: current_loss=10.28957 | best_loss=10.27083
Epoch 38/80: current_loss=10.31362 | best_loss=10.27083
Epoch 39/80: current_loss=10.28046 | best_loss=10.27083
Epoch 40/80: current_loss=10.29599 | best_loss=10.27083
Early Stopping at epoch 40
      explained_var=-0.00290 | mse_loss=10.49199
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.46262 | best_loss=10.46262
Epoch 1/80: current_loss=10.41509 | best_loss=10.41509
Epoch 2/80: current_loss=10.47045 | best_loss=10.41509
Epoch 3/80: current_loss=10.45026 | best_loss=10.41509
Epoch 4/80: current_loss=10.40963 | best_loss=10.40963
Epoch 5/80: current_loss=10.42533 | best_loss=10.40963
Epoch 6/80: current_loss=10.45132 | best_loss=10.40963
Epoch 7/80: current_loss=10.40435 | best_loss=10.40435
Epoch 8/80: current_loss=10.49732 | best_loss=10.40435
Epoch 9/80: current_loss=10.42172 | best_loss=10.40435
Epoch 10/80: current_loss=10.42001 | best_loss=10.40435
Epoch 11/80: current_loss=10.50594 | best_loss=10.40435
Epoch 12/80: current_loss=10.40865 | best_loss=10.40435
Epoch 13/80: current_loss=10.48912 | best_loss=10.40435
Epoch 14/80: current_loss=10.42229 | best_loss=10.40435
Epoch 15/80: current_loss=10.41027 | best_loss=10.40435
Epoch 16/80: current_loss=10.47151 | best_loss=10.40435
Epoch 17/80: current_loss=10.42624 | best_loss=10.40435
Epoch 18/80: current_loss=10.41940 | best_loss=10.40435
Epoch 19/80: current_loss=10.41943 | best_loss=10.40435
Epoch 20/80: current_loss=10.47690 | best_loss=10.40435
Epoch 21/80: current_loss=10.43990 | best_loss=10.40435
Epoch 22/80: current_loss=10.40931 | best_loss=10.40435
Epoch 23/80: current_loss=10.45041 | best_loss=10.40435
Epoch 24/80: current_loss=10.41193 | best_loss=10.40435
Epoch 25/80: current_loss=10.48269 | best_loss=10.40435
Epoch 26/80: current_loss=10.41616 | best_loss=10.40435
Epoch 27/80: current_loss=10.40623 | best_loss=10.40435
Early Stopping at epoch 27
      explained_var=0.00031 | mse_loss=10.06156

----------------------------------------------
Params for Trial 62
{'learning_rate': 0.001, 'weight_decay': 0.008091040914921637, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.78578 | best_loss=17.78578
Epoch 1/80: current_loss=10.84252 | best_loss=10.84252
Epoch 2/80: current_loss=10.45383 | best_loss=10.45383
Epoch 3/80: current_loss=10.40064 | best_loss=10.40064
Epoch 4/80: current_loss=10.36222 | best_loss=10.36222
Epoch 5/80: current_loss=10.37231 | best_loss=10.36222
Epoch 6/80: current_loss=10.33993 | best_loss=10.33993
Epoch 7/80: current_loss=10.34018 | best_loss=10.33993
Epoch 8/80: current_loss=10.30494 | best_loss=10.30494
Epoch 9/80: current_loss=10.29534 | best_loss=10.29534
Epoch 10/80: current_loss=10.32889 | best_loss=10.29534
Epoch 11/80: current_loss=10.29454 | best_loss=10.29454
Epoch 12/80: current_loss=10.31045 | best_loss=10.29454
Epoch 13/80: current_loss=10.30130 | best_loss=10.29454
Epoch 14/80: current_loss=10.30192 | best_loss=10.29454
Epoch 15/80: current_loss=10.29723 | best_loss=10.29454
Epoch 16/80: current_loss=10.31371 | best_loss=10.29454
Epoch 17/80: current_loss=10.33310 | best_loss=10.29454
Epoch 18/80: current_loss=10.31318 | best_loss=10.29454
Epoch 19/80: current_loss=10.29560 | best_loss=10.29454
Epoch 20/80: current_loss=10.29409 | best_loss=10.29409
Epoch 21/80: current_loss=10.28906 | best_loss=10.28906
Epoch 22/80: current_loss=10.29088 | best_loss=10.28906
Epoch 23/80: current_loss=10.30836 | best_loss=10.28906
Epoch 24/80: current_loss=10.27650 | best_loss=10.27650
Epoch 25/80: current_loss=10.30058 | best_loss=10.27650
Epoch 26/80: current_loss=10.33275 | best_loss=10.27650
Epoch 27/80: current_loss=10.28058 | best_loss=10.27650
Epoch 28/80: current_loss=10.30009 | best_loss=10.27650
Epoch 29/80: current_loss=10.28603 | best_loss=10.27650
Epoch 30/80: current_loss=10.32003 | best_loss=10.27650
Epoch 31/80: current_loss=10.26555 | best_loss=10.26555
Epoch 32/80: current_loss=10.28859 | best_loss=10.26555
Epoch 33/80: current_loss=10.26944 | best_loss=10.26555
Epoch 34/80: current_loss=10.26895 | best_loss=10.26555
Epoch 35/80: current_loss=10.28873 | best_loss=10.26555
Epoch 36/80: current_loss=10.27689 | best_loss=10.26555
Epoch 37/80: current_loss=10.31132 | best_loss=10.26555
Epoch 38/80: current_loss=10.28714 | best_loss=10.26555
Epoch 39/80: current_loss=10.28279 | best_loss=10.26555
Epoch 40/80: current_loss=10.29535 | best_loss=10.26555
Epoch 41/80: current_loss=10.28689 | best_loss=10.26555
Epoch 42/80: current_loss=10.27649 | best_loss=10.26555
Epoch 43/80: current_loss=10.26983 | best_loss=10.26555
Epoch 44/80: current_loss=10.27847 | best_loss=10.26555
Epoch 45/80: current_loss=10.30395 | best_loss=10.26555
Epoch 46/80: current_loss=10.29158 | best_loss=10.26555
Epoch 47/80: current_loss=10.29076 | best_loss=10.26555
Epoch 48/80: current_loss=10.31805 | best_loss=10.26555
Epoch 49/80: current_loss=10.26923 | best_loss=10.26555
Epoch 50/80: current_loss=10.33074 | best_loss=10.26555
Epoch 51/80: current_loss=10.27313 | best_loss=10.26555
Early Stopping at epoch 51
      explained_var=-0.00229 | mse_loss=10.48510
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.47900 | best_loss=10.47900
Epoch 1/80: current_loss=10.41546 | best_loss=10.41546
Epoch 2/80: current_loss=10.46630 | best_loss=10.41546
Epoch 3/80: current_loss=10.40468 | best_loss=10.40468
Epoch 4/80: current_loss=10.58272 | best_loss=10.40468
Epoch 5/80: current_loss=10.40384 | best_loss=10.40384
Epoch 6/80: current_loss=10.43114 | best_loss=10.40384
Epoch 7/80: current_loss=10.40202 | best_loss=10.40202
Epoch 8/80: current_loss=10.44429 | best_loss=10.40202
Epoch 9/80: current_loss=10.47482 | best_loss=10.40202
Epoch 10/80: current_loss=10.45953 | best_loss=10.40202
Epoch 11/80: current_loss=10.40518 | best_loss=10.40202
Epoch 12/80: current_loss=10.46770 | best_loss=10.40202
Epoch 13/80: current_loss=10.41286 | best_loss=10.40202
Epoch 14/80: current_loss=10.50551 | best_loss=10.40202
Epoch 15/80: current_loss=10.42563 | best_loss=10.40202
Epoch 16/80: current_loss=10.41477 | best_loss=10.40202
Epoch 17/80: current_loss=10.41328 | best_loss=10.40202
Epoch 18/80: current_loss=10.53748 | best_loss=10.40202
Epoch 19/80: current_loss=10.42206 | best_loss=10.40202
Epoch 20/80: current_loss=10.45434 | best_loss=10.40202
Epoch 21/80: current_loss=10.41206 | best_loss=10.40202
Epoch 22/80: current_loss=10.39223 | best_loss=10.39223
Epoch 23/80: current_loss=10.49085 | best_loss=10.39223
Epoch 24/80: current_loss=10.45805 | best_loss=10.39223
Epoch 25/80: current_loss=10.42038 | best_loss=10.39223
Epoch 26/80: current_loss=10.43228 | best_loss=10.39223
Epoch 27/80: current_loss=10.46059 | best_loss=10.39223
Epoch 28/80: current_loss=10.40341 | best_loss=10.39223
Epoch 29/80: current_loss=10.42378 | best_loss=10.39223
Epoch 30/80: current_loss=10.42967 | best_loss=10.39223
Epoch 31/80: current_loss=10.48463 | best_loss=10.39223
Epoch 32/80: current_loss=10.47357 | best_loss=10.39223
Epoch 33/80: current_loss=10.40457 | best_loss=10.39223
Epoch 34/80: current_loss=10.46795 | best_loss=10.39223
Epoch 35/80: current_loss=10.42664 | best_loss=10.39223
Epoch 36/80: current_loss=10.40847 | best_loss=10.39223
Epoch 37/80: current_loss=10.41082 | best_loss=10.39223
Epoch 38/80: current_loss=10.40275 | best_loss=10.39223
Epoch 39/80: current_loss=10.52046 | best_loss=10.39223
Epoch 40/80: current_loss=10.40571 | best_loss=10.39223
Epoch 41/80: current_loss=10.40583 | best_loss=10.39223
Epoch 42/80: current_loss=10.41761 | best_loss=10.39223
Early Stopping at epoch 42
      explained_var=0.00144 | mse_loss=10.05091

----------------------------------------------
Params for Trial 63
{'learning_rate': 0.001, 'weight_decay': 0.008524608716175415, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=15.15788 | best_loss=15.15788
Epoch 1/80: current_loss=10.67197 | best_loss=10.67197
Epoch 2/80: current_loss=10.46153 | best_loss=10.46153
Epoch 3/80: current_loss=10.38994 | best_loss=10.38994
Epoch 4/80: current_loss=10.39075 | best_loss=10.38994
Epoch 5/80: current_loss=10.36948 | best_loss=10.36948
Epoch 6/80: current_loss=10.33150 | best_loss=10.33150
Epoch 7/80: current_loss=10.31886 | best_loss=10.31886
Epoch 8/80: current_loss=10.32262 | best_loss=10.31886
Epoch 9/80: current_loss=10.30076 | best_loss=10.30076
Epoch 10/80: current_loss=10.31430 | best_loss=10.30076
Epoch 11/80: current_loss=10.29715 | best_loss=10.29715
Epoch 12/80: current_loss=10.31700 | best_loss=10.29715
Epoch 13/80: current_loss=10.29265 | best_loss=10.29265
Epoch 14/80: current_loss=10.31412 | best_loss=10.29265
Epoch 15/80: current_loss=10.31486 | best_loss=10.29265
Epoch 16/80: current_loss=10.30172 | best_loss=10.29265
Epoch 17/80: current_loss=10.30376 | best_loss=10.29265
Epoch 18/80: current_loss=10.31595 | best_loss=10.29265
Epoch 19/80: current_loss=10.28388 | best_loss=10.28388
Epoch 20/80: current_loss=10.33624 | best_loss=10.28388
Epoch 21/80: current_loss=10.29428 | best_loss=10.28388
Epoch 22/80: current_loss=10.28292 | best_loss=10.28292
Epoch 23/80: current_loss=10.27864 | best_loss=10.27864
Epoch 24/80: current_loss=10.28720 | best_loss=10.27864
Epoch 25/80: current_loss=10.27730 | best_loss=10.27730
Epoch 26/80: current_loss=10.28282 | best_loss=10.27730
Epoch 27/80: current_loss=10.27413 | best_loss=10.27413
Epoch 28/80: current_loss=10.28425 | best_loss=10.27413
Epoch 29/80: current_loss=10.28887 | best_loss=10.27413
Epoch 30/80: current_loss=10.31081 | best_loss=10.27413
Epoch 31/80: current_loss=10.33977 | best_loss=10.27413
Epoch 32/80: current_loss=10.32365 | best_loss=10.27413
Epoch 33/80: current_loss=10.27564 | best_loss=10.27413
Epoch 34/80: current_loss=10.28843 | best_loss=10.27413
Epoch 35/80: current_loss=10.31741 | best_loss=10.27413
Epoch 36/80: current_loss=10.27782 | best_loss=10.27413
Epoch 37/80: current_loss=10.29533 | best_loss=10.27413
Epoch 38/80: current_loss=10.27922 | best_loss=10.27413
Epoch 39/80: current_loss=10.27691 | best_loss=10.27413
Epoch 40/80: current_loss=10.29291 | best_loss=10.27413
Epoch 41/80: current_loss=10.28012 | best_loss=10.27413
Epoch 42/80: current_loss=10.30335 | best_loss=10.27413
Epoch 43/80: current_loss=10.29003 | best_loss=10.27413
Epoch 44/80: current_loss=10.28741 | best_loss=10.27413
Epoch 45/80: current_loss=10.28734 | best_loss=10.27413
Epoch 46/80: current_loss=10.27877 | best_loss=10.27413
Epoch 47/80: current_loss=10.28054 | best_loss=10.27413
Early Stopping at epoch 47
      explained_var=-0.00279 | mse_loss=10.50187
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.42183 | best_loss=10.42183
Epoch 1/80: current_loss=10.41834 | best_loss=10.41834
Epoch 2/80: current_loss=10.40878 | best_loss=10.40878
Epoch 3/80: current_loss=10.47873 | best_loss=10.40878
Epoch 4/80: current_loss=10.40429 | best_loss=10.40429
Epoch 5/80: current_loss=10.44375 | best_loss=10.40429
Epoch 6/80: current_loss=10.41079 | best_loss=10.40429
Epoch 7/80: current_loss=10.43476 | best_loss=10.40429
Epoch 8/80: current_loss=10.44081 | best_loss=10.40429
Epoch 9/80: current_loss=10.41392 | best_loss=10.40429
Epoch 10/80: current_loss=10.56914 | best_loss=10.40429
Epoch 11/80: current_loss=10.44668 | best_loss=10.40429
Epoch 12/80: current_loss=10.43948 | best_loss=10.40429
Epoch 13/80: current_loss=10.86674 | best_loss=10.40429
Epoch 14/80: current_loss=10.46653 | best_loss=10.40429
Epoch 15/80: current_loss=10.43992 | best_loss=10.40429
Epoch 16/80: current_loss=10.40765 | best_loss=10.40429
Epoch 17/80: current_loss=10.42968 | best_loss=10.40429
Epoch 18/80: current_loss=10.40722 | best_loss=10.40429
Epoch 19/80: current_loss=10.40800 | best_loss=10.40429
Epoch 20/80: current_loss=10.42003 | best_loss=10.40429
Epoch 21/80: current_loss=10.41803 | best_loss=10.40429
Epoch 22/80: current_loss=10.41747 | best_loss=10.40429
Epoch 23/80: current_loss=10.40569 | best_loss=10.40429
Epoch 24/80: current_loss=10.43573 | best_loss=10.40429
Early Stopping at epoch 24
      explained_var=0.00029 | mse_loss=10.06185

----------------------------------------------
Params for Trial 64
{'learning_rate': 0.001, 'weight_decay': 0.003217604220516162, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.97869 | best_loss=18.97869
Epoch 1/80: current_loss=11.01325 | best_loss=11.01325
Epoch 2/80: current_loss=10.42160 | best_loss=10.42160
Epoch 3/80: current_loss=10.36782 | best_loss=10.36782
Epoch 4/80: current_loss=10.38176 | best_loss=10.36782
Epoch 5/80: current_loss=10.37433 | best_loss=10.36782
Epoch 6/80: current_loss=10.35141 | best_loss=10.35141
Epoch 7/80: current_loss=10.31300 | best_loss=10.31300
Epoch 8/80: current_loss=10.31177 | best_loss=10.31177
Epoch 9/80: current_loss=10.30023 | best_loss=10.30023
Epoch 10/80: current_loss=10.28519 | best_loss=10.28519
Epoch 11/80: current_loss=10.31171 | best_loss=10.28519
Epoch 12/80: current_loss=10.28230 | best_loss=10.28230
Epoch 13/80: current_loss=10.29580 | best_loss=10.28230
Epoch 14/80: current_loss=10.31193 | best_loss=10.28230
Epoch 15/80: current_loss=10.30468 | best_loss=10.28230
Epoch 16/80: current_loss=10.28511 | best_loss=10.28230
Epoch 17/80: current_loss=10.30420 | best_loss=10.28230
Epoch 18/80: current_loss=10.36656 | best_loss=10.28230
Epoch 19/80: current_loss=10.27550 | best_loss=10.27550
Epoch 20/80: current_loss=10.30702 | best_loss=10.27550
Epoch 21/80: current_loss=10.28374 | best_loss=10.27550
Epoch 22/80: current_loss=10.30362 | best_loss=10.27550
Epoch 23/80: current_loss=10.29379 | best_loss=10.27550
Epoch 24/80: current_loss=10.29022 | best_loss=10.27550
Epoch 25/80: current_loss=10.29769 | best_loss=10.27550
Epoch 26/80: current_loss=10.29984 | best_loss=10.27550
Epoch 27/80: current_loss=10.29582 | best_loss=10.27550
Epoch 28/80: current_loss=10.30816 | best_loss=10.27550
Epoch 29/80: current_loss=10.27048 | best_loss=10.27048
Epoch 30/80: current_loss=10.27587 | best_loss=10.27048
Epoch 31/80: current_loss=10.32871 | best_loss=10.27048
Epoch 32/80: current_loss=10.28498 | best_loss=10.27048
Epoch 33/80: current_loss=10.27154 | best_loss=10.27048
Epoch 34/80: current_loss=10.31555 | best_loss=10.27048
Epoch 35/80: current_loss=10.28478 | best_loss=10.27048
Epoch 36/80: current_loss=10.26349 | best_loss=10.26349
Epoch 37/80: current_loss=10.26680 | best_loss=10.26349
Epoch 38/80: current_loss=10.27426 | best_loss=10.26349
Epoch 39/80: current_loss=10.29337 | best_loss=10.26349
Epoch 40/80: current_loss=10.27560 | best_loss=10.26349
Epoch 41/80: current_loss=10.27633 | best_loss=10.26349
Epoch 42/80: current_loss=10.27606 | best_loss=10.26349
Epoch 43/80: current_loss=10.30000 | best_loss=10.26349
Epoch 44/80: current_loss=10.28747 | best_loss=10.26349
Epoch 45/80: current_loss=10.29553 | best_loss=10.26349
Epoch 46/80: current_loss=10.28597 | best_loss=10.26349
Epoch 47/80: current_loss=10.28364 | best_loss=10.26349
Epoch 48/80: current_loss=10.27431 | best_loss=10.26349
Epoch 49/80: current_loss=10.28946 | best_loss=10.26349
Epoch 50/80: current_loss=10.29902 | best_loss=10.26349
Epoch 51/80: current_loss=10.30360 | best_loss=10.26349
Epoch 52/80: current_loss=10.30199 | best_loss=10.26349
Epoch 53/80: current_loss=10.25959 | best_loss=10.25959
Epoch 54/80: current_loss=10.25870 | best_loss=10.25870
Epoch 55/80: current_loss=10.27384 | best_loss=10.25870
Epoch 56/80: current_loss=10.26025 | best_loss=10.25870
Epoch 57/80: current_loss=10.29395 | best_loss=10.25870
Epoch 58/80: current_loss=10.25803 | best_loss=10.25803
Epoch 59/80: current_loss=10.30329 | best_loss=10.25803
Epoch 60/80: current_loss=10.26509 | best_loss=10.25803
Epoch 61/80: current_loss=10.33083 | best_loss=10.25803
Epoch 62/80: current_loss=10.28604 | best_loss=10.25803
Epoch 63/80: current_loss=10.27370 | best_loss=10.25803
Epoch 64/80: current_loss=10.26658 | best_loss=10.25803
Epoch 65/80: current_loss=10.29413 | best_loss=10.25803
Epoch 66/80: current_loss=10.27849 | best_loss=10.25803
Epoch 67/80: current_loss=10.28742 | best_loss=10.25803
Epoch 68/80: current_loss=10.28213 | best_loss=10.25803
Epoch 69/80: current_loss=10.26611 | best_loss=10.25803
Epoch 70/80: current_loss=10.31070 | best_loss=10.25803
Epoch 71/80: current_loss=10.31448 | best_loss=10.25803
Epoch 72/80: current_loss=10.28173 | best_loss=10.25803
Epoch 73/80: current_loss=10.27540 | best_loss=10.25803
Epoch 74/80: current_loss=10.26816 | best_loss=10.25803
Epoch 75/80: current_loss=10.29723 | best_loss=10.25803
Epoch 76/80: current_loss=10.26445 | best_loss=10.25803
Epoch 77/80: current_loss=10.31607 | best_loss=10.25803
Epoch 78/80: current_loss=10.26509 | best_loss=10.25803
Early Stopping at epoch 78
      explained_var=-0.00124 | mse_loss=10.47340
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.44947 | best_loss=10.44947
Epoch 1/80: current_loss=10.42593 | best_loss=10.42593
Epoch 2/80: current_loss=10.40887 | best_loss=10.40887
Epoch 3/80: current_loss=10.43293 | best_loss=10.40887
Epoch 4/80: current_loss=10.42064 | best_loss=10.40887
Epoch 5/80: current_loss=10.45747 | best_loss=10.40887
Epoch 6/80: current_loss=10.40800 | best_loss=10.40800
Epoch 7/80: current_loss=10.45123 | best_loss=10.40800
Epoch 8/80: current_loss=10.40752 | best_loss=10.40752
Epoch 9/80: current_loss=10.43874 | best_loss=10.40752
Epoch 10/80: current_loss=10.40708 | best_loss=10.40708
Epoch 11/80: current_loss=10.40794 | best_loss=10.40708
Epoch 12/80: current_loss=10.45883 | best_loss=10.40708
Epoch 13/80: current_loss=10.41300 | best_loss=10.40708
Epoch 14/80: current_loss=10.40809 | best_loss=10.40708
Epoch 15/80: current_loss=10.50251 | best_loss=10.40708
Epoch 16/80: current_loss=10.40449 | best_loss=10.40449
Epoch 17/80: current_loss=10.45983 | best_loss=10.40449
Epoch 18/80: current_loss=10.42699 | best_loss=10.40449
Epoch 19/80: current_loss=10.42570 | best_loss=10.40449
Epoch 20/80: current_loss=10.45023 | best_loss=10.40449
Epoch 21/80: current_loss=10.42427 | best_loss=10.40449
Epoch 22/80: current_loss=10.40467 | best_loss=10.40449
Epoch 23/80: current_loss=10.41862 | best_loss=10.40449
Epoch 24/80: current_loss=10.41740 | best_loss=10.40449
Epoch 25/80: current_loss=10.40702 | best_loss=10.40449
Epoch 26/80: current_loss=10.42244 | best_loss=10.40449
Epoch 27/80: current_loss=10.40928 | best_loss=10.40449
Epoch 28/80: current_loss=10.44450 | best_loss=10.40449
Epoch 29/80: current_loss=10.41422 | best_loss=10.40449
Epoch 30/80: current_loss=10.41575 | best_loss=10.40449
Epoch 31/80: current_loss=10.41966 | best_loss=10.40449
Epoch 32/80: current_loss=10.42330 | best_loss=10.40449
Epoch 33/80: current_loss=10.40549 | best_loss=10.40449
Epoch 34/80: current_loss=10.40440 | best_loss=10.40440
Epoch 35/80: current_loss=10.41409 | best_loss=10.40440
Epoch 36/80: current_loss=10.43874 | best_loss=10.40440
Epoch 37/80: current_loss=10.44970 | best_loss=10.40440
Epoch 38/80: current_loss=10.40410 | best_loss=10.40410
Epoch 39/80: current_loss=10.44307 | best_loss=10.40410
Epoch 40/80: current_loss=10.40651 | best_loss=10.40410
Epoch 41/80: current_loss=10.43063 | best_loss=10.40410
Epoch 42/80: current_loss=10.40800 | best_loss=10.40410
Epoch 43/80: current_loss=10.41611 | best_loss=10.40410
Epoch 44/80: current_loss=10.43808 | best_loss=10.40410
Epoch 45/80: current_loss=10.40565 | best_loss=10.40410
Epoch 46/80: current_loss=10.44232 | best_loss=10.40410
Epoch 47/80: current_loss=10.40473 | best_loss=10.40410
Epoch 48/80: current_loss=10.45139 | best_loss=10.40410
Epoch 49/80: current_loss=10.40449 | best_loss=10.40410
Epoch 50/80: current_loss=10.40778 | best_loss=10.40410
Epoch 51/80: current_loss=10.42529 | best_loss=10.40410
Epoch 52/80: current_loss=10.40797 | best_loss=10.40410
Epoch 53/80: current_loss=10.41055 | best_loss=10.40410
Epoch 54/80: current_loss=10.40821 | best_loss=10.40410
Epoch 55/80: current_loss=10.40609 | best_loss=10.40410
Epoch 56/80: current_loss=10.40829 | best_loss=10.40410
Epoch 57/80: current_loss=10.41569 | best_loss=10.40410
Epoch 58/80: current_loss=10.41026 | best_loss=10.40410
Early Stopping at epoch 58
      explained_var=0.00026 | mse_loss=10.06096

----------------------------------------------
Params for Trial 65
{'learning_rate': 0.001, 'weight_decay': 0.004638536290968584, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.07243 | best_loss=18.07243
Epoch 1/80: current_loss=10.83231 | best_loss=10.83231
Epoch 2/80: current_loss=10.47667 | best_loss=10.47667
Epoch 3/80: current_loss=10.38554 | best_loss=10.38554
Epoch 4/80: current_loss=10.42071 | best_loss=10.38554
Epoch 5/80: current_loss=10.33243 | best_loss=10.33243
Epoch 6/80: current_loss=10.35504 | best_loss=10.33243
Epoch 7/80: current_loss=10.29890 | best_loss=10.29890
Epoch 8/80: current_loss=10.31904 | best_loss=10.29890
Epoch 9/80: current_loss=10.29921 | best_loss=10.29890
Epoch 10/80: current_loss=10.36823 | best_loss=10.29890
Epoch 11/80: current_loss=10.28337 | best_loss=10.28337
Epoch 12/80: current_loss=10.28986 | best_loss=10.28337
Epoch 13/80: current_loss=10.31070 | best_loss=10.28337
Epoch 14/80: current_loss=10.29638 | best_loss=10.28337
Epoch 15/80: current_loss=10.29248 | best_loss=10.28337
Epoch 16/80: current_loss=10.31090 | best_loss=10.28337
Epoch 17/80: current_loss=10.27727 | best_loss=10.27727
Epoch 18/80: current_loss=10.28643 | best_loss=10.27727
Epoch 19/80: current_loss=10.30883 | best_loss=10.27727
Epoch 20/80: current_loss=10.27652 | best_loss=10.27652
Epoch 21/80: current_loss=10.31536 | best_loss=10.27652
Epoch 22/80: current_loss=10.27903 | best_loss=10.27652
Epoch 23/80: current_loss=10.31481 | best_loss=10.27652
Epoch 24/80: current_loss=10.28443 | best_loss=10.27652
Epoch 25/80: current_loss=10.30427 | best_loss=10.27652
Epoch 26/80: current_loss=10.29710 | best_loss=10.27652
Epoch 27/80: current_loss=10.29042 | best_loss=10.27652
Epoch 28/80: current_loss=10.27381 | best_loss=10.27381
Epoch 29/80: current_loss=10.29318 | best_loss=10.27381
Epoch 30/80: current_loss=10.27296 | best_loss=10.27296
Epoch 31/80: current_loss=10.31641 | best_loss=10.27296
Epoch 32/80: current_loss=10.29673 | best_loss=10.27296
Epoch 33/80: current_loss=10.27559 | best_loss=10.27296
Epoch 34/80: current_loss=10.27149 | best_loss=10.27149
Epoch 35/80: current_loss=10.30336 | best_loss=10.27149
Epoch 36/80: current_loss=10.27013 | best_loss=10.27013
Epoch 37/80: current_loss=10.28927 | best_loss=10.27013
Epoch 38/80: current_loss=10.28394 | best_loss=10.27013
Epoch 39/80: current_loss=10.26414 | best_loss=10.26414
Epoch 40/80: current_loss=10.28700 | best_loss=10.26414
Epoch 41/80: current_loss=10.26464 | best_loss=10.26414
Epoch 42/80: current_loss=10.31128 | best_loss=10.26414
Epoch 43/80: current_loss=10.28078 | best_loss=10.26414
Epoch 44/80: current_loss=10.31851 | best_loss=10.26414
Epoch 45/80: current_loss=10.28464 | best_loss=10.26414
Epoch 46/80: current_loss=10.27284 | best_loss=10.26414
Epoch 47/80: current_loss=10.30583 | best_loss=10.26414
Epoch 48/80: current_loss=10.30867 | best_loss=10.26414
Epoch 49/80: current_loss=10.27697 | best_loss=10.26414
Epoch 50/80: current_loss=10.26692 | best_loss=10.26414
Epoch 51/80: current_loss=10.28690 | best_loss=10.26414
Epoch 52/80: current_loss=10.31772 | best_loss=10.26414
Epoch 53/80: current_loss=10.28448 | best_loss=10.26414
Epoch 54/80: current_loss=10.26761 | best_loss=10.26414
Epoch 55/80: current_loss=10.30053 | best_loss=10.26414
Epoch 56/80: current_loss=10.27944 | best_loss=10.26414
Epoch 57/80: current_loss=10.27166 | best_loss=10.26414
Epoch 58/80: current_loss=10.31666 | best_loss=10.26414
Epoch 59/80: current_loss=10.30757 | best_loss=10.26414
Early Stopping at epoch 59
      explained_var=-0.00193 | mse_loss=10.47666
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.40599 | best_loss=10.40599
Epoch 1/80: current_loss=10.44772 | best_loss=10.40599
Epoch 2/80: current_loss=10.41287 | best_loss=10.40599
Epoch 3/80: current_loss=10.42162 | best_loss=10.40599
Epoch 4/80: current_loss=10.44017 | best_loss=10.40599
Epoch 5/80: current_loss=10.45632 | best_loss=10.40599
Epoch 6/80: current_loss=10.40725 | best_loss=10.40599
Epoch 7/80: current_loss=10.45666 | best_loss=10.40599
Epoch 8/80: current_loss=10.40483 | best_loss=10.40483
Epoch 9/80: current_loss=10.52856 | best_loss=10.40483
Epoch 10/80: current_loss=10.40495 | best_loss=10.40483
Epoch 11/80: current_loss=10.50180 | best_loss=10.40483
Epoch 12/80: current_loss=10.41943 | best_loss=10.40483
Epoch 13/80: current_loss=10.48471 | best_loss=10.40483
Epoch 14/80: current_loss=10.41832 | best_loss=10.40483
Epoch 15/80: current_loss=10.49563 | best_loss=10.40483
Epoch 16/80: current_loss=10.42013 | best_loss=10.40483
Epoch 17/80: current_loss=10.45491 | best_loss=10.40483
Epoch 18/80: current_loss=10.42981 | best_loss=10.40483
Epoch 19/80: current_loss=10.41053 | best_loss=10.40483
Epoch 20/80: current_loss=10.40951 | best_loss=10.40483
Epoch 21/80: current_loss=10.41724 | best_loss=10.40483
Epoch 22/80: current_loss=10.41485 | best_loss=10.40483
Epoch 23/80: current_loss=10.40737 | best_loss=10.40483
Epoch 24/80: current_loss=10.41265 | best_loss=10.40483
Epoch 25/80: current_loss=10.43160 | best_loss=10.40483
Epoch 26/80: current_loss=10.39769 | best_loss=10.39769
Epoch 27/80: current_loss=10.45564 | best_loss=10.39769
Epoch 28/80: current_loss=10.41779 | best_loss=10.39769
Epoch 29/80: current_loss=10.43288 | best_loss=10.39769
Epoch 30/80: current_loss=10.41335 | best_loss=10.39769
Epoch 31/80: current_loss=10.41312 | best_loss=10.39769
Epoch 32/80: current_loss=10.40555 | best_loss=10.39769
Epoch 33/80: current_loss=10.46399 | best_loss=10.39769
Epoch 34/80: current_loss=10.40664 | best_loss=10.39769
Epoch 35/80: current_loss=10.42512 | best_loss=10.39769
Epoch 36/80: current_loss=10.50051 | best_loss=10.39769
Epoch 37/80: current_loss=10.41712 | best_loss=10.39769
Epoch 38/80: current_loss=10.41387 | best_loss=10.39769
Epoch 39/80: current_loss=10.40490 | best_loss=10.39769
Epoch 40/80: current_loss=10.41748 | best_loss=10.39769
Epoch 41/80: current_loss=10.40741 | best_loss=10.39769
Epoch 42/80: current_loss=10.46311 | best_loss=10.39769
Epoch 43/80: current_loss=10.40365 | best_loss=10.39769
Epoch 44/80: current_loss=10.47063 | best_loss=10.39769
Epoch 45/80: current_loss=10.40378 | best_loss=10.39769
Epoch 46/80: current_loss=10.40449 | best_loss=10.39769
Early Stopping at epoch 46
      explained_var=0.00089 | mse_loss=10.05462

----------------------------------------------
Params for Trial 66
{'learning_rate': 0.001, 'weight_decay': 0.009084944421163593, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=27.94807 | best_loss=27.94807
Epoch 1/80: current_loss=14.84651 | best_loss=14.84651
Epoch 2/80: current_loss=13.76381 | best_loss=13.76381
Epoch 3/80: current_loss=13.48601 | best_loss=13.48601
Epoch 4/80: current_loss=13.27631 | best_loss=13.27631
Epoch 5/80: current_loss=13.08402 | best_loss=13.08402
Epoch 6/80: current_loss=12.95023 | best_loss=12.95023
Epoch 7/80: current_loss=12.77447 | best_loss=12.77447
Epoch 8/80: current_loss=12.62683 | best_loss=12.62683
Epoch 9/80: current_loss=12.51255 | best_loss=12.51255
Epoch 10/80: current_loss=12.42619 | best_loss=12.42619
Epoch 11/80: current_loss=12.32444 | best_loss=12.32444
Epoch 12/80: current_loss=12.25382 | best_loss=12.25382
Epoch 13/80: current_loss=12.13585 | best_loss=12.13585
Epoch 14/80: current_loss=12.10002 | best_loss=12.10002
Epoch 15/80: current_loss=12.00906 | best_loss=12.00906
Epoch 16/80: current_loss=11.94061 | best_loss=11.94061
Epoch 17/80: current_loss=11.89205 | best_loss=11.89205
Epoch 18/80: current_loss=11.80911 | best_loss=11.80911
Epoch 19/80: current_loss=11.79711 | best_loss=11.79711
Epoch 20/80: current_loss=11.71566 | best_loss=11.71566
Epoch 21/80: current_loss=11.67963 | best_loss=11.67963
Epoch 22/80: current_loss=11.62307 | best_loss=11.62307
Epoch 23/80: current_loss=11.54696 | best_loss=11.54696
Epoch 24/80: current_loss=11.50215 | best_loss=11.50215
Epoch 25/80: current_loss=11.45207 | best_loss=11.45207
Epoch 26/80: current_loss=11.43794 | best_loss=11.43794
Epoch 27/80: current_loss=11.37842 | best_loss=11.37842
Epoch 28/80: current_loss=11.34445 | best_loss=11.34445
Epoch 29/80: current_loss=11.30536 | best_loss=11.30536
Epoch 30/80: current_loss=11.30583 | best_loss=11.30536
Epoch 31/80: current_loss=11.22138 | best_loss=11.22138
Epoch 32/80: current_loss=11.21933 | best_loss=11.21933
Epoch 33/80: current_loss=11.15443 | best_loss=11.15443
Epoch 34/80: current_loss=11.13146 | best_loss=11.13146
Epoch 35/80: current_loss=11.13128 | best_loss=11.13128
Epoch 36/80: current_loss=11.08595 | best_loss=11.08595
Epoch 37/80: current_loss=10.99157 | best_loss=10.99157
Epoch 38/80: current_loss=11.00967 | best_loss=10.99157
Epoch 39/80: current_loss=10.97927 | best_loss=10.97927
Epoch 40/80: current_loss=10.91320 | best_loss=10.91320
Epoch 41/80: current_loss=10.88664 | best_loss=10.88664
Epoch 42/80: current_loss=10.80014 | best_loss=10.80014
Epoch 43/80: current_loss=10.76970 | best_loss=10.76970
Epoch 44/80: current_loss=10.75569 | best_loss=10.75569
Epoch 45/80: current_loss=10.75057 | best_loss=10.75057
Epoch 46/80: current_loss=10.70644 | best_loss=10.70644
Epoch 47/80: current_loss=10.66916 | best_loss=10.66916
Epoch 48/80: current_loss=10.63628 | best_loss=10.63628
Epoch 49/80: current_loss=10.62117 | best_loss=10.62117
Epoch 50/80: current_loss=10.61541 | best_loss=10.61541
Epoch 51/80: current_loss=10.68899 | best_loss=10.61541
Epoch 52/80: current_loss=10.58910 | best_loss=10.58910
Epoch 53/80: current_loss=10.63705 | best_loss=10.58910
Epoch 54/80: current_loss=10.61148 | best_loss=10.58910
Epoch 55/80: current_loss=10.50282 | best_loss=10.50282
Epoch 56/80: current_loss=10.46675 | best_loss=10.46675
Epoch 57/80: current_loss=10.39312 | best_loss=10.39312
Epoch 58/80: current_loss=10.42565 | best_loss=10.39312
Epoch 59/80: current_loss=10.36690 | best_loss=10.36690
Epoch 60/80: current_loss=10.33457 | best_loss=10.33457
Epoch 61/80: current_loss=10.43186 | best_loss=10.33457
Epoch 62/80: current_loss=10.29044 | best_loss=10.29044
Epoch 63/80: current_loss=10.25518 | best_loss=10.25518
Epoch 64/80: current_loss=10.26549 | best_loss=10.25518
Epoch 65/80: current_loss=10.30292 | best_loss=10.25518
Epoch 66/80: current_loss=10.21831 | best_loss=10.21831
Epoch 67/80: current_loss=10.19918 | best_loss=10.19918
Epoch 68/80: current_loss=10.17110 | best_loss=10.17110
Epoch 69/80: current_loss=10.41120 | best_loss=10.17110
Epoch 70/80: current_loss=10.92387 | best_loss=10.17110
Epoch 71/80: current_loss=10.20005 | best_loss=10.17110
Epoch 72/80: current_loss=10.17024 | best_loss=10.17024
Epoch 73/80: current_loss=10.14192 | best_loss=10.14192
Epoch 74/80: current_loss=10.25426 | best_loss=10.14192
Epoch 75/80: current_loss=10.09128 | best_loss=10.09128
Epoch 76/80: current_loss=10.08417 | best_loss=10.08417
Epoch 77/80: current_loss=10.13149 | best_loss=10.08417
Epoch 78/80: current_loss=10.14329 | best_loss=10.08417
Epoch 79/80: current_loss=10.28234 | best_loss=10.08417
      explained_var=0.01253 | mse_loss=10.33304
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.38354 | best_loss=10.38354
Epoch 1/80: current_loss=10.32148 | best_loss=10.32148
Epoch 2/80: current_loss=10.29916 | best_loss=10.29916
Epoch 3/80: current_loss=10.40718 | best_loss=10.29916
Epoch 4/80: current_loss=10.22120 | best_loss=10.22120
Epoch 5/80: current_loss=10.25759 | best_loss=10.22120
Epoch 6/80: current_loss=10.25726 | best_loss=10.22120
Epoch 7/80: current_loss=10.27375 | best_loss=10.22120
Epoch 8/80: current_loss=10.28437 | best_loss=10.22120
Epoch 9/80: current_loss=10.54198 | best_loss=10.22120
Epoch 10/80: current_loss=10.24952 | best_loss=10.22120
Epoch 11/80: current_loss=10.41129 | best_loss=10.22120
Epoch 12/80: current_loss=10.38485 | best_loss=10.22120
Epoch 13/80: current_loss=10.27792 | best_loss=10.22120
Epoch 14/80: current_loss=10.33085 | best_loss=10.22120
Epoch 15/80: current_loss=10.42931 | best_loss=10.22120
Epoch 16/80: current_loss=10.24245 | best_loss=10.22120
Epoch 17/80: current_loss=10.24842 | best_loss=10.22120
Epoch 18/80: current_loss=10.43119 | best_loss=10.22120
Epoch 19/80: current_loss=10.23438 | best_loss=10.22120
Epoch 20/80: current_loss=10.22311 | best_loss=10.22120
Epoch 21/80: current_loss=10.21531 | best_loss=10.21531
Epoch 22/80: current_loss=10.20603 | best_loss=10.20603
Epoch 23/80: current_loss=10.21230 | best_loss=10.20603
Epoch 24/80: current_loss=10.18738 | best_loss=10.18738
Epoch 25/80: current_loss=10.18620 | best_loss=10.18620
Epoch 26/80: current_loss=10.44376 | best_loss=10.18620
Epoch 27/80: current_loss=10.40706 | best_loss=10.18620
Epoch 28/80: current_loss=10.21960 | best_loss=10.18620
Epoch 29/80: current_loss=10.20230 | best_loss=10.18620
Epoch 30/80: current_loss=10.23463 | best_loss=10.18620
Epoch 31/80: current_loss=10.17994 | best_loss=10.17994
Epoch 32/80: current_loss=10.19978 | best_loss=10.17994
Epoch 33/80: current_loss=10.79620 | best_loss=10.17994
Epoch 34/80: current_loss=10.23609 | best_loss=10.17994
Epoch 35/80: current_loss=10.22792 | best_loss=10.17994
Epoch 36/80: current_loss=10.21746 | best_loss=10.17994
Epoch 37/80: current_loss=10.19873 | best_loss=10.17994
Epoch 38/80: current_loss=10.27150 | best_loss=10.17994
Epoch 39/80: current_loss=10.20398 | best_loss=10.17994
Epoch 40/80: current_loss=10.23303 | best_loss=10.17994
Epoch 41/80: current_loss=10.25270 | best_loss=10.17994
Epoch 42/80: current_loss=10.26692 | best_loss=10.17994
Epoch 43/80: current_loss=10.23670 | best_loss=10.17994
Epoch 44/80: current_loss=10.21539 | best_loss=10.17994
Epoch 45/80: current_loss=10.23577 | best_loss=10.17994
Epoch 46/80: current_loss=10.25020 | best_loss=10.17994
Epoch 47/80: current_loss=10.25945 | best_loss=10.17994
Epoch 48/80: current_loss=10.20172 | best_loss=10.17994
Epoch 49/80: current_loss=10.22914 | best_loss=10.17994
Epoch 50/80: current_loss=10.18759 | best_loss=10.17994
Epoch 51/80: current_loss=10.30498 | best_loss=10.17994
Early Stopping at epoch 51
      explained_var=0.02506 | mse_loss=9.81250
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.66697 | best_loss=7.66697
Epoch 1/80: current_loss=7.75807 | best_loss=7.66697
Epoch 2/80: current_loss=7.61838 | best_loss=7.61838
Epoch 3/80: current_loss=7.76519 | best_loss=7.61838
Epoch 4/80: current_loss=8.07487 | best_loss=7.61838
Epoch 5/80: current_loss=7.93825 | best_loss=7.61838
Epoch 6/80: current_loss=7.50912 | best_loss=7.50912
Epoch 7/80: current_loss=7.92834 | best_loss=7.50912
Epoch 8/80: current_loss=7.52374 | best_loss=7.50912
Epoch 9/80: current_loss=7.63381 | best_loss=7.50912
Epoch 10/80: current_loss=7.92550 | best_loss=7.50912
Epoch 11/80: current_loss=7.58269 | best_loss=7.50912
Epoch 12/80: current_loss=7.75382 | best_loss=7.50912
Epoch 13/80: current_loss=7.55048 | best_loss=7.50912
Epoch 14/80: current_loss=7.59995 | best_loss=7.50912
Epoch 15/80: current_loss=7.53214 | best_loss=7.50912
Epoch 16/80: current_loss=7.54501 | best_loss=7.50912
Epoch 17/80: current_loss=7.57468 | best_loss=7.50912
Epoch 18/80: current_loss=7.55180 | best_loss=7.50912
Epoch 19/80: current_loss=7.54722 | best_loss=7.50912
Epoch 20/80: current_loss=7.59676 | best_loss=7.50912
Epoch 21/80: current_loss=7.56474 | best_loss=7.50912
Epoch 22/80: current_loss=7.71428 | best_loss=7.50912
Epoch 23/80: current_loss=7.61144 | best_loss=7.50912
Epoch 24/80: current_loss=8.08481 | best_loss=7.50912
Epoch 25/80: current_loss=8.19663 | best_loss=7.50912
Epoch 26/80: current_loss=7.55113 | best_loss=7.50912
Early Stopping at epoch 26
      explained_var=-0.00744 | mse_loss=7.64366

----------------------------------------------
Params for Trial 67
{'learning_rate': 0.001, 'weight_decay': 0.0001480064049452492, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.11343 | best_loss=16.11343
Epoch 1/80: current_loss=10.67634 | best_loss=10.67634
Epoch 2/80: current_loss=10.44419 | best_loss=10.44419
Epoch 3/80: current_loss=10.38813 | best_loss=10.38813
Epoch 4/80: current_loss=10.34035 | best_loss=10.34035
Epoch 5/80: current_loss=10.33296 | best_loss=10.33296
Epoch 6/80: current_loss=10.31162 | best_loss=10.31162
Epoch 7/80: current_loss=10.29904 | best_loss=10.29904
Epoch 8/80: current_loss=10.30747 | best_loss=10.29904
Epoch 9/80: current_loss=10.29070 | best_loss=10.29070
Epoch 10/80: current_loss=10.28667 | best_loss=10.28667
Epoch 11/80: current_loss=10.27466 | best_loss=10.27466
Epoch 12/80: current_loss=10.29829 | best_loss=10.27466
Epoch 13/80: current_loss=10.27178 | best_loss=10.27178
Epoch 14/80: current_loss=10.30861 | best_loss=10.27178
Epoch 15/80: current_loss=10.29089 | best_loss=10.27178
Epoch 16/80: current_loss=10.27680 | best_loss=10.27178
Epoch 17/80: current_loss=10.29267 | best_loss=10.27178
Epoch 18/80: current_loss=10.28043 | best_loss=10.27178
Epoch 19/80: current_loss=10.26551 | best_loss=10.26551
Epoch 20/80: current_loss=10.29817 | best_loss=10.26551
Epoch 21/80: current_loss=10.26706 | best_loss=10.26551
Epoch 22/80: current_loss=10.28210 | best_loss=10.26551
Epoch 23/80: current_loss=10.26861 | best_loss=10.26551
Epoch 24/80: current_loss=10.28346 | best_loss=10.26551
Epoch 25/80: current_loss=10.27665 | best_loss=10.26551
Epoch 26/80: current_loss=10.28295 | best_loss=10.26551
Epoch 27/80: current_loss=10.26384 | best_loss=10.26384
Epoch 28/80: current_loss=10.29164 | best_loss=10.26384
Epoch 29/80: current_loss=10.27307 | best_loss=10.26384
Epoch 30/80: current_loss=10.32580 | best_loss=10.26384
Epoch 31/80: current_loss=10.27643 | best_loss=10.26384
Epoch 32/80: current_loss=10.26246 | best_loss=10.26246
Epoch 33/80: current_loss=10.28965 | best_loss=10.26246
Epoch 34/80: current_loss=10.29837 | best_loss=10.26246
Epoch 35/80: current_loss=10.28435 | best_loss=10.26246
Epoch 36/80: current_loss=10.26102 | best_loss=10.26102
Epoch 37/80: current_loss=10.26884 | best_loss=10.26102
Epoch 38/80: current_loss=10.27004 | best_loss=10.26102
Epoch 39/80: current_loss=10.27245 | best_loss=10.26102
Epoch 40/80: current_loss=10.28101 | best_loss=10.26102
Epoch 41/80: current_loss=10.28782 | best_loss=10.26102
Epoch 42/80: current_loss=10.26841 | best_loss=10.26102
Epoch 43/80: current_loss=10.29050 | best_loss=10.26102
Epoch 44/80: current_loss=10.26172 | best_loss=10.26102
Epoch 45/80: current_loss=10.28766 | best_loss=10.26102
Epoch 46/80: current_loss=10.28947 | best_loss=10.26102
Epoch 47/80: current_loss=10.30280 | best_loss=10.26102
Epoch 48/80: current_loss=10.27149 | best_loss=10.26102
Epoch 49/80: current_loss=10.26523 | best_loss=10.26102
Epoch 50/80: current_loss=10.34119 | best_loss=10.26102
Epoch 51/80: current_loss=10.25777 | best_loss=10.25777
Epoch 52/80: current_loss=10.26188 | best_loss=10.25777
Epoch 53/80: current_loss=10.27094 | best_loss=10.25777
Epoch 54/80: current_loss=10.26277 | best_loss=10.25777
Epoch 55/80: current_loss=10.26327 | best_loss=10.25777
Epoch 56/80: current_loss=10.27042 | best_loss=10.25777
Epoch 57/80: current_loss=10.26605 | best_loss=10.25777
Epoch 58/80: current_loss=10.25467 | best_loss=10.25467
Epoch 59/80: current_loss=10.28178 | best_loss=10.25467
Epoch 60/80: current_loss=10.27109 | best_loss=10.25467
Epoch 61/80: current_loss=10.25708 | best_loss=10.25467
Epoch 62/80: current_loss=10.28409 | best_loss=10.25467
Epoch 63/80: current_loss=10.28824 | best_loss=10.25467
Epoch 64/80: current_loss=10.29248 | best_loss=10.25467
Epoch 65/80: current_loss=10.26154 | best_loss=10.25467
Epoch 66/80: current_loss=10.25979 | best_loss=10.25467
Epoch 67/80: current_loss=10.30609 | best_loss=10.25467
Epoch 68/80: current_loss=10.28352 | best_loss=10.25467
Epoch 69/80: current_loss=10.26933 | best_loss=10.25467
Epoch 70/80: current_loss=10.31025 | best_loss=10.25467
Epoch 71/80: current_loss=10.25602 | best_loss=10.25467
Epoch 72/80: current_loss=10.27527 | best_loss=10.25467
Epoch 73/80: current_loss=10.25596 | best_loss=10.25467
Epoch 74/80: current_loss=10.26010 | best_loss=10.25467
Epoch 75/80: current_loss=10.28710 | best_loss=10.25467
Epoch 76/80: current_loss=10.27509 | best_loss=10.25467
Epoch 77/80: current_loss=10.28372 | best_loss=10.25467
Epoch 78/80: current_loss=10.25562 | best_loss=10.25467
Early Stopping at epoch 78
      explained_var=-0.00060 | mse_loss=10.47302
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.43887 | best_loss=10.43887
Epoch 1/80: current_loss=10.44217 | best_loss=10.43887
Epoch 2/80: current_loss=10.41248 | best_loss=10.41248
Epoch 3/80: current_loss=10.45668 | best_loss=10.41248
Epoch 4/80: current_loss=10.38232 | best_loss=10.38232
Epoch 5/80: current_loss=10.40831 | best_loss=10.38232
Epoch 6/80: current_loss=10.42503 | best_loss=10.38232
Epoch 7/80: current_loss=10.49198 | best_loss=10.38232
Epoch 8/80: current_loss=10.41047 | best_loss=10.38232
Epoch 9/80: current_loss=10.43002 | best_loss=10.38232
Epoch 10/80: current_loss=10.40510 | best_loss=10.38232
Epoch 11/80: current_loss=10.40878 | best_loss=10.38232
Epoch 12/80: current_loss=10.41514 | best_loss=10.38232
Epoch 13/80: current_loss=10.45350 | best_loss=10.38232
Epoch 14/80: current_loss=10.43460 | best_loss=10.38232
Epoch 15/80: current_loss=10.41510 | best_loss=10.38232
Epoch 16/80: current_loss=10.42079 | best_loss=10.38232
Epoch 17/80: current_loss=10.42136 | best_loss=10.38232
Epoch 18/80: current_loss=10.49769 | best_loss=10.38232
Epoch 19/80: current_loss=10.41446 | best_loss=10.38232
Epoch 20/80: current_loss=10.42058 | best_loss=10.38232
Epoch 21/80: current_loss=10.40689 | best_loss=10.38232
Epoch 22/80: current_loss=10.40551 | best_loss=10.38232
Epoch 23/80: current_loss=10.47897 | best_loss=10.38232
Epoch 24/80: current_loss=10.41690 | best_loss=10.38232
Early Stopping at epoch 24
      explained_var=0.00269 | mse_loss=10.03739
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.60720 | best_loss=7.60720
Epoch 1/80: current_loss=7.75624 | best_loss=7.60720
Epoch 2/80: current_loss=7.50733 | best_loss=7.50733
Epoch 3/80: current_loss=7.79055 | best_loss=7.50733
Epoch 4/80: current_loss=7.48053 | best_loss=7.48053
Epoch 5/80: current_loss=7.52634 | best_loss=7.48053
Epoch 6/80: current_loss=7.63949 | best_loss=7.48053
Epoch 7/80: current_loss=7.45227 | best_loss=7.45227
Epoch 8/80: current_loss=7.94785 | best_loss=7.45227
Epoch 9/80: current_loss=7.81051 | best_loss=7.45227
Epoch 10/80: current_loss=7.51086 | best_loss=7.45227
Epoch 11/80: current_loss=7.46137 | best_loss=7.45227
Epoch 12/80: current_loss=7.66046 | best_loss=7.45227
Epoch 13/80: current_loss=7.57674 | best_loss=7.45227
Epoch 14/80: current_loss=7.67335 | best_loss=7.45227
Epoch 15/80: current_loss=7.61067 | best_loss=7.45227
Epoch 16/80: current_loss=7.62541 | best_loss=7.45227
Epoch 17/80: current_loss=7.60289 | best_loss=7.45227
Epoch 18/80: current_loss=7.61576 | best_loss=7.45227
Epoch 19/80: current_loss=7.59859 | best_loss=7.45227
Epoch 20/80: current_loss=7.72486 | best_loss=7.45227
Epoch 21/80: current_loss=7.56409 | best_loss=7.45227
Epoch 22/80: current_loss=7.69745 | best_loss=7.45227
Epoch 23/80: current_loss=7.58019 | best_loss=7.45227
Epoch 24/80: current_loss=7.55378 | best_loss=7.45227
Epoch 25/80: current_loss=7.51066 | best_loss=7.45227
Epoch 26/80: current_loss=7.60208 | best_loss=7.45227
Epoch 27/80: current_loss=7.64524 | best_loss=7.45227
Early Stopping at epoch 27
      explained_var=0.00003 | mse_loss=7.58304
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.76576 | best_loss=8.76576
Epoch 1/80: current_loss=8.79255 | best_loss=8.76576
Epoch 2/80: current_loss=8.84330 | best_loss=8.76576
Epoch 3/80: current_loss=8.85562 | best_loss=8.76576
Epoch 4/80: current_loss=8.82583 | best_loss=8.76576
Epoch 5/80: current_loss=8.82831 | best_loss=8.76576
Epoch 6/80: current_loss=8.88925 | best_loss=8.76576
Epoch 7/80: current_loss=8.87964 | best_loss=8.76576
Epoch 8/80: current_loss=8.84033 | best_loss=8.76576
Epoch 9/80: current_loss=8.98862 | best_loss=8.76576
Epoch 10/80: current_loss=8.84754 | best_loss=8.76576
Epoch 11/80: current_loss=8.94635 | best_loss=8.76576
Epoch 12/80: current_loss=8.74257 | best_loss=8.74257
Epoch 13/80: current_loss=8.81851 | best_loss=8.74257
Epoch 14/80: current_loss=8.81079 | best_loss=8.74257
Epoch 15/80: current_loss=8.76518 | best_loss=8.74257
Epoch 16/80: current_loss=9.00612 | best_loss=8.74257
Epoch 17/80: current_loss=8.86434 | best_loss=8.74257
Epoch 18/80: current_loss=8.83557 | best_loss=8.74257
Epoch 19/80: current_loss=8.82991 | best_loss=8.74257
Epoch 20/80: current_loss=8.87145 | best_loss=8.74257
Epoch 21/80: current_loss=8.85401 | best_loss=8.74257
Epoch 22/80: current_loss=8.85793 | best_loss=8.74257
Epoch 23/80: current_loss=8.94525 | best_loss=8.74257
Epoch 24/80: current_loss=8.85937 | best_loss=8.74257
Epoch 25/80: current_loss=8.88950 | best_loss=8.74257
Epoch 26/80: current_loss=8.88835 | best_loss=8.74257
Epoch 27/80: current_loss=8.92275 | best_loss=8.74257
Epoch 28/80: current_loss=8.84274 | best_loss=8.74257
Epoch 29/80: current_loss=8.84902 | best_loss=8.74257
Epoch 30/80: current_loss=8.85114 | best_loss=8.74257
Epoch 31/80: current_loss=8.86249 | best_loss=8.74257
Epoch 32/80: current_loss=8.88747 | best_loss=8.74257
Early Stopping at epoch 32
      explained_var=0.00070 | mse_loss=8.71021
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.16329 | best_loss=9.16329
Epoch 1/80: current_loss=9.18720 | best_loss=9.16329
Epoch 2/80: current_loss=9.19176 | best_loss=9.16329
Epoch 3/80: current_loss=9.20030 | best_loss=9.16329
Epoch 4/80: current_loss=9.17627 | best_loss=9.16329
Epoch 5/80: current_loss=9.14711 | best_loss=9.14711
Epoch 6/80: current_loss=9.17023 | best_loss=9.14711
Epoch 7/80: current_loss=9.16220 | best_loss=9.14711
Epoch 8/80: current_loss=9.15343 | best_loss=9.14711
Epoch 9/80: current_loss=9.15264 | best_loss=9.14711
Epoch 10/80: current_loss=9.15973 | best_loss=9.14711
Epoch 11/80: current_loss=9.18653 | best_loss=9.14711
Epoch 12/80: current_loss=9.14173 | best_loss=9.14173
Epoch 13/80: current_loss=9.14499 | best_loss=9.14173
Epoch 14/80: current_loss=9.15122 | best_loss=9.14173
Epoch 15/80: current_loss=9.17327 | best_loss=9.14173
Epoch 16/80: current_loss=9.26444 | best_loss=9.14173
Epoch 17/80: current_loss=9.18282 | best_loss=9.14173
Epoch 18/80: current_loss=9.20911 | best_loss=9.14173
Epoch 19/80: current_loss=9.21140 | best_loss=9.14173
Epoch 20/80: current_loss=9.15387 | best_loss=9.14173
Epoch 21/80: current_loss=9.16689 | best_loss=9.14173
Epoch 22/80: current_loss=9.14584 | best_loss=9.14173
Epoch 23/80: current_loss=9.13828 | best_loss=9.13828
Epoch 24/80: current_loss=9.15758 | best_loss=9.13828
Epoch 25/80: current_loss=9.18982 | best_loss=9.13828
Epoch 26/80: current_loss=9.15087 | best_loss=9.13828
Epoch 27/80: current_loss=9.13561 | best_loss=9.13561
Epoch 28/80: current_loss=9.13591 | best_loss=9.13561
Epoch 29/80: current_loss=9.14397 | best_loss=9.13561
Epoch 30/80: current_loss=9.14685 | best_loss=9.13561
Epoch 31/80: current_loss=9.14651 | best_loss=9.13561
Epoch 32/80: current_loss=9.13828 | best_loss=9.13561
Epoch 33/80: current_loss=9.13770 | best_loss=9.13561
Epoch 34/80: current_loss=9.17635 | best_loss=9.13561
Epoch 35/80: current_loss=9.14622 | best_loss=9.13561
Epoch 36/80: current_loss=9.14385 | best_loss=9.13561
Epoch 37/80: current_loss=9.13637 | best_loss=9.13561
Epoch 38/80: current_loss=9.14402 | best_loss=9.13561
Epoch 39/80: current_loss=9.13829 | best_loss=9.13561
Epoch 40/80: current_loss=9.12554 | best_loss=9.12554
Epoch 41/80: current_loss=9.38402 | best_loss=9.12554
Epoch 42/80: current_loss=9.38226 | best_loss=9.12554
Epoch 43/80: current_loss=9.37389 | best_loss=9.12554
Epoch 44/80: current_loss=9.38145 | best_loss=9.12554
Epoch 45/80: current_loss=9.39217 | best_loss=9.12554
Epoch 46/80: current_loss=9.39164 | best_loss=9.12554
Epoch 47/80: current_loss=9.37487 | best_loss=9.12554
Epoch 48/80: current_loss=9.37542 | best_loss=9.12554
Epoch 49/80: current_loss=9.39418 | best_loss=9.12554
Epoch 50/80: current_loss=9.38758 | best_loss=9.12554
Epoch 51/80: current_loss=9.37978 | best_loss=9.12554
Epoch 52/80: current_loss=9.37486 | best_loss=9.12554
Epoch 53/80: current_loss=9.37671 | best_loss=9.12554
Epoch 54/80: current_loss=9.39461 | best_loss=9.12554
Epoch 55/80: current_loss=9.37572 | best_loss=9.12554
Epoch 56/80: current_loss=9.37514 | best_loss=9.12554
Epoch 57/80: current_loss=9.38933 | best_loss=9.12554
Epoch 58/80: current_loss=9.37455 | best_loss=9.12554
Epoch 59/80: current_loss=9.40723 | best_loss=9.12554
Epoch 60/80: current_loss=9.37492 | best_loss=9.12554
Early Stopping at epoch 60
      explained_var=0.02282 | mse_loss=9.08122
----------------------------------------------
Average early_stopping_point: 24| avg_exp_var=0.00513| avg_loss=9.17698
----------------------------------------------


----------------------------------------------
Params for Trial 68
{'learning_rate': 0.001, 'weight_decay': 0.007949292520815865, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=42.14887 | best_loss=42.14887
Epoch 1/80: current_loss=24.37973 | best_loss=24.37973
Epoch 2/80: current_loss=16.47514 | best_loss=16.47514
Epoch 3/80: current_loss=12.66553 | best_loss=12.66553
Epoch 4/80: current_loss=11.12286 | best_loss=11.12286
Epoch 5/80: current_loss=10.63998 | best_loss=10.63998
Epoch 6/80: current_loss=10.43469 | best_loss=10.43469
Epoch 7/80: current_loss=10.35113 | best_loss=10.35113
Epoch 8/80: current_loss=10.33813 | best_loss=10.33813
Epoch 9/80: current_loss=10.33026 | best_loss=10.33026
Epoch 10/80: current_loss=10.31322 | best_loss=10.31322
Epoch 11/80: current_loss=10.31402 | best_loss=10.31322
Epoch 12/80: current_loss=10.31618 | best_loss=10.31322
Epoch 13/80: current_loss=10.31886 | best_loss=10.31322
Epoch 14/80: current_loss=10.31505 | best_loss=10.31322
Epoch 15/80: current_loss=10.31911 | best_loss=10.31322
Epoch 16/80: current_loss=10.31502 | best_loss=10.31322
Epoch 17/80: current_loss=10.28660 | best_loss=10.28660
Epoch 18/80: current_loss=10.27963 | best_loss=10.27963
Epoch 19/80: current_loss=10.29686 | best_loss=10.27963
Epoch 20/80: current_loss=10.30531 | best_loss=10.27963
Epoch 21/80: current_loss=10.30443 | best_loss=10.27963
Epoch 22/80: current_loss=10.27713 | best_loss=10.27713
Epoch 23/80: current_loss=10.27937 | best_loss=10.27713
Epoch 24/80: current_loss=10.29617 | best_loss=10.27713
Epoch 25/80: current_loss=10.29121 | best_loss=10.27713
Epoch 26/80: current_loss=10.29370 | best_loss=10.27713
Epoch 27/80: current_loss=10.28551 | best_loss=10.27713
Epoch 28/80: current_loss=10.27686 | best_loss=10.27686
Epoch 29/80: current_loss=10.26794 | best_loss=10.26794
Epoch 30/80: current_loss=10.29270 | best_loss=10.26794
Epoch 31/80: current_loss=10.30379 | best_loss=10.26794
Epoch 32/80: current_loss=10.28244 | best_loss=10.26794
Epoch 33/80: current_loss=10.29472 | best_loss=10.26794
Epoch 34/80: current_loss=10.29566 | best_loss=10.26794
Epoch 35/80: current_loss=10.28290 | best_loss=10.26794
Epoch 36/80: current_loss=10.29971 | best_loss=10.26794
Epoch 37/80: current_loss=10.30709 | best_loss=10.26794
Epoch 38/80: current_loss=10.30373 | best_loss=10.26794
Epoch 39/80: current_loss=10.30935 | best_loss=10.26794
Epoch 40/80: current_loss=10.28164 | best_loss=10.26794
Epoch 41/80: current_loss=10.28303 | best_loss=10.26794
Epoch 42/80: current_loss=10.30944 | best_loss=10.26794
Epoch 43/80: current_loss=10.29460 | best_loss=10.26794
Epoch 44/80: current_loss=10.29963 | best_loss=10.26794
Epoch 45/80: current_loss=10.29371 | best_loss=10.26794
Epoch 46/80: current_loss=10.28890 | best_loss=10.26794
Epoch 47/80: current_loss=10.32699 | best_loss=10.26794
Epoch 48/80: current_loss=10.27177 | best_loss=10.26794
Epoch 49/80: current_loss=10.29039 | best_loss=10.26794
Early Stopping at epoch 49
      explained_var=-0.00243 | mse_loss=10.49159
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.46531 | best_loss=10.46531
Epoch 1/80: current_loss=10.42655 | best_loss=10.42655
Epoch 2/80: current_loss=10.43957 | best_loss=10.42655
Epoch 3/80: current_loss=10.41977 | best_loss=10.41977
Epoch 4/80: current_loss=10.40907 | best_loss=10.40907
Epoch 5/80: current_loss=10.44143 | best_loss=10.40907
Epoch 6/80: current_loss=10.43130 | best_loss=10.40907
Epoch 7/80: current_loss=10.41632 | best_loss=10.40907
Epoch 8/80: current_loss=10.42410 | best_loss=10.40907
Epoch 9/80: current_loss=10.42522 | best_loss=10.40907
Epoch 10/80: current_loss=10.40673 | best_loss=10.40673
Epoch 11/80: current_loss=10.43256 | best_loss=10.40673
Epoch 12/80: current_loss=10.41667 | best_loss=10.40673
Epoch 13/80: current_loss=10.39609 | best_loss=10.39609
Epoch 14/80: current_loss=10.40731 | best_loss=10.39609
Epoch 15/80: current_loss=10.42525 | best_loss=10.39609
Epoch 16/80: current_loss=10.43081 | best_loss=10.39609
Epoch 17/80: current_loss=10.42426 | best_loss=10.39609
Epoch 18/80: current_loss=10.42419 | best_loss=10.39609
Epoch 19/80: current_loss=10.40610 | best_loss=10.39609
Epoch 20/80: current_loss=10.42920 | best_loss=10.39609
Epoch 21/80: current_loss=10.42802 | best_loss=10.39609
Epoch 22/80: current_loss=10.42081 | best_loss=10.39609
Epoch 23/80: current_loss=10.46271 | best_loss=10.39609
Epoch 24/80: current_loss=10.40813 | best_loss=10.39609
Epoch 25/80: current_loss=10.40827 | best_loss=10.39609
Epoch 26/80: current_loss=10.42474 | best_loss=10.39609
Epoch 27/80: current_loss=10.39990 | best_loss=10.39609
Epoch 28/80: current_loss=10.42668 | best_loss=10.39609
Epoch 29/80: current_loss=10.42636 | best_loss=10.39609
Epoch 30/80: current_loss=10.45426 | best_loss=10.39609
Epoch 31/80: current_loss=10.40207 | best_loss=10.39609
Epoch 32/80: current_loss=10.45212 | best_loss=10.39609
Epoch 33/80: current_loss=10.43347 | best_loss=10.39609
Early Stopping at epoch 33
      explained_var=0.00171 | mse_loss=10.04812

----------------------------------------------
Params for Trial 69
{'learning_rate': 1e-05, 'weight_decay': 0.009666129609475486, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=101.03115 | best_loss=101.03115
Epoch 1/80: current_loss=91.48844 | best_loss=91.48844
Epoch 2/80: current_loss=66.18350 | best_loss=66.18350
Epoch 3/80: current_loss=37.75067 | best_loss=37.75067
Epoch 4/80: current_loss=25.61621 | best_loss=25.61621
Epoch 5/80: current_loss=21.09022 | best_loss=21.09022
Epoch 6/80: current_loss=19.12799 | best_loss=19.12799
Epoch 7/80: current_loss=18.07033 | best_loss=18.07033
Epoch 8/80: current_loss=17.42367 | best_loss=17.42367
Epoch 9/80: current_loss=17.01887 | best_loss=17.01887
Epoch 10/80: current_loss=16.69469 | best_loss=16.69469
Epoch 11/80: current_loss=16.44150 | best_loss=16.44150
Epoch 12/80: current_loss=16.19698 | best_loss=16.19698
Epoch 13/80: current_loss=15.98983 | best_loss=15.98983
Epoch 14/80: current_loss=15.78230 | best_loss=15.78230
Epoch 15/80: current_loss=15.59990 | best_loss=15.59990
Epoch 16/80: current_loss=15.44366 | best_loss=15.44366
Epoch 17/80: current_loss=15.28600 | best_loss=15.28600
Epoch 18/80: current_loss=15.13436 | best_loss=15.13436
Epoch 19/80: current_loss=14.97851 | best_loss=14.97851
Epoch 20/80: current_loss=14.85212 | best_loss=14.85212
Epoch 21/80: current_loss=14.72589 | best_loss=14.72589
Epoch 22/80: current_loss=14.60417 | best_loss=14.60417
Epoch 23/80: current_loss=14.47886 | best_loss=14.47886
Epoch 24/80: current_loss=14.36860 | best_loss=14.36860
Epoch 25/80: current_loss=14.26202 | best_loss=14.26202
Epoch 26/80: current_loss=14.16091 | best_loss=14.16091
Epoch 27/80: current_loss=14.06197 | best_loss=14.06197
Epoch 28/80: current_loss=13.96079 | best_loss=13.96079
Epoch 29/80: current_loss=13.86392 | best_loss=13.86392
Epoch 30/80: current_loss=13.77874 | best_loss=13.77874
Epoch 31/80: current_loss=13.69628 | best_loss=13.69628
Epoch 32/80: current_loss=13.61781 | best_loss=13.61781
Epoch 33/80: current_loss=13.53888 | best_loss=13.53888
Epoch 34/80: current_loss=13.46487 | best_loss=13.46487
Epoch 35/80: current_loss=13.38193 | best_loss=13.38193
Epoch 36/80: current_loss=13.30901 | best_loss=13.30901
Epoch 37/80: current_loss=13.23530 | best_loss=13.23530
Epoch 38/80: current_loss=13.15899 | best_loss=13.15899
Epoch 39/80: current_loss=13.07649 | best_loss=13.07649
Epoch 40/80: current_loss=13.00238 | best_loss=13.00238
Epoch 41/80: current_loss=12.93839 | best_loss=12.93839
Epoch 42/80: current_loss=12.87790 | best_loss=12.87790
Epoch 43/80: current_loss=12.82276 | best_loss=12.82276
Epoch 44/80: current_loss=12.76074 | best_loss=12.76074
Epoch 45/80: current_loss=12.70293 | best_loss=12.70293
Epoch 46/80: current_loss=12.65169 | best_loss=12.65169
Epoch 47/80: current_loss=12.59934 | best_loss=12.59934
Epoch 48/80: current_loss=12.53843 | best_loss=12.53843
Epoch 49/80: current_loss=12.48597 | best_loss=12.48597
Epoch 50/80: current_loss=12.43382 | best_loss=12.43382
Epoch 51/80: current_loss=12.38360 | best_loss=12.38360
Epoch 52/80: current_loss=12.32768 | best_loss=12.32768
Epoch 53/80: current_loss=12.28078 | best_loss=12.28078
Epoch 54/80: current_loss=12.23953 | best_loss=12.23953
Epoch 55/80: current_loss=12.20015 | best_loss=12.20015
Epoch 56/80: current_loss=12.15142 | best_loss=12.15142
Epoch 57/80: current_loss=12.10980 | best_loss=12.10980
Epoch 58/80: current_loss=12.05542 | best_loss=12.05542
Epoch 59/80: current_loss=12.01640 | best_loss=12.01640
Epoch 60/80: current_loss=11.97801 | best_loss=11.97801
Epoch 61/80: current_loss=11.93299 | best_loss=11.93299
Epoch 62/80: current_loss=11.89671 | best_loss=11.89671
Epoch 63/80: current_loss=11.85103 | best_loss=11.85103
Epoch 64/80: current_loss=11.81160 | best_loss=11.81160
Epoch 65/80: current_loss=11.78356 | best_loss=11.78356
Epoch 66/80: current_loss=11.75332 | best_loss=11.75332
Epoch 67/80: current_loss=11.71249 | best_loss=11.71249
Epoch 68/80: current_loss=11.67874 | best_loss=11.67874
Epoch 69/80: current_loss=11.65495 | best_loss=11.65495
Epoch 70/80: current_loss=11.61993 | best_loss=11.61993
Epoch 71/80: current_loss=11.58760 | best_loss=11.58760
Epoch 72/80: current_loss=11.55142 | best_loss=11.55142
Epoch 73/80: current_loss=11.51635 | best_loss=11.51635
Epoch 74/80: current_loss=11.48477 | best_loss=11.48477
Epoch 75/80: current_loss=11.45176 | best_loss=11.45176
Epoch 76/80: current_loss=11.41826 | best_loss=11.41826
Epoch 77/80: current_loss=11.39010 | best_loss=11.39010
Epoch 78/80: current_loss=11.35622 | best_loss=11.35622
Epoch 79/80: current_loss=11.32803 | best_loss=11.32803
      explained_var=-0.10238 | mse_loss=11.62315

----------------------------------------------
Params for Trial 70
{'learning_rate': 0.001, 'weight_decay': 0.007085157718400268, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=19.82795 | best_loss=19.82795
Epoch 1/80: current_loss=12.12435 | best_loss=12.12435
Epoch 2/80: current_loss=11.41388 | best_loss=11.41388
Epoch 3/80: current_loss=11.14546 | best_loss=11.14546
Epoch 4/80: current_loss=10.94596 | best_loss=10.94596
Epoch 5/80: current_loss=10.80202 | best_loss=10.80202
Epoch 6/80: current_loss=10.65416 | best_loss=10.65416
Epoch 7/80: current_loss=10.55387 | best_loss=10.55387
Epoch 8/80: current_loss=10.49353 | best_loss=10.49353
Epoch 9/80: current_loss=10.47999 | best_loss=10.47999
Epoch 10/80: current_loss=10.45328 | best_loss=10.45328
Epoch 11/80: current_loss=10.39855 | best_loss=10.39855
Epoch 12/80: current_loss=10.39836 | best_loss=10.39836
Epoch 13/80: current_loss=10.40198 | best_loss=10.39836
Epoch 14/80: current_loss=10.35948 | best_loss=10.35948
Epoch 15/80: current_loss=10.35393 | best_loss=10.35393
Epoch 16/80: current_loss=10.35558 | best_loss=10.35393
Epoch 17/80: current_loss=10.38178 | best_loss=10.35393
Epoch 18/80: current_loss=10.33546 | best_loss=10.33546
Epoch 19/80: current_loss=10.34400 | best_loss=10.33546
Epoch 20/80: current_loss=10.34403 | best_loss=10.33546
Epoch 21/80: current_loss=10.33349 | best_loss=10.33349
Epoch 22/80: current_loss=10.33905 | best_loss=10.33349
Epoch 23/80: current_loss=10.34141 | best_loss=10.33349
Epoch 24/80: current_loss=10.31973 | best_loss=10.31973
Epoch 25/80: current_loss=10.31313 | best_loss=10.31313
Epoch 26/80: current_loss=10.32969 | best_loss=10.31313
Epoch 27/80: current_loss=10.34619 | best_loss=10.31313
Epoch 28/80: current_loss=10.30922 | best_loss=10.30922
Epoch 29/80: current_loss=10.32761 | best_loss=10.30922
Epoch 30/80: current_loss=10.29487 | best_loss=10.29487
Epoch 31/80: current_loss=10.30989 | best_loss=10.29487
Epoch 32/80: current_loss=10.30766 | best_loss=10.29487
Epoch 33/80: current_loss=10.30697 | best_loss=10.29487
Epoch 34/80: current_loss=10.33730 | best_loss=10.29487
Epoch 35/80: current_loss=10.31682 | best_loss=10.29487
Epoch 36/80: current_loss=10.30599 | best_loss=10.29487
Epoch 37/80: current_loss=10.34122 | best_loss=10.29487
Epoch 38/80: current_loss=10.31750 | best_loss=10.29487
Epoch 39/80: current_loss=10.30427 | best_loss=10.29487
Epoch 40/80: current_loss=10.30822 | best_loss=10.29487
Epoch 41/80: current_loss=10.31266 | best_loss=10.29487
Epoch 42/80: current_loss=10.29651 | best_loss=10.29487
Epoch 43/80: current_loss=10.34491 | best_loss=10.29487
Epoch 44/80: current_loss=10.28838 | best_loss=10.28838
Epoch 45/80: current_loss=10.32665 | best_loss=10.28838
Epoch 46/80: current_loss=10.28786 | best_loss=10.28786
Epoch 47/80: current_loss=10.28525 | best_loss=10.28525
Epoch 48/80: current_loss=10.29461 | best_loss=10.28525
Epoch 49/80: current_loss=10.29790 | best_loss=10.28525
Epoch 50/80: current_loss=10.33169 | best_loss=10.28525
Epoch 51/80: current_loss=10.28824 | best_loss=10.28525
Epoch 52/80: current_loss=10.31262 | best_loss=10.28525
Epoch 53/80: current_loss=10.30680 | best_loss=10.28525
Epoch 54/80: current_loss=10.29054 | best_loss=10.28525
Epoch 55/80: current_loss=10.30241 | best_loss=10.28525
Epoch 56/80: current_loss=10.32751 | best_loss=10.28525
Epoch 57/80: current_loss=10.31224 | best_loss=10.28525
Epoch 58/80: current_loss=10.29697 | best_loss=10.28525
Epoch 59/80: current_loss=10.31790 | best_loss=10.28525
Epoch 60/80: current_loss=10.33509 | best_loss=10.28525
Epoch 61/80: current_loss=10.29845 | best_loss=10.28525
Epoch 62/80: current_loss=10.29965 | best_loss=10.28525
Epoch 63/80: current_loss=10.31146 | best_loss=10.28525
Epoch 64/80: current_loss=10.30044 | best_loss=10.28525
Epoch 65/80: current_loss=10.33791 | best_loss=10.28525
Epoch 66/80: current_loss=10.28164 | best_loss=10.28164
Epoch 67/80: current_loss=10.30620 | best_loss=10.28164
Epoch 68/80: current_loss=10.35721 | best_loss=10.28164
Epoch 69/80: current_loss=10.27509 | best_loss=10.27509
Epoch 70/80: current_loss=10.28952 | best_loss=10.27509
Epoch 71/80: current_loss=10.29272 | best_loss=10.27509
Epoch 72/80: current_loss=10.33073 | best_loss=10.27509
Epoch 73/80: current_loss=10.26584 | best_loss=10.26584
Epoch 74/80: current_loss=10.19691 | best_loss=10.19691
Epoch 75/80: current_loss=10.33494 | best_loss=10.19691
Epoch 76/80: current_loss=10.19472 | best_loss=10.19472
Epoch 77/80: current_loss=10.12190 | best_loss=10.12190
Epoch 78/80: current_loss=10.12151 | best_loss=10.12151
Epoch 79/80: current_loss=10.22389 | best_loss=10.12151
      explained_var=0.01194 | mse_loss=10.33374
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.37503 | best_loss=10.37503
Epoch 1/80: current_loss=10.29263 | best_loss=10.29263
Epoch 2/80: current_loss=10.22755 | best_loss=10.22755
Epoch 3/80: current_loss=10.19537 | best_loss=10.19537
Epoch 4/80: current_loss=10.19022 | best_loss=10.19022
Epoch 5/80: current_loss=10.50999 | best_loss=10.19022
Epoch 6/80: current_loss=10.41431 | best_loss=10.19022
Epoch 7/80: current_loss=10.33761 | best_loss=10.19022
Epoch 8/80: current_loss=10.27635 | best_loss=10.19022
Epoch 9/80: current_loss=10.27494 | best_loss=10.19022
Epoch 10/80: current_loss=10.21613 | best_loss=10.19022
Epoch 11/80: current_loss=10.31087 | best_loss=10.19022
Epoch 12/80: current_loss=10.29339 | best_loss=10.19022
Epoch 13/80: current_loss=10.28280 | best_loss=10.19022
Epoch 14/80: current_loss=10.26236 | best_loss=10.19022
Epoch 15/80: current_loss=10.20082 | best_loss=10.19022
Epoch 16/80: current_loss=10.21578 | best_loss=10.19022
Epoch 17/80: current_loss=10.25900 | best_loss=10.19022
Epoch 18/80: current_loss=10.25383 | best_loss=10.19022
Epoch 19/80: current_loss=10.20501 | best_loss=10.19022
Epoch 20/80: current_loss=10.18236 | best_loss=10.18236
Epoch 21/80: current_loss=10.21703 | best_loss=10.18236
Epoch 22/80: current_loss=10.61259 | best_loss=10.18236
Epoch 23/80: current_loss=10.27890 | best_loss=10.18236
Epoch 24/80: current_loss=10.21977 | best_loss=10.18236
Epoch 25/80: current_loss=10.30018 | best_loss=10.18236
Epoch 26/80: current_loss=10.26308 | best_loss=10.18236
Epoch 27/80: current_loss=10.27562 | best_loss=10.18236
Epoch 28/80: current_loss=10.24578 | best_loss=10.18236
Epoch 29/80: current_loss=10.19980 | best_loss=10.18236
Epoch 30/80: current_loss=10.18590 | best_loss=10.18236
Epoch 31/80: current_loss=10.18696 | best_loss=10.18236
Epoch 32/80: current_loss=10.17991 | best_loss=10.17991
Epoch 33/80: current_loss=10.40904 | best_loss=10.17991
Epoch 34/80: current_loss=10.38202 | best_loss=10.17991
Epoch 35/80: current_loss=10.29531 | best_loss=10.17991
Epoch 36/80: current_loss=10.27453 | best_loss=10.17991
Epoch 37/80: current_loss=10.27146 | best_loss=10.17991
Epoch 38/80: current_loss=10.23641 | best_loss=10.17991
Epoch 39/80: current_loss=10.44542 | best_loss=10.17991
Epoch 40/80: current_loss=10.31374 | best_loss=10.17991
Epoch 41/80: current_loss=10.27996 | best_loss=10.17991
Epoch 42/80: current_loss=10.26037 | best_loss=10.17991
Epoch 43/80: current_loss=10.30495 | best_loss=10.17991
Epoch 44/80: current_loss=10.20722 | best_loss=10.17991
Epoch 45/80: current_loss=10.19971 | best_loss=10.17991
Epoch 46/80: current_loss=10.28253 | best_loss=10.17991
Epoch 47/80: current_loss=10.27520 | best_loss=10.17991
Epoch 48/80: current_loss=10.24907 | best_loss=10.17991
Epoch 49/80: current_loss=10.22646 | best_loss=10.17991
Epoch 50/80: current_loss=10.27706 | best_loss=10.17991
Epoch 51/80: current_loss=10.34157 | best_loss=10.17991
Epoch 52/80: current_loss=10.25037 | best_loss=10.17991
Early Stopping at epoch 52
      explained_var=0.02505 | mse_loss=9.81484
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.70665 | best_loss=7.70665
Epoch 1/80: current_loss=7.68139 | best_loss=7.68139
Epoch 2/80: current_loss=7.78729 | best_loss=7.68139
Epoch 3/80: current_loss=7.76287 | best_loss=7.68139
Epoch 4/80: current_loss=7.52245 | best_loss=7.52245
Epoch 5/80: current_loss=7.50462 | best_loss=7.50462
Epoch 6/80: current_loss=7.70851 | best_loss=7.50462
Epoch 7/80: current_loss=7.55642 | best_loss=7.50462
Epoch 8/80: current_loss=7.50087 | best_loss=7.50087
Epoch 9/80: current_loss=7.61953 | best_loss=7.50087
Epoch 10/80: current_loss=7.64366 | best_loss=7.50087
Epoch 11/80: current_loss=7.70080 | best_loss=7.50087
Epoch 12/80: current_loss=7.62617 | best_loss=7.50087
Epoch 13/80: current_loss=7.60637 | best_loss=7.50087
Epoch 14/80: current_loss=7.72063 | best_loss=7.50087
Epoch 15/80: current_loss=7.59233 | best_loss=7.50087
Epoch 16/80: current_loss=7.70421 | best_loss=7.50087
Epoch 17/80: current_loss=7.63904 | best_loss=7.50087
Epoch 18/80: current_loss=7.66248 | best_loss=7.50087
Epoch 19/80: current_loss=7.52930 | best_loss=7.50087
Epoch 20/80: current_loss=7.57301 | best_loss=7.50087
Epoch 21/80: current_loss=7.64709 | best_loss=7.50087
Epoch 22/80: current_loss=7.54759 | best_loss=7.50087
Epoch 23/80: current_loss=7.66062 | best_loss=7.50087
Epoch 24/80: current_loss=7.63018 | best_loss=7.50087
Epoch 25/80: current_loss=7.67485 | best_loss=7.50087
Epoch 26/80: current_loss=7.55356 | best_loss=7.50087
Epoch 27/80: current_loss=7.68418 | best_loss=7.50087
Epoch 28/80: current_loss=7.66602 | best_loss=7.50087
Early Stopping at epoch 28
      explained_var=-0.00563 | mse_loss=7.62868

----------------------------------------------
Params for Trial 71
{'learning_rate': 0.001, 'weight_decay': 0.006525953014612539, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.10636 | best_loss=18.10636
Epoch 1/80: current_loss=10.91055 | best_loss=10.91055
Epoch 2/80: current_loss=10.43261 | best_loss=10.43261
Epoch 3/80: current_loss=10.38010 | best_loss=10.38010
Epoch 4/80: current_loss=10.37113 | best_loss=10.37113
Epoch 5/80: current_loss=10.31623 | best_loss=10.31623
Epoch 6/80: current_loss=10.30580 | best_loss=10.30580
Epoch 7/80: current_loss=10.31390 | best_loss=10.30580
Epoch 8/80: current_loss=10.30941 | best_loss=10.30580
Epoch 9/80: current_loss=10.31276 | best_loss=10.30580
Epoch 10/80: current_loss=10.28667 | best_loss=10.28667
Epoch 11/80: current_loss=10.30276 | best_loss=10.28667
Epoch 12/80: current_loss=10.30101 | best_loss=10.28667
Epoch 13/80: current_loss=10.27355 | best_loss=10.27355
Epoch 14/80: current_loss=10.29012 | best_loss=10.27355
Epoch 15/80: current_loss=10.26899 | best_loss=10.26899
Epoch 16/80: current_loss=10.28869 | best_loss=10.26899
Epoch 17/80: current_loss=10.27701 | best_loss=10.26899
Epoch 18/80: current_loss=10.27927 | best_loss=10.26899
Epoch 19/80: current_loss=10.31764 | best_loss=10.26899
Epoch 20/80: current_loss=10.27665 | best_loss=10.26899
Epoch 21/80: current_loss=10.27126 | best_loss=10.26899
Epoch 22/80: current_loss=10.28373 | best_loss=10.26899
Epoch 23/80: current_loss=10.29019 | best_loss=10.26899
Epoch 24/80: current_loss=10.26764 | best_loss=10.26764
Epoch 25/80: current_loss=10.27560 | best_loss=10.26764
Epoch 26/80: current_loss=10.28166 | best_loss=10.26764
Epoch 27/80: current_loss=10.28175 | best_loss=10.26764
Epoch 28/80: current_loss=10.29925 | best_loss=10.26764
Epoch 29/80: current_loss=10.28941 | best_loss=10.26764
Epoch 30/80: current_loss=10.27681 | best_loss=10.26764
Epoch 31/80: current_loss=10.31269 | best_loss=10.26764
Epoch 32/80: current_loss=10.32015 | best_loss=10.26764
Epoch 33/80: current_loss=10.30832 | best_loss=10.26764
Epoch 34/80: current_loss=10.30805 | best_loss=10.26764
Epoch 35/80: current_loss=10.29269 | best_loss=10.26764
Epoch 36/80: current_loss=10.31484 | best_loss=10.26764
Epoch 37/80: current_loss=10.29694 | best_loss=10.26764
Epoch 38/80: current_loss=10.29886 | best_loss=10.26764
Epoch 39/80: current_loss=10.31421 | best_loss=10.26764
Epoch 40/80: current_loss=10.31577 | best_loss=10.26764
Epoch 41/80: current_loss=10.28237 | best_loss=10.26764
Epoch 42/80: current_loss=10.32579 | best_loss=10.26764
Epoch 43/80: current_loss=10.30662 | best_loss=10.26764
Epoch 44/80: current_loss=10.27415 | best_loss=10.26764
Early Stopping at epoch 44
      explained_var=-0.00217 | mse_loss=10.49253
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.40365 | best_loss=10.40365
Epoch 1/80: current_loss=10.45560 | best_loss=10.40365
Epoch 2/80: current_loss=10.42026 | best_loss=10.40365
Epoch 3/80: current_loss=10.51059 | best_loss=10.40365
Epoch 4/80: current_loss=10.40600 | best_loss=10.40365
Epoch 5/80: current_loss=10.41888 | best_loss=10.40365
Epoch 6/80: current_loss=10.40801 | best_loss=10.40365
Epoch 7/80: current_loss=10.42175 | best_loss=10.40365
Epoch 8/80: current_loss=10.42873 | best_loss=10.40365
Epoch 9/80: current_loss=10.42691 | best_loss=10.40365
Epoch 10/80: current_loss=10.45101 | best_loss=10.40365
Epoch 11/80: current_loss=10.41904 | best_loss=10.40365
Epoch 12/80: current_loss=10.43591 | best_loss=10.40365
Epoch 13/80: current_loss=10.40423 | best_loss=10.40365
Epoch 14/80: current_loss=10.43472 | best_loss=10.40365
Epoch 15/80: current_loss=10.49053 | best_loss=10.40365
Epoch 16/80: current_loss=10.42381 | best_loss=10.40365
Epoch 17/80: current_loss=10.45382 | best_loss=10.40365
Epoch 18/80: current_loss=10.41702 | best_loss=10.40365
Epoch 19/80: current_loss=10.41511 | best_loss=10.40365
Epoch 20/80: current_loss=10.42037 | best_loss=10.40365
Early Stopping at epoch 20
      explained_var=0.00036 | mse_loss=10.06068

----------------------------------------------
Params for Trial 72
{'learning_rate': 0.001, 'weight_decay': 0.005988764550625316, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.32239 | best_loss=18.32239
Epoch 1/80: current_loss=10.93367 | best_loss=10.93367
Epoch 2/80: current_loss=10.43786 | best_loss=10.43786
Epoch 3/80: current_loss=10.37420 | best_loss=10.37420
Epoch 4/80: current_loss=10.36420 | best_loss=10.36420
Epoch 5/80: current_loss=10.33627 | best_loss=10.33627
Epoch 6/80: current_loss=10.35704 | best_loss=10.33627
Epoch 7/80: current_loss=10.31997 | best_loss=10.31997
Epoch 8/80: current_loss=10.34500 | best_loss=10.31997
Epoch 9/80: current_loss=10.32547 | best_loss=10.31997
Epoch 10/80: current_loss=10.31312 | best_loss=10.31312
Epoch 11/80: current_loss=10.31486 | best_loss=10.31312
Epoch 12/80: current_loss=10.29389 | best_loss=10.29389
Epoch 13/80: current_loss=10.30764 | best_loss=10.29389
Epoch 14/80: current_loss=10.29602 | best_loss=10.29389
Epoch 15/80: current_loss=10.28096 | best_loss=10.28096
Epoch 16/80: current_loss=10.29454 | best_loss=10.28096
Epoch 17/80: current_loss=10.27465 | best_loss=10.27465
Epoch 18/80: current_loss=10.29221 | best_loss=10.27465
Epoch 19/80: current_loss=10.27729 | best_loss=10.27465
Epoch 20/80: current_loss=10.27541 | best_loss=10.27465
Epoch 21/80: current_loss=10.28763 | best_loss=10.27465
Epoch 22/80: current_loss=10.30131 | best_loss=10.27465
Epoch 23/80: current_loss=10.29120 | best_loss=10.27465
Epoch 24/80: current_loss=10.30903 | best_loss=10.27465
Epoch 25/80: current_loss=10.28279 | best_loss=10.27465
Epoch 26/80: current_loss=10.28618 | best_loss=10.27465
Epoch 27/80: current_loss=10.30257 | best_loss=10.27465
Epoch 28/80: current_loss=10.27709 | best_loss=10.27465
Epoch 29/80: current_loss=10.29540 | best_loss=10.27465
Epoch 30/80: current_loss=10.32831 | best_loss=10.27465
Epoch 31/80: current_loss=10.30968 | best_loss=10.27465
Epoch 32/80: current_loss=10.28774 | best_loss=10.27465
Epoch 33/80: current_loss=10.29452 | best_loss=10.27465
Epoch 34/80: current_loss=10.30130 | best_loss=10.27465
Epoch 35/80: current_loss=10.30860 | best_loss=10.27465
Epoch 36/80: current_loss=10.27606 | best_loss=10.27465
Epoch 37/80: current_loss=10.30620 | best_loss=10.27465
Early Stopping at epoch 37
      explained_var=-0.00274 | mse_loss=10.50390
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.47230 | best_loss=10.47230
Epoch 1/80: current_loss=10.41863 | best_loss=10.41863
Epoch 2/80: current_loss=10.42094 | best_loss=10.41863
Epoch 3/80: current_loss=10.42575 | best_loss=10.41863
Epoch 4/80: current_loss=10.46694 | best_loss=10.41863
Epoch 5/80: current_loss=10.43655 | best_loss=10.41863
Epoch 6/80: current_loss=10.41172 | best_loss=10.41172
Epoch 7/80: current_loss=10.40912 | best_loss=10.40912
Epoch 8/80: current_loss=10.54084 | best_loss=10.40912
Epoch 9/80: current_loss=10.40807 | best_loss=10.40807
Epoch 10/80: current_loss=10.44746 | best_loss=10.40807
Epoch 11/80: current_loss=10.46869 | best_loss=10.40807
Epoch 12/80: current_loss=10.41371 | best_loss=10.40807
Epoch 13/80: current_loss=10.41533 | best_loss=10.40807
Epoch 14/80: current_loss=10.45126 | best_loss=10.40807
Epoch 15/80: current_loss=10.40945 | best_loss=10.40807
Epoch 16/80: current_loss=10.43860 | best_loss=10.40807
Epoch 17/80: current_loss=10.41283 | best_loss=10.40807
Epoch 18/80: current_loss=10.40000 | best_loss=10.40000
Epoch 19/80: current_loss=10.41468 | best_loss=10.40000
Epoch 20/80: current_loss=10.41985 | best_loss=10.40000
Epoch 21/80: current_loss=10.46965 | best_loss=10.40000
Epoch 22/80: current_loss=10.40645 | best_loss=10.40000
Epoch 23/80: current_loss=10.42387 | best_loss=10.40000
Epoch 24/80: current_loss=10.42933 | best_loss=10.40000
Epoch 25/80: current_loss=10.41443 | best_loss=10.40000
Epoch 26/80: current_loss=10.43530 | best_loss=10.40000
Epoch 27/80: current_loss=10.40569 | best_loss=10.40000
Epoch 28/80: current_loss=10.43714 | best_loss=10.40000
Epoch 29/80: current_loss=10.42874 | best_loss=10.40000
Epoch 30/80: current_loss=10.44556 | best_loss=10.40000
Epoch 31/80: current_loss=10.45542 | best_loss=10.40000
Epoch 32/80: current_loss=10.58448 | best_loss=10.40000
Epoch 33/80: current_loss=10.41204 | best_loss=10.40000
Epoch 34/80: current_loss=10.46247 | best_loss=10.40000
Epoch 35/80: current_loss=10.40298 | best_loss=10.40000
Epoch 36/80: current_loss=10.40946 | best_loss=10.40000
Epoch 37/80: current_loss=10.45106 | best_loss=10.40000
Epoch 38/80: current_loss=10.40524 | best_loss=10.40000
Early Stopping at epoch 38
      explained_var=0.00068 | mse_loss=10.05823

----------------------------------------------
Params for Trial 73
{'learning_rate': 0.001, 'weight_decay': 0.0073648441315336, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=14.51616 | best_loss=14.51616
Epoch 1/80: current_loss=10.58374 | best_loss=10.58374
Epoch 2/80: current_loss=10.41092 | best_loss=10.41092
Epoch 3/80: current_loss=10.38401 | best_loss=10.38401
Epoch 4/80: current_loss=10.34812 | best_loss=10.34812
Epoch 5/80: current_loss=10.36323 | best_loss=10.34812
Epoch 6/80: current_loss=10.33372 | best_loss=10.33372
Epoch 7/80: current_loss=10.31260 | best_loss=10.31260
Epoch 8/80: current_loss=10.34106 | best_loss=10.31260
Epoch 9/80: current_loss=10.29048 | best_loss=10.29048
Epoch 10/80: current_loss=10.31044 | best_loss=10.29048
Epoch 11/80: current_loss=10.29237 | best_loss=10.29048
Epoch 12/80: current_loss=10.27974 | best_loss=10.27974
Epoch 13/80: current_loss=10.28959 | best_loss=10.27974
Epoch 14/80: current_loss=10.27534 | best_loss=10.27534
Epoch 15/80: current_loss=10.27149 | best_loss=10.27149
Epoch 16/80: current_loss=10.28280 | best_loss=10.27149
Epoch 17/80: current_loss=10.29686 | best_loss=10.27149
Epoch 18/80: current_loss=10.27540 | best_loss=10.27149
Epoch 19/80: current_loss=10.30005 | best_loss=10.27149
Epoch 20/80: current_loss=10.28877 | best_loss=10.27149
Epoch 21/80: current_loss=10.28777 | best_loss=10.27149
Epoch 22/80: current_loss=10.30547 | best_loss=10.27149
Epoch 23/80: current_loss=10.29465 | best_loss=10.27149
Epoch 24/80: current_loss=10.31285 | best_loss=10.27149
Epoch 25/80: current_loss=10.31480 | best_loss=10.27149
Epoch 26/80: current_loss=10.28754 | best_loss=10.27149
Epoch 27/80: current_loss=10.27928 | best_loss=10.27149
Epoch 28/80: current_loss=10.27915 | best_loss=10.27149
Epoch 29/80: current_loss=10.29725 | best_loss=10.27149
Epoch 30/80: current_loss=10.27719 | best_loss=10.27149
Epoch 31/80: current_loss=10.27826 | best_loss=10.27149
Epoch 32/80: current_loss=10.31621 | best_loss=10.27149
Epoch 33/80: current_loss=10.27157 | best_loss=10.27149
Epoch 34/80: current_loss=10.26350 | best_loss=10.26350
Epoch 35/80: current_loss=10.29629 | best_loss=10.26350
Epoch 36/80: current_loss=10.27182 | best_loss=10.26350
Epoch 37/80: current_loss=10.28268 | best_loss=10.26350
Epoch 38/80: current_loss=10.27077 | best_loss=10.26350
Epoch 39/80: current_loss=10.28554 | best_loss=10.26350
Epoch 40/80: current_loss=10.28598 | best_loss=10.26350
Epoch 41/80: current_loss=10.26512 | best_loss=10.26350
Epoch 42/80: current_loss=10.27715 | best_loss=10.26350
Epoch 43/80: current_loss=10.30920 | best_loss=10.26350
Epoch 44/80: current_loss=10.28587 | best_loss=10.26350
Epoch 45/80: current_loss=10.32096 | best_loss=10.26350
Epoch 46/80: current_loss=10.30776 | best_loss=10.26350
Epoch 47/80: current_loss=10.30156 | best_loss=10.26350
Epoch 48/80: current_loss=10.27218 | best_loss=10.26350
Epoch 49/80: current_loss=10.27130 | best_loss=10.26350
Epoch 50/80: current_loss=10.29130 | best_loss=10.26350
Epoch 51/80: current_loss=10.30744 | best_loss=10.26350
Epoch 52/80: current_loss=10.28859 | best_loss=10.26350
Epoch 53/80: current_loss=10.28122 | best_loss=10.26350
Epoch 54/80: current_loss=10.27700 | best_loss=10.26350
Early Stopping at epoch 54
      explained_var=-0.00181 | mse_loss=10.48578
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.41129 | best_loss=10.41129
Epoch 1/80: current_loss=10.40945 | best_loss=10.40945
Epoch 2/80: current_loss=10.44889 | best_loss=10.40945
Epoch 3/80: current_loss=10.40461 | best_loss=10.40461
Epoch 4/80: current_loss=10.49128 | best_loss=10.40461
Epoch 5/80: current_loss=10.44534 | best_loss=10.40461
Epoch 6/80: current_loss=10.40728 | best_loss=10.40461
Epoch 7/80: current_loss=10.43379 | best_loss=10.40461
Epoch 8/80: current_loss=10.42170 | best_loss=10.40461
Epoch 9/80: current_loss=10.46032 | best_loss=10.40461
Epoch 10/80: current_loss=10.44202 | best_loss=10.40461
Epoch 11/80: current_loss=10.42567 | best_loss=10.40461
Epoch 12/80: current_loss=10.41351 | best_loss=10.40461
Epoch 13/80: current_loss=10.40503 | best_loss=10.40461
Epoch 14/80: current_loss=10.44582 | best_loss=10.40461
Epoch 15/80: current_loss=10.41017 | best_loss=10.40461
Epoch 16/80: current_loss=10.42833 | best_loss=10.40461
Epoch 17/80: current_loss=10.43694 | best_loss=10.40461
Epoch 18/80: current_loss=10.45698 | best_loss=10.40461
Epoch 19/80: current_loss=10.40489 | best_loss=10.40461
Epoch 20/80: current_loss=10.40663 | best_loss=10.40461
Epoch 21/80: current_loss=10.42105 | best_loss=10.40461
Epoch 22/80: current_loss=10.46360 | best_loss=10.40461
Epoch 23/80: current_loss=10.43257 | best_loss=10.40461
Early Stopping at epoch 23
      explained_var=0.00034 | mse_loss=10.06176

----------------------------------------------
Params for Trial 74
{'learning_rate': 0.001, 'weight_decay': 0.007678282427194871, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.33952 | best_loss=16.33952
Epoch 1/80: current_loss=10.70475 | best_loss=10.70475
Epoch 2/80: current_loss=10.42587 | best_loss=10.42587
Epoch 3/80: current_loss=10.36156 | best_loss=10.36156
Epoch 4/80: current_loss=10.34361 | best_loss=10.34361
Epoch 5/80: current_loss=10.32370 | best_loss=10.32370
Epoch 6/80: current_loss=10.29677 | best_loss=10.29677
Epoch 7/80: current_loss=10.33406 | best_loss=10.29677
Epoch 8/80: current_loss=10.29118 | best_loss=10.29118
Epoch 9/80: current_loss=10.28409 | best_loss=10.28409
Epoch 10/80: current_loss=10.29049 | best_loss=10.28409
Epoch 11/80: current_loss=10.29046 | best_loss=10.28409
Epoch 12/80: current_loss=10.29760 | best_loss=10.28409
Epoch 13/80: current_loss=10.28640 | best_loss=10.28409
Epoch 14/80: current_loss=10.28394 | best_loss=10.28394
Epoch 15/80: current_loss=10.28589 | best_loss=10.28394
Epoch 16/80: current_loss=10.28201 | best_loss=10.28201
Epoch 17/80: current_loss=10.28154 | best_loss=10.28154
Epoch 18/80: current_loss=10.28209 | best_loss=10.28154
Epoch 19/80: current_loss=10.27210 | best_loss=10.27210
Epoch 20/80: current_loss=10.29595 | best_loss=10.27210
Epoch 21/80: current_loss=10.32847 | best_loss=10.27210
Epoch 22/80: current_loss=10.30241 | best_loss=10.27210
Epoch 23/80: current_loss=10.26423 | best_loss=10.26423
Epoch 24/80: current_loss=10.27582 | best_loss=10.26423
Epoch 25/80: current_loss=10.27441 | best_loss=10.26423
Epoch 26/80: current_loss=10.33398 | best_loss=10.26423
Epoch 27/80: current_loss=10.28581 | best_loss=10.26423
Epoch 28/80: current_loss=10.30128 | best_loss=10.26423
Epoch 29/80: current_loss=10.29958 | best_loss=10.26423
Epoch 30/80: current_loss=10.28882 | best_loss=10.26423
Epoch 31/80: current_loss=10.29265 | best_loss=10.26423
Epoch 32/80: current_loss=10.31262 | best_loss=10.26423
Epoch 33/80: current_loss=10.29496 | best_loss=10.26423
Epoch 34/80: current_loss=10.30227 | best_loss=10.26423
Epoch 35/80: current_loss=10.27308 | best_loss=10.26423
Epoch 36/80: current_loss=10.28679 | best_loss=10.26423
Epoch 37/80: current_loss=10.27062 | best_loss=10.26423
Epoch 38/80: current_loss=10.28261 | best_loss=10.26423
Epoch 39/80: current_loss=10.31636 | best_loss=10.26423
Epoch 40/80: current_loss=10.29961 | best_loss=10.26423
Epoch 41/80: current_loss=10.30550 | best_loss=10.26423
Epoch 42/80: current_loss=10.28651 | best_loss=10.26423
Epoch 43/80: current_loss=10.30593 | best_loss=10.26423
Early Stopping at epoch 43
      explained_var=-0.00180 | mse_loss=10.48859
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.40793 | best_loss=10.40793
Epoch 1/80: current_loss=10.45910 | best_loss=10.40793
Epoch 2/80: current_loss=10.44314 | best_loss=10.40793
Epoch 3/80: current_loss=10.42203 | best_loss=10.40793
Epoch 4/80: current_loss=10.43031 | best_loss=10.40793
Epoch 5/80: current_loss=10.40808 | best_loss=10.40793
Epoch 6/80: current_loss=10.53586 | best_loss=10.40793
Epoch 7/80: current_loss=10.40698 | best_loss=10.40698
Epoch 8/80: current_loss=10.42235 | best_loss=10.40698
Epoch 9/80: current_loss=10.43230 | best_loss=10.40698
Epoch 10/80: current_loss=10.41013 | best_loss=10.40698
Epoch 11/80: current_loss=10.42118 | best_loss=10.40698
Epoch 12/80: current_loss=10.41779 | best_loss=10.40698
Epoch 13/80: current_loss=10.48327 | best_loss=10.40698
Epoch 14/80: current_loss=10.40875 | best_loss=10.40698
Epoch 15/80: current_loss=10.45843 | best_loss=10.40698
Epoch 16/80: current_loss=10.43483 | best_loss=10.40698
Epoch 17/80: current_loss=10.48770 | best_loss=10.40698
Epoch 18/80: current_loss=10.42892 | best_loss=10.40698
Epoch 19/80: current_loss=10.42216 | best_loss=10.40698
Epoch 20/80: current_loss=10.43784 | best_loss=10.40698
Epoch 21/80: current_loss=10.41249 | best_loss=10.40698
Epoch 22/80: current_loss=10.42866 | best_loss=10.40698
Epoch 23/80: current_loss=10.46340 | best_loss=10.40698
Epoch 24/80: current_loss=10.41641 | best_loss=10.40698
Epoch 25/80: current_loss=10.41036 | best_loss=10.40698
Epoch 26/80: current_loss=10.40845 | best_loss=10.40698
Epoch 27/80: current_loss=10.41851 | best_loss=10.40698
Early Stopping at epoch 27
      explained_var=0.00026 | mse_loss=10.06113

----------------------------------------------
Params for Trial 75
{'learning_rate': 0.0001, 'weight_decay': 0.006975364374196965, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=88.03912 | best_loss=88.03912
Epoch 1/80: current_loss=51.48702 | best_loss=51.48702
Epoch 2/80: current_loss=32.84710 | best_loss=32.84710
Epoch 3/80: current_loss=25.33651 | best_loss=25.33651
Epoch 4/80: current_loss=21.48843 | best_loss=21.48843
Epoch 5/80: current_loss=19.04893 | best_loss=19.04893
Epoch 6/80: current_loss=17.35226 | best_loss=17.35226
Epoch 7/80: current_loss=16.06287 | best_loss=16.06287
Epoch 8/80: current_loss=15.03672 | best_loss=15.03672
Epoch 9/80: current_loss=14.26636 | best_loss=14.26636
Epoch 10/80: current_loss=13.63093 | best_loss=13.63093
Epoch 11/80: current_loss=13.09794 | best_loss=13.09794
Epoch 12/80: current_loss=12.67415 | best_loss=12.67415
Epoch 13/80: current_loss=12.31120 | best_loss=12.31120
Epoch 14/80: current_loss=12.04880 | best_loss=12.04880
Epoch 15/80: current_loss=11.78368 | best_loss=11.78368
Epoch 16/80: current_loss=11.59897 | best_loss=11.59897
Epoch 17/80: current_loss=11.44676 | best_loss=11.44676
Epoch 18/80: current_loss=11.33923 | best_loss=11.33923
Epoch 19/80: current_loss=11.21951 | best_loss=11.21951
Epoch 20/80: current_loss=11.11555 | best_loss=11.11555
Epoch 21/80: current_loss=11.02558 | best_loss=11.02558
Epoch 22/80: current_loss=10.96702 | best_loss=10.96702
Epoch 23/80: current_loss=10.90202 | best_loss=10.90202
Epoch 24/80: current_loss=10.84728 | best_loss=10.84728
Epoch 25/80: current_loss=10.78984 | best_loss=10.78984
Epoch 26/80: current_loss=10.73822 | best_loss=10.73822
Epoch 27/80: current_loss=10.69381 | best_loss=10.69381
Epoch 28/80: current_loss=10.65703 | best_loss=10.65703
Epoch 29/80: current_loss=10.62999 | best_loss=10.62999
Epoch 30/80: current_loss=10.60143 | best_loss=10.60143
Epoch 31/80: current_loss=10.57619 | best_loss=10.57619
Epoch 32/80: current_loss=10.55422 | best_loss=10.55422
Epoch 33/80: current_loss=10.53588 | best_loss=10.53588
Epoch 34/80: current_loss=10.52547 | best_loss=10.52547
Epoch 35/80: current_loss=10.51072 | best_loss=10.51072
Epoch 36/80: current_loss=10.49584 | best_loss=10.49584
Epoch 37/80: current_loss=10.47641 | best_loss=10.47641
Epoch 38/80: current_loss=10.46744 | best_loss=10.46744
Epoch 39/80: current_loss=10.45774 | best_loss=10.45774
Epoch 40/80: current_loss=10.45325 | best_loss=10.45325
Epoch 41/80: current_loss=10.43435 | best_loss=10.43435
Epoch 42/80: current_loss=10.42576 | best_loss=10.42576
Epoch 43/80: current_loss=10.41978 | best_loss=10.41978
Epoch 44/80: current_loss=10.41441 | best_loss=10.41441
Epoch 45/80: current_loss=10.40838 | best_loss=10.40838
Epoch 46/80: current_loss=10.39734 | best_loss=10.39734
Epoch 47/80: current_loss=10.38955 | best_loss=10.38955
Epoch 48/80: current_loss=10.38613 | best_loss=10.38613
Epoch 49/80: current_loss=10.38418 | best_loss=10.38418
Epoch 50/80: current_loss=10.37739 | best_loss=10.37739
Epoch 51/80: current_loss=10.36701 | best_loss=10.36701
Epoch 52/80: current_loss=10.37033 | best_loss=10.36701
Epoch 53/80: current_loss=10.36066 | best_loss=10.36066
Epoch 54/80: current_loss=10.36345 | best_loss=10.36066
Epoch 55/80: current_loss=10.36230 | best_loss=10.36066
Epoch 56/80: current_loss=10.35442 | best_loss=10.35442
Epoch 57/80: current_loss=10.35452 | best_loss=10.35442
Epoch 58/80: current_loss=10.35050 | best_loss=10.35050
Epoch 59/80: current_loss=10.34644 | best_loss=10.34644
Epoch 60/80: current_loss=10.34527 | best_loss=10.34527
Epoch 61/80: current_loss=10.34196 | best_loss=10.34196
Epoch 62/80: current_loss=10.34167 | best_loss=10.34167
Epoch 63/80: current_loss=10.33458 | best_loss=10.33458
Epoch 64/80: current_loss=10.33486 | best_loss=10.33458
Epoch 65/80: current_loss=10.33162 | best_loss=10.33162
Epoch 66/80: current_loss=10.33049 | best_loss=10.33049
Epoch 67/80: current_loss=10.32987 | best_loss=10.32987
Epoch 68/80: current_loss=10.32616 | best_loss=10.32616
Epoch 69/80: current_loss=10.32226 | best_loss=10.32226
Epoch 70/80: current_loss=10.33169 | best_loss=10.32226
Epoch 71/80: current_loss=10.32712 | best_loss=10.32226
Epoch 72/80: current_loss=10.32464 | best_loss=10.32226
Epoch 73/80: current_loss=10.32566 | best_loss=10.32226
Epoch 74/80: current_loss=10.32015 | best_loss=10.32015
Epoch 75/80: current_loss=10.31811 | best_loss=10.31811
Epoch 76/80: current_loss=10.31648 | best_loss=10.31648
Epoch 77/80: current_loss=10.31760 | best_loss=10.31648
Epoch 78/80: current_loss=10.31643 | best_loss=10.31643
Epoch 79/80: current_loss=10.32239 | best_loss=10.31643
      explained_var=-0.00628 | mse_loss=10.55858

----------------------------------------------
Params for Trial 76
{'learning_rate': 0.001, 'weight_decay': 0.00831210399199769, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=63.01726 | best_loss=63.01726
Epoch 1/80: current_loss=43.85010 | best_loss=43.85010
Epoch 2/80: current_loss=34.27639 | best_loss=34.27639
Epoch 3/80: current_loss=27.68143 | best_loss=27.68143
Epoch 4/80: current_loss=22.66348 | best_loss=22.66348
Epoch 5/80: current_loss=19.00460 | best_loss=19.00460
Epoch 6/80: current_loss=16.28980 | best_loss=16.28980
Epoch 7/80: current_loss=14.37970 | best_loss=14.37970
Epoch 8/80: current_loss=13.01294 | best_loss=13.01294
Epoch 9/80: current_loss=12.09457 | best_loss=12.09457
Epoch 10/80: current_loss=11.47021 | best_loss=11.47021
Epoch 11/80: current_loss=11.06529 | best_loss=11.06529
Epoch 12/80: current_loss=10.80630 | best_loss=10.80630
Epoch 13/80: current_loss=10.60410 | best_loss=10.60410
Epoch 14/80: current_loss=10.52698 | best_loss=10.52698
Epoch 15/80: current_loss=10.42393 | best_loss=10.42393
Epoch 16/80: current_loss=10.38442 | best_loss=10.38442
Epoch 17/80: current_loss=10.36118 | best_loss=10.36118
Epoch 18/80: current_loss=10.34973 | best_loss=10.34973
Epoch 19/80: current_loss=10.32337 | best_loss=10.32337
Epoch 20/80: current_loss=10.32539 | best_loss=10.32337
Epoch 21/80: current_loss=10.32072 | best_loss=10.32072
Epoch 22/80: current_loss=10.32497 | best_loss=10.32072
Epoch 23/80: current_loss=10.30944 | best_loss=10.30944
Epoch 24/80: current_loss=10.31034 | best_loss=10.30944
Epoch 25/80: current_loss=10.31293 | best_loss=10.30944
Epoch 26/80: current_loss=10.30305 | best_loss=10.30305
Epoch 27/80: current_loss=10.30473 | best_loss=10.30305
Epoch 28/80: current_loss=10.30167 | best_loss=10.30167
Epoch 29/80: current_loss=10.31235 | best_loss=10.30167
Epoch 30/80: current_loss=10.30492 | best_loss=10.30167
Epoch 31/80: current_loss=10.30388 | best_loss=10.30167
Epoch 32/80: current_loss=10.29764 | best_loss=10.29764
Epoch 33/80: current_loss=10.29881 | best_loss=10.29764
Epoch 34/80: current_loss=10.29353 | best_loss=10.29353
Epoch 35/80: current_loss=10.29608 | best_loss=10.29353
Epoch 36/80: current_loss=10.29475 | best_loss=10.29353
Epoch 37/80: current_loss=10.29487 | best_loss=10.29353
Epoch 38/80: current_loss=10.28901 | best_loss=10.28901
Epoch 39/80: current_loss=10.30781 | best_loss=10.28901
Epoch 40/80: current_loss=10.30348 | best_loss=10.28901
Epoch 41/80: current_loss=10.31615 | best_loss=10.28901
Epoch 42/80: current_loss=10.31241 | best_loss=10.28901
Epoch 43/80: current_loss=10.30292 | best_loss=10.28901
Epoch 44/80: current_loss=10.30244 | best_loss=10.28901
Epoch 45/80: current_loss=10.29644 | best_loss=10.28901
Epoch 46/80: current_loss=10.30517 | best_loss=10.28901
Epoch 47/80: current_loss=10.29255 | best_loss=10.28901
Epoch 48/80: current_loss=10.29630 | best_loss=10.28901
Epoch 49/80: current_loss=10.29671 | best_loss=10.28901
Epoch 50/80: current_loss=10.28902 | best_loss=10.28901
Epoch 51/80: current_loss=10.29021 | best_loss=10.28901
Epoch 52/80: current_loss=10.29127 | best_loss=10.28901
Epoch 53/80: current_loss=10.30459 | best_loss=10.28901
Epoch 54/80: current_loss=10.31282 | best_loss=10.28901
Epoch 55/80: current_loss=10.31616 | best_loss=10.28901
Epoch 56/80: current_loss=10.30232 | best_loss=10.28901
Epoch 57/80: current_loss=10.29373 | best_loss=10.28901
Epoch 58/80: current_loss=10.29280 | best_loss=10.28901
Early Stopping at epoch 58
      explained_var=-0.00265 | mse_loss=10.52744

----------------------------------------------
Params for Trial 77
{'learning_rate': 0.1, 'weight_decay': 0.008715450759839153, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.12349 | best_loss=17.12349
Epoch 1/80: current_loss=12.00175 | best_loss=12.00175
Epoch 2/80: current_loss=10.87463 | best_loss=10.87463
Epoch 3/80: current_loss=11.05881 | best_loss=10.87463
Epoch 4/80: current_loss=11.05581 | best_loss=10.87463
Epoch 5/80: current_loss=10.34874 | best_loss=10.34874
Epoch 6/80: current_loss=11.19432 | best_loss=10.34874
Epoch 7/80: current_loss=10.26850 | best_loss=10.26850
Epoch 8/80: current_loss=12.74881 | best_loss=10.26850
Epoch 9/80: current_loss=12.07581 | best_loss=10.26850
Epoch 10/80: current_loss=10.40325 | best_loss=10.26850
Epoch 11/80: current_loss=11.62914 | best_loss=10.26850
Epoch 12/80: current_loss=10.70814 | best_loss=10.26850
Epoch 13/80: current_loss=13.59237 | best_loss=10.26850
Epoch 14/80: current_loss=12.18169 | best_loss=10.26850
Epoch 15/80: current_loss=11.11066 | best_loss=10.26850
Epoch 16/80: current_loss=11.44134 | best_loss=10.26850
Epoch 17/80: current_loss=14.85288 | best_loss=10.26850
Epoch 18/80: current_loss=13.26810 | best_loss=10.26850
Epoch 19/80: current_loss=19.69726 | best_loss=10.26850
Epoch 20/80: current_loss=10.45997 | best_loss=10.26850
Epoch 21/80: current_loss=14.84670 | best_loss=10.26850
Epoch 22/80: current_loss=10.58382 | best_loss=10.26850
Epoch 23/80: current_loss=10.45283 | best_loss=10.26850
Epoch 24/80: current_loss=11.15735 | best_loss=10.26850
Epoch 25/80: current_loss=11.06934 | best_loss=10.26850
Epoch 26/80: current_loss=17.25078 | best_loss=10.26850
Epoch 27/80: current_loss=10.83226 | best_loss=10.26850
Early Stopping at epoch 27
      explained_var=0.00178 | mse_loss=10.44871
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.48671 | best_loss=10.48671
Epoch 1/80: current_loss=10.74375 | best_loss=10.48671
Epoch 2/80: current_loss=10.48863 | best_loss=10.48671
Epoch 3/80: current_loss=10.75335 | best_loss=10.48671
Epoch 4/80: current_loss=14.08794 | best_loss=10.48671
Epoch 5/80: current_loss=14.51876 | best_loss=10.48671
Epoch 6/80: current_loss=23.21571 | best_loss=10.48671
Epoch 7/80: current_loss=10.80298 | best_loss=10.48671
Epoch 8/80: current_loss=11.51090 | best_loss=10.48671
Epoch 9/80: current_loss=11.22484 | best_loss=10.48671
Epoch 10/80: current_loss=15.12134 | best_loss=10.48671
Epoch 11/80: current_loss=13.51385 | best_loss=10.48671
Epoch 12/80: current_loss=10.58103 | best_loss=10.48671
Epoch 13/80: current_loss=11.20982 | best_loss=10.48671
Epoch 14/80: current_loss=10.57318 | best_loss=10.48671
Epoch 15/80: current_loss=13.22088 | best_loss=10.48671
Epoch 16/80: current_loss=11.66946 | best_loss=10.48671
Epoch 17/80: current_loss=11.08241 | best_loss=10.48671
Epoch 18/80: current_loss=14.31887 | best_loss=10.48671
Epoch 19/80: current_loss=11.98824 | best_loss=10.48671
Epoch 20/80: current_loss=11.43616 | best_loss=10.48671
Early Stopping at epoch 20
      explained_var=0.00470 | mse_loss=10.17254

----------------------------------------------
Params for Trial 78
{'learning_rate': 0.01, 'weight_decay': 0.009330084704754018, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.86351 | best_loss=10.86351
Epoch 1/80: current_loss=10.53378 | best_loss=10.53378
Epoch 2/80: current_loss=10.67108 | best_loss=10.53378
Epoch 3/80: current_loss=10.38305 | best_loss=10.38305
Epoch 4/80: current_loss=10.25654 | best_loss=10.25654
Epoch 5/80: current_loss=10.28893 | best_loss=10.25654
Epoch 6/80: current_loss=10.26287 | best_loss=10.25654
Epoch 7/80: current_loss=10.38463 | best_loss=10.25654
Epoch 8/80: current_loss=10.39473 | best_loss=10.25654
Epoch 9/80: current_loss=10.26935 | best_loss=10.25654
Epoch 10/80: current_loss=10.27138 | best_loss=10.25654
Epoch 11/80: current_loss=11.33692 | best_loss=10.25654
Epoch 12/80: current_loss=10.29655 | best_loss=10.25654
Epoch 13/80: current_loss=10.25937 | best_loss=10.25654
Epoch 14/80: current_loss=10.41809 | best_loss=10.25654
Epoch 15/80: current_loss=10.32608 | best_loss=10.25654
Epoch 16/80: current_loss=10.25423 | best_loss=10.25423
Epoch 17/80: current_loss=10.37037 | best_loss=10.25423
Epoch 18/80: current_loss=10.37266 | best_loss=10.25423
Epoch 19/80: current_loss=10.28657 | best_loss=10.25423
Epoch 20/80: current_loss=10.33238 | best_loss=10.25423
Epoch 21/80: current_loss=10.26128 | best_loss=10.25423
Epoch 22/80: current_loss=11.59218 | best_loss=10.25423
Epoch 23/80: current_loss=10.60395 | best_loss=10.25423
Epoch 24/80: current_loss=10.44417 | best_loss=10.25423
Epoch 25/80: current_loss=10.51669 | best_loss=10.25423
Epoch 26/80: current_loss=10.34241 | best_loss=10.25423
Epoch 27/80: current_loss=10.82979 | best_loss=10.25423
Epoch 28/80: current_loss=10.39223 | best_loss=10.25423
Epoch 29/80: current_loss=10.33300 | best_loss=10.25423
Epoch 30/80: current_loss=10.39900 | best_loss=10.25423
Epoch 31/80: current_loss=10.33221 | best_loss=10.25423
Epoch 32/80: current_loss=10.44277 | best_loss=10.25423
Epoch 33/80: current_loss=11.26078 | best_loss=10.25423
Epoch 34/80: current_loss=11.55661 | best_loss=10.25423
Epoch 35/80: current_loss=10.38083 | best_loss=10.25423
Epoch 36/80: current_loss=10.25898 | best_loss=10.25423
Early Stopping at epoch 36
      explained_var=-0.00075 | mse_loss=10.46542
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.90750 | best_loss=10.90750
Epoch 1/80: current_loss=10.92645 | best_loss=10.90750
Epoch 2/80: current_loss=10.47254 | best_loss=10.47254
Epoch 3/80: current_loss=10.38648 | best_loss=10.38648
Epoch 4/80: current_loss=10.85168 | best_loss=10.38648
Epoch 5/80: current_loss=10.54879 | best_loss=10.38648
Epoch 6/80: current_loss=10.43562 | best_loss=10.38648
Epoch 7/80: current_loss=10.41925 | best_loss=10.38648
Epoch 8/80: current_loss=10.57970 | best_loss=10.38648
Epoch 9/80: current_loss=10.44700 | best_loss=10.38648
Epoch 10/80: current_loss=11.79969 | best_loss=10.38648
Epoch 11/80: current_loss=10.75474 | best_loss=10.38648
Epoch 12/80: current_loss=10.47875 | best_loss=10.38648
Epoch 13/80: current_loss=11.13747 | best_loss=10.38648
Epoch 14/80: current_loss=11.25074 | best_loss=10.38648
Epoch 15/80: current_loss=10.75076 | best_loss=10.38648
Epoch 16/80: current_loss=11.01099 | best_loss=10.38648
Epoch 17/80: current_loss=10.46018 | best_loss=10.38648
Epoch 18/80: current_loss=10.41236 | best_loss=10.38648
Epoch 19/80: current_loss=10.42200 | best_loss=10.38648
Epoch 20/80: current_loss=10.39286 | best_loss=10.38648
Epoch 21/80: current_loss=10.56485 | best_loss=10.38648
Epoch 22/80: current_loss=10.39538 | best_loss=10.38648
Epoch 23/80: current_loss=10.49588 | best_loss=10.38648
Early Stopping at epoch 23
      explained_var=0.00317 | mse_loss=10.03447
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.52693 | best_loss=7.52693
Epoch 1/80: current_loss=7.54636 | best_loss=7.52693
Epoch 2/80: current_loss=7.84651 | best_loss=7.52693
Epoch 3/80: current_loss=7.50872 | best_loss=7.50872
Epoch 4/80: current_loss=8.15273 | best_loss=7.50872
Epoch 5/80: current_loss=7.52497 | best_loss=7.50872
Epoch 6/80: current_loss=7.52419 | best_loss=7.50872
Epoch 7/80: current_loss=7.51983 | best_loss=7.50872
Epoch 8/80: current_loss=7.62192 | best_loss=7.50872
Epoch 9/80: current_loss=7.60252 | best_loss=7.50872
Epoch 10/80: current_loss=7.69905 | best_loss=7.50872
Epoch 11/80: current_loss=7.74524 | best_loss=7.50872
Epoch 12/80: current_loss=8.88526 | best_loss=7.50872
Epoch 13/80: current_loss=7.99425 | best_loss=7.50872
Epoch 14/80: current_loss=7.73780 | best_loss=7.50872
Epoch 15/80: current_loss=7.62282 | best_loss=7.50872
Epoch 16/80: current_loss=8.60650 | best_loss=7.50872
Epoch 17/80: current_loss=7.65608 | best_loss=7.50872
Epoch 18/80: current_loss=7.50811 | best_loss=7.50811
Epoch 19/80: current_loss=8.00415 | best_loss=7.50811
Epoch 20/80: current_loss=7.54758 | best_loss=7.50811
Epoch 21/80: current_loss=7.47906 | best_loss=7.47906
Epoch 22/80: current_loss=7.98206 | best_loss=7.47906
Epoch 23/80: current_loss=7.70091 | best_loss=7.47906
Epoch 24/80: current_loss=7.61890 | best_loss=7.47906
Epoch 25/80: current_loss=7.63908 | best_loss=7.47906
Epoch 26/80: current_loss=8.80030 | best_loss=7.47906
Epoch 27/80: current_loss=7.80442 | best_loss=7.47906
Epoch 28/80: current_loss=8.29362 | best_loss=7.47906
Epoch 29/80: current_loss=7.45631 | best_loss=7.45631
Epoch 30/80: current_loss=7.72787 | best_loss=7.45631
Epoch 31/80: current_loss=7.72420 | best_loss=7.45631
Epoch 32/80: current_loss=7.71988 | best_loss=7.45631
Epoch 33/80: current_loss=7.58253 | best_loss=7.45631
Epoch 34/80: current_loss=8.60815 | best_loss=7.45631
Epoch 35/80: current_loss=7.70721 | best_loss=7.45631
Epoch 36/80: current_loss=7.73415 | best_loss=7.45631
Epoch 37/80: current_loss=7.50329 | best_loss=7.45631
Epoch 38/80: current_loss=7.68561 | best_loss=7.45631
Epoch 39/80: current_loss=7.73219 | best_loss=7.45631
Epoch 40/80: current_loss=7.58389 | best_loss=7.45631
Epoch 41/80: current_loss=7.58555 | best_loss=7.45631
Epoch 42/80: current_loss=7.64934 | best_loss=7.45631
Epoch 43/80: current_loss=8.56657 | best_loss=7.45631
Epoch 44/80: current_loss=8.05623 | best_loss=7.45631
Epoch 45/80: current_loss=7.48723 | best_loss=7.45631
Epoch 46/80: current_loss=7.49861 | best_loss=7.45631
Epoch 47/80: current_loss=7.48648 | best_loss=7.45631
Epoch 48/80: current_loss=7.52008 | best_loss=7.45631
Epoch 49/80: current_loss=7.71061 | best_loss=7.45631
Early Stopping at epoch 49
      explained_var=0.00999 | mse_loss=7.57032
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.43472 | best_loss=9.43472
Epoch 1/80: current_loss=8.89631 | best_loss=8.89631
Epoch 2/80: current_loss=8.90768 | best_loss=8.89631
Epoch 3/80: current_loss=8.99306 | best_loss=8.89631
Epoch 4/80: current_loss=8.83072 | best_loss=8.83072
Epoch 5/80: current_loss=9.50250 | best_loss=8.83072
Epoch 6/80: current_loss=9.05546 | best_loss=8.83072
Epoch 7/80: current_loss=8.92515 | best_loss=8.83072
Epoch 8/80: current_loss=10.14611 | best_loss=8.83072
Epoch 9/80: current_loss=9.11685 | best_loss=8.83072
Epoch 10/80: current_loss=8.95595 | best_loss=8.83072
Epoch 11/80: current_loss=8.79561 | best_loss=8.79561
Epoch 12/80: current_loss=9.02675 | best_loss=8.79561
Epoch 13/80: current_loss=8.81399 | best_loss=8.79561
Epoch 14/80: current_loss=9.18286 | best_loss=8.79561
Epoch 15/80: current_loss=9.15553 | best_loss=8.79561
Epoch 16/80: current_loss=8.89418 | best_loss=8.79561
Epoch 17/80: current_loss=8.88010 | best_loss=8.79561
Epoch 18/80: current_loss=8.82605 | best_loss=8.79561
Epoch 19/80: current_loss=9.32042 | best_loss=8.79561
Epoch 20/80: current_loss=9.20834 | best_loss=8.79561
Epoch 21/80: current_loss=8.89513 | best_loss=8.79561
Epoch 22/80: current_loss=8.93858 | best_loss=8.79561
Epoch 23/80: current_loss=10.37664 | best_loss=8.79561
Epoch 24/80: current_loss=9.27819 | best_loss=8.79561
Epoch 25/80: current_loss=8.91777 | best_loss=8.79561
Epoch 26/80: current_loss=8.98026 | best_loss=8.79561
Epoch 27/80: current_loss=8.86886 | best_loss=8.79561
Epoch 28/80: current_loss=8.86795 | best_loss=8.79561
Epoch 29/80: current_loss=9.28156 | best_loss=8.79561
Epoch 30/80: current_loss=8.81552 | best_loss=8.79561
Epoch 31/80: current_loss=8.80993 | best_loss=8.79561
Early Stopping at epoch 31
      explained_var=-0.00105 | mse_loss=8.73778
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.18065 | best_loss=10.18065
Epoch 1/80: current_loss=9.75109 | best_loss=9.75109
Epoch 2/80: current_loss=9.66430 | best_loss=9.66430
Epoch 3/80: current_loss=9.62571 | best_loss=9.62571
Epoch 4/80: current_loss=9.66731 | best_loss=9.62571
Epoch 5/80: current_loss=9.38284 | best_loss=9.38284
Epoch 6/80: current_loss=9.46957 | best_loss=9.38284
Epoch 7/80: current_loss=9.49476 | best_loss=9.38284
Epoch 8/80: current_loss=9.52184 | best_loss=9.38284
Epoch 9/80: current_loss=9.38595 | best_loss=9.38284
Epoch 10/80: current_loss=9.46982 | best_loss=9.38284
Epoch 11/80: current_loss=9.37580 | best_loss=9.37580
Epoch 12/80: current_loss=9.40241 | best_loss=9.37580
Epoch 13/80: current_loss=9.56091 | best_loss=9.37580
Epoch 14/80: current_loss=9.37593 | best_loss=9.37580
Epoch 15/80: current_loss=9.54984 | best_loss=9.37580
Epoch 16/80: current_loss=9.68497 | best_loss=9.37580
Epoch 17/80: current_loss=9.37663 | best_loss=9.37580
Epoch 18/80: current_loss=9.47353 | best_loss=9.37580
Epoch 19/80: current_loss=9.74504 | best_loss=9.37580
Epoch 20/80: current_loss=9.48092 | best_loss=9.37580
Epoch 21/80: current_loss=9.49016 | best_loss=9.37580
Epoch 22/80: current_loss=9.41161 | best_loss=9.37580
Epoch 23/80: current_loss=9.75352 | best_loss=9.37580
Epoch 24/80: current_loss=9.41103 | best_loss=9.37580
Epoch 25/80: current_loss=9.50789 | best_loss=9.37580
Epoch 26/80: current_loss=9.44445 | best_loss=9.37580
Epoch 27/80: current_loss=9.52331 | best_loss=9.37580
Epoch 28/80: current_loss=10.03011 | best_loss=9.37580
Epoch 29/80: current_loss=9.40356 | best_loss=9.37580
Epoch 30/80: current_loss=9.69603 | best_loss=9.37580
Epoch 31/80: current_loss=9.39208 | best_loss=9.37580
Early Stopping at epoch 31
      explained_var=-0.00058 | mse_loss=9.29982
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.00216| avg_loss=9.22156
----------------------------------------------


----------------------------------------------
Params for Trial 79
{'learning_rate': 0.001, 'weight_decay': 0.0038890570538338888, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=57.51685 | best_loss=57.51685
Epoch 1/80: current_loss=41.71718 | best_loss=41.71718
Epoch 2/80: current_loss=32.79944 | best_loss=32.79944
Epoch 3/80: current_loss=26.42132 | best_loss=26.42132
Epoch 4/80: current_loss=21.63239 | best_loss=21.63239
Epoch 5/80: current_loss=18.14112 | best_loss=18.14112
Epoch 6/80: current_loss=15.63221 | best_loss=15.63221
Epoch 7/80: current_loss=13.88991 | best_loss=13.88991
Epoch 8/80: current_loss=12.67420 | best_loss=12.67420
Epoch 9/80: current_loss=11.84864 | best_loss=11.84864
Epoch 10/80: current_loss=11.31020 | best_loss=11.31020
Epoch 11/80: current_loss=10.93893 | best_loss=10.93893
Epoch 12/80: current_loss=10.71221 | best_loss=10.71221
Epoch 13/80: current_loss=10.55024 | best_loss=10.55024
Epoch 14/80: current_loss=10.49116 | best_loss=10.49116
Epoch 15/80: current_loss=10.41589 | best_loss=10.41589
Epoch 16/80: current_loss=10.36812 | best_loss=10.36812
Epoch 17/80: current_loss=10.35618 | best_loss=10.35618
Epoch 18/80: current_loss=10.32765 | best_loss=10.32765
Epoch 19/80: current_loss=10.31447 | best_loss=10.31447
Epoch 20/80: current_loss=10.32053 | best_loss=10.31447
Epoch 21/80: current_loss=10.31025 | best_loss=10.31025
Epoch 22/80: current_loss=10.30929 | best_loss=10.30929
Epoch 23/80: current_loss=10.30014 | best_loss=10.30014
Epoch 24/80: current_loss=10.30890 | best_loss=10.30014
Epoch 25/80: current_loss=10.30511 | best_loss=10.30014
Epoch 26/80: current_loss=10.30221 | best_loss=10.30014
Epoch 27/80: current_loss=10.29869 | best_loss=10.29869
Epoch 28/80: current_loss=10.29956 | best_loss=10.29869
Epoch 29/80: current_loss=10.29267 | best_loss=10.29267
Epoch 30/80: current_loss=10.30242 | best_loss=10.29267
Epoch 31/80: current_loss=10.30671 | best_loss=10.29267
Epoch 32/80: current_loss=10.30325 | best_loss=10.29267
Epoch 33/80: current_loss=10.30975 | best_loss=10.29267
Epoch 34/80: current_loss=10.29947 | best_loss=10.29267
Epoch 35/80: current_loss=10.30944 | best_loss=10.29267
Epoch 36/80: current_loss=10.30656 | best_loss=10.29267
Epoch 37/80: current_loss=10.29644 | best_loss=10.29267
Epoch 38/80: current_loss=10.30881 | best_loss=10.29267
Epoch 39/80: current_loss=10.29493 | best_loss=10.29267
Epoch 40/80: current_loss=10.29177 | best_loss=10.29177
Epoch 41/80: current_loss=10.28446 | best_loss=10.28446
Epoch 42/80: current_loss=10.29291 | best_loss=10.28446
Epoch 43/80: current_loss=10.29573 | best_loss=10.28446
Epoch 44/80: current_loss=10.28798 | best_loss=10.28446
Epoch 45/80: current_loss=10.29748 | best_loss=10.28446
Epoch 46/80: current_loss=10.30011 | best_loss=10.28446
Epoch 47/80: current_loss=10.31169 | best_loss=10.28446
Epoch 48/80: current_loss=10.31769 | best_loss=10.28446
Epoch 49/80: current_loss=10.30219 | best_loss=10.28446
Epoch 50/80: current_loss=10.29852 | best_loss=10.28446
Epoch 51/80: current_loss=10.30846 | best_loss=10.28446
Epoch 52/80: current_loss=10.29694 | best_loss=10.28446
Epoch 53/80: current_loss=10.29930 | best_loss=10.28446
Epoch 54/80: current_loss=10.29134 | best_loss=10.28446
Epoch 55/80: current_loss=10.30083 | best_loss=10.28446
Epoch 56/80: current_loss=10.30256 | best_loss=10.28446
Epoch 57/80: current_loss=10.30378 | best_loss=10.28446
Epoch 58/80: current_loss=10.31510 | best_loss=10.28446
Epoch 59/80: current_loss=10.31730 | best_loss=10.28446
Epoch 60/80: current_loss=10.30571 | best_loss=10.28446
Epoch 61/80: current_loss=10.31545 | best_loss=10.28446
Early Stopping at epoch 61
      explained_var=-0.00241 | mse_loss=10.52093

----------------------------------------------
Params for Trial 80
{'learning_rate': 0.001, 'weight_decay': 0.005201603152699986, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.57371 | best_loss=16.57371
Epoch 1/80: current_loss=10.71881 | best_loss=10.71881
Epoch 2/80: current_loss=10.40926 | best_loss=10.40926
Epoch 3/80: current_loss=10.35251 | best_loss=10.35251
Epoch 4/80: current_loss=10.35156 | best_loss=10.35156
Epoch 5/80: current_loss=10.32957 | best_loss=10.32957
Epoch 6/80: current_loss=10.35920 | best_loss=10.32957
Epoch 7/80: current_loss=10.30416 | best_loss=10.30416
Epoch 8/80: current_loss=10.30564 | best_loss=10.30416
Epoch 9/80: current_loss=10.30277 | best_loss=10.30277
Epoch 10/80: current_loss=10.29488 | best_loss=10.29488
Epoch 11/80: current_loss=10.28393 | best_loss=10.28393
Epoch 12/80: current_loss=10.29323 | best_loss=10.28393
Epoch 13/80: current_loss=10.30111 | best_loss=10.28393
Epoch 14/80: current_loss=10.27367 | best_loss=10.27367
Epoch 15/80: current_loss=10.28184 | best_loss=10.27367
Epoch 16/80: current_loss=10.30780 | best_loss=10.27367
Epoch 17/80: current_loss=10.29034 | best_loss=10.27367
Epoch 18/80: current_loss=10.28834 | best_loss=10.27367
Epoch 19/80: current_loss=10.29847 | best_loss=10.27367
Epoch 20/80: current_loss=10.30062 | best_loss=10.27367
Epoch 21/80: current_loss=10.30007 | best_loss=10.27367
Epoch 22/80: current_loss=10.27989 | best_loss=10.27367
Epoch 23/80: current_loss=10.30011 | best_loss=10.27367
Epoch 24/80: current_loss=10.29566 | best_loss=10.27367
Epoch 25/80: current_loss=10.32794 | best_loss=10.27367
Epoch 26/80: current_loss=10.29093 | best_loss=10.27367
Epoch 27/80: current_loss=10.29189 | best_loss=10.27367
Epoch 28/80: current_loss=10.29796 | best_loss=10.27367
Epoch 29/80: current_loss=10.29681 | best_loss=10.27367
Epoch 30/80: current_loss=10.29373 | best_loss=10.27367
Epoch 31/80: current_loss=10.27982 | best_loss=10.27367
Epoch 32/80: current_loss=10.28709 | best_loss=10.27367
Epoch 33/80: current_loss=10.27524 | best_loss=10.27367
Epoch 34/80: current_loss=10.29224 | best_loss=10.27367
Early Stopping at epoch 34
      explained_var=-0.00294 | mse_loss=10.50053
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.40470 | best_loss=10.40470
Epoch 1/80: current_loss=10.41376 | best_loss=10.40470
Epoch 2/80: current_loss=10.43080 | best_loss=10.40470
Epoch 3/80: current_loss=10.49835 | best_loss=10.40470
Epoch 4/80: current_loss=10.44207 | best_loss=10.40470
Epoch 5/80: current_loss=10.40845 | best_loss=10.40470
Epoch 6/80: current_loss=10.41466 | best_loss=10.40470
Epoch 7/80: current_loss=10.43061 | best_loss=10.40470
Epoch 8/80: current_loss=10.43137 | best_loss=10.40470
Epoch 9/80: current_loss=10.42973 | best_loss=10.40470
Epoch 10/80: current_loss=10.40780 | best_loss=10.40470
Epoch 11/80: current_loss=10.44605 | best_loss=10.40470
Epoch 12/80: current_loss=10.42278 | best_loss=10.40470
Epoch 13/80: current_loss=10.44983 | best_loss=10.40470
Epoch 14/80: current_loss=10.42391 | best_loss=10.40470
Epoch 15/80: current_loss=10.40472 | best_loss=10.40470
Epoch 16/80: current_loss=10.42496 | best_loss=10.40470
Epoch 17/80: current_loss=10.41084 | best_loss=10.40470
Epoch 18/80: current_loss=10.42981 | best_loss=10.40470
Epoch 19/80: current_loss=10.40538 | best_loss=10.40470
Epoch 20/80: current_loss=10.48064 | best_loss=10.40470
Early Stopping at epoch 20
      explained_var=0.00026 | mse_loss=10.06215

----------------------------------------------
Params for Trial 81
{'learning_rate': 0.001, 'weight_decay': 0.006365702857524511, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.58546 | best_loss=17.58546
Epoch 1/80: current_loss=10.74743 | best_loss=10.74743
Epoch 2/80: current_loss=10.38717 | best_loss=10.38717
Epoch 3/80: current_loss=10.35309 | best_loss=10.35309
Epoch 4/80: current_loss=10.33386 | best_loss=10.33386
Epoch 5/80: current_loss=10.33825 | best_loss=10.33386
Epoch 6/80: current_loss=10.30423 | best_loss=10.30423
Epoch 7/80: current_loss=10.30752 | best_loss=10.30423
Epoch 8/80: current_loss=10.30370 | best_loss=10.30370
Epoch 9/80: current_loss=10.34994 | best_loss=10.30370
Epoch 10/80: current_loss=10.30798 | best_loss=10.30370
Epoch 11/80: current_loss=10.29372 | best_loss=10.29372
Epoch 12/80: current_loss=10.29673 | best_loss=10.29372
Epoch 13/80: current_loss=10.29826 | best_loss=10.29372
Epoch 14/80: current_loss=10.29162 | best_loss=10.29162
Epoch 15/80: current_loss=10.28202 | best_loss=10.28202
Epoch 16/80: current_loss=10.33581 | best_loss=10.28202
Epoch 17/80: current_loss=10.29673 | best_loss=10.28202
Epoch 18/80: current_loss=10.28679 | best_loss=10.28202
Epoch 19/80: current_loss=10.28405 | best_loss=10.28202
Epoch 20/80: current_loss=10.27725 | best_loss=10.27725
Epoch 21/80: current_loss=10.29537 | best_loss=10.27725
Epoch 22/80: current_loss=10.31557 | best_loss=10.27725
Epoch 23/80: current_loss=10.27123 | best_loss=10.27123
Epoch 24/80: current_loss=10.31780 | best_loss=10.27123
Epoch 25/80: current_loss=10.27270 | best_loss=10.27123
Epoch 26/80: current_loss=10.29262 | best_loss=10.27123
Epoch 27/80: current_loss=10.29199 | best_loss=10.27123
Epoch 28/80: current_loss=10.29004 | best_loss=10.27123
Epoch 29/80: current_loss=10.28345 | best_loss=10.27123
Epoch 30/80: current_loss=10.31418 | best_loss=10.27123
Epoch 31/80: current_loss=10.28133 | best_loss=10.27123
Epoch 32/80: current_loss=10.28184 | best_loss=10.27123
Epoch 33/80: current_loss=10.28612 | best_loss=10.27123
Epoch 34/80: current_loss=10.27334 | best_loss=10.27123
Epoch 35/80: current_loss=10.28343 | best_loss=10.27123
Epoch 36/80: current_loss=10.30038 | best_loss=10.27123
Epoch 37/80: current_loss=10.28632 | best_loss=10.27123
Epoch 38/80: current_loss=10.27655 | best_loss=10.27123
Epoch 39/80: current_loss=10.30648 | best_loss=10.27123
Epoch 40/80: current_loss=10.27763 | best_loss=10.27123
Epoch 41/80: current_loss=10.26497 | best_loss=10.26497
Epoch 42/80: current_loss=10.26798 | best_loss=10.26497
Epoch 43/80: current_loss=10.28595 | best_loss=10.26497
Epoch 44/80: current_loss=10.29263 | best_loss=10.26497
Epoch 45/80: current_loss=10.26958 | best_loss=10.26497
Epoch 46/80: current_loss=10.32724 | best_loss=10.26497
Epoch 47/80: current_loss=10.26404 | best_loss=10.26404
Epoch 48/80: current_loss=10.26626 | best_loss=10.26404
Epoch 49/80: current_loss=10.28756 | best_loss=10.26404
Epoch 50/80: current_loss=10.28713 | best_loss=10.26404
Epoch 51/80: current_loss=10.28238 | best_loss=10.26404
Epoch 52/80: current_loss=10.37739 | best_loss=10.26404
Epoch 53/80: current_loss=10.29123 | best_loss=10.26404
Epoch 54/80: current_loss=10.27855 | best_loss=10.26404
Epoch 55/80: current_loss=10.26723 | best_loss=10.26404
Epoch 56/80: current_loss=10.27593 | best_loss=10.26404
Epoch 57/80: current_loss=10.26705 | best_loss=10.26404
Epoch 58/80: current_loss=10.26631 | best_loss=10.26404
Epoch 59/80: current_loss=10.28882 | best_loss=10.26404
Epoch 60/80: current_loss=10.27403 | best_loss=10.26404
Epoch 61/80: current_loss=10.26708 | best_loss=10.26404
Epoch 62/80: current_loss=10.26255 | best_loss=10.26255
Epoch 63/80: current_loss=10.26514 | best_loss=10.26255
Epoch 64/80: current_loss=10.26983 | best_loss=10.26255
Epoch 65/80: current_loss=10.26634 | best_loss=10.26255
Epoch 66/80: current_loss=10.26818 | best_loss=10.26255
Epoch 67/80: current_loss=10.28120 | best_loss=10.26255
Epoch 68/80: current_loss=10.32749 | best_loss=10.26255
Epoch 69/80: current_loss=10.26761 | best_loss=10.26255
Epoch 70/80: current_loss=10.32036 | best_loss=10.26255
Epoch 71/80: current_loss=10.25804 | best_loss=10.25804
Epoch 72/80: current_loss=10.29597 | best_loss=10.25804
Epoch 73/80: current_loss=10.24945 | best_loss=10.24945
Epoch 74/80: current_loss=10.20301 | best_loss=10.20301
Epoch 75/80: current_loss=10.27275 | best_loss=10.20301
Epoch 76/80: current_loss=10.27088 | best_loss=10.20301
Epoch 77/80: current_loss=10.27862 | best_loss=10.20301
Epoch 78/80: current_loss=10.26558 | best_loss=10.20301
Epoch 79/80: current_loss=10.26308 | best_loss=10.20301
      explained_var=0.01014 | mse_loss=10.45725
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.42214 | best_loss=10.42214
Epoch 1/80: current_loss=10.37365 | best_loss=10.37365
Epoch 2/80: current_loss=10.40605 | best_loss=10.37365
Epoch 3/80: current_loss=10.42530 | best_loss=10.37365
Epoch 4/80: current_loss=10.43909 | best_loss=10.37365
Epoch 5/80: current_loss=10.41047 | best_loss=10.37365
Epoch 6/80: current_loss=10.43326 | best_loss=10.37365
Epoch 7/80: current_loss=10.42585 | best_loss=10.37365
Epoch 8/80: current_loss=10.41525 | best_loss=10.37365
Epoch 9/80: current_loss=10.40297 | best_loss=10.37365
Epoch 10/80: current_loss=10.46195 | best_loss=10.37365
Epoch 11/80: current_loss=10.40023 | best_loss=10.37365
Epoch 12/80: current_loss=10.41817 | best_loss=10.37365
Epoch 13/80: current_loss=10.41423 | best_loss=10.37365
Epoch 14/80: current_loss=10.40317 | best_loss=10.37365
Epoch 15/80: current_loss=10.48362 | best_loss=10.37365
Epoch 16/80: current_loss=10.40629 | best_loss=10.37365
Epoch 17/80: current_loss=10.47994 | best_loss=10.37365
Epoch 18/80: current_loss=10.40442 | best_loss=10.37365
Epoch 19/80: current_loss=10.40913 | best_loss=10.37365
Epoch 20/80: current_loss=10.40907 | best_loss=10.37365
Epoch 21/80: current_loss=10.45929 | best_loss=10.37365
Early Stopping at epoch 21
      explained_var=0.00895 | mse_loss=10.01630
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.48341 | best_loss=7.48341
Epoch 1/80: current_loss=7.66237 | best_loss=7.48341
Epoch 2/80: current_loss=7.70585 | best_loss=7.48341
Epoch 3/80: current_loss=7.46778 | best_loss=7.46778
Epoch 4/80: current_loss=7.72175 | best_loss=7.46778
Epoch 5/80: current_loss=7.75102 | best_loss=7.46778
Epoch 6/80: current_loss=7.57870 | best_loss=7.46778
Epoch 7/80: current_loss=7.59362 | best_loss=7.46778
Epoch 8/80: current_loss=7.58319 | best_loss=7.46778
Epoch 9/80: current_loss=7.50516 | best_loss=7.46778
Epoch 10/80: current_loss=7.70269 | best_loss=7.46778
Epoch 11/80: current_loss=7.68804 | best_loss=7.46778
Epoch 12/80: current_loss=7.53687 | best_loss=7.46778
Epoch 13/80: current_loss=7.75149 | best_loss=7.46778
Epoch 14/80: current_loss=7.55562 | best_loss=7.46778
Epoch 15/80: current_loss=7.58889 | best_loss=7.46778
Epoch 16/80: current_loss=7.73585 | best_loss=7.46778
Epoch 17/80: current_loss=7.54480 | best_loss=7.46778
Epoch 18/80: current_loss=7.51021 | best_loss=7.46778
Epoch 19/80: current_loss=7.62044 | best_loss=7.46778
Epoch 20/80: current_loss=7.62971 | best_loss=7.46778
Epoch 21/80: current_loss=7.67290 | best_loss=7.46778
Epoch 22/80: current_loss=7.58190 | best_loss=7.46778
Epoch 23/80: current_loss=7.57943 | best_loss=7.46778
Early Stopping at epoch 23
      explained_var=0.00164 | mse_loss=7.57166
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.75781 | best_loss=8.75781
Epoch 1/80: current_loss=8.83794 | best_loss=8.75781
Epoch 2/80: current_loss=9.34172 | best_loss=8.75781
Epoch 3/80: current_loss=8.85864 | best_loss=8.75781
Epoch 4/80: current_loss=8.78716 | best_loss=8.75781
Epoch 5/80: current_loss=8.85197 | best_loss=8.75781
Epoch 6/80: current_loss=8.98970 | best_loss=8.75781
Epoch 7/80: current_loss=8.94018 | best_loss=8.75781
Epoch 8/80: current_loss=8.86659 | best_loss=8.75781
Epoch 9/80: current_loss=8.79235 | best_loss=8.75781
Epoch 10/80: current_loss=8.77244 | best_loss=8.75781
Epoch 11/80: current_loss=8.84143 | best_loss=8.75781
Epoch 12/80: current_loss=8.81111 | best_loss=8.75781
Epoch 13/80: current_loss=8.84405 | best_loss=8.75781
Epoch 14/80: current_loss=8.91162 | best_loss=8.75781
Epoch 15/80: current_loss=8.79154 | best_loss=8.75781
Epoch 16/80: current_loss=8.74175 | best_loss=8.74175
Epoch 17/80: current_loss=8.77238 | best_loss=8.74175
Epoch 18/80: current_loss=8.75544 | best_loss=8.74175
Epoch 19/80: current_loss=8.79188 | best_loss=8.74175
Epoch 20/80: current_loss=8.74689 | best_loss=8.74175
Epoch 21/80: current_loss=8.72861 | best_loss=8.72861
Epoch 22/80: current_loss=8.79147 | best_loss=8.72861
Epoch 23/80: current_loss=8.77907 | best_loss=8.72861
Epoch 24/80: current_loss=9.10176 | best_loss=8.72861
Epoch 25/80: current_loss=8.79191 | best_loss=8.72861
Epoch 26/80: current_loss=8.81317 | best_loss=8.72861
Epoch 27/80: current_loss=8.80025 | best_loss=8.72861
Epoch 28/80: current_loss=8.81802 | best_loss=8.72861
Epoch 29/80: current_loss=8.89057 | best_loss=8.72861
Epoch 30/80: current_loss=8.87135 | best_loss=8.72861
Epoch 31/80: current_loss=8.88261 | best_loss=8.72861
Epoch 32/80: current_loss=8.81137 | best_loss=8.72861
Epoch 33/80: current_loss=8.85532 | best_loss=8.72861
Epoch 34/80: current_loss=8.80787 | best_loss=8.72861
Epoch 35/80: current_loss=8.87598 | best_loss=8.72861
Epoch 36/80: current_loss=8.87374 | best_loss=8.72861
Epoch 37/80: current_loss=8.86182 | best_loss=8.72861
Epoch 38/80: current_loss=8.84918 | best_loss=8.72861
Epoch 39/80: current_loss=8.79766 | best_loss=8.72861
Epoch 40/80: current_loss=8.79626 | best_loss=8.72861
Epoch 41/80: current_loss=8.75200 | best_loss=8.72861
Early Stopping at epoch 41
      explained_var=0.00302 | mse_loss=8.69362
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.16115 | best_loss=9.16115
Epoch 1/80: current_loss=9.25123 | best_loss=9.16115
Epoch 2/80: current_loss=9.27363 | best_loss=9.16115
Epoch 3/80: current_loss=9.23058 | best_loss=9.16115
Epoch 4/80: current_loss=9.24066 | best_loss=9.16115
Epoch 5/80: current_loss=9.19742 | best_loss=9.16115
Epoch 6/80: current_loss=9.21025 | best_loss=9.16115
Epoch 7/80: current_loss=9.21786 | best_loss=9.16115
Epoch 8/80: current_loss=9.18847 | best_loss=9.16115
Epoch 9/80: current_loss=9.14376 | best_loss=9.14376
Epoch 10/80: current_loss=9.12230 | best_loss=9.12230
Epoch 11/80: current_loss=9.20331 | best_loss=9.12230
Epoch 12/80: current_loss=9.19642 | best_loss=9.12230
Epoch 13/80: current_loss=9.23862 | best_loss=9.12230
Epoch 14/80: current_loss=9.16699 | best_loss=9.12230
Epoch 15/80: current_loss=9.24879 | best_loss=9.12230
Epoch 16/80: current_loss=9.24606 | best_loss=9.12230
Epoch 17/80: current_loss=9.28958 | best_loss=9.12230
Epoch 18/80: current_loss=9.20874 | best_loss=9.12230
Epoch 19/80: current_loss=9.19454 | best_loss=9.12230
Epoch 20/80: current_loss=9.15933 | best_loss=9.12230
Epoch 21/80: current_loss=9.17523 | best_loss=9.12230
Epoch 22/80: current_loss=9.16208 | best_loss=9.12230
Epoch 23/80: current_loss=9.18582 | best_loss=9.12230
Epoch 24/80: current_loss=9.17600 | best_loss=9.12230
Epoch 25/80: current_loss=9.17071 | best_loss=9.12230
Epoch 26/80: current_loss=9.21731 | best_loss=9.12230
Epoch 27/80: current_loss=9.18105 | best_loss=9.12230
Epoch 28/80: current_loss=9.21687 | best_loss=9.12230
Epoch 29/80: current_loss=9.17869 | best_loss=9.12230
Epoch 30/80: current_loss=9.17493 | best_loss=9.12230
Early Stopping at epoch 30
      explained_var=0.02302 | mse_loss=9.07921
----------------------------------------------
Average early_stopping_point: 23| avg_exp_var=0.00935| avg_loss=9.16361
----------------------------------------------


----------------------------------------------
Params for Trial 82
{'learning_rate': 0.001, 'weight_decay': 0.006330740058187831, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.01479 | best_loss=16.01479
Epoch 1/80: current_loss=10.65093 | best_loss=10.65093
Epoch 2/80: current_loss=10.40357 | best_loss=10.40357
Epoch 3/80: current_loss=10.37114 | best_loss=10.37114
Epoch 4/80: current_loss=10.33576 | best_loss=10.33576
Epoch 5/80: current_loss=10.32793 | best_loss=10.32793
Epoch 6/80: current_loss=10.32002 | best_loss=10.32002
Epoch 7/80: current_loss=10.31174 | best_loss=10.31174
Epoch 8/80: current_loss=10.30509 | best_loss=10.30509
Epoch 9/80: current_loss=10.28603 | best_loss=10.28603
Epoch 10/80: current_loss=10.29748 | best_loss=10.28603
Epoch 11/80: current_loss=10.28734 | best_loss=10.28603
Epoch 12/80: current_loss=10.28763 | best_loss=10.28603
Epoch 13/80: current_loss=10.28208 | best_loss=10.28208
Epoch 14/80: current_loss=10.30027 | best_loss=10.28208
Epoch 15/80: current_loss=10.27043 | best_loss=10.27043
Epoch 16/80: current_loss=10.28957 | best_loss=10.27043
Epoch 17/80: current_loss=10.28466 | best_loss=10.27043
Epoch 18/80: current_loss=10.28993 | best_loss=10.27043
Epoch 19/80: current_loss=10.28790 | best_loss=10.27043
Epoch 20/80: current_loss=10.29993 | best_loss=10.27043
Epoch 21/80: current_loss=10.32819 | best_loss=10.27043
Epoch 22/80: current_loss=10.29738 | best_loss=10.27043
Epoch 23/80: current_loss=10.34842 | best_loss=10.27043
Epoch 24/80: current_loss=10.30825 | best_loss=10.27043
Epoch 25/80: current_loss=10.31055 | best_loss=10.27043
Epoch 26/80: current_loss=10.30132 | best_loss=10.27043
Epoch 27/80: current_loss=10.28189 | best_loss=10.27043
Epoch 28/80: current_loss=10.27974 | best_loss=10.27043
Epoch 29/80: current_loss=10.28669 | best_loss=10.27043
Epoch 30/80: current_loss=10.29712 | best_loss=10.27043
Epoch 31/80: current_loss=10.29427 | best_loss=10.27043
Epoch 32/80: current_loss=10.27542 | best_loss=10.27043
Epoch 33/80: current_loss=10.26608 | best_loss=10.26608
Epoch 34/80: current_loss=10.29682 | best_loss=10.26608
Epoch 35/80: current_loss=10.27544 | best_loss=10.26608
Epoch 36/80: current_loss=10.35212 | best_loss=10.26608
Epoch 37/80: current_loss=10.26665 | best_loss=10.26608
Epoch 38/80: current_loss=10.27448 | best_loss=10.26608
Epoch 39/80: current_loss=10.27898 | best_loss=10.26608
Epoch 40/80: current_loss=10.28545 | best_loss=10.26608
Epoch 41/80: current_loss=10.29974 | best_loss=10.26608
Epoch 42/80: current_loss=10.27782 | best_loss=10.26608
Epoch 43/80: current_loss=10.31681 | best_loss=10.26608
Epoch 44/80: current_loss=10.28963 | best_loss=10.26608
Epoch 45/80: current_loss=10.29136 | best_loss=10.26608
Epoch 46/80: current_loss=10.30462 | best_loss=10.26608
Epoch 47/80: current_loss=10.28020 | best_loss=10.26608
Epoch 48/80: current_loss=10.27915 | best_loss=10.26608
Epoch 49/80: current_loss=10.30085 | best_loss=10.26608
Epoch 50/80: current_loss=10.28436 | best_loss=10.26608
Epoch 51/80: current_loss=10.28951 | best_loss=10.26608
Epoch 52/80: current_loss=10.27364 | best_loss=10.26608
Epoch 53/80: current_loss=10.28731 | best_loss=10.26608
Early Stopping at epoch 53
      explained_var=-0.00194 | mse_loss=10.49058
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.44671 | best_loss=10.44671
Epoch 1/80: current_loss=10.40390 | best_loss=10.40390
Epoch 2/80: current_loss=10.42216 | best_loss=10.40390
Epoch 3/80: current_loss=10.41725 | best_loss=10.40390
Epoch 4/80: current_loss=10.41467 | best_loss=10.40390
Epoch 5/80: current_loss=10.44041 | best_loss=10.40390
Epoch 6/80: current_loss=10.40099 | best_loss=10.40099
Epoch 7/80: current_loss=10.40656 | best_loss=10.40099
Epoch 8/80: current_loss=10.42713 | best_loss=10.40099
Epoch 9/80: current_loss=10.41134 | best_loss=10.40099
Epoch 10/80: current_loss=10.43436 | best_loss=10.40099
Epoch 11/80: current_loss=10.43610 | best_loss=10.40099
Epoch 12/80: current_loss=10.43108 | best_loss=10.40099
Epoch 13/80: current_loss=10.41998 | best_loss=10.40099
Epoch 14/80: current_loss=10.42472 | best_loss=10.40099
Epoch 15/80: current_loss=10.42244 | best_loss=10.40099
Epoch 16/80: current_loss=10.41563 | best_loss=10.40099
Epoch 17/80: current_loss=10.45275 | best_loss=10.40099
Epoch 18/80: current_loss=10.43905 | best_loss=10.40099
Epoch 19/80: current_loss=10.36762 | best_loss=10.36762
Epoch 20/80: current_loss=10.49441 | best_loss=10.36762
Epoch 21/80: current_loss=10.40655 | best_loss=10.36762
Epoch 22/80: current_loss=10.43245 | best_loss=10.36762
Epoch 23/80: current_loss=10.43613 | best_loss=10.36762
Epoch 24/80: current_loss=10.41680 | best_loss=10.36762
Epoch 25/80: current_loss=10.77659 | best_loss=10.36762
Epoch 26/80: current_loss=10.41683 | best_loss=10.36762
Epoch 27/80: current_loss=10.43361 | best_loss=10.36762
Epoch 28/80: current_loss=10.40426 | best_loss=10.36762
Epoch 29/80: current_loss=10.41892 | best_loss=10.36762
Epoch 30/80: current_loss=10.41385 | best_loss=10.36762
Epoch 31/80: current_loss=10.46002 | best_loss=10.36762
Epoch 32/80: current_loss=10.40731 | best_loss=10.36762
Epoch 33/80: current_loss=10.41581 | best_loss=10.36762
Epoch 34/80: current_loss=10.40745 | best_loss=10.36762
Epoch 35/80: current_loss=10.43602 | best_loss=10.36762
Epoch 36/80: current_loss=10.39666 | best_loss=10.36762
Epoch 37/80: current_loss=10.46361 | best_loss=10.36762
Epoch 38/80: current_loss=10.40534 | best_loss=10.36762
Epoch 39/80: current_loss=10.40206 | best_loss=10.36762
Early Stopping at epoch 39
      explained_var=0.00489 | mse_loss=10.01899
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.65081 | best_loss=7.65081
Epoch 1/80: current_loss=7.48491 | best_loss=7.48491
Epoch 2/80: current_loss=7.50816 | best_loss=7.48491
Epoch 3/80: current_loss=7.58503 | best_loss=7.48491
Epoch 4/80: current_loss=7.61541 | best_loss=7.48491
Epoch 5/80: current_loss=7.49931 | best_loss=7.48491
Epoch 6/80: current_loss=7.58561 | best_loss=7.48491
Epoch 7/80: current_loss=7.49760 | best_loss=7.48491
Epoch 8/80: current_loss=7.72178 | best_loss=7.48491
Epoch 9/80: current_loss=7.86757 | best_loss=7.48491
Epoch 10/80: current_loss=7.52487 | best_loss=7.48491
Epoch 11/80: current_loss=7.51892 | best_loss=7.48491
Epoch 12/80: current_loss=7.58768 | best_loss=7.48491
Epoch 13/80: current_loss=7.75950 | best_loss=7.48491
Epoch 14/80: current_loss=7.50110 | best_loss=7.48491
Epoch 15/80: current_loss=7.63462 | best_loss=7.48491
Epoch 16/80: current_loss=7.62021 | best_loss=7.48491
Epoch 17/80: current_loss=7.61245 | best_loss=7.48491
Epoch 18/80: current_loss=7.69470 | best_loss=7.48491
Epoch 19/80: current_loss=7.56252 | best_loss=7.48491
Epoch 20/80: current_loss=7.60306 | best_loss=7.48491
Epoch 21/80: current_loss=7.62014 | best_loss=7.48491
Early Stopping at epoch 21
      explained_var=0.00051 | mse_loss=7.58499
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.83923 | best_loss=8.83923
Epoch 1/80: current_loss=8.82124 | best_loss=8.82124
Epoch 2/80: current_loss=8.82064 | best_loss=8.82064
Epoch 3/80: current_loss=8.81613 | best_loss=8.81613
Epoch 4/80: current_loss=8.78191 | best_loss=8.78191
Epoch 5/80: current_loss=8.88248 | best_loss=8.78191
Epoch 6/80: current_loss=8.85483 | best_loss=8.78191
Epoch 7/80: current_loss=8.80178 | best_loss=8.78191
Epoch 8/80: current_loss=8.80356 | best_loss=8.78191
Epoch 9/80: current_loss=8.77080 | best_loss=8.77080
Epoch 10/80: current_loss=8.82311 | best_loss=8.77080
Epoch 11/80: current_loss=8.77861 | best_loss=8.77080
Epoch 12/80: current_loss=8.76045 | best_loss=8.76045
Epoch 13/80: current_loss=8.86796 | best_loss=8.76045
Epoch 14/80: current_loss=8.82954 | best_loss=8.76045
Epoch 15/80: current_loss=8.76748 | best_loss=8.76045
Epoch 16/80: current_loss=8.78213 | best_loss=8.76045
Epoch 17/80: current_loss=8.88480 | best_loss=8.76045
Epoch 18/80: current_loss=8.82104 | best_loss=8.76045
Epoch 19/80: current_loss=8.79120 | best_loss=8.76045
Epoch 20/80: current_loss=8.90039 | best_loss=8.76045
Epoch 21/80: current_loss=8.79468 | best_loss=8.76045
Epoch 22/80: current_loss=9.00369 | best_loss=8.76045
Epoch 23/80: current_loss=8.76551 | best_loss=8.76045
Epoch 24/80: current_loss=8.90826 | best_loss=8.76045
Epoch 25/80: current_loss=8.79487 | best_loss=8.76045
Epoch 26/80: current_loss=8.83076 | best_loss=8.76045
Epoch 27/80: current_loss=8.87003 | best_loss=8.76045
Epoch 28/80: current_loss=8.91081 | best_loss=8.76045
Epoch 29/80: current_loss=8.87766 | best_loss=8.76045
Epoch 30/80: current_loss=8.84766 | best_loss=8.76045
Epoch 31/80: current_loss=8.84670 | best_loss=8.76045
Epoch 32/80: current_loss=8.82746 | best_loss=8.76045
Early Stopping at epoch 32
      explained_var=-0.00016 | mse_loss=8.73104
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.30055 | best_loss=9.30055
Epoch 1/80: current_loss=9.23470 | best_loss=9.23470
Epoch 2/80: current_loss=9.23875 | best_loss=9.23470
Epoch 3/80: current_loss=9.15629 | best_loss=9.15629
Epoch 4/80: current_loss=9.14566 | best_loss=9.14566
Epoch 5/80: current_loss=9.16923 | best_loss=9.14566
Epoch 6/80: current_loss=9.28401 | best_loss=9.14566
Epoch 7/80: current_loss=9.14603 | best_loss=9.14566
Epoch 8/80: current_loss=9.17026 | best_loss=9.14566
Epoch 9/80: current_loss=9.27759 | best_loss=9.14566
Epoch 10/80: current_loss=9.18622 | best_loss=9.14566
Epoch 11/80: current_loss=9.16259 | best_loss=9.14566
Epoch 12/80: current_loss=9.14816 | best_loss=9.14566
Epoch 13/80: current_loss=9.44818 | best_loss=9.14566
Epoch 14/80: current_loss=9.16671 | best_loss=9.14566
Epoch 15/80: current_loss=9.17306 | best_loss=9.14566
Epoch 16/80: current_loss=9.20193 | best_loss=9.14566
Epoch 17/80: current_loss=9.18892 | best_loss=9.14566
Epoch 18/80: current_loss=9.15275 | best_loss=9.14566
Epoch 19/80: current_loss=9.25147 | best_loss=9.14566
Epoch 20/80: current_loss=9.25650 | best_loss=9.14566
Epoch 21/80: current_loss=9.17584 | best_loss=9.14566
Epoch 22/80: current_loss=9.16559 | best_loss=9.14566
Epoch 23/80: current_loss=9.21971 | best_loss=9.14566
Epoch 24/80: current_loss=9.19768 | best_loss=9.14566
Early Stopping at epoch 24
      explained_var=0.02054 | mse_loss=9.10430
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.00477| avg_loss=9.18598
----------------------------------------------


----------------------------------------------
Params for Trial 83
{'learning_rate': 0.001, 'weight_decay': 0.007478520442170924, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=18.61259 | best_loss=18.61259
Epoch 1/80: current_loss=10.82599 | best_loss=10.82599
Epoch 2/80: current_loss=10.43565 | best_loss=10.43565
Epoch 3/80: current_loss=10.38864 | best_loss=10.38864
Epoch 4/80: current_loss=10.36452 | best_loss=10.36452
Epoch 5/80: current_loss=10.33186 | best_loss=10.33186
Epoch 6/80: current_loss=10.31930 | best_loss=10.31930
Epoch 7/80: current_loss=10.31528 | best_loss=10.31528
Epoch 8/80: current_loss=10.29327 | best_loss=10.29327
Epoch 9/80: current_loss=10.32502 | best_loss=10.29327
Epoch 10/80: current_loss=10.31405 | best_loss=10.29327
Epoch 11/80: current_loss=10.29892 | best_loss=10.29327
Epoch 12/80: current_loss=10.34819 | best_loss=10.29327
Epoch 13/80: current_loss=10.29024 | best_loss=10.29024
Epoch 14/80: current_loss=10.31047 | best_loss=10.29024
Epoch 15/80: current_loss=10.31954 | best_loss=10.29024
Epoch 16/80: current_loss=10.30109 | best_loss=10.29024
Epoch 17/80: current_loss=10.28661 | best_loss=10.28661
Epoch 18/80: current_loss=10.30688 | best_loss=10.28661
Epoch 19/80: current_loss=10.29615 | best_loss=10.28661
Epoch 20/80: current_loss=10.31558 | best_loss=10.28661
Epoch 21/80: current_loss=10.31329 | best_loss=10.28661
Epoch 22/80: current_loss=10.29103 | best_loss=10.28661
Epoch 23/80: current_loss=10.28459 | best_loss=10.28459
Epoch 24/80: current_loss=10.30408 | best_loss=10.28459
Epoch 25/80: current_loss=10.29574 | best_loss=10.28459
Epoch 26/80: current_loss=10.30622 | best_loss=10.28459
Epoch 27/80: current_loss=10.33608 | best_loss=10.28459
Epoch 28/80: current_loss=10.28379 | best_loss=10.28379
Epoch 29/80: current_loss=10.30213 | best_loss=10.28379
Epoch 30/80: current_loss=10.28124 | best_loss=10.28124
Epoch 31/80: current_loss=10.27746 | best_loss=10.27746
Epoch 32/80: current_loss=10.27500 | best_loss=10.27500
Epoch 33/80: current_loss=10.30594 | best_loss=10.27500
Epoch 34/80: current_loss=10.31141 | best_loss=10.27500
Epoch 35/80: current_loss=10.27944 | best_loss=10.27500
Epoch 36/80: current_loss=10.28804 | best_loss=10.27500
Epoch 37/80: current_loss=10.28740 | best_loss=10.27500
Epoch 38/80: current_loss=10.29764 | best_loss=10.27500
Epoch 39/80: current_loss=10.36625 | best_loss=10.27500
Epoch 40/80: current_loss=10.28672 | best_loss=10.27500
Epoch 41/80: current_loss=10.28768 | best_loss=10.27500
Epoch 42/80: current_loss=10.27707 | best_loss=10.27500
Epoch 43/80: current_loss=10.31305 | best_loss=10.27500
Epoch 44/80: current_loss=10.27898 | best_loss=10.27500
Epoch 45/80: current_loss=10.30453 | best_loss=10.27500
Epoch 46/80: current_loss=10.30880 | best_loss=10.27500
Epoch 47/80: current_loss=10.28076 | best_loss=10.27500
Epoch 48/80: current_loss=10.31819 | best_loss=10.27500
Epoch 49/80: current_loss=10.31499 | best_loss=10.27500
Epoch 50/80: current_loss=10.27855 | best_loss=10.27500
Epoch 51/80: current_loss=10.28588 | best_loss=10.27500
Epoch 52/80: current_loss=10.27527 | best_loss=10.27500
Early Stopping at epoch 52
      explained_var=-0.00317 | mse_loss=10.49950
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.40241 | best_loss=10.40241
Epoch 1/80: current_loss=10.48835 | best_loss=10.40241
Epoch 2/80: current_loss=10.40211 | best_loss=10.40211
Epoch 3/80: current_loss=10.40718 | best_loss=10.40211
Epoch 4/80: current_loss=10.41939 | best_loss=10.40211
Epoch 5/80: current_loss=10.49803 | best_loss=10.40211
Epoch 6/80: current_loss=10.42005 | best_loss=10.40211
Epoch 7/80: current_loss=10.46052 | best_loss=10.40211
Epoch 8/80: current_loss=10.41303 | best_loss=10.40211
Epoch 9/80: current_loss=10.42765 | best_loss=10.40211
Epoch 10/80: current_loss=10.40475 | best_loss=10.40211
Epoch 11/80: current_loss=10.42108 | best_loss=10.40211
Epoch 12/80: current_loss=10.40477 | best_loss=10.40211
Epoch 13/80: current_loss=10.43064 | best_loss=10.40211
Epoch 14/80: current_loss=10.42600 | best_loss=10.40211
Epoch 15/80: current_loss=10.46870 | best_loss=10.40211
Epoch 16/80: current_loss=10.44462 | best_loss=10.40211
Epoch 17/80: current_loss=10.42217 | best_loss=10.40211
Epoch 18/80: current_loss=10.41092 | best_loss=10.40211
Epoch 19/80: current_loss=10.40552 | best_loss=10.40211
Epoch 20/80: current_loss=10.44712 | best_loss=10.40211
Epoch 21/80: current_loss=10.41279 | best_loss=10.40211
Epoch 22/80: current_loss=10.46317 | best_loss=10.40211
Early Stopping at epoch 22
      explained_var=0.00070 | mse_loss=10.05677

----------------------------------------------
Params for Trial 84
{'learning_rate': 0.001, 'weight_decay': 0.005960993869165641, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.46563 | best_loss=16.46563
Epoch 1/80: current_loss=10.75799 | best_loss=10.75799
Epoch 2/80: current_loss=10.44412 | best_loss=10.44412
Epoch 3/80: current_loss=10.36227 | best_loss=10.36227
Epoch 4/80: current_loss=10.35669 | best_loss=10.35669
Epoch 5/80: current_loss=10.32693 | best_loss=10.32693
Epoch 6/80: current_loss=10.30968 | best_loss=10.30968
Epoch 7/80: current_loss=10.28193 | best_loss=10.28193
Epoch 8/80: current_loss=10.32015 | best_loss=10.28193
Epoch 9/80: current_loss=10.29380 | best_loss=10.28193
Epoch 10/80: current_loss=10.29119 | best_loss=10.28193
Epoch 11/80: current_loss=10.30166 | best_loss=10.28193
Epoch 12/80: current_loss=10.32648 | best_loss=10.28193
Epoch 13/80: current_loss=10.27812 | best_loss=10.27812
Epoch 14/80: current_loss=10.29512 | best_loss=10.27812
Epoch 15/80: current_loss=10.28557 | best_loss=10.27812
Epoch 16/80: current_loss=10.27699 | best_loss=10.27699
Epoch 17/80: current_loss=10.31374 | best_loss=10.27699
Epoch 18/80: current_loss=10.28811 | best_loss=10.27699
Epoch 19/80: current_loss=10.28977 | best_loss=10.27699
Epoch 20/80: current_loss=10.28393 | best_loss=10.27699
Epoch 21/80: current_loss=10.27269 | best_loss=10.27269
Epoch 22/80: current_loss=10.33731 | best_loss=10.27269
Epoch 23/80: current_loss=10.28098 | best_loss=10.27269
Epoch 24/80: current_loss=10.28201 | best_loss=10.27269
Epoch 25/80: current_loss=10.28259 | best_loss=10.27269
Epoch 26/80: current_loss=10.28686 | best_loss=10.27269
Epoch 27/80: current_loss=10.27151 | best_loss=10.27151
Epoch 28/80: current_loss=10.27953 | best_loss=10.27151
Epoch 29/80: current_loss=10.28186 | best_loss=10.27151
Epoch 30/80: current_loss=10.29025 | best_loss=10.27151
Epoch 31/80: current_loss=10.29251 | best_loss=10.27151
Epoch 32/80: current_loss=10.27107 | best_loss=10.27107
Epoch 33/80: current_loss=10.27527 | best_loss=10.27107
Epoch 34/80: current_loss=10.33201 | best_loss=10.27107
Epoch 35/80: current_loss=10.28513 | best_loss=10.27107
Epoch 36/80: current_loss=10.29177 | best_loss=10.27107
Epoch 37/80: current_loss=10.30077 | best_loss=10.27107
Epoch 38/80: current_loss=10.28269 | best_loss=10.27107
Epoch 39/80: current_loss=10.31100 | best_loss=10.27107
Epoch 40/80: current_loss=10.30319 | best_loss=10.27107
Epoch 41/80: current_loss=10.30457 | best_loss=10.27107
Epoch 42/80: current_loss=10.27385 | best_loss=10.27107
Epoch 43/80: current_loss=10.29339 | best_loss=10.27107
Epoch 44/80: current_loss=10.32779 | best_loss=10.27107
Epoch 45/80: current_loss=10.28078 | best_loss=10.27107
Epoch 46/80: current_loss=10.29457 | best_loss=10.27107
Epoch 47/80: current_loss=10.33407 | best_loss=10.27107
Epoch 48/80: current_loss=10.27898 | best_loss=10.27107
Epoch 49/80: current_loss=10.30579 | best_loss=10.27107
Epoch 50/80: current_loss=10.29906 | best_loss=10.27107
Epoch 51/80: current_loss=10.29957 | best_loss=10.27107
Epoch 52/80: current_loss=10.28580 | best_loss=10.27107
Early Stopping at epoch 52
      explained_var=-0.00193 | mse_loss=10.50035
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.42177 | best_loss=10.42177
Epoch 1/80: current_loss=10.40895 | best_loss=10.40895
Epoch 2/80: current_loss=10.71734 | best_loss=10.40895
Epoch 3/80: current_loss=10.40131 | best_loss=10.40131
Epoch 4/80: current_loss=10.47701 | best_loss=10.40131
Epoch 5/80: current_loss=10.42261 | best_loss=10.40131
Epoch 6/80: current_loss=10.41181 | best_loss=10.40131
Epoch 7/80: current_loss=10.41051 | best_loss=10.40131
Epoch 8/80: current_loss=10.43230 | best_loss=10.40131
Epoch 9/80: current_loss=10.40851 | best_loss=10.40131
Epoch 10/80: current_loss=10.41195 | best_loss=10.40131
Epoch 11/80: current_loss=10.45132 | best_loss=10.40131
Epoch 12/80: current_loss=10.42717 | best_loss=10.40131
Epoch 13/80: current_loss=10.40338 | best_loss=10.40131
Epoch 14/80: current_loss=10.40398 | best_loss=10.40131
Epoch 15/80: current_loss=10.45928 | best_loss=10.40131
Epoch 16/80: current_loss=10.40205 | best_loss=10.40131
Epoch 17/80: current_loss=10.40367 | best_loss=10.40131
Epoch 18/80: current_loss=10.39249 | best_loss=10.39249
Epoch 19/80: current_loss=10.34190 | best_loss=10.34190
Epoch 20/80: current_loss=10.41423 | best_loss=10.34190
Epoch 21/80: current_loss=10.40828 | best_loss=10.34190
Epoch 22/80: current_loss=10.46875 | best_loss=10.34190
Epoch 23/80: current_loss=10.43410 | best_loss=10.34190
Epoch 24/80: current_loss=10.43520 | best_loss=10.34190
Epoch 25/80: current_loss=10.41563 | best_loss=10.34190
Epoch 26/80: current_loss=10.43397 | best_loss=10.34190
Epoch 27/80: current_loss=10.40417 | best_loss=10.34190
Epoch 28/80: current_loss=10.40462 | best_loss=10.34190
Epoch 29/80: current_loss=10.42550 | best_loss=10.34190
Epoch 30/80: current_loss=10.40989 | best_loss=10.34190
Epoch 31/80: current_loss=10.44814 | best_loss=10.34190
Epoch 32/80: current_loss=10.39814 | best_loss=10.34190
Epoch 33/80: current_loss=10.43282 | best_loss=10.34190
Epoch 34/80: current_loss=10.36985 | best_loss=10.34190
Epoch 35/80: current_loss=10.40461 | best_loss=10.34190
Epoch 36/80: current_loss=10.43369 | best_loss=10.34190
Epoch 37/80: current_loss=10.41786 | best_loss=10.34190
Epoch 38/80: current_loss=10.45673 | best_loss=10.34190
Epoch 39/80: current_loss=10.41958 | best_loss=10.34190
Early Stopping at epoch 39
      explained_var=0.00739 | mse_loss=9.99187
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.48907 | best_loss=7.48907
Epoch 1/80: current_loss=7.60111 | best_loss=7.48907
Epoch 2/80: current_loss=7.62882 | best_loss=7.48907
Epoch 3/80: current_loss=7.56528 | best_loss=7.48907
Epoch 4/80: current_loss=7.54096 | best_loss=7.48907
Epoch 5/80: current_loss=7.56443 | best_loss=7.48907
Epoch 6/80: current_loss=7.55772 | best_loss=7.48907
Epoch 7/80: current_loss=7.54096 | best_loss=7.48907
Epoch 8/80: current_loss=7.68432 | best_loss=7.48907
Epoch 9/80: current_loss=7.54689 | best_loss=7.48907
Epoch 10/80: current_loss=7.53244 | best_loss=7.48907
Epoch 11/80: current_loss=7.51990 | best_loss=7.48907
Epoch 12/80: current_loss=7.55949 | best_loss=7.48907
Epoch 13/80: current_loss=7.84261 | best_loss=7.48907
Epoch 14/80: current_loss=7.50047 | best_loss=7.48907
Epoch 15/80: current_loss=7.58522 | best_loss=7.48907
Epoch 16/80: current_loss=7.63657 | best_loss=7.48907
Epoch 17/80: current_loss=7.67553 | best_loss=7.48907
Epoch 18/80: current_loss=7.59342 | best_loss=7.48907
Epoch 19/80: current_loss=7.78944 | best_loss=7.48907
Epoch 20/80: current_loss=7.56735 | best_loss=7.48907
Early Stopping at epoch 20
      explained_var=0.00002 | mse_loss=7.58823

----------------------------------------------
Params for Trial 85
{'learning_rate': 0.001, 'weight_decay': 0.006621667148285894, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=42.25042 | best_loss=42.25042
Epoch 1/80: current_loss=26.02762 | best_loss=26.02762
Epoch 2/80: current_loss=17.57418 | best_loss=17.57418
Epoch 3/80: current_loss=13.40609 | best_loss=13.40609
Epoch 4/80: current_loss=11.48136 | best_loss=11.48136
Epoch 5/80: current_loss=10.73576 | best_loss=10.73576
Epoch 6/80: current_loss=10.45570 | best_loss=10.45570
Epoch 7/80: current_loss=10.34286 | best_loss=10.34286
Epoch 8/80: current_loss=10.31662 | best_loss=10.31662
Epoch 9/80: current_loss=10.31231 | best_loss=10.31231
Epoch 10/80: current_loss=10.30161 | best_loss=10.30161
Epoch 11/80: current_loss=10.28678 | best_loss=10.28678
Epoch 12/80: current_loss=10.29361 | best_loss=10.28678
Epoch 13/80: current_loss=10.30770 | best_loss=10.28678
Epoch 14/80: current_loss=10.30516 | best_loss=10.28678
Epoch 15/80: current_loss=10.30366 | best_loss=10.28678
Epoch 16/80: current_loss=10.30747 | best_loss=10.28678
Epoch 17/80: current_loss=10.31459 | best_loss=10.28678
Epoch 18/80: current_loss=10.29676 | best_loss=10.28678
Epoch 19/80: current_loss=10.27861 | best_loss=10.27861
Epoch 20/80: current_loss=10.28296 | best_loss=10.27861
Epoch 21/80: current_loss=10.28571 | best_loss=10.27861
Epoch 22/80: current_loss=10.29475 | best_loss=10.27861
Epoch 23/80: current_loss=10.31155 | best_loss=10.27861
Epoch 24/80: current_loss=10.30040 | best_loss=10.27861
Epoch 25/80: current_loss=10.29766 | best_loss=10.27861
Epoch 26/80: current_loss=10.28948 | best_loss=10.27861
Epoch 27/80: current_loss=10.29254 | best_loss=10.27861
Epoch 28/80: current_loss=10.28367 | best_loss=10.27861
Epoch 29/80: current_loss=10.30412 | best_loss=10.27861
Epoch 30/80: current_loss=10.30752 | best_loss=10.27861
Epoch 31/80: current_loss=10.27997 | best_loss=10.27861
Epoch 32/80: current_loss=10.29651 | best_loss=10.27861
Epoch 33/80: current_loss=10.29027 | best_loss=10.27861
Epoch 34/80: current_loss=10.28518 | best_loss=10.27861
Epoch 35/80: current_loss=10.29613 | best_loss=10.27861
Epoch 36/80: current_loss=10.27675 | best_loss=10.27675
Epoch 37/80: current_loss=10.27729 | best_loss=10.27675
Epoch 38/80: current_loss=10.27423 | best_loss=10.27423
Epoch 39/80: current_loss=10.28294 | best_loss=10.27423
Epoch 40/80: current_loss=10.30656 | best_loss=10.27423
Epoch 41/80: current_loss=10.28432 | best_loss=10.27423
Epoch 42/80: current_loss=10.27831 | best_loss=10.27423
Epoch 43/80: current_loss=10.29492 | best_loss=10.27423
Epoch 44/80: current_loss=10.28069 | best_loss=10.27423
Epoch 45/80: current_loss=10.27328 | best_loss=10.27328
Epoch 46/80: current_loss=10.29350 | best_loss=10.27328
Epoch 47/80: current_loss=10.29648 | best_loss=10.27328
Epoch 48/80: current_loss=10.29664 | best_loss=10.27328
Epoch 49/80: current_loss=10.28225 | best_loss=10.27328
Epoch 50/80: current_loss=10.30988 | best_loss=10.27328
Epoch 51/80: current_loss=10.28851 | best_loss=10.27328
Epoch 52/80: current_loss=10.28821 | best_loss=10.27328
Epoch 53/80: current_loss=10.29454 | best_loss=10.27328
Epoch 54/80: current_loss=10.28374 | best_loss=10.27328
Epoch 55/80: current_loss=10.27290 | best_loss=10.27290
Epoch 56/80: current_loss=10.28467 | best_loss=10.27290
Epoch 57/80: current_loss=10.27951 | best_loss=10.27290
Epoch 58/80: current_loss=10.28401 | best_loss=10.27290
Epoch 59/80: current_loss=10.26453 | best_loss=10.26453
Epoch 60/80: current_loss=10.26885 | best_loss=10.26453
Epoch 61/80: current_loss=10.29035 | best_loss=10.26453
Epoch 62/80: current_loss=10.25878 | best_loss=10.25878
Epoch 63/80: current_loss=10.30101 | best_loss=10.25878
Epoch 64/80: current_loss=10.26651 | best_loss=10.25878
Epoch 65/80: current_loss=10.27037 | best_loss=10.25878
Epoch 66/80: current_loss=10.25802 | best_loss=10.25802
Epoch 67/80: current_loss=10.28021 | best_loss=10.25802
Epoch 68/80: current_loss=10.25738 | best_loss=10.25738
Epoch 69/80: current_loss=10.27210 | best_loss=10.25738
Epoch 70/80: current_loss=10.15744 | best_loss=10.15744
Epoch 71/80: current_loss=10.23494 | best_loss=10.15744
Epoch 72/80: current_loss=10.13722 | best_loss=10.13722
Epoch 73/80: current_loss=10.09478 | best_loss=10.09478
Epoch 74/80: current_loss=10.09333 | best_loss=10.09333
Epoch 75/80: current_loss=10.15629 | best_loss=10.09333
Epoch 76/80: current_loss=10.17674 | best_loss=10.09333
Epoch 77/80: current_loss=10.34225 | best_loss=10.09333
Epoch 78/80: current_loss=10.26426 | best_loss=10.09333
Epoch 79/80: current_loss=10.30601 | best_loss=10.09333
      explained_var=0.01375 | mse_loss=10.32036
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.25157 | best_loss=10.25157
Epoch 1/80: current_loss=10.29368 | best_loss=10.25157
Epoch 2/80: current_loss=10.29736 | best_loss=10.25157
Epoch 3/80: current_loss=10.27870 | best_loss=10.25157
Epoch 4/80: current_loss=10.22044 | best_loss=10.22044
Epoch 5/80: current_loss=10.26737 | best_loss=10.22044
Epoch 6/80: current_loss=10.25038 | best_loss=10.22044
Epoch 7/80: current_loss=10.30086 | best_loss=10.22044
Epoch 8/80: current_loss=10.24945 | best_loss=10.22044
Epoch 9/80: current_loss=10.21029 | best_loss=10.21029
Epoch 10/80: current_loss=10.19892 | best_loss=10.19892
Epoch 11/80: current_loss=10.35021 | best_loss=10.19892
Epoch 12/80: current_loss=10.28335 | best_loss=10.19892
Epoch 13/80: current_loss=10.16774 | best_loss=10.16774
Epoch 14/80: current_loss=10.31796 | best_loss=10.16774
Epoch 15/80: current_loss=10.26412 | best_loss=10.16774
Epoch 16/80: current_loss=10.16836 | best_loss=10.16774
Epoch 17/80: current_loss=10.31927 | best_loss=10.16774
Epoch 18/80: current_loss=10.21911 | best_loss=10.16774
Epoch 19/80: current_loss=10.18170 | best_loss=10.16774
Epoch 20/80: current_loss=10.19722 | best_loss=10.16774
Epoch 21/80: current_loss=10.30423 | best_loss=10.16774
Epoch 22/80: current_loss=10.27319 | best_loss=10.16774
Epoch 23/80: current_loss=10.18025 | best_loss=10.16774
Epoch 24/80: current_loss=10.26427 | best_loss=10.16774
Epoch 25/80: current_loss=10.24110 | best_loss=10.16774
Epoch 26/80: current_loss=10.25201 | best_loss=10.16774
Epoch 27/80: current_loss=10.22053 | best_loss=10.16774
Epoch 28/80: current_loss=10.21237 | best_loss=10.16774
Epoch 29/80: current_loss=10.18293 | best_loss=10.16774
Epoch 30/80: current_loss=10.19782 | best_loss=10.16774
Epoch 31/80: current_loss=10.23466 | best_loss=10.16774
Epoch 32/80: current_loss=10.24524 | best_loss=10.16774
Epoch 33/80: current_loss=10.23003 | best_loss=10.16774
Early Stopping at epoch 33
      explained_var=0.03218 | mse_loss=9.80707
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.51904 | best_loss=7.51904
Epoch 1/80: current_loss=7.66769 | best_loss=7.51904
Epoch 2/80: current_loss=7.50321 | best_loss=7.50321
Epoch 3/80: current_loss=7.68349 | best_loss=7.50321
Epoch 4/80: current_loss=7.61763 | best_loss=7.50321
Epoch 5/80: current_loss=7.62157 | best_loss=7.50321
Epoch 6/80: current_loss=7.62070 | best_loss=7.50321
Epoch 7/80: current_loss=7.57667 | best_loss=7.50321
Epoch 8/80: current_loss=7.63647 | best_loss=7.50321
Epoch 9/80: current_loss=7.63187 | best_loss=7.50321
Epoch 10/80: current_loss=7.59675 | best_loss=7.50321
Epoch 11/80: current_loss=7.58565 | best_loss=7.50321
Epoch 12/80: current_loss=7.73295 | best_loss=7.50321
Epoch 13/80: current_loss=7.52798 | best_loss=7.50321
Epoch 14/80: current_loss=7.57740 | best_loss=7.50321
Epoch 15/80: current_loss=7.62247 | best_loss=7.50321
Epoch 16/80: current_loss=7.71326 | best_loss=7.50321
Epoch 17/80: current_loss=7.54250 | best_loss=7.50321
Epoch 18/80: current_loss=7.59481 | best_loss=7.50321
Epoch 19/80: current_loss=7.68925 | best_loss=7.50321
Epoch 20/80: current_loss=7.63109 | best_loss=7.50321
Epoch 21/80: current_loss=7.62310 | best_loss=7.50321
Epoch 22/80: current_loss=7.59430 | best_loss=7.50321
Early Stopping at epoch 22
      explained_var=-0.00047 | mse_loss=7.60949

----------------------------------------------
Params for Trial 86
{'learning_rate': 0.001, 'weight_decay': 0.007895725446801099, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=28.59789 | best_loss=28.59789
Epoch 1/80: current_loss=14.87919 | best_loss=14.87919
Epoch 2/80: current_loss=13.73952 | best_loss=13.73952
Epoch 3/80: current_loss=13.48920 | best_loss=13.48920
Epoch 4/80: current_loss=13.26405 | best_loss=13.26405
Epoch 5/80: current_loss=13.07773 | best_loss=13.07773
Epoch 6/80: current_loss=12.88509 | best_loss=12.88509
Epoch 7/80: current_loss=12.72441 | best_loss=12.72441
Epoch 8/80: current_loss=12.57267 | best_loss=12.57267
Epoch 9/80: current_loss=12.45750 | best_loss=12.45750
Epoch 10/80: current_loss=12.37919 | best_loss=12.37919
Epoch 11/80: current_loss=12.26721 | best_loss=12.26721
Epoch 12/80: current_loss=12.17977 | best_loss=12.17977
Epoch 13/80: current_loss=12.13702 | best_loss=12.13702
Epoch 14/80: current_loss=12.05763 | best_loss=12.05763
Epoch 15/80: current_loss=11.96523 | best_loss=11.96523
Epoch 16/80: current_loss=11.90402 | best_loss=11.90402
Epoch 17/80: current_loss=11.83403 | best_loss=11.83403
Epoch 18/80: current_loss=11.78176 | best_loss=11.78176
Epoch 19/80: current_loss=11.71250 | best_loss=11.71250
Epoch 20/80: current_loss=11.66614 | best_loss=11.66614
Epoch 21/80: current_loss=11.60361 | best_loss=11.60361
Epoch 22/80: current_loss=11.55208 | best_loss=11.55208
Epoch 23/80: current_loss=11.49654 | best_loss=11.49654
Epoch 24/80: current_loss=11.44992 | best_loss=11.44992
Epoch 25/80: current_loss=11.39881 | best_loss=11.39881
Epoch 26/80: current_loss=11.34661 | best_loss=11.34661
Epoch 27/80: current_loss=11.31595 | best_loss=11.31595
Epoch 28/80: current_loss=11.27461 | best_loss=11.27461
Epoch 29/80: current_loss=11.23209 | best_loss=11.23209
Epoch 30/80: current_loss=11.16514 | best_loss=11.16514
Epoch 31/80: current_loss=11.12352 | best_loss=11.12352
Epoch 32/80: current_loss=11.13968 | best_loss=11.12352
Epoch 33/80: current_loss=11.06888 | best_loss=11.06888
Epoch 34/80: current_loss=11.01087 | best_loss=11.01087
Epoch 35/80: current_loss=11.04709 | best_loss=11.01087
Epoch 36/80: current_loss=10.90938 | best_loss=10.90938
Epoch 37/80: current_loss=10.99440 | best_loss=10.90938
Epoch 38/80: current_loss=10.86276 | best_loss=10.86276
Epoch 39/80: current_loss=10.81810 | best_loss=10.81810
Epoch 40/80: current_loss=10.76839 | best_loss=10.76839
Epoch 41/80: current_loss=10.72591 | best_loss=10.72591
Epoch 42/80: current_loss=10.74274 | best_loss=10.72591
Epoch 43/80: current_loss=10.71938 | best_loss=10.71938
Epoch 44/80: current_loss=10.65136 | best_loss=10.65136
Epoch 45/80: current_loss=10.65046 | best_loss=10.65046
Epoch 46/80: current_loss=10.59258 | best_loss=10.59258
Epoch 47/80: current_loss=10.64989 | best_loss=10.59258
Epoch 48/80: current_loss=10.54626 | best_loss=10.54626
Epoch 49/80: current_loss=10.51670 | best_loss=10.51670
Epoch 50/80: current_loss=10.49183 | best_loss=10.49183
Epoch 51/80: current_loss=10.45306 | best_loss=10.45306
Epoch 52/80: current_loss=10.52491 | best_loss=10.45306
Epoch 53/80: current_loss=10.63871 | best_loss=10.45306
Epoch 54/80: current_loss=10.38437 | best_loss=10.38437
Epoch 55/80: current_loss=10.34531 | best_loss=10.34531
Epoch 56/80: current_loss=10.51281 | best_loss=10.34531
Epoch 57/80: current_loss=10.54828 | best_loss=10.34531
Epoch 58/80: current_loss=10.34306 | best_loss=10.34306
Epoch 59/80: current_loss=10.30186 | best_loss=10.30186
Epoch 60/80: current_loss=10.42014 | best_loss=10.30186
Epoch 61/80: current_loss=10.24811 | best_loss=10.24811
Epoch 62/80: current_loss=10.29666 | best_loss=10.24811
Epoch 63/80: current_loss=10.38222 | best_loss=10.24811
Epoch 64/80: current_loss=10.37260 | best_loss=10.24811
Epoch 65/80: current_loss=10.17092 | best_loss=10.17092
Epoch 66/80: current_loss=10.18417 | best_loss=10.17092
Epoch 67/80: current_loss=10.16534 | best_loss=10.16534
Epoch 68/80: current_loss=10.16153 | best_loss=10.16153
Epoch 69/80: current_loss=10.21148 | best_loss=10.16153
Epoch 70/80: current_loss=10.18701 | best_loss=10.16153
Epoch 71/80: current_loss=10.15619 | best_loss=10.15619
Epoch 72/80: current_loss=10.34585 | best_loss=10.15619
Epoch 73/80: current_loss=10.25194 | best_loss=10.15619
Epoch 74/80: current_loss=10.24569 | best_loss=10.15619
Epoch 75/80: current_loss=10.44152 | best_loss=10.15619
Epoch 76/80: current_loss=10.20587 | best_loss=10.15619
Epoch 77/80: current_loss=10.13415 | best_loss=10.13415
Epoch 78/80: current_loss=10.10434 | best_loss=10.10434
Epoch 79/80: current_loss=10.27801 | best_loss=10.10434
      explained_var=0.01183 | mse_loss=10.36091
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.45646 | best_loss=10.45646
Epoch 1/80: current_loss=10.26453 | best_loss=10.26453
Epoch 2/80: current_loss=10.41955 | best_loss=10.26453
Epoch 3/80: current_loss=10.30575 | best_loss=10.26453
Epoch 4/80: current_loss=10.23351 | best_loss=10.23351
Epoch 5/80: current_loss=10.40525 | best_loss=10.23351
Epoch 6/80: current_loss=10.22674 | best_loss=10.22674
Epoch 7/80: current_loss=10.21911 | best_loss=10.21911
Epoch 8/80: current_loss=10.19898 | best_loss=10.19898
Epoch 9/80: current_loss=10.25646 | best_loss=10.19898
Epoch 10/80: current_loss=10.22196 | best_loss=10.19898
Epoch 11/80: current_loss=10.26391 | best_loss=10.19898
Epoch 12/80: current_loss=10.47762 | best_loss=10.19898
Epoch 13/80: current_loss=10.22022 | best_loss=10.19898
Epoch 14/80: current_loss=10.20990 | best_loss=10.19898
Epoch 15/80: current_loss=10.23274 | best_loss=10.19898
Epoch 16/80: current_loss=10.30809 | best_loss=10.19898
Epoch 17/80: current_loss=10.21232 | best_loss=10.19898
Epoch 18/80: current_loss=10.23933 | best_loss=10.19898
Epoch 19/80: current_loss=10.20548 | best_loss=10.19898
Epoch 20/80: current_loss=10.17877 | best_loss=10.17877
Epoch 21/80: current_loss=10.19063 | best_loss=10.17877
Epoch 22/80: current_loss=10.24016 | best_loss=10.17877
Epoch 23/80: current_loss=10.31782 | best_loss=10.17877
Epoch 24/80: current_loss=10.21386 | best_loss=10.17877
Epoch 25/80: current_loss=10.21374 | best_loss=10.17877
Epoch 26/80: current_loss=10.38547 | best_loss=10.17877
Epoch 27/80: current_loss=10.22007 | best_loss=10.17877
Epoch 28/80: current_loss=10.36727 | best_loss=10.17877
Epoch 29/80: current_loss=10.23180 | best_loss=10.17877
Epoch 30/80: current_loss=10.22997 | best_loss=10.17877
Epoch 31/80: current_loss=10.26943 | best_loss=10.17877
Epoch 32/80: current_loss=10.23212 | best_loss=10.17877
Epoch 33/80: current_loss=10.34452 | best_loss=10.17877
Epoch 34/80: current_loss=10.23855 | best_loss=10.17877
Epoch 35/80: current_loss=10.36230 | best_loss=10.17877
Epoch 36/80: current_loss=10.22273 | best_loss=10.17877
Epoch 37/80: current_loss=10.43828 | best_loss=10.17877
Epoch 38/80: current_loss=10.20029 | best_loss=10.17877
Epoch 39/80: current_loss=10.20183 | best_loss=10.17877
Epoch 40/80: current_loss=10.19442 | best_loss=10.17877
Early Stopping at epoch 40
      explained_var=0.02426 | mse_loss=9.81940
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.50832 | best_loss=7.50832
Epoch 1/80: current_loss=7.79436 | best_loss=7.50832
Epoch 2/80: current_loss=7.58593 | best_loss=7.50832
Epoch 3/80: current_loss=7.61210 | best_loss=7.50832
Epoch 4/80: current_loss=7.52269 | best_loss=7.50832
Epoch 5/80: current_loss=7.83346 | best_loss=7.50832
Epoch 6/80: current_loss=7.56637 | best_loss=7.50832
Epoch 7/80: current_loss=7.63172 | best_loss=7.50832
Epoch 8/80: current_loss=7.54713 | best_loss=7.50832
Epoch 9/80: current_loss=7.56310 | best_loss=7.50832
Epoch 10/80: current_loss=7.82136 | best_loss=7.50832
Epoch 11/80: current_loss=7.58752 | best_loss=7.50832
Epoch 12/80: current_loss=7.67421 | best_loss=7.50832
Epoch 13/80: current_loss=7.56937 | best_loss=7.50832
Epoch 14/80: current_loss=7.75050 | best_loss=7.50832
Epoch 15/80: current_loss=7.54087 | best_loss=7.50832
Epoch 16/80: current_loss=7.59398 | best_loss=7.50832
Epoch 17/80: current_loss=7.67585 | best_loss=7.50832
Epoch 18/80: current_loss=7.57044 | best_loss=7.50832
Epoch 19/80: current_loss=7.59976 | best_loss=7.50832
Epoch 20/80: current_loss=7.66829 | best_loss=7.50832
Early Stopping at epoch 20
      explained_var=-0.00290 | mse_loss=7.61446

----------------------------------------------
Params for Trial 87
{'learning_rate': 0.001, 'weight_decay': 0.00897705298469431, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=19.13094 | best_loss=19.13094
Epoch 1/80: current_loss=12.05906 | best_loss=12.05906
Epoch 2/80: current_loss=11.54067 | best_loss=11.54067
Epoch 3/80: current_loss=11.23068 | best_loss=11.23068
Epoch 4/80: current_loss=10.96890 | best_loss=10.96890
Epoch 5/80: current_loss=10.77382 | best_loss=10.77382
Epoch 6/80: current_loss=10.66302 | best_loss=10.66302
Epoch 7/80: current_loss=10.54924 | best_loss=10.54924
Epoch 8/80: current_loss=10.51121 | best_loss=10.51121
Epoch 9/80: current_loss=10.45626 | best_loss=10.45626
Epoch 10/80: current_loss=10.44396 | best_loss=10.44396
Epoch 11/80: current_loss=10.42888 | best_loss=10.42888
Epoch 12/80: current_loss=10.40904 | best_loss=10.40904
Epoch 13/80: current_loss=10.42314 | best_loss=10.40904
Epoch 14/80: current_loss=10.40546 | best_loss=10.40546
Epoch 15/80: current_loss=10.40266 | best_loss=10.40266
Epoch 16/80: current_loss=10.41576 | best_loss=10.40266
Epoch 17/80: current_loss=10.37998 | best_loss=10.37998
Epoch 18/80: current_loss=10.40124 | best_loss=10.37998
Epoch 19/80: current_loss=10.42178 | best_loss=10.37998
Epoch 20/80: current_loss=10.36817 | best_loss=10.36817
Epoch 21/80: current_loss=10.38565 | best_loss=10.36817
Epoch 22/80: current_loss=10.35338 | best_loss=10.35338
Epoch 23/80: current_loss=10.35414 | best_loss=10.35338
Epoch 24/80: current_loss=10.33565 | best_loss=10.33565
Epoch 25/80: current_loss=10.38522 | best_loss=10.33565
Epoch 26/80: current_loss=10.31782 | best_loss=10.31782
Epoch 27/80: current_loss=10.37496 | best_loss=10.31782
Epoch 28/80: current_loss=10.32410 | best_loss=10.31782
Epoch 29/80: current_loss=10.31671 | best_loss=10.31671
Epoch 30/80: current_loss=10.34894 | best_loss=10.31671
Epoch 31/80: current_loss=10.33548 | best_loss=10.31671
Epoch 32/80: current_loss=10.32516 | best_loss=10.31671
Epoch 33/80: current_loss=10.30696 | best_loss=10.30696
Epoch 34/80: current_loss=10.33400 | best_loss=10.30696
Epoch 35/80: current_loss=10.32715 | best_loss=10.30696
Epoch 36/80: current_loss=10.32384 | best_loss=10.30696
Epoch 37/80: current_loss=10.31983 | best_loss=10.30696
Epoch 38/80: current_loss=10.31638 | best_loss=10.30696
Epoch 39/80: current_loss=10.35280 | best_loss=10.30696
Epoch 40/80: current_loss=10.31708 | best_loss=10.30696
Epoch 41/80: current_loss=10.34321 | best_loss=10.30696
Epoch 42/80: current_loss=10.30961 | best_loss=10.30696
Epoch 43/80: current_loss=10.32418 | best_loss=10.30696
Epoch 44/80: current_loss=10.31770 | best_loss=10.30696
Epoch 45/80: current_loss=10.38472 | best_loss=10.30696
Epoch 46/80: current_loss=10.33730 | best_loss=10.30696
Epoch 47/80: current_loss=10.31695 | best_loss=10.30696
Epoch 48/80: current_loss=10.35106 | best_loss=10.30696
Epoch 49/80: current_loss=10.34225 | best_loss=10.30696
Epoch 50/80: current_loss=10.32670 | best_loss=10.30696
Epoch 51/80: current_loss=10.37334 | best_loss=10.30696
Epoch 52/80: current_loss=10.30652 | best_loss=10.30652
Epoch 53/80: current_loss=10.29561 | best_loss=10.29561
Epoch 54/80: current_loss=10.31543 | best_loss=10.29561
Epoch 55/80: current_loss=10.28694 | best_loss=10.28694
Epoch 56/80: current_loss=10.29565 | best_loss=10.28694
Epoch 57/80: current_loss=10.32704 | best_loss=10.28694
Epoch 58/80: current_loss=10.28199 | best_loss=10.28199
Epoch 59/80: current_loss=10.31358 | best_loss=10.28199
Epoch 60/80: current_loss=10.30354 | best_loss=10.28199
Epoch 61/80: current_loss=10.31790 | best_loss=10.28199
Epoch 62/80: current_loss=10.28362 | best_loss=10.28199
Epoch 63/80: current_loss=10.29568 | best_loss=10.28199
Epoch 64/80: current_loss=10.27402 | best_loss=10.27402
Epoch 65/80: current_loss=10.26335 | best_loss=10.26335
Epoch 66/80: current_loss=10.29460 | best_loss=10.26335
Epoch 67/80: current_loss=10.33659 | best_loss=10.26335
Epoch 68/80: current_loss=10.29814 | best_loss=10.26335
Epoch 69/80: current_loss=10.29462 | best_loss=10.26335
Epoch 70/80: current_loss=10.29561 | best_loss=10.26335
Epoch 71/80: current_loss=10.30125 | best_loss=10.26335
Epoch 72/80: current_loss=10.30943 | best_loss=10.26335
Epoch 73/80: current_loss=10.29865 | best_loss=10.26335
Epoch 74/80: current_loss=10.31625 | best_loss=10.26335
Epoch 75/80: current_loss=10.31866 | best_loss=10.26335
Epoch 76/80: current_loss=10.27976 | best_loss=10.26335
Epoch 77/80: current_loss=10.24519 | best_loss=10.24519
Epoch 78/80: current_loss=10.21058 | best_loss=10.21058
Epoch 79/80: current_loss=10.29176 | best_loss=10.21058
      explained_var=0.00291 | mse_loss=10.44037
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.23588 | best_loss=10.23588
Epoch 1/80: current_loss=10.26584 | best_loss=10.23588
Epoch 2/80: current_loss=10.15068 | best_loss=10.15068
Epoch 3/80: current_loss=10.23523 | best_loss=10.15068
Epoch 4/80: current_loss=10.23884 | best_loss=10.15068
Epoch 5/80: current_loss=10.21234 | best_loss=10.15068
Epoch 6/80: current_loss=10.30159 | best_loss=10.15068
Epoch 7/80: current_loss=10.36549 | best_loss=10.15068
Epoch 8/80: current_loss=10.29381 | best_loss=10.15068
Epoch 9/80: current_loss=10.27626 | best_loss=10.15068
Epoch 10/80: current_loss=10.37842 | best_loss=10.15068
Epoch 11/80: current_loss=10.31772 | best_loss=10.15068
Epoch 12/80: current_loss=10.21435 | best_loss=10.15068
Epoch 13/80: current_loss=10.26782 | best_loss=10.15068
Epoch 14/80: current_loss=10.34903 | best_loss=10.15068
Epoch 15/80: current_loss=10.34380 | best_loss=10.15068
Epoch 16/80: current_loss=10.24541 | best_loss=10.15068
Epoch 17/80: current_loss=10.32135 | best_loss=10.15068
Epoch 18/80: current_loss=10.26483 | best_loss=10.15068
Epoch 19/80: current_loss=10.24768 | best_loss=10.15068
Epoch 20/80: current_loss=10.35092 | best_loss=10.15068
Epoch 21/80: current_loss=10.31415 | best_loss=10.15068
Epoch 22/80: current_loss=10.30796 | best_loss=10.15068
Early Stopping at epoch 22
      explained_var=0.03023 | mse_loss=9.78520
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.57212 | best_loss=7.57212
Epoch 1/80: current_loss=7.64044 | best_loss=7.57212
Epoch 2/80: current_loss=7.78159 | best_loss=7.57212
Epoch 3/80: current_loss=7.49783 | best_loss=7.49783
Epoch 4/80: current_loss=7.61909 | best_loss=7.49783
Epoch 5/80: current_loss=7.74034 | best_loss=7.49783
Epoch 6/80: current_loss=7.68365 | best_loss=7.49783
Epoch 7/80: current_loss=7.55059 | best_loss=7.49783
Epoch 8/80: current_loss=7.62671 | best_loss=7.49783
Epoch 9/80: current_loss=7.62803 | best_loss=7.49783
Epoch 10/80: current_loss=7.93131 | best_loss=7.49783
Epoch 11/80: current_loss=7.59775 | best_loss=7.49783
Epoch 12/80: current_loss=7.61367 | best_loss=7.49783
Epoch 13/80: current_loss=7.61015 | best_loss=7.49783
Epoch 14/80: current_loss=7.59627 | best_loss=7.49783
Epoch 15/80: current_loss=7.65452 | best_loss=7.49783
Epoch 16/80: current_loss=7.58680 | best_loss=7.49783
Epoch 17/80: current_loss=7.53224 | best_loss=7.49783
Epoch 18/80: current_loss=7.63630 | best_loss=7.49783
Epoch 19/80: current_loss=7.88551 | best_loss=7.49783
Epoch 20/80: current_loss=7.59921 | best_loss=7.49783
Epoch 21/80: current_loss=7.81159 | best_loss=7.49783
Epoch 22/80: current_loss=7.57016 | best_loss=7.49783
Epoch 23/80: current_loss=7.60972 | best_loss=7.49783
Early Stopping at epoch 23
      explained_var=-0.00394 | mse_loss=7.61411

----------------------------------------------
Params for Trial 88
{'learning_rate': 1e-05, 'weight_decay': 0.005714294913981482, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=99.73005 | best_loss=99.73005
Epoch 1/80: current_loss=92.10186 | best_loss=92.10186
Epoch 2/80: current_loss=72.08698 | best_loss=72.08698
Epoch 3/80: current_loss=42.14169 | best_loss=42.14169
Epoch 4/80: current_loss=27.61109 | best_loss=27.61109
Epoch 5/80: current_loss=22.09894 | best_loss=22.09894
Epoch 6/80: current_loss=19.53371 | best_loss=19.53371
Epoch 7/80: current_loss=18.14014 | best_loss=18.14014
Epoch 8/80: current_loss=17.31641 | best_loss=17.31641
Epoch 9/80: current_loss=16.78870 | best_loss=16.78870
Epoch 10/80: current_loss=16.45507 | best_loss=16.45507
Epoch 11/80: current_loss=16.18279 | best_loss=16.18279
Epoch 12/80: current_loss=15.97325 | best_loss=15.97325
Epoch 13/80: current_loss=15.78189 | best_loss=15.78189
Epoch 14/80: current_loss=15.63000 | best_loss=15.63000
Epoch 15/80: current_loss=15.45842 | best_loss=15.45842
Epoch 16/80: current_loss=15.29111 | best_loss=15.29111
Epoch 17/80: current_loss=15.16455 | best_loss=15.16455
Epoch 18/80: current_loss=15.03266 | best_loss=15.03266
Epoch 19/80: current_loss=14.90687 | best_loss=14.90687
Epoch 20/80: current_loss=14.76803 | best_loss=14.76803
Epoch 21/80: current_loss=14.64503 | best_loss=14.64503
Epoch 22/80: current_loss=14.52408 | best_loss=14.52408
Epoch 23/80: current_loss=14.42295 | best_loss=14.42295
Epoch 24/80: current_loss=14.32875 | best_loss=14.32875
Epoch 25/80: current_loss=14.23231 | best_loss=14.23231
Epoch 26/80: current_loss=14.12657 | best_loss=14.12657
Epoch 27/80: current_loss=14.03718 | best_loss=14.03718
Epoch 28/80: current_loss=13.95306 | best_loss=13.95306
Epoch 29/80: current_loss=13.86018 | best_loss=13.86018
Epoch 30/80: current_loss=13.76919 | best_loss=13.76919
Epoch 31/80: current_loss=13.67554 | best_loss=13.67554
Epoch 32/80: current_loss=13.59482 | best_loss=13.59482
Epoch 33/80: current_loss=13.51277 | best_loss=13.51277
Epoch 34/80: current_loss=13.44012 | best_loss=13.44012
Epoch 35/80: current_loss=13.37078 | best_loss=13.37078
Epoch 36/80: current_loss=13.29699 | best_loss=13.29699
Epoch 37/80: current_loss=13.23403 | best_loss=13.23403
Epoch 38/80: current_loss=13.15221 | best_loss=13.15221
Epoch 39/80: current_loss=13.08852 | best_loss=13.08852
Epoch 40/80: current_loss=13.02287 | best_loss=13.02287
Epoch 41/80: current_loss=12.95484 | best_loss=12.95484
Epoch 42/80: current_loss=12.88239 | best_loss=12.88239
Epoch 43/80: current_loss=12.81378 | best_loss=12.81378
Epoch 44/80: current_loss=12.74904 | best_loss=12.74904
Epoch 45/80: current_loss=12.68926 | best_loss=12.68926
Epoch 46/80: current_loss=12.63156 | best_loss=12.63156
Epoch 47/80: current_loss=12.58397 | best_loss=12.58397
Epoch 48/80: current_loss=12.53321 | best_loss=12.53321
Epoch 49/80: current_loss=12.48645 | best_loss=12.48645
Epoch 50/80: current_loss=12.42890 | best_loss=12.42890
Epoch 51/80: current_loss=12.38030 | best_loss=12.38030
Epoch 52/80: current_loss=12.33944 | best_loss=12.33944
Epoch 53/80: current_loss=12.29386 | best_loss=12.29386
Epoch 54/80: current_loss=12.25723 | best_loss=12.25723
Epoch 55/80: current_loss=12.21716 | best_loss=12.21716
Epoch 56/80: current_loss=12.18130 | best_loss=12.18130
Epoch 57/80: current_loss=12.13463 | best_loss=12.13463
Epoch 58/80: current_loss=12.09821 | best_loss=12.09821
Epoch 59/80: current_loss=12.05271 | best_loss=12.05271
Epoch 60/80: current_loss=12.00489 | best_loss=12.00489
Epoch 61/80: current_loss=11.97114 | best_loss=11.97114
Epoch 62/80: current_loss=11.94040 | best_loss=11.94040
Epoch 63/80: current_loss=11.88798 | best_loss=11.88798
Epoch 64/80: current_loss=11.85182 | best_loss=11.85182
Epoch 65/80: current_loss=11.81046 | best_loss=11.81046
Epoch 66/80: current_loss=11.77719 | best_loss=11.77719
Epoch 67/80: current_loss=11.74246 | best_loss=11.74246
Epoch 68/80: current_loss=11.71072 | best_loss=11.71072
Epoch 69/80: current_loss=11.67175 | best_loss=11.67175
Epoch 70/80: current_loss=11.64034 | best_loss=11.64034
Epoch 71/80: current_loss=11.60835 | best_loss=11.60835
Epoch 72/80: current_loss=11.57068 | best_loss=11.57068
Epoch 73/80: current_loss=11.53418 | best_loss=11.53418
Epoch 74/80: current_loss=11.50347 | best_loss=11.50347
Epoch 75/80: current_loss=11.47244 | best_loss=11.47244
Epoch 76/80: current_loss=11.44224 | best_loss=11.44224
Epoch 77/80: current_loss=11.41060 | best_loss=11.41060
Epoch 78/80: current_loss=11.38020 | best_loss=11.38020
Epoch 79/80: current_loss=11.35149 | best_loss=11.35149
      explained_var=-0.10360 | mse_loss=11.64750

----------------------------------------------
Params for Trial 89
{'learning_rate': 0.001, 'weight_decay': 0.006791304877845648, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.54443 | best_loss=16.54443
Epoch 1/80: current_loss=10.64008 | best_loss=10.64008
Epoch 2/80: current_loss=10.41435 | best_loss=10.41435
Epoch 3/80: current_loss=10.39584 | best_loss=10.39584
Epoch 4/80: current_loss=10.34227 | best_loss=10.34227
Epoch 5/80: current_loss=10.34487 | best_loss=10.34227
Epoch 6/80: current_loss=10.32914 | best_loss=10.32914
Epoch 7/80: current_loss=10.30752 | best_loss=10.30752
Epoch 8/80: current_loss=10.29518 | best_loss=10.29518
Epoch 9/80: current_loss=10.30498 | best_loss=10.29518
Epoch 10/80: current_loss=10.28562 | best_loss=10.28562
Epoch 11/80: current_loss=10.29025 | best_loss=10.28562
Epoch 12/80: current_loss=10.28688 | best_loss=10.28562
Epoch 13/80: current_loss=10.31357 | best_loss=10.28562
Epoch 14/80: current_loss=10.28086 | best_loss=10.28086
Epoch 15/80: current_loss=10.35880 | best_loss=10.28086
Epoch 16/80: current_loss=10.28138 | best_loss=10.28086
Epoch 17/80: current_loss=10.29969 | best_loss=10.28086
Epoch 18/80: current_loss=10.32664 | best_loss=10.28086
Epoch 19/80: current_loss=10.28070 | best_loss=10.28070
Epoch 20/80: current_loss=10.28242 | best_loss=10.28070
Epoch 21/80: current_loss=10.27456 | best_loss=10.27456
Epoch 22/80: current_loss=10.27833 | best_loss=10.27456
Epoch 23/80: current_loss=10.27256 | best_loss=10.27256
Epoch 24/80: current_loss=10.27661 | best_loss=10.27256
Epoch 25/80: current_loss=10.28216 | best_loss=10.27256
Epoch 26/80: current_loss=10.31773 | best_loss=10.27256
Epoch 27/80: current_loss=10.28661 | best_loss=10.27256
Epoch 28/80: current_loss=10.28478 | best_loss=10.27256
Epoch 29/80: current_loss=10.29047 | best_loss=10.27256
Epoch 30/80: current_loss=10.29343 | best_loss=10.27256
Epoch 31/80: current_loss=10.29877 | best_loss=10.27256
Epoch 32/80: current_loss=10.31320 | best_loss=10.27256
Epoch 33/80: current_loss=10.28908 | best_loss=10.27256
Epoch 34/80: current_loss=10.29471 | best_loss=10.27256
Epoch 35/80: current_loss=10.33322 | best_loss=10.27256
Epoch 36/80: current_loss=10.27963 | best_loss=10.27256
Epoch 37/80: current_loss=10.29330 | best_loss=10.27256
Epoch 38/80: current_loss=10.31022 | best_loss=10.27256
Epoch 39/80: current_loss=10.28115 | best_loss=10.27256
Epoch 40/80: current_loss=10.29675 | best_loss=10.27256
Epoch 41/80: current_loss=10.30431 | best_loss=10.27256
Epoch 42/80: current_loss=10.32350 | best_loss=10.27256
Epoch 43/80: current_loss=10.29769 | best_loss=10.27256
Early Stopping at epoch 43
      explained_var=-0.00278 | mse_loss=10.49882
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.41081 | best_loss=10.41081
Epoch 1/80: current_loss=10.44529 | best_loss=10.41081
Epoch 2/80: current_loss=10.40662 | best_loss=10.40662
Epoch 3/80: current_loss=10.43593 | best_loss=10.40662
Epoch 4/80: current_loss=10.40447 | best_loss=10.40447
Epoch 5/80: current_loss=10.40761 | best_loss=10.40447
Epoch 6/80: current_loss=10.46775 | best_loss=10.40447
Epoch 7/80: current_loss=10.46183 | best_loss=10.40447
Epoch 8/80: current_loss=10.40704 | best_loss=10.40447
Epoch 9/80: current_loss=10.43892 | best_loss=10.40447
Epoch 10/80: current_loss=10.40412 | best_loss=10.40412
Epoch 11/80: current_loss=10.48608 | best_loss=10.40412
Epoch 12/80: current_loss=10.43940 | best_loss=10.40412
Epoch 13/80: current_loss=10.44844 | best_loss=10.40412
Epoch 14/80: current_loss=10.40798 | best_loss=10.40412
Epoch 15/80: current_loss=10.41253 | best_loss=10.40412
Epoch 16/80: current_loss=10.44564 | best_loss=10.40412
Epoch 17/80: current_loss=10.41487 | best_loss=10.40412
Epoch 18/80: current_loss=10.40683 | best_loss=10.40412
Epoch 19/80: current_loss=10.41607 | best_loss=10.40412
Epoch 20/80: current_loss=10.40406 | best_loss=10.40406
Epoch 21/80: current_loss=10.45741 | best_loss=10.40406
Epoch 22/80: current_loss=10.41325 | best_loss=10.40406
Epoch 23/80: current_loss=10.42648 | best_loss=10.40406
Epoch 24/80: current_loss=10.40826 | best_loss=10.40406
Epoch 25/80: current_loss=10.40877 | best_loss=10.40406
Epoch 26/80: current_loss=10.43286 | best_loss=10.40406
Epoch 27/80: current_loss=10.43694 | best_loss=10.40406
Epoch 28/80: current_loss=10.45540 | best_loss=10.40406
Epoch 29/80: current_loss=10.41485 | best_loss=10.40406
Epoch 30/80: current_loss=10.45260 | best_loss=10.40406
Epoch 31/80: current_loss=10.41159 | best_loss=10.40406
Epoch 32/80: current_loss=10.48706 | best_loss=10.40406
Epoch 33/80: current_loss=10.41486 | best_loss=10.40406
Epoch 34/80: current_loss=10.47566 | best_loss=10.40406
Epoch 35/80: current_loss=10.40661 | best_loss=10.40406
Epoch 36/80: current_loss=10.47749 | best_loss=10.40406
Epoch 37/80: current_loss=10.42017 | best_loss=10.40406
Epoch 38/80: current_loss=10.40736 | best_loss=10.40406
Epoch 39/80: current_loss=10.45046 | best_loss=10.40406
Epoch 40/80: current_loss=10.48387 | best_loss=10.40406
Early Stopping at epoch 40
      explained_var=0.00041 | mse_loss=10.05936

----------------------------------------------
Params for Trial 90
{'learning_rate': 0.0001, 'weight_decay': 0.00864543658979499, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=98.10285 | best_loss=98.10285
Epoch 1/80: current_loss=94.61978 | best_loss=94.61978
Epoch 2/80: current_loss=90.04857 | best_loss=90.04857
Epoch 3/80: current_loss=84.01857 | best_loss=84.01857
Epoch 4/80: current_loss=77.18512 | best_loss=77.18512
Epoch 5/80: current_loss=70.65971 | best_loss=70.65971
Epoch 6/80: current_loss=65.41286 | best_loss=65.41286
Epoch 7/80: current_loss=61.39056 | best_loss=61.39056
Epoch 8/80: current_loss=58.09248 | best_loss=58.09248
Epoch 9/80: current_loss=55.40046 | best_loss=55.40046
Epoch 10/80: current_loss=53.10068 | best_loss=53.10068
Epoch 11/80: current_loss=51.03435 | best_loss=51.03435
Epoch 12/80: current_loss=49.21666 | best_loss=49.21666
Epoch 13/80: current_loss=47.57232 | best_loss=47.57232
Epoch 14/80: current_loss=46.07152 | best_loss=46.07152
Epoch 15/80: current_loss=44.64064 | best_loss=44.64064
Epoch 16/80: current_loss=43.27430 | best_loss=43.27430
Epoch 17/80: current_loss=41.97919 | best_loss=41.97919
Epoch 18/80: current_loss=40.72441 | best_loss=40.72441
Epoch 19/80: current_loss=39.48940 | best_loss=39.48940
Epoch 20/80: current_loss=38.30193 | best_loss=38.30193
Epoch 21/80: current_loss=37.19159 | best_loss=37.19159
Epoch 22/80: current_loss=36.14468 | best_loss=36.14468
Epoch 23/80: current_loss=35.16312 | best_loss=35.16312
Epoch 24/80: current_loss=34.25841 | best_loss=34.25841
Epoch 25/80: current_loss=33.39013 | best_loss=33.39013
Epoch 26/80: current_loss=32.58058 | best_loss=32.58058
Epoch 27/80: current_loss=31.80542 | best_loss=31.80542
Epoch 28/80: current_loss=31.07219 | best_loss=31.07219
Epoch 29/80: current_loss=30.37578 | best_loss=30.37578
Epoch 30/80: current_loss=29.69568 | best_loss=29.69568
Epoch 31/80: current_loss=29.04701 | best_loss=29.04701
Epoch 32/80: current_loss=28.43038 | best_loss=28.43038
Epoch 33/80: current_loss=27.82603 | best_loss=27.82603
Epoch 34/80: current_loss=27.24342 | best_loss=27.24342
Epoch 35/80: current_loss=26.68089 | best_loss=26.68089
Epoch 36/80: current_loss=26.12375 | best_loss=26.12375
Epoch 37/80: current_loss=25.58744 | best_loss=25.58744
Epoch 38/80: current_loss=25.07684 | best_loss=25.07684
Epoch 39/80: current_loss=24.57809 | best_loss=24.57809
Epoch 40/80: current_loss=24.09406 | best_loss=24.09406
Epoch 41/80: current_loss=23.62557 | best_loss=23.62557
Epoch 42/80: current_loss=23.16713 | best_loss=23.16713
Epoch 43/80: current_loss=22.72081 | best_loss=22.72081
Epoch 44/80: current_loss=22.29784 | best_loss=22.29784
Epoch 45/80: current_loss=21.88216 | best_loss=21.88216
Epoch 46/80: current_loss=21.48475 | best_loss=21.48475
Epoch 47/80: current_loss=21.09578 | best_loss=21.09578
Epoch 48/80: current_loss=20.71005 | best_loss=20.71005
Epoch 49/80: current_loss=20.33750 | best_loss=20.33750
Epoch 50/80: current_loss=19.98529 | best_loss=19.98529
Epoch 51/80: current_loss=19.63274 | best_loss=19.63274
Epoch 52/80: current_loss=19.29532 | best_loss=19.29532
Epoch 53/80: current_loss=18.96213 | best_loss=18.96213
Epoch 54/80: current_loss=18.64661 | best_loss=18.64661
Epoch 55/80: current_loss=18.34212 | best_loss=18.34212
Epoch 56/80: current_loss=18.05268 | best_loss=18.05268
Epoch 57/80: current_loss=17.76629 | best_loss=17.76629
Epoch 58/80: current_loss=17.49044 | best_loss=17.49044
Epoch 59/80: current_loss=17.22153 | best_loss=17.22153
Epoch 60/80: current_loss=16.95338 | best_loss=16.95338
Epoch 61/80: current_loss=16.69817 | best_loss=16.69817
Epoch 62/80: current_loss=16.44905 | best_loss=16.44905
Epoch 63/80: current_loss=16.20650 | best_loss=16.20650
Epoch 64/80: current_loss=15.98576 | best_loss=15.98576
Epoch 65/80: current_loss=15.76046 | best_loss=15.76046
Epoch 66/80: current_loss=15.54239 | best_loss=15.54239
Epoch 67/80: current_loss=15.33577 | best_loss=15.33577
Epoch 68/80: current_loss=15.13180 | best_loss=15.13180
Epoch 69/80: current_loss=14.93826 | best_loss=14.93826
Epoch 70/80: current_loss=14.74972 | best_loss=14.74972
Epoch 71/80: current_loss=14.56846 | best_loss=14.56846
Epoch 72/80: current_loss=14.38463 | best_loss=14.38463
Epoch 73/80: current_loss=14.21748 | best_loss=14.21748
Epoch 74/80: current_loss=14.05497 | best_loss=14.05497
Epoch 75/80: current_loss=13.89685 | best_loss=13.89685
Epoch 76/80: current_loss=13.74745 | best_loss=13.74745
Epoch 77/80: current_loss=13.60457 | best_loss=13.60457
Epoch 78/80: current_loss=13.46134 | best_loss=13.46134
Epoch 79/80: current_loss=13.31677 | best_loss=13.31677
      explained_var=-0.00775 | mse_loss=13.77117

----------------------------------------------
Params for Trial 91
{'learning_rate': 0.001, 'weight_decay': 0.006501777085462004, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.95282 | best_loss=16.95282
Epoch 1/80: current_loss=10.74452 | best_loss=10.74452
Epoch 2/80: current_loss=10.41795 | best_loss=10.41795
Epoch 3/80: current_loss=10.33750 | best_loss=10.33750
Epoch 4/80: current_loss=10.34911 | best_loss=10.33750
Epoch 5/80: current_loss=10.34604 | best_loss=10.33750
Epoch 6/80: current_loss=10.31100 | best_loss=10.31100
Epoch 7/80: current_loss=10.33015 | best_loss=10.31100
Epoch 8/80: current_loss=10.30742 | best_loss=10.30742
Epoch 9/80: current_loss=10.30903 | best_loss=10.30742
Epoch 10/80: current_loss=10.30937 | best_loss=10.30742
Epoch 11/80: current_loss=10.30730 | best_loss=10.30730
Epoch 12/80: current_loss=10.28675 | best_loss=10.28675
Epoch 13/80: current_loss=10.30978 | best_loss=10.28675
Epoch 14/80: current_loss=10.30418 | best_loss=10.28675
Epoch 15/80: current_loss=10.29911 | best_loss=10.28675
Epoch 16/80: current_loss=10.29115 | best_loss=10.28675
Epoch 17/80: current_loss=10.30157 | best_loss=10.28675
Epoch 18/80: current_loss=10.28741 | best_loss=10.28675
Epoch 19/80: current_loss=10.27308 | best_loss=10.27308
Epoch 20/80: current_loss=10.30617 | best_loss=10.27308
Epoch 21/80: current_loss=10.29979 | best_loss=10.27308
Epoch 22/80: current_loss=10.27263 | best_loss=10.27263
Epoch 23/80: current_loss=10.28064 | best_loss=10.27263
Epoch 24/80: current_loss=10.29566 | best_loss=10.27263
Epoch 25/80: current_loss=10.30534 | best_loss=10.27263
Epoch 26/80: current_loss=10.29330 | best_loss=10.27263
Epoch 27/80: current_loss=10.27727 | best_loss=10.27263
Epoch 28/80: current_loss=10.29544 | best_loss=10.27263
Epoch 29/80: current_loss=10.29363 | best_loss=10.27263
Epoch 30/80: current_loss=10.30043 | best_loss=10.27263
Epoch 31/80: current_loss=10.28899 | best_loss=10.27263
Epoch 32/80: current_loss=10.28010 | best_loss=10.27263
Epoch 33/80: current_loss=10.31901 | best_loss=10.27263
Epoch 34/80: current_loss=10.28092 | best_loss=10.27263
Epoch 35/80: current_loss=10.27894 | best_loss=10.27263
Epoch 36/80: current_loss=10.27037 | best_loss=10.27037
Epoch 37/80: current_loss=10.34734 | best_loss=10.27037
Epoch 38/80: current_loss=10.28960 | best_loss=10.27037
Epoch 39/80: current_loss=10.28967 | best_loss=10.27037
Epoch 40/80: current_loss=10.31133 | best_loss=10.27037
Epoch 41/80: current_loss=10.27632 | best_loss=10.27037
Epoch 42/80: current_loss=10.29190 | best_loss=10.27037
Epoch 43/80: current_loss=10.29090 | best_loss=10.27037
Epoch 44/80: current_loss=10.28875 | best_loss=10.27037
Epoch 45/80: current_loss=10.26977 | best_loss=10.26977
Epoch 46/80: current_loss=10.29069 | best_loss=10.26977
Epoch 47/80: current_loss=10.32606 | best_loss=10.26977
Epoch 48/80: current_loss=10.28679 | best_loss=10.26977
Epoch 49/80: current_loss=10.29668 | best_loss=10.26977
Epoch 50/80: current_loss=10.30999 | best_loss=10.26977
Epoch 51/80: current_loss=10.26415 | best_loss=10.26415
Epoch 52/80: current_loss=10.27771 | best_loss=10.26415
Epoch 53/80: current_loss=10.28378 | best_loss=10.26415
Epoch 54/80: current_loss=10.33263 | best_loss=10.26415
Epoch 55/80: current_loss=10.27925 | best_loss=10.26415
Epoch 56/80: current_loss=10.29131 | best_loss=10.26415
Epoch 57/80: current_loss=10.27641 | best_loss=10.26415
Epoch 58/80: current_loss=10.29621 | best_loss=10.26415
Epoch 59/80: current_loss=10.26987 | best_loss=10.26415
Epoch 60/80: current_loss=10.27470 | best_loss=10.26415
Epoch 61/80: current_loss=10.35807 | best_loss=10.26415
Epoch 62/80: current_loss=10.26774 | best_loss=10.26415
Epoch 63/80: current_loss=10.27193 | best_loss=10.26415
Epoch 64/80: current_loss=10.29552 | best_loss=10.26415
Epoch 65/80: current_loss=10.27497 | best_loss=10.26415
Epoch 66/80: current_loss=10.29924 | best_loss=10.26415
Epoch 67/80: current_loss=10.29892 | best_loss=10.26415
Epoch 68/80: current_loss=10.25778 | best_loss=10.25778
Epoch 69/80: current_loss=10.24579 | best_loss=10.24579
Epoch 70/80: current_loss=10.28619 | best_loss=10.24579
Epoch 71/80: current_loss=10.24746 | best_loss=10.24579
Epoch 72/80: current_loss=10.23145 | best_loss=10.23145
Epoch 73/80: current_loss=10.18196 | best_loss=10.18196
Epoch 74/80: current_loss=10.26444 | best_loss=10.18196
Epoch 75/80: current_loss=10.26188 | best_loss=10.18196
Epoch 76/80: current_loss=10.24403 | best_loss=10.18196
Epoch 77/80: current_loss=10.20663 | best_loss=10.18196
Epoch 78/80: current_loss=10.15331 | best_loss=10.15331
Epoch 79/80: current_loss=10.18130 | best_loss=10.15331
      explained_var=0.00978 | mse_loss=10.39045
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.47498 | best_loss=10.47498
Epoch 1/80: current_loss=10.45462 | best_loss=10.45462
Epoch 2/80: current_loss=10.27852 | best_loss=10.27852
Epoch 3/80: current_loss=10.29041 | best_loss=10.27852
Epoch 4/80: current_loss=10.37807 | best_loss=10.27852
Epoch 5/80: current_loss=10.42245 | best_loss=10.27852
Epoch 6/80: current_loss=10.36939 | best_loss=10.27852
Epoch 7/80: current_loss=10.28128 | best_loss=10.27852
Epoch 8/80: current_loss=10.33622 | best_loss=10.27852
Epoch 9/80: current_loss=10.52751 | best_loss=10.27852
Epoch 10/80: current_loss=10.40601 | best_loss=10.27852
Epoch 11/80: current_loss=10.38487 | best_loss=10.27852
Epoch 12/80: current_loss=10.28393 | best_loss=10.27852
Epoch 13/80: current_loss=10.37562 | best_loss=10.27852
Epoch 14/80: current_loss=10.50755 | best_loss=10.27852
Epoch 15/80: current_loss=10.40104 | best_loss=10.27852
Epoch 16/80: current_loss=10.44503 | best_loss=10.27852
Epoch 17/80: current_loss=10.41462 | best_loss=10.27852
Epoch 18/80: current_loss=10.38647 | best_loss=10.27852
Epoch 19/80: current_loss=10.46006 | best_loss=10.27852
Epoch 20/80: current_loss=10.43066 | best_loss=10.27852
Epoch 21/80: current_loss=10.49202 | best_loss=10.27852
Epoch 22/80: current_loss=10.41576 | best_loss=10.27852
Early Stopping at epoch 22
      explained_var=0.01729 | mse_loss=9.92225
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.68204 | best_loss=7.68204
Epoch 1/80: current_loss=7.59512 | best_loss=7.59512
Epoch 2/80: current_loss=7.74816 | best_loss=7.59512
Epoch 3/80: current_loss=7.91221 | best_loss=7.59512
Epoch 4/80: current_loss=7.50928 | best_loss=7.50928
Epoch 5/80: current_loss=7.52613 | best_loss=7.50928
Epoch 6/80: current_loss=7.57416 | best_loss=7.50928
Epoch 7/80: current_loss=7.70531 | best_loss=7.50928
Epoch 8/80: current_loss=7.55277 | best_loss=7.50928
Epoch 9/80: current_loss=7.55167 | best_loss=7.50928
Epoch 10/80: current_loss=7.59040 | best_loss=7.50928
Epoch 11/80: current_loss=7.61647 | best_loss=7.50928
Epoch 12/80: current_loss=7.52831 | best_loss=7.50928
Epoch 13/80: current_loss=7.57578 | best_loss=7.50928
Epoch 14/80: current_loss=7.64563 | best_loss=7.50928
Epoch 15/80: current_loss=7.52405 | best_loss=7.50928
Epoch 16/80: current_loss=7.57048 | best_loss=7.50928
Epoch 17/80: current_loss=7.74697 | best_loss=7.50928
Epoch 18/80: current_loss=7.63543 | best_loss=7.50928
Epoch 19/80: current_loss=7.61618 | best_loss=7.50928
Epoch 20/80: current_loss=7.77591 | best_loss=7.50928
Epoch 21/80: current_loss=7.59058 | best_loss=7.50928
Epoch 22/80: current_loss=7.61186 | best_loss=7.50928
Epoch 23/80: current_loss=7.59989 | best_loss=7.50928
Epoch 24/80: current_loss=7.59607 | best_loss=7.50928
Early Stopping at epoch 24
      explained_var=-0.00634 | mse_loss=7.64681

----------------------------------------------
Params for Trial 92
{'learning_rate': 0.001, 'weight_decay': 0.005474892820798748, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.83008 | best_loss=17.83008
Epoch 1/80: current_loss=10.81916 | best_loss=10.81916
Epoch 2/80: current_loss=10.41755 | best_loss=10.41755
Epoch 3/80: current_loss=10.35709 | best_loss=10.35709
Epoch 4/80: current_loss=10.34636 | best_loss=10.34636
Epoch 5/80: current_loss=10.31121 | best_loss=10.31121
Epoch 6/80: current_loss=10.32071 | best_loss=10.31121
Epoch 7/80: current_loss=10.31506 | best_loss=10.31121
Epoch 8/80: current_loss=10.33621 | best_loss=10.31121
Epoch 9/80: current_loss=10.33339 | best_loss=10.31121
Epoch 10/80: current_loss=10.29665 | best_loss=10.29665
Epoch 11/80: current_loss=10.29988 | best_loss=10.29665
Epoch 12/80: current_loss=10.28863 | best_loss=10.28863
Epoch 13/80: current_loss=10.30058 | best_loss=10.28863
Epoch 14/80: current_loss=10.29734 | best_loss=10.28863
Epoch 15/80: current_loss=10.29899 | best_loss=10.28863
Epoch 16/80: current_loss=10.28544 | best_loss=10.28544
Epoch 17/80: current_loss=10.35464 | best_loss=10.28544
Epoch 18/80: current_loss=10.27501 | best_loss=10.27501
Epoch 19/80: current_loss=10.28365 | best_loss=10.27501
Epoch 20/80: current_loss=10.29137 | best_loss=10.27501
Epoch 21/80: current_loss=10.27267 | best_loss=10.27267
Epoch 22/80: current_loss=10.28162 | best_loss=10.27267
Epoch 23/80: current_loss=10.27231 | best_loss=10.27231
Epoch 24/80: current_loss=10.27483 | best_loss=10.27231
Epoch 25/80: current_loss=10.33105 | best_loss=10.27231
Epoch 26/80: current_loss=10.28199 | best_loss=10.27231
Epoch 27/80: current_loss=10.28201 | best_loss=10.27231
Epoch 28/80: current_loss=10.32205 | best_loss=10.27231
Epoch 29/80: current_loss=10.30472 | best_loss=10.27231
Epoch 30/80: current_loss=10.29433 | best_loss=10.27231
Epoch 31/80: current_loss=10.30282 | best_loss=10.27231
Epoch 32/80: current_loss=10.30384 | best_loss=10.27231
Epoch 33/80: current_loss=10.30905 | best_loss=10.27231
Epoch 34/80: current_loss=10.30090 | best_loss=10.27231
Epoch 35/80: current_loss=10.29218 | best_loss=10.27231
Epoch 36/80: current_loss=10.32405 | best_loss=10.27231
Epoch 37/80: current_loss=10.28123 | best_loss=10.27231
Epoch 38/80: current_loss=10.28052 | best_loss=10.27231
Epoch 39/80: current_loss=10.28130 | best_loss=10.27231
Epoch 40/80: current_loss=10.27947 | best_loss=10.27231
Epoch 41/80: current_loss=10.29442 | best_loss=10.27231
Epoch 42/80: current_loss=10.27580 | best_loss=10.27231
Epoch 43/80: current_loss=10.29056 | best_loss=10.27231
Early Stopping at epoch 43
      explained_var=-0.00289 | mse_loss=10.49763
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.43505 | best_loss=10.43505
Epoch 1/80: current_loss=10.41539 | best_loss=10.41539
Epoch 2/80: current_loss=10.40822 | best_loss=10.40822
Epoch 3/80: current_loss=10.42586 | best_loss=10.40822
Epoch 4/80: current_loss=10.40867 | best_loss=10.40822
Epoch 5/80: current_loss=10.37262 | best_loss=10.37262
Epoch 6/80: current_loss=10.40146 | best_loss=10.37262
Epoch 7/80: current_loss=10.40097 | best_loss=10.37262
Epoch 8/80: current_loss=10.40589 | best_loss=10.37262
Epoch 9/80: current_loss=10.36251 | best_loss=10.36251
Epoch 10/80: current_loss=10.47103 | best_loss=10.36251
Epoch 11/80: current_loss=10.40733 | best_loss=10.36251
Epoch 12/80: current_loss=10.40829 | best_loss=10.36251
Epoch 13/80: current_loss=10.41947 | best_loss=10.36251
Epoch 14/80: current_loss=10.40464 | best_loss=10.36251
Epoch 15/80: current_loss=10.40292 | best_loss=10.36251
Epoch 16/80: current_loss=10.48250 | best_loss=10.36251
Epoch 17/80: current_loss=10.39965 | best_loss=10.36251
Epoch 18/80: current_loss=10.42315 | best_loss=10.36251
Epoch 19/80: current_loss=10.40727 | best_loss=10.36251
Epoch 20/80: current_loss=10.40502 | best_loss=10.36251
Epoch 21/80: current_loss=10.42214 | best_loss=10.36251
Epoch 22/80: current_loss=10.46033 | best_loss=10.36251
Epoch 23/80: current_loss=10.47129 | best_loss=10.36251
Epoch 24/80: current_loss=10.40706 | best_loss=10.36251
Epoch 25/80: current_loss=10.40756 | best_loss=10.36251
Epoch 26/80: current_loss=10.41786 | best_loss=10.36251
Epoch 27/80: current_loss=10.41972 | best_loss=10.36251
Epoch 28/80: current_loss=10.55418 | best_loss=10.36251
Epoch 29/80: current_loss=10.40597 | best_loss=10.36251
Early Stopping at epoch 29
      explained_var=0.00509 | mse_loss=10.01230
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.47170 | best_loss=7.47170
Epoch 1/80: current_loss=7.51328 | best_loss=7.47170
Epoch 2/80: current_loss=7.52162 | best_loss=7.47170
Epoch 3/80: current_loss=7.74861 | best_loss=7.47170
Epoch 4/80: current_loss=7.53902 | best_loss=7.47170
Epoch 5/80: current_loss=7.55845 | best_loss=7.47170
Epoch 6/80: current_loss=7.49889 | best_loss=7.47170
Epoch 7/80: current_loss=7.60651 | best_loss=7.47170
Epoch 8/80: current_loss=7.60508 | best_loss=7.47170
Epoch 9/80: current_loss=7.62821 | best_loss=7.47170
Epoch 10/80: current_loss=7.60522 | best_loss=7.47170
Epoch 11/80: current_loss=7.61394 | best_loss=7.47170
Epoch 12/80: current_loss=7.64417 | best_loss=7.47170
Epoch 13/80: current_loss=7.54136 | best_loss=7.47170
Epoch 14/80: current_loss=7.57605 | best_loss=7.47170
Epoch 15/80: current_loss=7.50171 | best_loss=7.47170
Epoch 16/80: current_loss=7.70386 | best_loss=7.47170
Epoch 17/80: current_loss=7.59841 | best_loss=7.47170
Epoch 18/80: current_loss=7.68690 | best_loss=7.47170
Epoch 19/80: current_loss=7.53143 | best_loss=7.47170
Epoch 20/80: current_loss=7.65432 | best_loss=7.47170
Early Stopping at epoch 20
      explained_var=0.00186 | mse_loss=7.57577
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.82309 | best_loss=8.82309
Epoch 1/80: current_loss=8.78588 | best_loss=8.78588
Epoch 2/80: current_loss=8.72455 | best_loss=8.72455
Epoch 3/80: current_loss=8.72589 | best_loss=8.72455
Epoch 4/80: current_loss=8.88089 | best_loss=8.72455
Epoch 5/80: current_loss=8.76547 | best_loss=8.72455
Epoch 6/80: current_loss=8.74169 | best_loss=8.72455
Epoch 7/80: current_loss=8.70623 | best_loss=8.70623
Epoch 8/80: current_loss=8.74503 | best_loss=8.70623
Epoch 9/80: current_loss=8.85252 | best_loss=8.70623
Epoch 10/80: current_loss=9.14399 | best_loss=8.70623
Epoch 11/80: current_loss=8.73007 | best_loss=8.70623
Epoch 12/80: current_loss=8.76694 | best_loss=8.70623
Epoch 13/80: current_loss=8.73489 | best_loss=8.70623
Epoch 14/80: current_loss=9.23568 | best_loss=8.70623
Epoch 15/80: current_loss=8.75830 | best_loss=8.70623
Epoch 16/80: current_loss=8.82885 | best_loss=8.70623
Epoch 17/80: current_loss=8.75072 | best_loss=8.70623
Epoch 18/80: current_loss=8.78538 | best_loss=8.70623
Epoch 19/80: current_loss=8.76828 | best_loss=8.70623
Epoch 20/80: current_loss=8.83601 | best_loss=8.70623
Epoch 21/80: current_loss=8.79621 | best_loss=8.70623
Epoch 22/80: current_loss=8.82406 | best_loss=8.70623
Epoch 23/80: current_loss=8.86388 | best_loss=8.70623
Epoch 24/80: current_loss=8.97816 | best_loss=8.70623
Epoch 25/80: current_loss=8.79283 | best_loss=8.70623
Epoch 26/80: current_loss=8.77901 | best_loss=8.70623
Epoch 27/80: current_loss=8.87127 | best_loss=8.70623
Early Stopping at epoch 27
      explained_var=0.00611 | mse_loss=8.66450
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.18189 | best_loss=9.18189
Epoch 1/80: current_loss=9.24426 | best_loss=9.18189
Epoch 2/80: current_loss=9.44650 | best_loss=9.18189
Epoch 3/80: current_loss=9.19277 | best_loss=9.18189
Epoch 4/80: current_loss=9.10388 | best_loss=9.10388
Epoch 5/80: current_loss=9.21843 | best_loss=9.10388
Epoch 6/80: current_loss=9.22493 | best_loss=9.10388
Epoch 7/80: current_loss=9.29365 | best_loss=9.10388
Epoch 8/80: current_loss=9.18262 | best_loss=9.10388
Epoch 9/80: current_loss=9.19483 | best_loss=9.10388
Epoch 10/80: current_loss=9.22841 | best_loss=9.10388
Epoch 11/80: current_loss=9.21301 | best_loss=9.10388
Epoch 12/80: current_loss=9.19205 | best_loss=9.10388
Epoch 13/80: current_loss=9.22497 | best_loss=9.10388
Epoch 14/80: current_loss=9.16816 | best_loss=9.10388
Epoch 15/80: current_loss=9.16314 | best_loss=9.10388
Epoch 16/80: current_loss=9.20444 | best_loss=9.10388
Epoch 17/80: current_loss=9.14886 | best_loss=9.10388
Epoch 18/80: current_loss=9.35846 | best_loss=9.10388
Epoch 19/80: current_loss=9.21543 | best_loss=9.10388
Epoch 20/80: current_loss=9.16989 | best_loss=9.10388
Epoch 21/80: current_loss=9.19584 | best_loss=9.10388
Epoch 22/80: current_loss=9.17910 | best_loss=9.10388
Epoch 23/80: current_loss=9.20423 | best_loss=9.10388
Epoch 24/80: current_loss=9.18375 | best_loss=9.10388
Early Stopping at epoch 24
      explained_var=0.02416 | mse_loss=9.06983
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=0.00687| avg_loss=9.16401
----------------------------------------------


----------------------------------------------
Params for Trial 93
{'learning_rate': 0.001, 'weight_decay': 0.004817729729684827, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=16.66071 | best_loss=16.66071
Epoch 1/80: current_loss=10.65445 | best_loss=10.65445
Epoch 2/80: current_loss=10.44234 | best_loss=10.44234
Epoch 3/80: current_loss=10.37484 | best_loss=10.37484
Epoch 4/80: current_loss=10.35699 | best_loss=10.35699
Epoch 5/80: current_loss=10.32574 | best_loss=10.32574
Epoch 6/80: current_loss=10.35578 | best_loss=10.32574
Epoch 7/80: current_loss=10.31188 | best_loss=10.31188
Epoch 8/80: current_loss=10.31074 | best_loss=10.31074
Epoch 9/80: current_loss=10.32147 | best_loss=10.31074
Epoch 10/80: current_loss=10.31995 | best_loss=10.31074
Epoch 11/80: current_loss=10.30122 | best_loss=10.30122
Epoch 12/80: current_loss=10.32949 | best_loss=10.30122
Epoch 13/80: current_loss=10.31065 | best_loss=10.30122
Epoch 14/80: current_loss=10.32990 | best_loss=10.30122
Epoch 15/80: current_loss=10.28291 | best_loss=10.28291
Epoch 16/80: current_loss=10.28286 | best_loss=10.28286
Epoch 17/80: current_loss=10.34702 | best_loss=10.28286
Epoch 18/80: current_loss=10.30058 | best_loss=10.28286
Epoch 19/80: current_loss=10.27990 | best_loss=10.27990
Epoch 20/80: current_loss=10.32367 | best_loss=10.27990
Epoch 21/80: current_loss=10.28610 | best_loss=10.27990
Epoch 22/80: current_loss=10.28106 | best_loss=10.27990
Epoch 23/80: current_loss=10.32019 | best_loss=10.27990
Epoch 24/80: current_loss=10.27245 | best_loss=10.27245
Epoch 25/80: current_loss=10.28055 | best_loss=10.27245
Epoch 26/80: current_loss=10.29742 | best_loss=10.27245
Epoch 27/80: current_loss=10.27685 | best_loss=10.27245
Epoch 28/80: current_loss=10.31156 | best_loss=10.27245
Epoch 29/80: current_loss=10.27129 | best_loss=10.27129
Epoch 30/80: current_loss=10.27165 | best_loss=10.27129
Epoch 31/80: current_loss=10.30186 | best_loss=10.27129
Epoch 32/80: current_loss=10.30298 | best_loss=10.27129
Epoch 33/80: current_loss=10.29323 | best_loss=10.27129
Epoch 34/80: current_loss=10.29884 | best_loss=10.27129
Epoch 35/80: current_loss=10.29954 | best_loss=10.27129
Epoch 36/80: current_loss=10.28213 | best_loss=10.27129
Epoch 37/80: current_loss=10.31296 | best_loss=10.27129
Epoch 38/80: current_loss=10.30829 | best_loss=10.27129
Epoch 39/80: current_loss=10.28572 | best_loss=10.27129
Epoch 40/80: current_loss=10.31774 | best_loss=10.27129
Epoch 41/80: current_loss=10.30121 | best_loss=10.27129
Epoch 42/80: current_loss=10.30308 | best_loss=10.27129
Epoch 43/80: current_loss=10.31758 | best_loss=10.27129
Epoch 44/80: current_loss=10.27552 | best_loss=10.27129
Epoch 45/80: current_loss=10.30381 | best_loss=10.27129
Epoch 46/80: current_loss=10.33983 | best_loss=10.27129
Epoch 47/80: current_loss=10.28970 | best_loss=10.27129
Epoch 48/80: current_loss=10.36955 | best_loss=10.27129
Epoch 49/80: current_loss=10.28063 | best_loss=10.27129
Early Stopping at epoch 49
      explained_var=-0.00241 | mse_loss=10.49802
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.42015 | best_loss=10.42015
Epoch 1/80: current_loss=10.41915 | best_loss=10.41915
Epoch 2/80: current_loss=10.46570 | best_loss=10.41915
Epoch 3/80: current_loss=10.42210 | best_loss=10.41915
Epoch 4/80: current_loss=10.49879 | best_loss=10.41915
Epoch 5/80: current_loss=10.44065 | best_loss=10.41915
Epoch 6/80: current_loss=10.49131 | best_loss=10.41915
Epoch 7/80: current_loss=10.42146 | best_loss=10.41915
Epoch 8/80: current_loss=10.40489 | best_loss=10.40489
Epoch 9/80: current_loss=10.41853 | best_loss=10.40489
Epoch 10/80: current_loss=10.45350 | best_loss=10.40489
Epoch 11/80: current_loss=10.40814 | best_loss=10.40489
Epoch 12/80: current_loss=10.41678 | best_loss=10.40489
Epoch 13/80: current_loss=10.45036 | best_loss=10.40489
Epoch 14/80: current_loss=10.43062 | best_loss=10.40489
Epoch 15/80: current_loss=10.40437 | best_loss=10.40437
Epoch 16/80: current_loss=10.42860 | best_loss=10.40437
Epoch 17/80: current_loss=10.40454 | best_loss=10.40437
Epoch 18/80: current_loss=10.40441 | best_loss=10.40437
Epoch 19/80: current_loss=10.44023 | best_loss=10.40437
Epoch 20/80: current_loss=10.40428 | best_loss=10.40428
Epoch 21/80: current_loss=10.41217 | best_loss=10.40428
Epoch 22/80: current_loss=10.41133 | best_loss=10.40428
Epoch 23/80: current_loss=10.50465 | best_loss=10.40428
Epoch 24/80: current_loss=10.40965 | best_loss=10.40428
Epoch 25/80: current_loss=10.41234 | best_loss=10.40428
Epoch 26/80: current_loss=10.42312 | best_loss=10.40428
Epoch 27/80: current_loss=10.42293 | best_loss=10.40428
Epoch 28/80: current_loss=10.43483 | best_loss=10.40428
Epoch 29/80: current_loss=10.41620 | best_loss=10.40428
Epoch 30/80: current_loss=10.41701 | best_loss=10.40428
Epoch 31/80: current_loss=10.43775 | best_loss=10.40428
Epoch 32/80: current_loss=10.40879 | best_loss=10.40428
Epoch 33/80: current_loss=10.42691 | best_loss=10.40428
Epoch 34/80: current_loss=10.43153 | best_loss=10.40428
Epoch 35/80: current_loss=10.40715 | best_loss=10.40428
Epoch 36/80: current_loss=10.41200 | best_loss=10.40428
Epoch 37/80: current_loss=10.41075 | best_loss=10.40428
Epoch 38/80: current_loss=10.40758 | best_loss=10.40428
Epoch 39/80: current_loss=10.46957 | best_loss=10.40428
Epoch 40/80: current_loss=10.40209 | best_loss=10.40209
Epoch 41/80: current_loss=10.39122 | best_loss=10.39122
Epoch 42/80: current_loss=10.37329 | best_loss=10.37329
Epoch 43/80: current_loss=10.43563 | best_loss=10.37329
Epoch 44/80: current_loss=10.44374 | best_loss=10.37329
Epoch 45/80: current_loss=10.41976 | best_loss=10.37329
Epoch 46/80: current_loss=10.40970 | best_loss=10.37329
Epoch 47/80: current_loss=10.48338 | best_loss=10.37329
Epoch 48/80: current_loss=10.42502 | best_loss=10.37329
Epoch 49/80: current_loss=10.39603 | best_loss=10.37329
Epoch 50/80: current_loss=10.39383 | best_loss=10.37329
Epoch 51/80: current_loss=10.26845 | best_loss=10.26845
Epoch 52/80: current_loss=10.40716 | best_loss=10.26845
Epoch 53/80: current_loss=10.42006 | best_loss=10.26845
Epoch 54/80: current_loss=10.43989 | best_loss=10.26845
Epoch 55/80: current_loss=10.40794 | best_loss=10.26845
Epoch 56/80: current_loss=10.46582 | best_loss=10.26845
Epoch 57/80: current_loss=10.41252 | best_loss=10.26845
Epoch 58/80: current_loss=10.44854 | best_loss=10.26845
Epoch 59/80: current_loss=10.40611 | best_loss=10.26845
Epoch 60/80: current_loss=10.40919 | best_loss=10.26845
Epoch 61/80: current_loss=10.41827 | best_loss=10.26845
Epoch 62/80: current_loss=10.44847 | best_loss=10.26845
Epoch 63/80: current_loss=10.41784 | best_loss=10.26845
Epoch 64/80: current_loss=10.45168 | best_loss=10.26845
Epoch 65/80: current_loss=10.41476 | best_loss=10.26845
Epoch 66/80: current_loss=10.37644 | best_loss=10.26845
Epoch 67/80: current_loss=10.36902 | best_loss=10.26845
Epoch 68/80: current_loss=10.49734 | best_loss=10.26845
Epoch 69/80: current_loss=10.62350 | best_loss=10.26845
Epoch 70/80: current_loss=10.41606 | best_loss=10.26845
Epoch 71/80: current_loss=10.42947 | best_loss=10.26845
Early Stopping at epoch 71
      explained_var=0.02326 | mse_loss=9.90594
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.83467 | best_loss=7.83467
Epoch 1/80: current_loss=7.60464 | best_loss=7.60464
Epoch 2/80: current_loss=7.50649 | best_loss=7.50649
Epoch 3/80: current_loss=7.48281 | best_loss=7.48281
Epoch 4/80: current_loss=7.63206 | best_loss=7.48281
Epoch 5/80: current_loss=7.52905 | best_loss=7.48281
Epoch 6/80: current_loss=7.60066 | best_loss=7.48281
Epoch 7/80: current_loss=7.68797 | best_loss=7.48281
Epoch 8/80: current_loss=7.64131 | best_loss=7.48281
Epoch 9/80: current_loss=7.59007 | best_loss=7.48281
Epoch 10/80: current_loss=7.53958 | best_loss=7.48281
Epoch 11/80: current_loss=7.58291 | best_loss=7.48281
Epoch 12/80: current_loss=7.54616 | best_loss=7.48281
Epoch 13/80: current_loss=7.70362 | best_loss=7.48281
Epoch 14/80: current_loss=7.54147 | best_loss=7.48281
Epoch 15/80: current_loss=7.68715 | best_loss=7.48281
Epoch 16/80: current_loss=7.55838 | best_loss=7.48281
Epoch 17/80: current_loss=7.46579 | best_loss=7.46579
Epoch 18/80: current_loss=7.60741 | best_loss=7.46579
Epoch 19/80: current_loss=7.64032 | best_loss=7.46579
Epoch 20/80: current_loss=7.61372 | best_loss=7.46579
Epoch 21/80: current_loss=7.53458 | best_loss=7.46579
Epoch 22/80: current_loss=7.59189 | best_loss=7.46579
Epoch 23/80: current_loss=7.50597 | best_loss=7.46579
Epoch 24/80: current_loss=7.73487 | best_loss=7.46579
Epoch 25/80: current_loss=7.54014 | best_loss=7.46579
Epoch 26/80: current_loss=7.59733 | best_loss=7.46579
Epoch 27/80: current_loss=7.56282 | best_loss=7.46579
Epoch 28/80: current_loss=7.69492 | best_loss=7.46579
Epoch 29/80: current_loss=7.51376 | best_loss=7.46579
Epoch 30/80: current_loss=7.74555 | best_loss=7.46579
Epoch 31/80: current_loss=7.51236 | best_loss=7.46579
Epoch 32/80: current_loss=7.67015 | best_loss=7.46579
Epoch 33/80: current_loss=7.50774 | best_loss=7.46579
Epoch 34/80: current_loss=7.67363 | best_loss=7.46579
Epoch 35/80: current_loss=7.51939 | best_loss=7.46579
Epoch 36/80: current_loss=7.57489 | best_loss=7.46579
Epoch 37/80: current_loss=7.59120 | best_loss=7.46579
Early Stopping at epoch 37
      explained_var=0.00232 | mse_loss=7.57298
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=8.79064 | best_loss=8.79064
Epoch 1/80: current_loss=8.77617 | best_loss=8.77617
Epoch 2/80: current_loss=8.77532 | best_loss=8.77532
Epoch 3/80: current_loss=8.89755 | best_loss=8.77532
Epoch 4/80: current_loss=8.81319 | best_loss=8.77532
Epoch 5/80: current_loss=8.76095 | best_loss=8.76095
Epoch 6/80: current_loss=8.69918 | best_loss=8.69918
Epoch 7/80: current_loss=8.82391 | best_loss=8.69918
Epoch 8/80: current_loss=8.78336 | best_loss=8.69918
Epoch 9/80: current_loss=8.79224 | best_loss=8.69918
Epoch 10/80: current_loss=8.84519 | best_loss=8.69918
Epoch 11/80: current_loss=8.81238 | best_loss=8.69918
Epoch 12/80: current_loss=8.84416 | best_loss=8.69918
Epoch 13/80: current_loss=8.73501 | best_loss=8.69918
Epoch 14/80: current_loss=8.76101 | best_loss=8.69918
Epoch 15/80: current_loss=8.93048 | best_loss=8.69918
Epoch 16/80: current_loss=8.97592 | best_loss=8.69918
Epoch 17/80: current_loss=8.75570 | best_loss=8.69918
Epoch 18/80: current_loss=8.77016 | best_loss=8.69918
Epoch 19/80: current_loss=8.74304 | best_loss=8.69918
Epoch 20/80: current_loss=8.79080 | best_loss=8.69918
Epoch 21/80: current_loss=8.76036 | best_loss=8.69918
Epoch 22/80: current_loss=8.77953 | best_loss=8.69918
Epoch 23/80: current_loss=8.82988 | best_loss=8.69918
Epoch 24/80: current_loss=8.79845 | best_loss=8.69918
Epoch 25/80: current_loss=8.79635 | best_loss=8.69918
Epoch 26/80: current_loss=8.71337 | best_loss=8.69918
Early Stopping at epoch 26
      explained_var=0.00747 | mse_loss=8.65110
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=9.27278 | best_loss=9.27278
Epoch 1/80: current_loss=9.13430 | best_loss=9.13430
Epoch 2/80: current_loss=9.17095 | best_loss=9.13430
Epoch 3/80: current_loss=9.16763 | best_loss=9.13430
Epoch 4/80: current_loss=9.19202 | best_loss=9.13430
Epoch 5/80: current_loss=9.19022 | best_loss=9.13430
Epoch 6/80: current_loss=9.61111 | best_loss=9.13430
Epoch 7/80: current_loss=9.23551 | best_loss=9.13430
Epoch 8/80: current_loss=9.37265 | best_loss=9.13430
Epoch 9/80: current_loss=9.27021 | best_loss=9.13430
Epoch 10/80: current_loss=9.15508 | best_loss=9.13430
Epoch 11/80: current_loss=9.30092 | best_loss=9.13430
Epoch 12/80: current_loss=9.27905 | best_loss=9.13430
Epoch 13/80: current_loss=9.18107 | best_loss=9.13430
Epoch 14/80: current_loss=9.17817 | best_loss=9.13430
Epoch 15/80: current_loss=9.23913 | best_loss=9.13430
Epoch 16/80: current_loss=9.26023 | best_loss=9.13430
Epoch 17/80: current_loss=9.15318 | best_loss=9.13430
Epoch 18/80: current_loss=9.25492 | best_loss=9.13430
Epoch 19/80: current_loss=9.18897 | best_loss=9.13430
Epoch 20/80: current_loss=9.15558 | best_loss=9.13430
Epoch 21/80: current_loss=9.18740 | best_loss=9.13430
Early Stopping at epoch 21
      explained_var=0.02210 | mse_loss=9.09027
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.01055| avg_loss=9.14366
----------------------------------------------


----------------------------------------------
Params for Trial 94
{'learning_rate': 0.001, 'weight_decay': 0.00482697967311886, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.62282 | best_loss=17.62282
Epoch 1/80: current_loss=10.80191 | best_loss=10.80191
Epoch 2/80: current_loss=10.39579 | best_loss=10.39579
Epoch 3/80: current_loss=10.34739 | best_loss=10.34739
Epoch 4/80: current_loss=10.36382 | best_loss=10.34739
Epoch 5/80: current_loss=10.33269 | best_loss=10.33269
Epoch 6/80: current_loss=10.30324 | best_loss=10.30324
Epoch 7/80: current_loss=10.31259 | best_loss=10.30324
Epoch 8/80: current_loss=10.31661 | best_loss=10.30324
Epoch 9/80: current_loss=10.31391 | best_loss=10.30324
Epoch 10/80: current_loss=10.30270 | best_loss=10.30270
Epoch 11/80: current_loss=10.29212 | best_loss=10.29212
Epoch 12/80: current_loss=10.28977 | best_loss=10.28977
Epoch 13/80: current_loss=10.31328 | best_loss=10.28977
Epoch 14/80: current_loss=10.28798 | best_loss=10.28798
Epoch 15/80: current_loss=10.28352 | best_loss=10.28352
Epoch 16/80: current_loss=10.29003 | best_loss=10.28352
Epoch 17/80: current_loss=10.30403 | best_loss=10.28352
Epoch 18/80: current_loss=10.27141 | best_loss=10.27141
Epoch 19/80: current_loss=10.27982 | best_loss=10.27141
Epoch 20/80: current_loss=10.27820 | best_loss=10.27141
Epoch 21/80: current_loss=10.28643 | best_loss=10.27141
Epoch 22/80: current_loss=10.32162 | best_loss=10.27141
Epoch 23/80: current_loss=10.29971 | best_loss=10.27141
Epoch 24/80: current_loss=10.27896 | best_loss=10.27141
Epoch 25/80: current_loss=10.28065 | best_loss=10.27141
Epoch 26/80: current_loss=10.29648 | best_loss=10.27141
Epoch 27/80: current_loss=10.29451 | best_loss=10.27141
Epoch 28/80: current_loss=10.28382 | best_loss=10.27141
Epoch 29/80: current_loss=10.27854 | best_loss=10.27141
Epoch 30/80: current_loss=10.32195 | best_loss=10.27141
Epoch 31/80: current_loss=10.28411 | best_loss=10.27141
Epoch 32/80: current_loss=10.29891 | best_loss=10.27141
Epoch 33/80: current_loss=10.30617 | best_loss=10.27141
Epoch 34/80: current_loss=10.31147 | best_loss=10.27141
Epoch 35/80: current_loss=10.30786 | best_loss=10.27141
Epoch 36/80: current_loss=10.28877 | best_loss=10.27141
Epoch 37/80: current_loss=10.26792 | best_loss=10.26792
Epoch 38/80: current_loss=10.28356 | best_loss=10.26792
Epoch 39/80: current_loss=10.27984 | best_loss=10.26792
Epoch 40/80: current_loss=10.29977 | best_loss=10.26792
Epoch 41/80: current_loss=10.28768 | best_loss=10.26792
Epoch 42/80: current_loss=10.32603 | best_loss=10.26792
Epoch 43/80: current_loss=10.32648 | best_loss=10.26792
Epoch 44/80: current_loss=10.30100 | best_loss=10.26792
Epoch 45/80: current_loss=10.31813 | best_loss=10.26792
Epoch 46/80: current_loss=10.29737 | best_loss=10.26792
Epoch 47/80: current_loss=10.27015 | best_loss=10.26792
Epoch 48/80: current_loss=10.28843 | best_loss=10.26792
Epoch 49/80: current_loss=10.28053 | best_loss=10.26792
Epoch 50/80: current_loss=10.26954 | best_loss=10.26792
Epoch 51/80: current_loss=10.32668 | best_loss=10.26792
Epoch 52/80: current_loss=10.27693 | best_loss=10.26792
Epoch 53/80: current_loss=10.28931 | best_loss=10.26792
Epoch 54/80: current_loss=10.30521 | best_loss=10.26792
Epoch 55/80: current_loss=10.34608 | best_loss=10.26792
Epoch 56/80: current_loss=10.27771 | best_loss=10.26792
Epoch 57/80: current_loss=10.30773 | best_loss=10.26792
Early Stopping at epoch 57
      explained_var=-0.00193 | mse_loss=10.49358
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.44574 | best_loss=10.44574
Epoch 1/80: current_loss=10.48382 | best_loss=10.44574
Epoch 2/80: current_loss=10.41417 | best_loss=10.41417
Epoch 3/80: current_loss=10.40890 | best_loss=10.40890
Epoch 4/80: current_loss=10.43696 | best_loss=10.40890
Epoch 5/80: current_loss=10.40581 | best_loss=10.40581
Epoch 6/80: current_loss=10.42723 | best_loss=10.40581
Epoch 7/80: current_loss=10.40909 | best_loss=10.40581
Epoch 8/80: current_loss=10.44996 | best_loss=10.40581
Epoch 9/80: current_loss=10.40549 | best_loss=10.40549
Epoch 10/80: current_loss=10.45951 | best_loss=10.40549
Epoch 11/80: current_loss=10.41072 | best_loss=10.40549
Epoch 12/80: current_loss=10.50313 | best_loss=10.40549
Epoch 13/80: current_loss=10.40636 | best_loss=10.40549
Epoch 14/80: current_loss=10.40737 | best_loss=10.40549
Epoch 15/80: current_loss=10.42382 | best_loss=10.40549
Epoch 16/80: current_loss=10.41330 | best_loss=10.40549
Epoch 17/80: current_loss=10.40679 | best_loss=10.40549
Epoch 18/80: current_loss=10.39378 | best_loss=10.39378
Epoch 19/80: current_loss=10.40144 | best_loss=10.39378
Epoch 20/80: current_loss=10.41057 | best_loss=10.39378
Epoch 21/80: current_loss=10.41971 | best_loss=10.39378
Epoch 22/80: current_loss=10.41846 | best_loss=10.39378
Epoch 23/80: current_loss=10.40706 | best_loss=10.39378
Epoch 24/80: current_loss=10.41196 | best_loss=10.39378
Epoch 25/80: current_loss=10.42230 | best_loss=10.39378
Epoch 26/80: current_loss=10.43805 | best_loss=10.39378
Epoch 27/80: current_loss=10.42006 | best_loss=10.39378
Epoch 28/80: current_loss=10.45721 | best_loss=10.39378
Epoch 29/80: current_loss=10.41052 | best_loss=10.39378
Epoch 30/80: current_loss=10.44868 | best_loss=10.39378
Epoch 31/80: current_loss=10.40630 | best_loss=10.39378
Epoch 32/80: current_loss=10.43470 | best_loss=10.39378
Epoch 33/80: current_loss=10.40500 | best_loss=10.39378
Epoch 34/80: current_loss=10.39912 | best_loss=10.39378
Epoch 35/80: current_loss=10.41038 | best_loss=10.39378
Epoch 36/80: current_loss=10.44649 | best_loss=10.39378
Epoch 37/80: current_loss=10.42502 | best_loss=10.39378
Epoch 38/80: current_loss=10.42542 | best_loss=10.39378
Early Stopping at epoch 38
      explained_var=0.00224 | mse_loss=10.04622

----------------------------------------------
Params for Trial 95
{'learning_rate': 0.001, 'weight_decay': 0.005404211484023139, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=15.01409 | best_loss=15.01409
Epoch 1/80: current_loss=10.63864 | best_loss=10.63864
Epoch 2/80: current_loss=10.44455 | best_loss=10.44455
Epoch 3/80: current_loss=10.36882 | best_loss=10.36882
Epoch 4/80: current_loss=10.35503 | best_loss=10.35503
Epoch 5/80: current_loss=10.31540 | best_loss=10.31540
Epoch 6/80: current_loss=10.32170 | best_loss=10.31540
Epoch 7/80: current_loss=10.32833 | best_loss=10.31540
Epoch 8/80: current_loss=10.29839 | best_loss=10.29839
Epoch 9/80: current_loss=10.29283 | best_loss=10.29283
Epoch 10/80: current_loss=10.29980 | best_loss=10.29283
Epoch 11/80: current_loss=10.28311 | best_loss=10.28311
Epoch 12/80: current_loss=10.28304 | best_loss=10.28304
Epoch 13/80: current_loss=10.29705 | best_loss=10.28304
Epoch 14/80: current_loss=10.28333 | best_loss=10.28304
Epoch 15/80: current_loss=10.28784 | best_loss=10.28304
Epoch 16/80: current_loss=10.27850 | best_loss=10.27850
Epoch 17/80: current_loss=10.27634 | best_loss=10.27634
Epoch 18/80: current_loss=10.28995 | best_loss=10.27634
Epoch 19/80: current_loss=10.28437 | best_loss=10.27634
Epoch 20/80: current_loss=10.27616 | best_loss=10.27616
Epoch 21/80: current_loss=10.31435 | best_loss=10.27616
Epoch 22/80: current_loss=10.27699 | best_loss=10.27616
Epoch 23/80: current_loss=10.30463 | best_loss=10.27616
Epoch 24/80: current_loss=10.27683 | best_loss=10.27616
Epoch 25/80: current_loss=10.27388 | best_loss=10.27388
Epoch 26/80: current_loss=10.27033 | best_loss=10.27033
Epoch 27/80: current_loss=10.27322 | best_loss=10.27033
Epoch 28/80: current_loss=10.28405 | best_loss=10.27033
Epoch 29/80: current_loss=10.28973 | best_loss=10.27033
Epoch 30/80: current_loss=10.27347 | best_loss=10.27033
Epoch 31/80: current_loss=10.28104 | best_loss=10.27033
Epoch 32/80: current_loss=10.28743 | best_loss=10.27033
Epoch 33/80: current_loss=10.28696 | best_loss=10.27033
Epoch 34/80: current_loss=10.28229 | best_loss=10.27033
Epoch 35/80: current_loss=10.27931 | best_loss=10.27033
Epoch 36/80: current_loss=10.28650 | best_loss=10.27033
Epoch 37/80: current_loss=10.28106 | best_loss=10.27033
Epoch 38/80: current_loss=10.30833 | best_loss=10.27033
Epoch 39/80: current_loss=10.28949 | best_loss=10.27033
Epoch 40/80: current_loss=10.30824 | best_loss=10.27033
Epoch 41/80: current_loss=10.27500 | best_loss=10.27033
Epoch 42/80: current_loss=10.27264 | best_loss=10.27033
Epoch 43/80: current_loss=10.31558 | best_loss=10.27033
Epoch 44/80: current_loss=10.28809 | best_loss=10.27033
Epoch 45/80: current_loss=10.26924 | best_loss=10.26924
Epoch 46/80: current_loss=10.28452 | best_loss=10.26924
Epoch 47/80: current_loss=10.29815 | best_loss=10.26924
Epoch 48/80: current_loss=10.27963 | best_loss=10.26924
Epoch 49/80: current_loss=10.37459 | best_loss=10.26924
Epoch 50/80: current_loss=10.26555 | best_loss=10.26555
Epoch 51/80: current_loss=10.31042 | best_loss=10.26555
Epoch 52/80: current_loss=10.28865 | best_loss=10.26555
Epoch 53/80: current_loss=10.37478 | best_loss=10.26555
Epoch 54/80: current_loss=10.30477 | best_loss=10.26555
Epoch 55/80: current_loss=10.30727 | best_loss=10.26555
Epoch 56/80: current_loss=10.27102 | best_loss=10.26555
Epoch 57/80: current_loss=10.27685 | best_loss=10.26555
Epoch 58/80: current_loss=10.27569 | best_loss=10.26555
Epoch 59/80: current_loss=10.30350 | best_loss=10.26555
Epoch 60/80: current_loss=10.27647 | best_loss=10.26555
Epoch 61/80: current_loss=10.30973 | best_loss=10.26555
Epoch 62/80: current_loss=10.27469 | best_loss=10.26555
Epoch 63/80: current_loss=10.27085 | best_loss=10.26555
Epoch 64/80: current_loss=10.27599 | best_loss=10.26555
Epoch 65/80: current_loss=10.25895 | best_loss=10.25895
Epoch 66/80: current_loss=10.31478 | best_loss=10.25895
Epoch 67/80: current_loss=10.31425 | best_loss=10.25895
Epoch 68/80: current_loss=10.27919 | best_loss=10.25895
Epoch 69/80: current_loss=10.28909 | best_loss=10.25895
Epoch 70/80: current_loss=10.28796 | best_loss=10.25895
Epoch 71/80: current_loss=10.25241 | best_loss=10.25241
Epoch 72/80: current_loss=10.18898 | best_loss=10.18898
Epoch 73/80: current_loss=10.24897 | best_loss=10.18898
Epoch 74/80: current_loss=10.02497 | best_loss=10.02497
Epoch 75/80: current_loss=10.16908 | best_loss=10.02497
Epoch 76/80: current_loss=10.23328 | best_loss=10.02497
Epoch 77/80: current_loss=10.26131 | best_loss=10.02497
Epoch 78/80: current_loss=10.17639 | best_loss=10.02497
Epoch 79/80: current_loss=10.28317 | best_loss=10.02497
      explained_var=0.02280 | mse_loss=10.26158
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.37486 | best_loss=10.37486
Epoch 1/80: current_loss=10.43561 | best_loss=10.37486
Epoch 2/80: current_loss=10.33490 | best_loss=10.33490
Epoch 3/80: current_loss=10.31488 | best_loss=10.31488
Epoch 4/80: current_loss=10.38507 | best_loss=10.31488
Epoch 5/80: current_loss=10.43814 | best_loss=10.31488
Epoch 6/80: current_loss=10.41814 | best_loss=10.31488
Epoch 7/80: current_loss=10.36303 | best_loss=10.31488
Epoch 8/80: current_loss=10.39813 | best_loss=10.31488
Epoch 9/80: current_loss=10.35449 | best_loss=10.31488
Epoch 10/80: current_loss=10.33967 | best_loss=10.31488
Epoch 11/80: current_loss=10.52004 | best_loss=10.31488
Epoch 12/80: current_loss=10.31138 | best_loss=10.31138
Epoch 13/80: current_loss=10.36734 | best_loss=10.31138
Epoch 14/80: current_loss=10.30149 | best_loss=10.30149
Epoch 15/80: current_loss=10.31374 | best_loss=10.30149
Epoch 16/80: current_loss=10.38944 | best_loss=10.30149
Epoch 17/80: current_loss=10.29401 | best_loss=10.29401
Epoch 18/80: current_loss=10.30931 | best_loss=10.29401
Epoch 19/80: current_loss=10.44599 | best_loss=10.29401
Epoch 20/80: current_loss=10.41525 | best_loss=10.29401
Epoch 21/80: current_loss=10.40426 | best_loss=10.29401
Epoch 22/80: current_loss=10.43548 | best_loss=10.29401
Epoch 23/80: current_loss=10.47810 | best_loss=10.29401
Epoch 24/80: current_loss=10.39867 | best_loss=10.29401
Epoch 25/80: current_loss=10.39494 | best_loss=10.29401
Epoch 26/80: current_loss=10.39075 | best_loss=10.29401
Epoch 27/80: current_loss=10.38459 | best_loss=10.29401
Epoch 28/80: current_loss=10.38113 | best_loss=10.29401
Epoch 29/80: current_loss=10.39680 | best_loss=10.29401
Epoch 30/80: current_loss=10.39365 | best_loss=10.29401
Epoch 31/80: current_loss=10.35759 | best_loss=10.29401
Epoch 32/80: current_loss=10.29778 | best_loss=10.29401
Epoch 33/80: current_loss=10.27188 | best_loss=10.27188
Epoch 34/80: current_loss=10.30315 | best_loss=10.27188
Epoch 35/80: current_loss=10.24670 | best_loss=10.24670
Epoch 36/80: current_loss=10.37726 | best_loss=10.24670
Epoch 37/80: current_loss=10.26786 | best_loss=10.24670
Epoch 38/80: current_loss=10.40721 | best_loss=10.24670
Epoch 39/80: current_loss=10.38493 | best_loss=10.24670
Epoch 40/80: current_loss=10.42871 | best_loss=10.24670
Epoch 41/80: current_loss=10.35968 | best_loss=10.24670
Epoch 42/80: current_loss=10.33209 | best_loss=10.24670
Epoch 43/80: current_loss=10.42469 | best_loss=10.24670
Epoch 44/80: current_loss=10.35312 | best_loss=10.24670
Epoch 45/80: current_loss=10.35730 | best_loss=10.24670
Epoch 46/80: current_loss=10.19140 | best_loss=10.19140
Epoch 47/80: current_loss=10.30797 | best_loss=10.19140
Epoch 48/80: current_loss=10.23096 | best_loss=10.19140
Epoch 49/80: current_loss=10.25342 | best_loss=10.19140
Epoch 50/80: current_loss=10.41146 | best_loss=10.19140
Epoch 51/80: current_loss=10.23217 | best_loss=10.19140
Epoch 52/80: current_loss=10.24935 | best_loss=10.19140
Epoch 53/80: current_loss=10.42743 | best_loss=10.19140
Epoch 54/80: current_loss=10.40504 | best_loss=10.19140
Epoch 55/80: current_loss=10.40410 | best_loss=10.19140
Epoch 56/80: current_loss=10.38023 | best_loss=10.19140
Epoch 57/80: current_loss=10.37829 | best_loss=10.19140
Epoch 58/80: current_loss=10.37292 | best_loss=10.19140
Epoch 59/80: current_loss=10.44527 | best_loss=10.19140
Epoch 60/80: current_loss=10.35140 | best_loss=10.19140
Epoch 61/80: current_loss=10.34339 | best_loss=10.19140
Epoch 62/80: current_loss=10.31974 | best_loss=10.19140
Epoch 63/80: current_loss=10.29998 | best_loss=10.19140
Epoch 64/80: current_loss=10.43002 | best_loss=10.19140
Epoch 65/80: current_loss=10.37565 | best_loss=10.19140
Epoch 66/80: current_loss=10.33637 | best_loss=10.19140
Early Stopping at epoch 66
      explained_var=0.02320 | mse_loss=9.84090
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=7.70217 | best_loss=7.70217
Epoch 1/80: current_loss=7.70611 | best_loss=7.70217
Epoch 2/80: current_loss=7.65485 | best_loss=7.65485
Epoch 3/80: current_loss=7.73892 | best_loss=7.65485
Epoch 4/80: current_loss=7.69503 | best_loss=7.65485
Epoch 5/80: current_loss=7.78986 | best_loss=7.65485
Epoch 6/80: current_loss=7.70216 | best_loss=7.65485
Epoch 7/80: current_loss=7.56966 | best_loss=7.56966
Epoch 8/80: current_loss=7.58574 | best_loss=7.56966
Epoch 9/80: current_loss=7.72276 | best_loss=7.56966
Epoch 10/80: current_loss=7.95238 | best_loss=7.56966
Epoch 11/80: current_loss=7.48966 | best_loss=7.48966
Epoch 12/80: current_loss=7.64564 | best_loss=7.48966
Epoch 13/80: current_loss=7.51059 | best_loss=7.48966
Epoch 14/80: current_loss=7.71162 | best_loss=7.48966
Epoch 15/80: current_loss=7.62500 | best_loss=7.48966
Epoch 16/80: current_loss=7.67280 | best_loss=7.48966
Epoch 17/80: current_loss=7.68218 | best_loss=7.48966
Epoch 18/80: current_loss=7.63287 | best_loss=7.48966
Epoch 19/80: current_loss=7.62400 | best_loss=7.48966
Epoch 20/80: current_loss=7.62991 | best_loss=7.48966
Epoch 21/80: current_loss=7.73203 | best_loss=7.48966
Epoch 22/80: current_loss=7.64747 | best_loss=7.48966
Epoch 23/80: current_loss=7.64932 | best_loss=7.48966
Epoch 24/80: current_loss=7.60805 | best_loss=7.48966
Epoch 25/80: current_loss=7.84725 | best_loss=7.48966
Epoch 26/80: current_loss=7.55481 | best_loss=7.48966
Epoch 27/80: current_loss=7.58234 | best_loss=7.48966
Epoch 28/80: current_loss=7.63648 | best_loss=7.48966
Epoch 29/80: current_loss=7.65577 | best_loss=7.48966
Epoch 30/80: current_loss=7.60727 | best_loss=7.48966
Epoch 31/80: current_loss=7.74794 | best_loss=7.48966
Early Stopping at epoch 31
      explained_var=-0.00230 | mse_loss=7.60161

----------------------------------------------
Params for Trial 96
{'learning_rate': 0.1, 'weight_decay': 0.004207455428180043, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=11.15916 | best_loss=11.15916
Epoch 1/80: current_loss=12.17158 | best_loss=11.15916
Epoch 2/80: current_loss=11.07978 | best_loss=11.07978
Epoch 3/80: current_loss=12.33486 | best_loss=11.07978
Epoch 4/80: current_loss=10.53492 | best_loss=10.53492
Epoch 5/80: current_loss=11.24202 | best_loss=10.53492
Epoch 6/80: current_loss=10.44460 | best_loss=10.44460
Epoch 7/80: current_loss=13.11107 | best_loss=10.44460
Epoch 8/80: current_loss=11.19960 | best_loss=10.44460
Epoch 9/80: current_loss=11.08907 | best_loss=10.44460
Epoch 10/80: current_loss=11.60198 | best_loss=10.44460
Epoch 11/80: current_loss=11.40309 | best_loss=10.44460
Epoch 12/80: current_loss=11.24399 | best_loss=10.44460
Epoch 13/80: current_loss=10.31082 | best_loss=10.31082
Epoch 14/80: current_loss=12.73644 | best_loss=10.31082
Epoch 15/80: current_loss=18.85021 | best_loss=10.31082
Epoch 16/80: current_loss=10.49878 | best_loss=10.31082
Epoch 17/80: current_loss=12.14949 | best_loss=10.31082
Epoch 18/80: current_loss=10.43267 | best_loss=10.31082
Epoch 19/80: current_loss=17.45350 | best_loss=10.31082
Epoch 20/80: current_loss=18.24541 | best_loss=10.31082
Epoch 21/80: current_loss=12.45834 | best_loss=10.31082
Epoch 22/80: current_loss=10.67108 | best_loss=10.31082
Epoch 23/80: current_loss=10.56604 | best_loss=10.31082
Epoch 24/80: current_loss=10.49778 | best_loss=10.31082
Epoch 25/80: current_loss=12.78240 | best_loss=10.31082
Epoch 26/80: current_loss=12.14562 | best_loss=10.31082
Epoch 27/80: current_loss=10.86640 | best_loss=10.31082
Epoch 28/80: current_loss=11.15386 | best_loss=10.31082
Epoch 29/80: current_loss=10.42326 | best_loss=10.31082
Epoch 30/80: current_loss=26.41922 | best_loss=10.31082
Epoch 31/80: current_loss=10.28210 | best_loss=10.28210
Epoch 32/80: current_loss=10.56758 | best_loss=10.28210
Epoch 33/80: current_loss=10.56316 | best_loss=10.28210
Epoch 34/80: current_loss=11.21974 | best_loss=10.28210
Epoch 35/80: current_loss=10.88719 | best_loss=10.28210
Epoch 36/80: current_loss=11.60342 | best_loss=10.28210
Epoch 37/80: current_loss=12.50414 | best_loss=10.28210
Epoch 38/80: current_loss=10.29111 | best_loss=10.28210
Epoch 39/80: current_loss=15.38641 | best_loss=10.28210
Epoch 40/80: current_loss=10.98003 | best_loss=10.28210
Epoch 41/80: current_loss=10.83619 | best_loss=10.28210
Epoch 42/80: current_loss=12.43892 | best_loss=10.28210
Epoch 43/80: current_loss=11.03671 | best_loss=10.28210
Epoch 44/80: current_loss=10.78001 | best_loss=10.28210
Epoch 45/80: current_loss=12.17432 | best_loss=10.28210
Epoch 46/80: current_loss=11.58059 | best_loss=10.28210
Epoch 47/80: current_loss=13.01249 | best_loss=10.28210
Epoch 48/80: current_loss=11.17716 | best_loss=10.28210
Epoch 49/80: current_loss=10.53938 | best_loss=10.28210
Epoch 50/80: current_loss=10.94665 | best_loss=10.28210
Epoch 51/80: current_loss=10.34382 | best_loss=10.28210
Early Stopping at epoch 51
      explained_var=-0.00348 | mse_loss=10.50376
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=11.02625 | best_loss=11.02625
Epoch 1/80: current_loss=10.52984 | best_loss=10.52984
Epoch 2/80: current_loss=10.34938 | best_loss=10.34938
Epoch 3/80: current_loss=12.76951 | best_loss=10.34938
Epoch 4/80: current_loss=11.20832 | best_loss=10.34938
Epoch 5/80: current_loss=11.05927 | best_loss=10.34938
Epoch 6/80: current_loss=12.74595 | best_loss=10.34938
Epoch 7/80: current_loss=10.94604 | best_loss=10.34938
Epoch 8/80: current_loss=15.80665 | best_loss=10.34938
Epoch 9/80: current_loss=13.28801 | best_loss=10.34938
Epoch 10/80: current_loss=14.51104 | best_loss=10.34938
Epoch 11/80: current_loss=10.98059 | best_loss=10.34938
Epoch 12/80: current_loss=12.94477 | best_loss=10.34938
Epoch 13/80: current_loss=11.64675 | best_loss=10.34938
Epoch 14/80: current_loss=11.75256 | best_loss=10.34938
Epoch 15/80: current_loss=12.78404 | best_loss=10.34938
Epoch 16/80: current_loss=13.85771 | best_loss=10.34938
Epoch 17/80: current_loss=14.80089 | best_loss=10.34938
Epoch 18/80: current_loss=12.72526 | best_loss=10.34938
Epoch 19/80: current_loss=13.07257 | best_loss=10.34938
Epoch 20/80: current_loss=14.56921 | best_loss=10.34938
Epoch 21/80: current_loss=11.70501 | best_loss=10.34938
Epoch 22/80: current_loss=13.03254 | best_loss=10.34938
Early Stopping at epoch 22
      explained_var=0.01375 | mse_loss=9.96719
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=12.02463 | best_loss=12.02463
Epoch 1/80: current_loss=9.94070 | best_loss=9.94070
Epoch 2/80: current_loss=12.27540 | best_loss=9.94070
Epoch 3/80: current_loss=10.61347 | best_loss=9.94070
Epoch 4/80: current_loss=8.40350 | best_loss=8.40350
Epoch 5/80: current_loss=7.74756 | best_loss=7.74756
Epoch 6/80: current_loss=8.41015 | best_loss=7.74756
Epoch 7/80: current_loss=8.29029 | best_loss=7.74756
Epoch 8/80: current_loss=10.37295 | best_loss=7.74756
Epoch 9/80: current_loss=8.93360 | best_loss=7.74756
Epoch 10/80: current_loss=11.29031 | best_loss=7.74756
Epoch 11/80: current_loss=7.87505 | best_loss=7.74756
Epoch 12/80: current_loss=9.84140 | best_loss=7.74756
Epoch 13/80: current_loss=8.97176 | best_loss=7.74756
Epoch 14/80: current_loss=9.80288 | best_loss=7.74756
Epoch 15/80: current_loss=9.26605 | best_loss=7.74756
Epoch 16/80: current_loss=8.27724 | best_loss=7.74756
Epoch 17/80: current_loss=10.40618 | best_loss=7.74756
Epoch 18/80: current_loss=7.63095 | best_loss=7.63095
Epoch 19/80: current_loss=8.86126 | best_loss=7.63095
Epoch 20/80: current_loss=7.51959 | best_loss=7.51959
Epoch 21/80: current_loss=7.98173 | best_loss=7.51959
Epoch 22/80: current_loss=17.14509 | best_loss=7.51959
Epoch 23/80: current_loss=9.31433 | best_loss=7.51959
Epoch 24/80: current_loss=7.60423 | best_loss=7.51959
Epoch 25/80: current_loss=9.85489 | best_loss=7.51959
Epoch 26/80: current_loss=10.20787 | best_loss=7.51959
Epoch 27/80: current_loss=7.51894 | best_loss=7.51894
Epoch 28/80: current_loss=7.51225 | best_loss=7.51225
Epoch 29/80: current_loss=7.91677 | best_loss=7.51225
Epoch 30/80: current_loss=7.66514 | best_loss=7.51225
Epoch 31/80: current_loss=7.61113 | best_loss=7.51225
Epoch 32/80: current_loss=7.67419 | best_loss=7.51225
Epoch 33/80: current_loss=8.83673 | best_loss=7.51225
Epoch 34/80: current_loss=8.88135 | best_loss=7.51225
Epoch 35/80: current_loss=7.60670 | best_loss=7.51225
Epoch 36/80: current_loss=7.63208 | best_loss=7.51225
Epoch 37/80: current_loss=7.62143 | best_loss=7.51225
Epoch 38/80: current_loss=9.51946 | best_loss=7.51225
Epoch 39/80: current_loss=8.28536 | best_loss=7.51225
Epoch 40/80: current_loss=7.90973 | best_loss=7.51225
Epoch 41/80: current_loss=10.36964 | best_loss=7.51225
Epoch 42/80: current_loss=11.41070 | best_loss=7.51225
Epoch 43/80: current_loss=9.33871 | best_loss=7.51225
Epoch 44/80: current_loss=7.67373 | best_loss=7.51225
Epoch 45/80: current_loss=10.98853 | best_loss=7.51225
Epoch 46/80: current_loss=8.92669 | best_loss=7.51225
Epoch 47/80: current_loss=7.61875 | best_loss=7.51225
Epoch 48/80: current_loss=8.69084 | best_loss=7.51225
Early Stopping at epoch 48
      explained_var=0.01942 | mse_loss=7.60569

----------------------------------------------
Params for Trial 97
{'learning_rate': 0.001, 'weight_decay': 0.00456274768792015, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=17.11561 | best_loss=17.11561
Epoch 1/80: current_loss=10.67821 | best_loss=10.67821
Epoch 2/80: current_loss=10.41356 | best_loss=10.41356
Epoch 3/80: current_loss=10.37077 | best_loss=10.37077
Epoch 4/80: current_loss=10.33446 | best_loss=10.33446
Epoch 5/80: current_loss=10.30099 | best_loss=10.30099
Epoch 6/80: current_loss=10.31077 | best_loss=10.30099
Epoch 7/80: current_loss=10.29440 | best_loss=10.29440
Epoch 8/80: current_loss=10.30807 | best_loss=10.29440
Epoch 9/80: current_loss=10.32570 | best_loss=10.29440
Epoch 10/80: current_loss=10.32622 | best_loss=10.29440
Epoch 11/80: current_loss=10.28965 | best_loss=10.28965
Epoch 12/80: current_loss=10.29773 | best_loss=10.28965
Epoch 13/80: current_loss=10.31051 | best_loss=10.28965
Epoch 14/80: current_loss=10.31099 | best_loss=10.28965
Epoch 15/80: current_loss=10.31472 | best_loss=10.28965
Epoch 16/80: current_loss=10.28925 | best_loss=10.28925
Epoch 17/80: current_loss=10.31692 | best_loss=10.28925
Epoch 18/80: current_loss=10.28169 | best_loss=10.28169
Epoch 19/80: current_loss=10.29915 | best_loss=10.28169
Epoch 20/80: current_loss=10.27711 | best_loss=10.27711
Epoch 21/80: current_loss=10.30077 | best_loss=10.27711
Epoch 22/80: current_loss=10.27610 | best_loss=10.27610
Epoch 23/80: current_loss=10.29538 | best_loss=10.27610
Epoch 24/80: current_loss=10.28577 | best_loss=10.27610
Epoch 25/80: current_loss=10.26355 | best_loss=10.26355
Epoch 26/80: current_loss=10.28229 | best_loss=10.26355
Epoch 27/80: current_loss=10.30746 | best_loss=10.26355
Epoch 28/80: current_loss=10.29413 | best_loss=10.26355
Epoch 29/80: current_loss=10.28127 | best_loss=10.26355
Epoch 30/80: current_loss=10.27787 | best_loss=10.26355
Epoch 31/80: current_loss=10.30107 | best_loss=10.26355
Epoch 32/80: current_loss=10.28039 | best_loss=10.26355
Epoch 33/80: current_loss=10.27771 | best_loss=10.26355
Epoch 34/80: current_loss=10.30330 | best_loss=10.26355
Epoch 35/80: current_loss=10.27016 | best_loss=10.26355
Epoch 36/80: current_loss=10.27204 | best_loss=10.26355
Epoch 37/80: current_loss=10.29614 | best_loss=10.26355
Epoch 38/80: current_loss=10.31544 | best_loss=10.26355
Epoch 39/80: current_loss=10.26889 | best_loss=10.26355
Epoch 40/80: current_loss=10.29553 | best_loss=10.26355
Epoch 41/80: current_loss=10.28902 | best_loss=10.26355
Epoch 42/80: current_loss=10.27763 | best_loss=10.26355
Epoch 43/80: current_loss=10.27800 | best_loss=10.26355
Epoch 44/80: current_loss=10.26207 | best_loss=10.26207
Epoch 45/80: current_loss=10.30570 | best_loss=10.26207
Epoch 46/80: current_loss=10.31095 | best_loss=10.26207
Epoch 47/80: current_loss=10.28280 | best_loss=10.26207
Epoch 48/80: current_loss=10.28252 | best_loss=10.26207
Epoch 49/80: current_loss=10.28585 | best_loss=10.26207
Epoch 50/80: current_loss=10.27081 | best_loss=10.26207
Epoch 51/80: current_loss=10.26268 | best_loss=10.26207
Epoch 52/80: current_loss=10.30433 | best_loss=10.26207
Epoch 53/80: current_loss=10.26371 | best_loss=10.26207
Epoch 54/80: current_loss=10.29975 | best_loss=10.26207
Epoch 55/80: current_loss=10.27233 | best_loss=10.26207
Epoch 56/80: current_loss=10.27992 | best_loss=10.26207
Epoch 57/80: current_loss=10.29557 | best_loss=10.26207
Epoch 58/80: current_loss=10.29111 | best_loss=10.26207
Epoch 59/80: current_loss=10.31689 | best_loss=10.26207
Epoch 60/80: current_loss=10.26836 | best_loss=10.26207
Epoch 61/80: current_loss=10.26963 | best_loss=10.26207
Epoch 62/80: current_loss=10.35921 | best_loss=10.26207
Epoch 63/80: current_loss=10.26744 | best_loss=10.26207
Epoch 64/80: current_loss=10.27513 | best_loss=10.26207
Early Stopping at epoch 64
      explained_var=-0.00178 | mse_loss=10.47886
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.41890 | best_loss=10.41890
Epoch 1/80: current_loss=10.40253 | best_loss=10.40253
Epoch 2/80: current_loss=10.41072 | best_loss=10.40253
Epoch 3/80: current_loss=10.41219 | best_loss=10.40253
Epoch 4/80: current_loss=10.40513 | best_loss=10.40253
Epoch 5/80: current_loss=10.42930 | best_loss=10.40253
Epoch 6/80: current_loss=10.43634 | best_loss=10.40253
Epoch 7/80: current_loss=10.41173 | best_loss=10.40253
Epoch 8/80: current_loss=10.45925 | best_loss=10.40253
Epoch 9/80: current_loss=10.40141 | best_loss=10.40141
Epoch 10/80: current_loss=10.47001 | best_loss=10.40141
Epoch 11/80: current_loss=10.40408 | best_loss=10.40141
Epoch 12/80: current_loss=10.42456 | best_loss=10.40141
Epoch 13/80: current_loss=10.40287 | best_loss=10.40141
Epoch 14/80: current_loss=10.64000 | best_loss=10.40141
Epoch 15/80: current_loss=10.40473 | best_loss=10.40141
Epoch 16/80: current_loss=10.41416 | best_loss=10.40141
Epoch 17/80: current_loss=10.47718 | best_loss=10.40141
Epoch 18/80: current_loss=10.42267 | best_loss=10.40141
Epoch 19/80: current_loss=10.40618 | best_loss=10.40141
Epoch 20/80: current_loss=10.43855 | best_loss=10.40141
Epoch 21/80: current_loss=10.41118 | best_loss=10.40141
Epoch 22/80: current_loss=10.40854 | best_loss=10.40141
Epoch 23/80: current_loss=10.45937 | best_loss=10.40141
Epoch 24/80: current_loss=10.45755 | best_loss=10.40141
Epoch 25/80: current_loss=10.40380 | best_loss=10.40141
Epoch 26/80: current_loss=10.43535 | best_loss=10.40141
Epoch 27/80: current_loss=10.40765 | best_loss=10.40141
Epoch 28/80: current_loss=10.44902 | best_loss=10.40141
Epoch 29/80: current_loss=10.48030 | best_loss=10.40141
Early Stopping at epoch 29
      explained_var=0.00056 | mse_loss=10.05887

----------------------------------------------
Params for Trial 98
{'learning_rate': 0.01, 'weight_decay': 0.0051535683972798625, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.71988 | best_loss=10.71988
Epoch 1/80: current_loss=10.31356 | best_loss=10.31356
Epoch 2/80: current_loss=10.37170 | best_loss=10.31356
Epoch 3/80: current_loss=10.33148 | best_loss=10.31356
Epoch 4/80: current_loss=10.24927 | best_loss=10.24927
Epoch 5/80: current_loss=10.24971 | best_loss=10.24927
Epoch 6/80: current_loss=10.31981 | best_loss=10.24927
Epoch 7/80: current_loss=10.51243 | best_loss=10.24927
Epoch 8/80: current_loss=10.28121 | best_loss=10.24927
Epoch 9/80: current_loss=10.44441 | best_loss=10.24927
Epoch 10/80: current_loss=10.65216 | best_loss=10.24927
Epoch 11/80: current_loss=10.34364 | best_loss=10.24927
Epoch 12/80: current_loss=10.91773 | best_loss=10.24927
Epoch 13/80: current_loss=10.60750 | best_loss=10.24927
Epoch 14/80: current_loss=10.30766 | best_loss=10.24927
Epoch 15/80: current_loss=10.62633 | best_loss=10.24927
Epoch 16/80: current_loss=10.26450 | best_loss=10.24927
Epoch 17/80: current_loss=10.25424 | best_loss=10.24927
Epoch 18/80: current_loss=10.74758 | best_loss=10.24927
Epoch 19/80: current_loss=10.39157 | best_loss=10.24927
Epoch 20/80: current_loss=10.25134 | best_loss=10.24927
Epoch 21/80: current_loss=10.35485 | best_loss=10.24927
Epoch 22/80: current_loss=10.31066 | best_loss=10.24927
Epoch 23/80: current_loss=10.25942 | best_loss=10.24927
Epoch 24/80: current_loss=10.31701 | best_loss=10.24927
Early Stopping at epoch 24
      explained_var=0.00000 | mse_loss=10.45722
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=10.50166 | best_loss=10.50166
Epoch 1/80: current_loss=10.58772 | best_loss=10.50166
Epoch 2/80: current_loss=10.41168 | best_loss=10.41168
Epoch 3/80: current_loss=10.43556 | best_loss=10.41168
Epoch 4/80: current_loss=10.67945 | best_loss=10.41168
Epoch 5/80: current_loss=10.48295 | best_loss=10.41168
Epoch 6/80: current_loss=10.99225 | best_loss=10.41168
Epoch 7/80: current_loss=10.52384 | best_loss=10.41168
Epoch 8/80: current_loss=10.76526 | best_loss=10.41168
Epoch 9/80: current_loss=10.59126 | best_loss=10.41168
Epoch 10/80: current_loss=10.44792 | best_loss=10.41168
Epoch 11/80: current_loss=10.51643 | best_loss=10.41168
Epoch 12/80: current_loss=11.10105 | best_loss=10.41168
Epoch 13/80: current_loss=10.74305 | best_loss=10.41168
Epoch 14/80: current_loss=10.41073 | best_loss=10.41073
Epoch 15/80: current_loss=10.41722 | best_loss=10.41073
Epoch 16/80: current_loss=10.41093 | best_loss=10.41073
Epoch 17/80: current_loss=10.72507 | best_loss=10.41073
Epoch 18/80: current_loss=10.55611 | best_loss=10.41073
Epoch 19/80: current_loss=10.92575 | best_loss=10.41073
Epoch 20/80: current_loss=10.44738 | best_loss=10.41073
Epoch 21/80: current_loss=10.54344 | best_loss=10.41073
Epoch 22/80: current_loss=10.41147 | best_loss=10.41073
Epoch 23/80: current_loss=10.52456 | best_loss=10.41073
Epoch 24/80: current_loss=10.46925 | best_loss=10.41073
Epoch 25/80: current_loss=11.22395 | best_loss=10.41073
Epoch 26/80: current_loss=10.65164 | best_loss=10.41073
Epoch 27/80: current_loss=11.21649 | best_loss=10.41073
Epoch 28/80: current_loss=10.62812 | best_loss=10.41073
Epoch 29/80: current_loss=11.12377 | best_loss=10.41073
Epoch 30/80: current_loss=10.44402 | best_loss=10.41073
Epoch 31/80: current_loss=11.19842 | best_loss=10.41073
Epoch 32/80: current_loss=10.71117 | best_loss=10.41073
Epoch 33/80: current_loss=10.98632 | best_loss=10.41073
Epoch 34/80: current_loss=10.62975 | best_loss=10.41073
Early Stopping at epoch 34
      explained_var=0.00005 | mse_loss=10.07288

----------------------------------------------
Params for Trial 99
{'learning_rate': 0.001, 'weight_decay': 0.005466797701729961, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=73.41746 | best_loss=73.41746
Epoch 1/80: current_loss=50.67361 | best_loss=50.67361
Epoch 2/80: current_loss=39.61234 | best_loss=39.61234
Epoch 3/80: current_loss=31.79456 | best_loss=31.79456
Epoch 4/80: current_loss=26.05212 | best_loss=26.05212
Epoch 5/80: current_loss=21.63283 | best_loss=21.63283
Epoch 6/80: current_loss=18.37554 | best_loss=18.37554
Epoch 7/80: current_loss=15.96165 | best_loss=15.96165
Epoch 8/80: current_loss=14.17704 | best_loss=14.17704
Epoch 9/80: current_loss=12.93777 | best_loss=12.93777
Epoch 10/80: current_loss=12.09750 | best_loss=12.09750
Epoch 11/80: current_loss=11.51528 | best_loss=11.51528
Epoch 12/80: current_loss=11.15169 | best_loss=11.15169
Epoch 13/80: current_loss=10.88338 | best_loss=10.88338
Epoch 14/80: current_loss=10.72069 | best_loss=10.72069
Epoch 15/80: current_loss=10.58943 | best_loss=10.58943
Epoch 16/80: current_loss=10.51026 | best_loss=10.51026
Epoch 17/80: current_loss=10.44693 | best_loss=10.44693
Epoch 18/80: current_loss=10.42729 | best_loss=10.42729
Epoch 19/80: current_loss=10.38986 | best_loss=10.38986
Epoch 20/80: current_loss=10.37285 | best_loss=10.37285
Epoch 21/80: current_loss=10.36338 | best_loss=10.36338
Epoch 22/80: current_loss=10.35770 | best_loss=10.35770
Epoch 23/80: current_loss=10.34837 | best_loss=10.34837
Epoch 24/80: current_loss=10.34100 | best_loss=10.34100
Epoch 25/80: current_loss=10.33573 | best_loss=10.33573
Epoch 26/80: current_loss=10.33472 | best_loss=10.33472
Epoch 27/80: current_loss=10.33075 | best_loss=10.33075
Epoch 28/80: current_loss=10.31820 | best_loss=10.31820
Epoch 29/80: current_loss=10.31746 | best_loss=10.31746
Epoch 30/80: current_loss=10.32674 | best_loss=10.31746
Epoch 31/80: current_loss=10.31905 | best_loss=10.31746
Epoch 32/80: current_loss=10.31605 | best_loss=10.31605
Epoch 33/80: current_loss=10.31222 | best_loss=10.31222
Epoch 34/80: current_loss=10.30690 | best_loss=10.30690
Epoch 35/80: current_loss=10.30688 | best_loss=10.30688
Epoch 36/80: current_loss=10.30628 | best_loss=10.30628
Epoch 37/80: current_loss=10.30688 | best_loss=10.30628
Epoch 38/80: current_loss=10.31353 | best_loss=10.30628
Epoch 39/80: current_loss=10.30757 | best_loss=10.30628
Epoch 40/80: current_loss=10.31093 | best_loss=10.30628
Epoch 41/80: current_loss=10.31393 | best_loss=10.30628
Epoch 42/80: current_loss=10.30653 | best_loss=10.30628
Epoch 43/80: current_loss=10.30104 | best_loss=10.30104
Epoch 44/80: current_loss=10.29743 | best_loss=10.29743
Epoch 45/80: current_loss=10.31305 | best_loss=10.29743
Epoch 46/80: current_loss=10.31192 | best_loss=10.29743
Epoch 47/80: current_loss=10.29900 | best_loss=10.29743
Epoch 48/80: current_loss=10.30386 | best_loss=10.29743
Epoch 49/80: current_loss=10.29918 | best_loss=10.29743
Epoch 50/80: current_loss=10.29816 | best_loss=10.29743
Epoch 51/80: current_loss=10.29865 | best_loss=10.29743
Epoch 52/80: current_loss=10.29516 | best_loss=10.29516
Epoch 53/80: current_loss=10.30629 | best_loss=10.29516
Epoch 54/80: current_loss=10.29868 | best_loss=10.29516
Epoch 55/80: current_loss=10.29738 | best_loss=10.29516
Epoch 56/80: current_loss=10.29651 | best_loss=10.29516
Epoch 57/80: current_loss=10.29889 | best_loss=10.29516
Epoch 58/80: current_loss=10.30217 | best_loss=10.29516
Epoch 59/80: current_loss=10.29277 | best_loss=10.29277
Epoch 60/80: current_loss=10.30088 | best_loss=10.29277
Epoch 61/80: current_loss=10.29559 | best_loss=10.29277
Epoch 62/80: current_loss=10.29610 | best_loss=10.29277
Epoch 63/80: current_loss=10.29930 | best_loss=10.29277
Epoch 64/80: current_loss=10.29556 | best_loss=10.29277
Epoch 65/80: current_loss=10.29687 | best_loss=10.29277
Epoch 66/80: current_loss=10.29615 | best_loss=10.29277
Epoch 67/80: current_loss=10.30841 | best_loss=10.29277
Epoch 68/80: current_loss=10.29519 | best_loss=10.29277
Epoch 69/80: current_loss=10.30790 | best_loss=10.29277
Epoch 70/80: current_loss=10.31857 | best_loss=10.29277
Epoch 71/80: current_loss=10.30459 | best_loss=10.29277
Epoch 72/80: current_loss=10.30889 | best_loss=10.29277
Epoch 73/80: current_loss=10.29293 | best_loss=10.29277
Epoch 74/80: current_loss=10.29665 | best_loss=10.29277
Epoch 75/80: current_loss=10.30039 | best_loss=10.29277
Epoch 76/80: current_loss=10.29654 | best_loss=10.29277
Epoch 77/80: current_loss=10.30404 | best_loss=10.29277
Epoch 78/80: current_loss=10.29317 | best_loss=10.29277
Epoch 79/80: current_loss=10.30614 | best_loss=10.29277
Early Stopping at epoch 79
      explained_var=-0.00360 | mse_loss=10.52972
Optuna study finished, study statistics:
  Finished trials:  100
  Pruned trials:  66
  Completed trials:  34
  Best Trial:  11
  Value:  9.1078756667652
  AVG stopping:  26
  Params: 
    learning_rate: 0.001
    weight_decay: 0.009747616659132213
    n_layers: 3
    hidden_size: 128
    dropout: 0.2
----------------------------------------------

Check best params: {'learning_rate': 0.001, 'weight_decay': 0.009747616659132213, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2, 'avg_epochs': 26}
--------------------------------------------------------------
Test CNN results: avg_loss=10.1882, avg_expvar=-0.0065, avg_r2score=-0.0581, avg_mae=2.5295
--------------------------------------------------------------
