[I 2024-01-05 20:55:57,303] A new study created in memory with name: cnn_mseloss_data1
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 20:56:14,763] Trial 0 finished with value: 12.350666507313921 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 12.350666507313921.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 20:57:07,753] Trial 1 finished with value: 24.13483429945642 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}. Best is trial 0 with value: 12.350666507313921.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 20:58:38,745] Trial 2 finished with value: 8.160779759816034 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 8.160779759816034.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 20:58:47,920] Trial 3 finished with value: 8.245233370920484 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 8.160779759816034.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 20:59:05,279] Trial 4 finished with value: 8.142289705440675 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 4 with value: 8.142289705440675.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 20:59:25,839] Trial 5 finished with value: 8.213333633530624 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 4 with value: 8.142289705440675.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:00:11,633] Trial 6 finished with value: 8.271800148801374 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 4 with value: 8.142289705440675.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:00:53,435] Trial 7 finished with value: 8.5544136390665 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 4 with value: 8.142289705440675.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:01:12,156] Trial 8 finished with value: 45.65935456181497 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 4 with value: 8.142289705440675.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:02:19,963] Trial 9 finished with value: 8.354677946901296 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 4 with value: 8.142289705440675.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:02:47,910] Trial 10 finished with value: 8.101348076714846 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.009325991845748673, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 10 with value: 8.101348076714846.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:03:19,892] Trial 11 finished with value: 8.122862931061988 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.009853936657752537, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 10 with value: 8.101348076714846.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:03:50,883] Trial 12 finished with value: 8.131075894948246 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.009685524867852728, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 10 with value: 8.101348076714846.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:04:20,662] Trial 13 finished with value: 8.105433910247445 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.008251881902137582, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.1}. Best is trial 10 with value: 8.101348076714846.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:05:12,528] Trial 14 finished with value: 8.080924892545388 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.007908358003129208, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 14 with value: 8.080924892545388.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:06:06,005] Trial 15 finished with value: 8.05641704192914 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.007041146666322683, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 15 with value: 8.05641704192914.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:07:03,350] Trial 16 finished with value: 8.071700987704064 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.006893789237175351, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 15 with value: 8.05641704192914.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:08:05,110] Trial 17 finished with value: 8.035604889437069 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.003309128484976863, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 17 with value: 8.035604889437069.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:09:18,370] Trial 18 finished with value: 8.077796374983212 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0031521680053919715, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 17 with value: 8.035604889437069.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:10:02,089] Trial 19 finished with value: 8.07962531001394 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0031199700465094505, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 17 with value: 8.035604889437069.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:10:42,106] Trial 20 finished with value: 8.011918722634514 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.003494494880264359, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 20 with value: 8.011918722634514.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:11:30,508] Trial 21 finished with value: 7.895751900406469 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0033061301698202017, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 21 with value: 7.895751900406469.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:12:41,438] Trial 22 finished with value: 7.968738795121605 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0034099644136067657, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 21 with value: 7.895751900406469.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:13:16,905] Trial 23 finished with value: 8.083351222843604 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.002064595745559742, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 21 with value: 7.895751900406469.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:14:15,442] Trial 24 finished with value: 7.956887718394084 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0038983467781305396, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 21 with value: 7.895751900406469.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:15:19,342] Trial 25 finished with value: 8.025573393381604 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004283058043447493, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 21 with value: 7.895751900406469.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:16:04,661] Trial 26 finished with value: 8.06326822800046 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.002103762622531458, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 21 with value: 7.895751900406469.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:17:08,582] Trial 27 finished with value: 7.981410016813934 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004030402743807104, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 21 with value: 7.895751900406469.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:18:18,168] Trial 28 finished with value: 8.030769787846218 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0024322036542309957, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 21 with value: 7.895751900406469.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:19:15,499] Trial 29 finished with value: 7.707018709785889 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0013876994804289905, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:19:40,625] Trial 30 finished with value: 8.064679081543431 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0010126588970531215, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:19:48,318] Trial 31 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:20:28,012] Trial 32 finished with value: 8.06067210870457 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0026926034906923652, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:20:44,230] Trial 33 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:21:28,770] Trial 34 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:21:37,313] Trial 35 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:22:16,464] Trial 36 finished with value: 8.04268065286627 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0016166365393631337, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:22:49,854] Trial 37 finished with value: 8.116799852936445 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004693052923663803, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.15000000000000002}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:23:03,576] Trial 38 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:23:19,841] Trial 39 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:23:27,964] Trial 40 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:24:37,732] Trial 41 finished with value: 8.03329455950966 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004547582847824539, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:25:41,327] Trial 42 finished with value: 8.085630973750117 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0037665179576084427, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:26:40,361] Trial 43 finished with value: 7.810356949593844 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.005933897454538834, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:26:47,671] Trial 44 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:27:18,204] Trial 45 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:27:54,570] Trial 46 finished with value: 8.087682694621332 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.006412961772115979, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:28:29,879] Trial 47 finished with value: 8.066981326084738 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0007137446078104952, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.1}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:28:36,443] Trial 48 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:29:00,658] Trial 49 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:29:33,820] Trial 50 finished with value: 7.7434893285083435 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.007382303183224713, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:30:05,594] Trial 51 finished with value: 8.059314152270717 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.007549957762672246, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:30:40,755] Trial 52 finished with value: 8.056060036059234 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.006355783027331873, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:31:12,256] Trial 53 finished with value: 8.047789854971683 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.007378705494742956, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.15000000000000002}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:31:38,680] Trial 54 finished with value: 8.117957028831915 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008879903304231945, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:32:03,624] Trial 55 finished with value: 8.071668364351627 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.005746814082902043, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:33:10,105] Trial 56 finished with value: 7.9366221531831 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008632600292830538, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:34:06,279] Trial 57 finished with value: 8.035583212704505 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.009165328704009157, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:34:33,027] Trial 58 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:34:44,598] Trial 59 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:34:49,174] Trial 60 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:35:57,940] Trial 61 finished with value: 7.954544719119833 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.00294245489551131, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:37:04,456] Trial 62 finished with value: 8.084543629870389 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.002986006237257322, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:37:15,384] Trial 63 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:37:29,939] Trial 64 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:38:21,331] Trial 65 finished with value: 7.927986497510574 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.009510005316160676, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:38:45,483] Trial 66 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:38:58,117] Trial 67 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:39:57,612] Trial 68 finished with value: 8.044128833243025 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.006721037980471418, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:40:41,916] Trial 69 finished with value: 7.851329237689461 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008502514508871024, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:40:48,887] Trial 70 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:40:54,425] Trial 71 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:41:00,382] Trial 72 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:41:08,757] Trial 73 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:41:19,386] Trial 74 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:41:27,769] Trial 75 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:42:24,754] Trial 76 finished with value: 7.968997918985797 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008309185373577117, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:42:33,783] Trial 77 finished with value: 8.072051771953216 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.009382961602135682, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:42:45,284] Trial 78 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:43:06,757] Trial 79 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:43:12,464] Trial 80 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:43:23,987] Trial 81 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:44:21,160] Trial 82 finished with value: 8.011878243134577 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004419024432487524, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:45:26,896] Trial 83 finished with value: 7.874181783628922 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0028995467352481833, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:46:29,508] Trial 84 finished with value: 8.076933606999177 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0026881299584821414, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:46:40,503] Trial 85 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:47:00,241] Trial 86 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:47:12,983] Trial 87 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:47:22,093] Trial 88 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:47:29,471] Trial 89 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:47:41,255] Trial 90 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:47:51,258] Trial 91 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:48:57,285] Trial 92 finished with value: 8.144664082780325 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.003954949803359157, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:49:11,616] Trial 93 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:50:12,077] Trial 94 finished with value: 7.9386108482432345 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0033932131700765036, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:51:26,551] Trial 95 finished with value: 8.070136960795253 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.002242239042106237, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:52:25,400] Trial 96 finished with value: 7.980491362032777 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.009111491895869205, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:54:14,803] Trial 97 finished with value: 7.979864076909979 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0087331784886066, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.15000000000000002}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:54:56,069] Trial 98 finished with value: 7.978944039892669 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.003280800347435423, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 29 with value: 7.707018709785889.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:55:53,181] Trial 99 finished with value: 8.032022808539404 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.0024925442055966796, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 29 with value: 7.707018709785889.
GPU CUDA available, using GPU for training the models.
-----------------------------------------------
Input arguments: 
   + data_dir: /home/ra56kop/nnets_genomic_prediction/src
   + model: RNN
   + tuned: 1
   + minmax_scale: 0
   + standa_scale: 0
   + pca_fitting: 0
   + dataset: pheno_1
   + gpucuda: 2
   + data_variants: [0, 0, 0, 1]
-----------------------------------------------

---------------------------------------------------------
Tuning RNN with dataset pheno-1
---------------------------------------------------------


----------------------------------------------
Params for Trial 0
{'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=83.41127 | best_loss=83.41127
Epoch 1/80: current_loss=82.80783 | best_loss=82.80783
Epoch 2/80: current_loss=82.18703 | best_loss=82.18703
Epoch 3/80: current_loss=81.52949 | best_loss=81.52949
Epoch 4/80: current_loss=80.81331 | best_loss=80.81331
Epoch 5/80: current_loss=80.01361 | best_loss=80.01361
Epoch 6/80: current_loss=79.12167 | best_loss=79.12167
Epoch 7/80: current_loss=78.12836 | best_loss=78.12836
Epoch 8/80: current_loss=76.99077 | best_loss=76.99077
Epoch 9/80: current_loss=75.66969 | best_loss=75.66969
Epoch 10/80: current_loss=74.14663 | best_loss=74.14663
Epoch 11/80: current_loss=72.42827 | best_loss=72.42827
Epoch 12/80: current_loss=70.56462 | best_loss=70.56462
Epoch 13/80: current_loss=68.54439 | best_loss=68.54439
Epoch 14/80: current_loss=66.45261 | best_loss=66.45261
Epoch 15/80: current_loss=64.40174 | best_loss=64.40174
Epoch 16/80: current_loss=62.42476 | best_loss=62.42476
Epoch 17/80: current_loss=60.51366 | best_loss=60.51366
Epoch 18/80: current_loss=58.70214 | best_loss=58.70214
Epoch 19/80: current_loss=57.02302 | best_loss=57.02302
Epoch 20/80: current_loss=55.43293 | best_loss=55.43293
Epoch 21/80: current_loss=53.97011 | best_loss=53.97011
Epoch 22/80: current_loss=52.61989 | best_loss=52.61989
Epoch 23/80: current_loss=51.38146 | best_loss=51.38146
Epoch 24/80: current_loss=50.20300 | best_loss=50.20300
Epoch 25/80: current_loss=49.10316 | best_loss=49.10316
Epoch 26/80: current_loss=48.06907 | best_loss=48.06907
Epoch 27/80: current_loss=47.09690 | best_loss=47.09690
Epoch 28/80: current_loss=46.16140 | best_loss=46.16140
Epoch 29/80: current_loss=45.30331 | best_loss=45.30331
Epoch 30/80: current_loss=44.49500 | best_loss=44.49500
Epoch 31/80: current_loss=43.70256 | best_loss=43.70256
Epoch 32/80: current_loss=42.93702 | best_loss=42.93702
Epoch 33/80: current_loss=42.20619 | best_loss=42.20619
Epoch 34/80: current_loss=41.51298 | best_loss=41.51298
Epoch 35/80: current_loss=40.82616 | best_loss=40.82616
Epoch 36/80: current_loss=40.17015 | best_loss=40.17015
Epoch 37/80: current_loss=39.53724 | best_loss=39.53724
Epoch 38/80: current_loss=38.92607 | best_loss=38.92607
Epoch 39/80: current_loss=38.32785 | best_loss=38.32785
Epoch 40/80: current_loss=37.74874 | best_loss=37.74874
Epoch 41/80: current_loss=37.17756 | best_loss=37.17756
Epoch 42/80: current_loss=36.62021 | best_loss=36.62021
Epoch 43/80: current_loss=36.09054 | best_loss=36.09054
Epoch 44/80: current_loss=35.56631 | best_loss=35.56631
Epoch 45/80: current_loss=35.04527 | best_loss=35.04527
Epoch 46/80: current_loss=34.54728 | best_loss=34.54728
Epoch 47/80: current_loss=34.05125 | best_loss=34.05125
Epoch 48/80: current_loss=33.57736 | best_loss=33.57736
Epoch 49/80: current_loss=33.10302 | best_loss=33.10302
Epoch 50/80: current_loss=32.62762 | best_loss=32.62762
Epoch 51/80: current_loss=32.17839 | best_loss=32.17839
Epoch 52/80: current_loss=31.73324 | best_loss=31.73324
Epoch 53/80: current_loss=31.29354 | best_loss=31.29354
Epoch 54/80: current_loss=30.86568 | best_loss=30.86568
Epoch 55/80: current_loss=30.44133 | best_loss=30.44133
Epoch 56/80: current_loss=30.02641 | best_loss=30.02641
Epoch 57/80: current_loss=29.62030 | best_loss=29.62030
Epoch 58/80: current_loss=29.22171 | best_loss=29.22171
Epoch 59/80: current_loss=28.83798 | best_loss=28.83798
Epoch 60/80: current_loss=28.45845 | best_loss=28.45845
Epoch 61/80: current_loss=28.09037 | best_loss=28.09037
Epoch 62/80: current_loss=27.72562 | best_loss=27.72562
Epoch 63/80: current_loss=27.36090 | best_loss=27.36090
Epoch 64/80: current_loss=27.00876 | best_loss=27.00876
Epoch 65/80: current_loss=26.65950 | best_loss=26.65950
Epoch 66/80: current_loss=26.32118 | best_loss=26.32118
Epoch 67/80: current_loss=25.98659 | best_loss=25.98659
Epoch 68/80: current_loss=25.65830 | best_loss=25.65830
Epoch 69/80: current_loss=25.33356 | best_loss=25.33356
Epoch 70/80: current_loss=25.01601 | best_loss=25.01601
Epoch 71/80: current_loss=24.70430 | best_loss=24.70430
Epoch 72/80: current_loss=24.39972 | best_loss=24.39972
Epoch 73/80: current_loss=24.09864 | best_loss=24.09864
Epoch 74/80: current_loss=23.80346 | best_loss=23.80346
Epoch 75/80: current_loss=23.51214 | best_loss=23.51214
Epoch 76/80: current_loss=23.22324 | best_loss=23.22324
Epoch 77/80: current_loss=22.94567 | best_loss=22.94567
Epoch 78/80: current_loss=22.66896 | best_loss=22.66896
Epoch 79/80: current_loss=22.39844 | best_loss=22.39844
      explained_var=-0.05731 | mse_loss=21.85578
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=27.46903 | best_loss=27.46903
Epoch 1/80: current_loss=27.05708 | best_loss=27.05708
Epoch 2/80: current_loss=26.65810 | best_loss=26.65810
Epoch 3/80: current_loss=26.26907 | best_loss=26.26907
Epoch 4/80: current_loss=25.90270 | best_loss=25.90270
Epoch 5/80: current_loss=25.55045 | best_loss=25.55045
Epoch 6/80: current_loss=25.19908 | best_loss=25.19908
Epoch 7/80: current_loss=24.86799 | best_loss=24.86799
Epoch 8/80: current_loss=24.54051 | best_loss=24.54051
Epoch 9/80: current_loss=24.21818 | best_loss=24.21818
Epoch 10/80: current_loss=23.90614 | best_loss=23.90614
Epoch 11/80: current_loss=23.60502 | best_loss=23.60502
Epoch 12/80: current_loss=23.30489 | best_loss=23.30489
Epoch 13/80: current_loss=23.01381 | best_loss=23.01381
Epoch 14/80: current_loss=22.72654 | best_loss=22.72654
Epoch 15/80: current_loss=22.44987 | best_loss=22.44987
Epoch 16/80: current_loss=22.16761 | best_loss=22.16761
Epoch 17/80: current_loss=21.90107 | best_loss=21.90107
Epoch 18/80: current_loss=21.65773 | best_loss=21.65773
Epoch 19/80: current_loss=21.40797 | best_loss=21.40797
Epoch 20/80: current_loss=21.16714 | best_loss=21.16714
Epoch 21/80: current_loss=20.92212 | best_loss=20.92212
Epoch 22/80: current_loss=20.68683 | best_loss=20.68683
Epoch 23/80: current_loss=20.45561 | best_loss=20.45561
Epoch 24/80: current_loss=20.22890 | best_loss=20.22890
Epoch 25/80: current_loss=20.00138 | best_loss=20.00138
Epoch 26/80: current_loss=19.78451 | best_loss=19.78451
Epoch 27/80: current_loss=19.56403 | best_loss=19.56403
Epoch 28/80: current_loss=19.34638 | best_loss=19.34638
Epoch 29/80: current_loss=19.13258 | best_loss=19.13258
Epoch 30/80: current_loss=18.93210 | best_loss=18.93210
Epoch 31/80: current_loss=18.73129 | best_loss=18.73129
Epoch 32/80: current_loss=18.53340 | best_loss=18.53340
Epoch 33/80: current_loss=18.34096 | best_loss=18.34096
Epoch 34/80: current_loss=18.15133 | best_loss=18.15133
Epoch 35/80: current_loss=17.95892 | best_loss=17.95892
Epoch 36/80: current_loss=17.77284 | best_loss=17.77284
Epoch 37/80: current_loss=17.58846 | best_loss=17.58846
Epoch 38/80: current_loss=17.41215 | best_loss=17.41215
Epoch 39/80: current_loss=17.23834 | best_loss=17.23834
Epoch 40/80: current_loss=17.06918 | best_loss=17.06918
Epoch 41/80: current_loss=16.90327 | best_loss=16.90327
Epoch 42/80: current_loss=16.73613 | best_loss=16.73613
Epoch 43/80: current_loss=16.57128 | best_loss=16.57128
Epoch 44/80: current_loss=16.41648 | best_loss=16.41648
Epoch 45/80: current_loss=16.25857 | best_loss=16.25857
Epoch 46/80: current_loss=16.10105 | best_loss=16.10105
Epoch 47/80: current_loss=15.95651 | best_loss=15.95651
Epoch 48/80: current_loss=15.80694 | best_loss=15.80694
Epoch 49/80: current_loss=15.66531 | best_loss=15.66531
Epoch 50/80: current_loss=15.52681 | best_loss=15.52681
Epoch 51/80: current_loss=15.39390 | best_loss=15.39390
Epoch 52/80: current_loss=15.25622 | best_loss=15.25622
Epoch 53/80: current_loss=15.11974 | best_loss=15.11974
Epoch 54/80: current_loss=14.98196 | best_loss=14.98196
Epoch 55/80: current_loss=14.84880 | best_loss=14.84880
Epoch 56/80: current_loss=14.72313 | best_loss=14.72313
Epoch 57/80: current_loss=14.59990 | best_loss=14.59990
Epoch 58/80: current_loss=14.47987 | best_loss=14.47987
Epoch 59/80: current_loss=14.36717 | best_loss=14.36717
Epoch 60/80: current_loss=14.25281 | best_loss=14.25281
Epoch 61/80: current_loss=14.13963 | best_loss=14.13963
Epoch 62/80: current_loss=14.02802 | best_loss=14.02802
Epoch 63/80: current_loss=13.91240 | best_loss=13.91240
Epoch 64/80: current_loss=13.79728 | best_loss=13.79728
Epoch 65/80: current_loss=13.68697 | best_loss=13.68697
Epoch 66/80: current_loss=13.59067 | best_loss=13.59067
Epoch 67/80: current_loss=13.49539 | best_loss=13.49539
Epoch 68/80: current_loss=13.39782 | best_loss=13.39782
Epoch 69/80: current_loss=13.30940 | best_loss=13.30940
Epoch 70/80: current_loss=13.21878 | best_loss=13.21878
Epoch 71/80: current_loss=13.13416 | best_loss=13.13416
Epoch 72/80: current_loss=13.03815 | best_loss=13.03815
Epoch 73/80: current_loss=12.95534 | best_loss=12.95534
Epoch 74/80: current_loss=12.87081 | best_loss=12.87081
Epoch 75/80: current_loss=12.78646 | best_loss=12.78646
Epoch 76/80: current_loss=12.70000 | best_loss=12.70000
Epoch 77/80: current_loss=12.61801 | best_loss=12.61801
Epoch 78/80: current_loss=12.54268 | best_loss=12.54268
Epoch 79/80: current_loss=12.46412 | best_loss=12.46412
      explained_var=-0.14384 | mse_loss=12.38446
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.97540 | best_loss=13.97540
Epoch 1/80: current_loss=13.86062 | best_loss=13.86062
Epoch 2/80: current_loss=13.73709 | best_loss=13.73709
Epoch 3/80: current_loss=13.61203 | best_loss=13.61203
Epoch 4/80: current_loss=13.49597 | best_loss=13.49597
Epoch 5/80: current_loss=13.38100 | best_loss=13.38100
Epoch 6/80: current_loss=13.27198 | best_loss=13.27198
Epoch 7/80: current_loss=13.17764 | best_loss=13.17764
Epoch 8/80: current_loss=13.07760 | best_loss=13.07760
Epoch 9/80: current_loss=12.98040 | best_loss=12.98040
Epoch 10/80: current_loss=12.88578 | best_loss=12.88578
Epoch 11/80: current_loss=12.79723 | best_loss=12.79723
Epoch 12/80: current_loss=12.71252 | best_loss=12.71252
Epoch 13/80: current_loss=12.62607 | best_loss=12.62607
Epoch 14/80: current_loss=12.53660 | best_loss=12.53660
Epoch 15/80: current_loss=12.45672 | best_loss=12.45672
Epoch 16/80: current_loss=12.38168 | best_loss=12.38168
Epoch 17/80: current_loss=12.30823 | best_loss=12.30823
Epoch 18/80: current_loss=12.23707 | best_loss=12.23707
Epoch 19/80: current_loss=12.18002 | best_loss=12.18002
Epoch 20/80: current_loss=12.11584 | best_loss=12.11584
Epoch 21/80: current_loss=12.04868 | best_loss=12.04868
Epoch 22/80: current_loss=11.98434 | best_loss=11.98434
Epoch 23/80: current_loss=11.93005 | best_loss=11.93005
Epoch 24/80: current_loss=11.88778 | best_loss=11.88778
Epoch 25/80: current_loss=11.82907 | best_loss=11.82907
Epoch 26/80: current_loss=11.77049 | best_loss=11.77049
Epoch 27/80: current_loss=11.71998 | best_loss=11.71998
Epoch 28/80: current_loss=11.67261 | best_loss=11.67261
Epoch 29/80: current_loss=11.62231 | best_loss=11.62231
Epoch 30/80: current_loss=11.57867 | best_loss=11.57867
Epoch 31/80: current_loss=11.55100 | best_loss=11.55100
Epoch 32/80: current_loss=11.50836 | best_loss=11.50836
Epoch 33/80: current_loss=11.48349 | best_loss=11.48349
Epoch 34/80: current_loss=11.45492 | best_loss=11.45492
Epoch 35/80: current_loss=11.41557 | best_loss=11.41557
Epoch 36/80: current_loss=11.37409 | best_loss=11.37409
Epoch 37/80: current_loss=11.34146 | best_loss=11.34146
Epoch 38/80: current_loss=11.30182 | best_loss=11.30182
Epoch 39/80: current_loss=11.27008 | best_loss=11.27008
Epoch 40/80: current_loss=11.23745 | best_loss=11.23745
Epoch 41/80: current_loss=11.20308 | best_loss=11.20308
Epoch 42/80: current_loss=11.17901 | best_loss=11.17901
Epoch 43/80: current_loss=11.15450 | best_loss=11.15450
Epoch 44/80: current_loss=11.12918 | best_loss=11.12918
Epoch 45/80: current_loss=11.10370 | best_loss=11.10370
Epoch 46/80: current_loss=11.08593 | best_loss=11.08593
Epoch 47/80: current_loss=11.06320 | best_loss=11.06320
Epoch 48/80: current_loss=11.04238 | best_loss=11.04238
Epoch 49/80: current_loss=11.02251 | best_loss=11.02251
Epoch 50/80: current_loss=11.00164 | best_loss=11.00164
Epoch 51/80: current_loss=10.98502 | best_loss=10.98502
Epoch 52/80: current_loss=10.96451 | best_loss=10.96451
Epoch 53/80: current_loss=10.95070 | best_loss=10.95070
Epoch 54/80: current_loss=10.93312 | best_loss=10.93312
Epoch 55/80: current_loss=10.91402 | best_loss=10.91402
Epoch 56/80: current_loss=10.88722 | best_loss=10.88722
Epoch 57/80: current_loss=10.86397 | best_loss=10.86397
Epoch 58/80: current_loss=10.83980 | best_loss=10.83980
Epoch 59/80: current_loss=10.83172 | best_loss=10.83172
Epoch 60/80: current_loss=10.82404 | best_loss=10.82404
Epoch 61/80: current_loss=10.80681 | best_loss=10.80681
Epoch 62/80: current_loss=10.78771 | best_loss=10.78771
Epoch 63/80: current_loss=10.76660 | best_loss=10.76660
Epoch 64/80: current_loss=10.74894 | best_loss=10.74894
Epoch 65/80: current_loss=10.73066 | best_loss=10.73066
Epoch 66/80: current_loss=10.71142 | best_loss=10.71142
Epoch 67/80: current_loss=10.69894 | best_loss=10.69894
Epoch 68/80: current_loss=10.68464 | best_loss=10.68464
Epoch 69/80: current_loss=10.66889 | best_loss=10.66889
Epoch 70/80: current_loss=10.66475 | best_loss=10.66475
Epoch 71/80: current_loss=10.65416 | best_loss=10.65416
Epoch 72/80: current_loss=10.63614 | best_loss=10.63614
Epoch 73/80: current_loss=10.62309 | best_loss=10.62309
Epoch 74/80: current_loss=10.60789 | best_loss=10.60789
Epoch 75/80: current_loss=10.59819 | best_loss=10.59819
Epoch 76/80: current_loss=10.59234 | best_loss=10.59234
Epoch 77/80: current_loss=10.58217 | best_loss=10.58217
Epoch 78/80: current_loss=10.57679 | best_loss=10.57679
Epoch 79/80: current_loss=10.57552 | best_loss=10.57552
      explained_var=-0.14219 | mse_loss=10.43098
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.95755 | best_loss=8.95755
Epoch 1/80: current_loss=8.94195 | best_loss=8.94195
Epoch 2/80: current_loss=8.92946 | best_loss=8.92946
Epoch 3/80: current_loss=8.91518 | best_loss=8.91518
Epoch 4/80: current_loss=8.90880 | best_loss=8.90880
Epoch 5/80: current_loss=8.90505 | best_loss=8.90505
Epoch 6/80: current_loss=8.89777 | best_loss=8.89777
Epoch 7/80: current_loss=8.89063 | best_loss=8.89063
Epoch 8/80: current_loss=8.87968 | best_loss=8.87968
Epoch 9/80: current_loss=8.87321 | best_loss=8.87321
Epoch 10/80: current_loss=8.86548 | best_loss=8.86548
Epoch 11/80: current_loss=8.85725 | best_loss=8.85725
Epoch 12/80: current_loss=8.85078 | best_loss=8.85078
Epoch 13/80: current_loss=8.84592 | best_loss=8.84592
Epoch 14/80: current_loss=8.83982 | best_loss=8.83982
Epoch 15/80: current_loss=8.83618 | best_loss=8.83618
Epoch 16/80: current_loss=8.83076 | best_loss=8.83076
Epoch 17/80: current_loss=8.82411 | best_loss=8.82411
Epoch 18/80: current_loss=8.82014 | best_loss=8.82014
Epoch 19/80: current_loss=8.81733 | best_loss=8.81733
Epoch 20/80: current_loss=8.81003 | best_loss=8.81003
Epoch 21/80: current_loss=8.80424 | best_loss=8.80424
Epoch 22/80: current_loss=8.80131 | best_loss=8.80131
Epoch 23/80: current_loss=8.79894 | best_loss=8.79894
Epoch 24/80: current_loss=8.79517 | best_loss=8.79517
Epoch 25/80: current_loss=8.78903 | best_loss=8.78903
Epoch 26/80: current_loss=8.78229 | best_loss=8.78229
Epoch 27/80: current_loss=8.77848 | best_loss=8.77848
Epoch 28/80: current_loss=8.77597 | best_loss=8.77597
Epoch 29/80: current_loss=8.77324 | best_loss=8.77324
Epoch 30/80: current_loss=8.77170 | best_loss=8.77170
Epoch 31/80: current_loss=8.77032 | best_loss=8.77032
Epoch 32/80: current_loss=8.76694 | best_loss=8.76694
Epoch 33/80: current_loss=8.76393 | best_loss=8.76393
Epoch 34/80: current_loss=8.75930 | best_loss=8.75930
Epoch 35/80: current_loss=8.75152 | best_loss=8.75152
Epoch 36/80: current_loss=8.74662 | best_loss=8.74662
Epoch 37/80: current_loss=8.73940 | best_loss=8.73940
Epoch 38/80: current_loss=8.73437 | best_loss=8.73437
Epoch 39/80: current_loss=8.72862 | best_loss=8.72862
Epoch 40/80: current_loss=8.72421 | best_loss=8.72421
Epoch 41/80: current_loss=8.72244 | best_loss=8.72244
Epoch 42/80: current_loss=8.72063 | best_loss=8.72063
Epoch 43/80: current_loss=8.71696 | best_loss=8.71696
Epoch 44/80: current_loss=8.71527 | best_loss=8.71527
Epoch 45/80: current_loss=8.70990 | best_loss=8.70990
Epoch 46/80: current_loss=8.70744 | best_loss=8.70744
Epoch 47/80: current_loss=8.70221 | best_loss=8.70221
Epoch 48/80: current_loss=8.69680 | best_loss=8.69680
Epoch 49/80: current_loss=8.69526 | best_loss=8.69526
Epoch 50/80: current_loss=8.69441 | best_loss=8.69441
Epoch 51/80: current_loss=8.69388 | best_loss=8.69388
Epoch 52/80: current_loss=8.69080 | best_loss=8.69080
Epoch 53/80: current_loss=8.68508 | best_loss=8.68508
Epoch 54/80: current_loss=8.68257 | best_loss=8.68257
Epoch 55/80: current_loss=8.68118 | best_loss=8.68118
Epoch 56/80: current_loss=8.67984 | best_loss=8.67984
Epoch 57/80: current_loss=8.67866 | best_loss=8.67866
Epoch 58/80: current_loss=8.67478 | best_loss=8.67478
Epoch 59/80: current_loss=8.67426 | best_loss=8.67426
Epoch 60/80: current_loss=8.67005 | best_loss=8.67005
Epoch 61/80: current_loss=8.67106 | best_loss=8.67005
Epoch 62/80: current_loss=8.66732 | best_loss=8.66732
Epoch 63/80: current_loss=8.66305 | best_loss=8.66305
Epoch 64/80: current_loss=8.66304 | best_loss=8.66304
Epoch 65/80: current_loss=8.66128 | best_loss=8.66128
Epoch 66/80: current_loss=8.65782 | best_loss=8.65782
Epoch 67/80: current_loss=8.65630 | best_loss=8.65630
Epoch 68/80: current_loss=8.65949 | best_loss=8.65630
Epoch 69/80: current_loss=8.65820 | best_loss=8.65630
Epoch 70/80: current_loss=8.65493 | best_loss=8.65493
Epoch 71/80: current_loss=8.64831 | best_loss=8.64831
Epoch 72/80: current_loss=8.64479 | best_loss=8.64479
Epoch 73/80: current_loss=8.64376 | best_loss=8.64376
Epoch 74/80: current_loss=8.64105 | best_loss=8.64105
Epoch 75/80: current_loss=8.64124 | best_loss=8.64105
Epoch 76/80: current_loss=8.63664 | best_loss=8.63664
Epoch 77/80: current_loss=8.63711 | best_loss=8.63664
Epoch 78/80: current_loss=8.63778 | best_loss=8.63664
Epoch 79/80: current_loss=8.63911 | best_loss=8.63664
      explained_var=-0.02858 | mse_loss=8.66950
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.42640 | best_loss=8.42640
Epoch 1/80: current_loss=8.41740 | best_loss=8.41740
Epoch 2/80: current_loss=8.41666 | best_loss=8.41666
Epoch 3/80: current_loss=8.41276 | best_loss=8.41276
Epoch 4/80: current_loss=8.40812 | best_loss=8.40812
Epoch 5/80: current_loss=8.40305 | best_loss=8.40305
Epoch 6/80: current_loss=8.39906 | best_loss=8.39906
Epoch 7/80: current_loss=8.39335 | best_loss=8.39335
Epoch 8/80: current_loss=8.38909 | best_loss=8.38909
Epoch 9/80: current_loss=8.38737 | best_loss=8.38737
Epoch 10/80: current_loss=8.38717 | best_loss=8.38717
Epoch 11/80: current_loss=8.38127 | best_loss=8.38127
Epoch 12/80: current_loss=8.37944 | best_loss=8.37944
Epoch 13/80: current_loss=8.37625 | best_loss=8.37625
Epoch 14/80: current_loss=8.37580 | best_loss=8.37580
Epoch 15/80: current_loss=8.37553 | best_loss=8.37553
Epoch 16/80: current_loss=8.37367 | best_loss=8.37367
Epoch 17/80: current_loss=8.37391 | best_loss=8.37367
Epoch 18/80: current_loss=8.37401 | best_loss=8.37367
Epoch 19/80: current_loss=8.37462 | best_loss=8.37367
Epoch 20/80: current_loss=8.37398 | best_loss=8.37367
Epoch 21/80: current_loss=8.37497 | best_loss=8.37367
Epoch 22/80: current_loss=8.37108 | best_loss=8.37108
Epoch 23/80: current_loss=8.36691 | best_loss=8.36691
Epoch 24/80: current_loss=8.36521 | best_loss=8.36521
Epoch 25/80: current_loss=8.36095 | best_loss=8.36095
Epoch 26/80: current_loss=8.35616 | best_loss=8.35616
Epoch 27/80: current_loss=8.35128 | best_loss=8.35128
Epoch 28/80: current_loss=8.35387 | best_loss=8.35128
Epoch 29/80: current_loss=8.35293 | best_loss=8.35128
Epoch 30/80: current_loss=8.35372 | best_loss=8.35128
Epoch 31/80: current_loss=8.35313 | best_loss=8.35128
Epoch 32/80: current_loss=8.35331 | best_loss=8.35128
Epoch 33/80: current_loss=8.35320 | best_loss=8.35128
Epoch 34/80: current_loss=8.35713 | best_loss=8.35128
Epoch 35/80: current_loss=8.35784 | best_loss=8.35128
Epoch 36/80: current_loss=8.35859 | best_loss=8.35128
Epoch 37/80: current_loss=8.35923 | best_loss=8.35128
Epoch 38/80: current_loss=8.35760 | best_loss=8.35128
Epoch 39/80: current_loss=8.35422 | best_loss=8.35128
Epoch 40/80: current_loss=8.35201 | best_loss=8.35128
Epoch 41/80: current_loss=8.35196 | best_loss=8.35128
Epoch 42/80: current_loss=8.34959 | best_loss=8.34959
Epoch 43/80: current_loss=8.34825 | best_loss=8.34825
Epoch 44/80: current_loss=8.34719 | best_loss=8.34719
Epoch 45/80: current_loss=8.34339 | best_loss=8.34339
Epoch 46/80: current_loss=8.33917 | best_loss=8.33917
Epoch 47/80: current_loss=8.33524 | best_loss=8.33524
Epoch 48/80: current_loss=8.33299 | best_loss=8.33299
Epoch 49/80: current_loss=8.32958 | best_loss=8.32958
Epoch 50/80: current_loss=8.32715 | best_loss=8.32715
Epoch 51/80: current_loss=8.32105 | best_loss=8.32105
Epoch 52/80: current_loss=8.32166 | best_loss=8.32105
Epoch 53/80: current_loss=8.32068 | best_loss=8.32068
Epoch 54/80: current_loss=8.31863 | best_loss=8.31863
Epoch 55/80: current_loss=8.31837 | best_loss=8.31837
Epoch 56/80: current_loss=8.31589 | best_loss=8.31589
Epoch 57/80: current_loss=8.31177 | best_loss=8.31177
Epoch 58/80: current_loss=8.30632 | best_loss=8.30632
Epoch 59/80: current_loss=8.30513 | best_loss=8.30513
Epoch 60/80: current_loss=8.30480 | best_loss=8.30480
Epoch 61/80: current_loss=8.30240 | best_loss=8.30240
Epoch 62/80: current_loss=8.30160 | best_loss=8.30160
Epoch 63/80: current_loss=8.29994 | best_loss=8.29994
Epoch 64/80: current_loss=8.29951 | best_loss=8.29951
Epoch 65/80: current_loss=8.29324 | best_loss=8.29324
Epoch 66/80: current_loss=8.28967 | best_loss=8.28967
Epoch 67/80: current_loss=8.28948 | best_loss=8.28948
Epoch 68/80: current_loss=8.28780 | best_loss=8.28780
Epoch 69/80: current_loss=8.28295 | best_loss=8.28295
Epoch 70/80: current_loss=8.27570 | best_loss=8.27570
Epoch 71/80: current_loss=8.27200 | best_loss=8.27200
Epoch 72/80: current_loss=8.26989 | best_loss=8.26989
Epoch 73/80: current_loss=8.26346 | best_loss=8.26346
Epoch 74/80: current_loss=8.26182 | best_loss=8.26182
Epoch 75/80: current_loss=8.25595 | best_loss=8.25595
Epoch 76/80: current_loss=8.25208 | best_loss=8.25208
Epoch 77/80: current_loss=8.24783 | best_loss=8.24783
Epoch 78/80: current_loss=8.24406 | best_loss=8.24406
Epoch 79/80: current_loss=8.24513 | best_loss=8.24406
      explained_var=-0.05567 | mse_loss=8.41261
----------------------------------------------
Average early_stopping_point: 80| avg_exp_var=-0.08552| avg_loss=12.35067
----------------------------------------------


----------------------------------------------
Params for Trial 1
{'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=81.39963 | best_loss=81.39963
Epoch 1/80: current_loss=81.15334 | best_loss=81.15334
Epoch 2/80: current_loss=80.90924 | best_loss=80.90924
Epoch 3/80: current_loss=80.66783 | best_loss=80.66783
Epoch 4/80: current_loss=80.42640 | best_loss=80.42640
Epoch 5/80: current_loss=80.18960 | best_loss=80.18960
Epoch 6/80: current_loss=79.95344 | best_loss=79.95344
Epoch 7/80: current_loss=79.71824 | best_loss=79.71824
Epoch 8/80: current_loss=79.48290 | best_loss=79.48290
Epoch 9/80: current_loss=79.24528 | best_loss=79.24528
Epoch 10/80: current_loss=79.00473 | best_loss=79.00473
Epoch 11/80: current_loss=78.76488 | best_loss=78.76488
Epoch 12/80: current_loss=78.52337 | best_loss=78.52337
Epoch 13/80: current_loss=78.27789 | best_loss=78.27789
Epoch 14/80: current_loss=78.03088 | best_loss=78.03088
Epoch 15/80: current_loss=77.78348 | best_loss=77.78348
Epoch 16/80: current_loss=77.53145 | best_loss=77.53145
Epoch 17/80: current_loss=77.27376 | best_loss=77.27376
Epoch 18/80: current_loss=77.00763 | best_loss=77.00763
Epoch 19/80: current_loss=76.73888 | best_loss=76.73888
Epoch 20/80: current_loss=76.46424 | best_loss=76.46424
Epoch 21/80: current_loss=76.17899 | best_loss=76.17899
Epoch 22/80: current_loss=75.88768 | best_loss=75.88768
Epoch 23/80: current_loss=75.58592 | best_loss=75.58592
Epoch 24/80: current_loss=75.27748 | best_loss=75.27748
Epoch 25/80: current_loss=74.95346 | best_loss=74.95346
Epoch 26/80: current_loss=74.62586 | best_loss=74.62586
Epoch 27/80: current_loss=74.28296 | best_loss=74.28296
Epoch 28/80: current_loss=73.92779 | best_loss=73.92779
Epoch 29/80: current_loss=73.55569 | best_loss=73.55569
Epoch 30/80: current_loss=73.16930 | best_loss=73.16930
Epoch 31/80: current_loss=72.76729 | best_loss=72.76729
Epoch 32/80: current_loss=72.35007 | best_loss=72.35007
Epoch 33/80: current_loss=71.92055 | best_loss=71.92055
Epoch 34/80: current_loss=71.46810 | best_loss=71.46810
Epoch 35/80: current_loss=70.99235 | best_loss=70.99235
Epoch 36/80: current_loss=70.49686 | best_loss=70.49686
Epoch 37/80: current_loss=69.98380 | best_loss=69.98380
Epoch 38/80: current_loss=69.44895 | best_loss=69.44895
Epoch 39/80: current_loss=68.89081 | best_loss=68.89081
Epoch 40/80: current_loss=68.30823 | best_loss=68.30823
Epoch 41/80: current_loss=67.71295 | best_loss=67.71295
Epoch 42/80: current_loss=67.09497 | best_loss=67.09497
Epoch 43/80: current_loss=66.45750 | best_loss=66.45750
Epoch 44/80: current_loss=65.80186 | best_loss=65.80186
Epoch 45/80: current_loss=65.12163 | best_loss=65.12163
Epoch 46/80: current_loss=64.42802 | best_loss=64.42802
Epoch 47/80: current_loss=63.70314 | best_loss=63.70314
Epoch 48/80: current_loss=62.96763 | best_loss=62.96763
Epoch 49/80: current_loss=62.22091 | best_loss=62.22091
Epoch 50/80: current_loss=61.46317 | best_loss=61.46317
Epoch 51/80: current_loss=60.69635 | best_loss=60.69635
Epoch 52/80: current_loss=59.92002 | best_loss=59.92002
Epoch 53/80: current_loss=59.13781 | best_loss=59.13781
Epoch 54/80: current_loss=58.34567 | best_loss=58.34567
Epoch 55/80: current_loss=57.55538 | best_loss=57.55538
Epoch 56/80: current_loss=56.77071 | best_loss=56.77071
Epoch 57/80: current_loss=55.99094 | best_loss=55.99094
Epoch 58/80: current_loss=55.22934 | best_loss=55.22934
Epoch 59/80: current_loss=54.46575 | best_loss=54.46575
Epoch 60/80: current_loss=53.71276 | best_loss=53.71276
Epoch 61/80: current_loss=52.96316 | best_loss=52.96316
Epoch 62/80: current_loss=52.22884 | best_loss=52.22884
Epoch 63/80: current_loss=51.50539 | best_loss=51.50539
Epoch 64/80: current_loss=50.78043 | best_loss=50.78043
Epoch 65/80: current_loss=50.07118 | best_loss=50.07118
Epoch 66/80: current_loss=49.39041 | best_loss=49.39041
Epoch 67/80: current_loss=48.70211 | best_loss=48.70211
Epoch 68/80: current_loss=48.04188 | best_loss=48.04188
Epoch 69/80: current_loss=47.38303 | best_loss=47.38303
Epoch 70/80: current_loss=46.73740 | best_loss=46.73740
Epoch 71/80: current_loss=46.09764 | best_loss=46.09764
Epoch 72/80: current_loss=45.47811 | best_loss=45.47811
Epoch 73/80: current_loss=44.87290 | best_loss=44.87290
Epoch 74/80: current_loss=44.27554 | best_loss=44.27554
Epoch 75/80: current_loss=43.68659 | best_loss=43.68659
Epoch 76/80: current_loss=43.13061 | best_loss=43.13061
Epoch 77/80: current_loss=42.58586 | best_loss=42.58586
Epoch 78/80: current_loss=42.05644 | best_loss=42.05644
Epoch 79/80: current_loss=41.53173 | best_loss=41.53173
      explained_var=-0.04521 | mse_loss=40.77772
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=48.56104 | best_loss=48.56104
Epoch 1/80: current_loss=48.09959 | best_loss=48.09959
Epoch 2/80: current_loss=47.64264 | best_loss=47.64264
Epoch 3/80: current_loss=47.20302 | best_loss=47.20302
Epoch 4/80: current_loss=46.76478 | best_loss=46.76478
Epoch 5/80: current_loss=46.34080 | best_loss=46.34080
Epoch 6/80: current_loss=45.92849 | best_loss=45.92849
Epoch 7/80: current_loss=45.52840 | best_loss=45.52840
Epoch 8/80: current_loss=45.12850 | best_loss=45.12850
Epoch 9/80: current_loss=44.73408 | best_loss=44.73408
Epoch 10/80: current_loss=44.35815 | best_loss=44.35815
Epoch 11/80: current_loss=43.99054 | best_loss=43.99054
Epoch 12/80: current_loss=43.62318 | best_loss=43.62318
Epoch 13/80: current_loss=43.27094 | best_loss=43.27094
Epoch 14/80: current_loss=42.92274 | best_loss=42.92274
Epoch 15/80: current_loss=42.58355 | best_loss=42.58355
Epoch 16/80: current_loss=42.24787 | best_loss=42.24787
Epoch 17/80: current_loss=41.92331 | best_loss=41.92331
Epoch 18/80: current_loss=41.59867 | best_loss=41.59867
Epoch 19/80: current_loss=41.27876 | best_loss=41.27876
Epoch 20/80: current_loss=40.96857 | best_loss=40.96857
Epoch 21/80: current_loss=40.65207 | best_loss=40.65207
Epoch 22/80: current_loss=40.35041 | best_loss=40.35041
Epoch 23/80: current_loss=40.05761 | best_loss=40.05761
Epoch 24/80: current_loss=39.77093 | best_loss=39.77093
Epoch 25/80: current_loss=39.48622 | best_loss=39.48622
Epoch 26/80: current_loss=39.20508 | best_loss=39.20508
Epoch 27/80: current_loss=38.93054 | best_loss=38.93054
Epoch 28/80: current_loss=38.65782 | best_loss=38.65782
Epoch 29/80: current_loss=38.38937 | best_loss=38.38937
Epoch 30/80: current_loss=38.12284 | best_loss=38.12284
Epoch 31/80: current_loss=37.86167 | best_loss=37.86167
Epoch 32/80: current_loss=37.61027 | best_loss=37.61027
Epoch 33/80: current_loss=37.36214 | best_loss=37.36214
Epoch 34/80: current_loss=37.11848 | best_loss=37.11848
Epoch 35/80: current_loss=36.87731 | best_loss=36.87731
Epoch 36/80: current_loss=36.64373 | best_loss=36.64373
Epoch 37/80: current_loss=36.40612 | best_loss=36.40612
Epoch 38/80: current_loss=36.17461 | best_loss=36.17461
Epoch 39/80: current_loss=35.94822 | best_loss=35.94822
Epoch 40/80: current_loss=35.71895 | best_loss=35.71895
Epoch 41/80: current_loss=35.49634 | best_loss=35.49634
Epoch 42/80: current_loss=35.27936 | best_loss=35.27936
Epoch 43/80: current_loss=35.06613 | best_loss=35.06613
Epoch 44/80: current_loss=34.85378 | best_loss=34.85378
Epoch 45/80: current_loss=34.64893 | best_loss=34.64893
Epoch 46/80: current_loss=34.43993 | best_loss=34.43993
Epoch 47/80: current_loss=34.23594 | best_loss=34.23594
Epoch 48/80: current_loss=34.03813 | best_loss=34.03813
Epoch 49/80: current_loss=33.84122 | best_loss=33.84122
Epoch 50/80: current_loss=33.64648 | best_loss=33.64648
Epoch 51/80: current_loss=33.45868 | best_loss=33.45868
Epoch 52/80: current_loss=33.27700 | best_loss=33.27700
Epoch 53/80: current_loss=33.09481 | best_loss=33.09481
Epoch 54/80: current_loss=32.91083 | best_loss=32.91083
Epoch 55/80: current_loss=32.73209 | best_loss=32.73209
Epoch 56/80: current_loss=32.54969 | best_loss=32.54969
Epoch 57/80: current_loss=32.37365 | best_loss=32.37365
Epoch 58/80: current_loss=32.20339 | best_loss=32.20339
Epoch 59/80: current_loss=32.03093 | best_loss=32.03093
Epoch 60/80: current_loss=31.85981 | best_loss=31.85981
Epoch 61/80: current_loss=31.69171 | best_loss=31.69171
Epoch 62/80: current_loss=31.52888 | best_loss=31.52888
Epoch 63/80: current_loss=31.36759 | best_loss=31.36759
Epoch 64/80: current_loss=31.20546 | best_loss=31.20546
Epoch 65/80: current_loss=31.04561 | best_loss=31.04561
Epoch 66/80: current_loss=30.88885 | best_loss=30.88885
Epoch 67/80: current_loss=30.73661 | best_loss=30.73661
Epoch 68/80: current_loss=30.58154 | best_loss=30.58154
Epoch 69/80: current_loss=30.43564 | best_loss=30.43564
Epoch 70/80: current_loss=30.28286 | best_loss=30.28286
Epoch 71/80: current_loss=30.13652 | best_loss=30.13652
Epoch 72/80: current_loss=29.98863 | best_loss=29.98863
Epoch 73/80: current_loss=29.84288 | best_loss=29.84288
Epoch 74/80: current_loss=29.69820 | best_loss=29.69820
Epoch 75/80: current_loss=29.56122 | best_loss=29.56122
Epoch 76/80: current_loss=29.42388 | best_loss=29.42388
Epoch 77/80: current_loss=29.28654 | best_loss=29.28654
Epoch 78/80: current_loss=29.14981 | best_loss=29.14981
Epoch 79/80: current_loss=29.01285 | best_loss=29.01285
      explained_var=-0.16794 | mse_loss=28.85613
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=31.48698 | best_loss=31.48698
Epoch 1/80: current_loss=31.30181 | best_loss=31.30181
Epoch 2/80: current_loss=31.12259 | best_loss=31.12259
Epoch 3/80: current_loss=30.94518 | best_loss=30.94518
Epoch 4/80: current_loss=30.77255 | best_loss=30.77255
Epoch 5/80: current_loss=30.60557 | best_loss=30.60557
Epoch 6/80: current_loss=30.43859 | best_loss=30.43859
Epoch 7/80: current_loss=30.27884 | best_loss=30.27884
Epoch 8/80: current_loss=30.12448 | best_loss=30.12448
Epoch 9/80: current_loss=29.97258 | best_loss=29.97258
Epoch 10/80: current_loss=29.82324 | best_loss=29.82324
Epoch 11/80: current_loss=29.67410 | best_loss=29.67410
Epoch 12/80: current_loss=29.52794 | best_loss=29.52794
Epoch 13/80: current_loss=29.38672 | best_loss=29.38672
Epoch 14/80: current_loss=29.24767 | best_loss=29.24767
Epoch 15/80: current_loss=29.10953 | best_loss=29.10953
Epoch 16/80: current_loss=28.97081 | best_loss=28.97081
Epoch 17/80: current_loss=28.83771 | best_loss=28.83771
Epoch 18/80: current_loss=28.70816 | best_loss=28.70816
Epoch 19/80: current_loss=28.57897 | best_loss=28.57897
Epoch 20/80: current_loss=28.45595 | best_loss=28.45595
Epoch 21/80: current_loss=28.33461 | best_loss=28.33461
Epoch 22/80: current_loss=28.21347 | best_loss=28.21347
Epoch 23/80: current_loss=28.09673 | best_loss=28.09673
Epoch 24/80: current_loss=27.98067 | best_loss=27.98067
Epoch 25/80: current_loss=27.86478 | best_loss=27.86478
Epoch 26/80: current_loss=27.74855 | best_loss=27.74855
Epoch 27/80: current_loss=27.63396 | best_loss=27.63396
Epoch 28/80: current_loss=27.52114 | best_loss=27.52114
Epoch 29/80: current_loss=27.40796 | best_loss=27.40796
Epoch 30/80: current_loss=27.29562 | best_loss=27.29562
Epoch 31/80: current_loss=27.18445 | best_loss=27.18445
Epoch 32/80: current_loss=27.07628 | best_loss=27.07628
Epoch 33/80: current_loss=26.96912 | best_loss=26.96912
Epoch 34/80: current_loss=26.86325 | best_loss=26.86325
Epoch 35/80: current_loss=26.75729 | best_loss=26.75729
Epoch 36/80: current_loss=26.65501 | best_loss=26.65501
Epoch 37/80: current_loss=26.55371 | best_loss=26.55371
Epoch 38/80: current_loss=26.45199 | best_loss=26.45199
Epoch 39/80: current_loss=26.34909 | best_loss=26.34909
Epoch 40/80: current_loss=26.24919 | best_loss=26.24919
Epoch 41/80: current_loss=26.14855 | best_loss=26.14855
Epoch 42/80: current_loss=26.05116 | best_loss=26.05116
Epoch 43/80: current_loss=25.95406 | best_loss=25.95406
Epoch 44/80: current_loss=25.85568 | best_loss=25.85568
Epoch 45/80: current_loss=25.76037 | best_loss=25.76037
Epoch 46/80: current_loss=25.66835 | best_loss=25.66835
Epoch 47/80: current_loss=25.57813 | best_loss=25.57813
Epoch 48/80: current_loss=25.48560 | best_loss=25.48560
Epoch 49/80: current_loss=25.39160 | best_loss=25.39160
Epoch 50/80: current_loss=25.29982 | best_loss=25.29982
Epoch 51/80: current_loss=25.20907 | best_loss=25.20907
Epoch 52/80: current_loss=25.11821 | best_loss=25.11821
Epoch 53/80: current_loss=25.02786 | best_loss=25.02786
Epoch 54/80: current_loss=24.93916 | best_loss=24.93916
Epoch 55/80: current_loss=24.85205 | best_loss=24.85205
Epoch 56/80: current_loss=24.76735 | best_loss=24.76735
Epoch 57/80: current_loss=24.68260 | best_loss=24.68260
Epoch 58/80: current_loss=24.59916 | best_loss=24.59916
Epoch 59/80: current_loss=24.51404 | best_loss=24.51404
Epoch 60/80: current_loss=24.43221 | best_loss=24.43221
Epoch 61/80: current_loss=24.34981 | best_loss=24.34981
Epoch 62/80: current_loss=24.26698 | best_loss=24.26698
Epoch 63/80: current_loss=24.18394 | best_loss=24.18394
Epoch 64/80: current_loss=24.10302 | best_loss=24.10302
Epoch 65/80: current_loss=24.02461 | best_loss=24.02461
Epoch 66/80: current_loss=23.94647 | best_loss=23.94647
Epoch 67/80: current_loss=23.87034 | best_loss=23.87034
Epoch 68/80: current_loss=23.79250 | best_loss=23.79250
Epoch 69/80: current_loss=23.71541 | best_loss=23.71541
Epoch 70/80: current_loss=23.64114 | best_loss=23.64114
Epoch 71/80: current_loss=23.56507 | best_loss=23.56507
Epoch 72/80: current_loss=23.48943 | best_loss=23.48943
Epoch 73/80: current_loss=23.41302 | best_loss=23.41302
Epoch 74/80: current_loss=23.33915 | best_loss=23.33915
Epoch 75/80: current_loss=23.26618 | best_loss=23.26618
Epoch 76/80: current_loss=23.19158 | best_loss=23.19158
Epoch 77/80: current_loss=23.11814 | best_loss=23.11814
Epoch 78/80: current_loss=23.04966 | best_loss=23.04966
Epoch 79/80: current_loss=22.98003 | best_loss=22.98003
      explained_var=-0.11718 | mse_loss=22.89907
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=19.99332 | best_loss=19.99332
Epoch 1/80: current_loss=19.91195 | best_loss=19.91195
Epoch 2/80: current_loss=19.83080 | best_loss=19.83080
Epoch 3/80: current_loss=19.75437 | best_loss=19.75437
Epoch 4/80: current_loss=19.67710 | best_loss=19.67710
Epoch 5/80: current_loss=19.59976 | best_loss=19.59976
Epoch 6/80: current_loss=19.52349 | best_loss=19.52349
Epoch 7/80: current_loss=19.44484 | best_loss=19.44484
Epoch 8/80: current_loss=19.37120 | best_loss=19.37120
Epoch 9/80: current_loss=19.29588 | best_loss=19.29588
Epoch 10/80: current_loss=19.22353 | best_loss=19.22353
Epoch 11/80: current_loss=19.15192 | best_loss=19.15192
Epoch 12/80: current_loss=19.08399 | best_loss=19.08399
Epoch 13/80: current_loss=19.01659 | best_loss=19.01659
Epoch 14/80: current_loss=18.94726 | best_loss=18.94726
Epoch 15/80: current_loss=18.88035 | best_loss=18.88035
Epoch 16/80: current_loss=18.80978 | best_loss=18.80978
Epoch 17/80: current_loss=18.74211 | best_loss=18.74211
Epoch 18/80: current_loss=18.67664 | best_loss=18.67664
Epoch 19/80: current_loss=18.61132 | best_loss=18.61132
Epoch 20/80: current_loss=18.54574 | best_loss=18.54574
Epoch 21/80: current_loss=18.48521 | best_loss=18.48521
Epoch 22/80: current_loss=18.42365 | best_loss=18.42365
Epoch 23/80: current_loss=18.36393 | best_loss=18.36393
Epoch 24/80: current_loss=18.30095 | best_loss=18.30095
Epoch 25/80: current_loss=18.23965 | best_loss=18.23965
Epoch 26/80: current_loss=18.18025 | best_loss=18.18025
Epoch 27/80: current_loss=18.12071 | best_loss=18.12071
Epoch 28/80: current_loss=18.05971 | best_loss=18.05971
Epoch 29/80: current_loss=18.00058 | best_loss=18.00058
Epoch 30/80: current_loss=17.94261 | best_loss=17.94261
Epoch 31/80: current_loss=17.88472 | best_loss=17.88472
Epoch 32/80: current_loss=17.82656 | best_loss=17.82656
Epoch 33/80: current_loss=17.77127 | best_loss=17.77127
Epoch 34/80: current_loss=17.71500 | best_loss=17.71500
Epoch 35/80: current_loss=17.65915 | best_loss=17.65915
Epoch 36/80: current_loss=17.60417 | best_loss=17.60417
Epoch 37/80: current_loss=17.54727 | best_loss=17.54727
Epoch 38/80: current_loss=17.49082 | best_loss=17.49082
Epoch 39/80: current_loss=17.43558 | best_loss=17.43558
Epoch 40/80: current_loss=17.38255 | best_loss=17.38255
Epoch 41/80: current_loss=17.33011 | best_loss=17.33011
Epoch 42/80: current_loss=17.27708 | best_loss=17.27708
Epoch 43/80: current_loss=17.22543 | best_loss=17.22543
Epoch 44/80: current_loss=17.17186 | best_loss=17.17186
Epoch 45/80: current_loss=17.12071 | best_loss=17.12071
Epoch 46/80: current_loss=17.06935 | best_loss=17.06935
Epoch 47/80: current_loss=17.01706 | best_loss=17.01706
Epoch 48/80: current_loss=16.96598 | best_loss=16.96598
Epoch 49/80: current_loss=16.91758 | best_loss=16.91758
Epoch 50/80: current_loss=16.86687 | best_loss=16.86687
Epoch 51/80: current_loss=16.81827 | best_loss=16.81827
Epoch 52/80: current_loss=16.76945 | best_loss=16.76945
Epoch 53/80: current_loss=16.72085 | best_loss=16.72085
Epoch 54/80: current_loss=16.67341 | best_loss=16.67341
Epoch 55/80: current_loss=16.62597 | best_loss=16.62597
Epoch 56/80: current_loss=16.57850 | best_loss=16.57850
Epoch 57/80: current_loss=16.52988 | best_loss=16.52988
Epoch 58/80: current_loss=16.48236 | best_loss=16.48236
Epoch 59/80: current_loss=16.43614 | best_loss=16.43614
Epoch 60/80: current_loss=16.39119 | best_loss=16.39119
Epoch 61/80: current_loss=16.34422 | best_loss=16.34422
Epoch 62/80: current_loss=16.29787 | best_loss=16.29787
Epoch 63/80: current_loss=16.25345 | best_loss=16.25345
Epoch 64/80: current_loss=16.20774 | best_loss=16.20774
Epoch 65/80: current_loss=16.16264 | best_loss=16.16264
Epoch 66/80: current_loss=16.11870 | best_loss=16.11870
Epoch 67/80: current_loss=16.07505 | best_loss=16.07505
Epoch 68/80: current_loss=16.02937 | best_loss=16.02937
Epoch 69/80: current_loss=15.98523 | best_loss=15.98523
Epoch 70/80: current_loss=15.94197 | best_loss=15.94197
Epoch 71/80: current_loss=15.89951 | best_loss=15.89951
Epoch 72/80: current_loss=15.85658 | best_loss=15.85658
Epoch 73/80: current_loss=15.81455 | best_loss=15.81455
Epoch 74/80: current_loss=15.77179 | best_loss=15.77179
Epoch 75/80: current_loss=15.72885 | best_loss=15.72885
Epoch 76/80: current_loss=15.68547 | best_loss=15.68547
Epoch 77/80: current_loss=15.64358 | best_loss=15.64358
Epoch 78/80: current_loss=15.60048 | best_loss=15.60048
Epoch 79/80: current_loss=15.55982 | best_loss=15.55982
      explained_var=-0.05845 | mse_loss=15.53033
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=15.23708 | best_loss=15.23708
Epoch 1/80: current_loss=15.19034 | best_loss=15.19034
Epoch 2/80: current_loss=15.14422 | best_loss=15.14422
Epoch 3/80: current_loss=15.09732 | best_loss=15.09732
Epoch 4/80: current_loss=15.05103 | best_loss=15.05103
Epoch 5/80: current_loss=15.00452 | best_loss=15.00452
Epoch 6/80: current_loss=14.95971 | best_loss=14.95971
Epoch 7/80: current_loss=14.91738 | best_loss=14.91738
Epoch 8/80: current_loss=14.87420 | best_loss=14.87420
Epoch 9/80: current_loss=14.83166 | best_loss=14.83166
Epoch 10/80: current_loss=14.78520 | best_loss=14.78520
Epoch 11/80: current_loss=14.74238 | best_loss=14.74238
Epoch 12/80: current_loss=14.69958 | best_loss=14.69958
Epoch 13/80: current_loss=14.65678 | best_loss=14.65678
Epoch 14/80: current_loss=14.61348 | best_loss=14.61348
Epoch 15/80: current_loss=14.57032 | best_loss=14.57032
Epoch 16/80: current_loss=14.52839 | best_loss=14.52839
Epoch 17/80: current_loss=14.48728 | best_loss=14.48728
Epoch 18/80: current_loss=14.44769 | best_loss=14.44769
Epoch 19/80: current_loss=14.40825 | best_loss=14.40825
Epoch 20/80: current_loss=14.36821 | best_loss=14.36821
Epoch 21/80: current_loss=14.32934 | best_loss=14.32934
Epoch 22/80: current_loss=14.28891 | best_loss=14.28891
Epoch 23/80: current_loss=14.24904 | best_loss=14.24904
Epoch 24/80: current_loss=14.20862 | best_loss=14.20862
Epoch 25/80: current_loss=14.17044 | best_loss=14.17044
Epoch 26/80: current_loss=14.13163 | best_loss=14.13163
Epoch 27/80: current_loss=14.09231 | best_loss=14.09231
Epoch 28/80: current_loss=14.05371 | best_loss=14.05371
Epoch 29/80: current_loss=14.01519 | best_loss=14.01519
Epoch 30/80: current_loss=13.97753 | best_loss=13.97753
Epoch 31/80: current_loss=13.93953 | best_loss=13.93953
Epoch 32/80: current_loss=13.90242 | best_loss=13.90242
Epoch 33/80: current_loss=13.86582 | best_loss=13.86582
Epoch 34/80: current_loss=13.82901 | best_loss=13.82901
Epoch 35/80: current_loss=13.79361 | best_loss=13.79361
Epoch 36/80: current_loss=13.75741 | best_loss=13.75741
Epoch 37/80: current_loss=13.72149 | best_loss=13.72149
Epoch 38/80: current_loss=13.68572 | best_loss=13.68572
Epoch 39/80: current_loss=13.64936 | best_loss=13.64936
Epoch 40/80: current_loss=13.61231 | best_loss=13.61231
Epoch 41/80: current_loss=13.57817 | best_loss=13.57817
Epoch 42/80: current_loss=13.54302 | best_loss=13.54302
Epoch 43/80: current_loss=13.50772 | best_loss=13.50772
Epoch 44/80: current_loss=13.47379 | best_loss=13.47379
Epoch 45/80: current_loss=13.43924 | best_loss=13.43924
Epoch 46/80: current_loss=13.40487 | best_loss=13.40487
Epoch 47/80: current_loss=13.37129 | best_loss=13.37129
Epoch 48/80: current_loss=13.33831 | best_loss=13.33831
Epoch 49/80: current_loss=13.30377 | best_loss=13.30377
Epoch 50/80: current_loss=13.26909 | best_loss=13.26909
Epoch 51/80: current_loss=13.23391 | best_loss=13.23391
Epoch 52/80: current_loss=13.19949 | best_loss=13.19949
Epoch 53/80: current_loss=13.16685 | best_loss=13.16685
Epoch 54/80: current_loss=13.13456 | best_loss=13.13456
Epoch 55/80: current_loss=13.10262 | best_loss=13.10262
Epoch 56/80: current_loss=13.07015 | best_loss=13.07015
Epoch 57/80: current_loss=13.03680 | best_loss=13.03680
Epoch 58/80: current_loss=13.00425 | best_loss=13.00425
Epoch 59/80: current_loss=12.97335 | best_loss=12.97335
Epoch 60/80: current_loss=12.94092 | best_loss=12.94092
Epoch 61/80: current_loss=12.90895 | best_loss=12.90895
Epoch 62/80: current_loss=12.87753 | best_loss=12.87753
Epoch 63/80: current_loss=12.84734 | best_loss=12.84734
Epoch 64/80: current_loss=12.81701 | best_loss=12.81701
Epoch 65/80: current_loss=12.78643 | best_loss=12.78643
Epoch 66/80: current_loss=12.75886 | best_loss=12.75886
Epoch 67/80: current_loss=12.73087 | best_loss=12.73087
Epoch 68/80: current_loss=12.70242 | best_loss=12.70242
Epoch 69/80: current_loss=12.67276 | best_loss=12.67276
Epoch 70/80: current_loss=12.64205 | best_loss=12.64205
Epoch 71/80: current_loss=12.61225 | best_loss=12.61225
Epoch 72/80: current_loss=12.58282 | best_loss=12.58282
Epoch 73/80: current_loss=12.55219 | best_loss=12.55219
Epoch 74/80: current_loss=12.52372 | best_loss=12.52372
Epoch 75/80: current_loss=12.49466 | best_loss=12.49466
Epoch 76/80: current_loss=12.46581 | best_loss=12.46581
Epoch 77/80: current_loss=12.43456 | best_loss=12.43456
Epoch 78/80: current_loss=12.40667 | best_loss=12.40667
Epoch 79/80: current_loss=12.37916 | best_loss=12.37916
      explained_var=-0.09136 | mse_loss=12.61092
----------------------------------------------
Average early_stopping_point: 80| avg_exp_var=-0.09603| avg_loss=24.13483
----------------------------------------------


----------------------------------------------
Params for Trial 2
{'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=53.90691 | best_loss=53.90691
Epoch 1/80: current_loss=13.41214 | best_loss=13.41214
Epoch 2/80: current_loss=12.35025 | best_loss=12.35025
Epoch 3/80: current_loss=12.04981 | best_loss=12.04981
Epoch 4/80: current_loss=11.24240 | best_loss=11.24240
Epoch 5/80: current_loss=10.85259 | best_loss=10.85259
Epoch 6/80: current_loss=10.74323 | best_loss=10.74323
Epoch 7/80: current_loss=10.57131 | best_loss=10.57131
Epoch 8/80: current_loss=10.45603 | best_loss=10.45603
Epoch 9/80: current_loss=10.17990 | best_loss=10.17990
Epoch 10/80: current_loss=10.23283 | best_loss=10.17990
Epoch 11/80: current_loss=10.25291 | best_loss=10.17990
Epoch 12/80: current_loss=9.86269 | best_loss=9.86269
Epoch 13/80: current_loss=9.74923 | best_loss=9.74923
Epoch 14/80: current_loss=9.57025 | best_loss=9.57025
Epoch 15/80: current_loss=9.56479 | best_loss=9.56479
Epoch 16/80: current_loss=9.48763 | best_loss=9.48763
Epoch 17/80: current_loss=9.50483 | best_loss=9.48763
Epoch 18/80: current_loss=9.33999 | best_loss=9.33999
Epoch 19/80: current_loss=9.21840 | best_loss=9.21840
Epoch 20/80: current_loss=9.14399 | best_loss=9.14399
Epoch 21/80: current_loss=9.11375 | best_loss=9.11375
Epoch 22/80: current_loss=9.03447 | best_loss=9.03447
Epoch 23/80: current_loss=9.10539 | best_loss=9.03447
Epoch 24/80: current_loss=8.94634 | best_loss=8.94634
Epoch 25/80: current_loss=8.79745 | best_loss=8.79745
Epoch 26/80: current_loss=8.83212 | best_loss=8.79745
Epoch 27/80: current_loss=8.84759 | best_loss=8.79745
Epoch 28/80: current_loss=8.76130 | best_loss=8.76130
Epoch 29/80: current_loss=8.81571 | best_loss=8.76130
Epoch 30/80: current_loss=8.76413 | best_loss=8.76130
Epoch 31/80: current_loss=8.60328 | best_loss=8.60328
Epoch 32/80: current_loss=8.53260 | best_loss=8.53260
Epoch 33/80: current_loss=8.59344 | best_loss=8.53260
Epoch 34/80: current_loss=8.51291 | best_loss=8.51291
Epoch 35/80: current_loss=8.49985 | best_loss=8.49985
Epoch 36/80: current_loss=8.43372 | best_loss=8.43372
Epoch 37/80: current_loss=8.53144 | best_loss=8.43372
Epoch 38/80: current_loss=8.59571 | best_loss=8.43372
Epoch 39/80: current_loss=8.52266 | best_loss=8.43372
Epoch 40/80: current_loss=8.54020 | best_loss=8.43372
Epoch 41/80: current_loss=8.45834 | best_loss=8.43372
Epoch 42/80: current_loss=8.34381 | best_loss=8.34381
Epoch 43/80: current_loss=8.36871 | best_loss=8.34381
Epoch 44/80: current_loss=8.33405 | best_loss=8.33405
Epoch 45/80: current_loss=8.26276 | best_loss=8.26276
Epoch 46/80: current_loss=8.39177 | best_loss=8.26276
Epoch 47/80: current_loss=8.33293 | best_loss=8.26276
Epoch 48/80: current_loss=8.17463 | best_loss=8.17463
Epoch 49/80: current_loss=8.16056 | best_loss=8.16056
Epoch 50/80: current_loss=8.27935 | best_loss=8.16056
Epoch 51/80: current_loss=8.29115 | best_loss=8.16056
Epoch 52/80: current_loss=8.18350 | best_loss=8.16056
Epoch 53/80: current_loss=8.13595 | best_loss=8.13595
Epoch 54/80: current_loss=8.10843 | best_loss=8.10843
Epoch 55/80: current_loss=8.08921 | best_loss=8.08921
Epoch 56/80: current_loss=8.03518 | best_loss=8.03518
Epoch 57/80: current_loss=8.09244 | best_loss=8.03518
Epoch 58/80: current_loss=8.14290 | best_loss=8.03518
Epoch 59/80: current_loss=8.25062 | best_loss=8.03518
Epoch 60/80: current_loss=8.24244 | best_loss=8.03518
Epoch 61/80: current_loss=8.13899 | best_loss=8.03518
Epoch 62/80: current_loss=8.11577 | best_loss=8.03518
Epoch 63/80: current_loss=8.09845 | best_loss=8.03518
Epoch 64/80: current_loss=8.19448 | best_loss=8.03518
Epoch 65/80: current_loss=8.14314 | best_loss=8.03518
Epoch 66/80: current_loss=8.02327 | best_loss=8.02327
Epoch 67/80: current_loss=8.03996 | best_loss=8.02327
Epoch 68/80: current_loss=8.30551 | best_loss=8.02327
Epoch 69/80: current_loss=8.14969 | best_loss=8.02327
Epoch 70/80: current_loss=8.06007 | best_loss=8.02327
Epoch 71/80: current_loss=8.03766 | best_loss=8.02327
Epoch 72/80: current_loss=8.08816 | best_loss=8.02327
Epoch 73/80: current_loss=8.05536 | best_loss=8.02327
Epoch 74/80: current_loss=8.02929 | best_loss=8.02327
Epoch 75/80: current_loss=8.00234 | best_loss=8.00234
Epoch 76/80: current_loss=7.95309 | best_loss=7.95309
Epoch 77/80: current_loss=7.97424 | best_loss=7.95309
Epoch 78/80: current_loss=8.09821 | best_loss=7.95309
Epoch 79/80: current_loss=8.01430 | best_loss=7.95309
      explained_var=0.00235 | mse_loss=7.81255
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.51897 | best_loss=8.51897
Epoch 1/80: current_loss=8.48588 | best_loss=8.48588
Epoch 2/80: current_loss=8.46372 | best_loss=8.46372
Epoch 3/80: current_loss=8.42926 | best_loss=8.42926
Epoch 4/80: current_loss=8.39847 | best_loss=8.39847
Epoch 5/80: current_loss=8.38622 | best_loss=8.38622
Epoch 6/80: current_loss=8.38004 | best_loss=8.38004
Epoch 7/80: current_loss=8.36794 | best_loss=8.36794
Epoch 8/80: current_loss=8.34646 | best_loss=8.34646
Epoch 9/80: current_loss=8.34548 | best_loss=8.34548
Epoch 10/80: current_loss=8.35046 | best_loss=8.34548
Epoch 11/80: current_loss=8.36925 | best_loss=8.34548
Epoch 12/80: current_loss=8.37893 | best_loss=8.34548
Epoch 13/80: current_loss=8.34035 | best_loss=8.34035
Epoch 14/80: current_loss=8.34606 | best_loss=8.34035
Epoch 15/80: current_loss=8.35463 | best_loss=8.34035
Epoch 16/80: current_loss=8.34178 | best_loss=8.34035
Epoch 17/80: current_loss=8.34549 | best_loss=8.34035
Epoch 18/80: current_loss=8.33762 | best_loss=8.33762
Epoch 19/80: current_loss=8.32766 | best_loss=8.32766
Epoch 20/80: current_loss=8.35333 | best_loss=8.32766
Epoch 21/80: current_loss=8.32323 | best_loss=8.32323
Epoch 22/80: current_loss=8.30845 | best_loss=8.30845
Epoch 23/80: current_loss=8.31280 | best_loss=8.30845
Epoch 24/80: current_loss=8.33537 | best_loss=8.30845
Epoch 25/80: current_loss=8.32450 | best_loss=8.30845
Epoch 26/80: current_loss=8.32944 | best_loss=8.30845
Epoch 27/80: current_loss=8.34060 | best_loss=8.30845
Epoch 28/80: current_loss=8.34833 | best_loss=8.30845
Epoch 29/80: current_loss=8.33962 | best_loss=8.30845
Epoch 30/80: current_loss=8.33019 | best_loss=8.30845
Epoch 31/80: current_loss=8.34050 | best_loss=8.30845
Epoch 32/80: current_loss=8.32949 | best_loss=8.30845
Epoch 33/80: current_loss=8.33130 | best_loss=8.30845
Epoch 34/80: current_loss=8.35839 | best_loss=8.30845
Epoch 35/80: current_loss=8.35083 | best_loss=8.30845
Epoch 36/80: current_loss=8.37990 | best_loss=8.30845
Epoch 37/80: current_loss=8.35999 | best_loss=8.30845
Epoch 38/80: current_loss=8.33977 | best_loss=8.30845
Epoch 39/80: current_loss=8.31664 | best_loss=8.30845
Epoch 40/80: current_loss=8.31687 | best_loss=8.30845
Epoch 41/80: current_loss=8.35771 | best_loss=8.30845
Epoch 42/80: current_loss=8.36320 | best_loss=8.30845
Early Stopping at epoch 42
      explained_var=0.00038 | mse_loss=8.16063
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.97437 | best_loss=8.97437
Epoch 1/80: current_loss=9.02276 | best_loss=8.97437
Epoch 2/80: current_loss=8.98845 | best_loss=8.97437
Epoch 3/80: current_loss=9.04250 | best_loss=8.97437
Epoch 4/80: current_loss=8.97444 | best_loss=8.97437
Epoch 5/80: current_loss=9.03026 | best_loss=8.97437
Epoch 6/80: current_loss=8.99980 | best_loss=8.97437
Epoch 7/80: current_loss=9.01999 | best_loss=8.97437
Epoch 8/80: current_loss=9.01043 | best_loss=8.97437
Epoch 9/80: current_loss=9.02061 | best_loss=8.97437
Epoch 10/80: current_loss=9.04696 | best_loss=8.97437
Epoch 11/80: current_loss=8.95326 | best_loss=8.95326
Epoch 12/80: current_loss=8.97189 | best_loss=8.95326
Epoch 13/80: current_loss=9.01786 | best_loss=8.95326
Epoch 14/80: current_loss=8.98843 | best_loss=8.95326
Epoch 15/80: current_loss=9.03068 | best_loss=8.95326
Epoch 16/80: current_loss=9.01528 | best_loss=8.95326
Epoch 17/80: current_loss=9.04057 | best_loss=8.95326
Epoch 18/80: current_loss=9.05052 | best_loss=8.95326
Epoch 19/80: current_loss=9.01711 | best_loss=8.95326
Epoch 20/80: current_loss=9.02651 | best_loss=8.95326
Epoch 21/80: current_loss=9.04470 | best_loss=8.95326
Epoch 22/80: current_loss=9.05288 | best_loss=8.95326
Epoch 23/80: current_loss=9.00052 | best_loss=8.95326
Epoch 24/80: current_loss=8.97365 | best_loss=8.95326
Epoch 25/80: current_loss=8.99635 | best_loss=8.95326
Epoch 26/80: current_loss=8.98880 | best_loss=8.95326
Epoch 27/80: current_loss=9.05164 | best_loss=8.95326
Epoch 28/80: current_loss=9.01064 | best_loss=8.95326
Epoch 29/80: current_loss=8.84688 | best_loss=8.84688
Epoch 30/80: current_loss=8.96964 | best_loss=8.84688
Epoch 31/80: current_loss=9.06586 | best_loss=8.84688
Epoch 32/80: current_loss=9.00582 | best_loss=8.84688
Epoch 33/80: current_loss=9.01092 | best_loss=8.84688
Epoch 34/80: current_loss=9.06044 | best_loss=8.84688
Epoch 35/80: current_loss=9.04768 | best_loss=8.84688
Epoch 36/80: current_loss=9.06507 | best_loss=8.84688
Epoch 37/80: current_loss=9.05245 | best_loss=8.84688
Epoch 38/80: current_loss=9.16416 | best_loss=8.84688
Epoch 39/80: current_loss=9.06407 | best_loss=8.84688
Epoch 40/80: current_loss=8.99059 | best_loss=8.84688
Epoch 41/80: current_loss=8.97908 | best_loss=8.84688
Epoch 42/80: current_loss=8.93724 | best_loss=8.84688
Epoch 43/80: current_loss=8.96080 | best_loss=8.84688
Epoch 44/80: current_loss=9.02925 | best_loss=8.84688
Epoch 45/80: current_loss=9.03865 | best_loss=8.84688
Epoch 46/80: current_loss=8.95061 | best_loss=8.84688
Epoch 47/80: current_loss=8.94731 | best_loss=8.84688
Epoch 48/80: current_loss=8.95293 | best_loss=8.84688
Epoch 49/80: current_loss=9.12911 | best_loss=8.84688
Early Stopping at epoch 49
      explained_var=-0.01231 | mse_loss=8.62992
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.25669 | best_loss=8.25669
Epoch 1/80: current_loss=8.24661 | best_loss=8.24661
Epoch 2/80: current_loss=8.24843 | best_loss=8.24661
Epoch 3/80: current_loss=8.24907 | best_loss=8.24661
Epoch 4/80: current_loss=8.24611 | best_loss=8.24611
Epoch 5/80: current_loss=8.24167 | best_loss=8.24167
Epoch 6/80: current_loss=8.24381 | best_loss=8.24167
Epoch 7/80: current_loss=8.26995 | best_loss=8.24167
Epoch 8/80: current_loss=8.24401 | best_loss=8.24167
Epoch 9/80: current_loss=8.24536 | best_loss=8.24167
Epoch 10/80: current_loss=8.24545 | best_loss=8.24167
Epoch 11/80: current_loss=8.24489 | best_loss=8.24167
Epoch 12/80: current_loss=8.25825 | best_loss=8.24167
Epoch 13/80: current_loss=8.25548 | best_loss=8.24167
Epoch 14/80: current_loss=8.26181 | best_loss=8.24167
Epoch 15/80: current_loss=8.25278 | best_loss=8.24167
Epoch 16/80: current_loss=8.25260 | best_loss=8.24167
Epoch 17/80: current_loss=8.24453 | best_loss=8.24167
Epoch 18/80: current_loss=8.24647 | best_loss=8.24167
Epoch 19/80: current_loss=8.26520 | best_loss=8.24167
Epoch 20/80: current_loss=8.26937 | best_loss=8.24167
Epoch 21/80: current_loss=8.25916 | best_loss=8.24167
Epoch 22/80: current_loss=8.27318 | best_loss=8.24167
Epoch 23/80: current_loss=8.27563 | best_loss=8.24167
Epoch 24/80: current_loss=8.29266 | best_loss=8.24167
Epoch 25/80: current_loss=8.28436 | best_loss=8.24167
Early Stopping at epoch 25
      explained_var=-0.00109 | mse_loss=8.34503
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.70111 | best_loss=7.70111
Epoch 1/80: current_loss=7.69365 | best_loss=7.69365
Epoch 2/80: current_loss=7.70088 | best_loss=7.69365
Epoch 3/80: current_loss=7.68563 | best_loss=7.68563
Epoch 4/80: current_loss=7.70658 | best_loss=7.68563
Epoch 5/80: current_loss=7.67987 | best_loss=7.67987
Epoch 6/80: current_loss=7.68664 | best_loss=7.67987
Epoch 7/80: current_loss=7.69399 | best_loss=7.67987
Epoch 8/80: current_loss=7.69661 | best_loss=7.67987
Epoch 9/80: current_loss=7.69101 | best_loss=7.67987
Epoch 10/80: current_loss=7.70401 | best_loss=7.67987
Epoch 11/80: current_loss=7.70748 | best_loss=7.67987
Epoch 12/80: current_loss=7.69999 | best_loss=7.67987
Epoch 13/80: current_loss=7.69970 | best_loss=7.67987
Epoch 14/80: current_loss=7.69915 | best_loss=7.67987
Epoch 15/80: current_loss=7.70447 | best_loss=7.67987
Epoch 16/80: current_loss=7.70476 | best_loss=7.67987
Epoch 17/80: current_loss=7.71221 | best_loss=7.67987
Epoch 18/80: current_loss=7.71885 | best_loss=7.67987
Epoch 19/80: current_loss=7.72036 | best_loss=7.67987
Epoch 20/80: current_loss=7.74210 | best_loss=7.67987
Epoch 21/80: current_loss=7.72557 | best_loss=7.67987
Epoch 22/80: current_loss=7.70288 | best_loss=7.67987
Epoch 23/80: current_loss=7.70561 | best_loss=7.67987
Epoch 24/80: current_loss=7.69803 | best_loss=7.67987
Epoch 25/80: current_loss=7.71955 | best_loss=7.67987
Early Stopping at epoch 25
      explained_var=0.00016 | mse_loss=7.85577
----------------------------------------------
Average early_stopping_point: 28| avg_exp_var=-0.00210| avg_loss=8.16078
----------------------------------------------


----------------------------------------------
Params for Trial 3
{'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=76.32893 | best_loss=76.32893
Epoch 1/80: current_loss=67.88653 | best_loss=67.88653
Epoch 2/80: current_loss=55.52251 | best_loss=55.52251
Epoch 3/80: current_loss=44.40129 | best_loss=44.40129
Epoch 4/80: current_loss=36.21532 | best_loss=36.21532
Epoch 5/80: current_loss=30.44237 | best_loss=30.44237
Epoch 6/80: current_loss=26.09279 | best_loss=26.09279
Epoch 7/80: current_loss=22.61903 | best_loss=22.61903
Epoch 8/80: current_loss=19.85316 | best_loss=19.85316
Epoch 9/80: current_loss=17.53740 | best_loss=17.53740
Epoch 10/80: current_loss=15.59544 | best_loss=15.59544
Epoch 11/80: current_loss=14.04190 | best_loss=14.04190
Epoch 12/80: current_loss=12.77119 | best_loss=12.77119
Epoch 13/80: current_loss=11.76322 | best_loss=11.76322
Epoch 14/80: current_loss=10.92748 | best_loss=10.92748
Epoch 15/80: current_loss=10.29324 | best_loss=10.29324
Epoch 16/80: current_loss=9.77755 | best_loss=9.77755
Epoch 17/80: current_loss=9.36542 | best_loss=9.36542
Epoch 18/80: current_loss=9.05617 | best_loss=9.05617
Epoch 19/80: current_loss=8.83968 | best_loss=8.83968
Epoch 20/80: current_loss=8.67656 | best_loss=8.67656
Epoch 21/80: current_loss=8.56498 | best_loss=8.56498
Epoch 22/80: current_loss=8.47970 | best_loss=8.47970
Epoch 23/80: current_loss=8.42737 | best_loss=8.42737
Epoch 24/80: current_loss=8.39054 | best_loss=8.39054
Epoch 25/80: current_loss=8.36892 | best_loss=8.36892
Epoch 26/80: current_loss=8.35560 | best_loss=8.35560
Epoch 27/80: current_loss=8.34915 | best_loss=8.34915
Epoch 28/80: current_loss=8.35010 | best_loss=8.34915
Epoch 29/80: current_loss=8.34803 | best_loss=8.34803
Epoch 30/80: current_loss=8.34430 | best_loss=8.34430
Epoch 31/80: current_loss=8.34519 | best_loss=8.34430
Epoch 32/80: current_loss=8.35610 | best_loss=8.34430
Epoch 33/80: current_loss=8.35382 | best_loss=8.34430
Epoch 34/80: current_loss=8.34878 | best_loss=8.34430
Epoch 35/80: current_loss=8.34768 | best_loss=8.34430
Epoch 36/80: current_loss=8.34974 | best_loss=8.34430
Epoch 37/80: current_loss=8.34792 | best_loss=8.34430
Epoch 38/80: current_loss=8.35272 | best_loss=8.34430
Epoch 39/80: current_loss=8.34206 | best_loss=8.34206
Epoch 40/80: current_loss=8.35294 | best_loss=8.34206
Epoch 41/80: current_loss=8.34413 | best_loss=8.34206
Epoch 42/80: current_loss=8.34727 | best_loss=8.34206
Epoch 43/80: current_loss=8.33096 | best_loss=8.33096
Epoch 44/80: current_loss=8.32989 | best_loss=8.32989
Epoch 45/80: current_loss=8.32905 | best_loss=8.32905
Epoch 46/80: current_loss=8.32086 | best_loss=8.32086
Epoch 47/80: current_loss=8.32190 | best_loss=8.32086
Epoch 48/80: current_loss=8.31612 | best_loss=8.31612
Epoch 49/80: current_loss=8.30239 | best_loss=8.30239
Epoch 50/80: current_loss=8.28396 | best_loss=8.28396
Epoch 51/80: current_loss=8.26492 | best_loss=8.26492
Epoch 52/80: current_loss=8.27797 | best_loss=8.26492
Epoch 53/80: current_loss=8.27753 | best_loss=8.26492
Epoch 54/80: current_loss=8.25852 | best_loss=8.25852
Epoch 55/80: current_loss=8.24804 | best_loss=8.24804
Epoch 56/80: current_loss=8.23236 | best_loss=8.23236
Epoch 57/80: current_loss=8.22268 | best_loss=8.22268
Epoch 58/80: current_loss=8.22097 | best_loss=8.22097
Epoch 59/80: current_loss=8.21481 | best_loss=8.21481
Epoch 60/80: current_loss=8.21335 | best_loss=8.21335
Epoch 61/80: current_loss=8.21678 | best_loss=8.21335
Epoch 62/80: current_loss=8.20270 | best_loss=8.20270
Epoch 63/80: current_loss=8.17735 | best_loss=8.17735
Epoch 64/80: current_loss=8.14780 | best_loss=8.14780
Epoch 65/80: current_loss=8.13949 | best_loss=8.13949
Epoch 66/80: current_loss=8.13156 | best_loss=8.13156
Epoch 67/80: current_loss=8.14054 | best_loss=8.13156
Epoch 68/80: current_loss=8.16804 | best_loss=8.13156
Epoch 69/80: current_loss=8.16109 | best_loss=8.13156
Epoch 70/80: current_loss=8.14138 | best_loss=8.13156
Epoch 71/80: current_loss=8.13145 | best_loss=8.13145
Epoch 72/80: current_loss=8.13197 | best_loss=8.13145
Epoch 73/80: current_loss=8.12282 | best_loss=8.12282
Epoch 74/80: current_loss=8.14825 | best_loss=8.12282
Epoch 75/80: current_loss=8.14560 | best_loss=8.12282
Epoch 76/80: current_loss=8.12550 | best_loss=8.12282
Epoch 77/80: current_loss=8.11606 | best_loss=8.11606
Epoch 78/80: current_loss=8.13872 | best_loss=8.11606
Epoch 79/80: current_loss=8.14178 | best_loss=8.11606
      explained_var=-0.01907 | mse_loss=7.99100
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.88497 | best_loss=8.88497
Epoch 1/80: current_loss=8.85799 | best_loss=8.85799
Epoch 2/80: current_loss=8.82751 | best_loss=8.82751
Epoch 3/80: current_loss=8.80431 | best_loss=8.80431
Epoch 4/80: current_loss=8.80031 | best_loss=8.80031
Epoch 5/80: current_loss=8.82948 | best_loss=8.80031
Epoch 6/80: current_loss=8.79063 | best_loss=8.79063
Epoch 7/80: current_loss=8.76403 | best_loss=8.76403
Epoch 8/80: current_loss=8.73466 | best_loss=8.73466
Epoch 9/80: current_loss=8.73316 | best_loss=8.73316
Epoch 10/80: current_loss=8.74302 | best_loss=8.73316
Epoch 11/80: current_loss=8.78435 | best_loss=8.73316
Epoch 12/80: current_loss=8.70526 | best_loss=8.70526
Epoch 13/80: current_loss=8.66314 | best_loss=8.66314
Epoch 14/80: current_loss=8.64928 | best_loss=8.64928
Epoch 15/80: current_loss=8.62922 | best_loss=8.62922
Epoch 16/80: current_loss=8.66764 | best_loss=8.62922
Epoch 17/80: current_loss=8.64835 | best_loss=8.62922
Epoch 18/80: current_loss=8.62801 | best_loss=8.62801
Epoch 19/80: current_loss=8.61441 | best_loss=8.61441
Epoch 20/80: current_loss=8.61257 | best_loss=8.61257
Epoch 21/80: current_loss=8.57873 | best_loss=8.57873
Epoch 22/80: current_loss=8.56106 | best_loss=8.56106
Epoch 23/80: current_loss=8.56108 | best_loss=8.56106
Epoch 24/80: current_loss=8.54079 | best_loss=8.54079
Epoch 25/80: current_loss=8.52936 | best_loss=8.52936
Epoch 26/80: current_loss=8.54657 | best_loss=8.52936
Epoch 27/80: current_loss=8.50033 | best_loss=8.50033
Epoch 28/80: current_loss=8.48962 | best_loss=8.48962
Epoch 29/80: current_loss=8.48362 | best_loss=8.48362
Epoch 30/80: current_loss=8.48357 | best_loss=8.48357
Epoch 31/80: current_loss=8.49479 | best_loss=8.48357
Epoch 32/80: current_loss=8.49824 | best_loss=8.48357
Epoch 33/80: current_loss=8.49065 | best_loss=8.48357
Epoch 34/80: current_loss=8.49370 | best_loss=8.48357
Epoch 35/80: current_loss=8.51171 | best_loss=8.48357
Epoch 36/80: current_loss=8.48450 | best_loss=8.48357
Epoch 37/80: current_loss=8.47732 | best_loss=8.47732
Epoch 38/80: current_loss=8.45483 | best_loss=8.45483
Epoch 39/80: current_loss=8.44961 | best_loss=8.44961
Epoch 40/80: current_loss=8.46813 | best_loss=8.44961
Epoch 41/80: current_loss=8.44079 | best_loss=8.44079
Epoch 42/80: current_loss=8.43264 | best_loss=8.43264
Epoch 43/80: current_loss=8.43055 | best_loss=8.43055
Epoch 44/80: current_loss=8.45930 | best_loss=8.43055
Epoch 45/80: current_loss=8.42480 | best_loss=8.42480
Epoch 46/80: current_loss=8.41972 | best_loss=8.41972
Epoch 47/80: current_loss=8.42413 | best_loss=8.41972
Epoch 48/80: current_loss=8.45134 | best_loss=8.41972
Epoch 49/80: current_loss=8.48336 | best_loss=8.41972
Epoch 50/80: current_loss=8.43854 | best_loss=8.41972
Epoch 51/80: current_loss=8.44603 | best_loss=8.41972
Epoch 52/80: current_loss=8.48981 | best_loss=8.41972
Epoch 53/80: current_loss=8.45497 | best_loss=8.41972
Epoch 54/80: current_loss=8.45815 | best_loss=8.41972
Epoch 55/80: current_loss=8.46857 | best_loss=8.41972
Epoch 56/80: current_loss=8.41486 | best_loss=8.41486
Epoch 57/80: current_loss=8.45325 | best_loss=8.41486
Epoch 58/80: current_loss=8.41793 | best_loss=8.41486
Epoch 59/80: current_loss=8.38380 | best_loss=8.38380
Epoch 60/80: current_loss=8.41737 | best_loss=8.38380
Epoch 61/80: current_loss=8.43661 | best_loss=8.38380
Epoch 62/80: current_loss=8.36798 | best_loss=8.36798
Epoch 63/80: current_loss=8.37208 | best_loss=8.36798
Epoch 64/80: current_loss=8.39659 | best_loss=8.36798
Epoch 65/80: current_loss=8.36015 | best_loss=8.36015
Epoch 66/80: current_loss=8.35744 | best_loss=8.35744
Epoch 67/80: current_loss=8.36562 | best_loss=8.35744
Epoch 68/80: current_loss=8.34285 | best_loss=8.34285
Epoch 69/80: current_loss=8.35383 | best_loss=8.34285
Epoch 70/80: current_loss=8.35658 | best_loss=8.34285
Epoch 71/80: current_loss=8.32705 | best_loss=8.32705
Epoch 72/80: current_loss=8.33340 | best_loss=8.32705
Epoch 73/80: current_loss=8.32713 | best_loss=8.32705
Epoch 74/80: current_loss=8.36903 | best_loss=8.32705
Epoch 75/80: current_loss=8.33573 | best_loss=8.32705
Epoch 76/80: current_loss=8.34001 | best_loss=8.32705
Epoch 77/80: current_loss=8.30598 | best_loss=8.30598
Epoch 78/80: current_loss=8.31702 | best_loss=8.30598
Epoch 79/80: current_loss=8.31639 | best_loss=8.30598
      explained_var=0.00055 | mse_loss=8.16686
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.01254 | best_loss=9.01254
Epoch 1/80: current_loss=9.00559 | best_loss=9.00559
Epoch 2/80: current_loss=9.03010 | best_loss=9.00559
Epoch 3/80: current_loss=9.00107 | best_loss=9.00107
Epoch 4/80: current_loss=9.00259 | best_loss=9.00107
Epoch 5/80: current_loss=9.18452 | best_loss=9.00107
Epoch 6/80: current_loss=9.11651 | best_loss=9.00107
Epoch 7/80: current_loss=9.05449 | best_loss=9.00107
Epoch 8/80: current_loss=9.06123 | best_loss=9.00107
Epoch 9/80: current_loss=8.93258 | best_loss=8.93258
Epoch 10/80: current_loss=9.07699 | best_loss=8.93258
Epoch 11/80: current_loss=8.90106 | best_loss=8.90106
Epoch 12/80: current_loss=8.83011 | best_loss=8.83011
Epoch 13/80: current_loss=8.94277 | best_loss=8.83011
Epoch 14/80: current_loss=9.00758 | best_loss=8.83011
Epoch 15/80: current_loss=8.93432 | best_loss=8.83011
Epoch 16/80: current_loss=8.85562 | best_loss=8.83011
Epoch 17/80: current_loss=8.90652 | best_loss=8.83011
Epoch 18/80: current_loss=8.98127 | best_loss=8.83011
Epoch 19/80: current_loss=8.94808 | best_loss=8.83011
Epoch 20/80: current_loss=8.96811 | best_loss=8.83011
Epoch 21/80: current_loss=8.91452 | best_loss=8.83011
Epoch 22/80: current_loss=9.04891 | best_loss=8.83011
Epoch 23/80: current_loss=9.00602 | best_loss=8.83011
Epoch 24/80: current_loss=8.91629 | best_loss=8.83011
Epoch 25/80: current_loss=8.99222 | best_loss=8.83011
Epoch 26/80: current_loss=8.97353 | best_loss=8.83011
Epoch 27/80: current_loss=8.96312 | best_loss=8.83011
Epoch 28/80: current_loss=8.95253 | best_loss=8.83011
Epoch 29/80: current_loss=8.96014 | best_loss=8.83011
Epoch 30/80: current_loss=8.88390 | best_loss=8.83011
Epoch 31/80: current_loss=8.91591 | best_loss=8.83011
Epoch 32/80: current_loss=8.98480 | best_loss=8.83011
Early Stopping at epoch 32
      explained_var=0.00564 | mse_loss=8.62155
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.30785 | best_loss=8.30785
Epoch 1/80: current_loss=8.31632 | best_loss=8.30785
Epoch 2/80: current_loss=8.28505 | best_loss=8.28505
Epoch 3/80: current_loss=8.31978 | best_loss=8.28505
Epoch 4/80: current_loss=8.38474 | best_loss=8.28505
Epoch 5/80: current_loss=8.29403 | best_loss=8.28505
Epoch 6/80: current_loss=8.25859 | best_loss=8.25859
Epoch 7/80: current_loss=8.28541 | best_loss=8.25859
Epoch 8/80: current_loss=8.31085 | best_loss=8.25859
Epoch 9/80: current_loss=8.29162 | best_loss=8.25859
Epoch 10/80: current_loss=8.27526 | best_loss=8.25859
Epoch 11/80: current_loss=8.30820 | best_loss=8.25859
Epoch 12/80: current_loss=8.26994 | best_loss=8.25859
Epoch 13/80: current_loss=8.27274 | best_loss=8.25859
Epoch 14/80: current_loss=8.29874 | best_loss=8.25859
Epoch 15/80: current_loss=8.28034 | best_loss=8.25859
Epoch 16/80: current_loss=8.25960 | best_loss=8.25859
Epoch 17/80: current_loss=8.30628 | best_loss=8.25859
Epoch 18/80: current_loss=8.35472 | best_loss=8.25859
Epoch 19/80: current_loss=8.42603 | best_loss=8.25859
Epoch 20/80: current_loss=8.33310 | best_loss=8.25859
Epoch 21/80: current_loss=8.30571 | best_loss=8.25859
Epoch 22/80: current_loss=8.31652 | best_loss=8.25859
Epoch 23/80: current_loss=8.28924 | best_loss=8.25859
Epoch 24/80: current_loss=8.30647 | best_loss=8.25859
Epoch 25/80: current_loss=8.29857 | best_loss=8.25859
Epoch 26/80: current_loss=8.34055 | best_loss=8.25859
Early Stopping at epoch 26
      explained_var=-0.00215 | mse_loss=8.35619
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.94000 | best_loss=7.94000
Epoch 1/80: current_loss=7.90507 | best_loss=7.90507
Epoch 2/80: current_loss=7.95712 | best_loss=7.90507
Epoch 3/80: current_loss=7.93033 | best_loss=7.90507
Epoch 4/80: current_loss=7.94507 | best_loss=7.90507
Epoch 5/80: current_loss=7.96148 | best_loss=7.90507
Epoch 6/80: current_loss=7.95991 | best_loss=7.90507
Epoch 7/80: current_loss=8.01914 | best_loss=7.90507
Epoch 8/80: current_loss=7.99939 | best_loss=7.90507
Epoch 9/80: current_loss=7.98746 | best_loss=7.90507
Epoch 10/80: current_loss=7.99430 | best_loss=7.90507
Epoch 11/80: current_loss=8.05077 | best_loss=7.90507
Epoch 12/80: current_loss=7.99320 | best_loss=7.90507
Epoch 13/80: current_loss=8.00810 | best_loss=7.90507
Epoch 14/80: current_loss=8.00168 | best_loss=7.90507
Epoch 15/80: current_loss=8.10507 | best_loss=7.90507
Epoch 16/80: current_loss=8.06458 | best_loss=7.90507
Epoch 17/80: current_loss=8.09572 | best_loss=7.90507
Epoch 18/80: current_loss=8.03405 | best_loss=7.90507
Epoch 19/80: current_loss=8.02761 | best_loss=7.90507
Epoch 20/80: current_loss=8.01995 | best_loss=7.90507
Epoch 21/80: current_loss=8.03999 | best_loss=7.90507
Early Stopping at epoch 21
      explained_var=-0.02930 | mse_loss=8.09056
----------------------------------------------
Average early_stopping_point: 35| avg_exp_var=-0.00886| avg_loss=8.24523
----------------------------------------------


----------------------------------------------
Params for Trial 4
{'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=71.42726 | best_loss=71.42726
Epoch 1/80: current_loss=54.61329 | best_loss=54.61329
Epoch 2/80: current_loss=38.85438 | best_loss=38.85438
Epoch 3/80: current_loss=30.17015 | best_loss=30.17015
Epoch 4/80: current_loss=24.78258 | best_loss=24.78258
Epoch 5/80: current_loss=20.97242 | best_loss=20.97242
Epoch 6/80: current_loss=18.03671 | best_loss=18.03671
Epoch 7/80: current_loss=15.70155 | best_loss=15.70155
Epoch 8/80: current_loss=13.86351 | best_loss=13.86351
Epoch 9/80: current_loss=12.31812 | best_loss=12.31812
Epoch 10/80: current_loss=11.17302 | best_loss=11.17302
Epoch 11/80: current_loss=10.27655 | best_loss=10.27655
Epoch 12/80: current_loss=9.58078 | best_loss=9.58078
Epoch 13/80: current_loss=9.01546 | best_loss=9.01546
Epoch 14/80: current_loss=8.59616 | best_loss=8.59616
Epoch 15/80: current_loss=8.29955 | best_loss=8.29955
Epoch 16/80: current_loss=8.07271 | best_loss=8.07271
Epoch 17/80: current_loss=7.91975 | best_loss=7.91975
Epoch 18/80: current_loss=7.82354 | best_loss=7.82354
Epoch 19/80: current_loss=7.77003 | best_loss=7.77003
Epoch 20/80: current_loss=7.75140 | best_loss=7.75140
Epoch 21/80: current_loss=7.75388 | best_loss=7.75140
Epoch 22/80: current_loss=7.77188 | best_loss=7.75140
Epoch 23/80: current_loss=7.79088 | best_loss=7.75140
Epoch 24/80: current_loss=7.82053 | best_loss=7.75140
Epoch 25/80: current_loss=7.84269 | best_loss=7.75140
Epoch 26/80: current_loss=7.85122 | best_loss=7.75140
Epoch 27/80: current_loss=7.87105 | best_loss=7.75140
Epoch 28/80: current_loss=7.88320 | best_loss=7.75140
Epoch 29/80: current_loss=7.91870 | best_loss=7.75140
Epoch 30/80: current_loss=7.93506 | best_loss=7.75140
Epoch 31/80: current_loss=7.94512 | best_loss=7.75140
Epoch 32/80: current_loss=7.94592 | best_loss=7.75140
Epoch 33/80: current_loss=7.96483 | best_loss=7.75140
Epoch 34/80: current_loss=7.96570 | best_loss=7.75140
Epoch 35/80: current_loss=7.96164 | best_loss=7.75140
Epoch 36/80: current_loss=7.98100 | best_loss=7.75140
Epoch 37/80: current_loss=7.99430 | best_loss=7.75140
Epoch 38/80: current_loss=8.02732 | best_loss=7.75140
Epoch 39/80: current_loss=8.02366 | best_loss=7.75140
Epoch 40/80: current_loss=8.04594 | best_loss=7.75140
Early Stopping at epoch 40
      explained_var=0.00183 | mse_loss=7.57351
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.43105 | best_loss=8.43105
Epoch 1/80: current_loss=8.43316 | best_loss=8.43105
Epoch 2/80: current_loss=8.43708 | best_loss=8.43105
Epoch 3/80: current_loss=8.43064 | best_loss=8.43064
Epoch 4/80: current_loss=8.40173 | best_loss=8.40173
Epoch 5/80: current_loss=8.38647 | best_loss=8.38647
Epoch 6/80: current_loss=8.38740 | best_loss=8.38647
Epoch 7/80: current_loss=8.38186 | best_loss=8.38186
Epoch 8/80: current_loss=8.41062 | best_loss=8.38186
Epoch 9/80: current_loss=8.39551 | best_loss=8.38186
Epoch 10/80: current_loss=8.37358 | best_loss=8.37358
Epoch 11/80: current_loss=8.36836 | best_loss=8.36836
Epoch 12/80: current_loss=8.38091 | best_loss=8.36836
Epoch 13/80: current_loss=8.36806 | best_loss=8.36806
Epoch 14/80: current_loss=8.36895 | best_loss=8.36806
Epoch 15/80: current_loss=8.37590 | best_loss=8.36806
Epoch 16/80: current_loss=8.41208 | best_loss=8.36806
Epoch 17/80: current_loss=8.42217 | best_loss=8.36806
Epoch 18/80: current_loss=8.35137 | best_loss=8.35137
Epoch 19/80: current_loss=8.35664 | best_loss=8.35137
Epoch 20/80: current_loss=8.34106 | best_loss=8.34106
Epoch 21/80: current_loss=8.34897 | best_loss=8.34106
Epoch 22/80: current_loss=8.34816 | best_loss=8.34106
Epoch 23/80: current_loss=8.35991 | best_loss=8.34106
Epoch 24/80: current_loss=8.35782 | best_loss=8.34106
Epoch 25/80: current_loss=8.35871 | best_loss=8.34106
Epoch 26/80: current_loss=8.35605 | best_loss=8.34106
Epoch 27/80: current_loss=8.36480 | best_loss=8.34106
Epoch 28/80: current_loss=8.38562 | best_loss=8.34106
Epoch 29/80: current_loss=8.37937 | best_loss=8.34106
Epoch 30/80: current_loss=8.36844 | best_loss=8.34106
Epoch 31/80: current_loss=8.34455 | best_loss=8.34106
Epoch 32/80: current_loss=8.36252 | best_loss=8.34106
Epoch 33/80: current_loss=8.36332 | best_loss=8.34106
Epoch 34/80: current_loss=8.35897 | best_loss=8.34106
Epoch 35/80: current_loss=8.36065 | best_loss=8.34106
Epoch 36/80: current_loss=8.39759 | best_loss=8.34106
Epoch 37/80: current_loss=8.38801 | best_loss=8.34106
Epoch 38/80: current_loss=8.38419 | best_loss=8.34106
Epoch 39/80: current_loss=8.36661 | best_loss=8.34106
Epoch 40/80: current_loss=8.37445 | best_loss=8.34106
Early Stopping at epoch 40
      explained_var=-0.00413 | mse_loss=8.19884
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.06379 | best_loss=9.06379
Epoch 1/80: current_loss=9.00070 | best_loss=9.00070
Epoch 2/80: current_loss=8.95861 | best_loss=8.95861
Epoch 3/80: current_loss=8.96906 | best_loss=8.95861
Epoch 4/80: current_loss=8.99510 | best_loss=8.95861
Epoch 5/80: current_loss=9.02669 | best_loss=8.95861
Epoch 6/80: current_loss=9.07666 | best_loss=8.95861
Epoch 7/80: current_loss=9.19257 | best_loss=8.95861
Epoch 8/80: current_loss=9.09315 | best_loss=8.95861
Epoch 9/80: current_loss=9.06218 | best_loss=8.95861
Epoch 10/80: current_loss=9.03161 | best_loss=8.95861
Epoch 11/80: current_loss=9.04880 | best_loss=8.95861
Epoch 12/80: current_loss=9.08279 | best_loss=8.95861
Epoch 13/80: current_loss=9.08950 | best_loss=8.95861
Epoch 14/80: current_loss=9.11387 | best_loss=8.95861
Epoch 15/80: current_loss=9.10909 | best_loss=8.95861
Epoch 16/80: current_loss=9.09009 | best_loss=8.95861
Epoch 17/80: current_loss=8.98399 | best_loss=8.95861
Epoch 18/80: current_loss=8.97614 | best_loss=8.95861
Epoch 19/80: current_loss=9.06244 | best_loss=8.95861
Epoch 20/80: current_loss=9.09592 | best_loss=8.95861
Epoch 21/80: current_loss=9.10882 | best_loss=8.95861
Epoch 22/80: current_loss=9.13753 | best_loss=8.95861
Early Stopping at epoch 22
      explained_var=-0.00780 | mse_loss=8.74527
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.23317 | best_loss=8.23317
Epoch 1/80: current_loss=8.23847 | best_loss=8.23317
Epoch 2/80: current_loss=8.23044 | best_loss=8.23044
Epoch 3/80: current_loss=8.24073 | best_loss=8.23044
Epoch 4/80: current_loss=8.23455 | best_loss=8.23044
Epoch 5/80: current_loss=8.24298 | best_loss=8.23044
Epoch 6/80: current_loss=8.21758 | best_loss=8.21758
Epoch 7/80: current_loss=8.21843 | best_loss=8.21758
Epoch 8/80: current_loss=8.22343 | best_loss=8.21758
Epoch 9/80: current_loss=8.23102 | best_loss=8.21758
Epoch 10/80: current_loss=8.22579 | best_loss=8.21758
Epoch 11/80: current_loss=8.22197 | best_loss=8.21758
Epoch 12/80: current_loss=8.22313 | best_loss=8.21758
Epoch 13/80: current_loss=8.22286 | best_loss=8.21758
Epoch 14/80: current_loss=8.23491 | best_loss=8.21758
Epoch 15/80: current_loss=8.21861 | best_loss=8.21758
Epoch 16/80: current_loss=8.21845 | best_loss=8.21758
Epoch 17/80: current_loss=8.24034 | best_loss=8.21758
Epoch 18/80: current_loss=8.24350 | best_loss=8.21758
Epoch 19/80: current_loss=8.24629 | best_loss=8.21758
Epoch 20/80: current_loss=8.23420 | best_loss=8.21758
Epoch 21/80: current_loss=8.22715 | best_loss=8.21758
Epoch 22/80: current_loss=8.22532 | best_loss=8.21758
Epoch 23/80: current_loss=8.23156 | best_loss=8.21758
Epoch 24/80: current_loss=8.23170 | best_loss=8.21758
Epoch 25/80: current_loss=8.22326 | best_loss=8.21758
Epoch 26/80: current_loss=8.23752 | best_loss=8.21758
Early Stopping at epoch 26
      explained_var=0.00190 | mse_loss=8.31607
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.70617 | best_loss=7.70617
Epoch 1/80: current_loss=7.71378 | best_loss=7.70617
Epoch 2/80: current_loss=7.72154 | best_loss=7.70617
Epoch 3/80: current_loss=7.70103 | best_loss=7.70103
Epoch 4/80: current_loss=7.70113 | best_loss=7.70103
Epoch 5/80: current_loss=7.70234 | best_loss=7.70103
Epoch 6/80: current_loss=7.70110 | best_loss=7.70103
Epoch 7/80: current_loss=7.70132 | best_loss=7.70103
Epoch 8/80: current_loss=7.70060 | best_loss=7.70060
Epoch 9/80: current_loss=7.70474 | best_loss=7.70060
Epoch 10/80: current_loss=7.72106 | best_loss=7.70060
Epoch 11/80: current_loss=7.72176 | best_loss=7.70060
Epoch 12/80: current_loss=7.71002 | best_loss=7.70060
Epoch 13/80: current_loss=7.73844 | best_loss=7.70060
Epoch 14/80: current_loss=7.73257 | best_loss=7.70060
Epoch 15/80: current_loss=7.72503 | best_loss=7.70060
Epoch 16/80: current_loss=7.71218 | best_loss=7.70060
Epoch 17/80: current_loss=7.73711 | best_loss=7.70060
Epoch 18/80: current_loss=7.76056 | best_loss=7.70060
Epoch 19/80: current_loss=7.73670 | best_loss=7.70060
Epoch 20/80: current_loss=7.72573 | best_loss=7.70060
Epoch 21/80: current_loss=7.71247 | best_loss=7.70060
Epoch 22/80: current_loss=7.71460 | best_loss=7.70060
Epoch 23/80: current_loss=7.73349 | best_loss=7.70060
Epoch 24/80: current_loss=7.73591 | best_loss=7.70060
Epoch 25/80: current_loss=7.72643 | best_loss=7.70060
Epoch 26/80: current_loss=7.73518 | best_loss=7.70060
Epoch 27/80: current_loss=7.71096 | best_loss=7.70060
Epoch 28/80: current_loss=7.71460 | best_loss=7.70060
Early Stopping at epoch 28
      explained_var=-0.00247 | mse_loss=7.87776
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=-0.00213| avg_loss=8.14229
----------------------------------------------


----------------------------------------------
Params for Trial 5
{'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=40.26960 | best_loss=40.26960
Epoch 1/80: current_loss=16.55418 | best_loss=16.55418
Epoch 2/80: current_loss=10.76026 | best_loss=10.76026
Epoch 3/80: current_loss=10.10333 | best_loss=10.10333
Epoch 4/80: current_loss=10.40871 | best_loss=10.10333
Epoch 5/80: current_loss=10.30805 | best_loss=10.10333
Epoch 6/80: current_loss=9.95706 | best_loss=9.95706
Epoch 7/80: current_loss=9.73932 | best_loss=9.73932
Epoch 8/80: current_loss=9.65534 | best_loss=9.65534
Epoch 9/80: current_loss=9.52753 | best_loss=9.52753
Epoch 10/80: current_loss=9.52417 | best_loss=9.52417
Epoch 11/80: current_loss=9.40690 | best_loss=9.40690
Epoch 12/80: current_loss=9.32920 | best_loss=9.32920
Epoch 13/80: current_loss=9.28385 | best_loss=9.28385
Epoch 14/80: current_loss=9.25752 | best_loss=9.25752
Epoch 15/80: current_loss=9.29705 | best_loss=9.25752
Epoch 16/80: current_loss=9.23278 | best_loss=9.23278
Epoch 17/80: current_loss=9.16613 | best_loss=9.16613
Epoch 18/80: current_loss=9.15978 | best_loss=9.15978
Epoch 19/80: current_loss=9.09762 | best_loss=9.09762
Epoch 20/80: current_loss=9.01060 | best_loss=9.01060
Epoch 21/80: current_loss=8.98831 | best_loss=8.98831
Epoch 22/80: current_loss=8.92961 | best_loss=8.92961
Epoch 23/80: current_loss=9.01699 | best_loss=8.92961
Epoch 24/80: current_loss=9.06405 | best_loss=8.92961
Epoch 25/80: current_loss=8.99653 | best_loss=8.92961
Epoch 26/80: current_loss=8.88856 | best_loss=8.88856
Epoch 27/80: current_loss=8.87387 | best_loss=8.87387
Epoch 28/80: current_loss=8.90000 | best_loss=8.87387
Epoch 29/80: current_loss=8.71707 | best_loss=8.71707
Epoch 30/80: current_loss=8.77463 | best_loss=8.71707
Epoch 31/80: current_loss=8.75874 | best_loss=8.71707
Epoch 32/80: current_loss=8.60944 | best_loss=8.60944
Epoch 33/80: current_loss=8.65849 | best_loss=8.60944
Epoch 34/80: current_loss=8.77626 | best_loss=8.60944
Epoch 35/80: current_loss=8.77816 | best_loss=8.60944
Epoch 36/80: current_loss=8.69453 | best_loss=8.60944
Epoch 37/80: current_loss=8.73913 | best_loss=8.60944
Epoch 38/80: current_loss=8.55786 | best_loss=8.55786
Epoch 39/80: current_loss=8.51104 | best_loss=8.51104
Epoch 40/80: current_loss=8.52853 | best_loss=8.51104
Epoch 41/80: current_loss=8.57059 | best_loss=8.51104
Epoch 42/80: current_loss=8.54337 | best_loss=8.51104
Epoch 43/80: current_loss=8.50824 | best_loss=8.50824
Epoch 44/80: current_loss=8.65151 | best_loss=8.50824
Epoch 45/80: current_loss=8.36586 | best_loss=8.36586
Epoch 46/80: current_loss=8.44005 | best_loss=8.36586
Epoch 47/80: current_loss=8.47604 | best_loss=8.36586
Epoch 48/80: current_loss=8.34491 | best_loss=8.34491
Epoch 49/80: current_loss=8.41877 | best_loss=8.34491
Epoch 50/80: current_loss=8.54044 | best_loss=8.34491
Epoch 51/80: current_loss=8.34175 | best_loss=8.34175
Epoch 52/80: current_loss=8.36884 | best_loss=8.34175
Epoch 53/80: current_loss=8.26634 | best_loss=8.26634
Epoch 54/80: current_loss=8.44290 | best_loss=8.26634
Epoch 55/80: current_loss=8.43978 | best_loss=8.26634
Epoch 56/80: current_loss=8.36298 | best_loss=8.26634
Epoch 57/80: current_loss=8.33669 | best_loss=8.26634
Epoch 58/80: current_loss=8.31114 | best_loss=8.26634
Epoch 59/80: current_loss=8.41039 | best_loss=8.26634
Epoch 60/80: current_loss=8.23585 | best_loss=8.23585
Epoch 61/80: current_loss=8.25139 | best_loss=8.23585
Epoch 62/80: current_loss=8.30212 | best_loss=8.23585
Epoch 63/80: current_loss=8.30046 | best_loss=8.23585
Epoch 64/80: current_loss=8.27880 | best_loss=8.23585
Epoch 65/80: current_loss=8.34698 | best_loss=8.23585
Epoch 66/80: current_loss=8.22395 | best_loss=8.22395
Epoch 67/80: current_loss=8.24076 | best_loss=8.22395
Epoch 68/80: current_loss=8.14545 | best_loss=8.14545
Epoch 69/80: current_loss=8.34880 | best_loss=8.14545
Epoch 70/80: current_loss=8.24911 | best_loss=8.14545
Epoch 71/80: current_loss=8.10564 | best_loss=8.10564
Epoch 72/80: current_loss=8.30745 | best_loss=8.10564
Epoch 73/80: current_loss=8.22536 | best_loss=8.10564
Epoch 74/80: current_loss=8.05235 | best_loss=8.05235
Epoch 75/80: current_loss=8.08530 | best_loss=8.05235
Epoch 76/80: current_loss=8.20144 | best_loss=8.05235
Epoch 77/80: current_loss=8.15255 | best_loss=8.05235
Epoch 78/80: current_loss=8.12946 | best_loss=8.05235
Epoch 79/80: current_loss=8.19463 | best_loss=8.05235
      explained_var=-0.01732 | mse_loss=7.91820
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.78866 | best_loss=8.78866
Epoch 1/80: current_loss=8.72221 | best_loss=8.72221
Epoch 2/80: current_loss=8.69944 | best_loss=8.69944
Epoch 3/80: current_loss=8.65167 | best_loss=8.65167
Epoch 4/80: current_loss=8.61193 | best_loss=8.61193
Epoch 5/80: current_loss=8.60069 | best_loss=8.60069
Epoch 6/80: current_loss=8.57924 | best_loss=8.57924
Epoch 7/80: current_loss=8.58679 | best_loss=8.57924
Epoch 8/80: current_loss=8.55197 | best_loss=8.55197
Epoch 9/80: current_loss=8.55109 | best_loss=8.55109
Epoch 10/80: current_loss=8.51687 | best_loss=8.51687
Epoch 11/80: current_loss=8.53053 | best_loss=8.51687
Epoch 12/80: current_loss=8.51714 | best_loss=8.51687
Epoch 13/80: current_loss=8.49044 | best_loss=8.49044
Epoch 14/80: current_loss=8.48529 | best_loss=8.48529
Epoch 15/80: current_loss=8.48307 | best_loss=8.48307
Epoch 16/80: current_loss=8.46625 | best_loss=8.46625
Epoch 17/80: current_loss=8.43609 | best_loss=8.43609
Epoch 18/80: current_loss=8.43876 | best_loss=8.43609
Epoch 19/80: current_loss=8.40793 | best_loss=8.40793
Epoch 20/80: current_loss=8.43874 | best_loss=8.40793
Epoch 21/80: current_loss=8.39177 | best_loss=8.39177
Epoch 22/80: current_loss=8.42653 | best_loss=8.39177
Epoch 23/80: current_loss=8.48770 | best_loss=8.39177
Epoch 24/80: current_loss=8.39279 | best_loss=8.39177
Epoch 25/80: current_loss=8.39785 | best_loss=8.39177
Epoch 26/80: current_loss=8.40890 | best_loss=8.39177
Epoch 27/80: current_loss=8.40287 | best_loss=8.39177
Epoch 28/80: current_loss=8.44746 | best_loss=8.39177
Epoch 29/80: current_loss=8.38779 | best_loss=8.38779
Epoch 30/80: current_loss=8.38042 | best_loss=8.38042
Epoch 31/80: current_loss=8.39465 | best_loss=8.38042
Epoch 32/80: current_loss=8.44194 | best_loss=8.38042
Epoch 33/80: current_loss=8.40317 | best_loss=8.38042
Epoch 34/80: current_loss=8.41400 | best_loss=8.38042
Epoch 35/80: current_loss=8.40781 | best_loss=8.38042
Epoch 36/80: current_loss=8.40864 | best_loss=8.38042
Epoch 37/80: current_loss=8.39412 | best_loss=8.38042
Epoch 38/80: current_loss=8.41220 | best_loss=8.38042
Epoch 39/80: current_loss=8.40859 | best_loss=8.38042
Epoch 40/80: current_loss=8.42082 | best_loss=8.38042
Epoch 41/80: current_loss=8.45149 | best_loss=8.38042
Epoch 42/80: current_loss=8.43238 | best_loss=8.38042
Epoch 43/80: current_loss=8.41343 | best_loss=8.38042
Epoch 44/80: current_loss=8.44362 | best_loss=8.38042
Epoch 45/80: current_loss=8.44039 | best_loss=8.38042
Epoch 46/80: current_loss=8.42711 | best_loss=8.38042
Epoch 47/80: current_loss=8.37967 | best_loss=8.37967
Epoch 48/80: current_loss=8.36546 | best_loss=8.36546
Epoch 49/80: current_loss=8.33284 | best_loss=8.33284
Epoch 50/80: current_loss=8.33126 | best_loss=8.33126
Epoch 51/80: current_loss=8.32122 | best_loss=8.32122
Epoch 52/80: current_loss=8.32452 | best_loss=8.32122
Epoch 53/80: current_loss=8.36930 | best_loss=8.32122
Epoch 54/80: current_loss=8.34530 | best_loss=8.32122
Epoch 55/80: current_loss=8.33668 | best_loss=8.32122
Epoch 56/80: current_loss=8.37975 | best_loss=8.32122
Epoch 57/80: current_loss=8.35435 | best_loss=8.32122
Epoch 58/80: current_loss=8.33237 | best_loss=8.32122
Epoch 59/80: current_loss=8.36323 | best_loss=8.32122
Epoch 60/80: current_loss=8.35752 | best_loss=8.32122
Epoch 61/80: current_loss=8.32096 | best_loss=8.32096
Epoch 62/80: current_loss=8.29509 | best_loss=8.29509
Epoch 63/80: current_loss=8.29350 | best_loss=8.29350
Epoch 64/80: current_loss=8.30751 | best_loss=8.29350
Epoch 65/80: current_loss=8.30155 | best_loss=8.29350
Epoch 66/80: current_loss=8.31206 | best_loss=8.29350
Epoch 67/80: current_loss=8.33897 | best_loss=8.29350
Epoch 68/80: current_loss=8.30900 | best_loss=8.29350
Epoch 69/80: current_loss=8.32560 | best_loss=8.29350
Epoch 70/80: current_loss=8.33960 | best_loss=8.29350
Epoch 71/80: current_loss=8.31519 | best_loss=8.29350
Epoch 72/80: current_loss=8.29509 | best_loss=8.29350
Epoch 73/80: current_loss=8.30889 | best_loss=8.29350
Epoch 74/80: current_loss=8.31400 | best_loss=8.29350
Epoch 75/80: current_loss=8.34255 | best_loss=8.29350
Epoch 76/80: current_loss=8.32588 | best_loss=8.29350
Epoch 77/80: current_loss=8.30603 | best_loss=8.29350
Epoch 78/80: current_loss=8.42631 | best_loss=8.29350
Epoch 79/80: current_loss=8.29614 | best_loss=8.29350
      explained_var=0.00200 | mse_loss=8.15021
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.91287 | best_loss=8.91287
Epoch 1/80: current_loss=9.16259 | best_loss=8.91287
Epoch 2/80: current_loss=8.83342 | best_loss=8.83342
Epoch 3/80: current_loss=8.80322 | best_loss=8.80322
Epoch 4/80: current_loss=9.08793 | best_loss=8.80322
Epoch 5/80: current_loss=8.98789 | best_loss=8.80322
Epoch 6/80: current_loss=8.97914 | best_loss=8.80322
Epoch 7/80: current_loss=8.78068 | best_loss=8.78068
Epoch 8/80: current_loss=9.14162 | best_loss=8.78068
Epoch 9/80: current_loss=9.09380 | best_loss=8.78068
Epoch 10/80: current_loss=8.78296 | best_loss=8.78068
Epoch 11/80: current_loss=9.10734 | best_loss=8.78068
Epoch 12/80: current_loss=8.91829 | best_loss=8.78068
Epoch 13/80: current_loss=8.93362 | best_loss=8.78068
Epoch 14/80: current_loss=8.95710 | best_loss=8.78068
Epoch 15/80: current_loss=9.10836 | best_loss=8.78068
Epoch 16/80: current_loss=8.82727 | best_loss=8.78068
Epoch 17/80: current_loss=8.96921 | best_loss=8.78068
Epoch 18/80: current_loss=8.98672 | best_loss=8.78068
Epoch 19/80: current_loss=9.11482 | best_loss=8.78068
Epoch 20/80: current_loss=8.76349 | best_loss=8.76349
Epoch 21/80: current_loss=8.95993 | best_loss=8.76349
Epoch 22/80: current_loss=9.04326 | best_loss=8.76349
Epoch 23/80: current_loss=8.84033 | best_loss=8.76349
Epoch 24/80: current_loss=8.96226 | best_loss=8.76349
Epoch 25/80: current_loss=9.03497 | best_loss=8.76349
Epoch 26/80: current_loss=9.02634 | best_loss=8.76349
Epoch 27/80: current_loss=8.92913 | best_loss=8.76349
Epoch 28/80: current_loss=9.06715 | best_loss=8.76349
Epoch 29/80: current_loss=8.83251 | best_loss=8.76349
Epoch 30/80: current_loss=9.10227 | best_loss=8.76349
Epoch 31/80: current_loss=8.95544 | best_loss=8.76349
Epoch 32/80: current_loss=8.89052 | best_loss=8.76349
Epoch 33/80: current_loss=8.94503 | best_loss=8.76349
Epoch 34/80: current_loss=8.95988 | best_loss=8.76349
Epoch 35/80: current_loss=9.08369 | best_loss=8.76349
Epoch 36/80: current_loss=8.90237 | best_loss=8.76349
Epoch 37/80: current_loss=8.79828 | best_loss=8.76349
Epoch 38/80: current_loss=9.00627 | best_loss=8.76349
Epoch 39/80: current_loss=8.94655 | best_loss=8.76349
Epoch 40/80: current_loss=8.87484 | best_loss=8.76349
Early Stopping at epoch 40
      explained_var=-0.00220 | mse_loss=8.54916
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.43714 | best_loss=8.43714
Epoch 1/80: current_loss=8.22171 | best_loss=8.22171
Epoch 2/80: current_loss=8.22144 | best_loss=8.22144
Epoch 3/80: current_loss=8.23351 | best_loss=8.22144
Epoch 4/80: current_loss=8.21928 | best_loss=8.21928
Epoch 5/80: current_loss=8.31000 | best_loss=8.21928
Epoch 6/80: current_loss=8.21531 | best_loss=8.21531
Epoch 7/80: current_loss=8.24101 | best_loss=8.21531
Epoch 8/80: current_loss=8.25798 | best_loss=8.21531
Epoch 9/80: current_loss=8.23300 | best_loss=8.21531
Epoch 10/80: current_loss=8.27006 | best_loss=8.21531
Epoch 11/80: current_loss=8.24114 | best_loss=8.21531
Epoch 12/80: current_loss=8.23807 | best_loss=8.21531
Epoch 13/80: current_loss=8.25198 | best_loss=8.21531
Epoch 14/80: current_loss=8.28434 | best_loss=8.21531
Epoch 15/80: current_loss=8.26898 | best_loss=8.21531
Epoch 16/80: current_loss=8.25890 | best_loss=8.21531
Epoch 17/80: current_loss=8.25783 | best_loss=8.21531
Epoch 18/80: current_loss=8.26299 | best_loss=8.21531
Epoch 19/80: current_loss=8.29218 | best_loss=8.21531
Epoch 20/80: current_loss=8.24917 | best_loss=8.21531
Epoch 21/80: current_loss=8.23514 | best_loss=8.21531
Epoch 22/80: current_loss=8.23665 | best_loss=8.21531
Epoch 23/80: current_loss=8.25384 | best_loss=8.21531
Epoch 24/80: current_loss=8.26759 | best_loss=8.21531
Epoch 25/80: current_loss=8.28767 | best_loss=8.21531
Epoch 26/80: current_loss=8.22832 | best_loss=8.21531
Early Stopping at epoch 26
      explained_var=0.00322 | mse_loss=8.31484
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.94556 | best_loss=7.94556
Epoch 1/80: current_loss=7.97465 | best_loss=7.94556
Epoch 2/80: current_loss=7.94269 | best_loss=7.94269
Epoch 3/80: current_loss=7.95554 | best_loss=7.94269
Epoch 4/80: current_loss=7.96863 | best_loss=7.94269
Epoch 5/80: current_loss=7.98524 | best_loss=7.94269
Epoch 6/80: current_loss=7.99300 | best_loss=7.94269
Epoch 7/80: current_loss=8.00410 | best_loss=7.94269
Epoch 8/80: current_loss=7.96211 | best_loss=7.94269
Epoch 9/80: current_loss=7.97604 | best_loss=7.94269
Epoch 10/80: current_loss=7.98316 | best_loss=7.94269
Epoch 11/80: current_loss=8.02380 | best_loss=7.94269
Epoch 12/80: current_loss=8.04959 | best_loss=7.94269
Epoch 13/80: current_loss=8.05156 | best_loss=7.94269
Epoch 14/80: current_loss=8.08102 | best_loss=7.94269
Epoch 15/80: current_loss=8.11967 | best_loss=7.94269
Epoch 16/80: current_loss=8.14990 | best_loss=7.94269
Epoch 17/80: current_loss=8.11305 | best_loss=7.94269
Epoch 18/80: current_loss=8.10199 | best_loss=7.94269
Epoch 19/80: current_loss=8.07898 | best_loss=7.94269
Epoch 20/80: current_loss=8.10290 | best_loss=7.94269
Epoch 21/80: current_loss=8.10874 | best_loss=7.94269
Epoch 22/80: current_loss=8.09173 | best_loss=7.94269
Early Stopping at epoch 22
      explained_var=-0.03521 | mse_loss=8.13427
----------------------------------------------
Average early_stopping_point: 37| avg_exp_var=-0.00990| avg_loss=8.21333
----------------------------------------------


----------------------------------------------
Params for Trial 6
{'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=77.72794 | best_loss=77.72794
Epoch 1/80: current_loss=75.03513 | best_loss=75.03513
Epoch 2/80: current_loss=71.70588 | best_loss=71.70588
Epoch 3/80: current_loss=67.21254 | best_loss=67.21254
Epoch 4/80: current_loss=61.19781 | best_loss=61.19781
Epoch 5/80: current_loss=54.10880 | best_loss=54.10880
Epoch 6/80: current_loss=47.62186 | best_loss=47.62186
Epoch 7/80: current_loss=42.55542 | best_loss=42.55542
Epoch 8/80: current_loss=38.74725 | best_loss=38.74725
Epoch 9/80: current_loss=35.71933 | best_loss=35.71933
Epoch 10/80: current_loss=33.29585 | best_loss=33.29585
Epoch 11/80: current_loss=31.29873 | best_loss=31.29873
Epoch 12/80: current_loss=29.56434 | best_loss=29.56434
Epoch 13/80: current_loss=28.01418 | best_loss=28.01418
Epoch 14/80: current_loss=26.61770 | best_loss=26.61770
Epoch 15/80: current_loss=25.33874 | best_loss=25.33874
Epoch 16/80: current_loss=24.20402 | best_loss=24.20402
Epoch 17/80: current_loss=23.15894 | best_loss=23.15894
Epoch 18/80: current_loss=22.21251 | best_loss=22.21251
Epoch 19/80: current_loss=21.36211 | best_loss=21.36211
Epoch 20/80: current_loss=20.58293 | best_loss=20.58293
Epoch 21/80: current_loss=19.88468 | best_loss=19.88468
Epoch 22/80: current_loss=19.23181 | best_loss=19.23181
Epoch 23/80: current_loss=18.63303 | best_loss=18.63303
Epoch 24/80: current_loss=18.05823 | best_loss=18.05823
Epoch 25/80: current_loss=17.53740 | best_loss=17.53740
Epoch 26/80: current_loss=17.04422 | best_loss=17.04422
Epoch 27/80: current_loss=16.58442 | best_loss=16.58442
Epoch 28/80: current_loss=16.13804 | best_loss=16.13804
Epoch 29/80: current_loss=15.71370 | best_loss=15.71370
Epoch 30/80: current_loss=15.32217 | best_loss=15.32217
Epoch 31/80: current_loss=14.94746 | best_loss=14.94746
Epoch 32/80: current_loss=14.58184 | best_loss=14.58184
Epoch 33/80: current_loss=14.24473 | best_loss=14.24473
Epoch 34/80: current_loss=13.91041 | best_loss=13.91041
Epoch 35/80: current_loss=13.59483 | best_loss=13.59483
Epoch 36/80: current_loss=13.30206 | best_loss=13.30206
Epoch 37/80: current_loss=13.01891 | best_loss=13.01891
Epoch 38/80: current_loss=12.73755 | best_loss=12.73755
Epoch 39/80: current_loss=12.47509 | best_loss=12.47509
Epoch 40/80: current_loss=12.22180 | best_loss=12.22180
Epoch 41/80: current_loss=11.99123 | best_loss=11.99123
Epoch 42/80: current_loss=11.76403 | best_loss=11.76403
Epoch 43/80: current_loss=11.55927 | best_loss=11.55927
Epoch 44/80: current_loss=11.35499 | best_loss=11.35499
Epoch 45/80: current_loss=11.16332 | best_loss=11.16332
Epoch 46/80: current_loss=10.97483 | best_loss=10.97483
Epoch 47/80: current_loss=10.79952 | best_loss=10.79952
Epoch 48/80: current_loss=10.63614 | best_loss=10.63614
Epoch 49/80: current_loss=10.47810 | best_loss=10.47810
Epoch 50/80: current_loss=10.32544 | best_loss=10.32544
Epoch 51/80: current_loss=10.17755 | best_loss=10.17755
Epoch 52/80: current_loss=10.04125 | best_loss=10.04125
Epoch 53/80: current_loss=9.91180 | best_loss=9.91180
Epoch 54/80: current_loss=9.79279 | best_loss=9.79279
Epoch 55/80: current_loss=9.67893 | best_loss=9.67893
Epoch 56/80: current_loss=9.56246 | best_loss=9.56246
Epoch 57/80: current_loss=9.46136 | best_loss=9.46136
Epoch 58/80: current_loss=9.36222 | best_loss=9.36222
Epoch 59/80: current_loss=9.27231 | best_loss=9.27231
Epoch 60/80: current_loss=9.18544 | best_loss=9.18544
Epoch 61/80: current_loss=9.10406 | best_loss=9.10406
Epoch 62/80: current_loss=9.03022 | best_loss=9.03022
Epoch 63/80: current_loss=8.95826 | best_loss=8.95826
Epoch 64/80: current_loss=8.88844 | best_loss=8.88844
Epoch 65/80: current_loss=8.82736 | best_loss=8.82736
Epoch 66/80: current_loss=8.76910 | best_loss=8.76910
Epoch 67/80: current_loss=8.71267 | best_loss=8.71267
Epoch 68/80: current_loss=8.65570 | best_loss=8.65570
Epoch 69/80: current_loss=8.60602 | best_loss=8.60602
Epoch 70/80: current_loss=8.55994 | best_loss=8.55994
Epoch 71/80: current_loss=8.51823 | best_loss=8.51823
Epoch 72/80: current_loss=8.47406 | best_loss=8.47406
Epoch 73/80: current_loss=8.43548 | best_loss=8.43548
Epoch 74/80: current_loss=8.40289 | best_loss=8.40289
Epoch 75/80: current_loss=8.37323 | best_loss=8.37323
Epoch 76/80: current_loss=8.34406 | best_loss=8.34406
Epoch 77/80: current_loss=8.31655 | best_loss=8.31655
Epoch 78/80: current_loss=8.28961 | best_loss=8.28961
Epoch 79/80: current_loss=8.26282 | best_loss=8.26282
      explained_var=-0.05465 | mse_loss=8.06617
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.93248 | best_loss=9.93248
Epoch 1/80: current_loss=9.80313 | best_loss=9.80313
Epoch 2/80: current_loss=9.69031 | best_loss=9.69031
Epoch 3/80: current_loss=9.58913 | best_loss=9.58913
Epoch 4/80: current_loss=9.50096 | best_loss=9.50096
Epoch 5/80: current_loss=9.42911 | best_loss=9.42911
Epoch 6/80: current_loss=9.36606 | best_loss=9.36606
Epoch 7/80: current_loss=9.30771 | best_loss=9.30771
Epoch 8/80: current_loss=9.24839 | best_loss=9.24839
Epoch 9/80: current_loss=9.19960 | best_loss=9.19960
Epoch 10/80: current_loss=9.16308 | best_loss=9.16308
Epoch 11/80: current_loss=9.12851 | best_loss=9.12851
Epoch 12/80: current_loss=9.08898 | best_loss=9.08898
Epoch 13/80: current_loss=9.05045 | best_loss=9.05045
Epoch 14/80: current_loss=9.02046 | best_loss=9.02046
Epoch 15/80: current_loss=8.98701 | best_loss=8.98701
Epoch 16/80: current_loss=8.95953 | best_loss=8.95953
Epoch 17/80: current_loss=8.93586 | best_loss=8.93586
Epoch 18/80: current_loss=8.91694 | best_loss=8.91694
Epoch 19/80: current_loss=8.89180 | best_loss=8.89180
Epoch 20/80: current_loss=8.87275 | best_loss=8.87275
Epoch 21/80: current_loss=8.85280 | best_loss=8.85280
Epoch 22/80: current_loss=8.83636 | best_loss=8.83636
Epoch 23/80: current_loss=8.81982 | best_loss=8.81982
Epoch 24/80: current_loss=8.80784 | best_loss=8.80784
Epoch 25/80: current_loss=8.78701 | best_loss=8.78701
Epoch 26/80: current_loss=8.77373 | best_loss=8.77373
Epoch 27/80: current_loss=8.76654 | best_loss=8.76654
Epoch 28/80: current_loss=8.75775 | best_loss=8.75775
Epoch 29/80: current_loss=8.74852 | best_loss=8.74852
Epoch 30/80: current_loss=8.73619 | best_loss=8.73619
Epoch 31/80: current_loss=8.72109 | best_loss=8.72109
Epoch 32/80: current_loss=8.70832 | best_loss=8.70832
Epoch 33/80: current_loss=8.69573 | best_loss=8.69573
Epoch 34/80: current_loss=8.68281 | best_loss=8.68281
Epoch 35/80: current_loss=8.66623 | best_loss=8.66623
Epoch 36/80: current_loss=8.65694 | best_loss=8.65694
Epoch 37/80: current_loss=8.64540 | best_loss=8.64540
Epoch 38/80: current_loss=8.63956 | best_loss=8.63956
Epoch 39/80: current_loss=8.63043 | best_loss=8.63043
Epoch 40/80: current_loss=8.61698 | best_loss=8.61698
Epoch 41/80: current_loss=8.60309 | best_loss=8.60309
Epoch 42/80: current_loss=8.59392 | best_loss=8.59392
Epoch 43/80: current_loss=8.58353 | best_loss=8.58353
Epoch 44/80: current_loss=8.57559 | best_loss=8.57559
Epoch 45/80: current_loss=8.56780 | best_loss=8.56780
Epoch 46/80: current_loss=8.56323 | best_loss=8.56323
Epoch 47/80: current_loss=8.56137 | best_loss=8.56137
Epoch 48/80: current_loss=8.55566 | best_loss=8.55566
Epoch 49/80: current_loss=8.55193 | best_loss=8.55193
Epoch 50/80: current_loss=8.54769 | best_loss=8.54769
Epoch 51/80: current_loss=8.54135 | best_loss=8.54135
Epoch 52/80: current_loss=8.53845 | best_loss=8.53845
Epoch 53/80: current_loss=8.53468 | best_loss=8.53468
Epoch 54/80: current_loss=8.53329 | best_loss=8.53329
Epoch 55/80: current_loss=8.52855 | best_loss=8.52855
Epoch 56/80: current_loss=8.52596 | best_loss=8.52596
Epoch 57/80: current_loss=8.51733 | best_loss=8.51733
Epoch 58/80: current_loss=8.50868 | best_loss=8.50868
Epoch 59/80: current_loss=8.50388 | best_loss=8.50388
Epoch 60/80: current_loss=8.50183 | best_loss=8.50183
Epoch 61/80: current_loss=8.49996 | best_loss=8.49996
Epoch 62/80: current_loss=8.49899 | best_loss=8.49899
Epoch 63/80: current_loss=8.49493 | best_loss=8.49493
Epoch 64/80: current_loss=8.49050 | best_loss=8.49050
Epoch 65/80: current_loss=8.48853 | best_loss=8.48853
Epoch 66/80: current_loss=8.48327 | best_loss=8.48327
Epoch 67/80: current_loss=8.47717 | best_loss=8.47717
Epoch 68/80: current_loss=8.47238 | best_loss=8.47238
Epoch 69/80: current_loss=8.46867 | best_loss=8.46867
Epoch 70/80: current_loss=8.46878 | best_loss=8.46867
Epoch 71/80: current_loss=8.46566 | best_loss=8.46566
Epoch 72/80: current_loss=8.46333 | best_loss=8.46333
Epoch 73/80: current_loss=8.46537 | best_loss=8.46333
Epoch 74/80: current_loss=8.46523 | best_loss=8.46333
Epoch 75/80: current_loss=8.46350 | best_loss=8.46333
Epoch 76/80: current_loss=8.45969 | best_loss=8.45969
Epoch 77/80: current_loss=8.46222 | best_loss=8.45969
Epoch 78/80: current_loss=8.46218 | best_loss=8.45969
Epoch 79/80: current_loss=8.45912 | best_loss=8.45912
      explained_var=-0.01933 | mse_loss=8.33372
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.10221 | best_loss=9.10221
Epoch 1/80: current_loss=9.11265 | best_loss=9.10221
Epoch 2/80: current_loss=9.10501 | best_loss=9.10221
Epoch 3/80: current_loss=9.09542 | best_loss=9.09542
Epoch 4/80: current_loss=9.11069 | best_loss=9.09542
Epoch 5/80: current_loss=9.12057 | best_loss=9.09542
Epoch 6/80: current_loss=9.11059 | best_loss=9.09542
Epoch 7/80: current_loss=9.11632 | best_loss=9.09542
Epoch 8/80: current_loss=9.11307 | best_loss=9.09542
Epoch 9/80: current_loss=9.11820 | best_loss=9.09542
Epoch 10/80: current_loss=9.13206 | best_loss=9.09542
Epoch 11/80: current_loss=9.13184 | best_loss=9.09542
Epoch 12/80: current_loss=9.12910 | best_loss=9.09542
Epoch 13/80: current_loss=9.11791 | best_loss=9.09542
Epoch 14/80: current_loss=9.12479 | best_loss=9.09542
Epoch 15/80: current_loss=9.11680 | best_loss=9.09542
Epoch 16/80: current_loss=9.11468 | best_loss=9.09542
Epoch 17/80: current_loss=9.10879 | best_loss=9.09542
Epoch 18/80: current_loss=9.10655 | best_loss=9.09542
Epoch 19/80: current_loss=9.11695 | best_loss=9.09542
Epoch 20/80: current_loss=9.11012 | best_loss=9.09542
Epoch 21/80: current_loss=9.09709 | best_loss=9.09542
Epoch 22/80: current_loss=9.08143 | best_loss=9.08143
Epoch 23/80: current_loss=9.09663 | best_loss=9.08143
Epoch 24/80: current_loss=9.10133 | best_loss=9.08143
Epoch 25/80: current_loss=9.09618 | best_loss=9.08143
Epoch 26/80: current_loss=9.10121 | best_loss=9.08143
Epoch 27/80: current_loss=9.10107 | best_loss=9.08143
Epoch 28/80: current_loss=9.08612 | best_loss=9.08143
Epoch 29/80: current_loss=9.07989 | best_loss=9.07989
Epoch 30/80: current_loss=9.06907 | best_loss=9.06907
Epoch 31/80: current_loss=9.05023 | best_loss=9.05023
Epoch 32/80: current_loss=9.04210 | best_loss=9.04210
Epoch 33/80: current_loss=9.03775 | best_loss=9.03775
Epoch 34/80: current_loss=9.03225 | best_loss=9.03225
Epoch 35/80: current_loss=9.03581 | best_loss=9.03225
Epoch 36/80: current_loss=9.03376 | best_loss=9.03225
Epoch 37/80: current_loss=9.03421 | best_loss=9.03225
Epoch 38/80: current_loss=9.03511 | best_loss=9.03225
Epoch 39/80: current_loss=9.05073 | best_loss=9.03225
Epoch 40/80: current_loss=9.06074 | best_loss=9.03225
Epoch 41/80: current_loss=9.06989 | best_loss=9.03225
Epoch 42/80: current_loss=9.06886 | best_loss=9.03225
Epoch 43/80: current_loss=9.05837 | best_loss=9.03225
Epoch 44/80: current_loss=9.06065 | best_loss=9.03225
Epoch 45/80: current_loss=9.06266 | best_loss=9.03225
Epoch 46/80: current_loss=9.05908 | best_loss=9.03225
Epoch 47/80: current_loss=9.06352 | best_loss=9.03225
Epoch 48/80: current_loss=9.05582 | best_loss=9.03225
Epoch 49/80: current_loss=9.04683 | best_loss=9.03225
Epoch 50/80: current_loss=9.03336 | best_loss=9.03225
Epoch 51/80: current_loss=9.02722 | best_loss=9.02722
Epoch 52/80: current_loss=9.03595 | best_loss=9.02722
Epoch 53/80: current_loss=9.02318 | best_loss=9.02318
Epoch 54/80: current_loss=9.01517 | best_loss=9.01517
Epoch 55/80: current_loss=9.00334 | best_loss=9.00334
Epoch 56/80: current_loss=8.99883 | best_loss=8.99883
Epoch 57/80: current_loss=8.99546 | best_loss=8.99546
Epoch 58/80: current_loss=8.98615 | best_loss=8.98615
Epoch 59/80: current_loss=8.98499 | best_loss=8.98499
Epoch 60/80: current_loss=8.97648 | best_loss=8.97648
Epoch 61/80: current_loss=8.98070 | best_loss=8.97648
Epoch 62/80: current_loss=8.98999 | best_loss=8.97648
Epoch 63/80: current_loss=8.98702 | best_loss=8.97648
Epoch 64/80: current_loss=8.98908 | best_loss=8.97648
Epoch 65/80: current_loss=8.97571 | best_loss=8.97571
Epoch 66/80: current_loss=8.98053 | best_loss=8.97571
Epoch 67/80: current_loss=8.98727 | best_loss=8.97571
Epoch 68/80: current_loss=8.99473 | best_loss=8.97571
Epoch 69/80: current_loss=8.99601 | best_loss=8.97571
Epoch 70/80: current_loss=9.00222 | best_loss=8.97571
Epoch 71/80: current_loss=8.99379 | best_loss=8.97571
Epoch 72/80: current_loss=8.99290 | best_loss=8.97571
Epoch 73/80: current_loss=8.99660 | best_loss=8.97571
Epoch 74/80: current_loss=8.99925 | best_loss=8.97571
Epoch 75/80: current_loss=9.00312 | best_loss=8.97571
Epoch 76/80: current_loss=9.00220 | best_loss=8.97571
Epoch 77/80: current_loss=9.00049 | best_loss=8.97571
Epoch 78/80: current_loss=9.01249 | best_loss=8.97571
Epoch 79/80: current_loss=9.01842 | best_loss=8.97571
      explained_var=-0.01116 | mse_loss=8.76389
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.23970 | best_loss=8.23970
Epoch 1/80: current_loss=8.23419 | best_loss=8.23419
Epoch 2/80: current_loss=8.23323 | best_loss=8.23323
Epoch 3/80: current_loss=8.23284 | best_loss=8.23284
Epoch 4/80: current_loss=8.22916 | best_loss=8.22916
Epoch 5/80: current_loss=8.22711 | best_loss=8.22711
Epoch 6/80: current_loss=8.22749 | best_loss=8.22711
Epoch 7/80: current_loss=8.22692 | best_loss=8.22692
Epoch 8/80: current_loss=8.22458 | best_loss=8.22458
Epoch 9/80: current_loss=8.22534 | best_loss=8.22458
Epoch 10/80: current_loss=8.22510 | best_loss=8.22458
Epoch 11/80: current_loss=8.22559 | best_loss=8.22458
Epoch 12/80: current_loss=8.22523 | best_loss=8.22458
Epoch 13/80: current_loss=8.22575 | best_loss=8.22458
Epoch 14/80: current_loss=8.22355 | best_loss=8.22355
Epoch 15/80: current_loss=8.22109 | best_loss=8.22109
Epoch 16/80: current_loss=8.22082 | best_loss=8.22082
Epoch 17/80: current_loss=8.22054 | best_loss=8.22054
Epoch 18/80: current_loss=8.22141 | best_loss=8.22054
Epoch 19/80: current_loss=8.22194 | best_loss=8.22054
Epoch 20/80: current_loss=8.22020 | best_loss=8.22020
Epoch 21/80: current_loss=8.22018 | best_loss=8.22018
Epoch 22/80: current_loss=8.22067 | best_loss=8.22018
Epoch 23/80: current_loss=8.22072 | best_loss=8.22018
Epoch 24/80: current_loss=8.22093 | best_loss=8.22018
Epoch 25/80: current_loss=8.22106 | best_loss=8.22018
Epoch 26/80: current_loss=8.22042 | best_loss=8.22018
Epoch 27/80: current_loss=8.21984 | best_loss=8.21984
Epoch 28/80: current_loss=8.21893 | best_loss=8.21893
Epoch 29/80: current_loss=8.21881 | best_loss=8.21881
Epoch 30/80: current_loss=8.22111 | best_loss=8.21881
Epoch 31/80: current_loss=8.22061 | best_loss=8.21881
Epoch 32/80: current_loss=8.22124 | best_loss=8.21881
Epoch 33/80: current_loss=8.22118 | best_loss=8.21881
Epoch 34/80: current_loss=8.22136 | best_loss=8.21881
Epoch 35/80: current_loss=8.22260 | best_loss=8.21881
Epoch 36/80: current_loss=8.22377 | best_loss=8.21881
Epoch 37/80: current_loss=8.22375 | best_loss=8.21881
Epoch 38/80: current_loss=8.22343 | best_loss=8.21881
Epoch 39/80: current_loss=8.22238 | best_loss=8.21881
Epoch 40/80: current_loss=8.22349 | best_loss=8.21881
Epoch 41/80: current_loss=8.22370 | best_loss=8.21881
Epoch 42/80: current_loss=8.22394 | best_loss=8.21881
Epoch 43/80: current_loss=8.22451 | best_loss=8.21881
Epoch 44/80: current_loss=8.22380 | best_loss=8.21881
Epoch 45/80: current_loss=8.22086 | best_loss=8.21881
Epoch 46/80: current_loss=8.22174 | best_loss=8.21881
Epoch 47/80: current_loss=8.22211 | best_loss=8.21881
Epoch 48/80: current_loss=8.22231 | best_loss=8.21881
Epoch 49/80: current_loss=8.22091 | best_loss=8.21881
Early Stopping at epoch 49
      explained_var=0.00208 | mse_loss=8.31654
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.70794 | best_loss=7.70794
Epoch 1/80: current_loss=7.70798 | best_loss=7.70794
Epoch 2/80: current_loss=7.70830 | best_loss=7.70794
Epoch 3/80: current_loss=7.70899 | best_loss=7.70794
Epoch 4/80: current_loss=7.70748 | best_loss=7.70748
Epoch 5/80: current_loss=7.70715 | best_loss=7.70715
Epoch 6/80: current_loss=7.70807 | best_loss=7.70715
Epoch 7/80: current_loss=7.70808 | best_loss=7.70715
Epoch 8/80: current_loss=7.70848 | best_loss=7.70715
Epoch 9/80: current_loss=7.70779 | best_loss=7.70715
Epoch 10/80: current_loss=7.70834 | best_loss=7.70715
Epoch 11/80: current_loss=7.70908 | best_loss=7.70715
Epoch 12/80: current_loss=7.70916 | best_loss=7.70715
Epoch 13/80: current_loss=7.70755 | best_loss=7.70715
Epoch 14/80: current_loss=7.70683 | best_loss=7.70683
Epoch 15/80: current_loss=7.70694 | best_loss=7.70683
Epoch 16/80: current_loss=7.70651 | best_loss=7.70651
Epoch 17/80: current_loss=7.70603 | best_loss=7.70603
Epoch 18/80: current_loss=7.70533 | best_loss=7.70533
Epoch 19/80: current_loss=7.70574 | best_loss=7.70533
Epoch 20/80: current_loss=7.70616 | best_loss=7.70533
Epoch 21/80: current_loss=7.70501 | best_loss=7.70501
Epoch 22/80: current_loss=7.70542 | best_loss=7.70501
Epoch 23/80: current_loss=7.70555 | best_loss=7.70501
Epoch 24/80: current_loss=7.70553 | best_loss=7.70501
Epoch 25/80: current_loss=7.70676 | best_loss=7.70501
Epoch 26/80: current_loss=7.70742 | best_loss=7.70501
Epoch 27/80: current_loss=7.70631 | best_loss=7.70501
Epoch 28/80: current_loss=7.70686 | best_loss=7.70501
Epoch 29/80: current_loss=7.70773 | best_loss=7.70501
Epoch 30/80: current_loss=7.70958 | best_loss=7.70501
Epoch 31/80: current_loss=7.70894 | best_loss=7.70501
Epoch 32/80: current_loss=7.70746 | best_loss=7.70501
Epoch 33/80: current_loss=7.70662 | best_loss=7.70501
Epoch 34/80: current_loss=7.70336 | best_loss=7.70336
Epoch 35/80: current_loss=7.70142 | best_loss=7.70142
Epoch 36/80: current_loss=7.70117 | best_loss=7.70117
Epoch 37/80: current_loss=7.70181 | best_loss=7.70117
Epoch 38/80: current_loss=7.70148 | best_loss=7.70117
Epoch 39/80: current_loss=7.70089 | best_loss=7.70089
Epoch 40/80: current_loss=7.70144 | best_loss=7.70089
Epoch 41/80: current_loss=7.70116 | best_loss=7.70089
Epoch 42/80: current_loss=7.70108 | best_loss=7.70089
Epoch 43/80: current_loss=7.70183 | best_loss=7.70089
Epoch 44/80: current_loss=7.70320 | best_loss=7.70089
Epoch 45/80: current_loss=7.70316 | best_loss=7.70089
Epoch 46/80: current_loss=7.70537 | best_loss=7.70089
Epoch 47/80: current_loss=7.70796 | best_loss=7.70089
Epoch 48/80: current_loss=7.70689 | best_loss=7.70089
Epoch 49/80: current_loss=7.70669 | best_loss=7.70089
Epoch 50/80: current_loss=7.70848 | best_loss=7.70089
Epoch 51/80: current_loss=7.70616 | best_loss=7.70089
Epoch 52/80: current_loss=7.70629 | best_loss=7.70089
Epoch 53/80: current_loss=7.70728 | best_loss=7.70089
Epoch 54/80: current_loss=7.70583 | best_loss=7.70089
Epoch 55/80: current_loss=7.70547 | best_loss=7.70089
Epoch 56/80: current_loss=7.70350 | best_loss=7.70089
Epoch 57/80: current_loss=7.70369 | best_loss=7.70089
Epoch 58/80: current_loss=7.70352 | best_loss=7.70089
Epoch 59/80: current_loss=7.70343 | best_loss=7.70089
Early Stopping at epoch 59
      explained_var=-0.00237 | mse_loss=7.87868
----------------------------------------------
Average early_stopping_point: 61| avg_exp_var=-0.01709| avg_loss=8.27180
----------------------------------------------


----------------------------------------------
Params for Trial 7
{'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=105.19217 | best_loss=105.19217
Epoch 1/80: current_loss=49.31063 | best_loss=49.31063
Epoch 2/80: current_loss=28.16857 | best_loss=28.16857
Epoch 3/80: current_loss=38.05173 | best_loss=28.16857
Epoch 4/80: current_loss=28.84409 | best_loss=28.16857
Epoch 5/80: current_loss=23.74547 | best_loss=23.74547
Epoch 6/80: current_loss=19.79348 | best_loss=19.79348
Epoch 7/80: current_loss=46.38634 | best_loss=19.79348
Epoch 8/80: current_loss=21.92872 | best_loss=19.79348
Epoch 9/80: current_loss=15.01173 | best_loss=15.01173
Epoch 10/80: current_loss=13.98809 | best_loss=13.98809
Epoch 11/80: current_loss=21.48860 | best_loss=13.98809
Epoch 12/80: current_loss=20.98157 | best_loss=13.98809
Epoch 13/80: current_loss=8.65855 | best_loss=8.65855
Epoch 14/80: current_loss=18.41528 | best_loss=8.65855
Epoch 15/80: current_loss=10.04192 | best_loss=8.65855
Epoch 16/80: current_loss=16.62913 | best_loss=8.65855
Epoch 17/80: current_loss=10.46184 | best_loss=8.65855
Epoch 18/80: current_loss=11.01957 | best_loss=8.65855
Epoch 19/80: current_loss=11.22999 | best_loss=8.65855
Epoch 20/80: current_loss=10.69100 | best_loss=8.65855
Epoch 21/80: current_loss=11.76352 | best_loss=8.65855
Epoch 22/80: current_loss=17.81498 | best_loss=8.65855
Epoch 23/80: current_loss=16.00270 | best_loss=8.65855
Epoch 24/80: current_loss=20.04151 | best_loss=8.65855
Epoch 25/80: current_loss=9.04420 | best_loss=8.65855
Epoch 26/80: current_loss=12.45438 | best_loss=8.65855
Epoch 27/80: current_loss=16.82065 | best_loss=8.65855
Epoch 28/80: current_loss=9.75912 | best_loss=8.65855
Epoch 29/80: current_loss=9.02521 | best_loss=8.65855
Epoch 30/80: current_loss=9.36696 | best_loss=8.65855
Epoch 31/80: current_loss=28.67291 | best_loss=8.65855
Epoch 32/80: current_loss=30.09687 | best_loss=8.65855
Epoch 33/80: current_loss=23.23631 | best_loss=8.65855
Early Stopping at epoch 33
      explained_var=-0.10398 | mse_loss=8.46406
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=16.01932 | best_loss=16.01932
Epoch 1/80: current_loss=21.87612 | best_loss=16.01932
Epoch 2/80: current_loss=16.35361 | best_loss=16.01932
Epoch 3/80: current_loss=11.92694 | best_loss=11.92694
Epoch 4/80: current_loss=9.75728 | best_loss=9.75728
Epoch 5/80: current_loss=8.88699 | best_loss=8.88699
Epoch 6/80: current_loss=24.02945 | best_loss=8.88699
Epoch 7/80: current_loss=10.06237 | best_loss=8.88699
Epoch 8/80: current_loss=16.08827 | best_loss=8.88699
Epoch 9/80: current_loss=33.14777 | best_loss=8.88699
Epoch 10/80: current_loss=135.24229 | best_loss=8.88699
Epoch 11/80: current_loss=39.17948 | best_loss=8.88699
Epoch 12/80: current_loss=33.56867 | best_loss=8.88699
Epoch 13/80: current_loss=11.49267 | best_loss=8.88699
Epoch 14/80: current_loss=29.14753 | best_loss=8.88699
Epoch 15/80: current_loss=19.25669 | best_loss=8.88699
Epoch 16/80: current_loss=48.87884 | best_loss=8.88699
Epoch 17/80: current_loss=14.66382 | best_loss=8.88699
Epoch 18/80: current_loss=10.67299 | best_loss=8.88699
Epoch 19/80: current_loss=11.25324 | best_loss=8.88699
Epoch 20/80: current_loss=14.17482 | best_loss=8.88699
Epoch 21/80: current_loss=9.13054 | best_loss=8.88699
Epoch 22/80: current_loss=8.53963 | best_loss=8.53963
Epoch 23/80: current_loss=17.01988 | best_loss=8.53963
Epoch 24/80: current_loss=20.35191 | best_loss=8.53963
Epoch 25/80: current_loss=12.83432 | best_loss=8.53963
Epoch 26/80: current_loss=8.91396 | best_loss=8.53963
Epoch 27/80: current_loss=11.04749 | best_loss=8.53963
Epoch 28/80: current_loss=23.40356 | best_loss=8.53963
Epoch 29/80: current_loss=15.55180 | best_loss=8.53963
Epoch 30/80: current_loss=18.63939 | best_loss=8.53963
Epoch 31/80: current_loss=8.43802 | best_loss=8.43802
Epoch 32/80: current_loss=16.28814 | best_loss=8.43802
Epoch 33/80: current_loss=16.41564 | best_loss=8.43802
Epoch 34/80: current_loss=9.51603 | best_loss=8.43802
Epoch 35/80: current_loss=8.89579 | best_loss=8.43802
Epoch 36/80: current_loss=10.59116 | best_loss=8.43802
Epoch 37/80: current_loss=21.50092 | best_loss=8.43802
Epoch 38/80: current_loss=14.17837 | best_loss=8.43802
Epoch 39/80: current_loss=11.13708 | best_loss=8.43802
Epoch 40/80: current_loss=35.90714 | best_loss=8.43802
Epoch 41/80: current_loss=55.49097 | best_loss=8.43802
Epoch 42/80: current_loss=27.86872 | best_loss=8.43802
Epoch 43/80: current_loss=21.89952 | best_loss=8.43802
Epoch 44/80: current_loss=10.63447 | best_loss=8.43802
Epoch 45/80: current_loss=8.71028 | best_loss=8.43802
Epoch 46/80: current_loss=8.37909 | best_loss=8.37909
Epoch 47/80: current_loss=10.59891 | best_loss=8.37909
Epoch 48/80: current_loss=40.86902 | best_loss=8.37909
Epoch 49/80: current_loss=17.72593 | best_loss=8.37909
Epoch 50/80: current_loss=72.34454 | best_loss=8.37909
Epoch 51/80: current_loss=14.21269 | best_loss=8.37909
Epoch 52/80: current_loss=16.09329 | best_loss=8.37909
Epoch 53/80: current_loss=11.14560 | best_loss=8.37909
Epoch 54/80: current_loss=8.85486 | best_loss=8.37909
Epoch 55/80: current_loss=9.03128 | best_loss=8.37909
Epoch 56/80: current_loss=15.85008 | best_loss=8.37909
Epoch 57/80: current_loss=11.08322 | best_loss=8.37909
Epoch 58/80: current_loss=36.06224 | best_loss=8.37909
Epoch 59/80: current_loss=20.50940 | best_loss=8.37909
Epoch 60/80: current_loss=8.65005 | best_loss=8.37909
Epoch 61/80: current_loss=12.96374 | best_loss=8.37909
Epoch 62/80: current_loss=14.27136 | best_loss=8.37909
Epoch 63/80: current_loss=10.21539 | best_loss=8.37909
Epoch 64/80: current_loss=9.04810 | best_loss=8.37909
Epoch 65/80: current_loss=9.68035 | best_loss=8.37909
Epoch 66/80: current_loss=8.32708 | best_loss=8.32708
Epoch 67/80: current_loss=27.34830 | best_loss=8.32708
Epoch 68/80: current_loss=10.88152 | best_loss=8.32708
Epoch 69/80: current_loss=11.11275 | best_loss=8.32708
Epoch 70/80: current_loss=12.11238 | best_loss=8.32708
Epoch 71/80: current_loss=39.28440 | best_loss=8.32708
Epoch 72/80: current_loss=17.44807 | best_loss=8.32708
Epoch 73/80: current_loss=9.21007 | best_loss=8.32708
Epoch 74/80: current_loss=17.80604 | best_loss=8.32708
Epoch 75/80: current_loss=22.28126 | best_loss=8.32708
Epoch 76/80: current_loss=22.69350 | best_loss=8.32708
Epoch 77/80: current_loss=46.53769 | best_loss=8.32708
Epoch 78/80: current_loss=9.81023 | best_loss=8.32708
Epoch 79/80: current_loss=9.98213 | best_loss=8.32708
      explained_var=0.00625 | mse_loss=8.16474
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=15.03306 | best_loss=15.03306
Epoch 1/80: current_loss=21.22639 | best_loss=15.03306
Epoch 2/80: current_loss=23.57387 | best_loss=15.03306
Epoch 3/80: current_loss=15.31868 | best_loss=15.03306
Epoch 4/80: current_loss=13.65200 | best_loss=13.65200
Epoch 5/80: current_loss=37.60145 | best_loss=13.65200
Epoch 6/80: current_loss=13.66581 | best_loss=13.65200
Epoch 7/80: current_loss=9.62421 | best_loss=9.62421
Epoch 8/80: current_loss=13.34915 | best_loss=9.62421
Epoch 9/80: current_loss=9.37936 | best_loss=9.37936
Epoch 10/80: current_loss=17.28782 | best_loss=9.37936
Epoch 11/80: current_loss=13.18975 | best_loss=9.37936
Epoch 12/80: current_loss=13.41922 | best_loss=9.37936
Epoch 13/80: current_loss=11.74823 | best_loss=9.37936
Epoch 14/80: current_loss=11.65160 | best_loss=9.37936
Epoch 15/80: current_loss=11.48149 | best_loss=9.37936
Epoch 16/80: current_loss=10.13116 | best_loss=9.37936
Epoch 17/80: current_loss=21.72354 | best_loss=9.37936
Epoch 18/80: current_loss=9.27745 | best_loss=9.27745
Epoch 19/80: current_loss=22.64266 | best_loss=9.27745
Epoch 20/80: current_loss=18.76778 | best_loss=9.27745
Epoch 21/80: current_loss=9.34649 | best_loss=9.27745
Epoch 22/80: current_loss=11.26188 | best_loss=9.27745
Epoch 23/80: current_loss=14.63097 | best_loss=9.27745
Epoch 24/80: current_loss=9.39954 | best_loss=9.27745
Epoch 25/80: current_loss=10.87444 | best_loss=9.27745
Epoch 26/80: current_loss=9.30114 | best_loss=9.27745
Epoch 27/80: current_loss=10.58772 | best_loss=9.27745
Epoch 28/80: current_loss=28.67789 | best_loss=9.27745
Epoch 29/80: current_loss=20.75224 | best_loss=9.27745
Epoch 30/80: current_loss=13.96880 | best_loss=9.27745
Epoch 31/80: current_loss=10.21082 | best_loss=9.27745
Epoch 32/80: current_loss=13.49303 | best_loss=9.27745
Epoch 33/80: current_loss=12.84070 | best_loss=9.27745
Epoch 34/80: current_loss=12.17411 | best_loss=9.27745
Epoch 35/80: current_loss=11.30098 | best_loss=9.27745
Epoch 36/80: current_loss=26.77392 | best_loss=9.27745
Epoch 37/80: current_loss=14.75273 | best_loss=9.27745
Epoch 38/80: current_loss=40.07346 | best_loss=9.27745
Early Stopping at epoch 38
      explained_var=-0.06217 | mse_loss=9.03042
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=20.81987 | best_loss=20.81987
Epoch 1/80: current_loss=12.09736 | best_loss=12.09736
Epoch 2/80: current_loss=37.30454 | best_loss=12.09736
Epoch 3/80: current_loss=37.09127 | best_loss=12.09736
Epoch 4/80: current_loss=14.20106 | best_loss=12.09736
Epoch 5/80: current_loss=9.90233 | best_loss=9.90233
Epoch 6/80: current_loss=11.70874 | best_loss=9.90233
Epoch 7/80: current_loss=13.95096 | best_loss=9.90233
Epoch 8/80: current_loss=9.34188 | best_loss=9.34188
Epoch 9/80: current_loss=10.45046 | best_loss=9.34188
Epoch 10/80: current_loss=13.96243 | best_loss=9.34188
Epoch 11/80: current_loss=18.49052 | best_loss=9.34188
Epoch 12/80: current_loss=15.58053 | best_loss=9.34188
Epoch 13/80: current_loss=23.86114 | best_loss=9.34188
Epoch 14/80: current_loss=34.42857 | best_loss=9.34188
Epoch 15/80: current_loss=16.47728 | best_loss=9.34188
Epoch 16/80: current_loss=14.35947 | best_loss=9.34188
Epoch 17/80: current_loss=11.57938 | best_loss=9.34188
Epoch 18/80: current_loss=12.24112 | best_loss=9.34188
Epoch 19/80: current_loss=30.09923 | best_loss=9.34188
Epoch 20/80: current_loss=22.03676 | best_loss=9.34188
Epoch 21/80: current_loss=27.78024 | best_loss=9.34188
Epoch 22/80: current_loss=19.70283 | best_loss=9.34188
Epoch 23/80: current_loss=20.01257 | best_loss=9.34188
Epoch 24/80: current_loss=14.42663 | best_loss=9.34188
Epoch 25/80: current_loss=9.42756 | best_loss=9.34188
Epoch 26/80: current_loss=12.21087 | best_loss=9.34188
Epoch 27/80: current_loss=12.12759 | best_loss=9.34188
Epoch 28/80: current_loss=8.99187 | best_loss=8.99187
Epoch 29/80: current_loss=14.31911 | best_loss=8.99187
Epoch 30/80: current_loss=27.73072 | best_loss=8.99187
Epoch 31/80: current_loss=24.47199 | best_loss=8.99187
Epoch 32/80: current_loss=26.61590 | best_loss=8.99187
Epoch 33/80: current_loss=13.12543 | best_loss=8.99187
Epoch 34/80: current_loss=20.98401 | best_loss=8.99187
Epoch 35/80: current_loss=9.49960 | best_loss=8.99187
Epoch 36/80: current_loss=9.14409 | best_loss=8.99187
Epoch 37/80: current_loss=9.18797 | best_loss=8.99187
Epoch 38/80: current_loss=24.32345 | best_loss=8.99187
Epoch 39/80: current_loss=10.52238 | best_loss=8.99187
Epoch 40/80: current_loss=20.55571 | best_loss=8.99187
Epoch 41/80: current_loss=26.04504 | best_loss=8.99187
Epoch 42/80: current_loss=10.66410 | best_loss=8.99187
Epoch 43/80: current_loss=9.99137 | best_loss=8.99187
Epoch 44/80: current_loss=16.73737 | best_loss=8.99187
Epoch 45/80: current_loss=14.63105 | best_loss=8.99187
Epoch 46/80: current_loss=50.50813 | best_loss=8.99187
Epoch 47/80: current_loss=14.75278 | best_loss=8.99187
Epoch 48/80: current_loss=77.00182 | best_loss=8.99187
Early Stopping at epoch 48
      explained_var=-0.09193 | mse_loss=9.11888
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=17.90606 | best_loss=17.90606
Epoch 1/80: current_loss=9.21437 | best_loss=9.21437
Epoch 2/80: current_loss=8.38965 | best_loss=8.38965
Epoch 3/80: current_loss=21.81735 | best_loss=8.38965
Epoch 4/80: current_loss=9.01606 | best_loss=8.38965
Epoch 5/80: current_loss=8.23064 | best_loss=8.23064
Epoch 6/80: current_loss=13.33865 | best_loss=8.23064
Epoch 7/80: current_loss=10.50924 | best_loss=8.23064
Epoch 8/80: current_loss=8.29271 | best_loss=8.23064
Epoch 9/80: current_loss=17.18246 | best_loss=8.23064
Epoch 10/80: current_loss=8.51377 | best_loss=8.23064
Epoch 11/80: current_loss=12.46334 | best_loss=8.23064
Epoch 12/80: current_loss=14.57972 | best_loss=8.23064
Epoch 13/80: current_loss=12.58463 | best_loss=8.23064
Epoch 14/80: current_loss=17.69793 | best_loss=8.23064
Epoch 15/80: current_loss=42.49890 | best_loss=8.23064
Epoch 16/80: current_loss=9.45988 | best_loss=8.23064
Epoch 17/80: current_loss=9.50940 | best_loss=8.23064
Epoch 18/80: current_loss=11.41412 | best_loss=8.23064
Epoch 19/80: current_loss=11.40814 | best_loss=8.23064
Epoch 20/80: current_loss=21.84799 | best_loss=8.23064
Epoch 21/80: current_loss=8.17011 | best_loss=8.17011
Epoch 22/80: current_loss=12.18633 | best_loss=8.17011
Epoch 23/80: current_loss=7.86043 | best_loss=7.86043
Epoch 24/80: current_loss=10.55503 | best_loss=7.86043
Epoch 25/80: current_loss=8.05107 | best_loss=7.86043
Epoch 26/80: current_loss=35.46765 | best_loss=7.86043
Epoch 27/80: current_loss=18.87447 | best_loss=7.86043
Epoch 28/80: current_loss=16.11985 | best_loss=7.86043
Epoch 29/80: current_loss=12.81034 | best_loss=7.86043
Epoch 30/80: current_loss=41.83753 | best_loss=7.86043
Epoch 31/80: current_loss=8.57294 | best_loss=7.86043
Epoch 32/80: current_loss=9.46176 | best_loss=7.86043
Epoch 33/80: current_loss=26.05283 | best_loss=7.86043
Epoch 34/80: current_loss=11.62630 | best_loss=7.86043
Epoch 35/80: current_loss=8.98490 | best_loss=7.86043
Epoch 36/80: current_loss=10.68597 | best_loss=7.86043
Epoch 37/80: current_loss=11.71566 | best_loss=7.86043
Epoch 38/80: current_loss=19.27289 | best_loss=7.86043
Epoch 39/80: current_loss=24.39549 | best_loss=7.86043
Epoch 40/80: current_loss=10.34563 | best_loss=7.86043
Epoch 41/80: current_loss=9.68004 | best_loss=7.86043
Epoch 42/80: current_loss=8.14096 | best_loss=7.86043
Epoch 43/80: current_loss=14.69401 | best_loss=7.86043
Early Stopping at epoch 43
      explained_var=-0.01714 | mse_loss=7.99397
----------------------------------------------
Average early_stopping_point: 32| avg_exp_var=-0.05380| avg_loss=8.55441
----------------------------------------------


----------------------------------------------
Params for Trial 8
{'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=87.00462 | best_loss=87.00462
Epoch 1/80: current_loss=86.89799 | best_loss=86.89799
Epoch 2/80: current_loss=86.79250 | best_loss=86.79250
Epoch 3/80: current_loss=86.68776 | best_loss=86.68776
Epoch 4/80: current_loss=86.58520 | best_loss=86.58520
Epoch 5/80: current_loss=86.48560 | best_loss=86.48560
Epoch 6/80: current_loss=86.38466 | best_loss=86.38466
Epoch 7/80: current_loss=86.28468 | best_loss=86.28468
Epoch 8/80: current_loss=86.18439 | best_loss=86.18439
Epoch 9/80: current_loss=86.08699 | best_loss=86.08699
Epoch 10/80: current_loss=85.98956 | best_loss=85.98956
Epoch 11/80: current_loss=85.89185 | best_loss=85.89185
Epoch 12/80: current_loss=85.79389 | best_loss=85.79389
Epoch 13/80: current_loss=85.69771 | best_loss=85.69771
Epoch 14/80: current_loss=85.60213 | best_loss=85.60213
Epoch 15/80: current_loss=85.50436 | best_loss=85.50436
Epoch 16/80: current_loss=85.40768 | best_loss=85.40768
Epoch 17/80: current_loss=85.31243 | best_loss=85.31243
Epoch 18/80: current_loss=85.21519 | best_loss=85.21519
Epoch 19/80: current_loss=85.11760 | best_loss=85.11760
Epoch 20/80: current_loss=85.01866 | best_loss=85.01866
Epoch 21/80: current_loss=84.92045 | best_loss=84.92045
Epoch 22/80: current_loss=84.82203 | best_loss=84.82203
Epoch 23/80: current_loss=84.72076 | best_loss=84.72076
Epoch 24/80: current_loss=84.61765 | best_loss=84.61765
Epoch 25/80: current_loss=84.51498 | best_loss=84.51498
Epoch 26/80: current_loss=84.41298 | best_loss=84.41298
Epoch 27/80: current_loss=84.30888 | best_loss=84.30888
Epoch 28/80: current_loss=84.20211 | best_loss=84.20211
Epoch 29/80: current_loss=84.09402 | best_loss=84.09402
Epoch 30/80: current_loss=83.98482 | best_loss=83.98482
Epoch 31/80: current_loss=83.87390 | best_loss=83.87390
Epoch 32/80: current_loss=83.76013 | best_loss=83.76013
Epoch 33/80: current_loss=83.64500 | best_loss=83.64500
Epoch 34/80: current_loss=83.52938 | best_loss=83.52938
Epoch 35/80: current_loss=83.41078 | best_loss=83.41078
Epoch 36/80: current_loss=83.28965 | best_loss=83.28965
Epoch 37/80: current_loss=83.16717 | best_loss=83.16717
Epoch 38/80: current_loss=83.04322 | best_loss=83.04322
Epoch 39/80: current_loss=82.91474 | best_loss=82.91474
Epoch 40/80: current_loss=82.78233 | best_loss=82.78233
Epoch 41/80: current_loss=82.64501 | best_loss=82.64501
Epoch 42/80: current_loss=82.50565 | best_loss=82.50565
Epoch 43/80: current_loss=82.36174 | best_loss=82.36174
Epoch 44/80: current_loss=82.21544 | best_loss=82.21544
Epoch 45/80: current_loss=82.06521 | best_loss=82.06521
Epoch 46/80: current_loss=81.90835 | best_loss=81.90835
Epoch 47/80: current_loss=81.74834 | best_loss=81.74834
Epoch 48/80: current_loss=81.58337 | best_loss=81.58337
Epoch 49/80: current_loss=81.41105 | best_loss=81.41105
Epoch 50/80: current_loss=81.23351 | best_loss=81.23351
Epoch 51/80: current_loss=81.05023 | best_loss=81.05023
Epoch 52/80: current_loss=80.86034 | best_loss=80.86034
Epoch 53/80: current_loss=80.65980 | best_loss=80.65980
Epoch 54/80: current_loss=80.45133 | best_loss=80.45133
Epoch 55/80: current_loss=80.23738 | best_loss=80.23738
Epoch 56/80: current_loss=80.01527 | best_loss=80.01527
Epoch 57/80: current_loss=79.78125 | best_loss=79.78125
Epoch 58/80: current_loss=79.53115 | best_loss=79.53115
Epoch 59/80: current_loss=79.27514 | best_loss=79.27514
Epoch 60/80: current_loss=79.00219 | best_loss=79.00219
Epoch 61/80: current_loss=78.71224 | best_loss=78.71224
Epoch 62/80: current_loss=78.41139 | best_loss=78.41139
Epoch 63/80: current_loss=78.10083 | best_loss=78.10083
Epoch 64/80: current_loss=77.77510 | best_loss=77.77510
Epoch 65/80: current_loss=77.43105 | best_loss=77.43105
Epoch 66/80: current_loss=77.06712 | best_loss=77.06712
Epoch 67/80: current_loss=76.69378 | best_loss=76.69378
Epoch 68/80: current_loss=76.30109 | best_loss=76.30109
Epoch 69/80: current_loss=75.89372 | best_loss=75.89372
Epoch 70/80: current_loss=75.47074 | best_loss=75.47074
Epoch 71/80: current_loss=75.03565 | best_loss=75.03565
Epoch 72/80: current_loss=74.57834 | best_loss=74.57834
Epoch 73/80: current_loss=74.11158 | best_loss=74.11158
Epoch 74/80: current_loss=73.63272 | best_loss=73.63272
Epoch 75/80: current_loss=73.13608 | best_loss=73.13608
Epoch 76/80: current_loss=72.62773 | best_loss=72.62773
Epoch 77/80: current_loss=72.11016 | best_loss=72.11016
Epoch 78/80: current_loss=71.59079 | best_loss=71.59079
Epoch 79/80: current_loss=71.06240 | best_loss=71.06240
      explained_var=-0.00723 | mse_loss=70.05457
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=80.08284 | best_loss=80.08284
Epoch 1/80: current_loss=79.76897 | best_loss=79.76897
Epoch 2/80: current_loss=79.45207 | best_loss=79.45207
Epoch 3/80: current_loss=79.13341 | best_loss=79.13341
Epoch 4/80: current_loss=78.81027 | best_loss=78.81027
Epoch 5/80: current_loss=78.48953 | best_loss=78.48953
Epoch 6/80: current_loss=78.16672 | best_loss=78.16672
Epoch 7/80: current_loss=77.84496 | best_loss=77.84496
Epoch 8/80: current_loss=77.51707 | best_loss=77.51707
Epoch 9/80: current_loss=77.18903 | best_loss=77.18903
Epoch 10/80: current_loss=76.86018 | best_loss=76.86018
Epoch 11/80: current_loss=76.53290 | best_loss=76.53290
Epoch 12/80: current_loss=76.20982 | best_loss=76.20982
Epoch 13/80: current_loss=75.88038 | best_loss=75.88038
Epoch 14/80: current_loss=75.55707 | best_loss=75.55707
Epoch 15/80: current_loss=75.23370 | best_loss=75.23370
Epoch 16/80: current_loss=74.90583 | best_loss=74.90583
Epoch 17/80: current_loss=74.58057 | best_loss=74.58057
Epoch 18/80: current_loss=74.25503 | best_loss=74.25503
Epoch 19/80: current_loss=73.92911 | best_loss=73.92911
Epoch 20/80: current_loss=73.60124 | best_loss=73.60124
Epoch 21/80: current_loss=73.27804 | best_loss=73.27804
Epoch 22/80: current_loss=72.95863 | best_loss=72.95863
Epoch 23/80: current_loss=72.63934 | best_loss=72.63934
Epoch 24/80: current_loss=72.31886 | best_loss=72.31886
Epoch 25/80: current_loss=71.99809 | best_loss=71.99809
Epoch 26/80: current_loss=71.68241 | best_loss=71.68241
Epoch 27/80: current_loss=71.36665 | best_loss=71.36665
Epoch 28/80: current_loss=71.05556 | best_loss=71.05556
Epoch 29/80: current_loss=70.74134 | best_loss=70.74134
Epoch 30/80: current_loss=70.42839 | best_loss=70.42839
Epoch 31/80: current_loss=70.11301 | best_loss=70.11301
Epoch 32/80: current_loss=69.80141 | best_loss=69.80141
Epoch 33/80: current_loss=69.49141 | best_loss=69.49141
Epoch 34/80: current_loss=69.18265 | best_loss=69.18265
Epoch 35/80: current_loss=68.87597 | best_loss=68.87597
Epoch 36/80: current_loss=68.57235 | best_loss=68.57235
Epoch 37/80: current_loss=68.26994 | best_loss=68.26994
Epoch 38/80: current_loss=67.96896 | best_loss=67.96896
Epoch 39/80: current_loss=67.67164 | best_loss=67.67164
Epoch 40/80: current_loss=67.36883 | best_loss=67.36883
Epoch 41/80: current_loss=67.07308 | best_loss=67.07308
Epoch 42/80: current_loss=66.77666 | best_loss=66.77666
Epoch 43/80: current_loss=66.48073 | best_loss=66.48073
Epoch 44/80: current_loss=66.18587 | best_loss=66.18587
Epoch 45/80: current_loss=65.89273 | best_loss=65.89273
Epoch 46/80: current_loss=65.60978 | best_loss=65.60978
Epoch 47/80: current_loss=65.32498 | best_loss=65.32498
Epoch 48/80: current_loss=65.03977 | best_loss=65.03977
Epoch 49/80: current_loss=64.75600 | best_loss=64.75600
Epoch 50/80: current_loss=64.47693 | best_loss=64.47693
Epoch 51/80: current_loss=64.19700 | best_loss=64.19700
Epoch 52/80: current_loss=63.91988 | best_loss=63.91988
Epoch 53/80: current_loss=63.64302 | best_loss=63.64302
Epoch 54/80: current_loss=63.36783 | best_loss=63.36783
Epoch 55/80: current_loss=63.08938 | best_loss=63.08938
Epoch 56/80: current_loss=62.81481 | best_loss=62.81481
Epoch 57/80: current_loss=62.54253 | best_loss=62.54253
Epoch 58/80: current_loss=62.27146 | best_loss=62.27146
Epoch 59/80: current_loss=61.99900 | best_loss=61.99900
Epoch 60/80: current_loss=61.72874 | best_loss=61.72874
Epoch 61/80: current_loss=61.45768 | best_loss=61.45768
Epoch 62/80: current_loss=61.19084 | best_loss=61.19084
Epoch 63/80: current_loss=60.92429 | best_loss=60.92429
Epoch 64/80: current_loss=60.66294 | best_loss=60.66294
Epoch 65/80: current_loss=60.39979 | best_loss=60.39979
Epoch 66/80: current_loss=60.13845 | best_loss=60.13845
Epoch 67/80: current_loss=59.87898 | best_loss=59.87898
Epoch 68/80: current_loss=59.61951 | best_loss=59.61951
Epoch 69/80: current_loss=59.35947 | best_loss=59.35947
Epoch 70/80: current_loss=59.10694 | best_loss=59.10694
Epoch 71/80: current_loss=58.84901 | best_loss=58.84901
Epoch 72/80: current_loss=58.59287 | best_loss=58.59287
Epoch 73/80: current_loss=58.34050 | best_loss=58.34050
Epoch 74/80: current_loss=58.09121 | best_loss=58.09121
Epoch 75/80: current_loss=57.84691 | best_loss=57.84691
Epoch 76/80: current_loss=57.60067 | best_loss=57.60067
Epoch 77/80: current_loss=57.35318 | best_loss=57.35318
Epoch 78/80: current_loss=57.10992 | best_loss=57.10992
Epoch 79/80: current_loss=56.86678 | best_loss=56.86678
      explained_var=-0.11544 | mse_loss=56.66176
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=61.04439 | best_loss=61.04439
Epoch 1/80: current_loss=60.77098 | best_loss=60.77098
Epoch 2/80: current_loss=60.50284 | best_loss=60.50284
Epoch 3/80: current_loss=60.23691 | best_loss=60.23691
Epoch 4/80: current_loss=59.97721 | best_loss=59.97721
Epoch 5/80: current_loss=59.72069 | best_loss=59.72069
Epoch 6/80: current_loss=59.46797 | best_loss=59.46797
Epoch 7/80: current_loss=59.21250 | best_loss=59.21250
Epoch 8/80: current_loss=58.95931 | best_loss=58.95931
Epoch 9/80: current_loss=58.70931 | best_loss=58.70931
Epoch 10/80: current_loss=58.45761 | best_loss=58.45761
Epoch 11/80: current_loss=58.21140 | best_loss=58.21140
Epoch 12/80: current_loss=57.97223 | best_loss=57.97223
Epoch 13/80: current_loss=57.73338 | best_loss=57.73338
Epoch 14/80: current_loss=57.49557 | best_loss=57.49557
Epoch 15/80: current_loss=57.25768 | best_loss=57.25768
Epoch 16/80: current_loss=57.02215 | best_loss=57.02215
Epoch 17/80: current_loss=56.78688 | best_loss=56.78688
Epoch 18/80: current_loss=56.55554 | best_loss=56.55554
Epoch 19/80: current_loss=56.32849 | best_loss=56.32849
Epoch 20/80: current_loss=56.10253 | best_loss=56.10253
Epoch 21/80: current_loss=55.87915 | best_loss=55.87915
Epoch 22/80: current_loss=55.65608 | best_loss=55.65608
Epoch 23/80: current_loss=55.43330 | best_loss=55.43330
Epoch 24/80: current_loss=55.21892 | best_loss=55.21892
Epoch 25/80: current_loss=55.00449 | best_loss=55.00449
Epoch 26/80: current_loss=54.79166 | best_loss=54.79166
Epoch 27/80: current_loss=54.57939 | best_loss=54.57939
Epoch 28/80: current_loss=54.37027 | best_loss=54.37027
Epoch 29/80: current_loss=54.16442 | best_loss=54.16442
Epoch 30/80: current_loss=53.95724 | best_loss=53.95724
Epoch 31/80: current_loss=53.74960 | best_loss=53.74960
Epoch 32/80: current_loss=53.54678 | best_loss=53.54678
Epoch 33/80: current_loss=53.34125 | best_loss=53.34125
Epoch 34/80: current_loss=53.14102 | best_loss=53.14102
Epoch 35/80: current_loss=52.94019 | best_loss=52.94019
Epoch 36/80: current_loss=52.74261 | best_loss=52.74261
Epoch 37/80: current_loss=52.54553 | best_loss=52.54553
Epoch 38/80: current_loss=52.35219 | best_loss=52.35219
Epoch 39/80: current_loss=52.15892 | best_loss=52.15892
Epoch 40/80: current_loss=51.96694 | best_loss=51.96694
Epoch 41/80: current_loss=51.77501 | best_loss=51.77501
Epoch 42/80: current_loss=51.58420 | best_loss=51.58420
Epoch 43/80: current_loss=51.39756 | best_loss=51.39756
Epoch 44/80: current_loss=51.21235 | best_loss=51.21235
Epoch 45/80: current_loss=51.02737 | best_loss=51.02737
Epoch 46/80: current_loss=50.84162 | best_loss=50.84162
Epoch 47/80: current_loss=50.65969 | best_loss=50.65969
Epoch 48/80: current_loss=50.48113 | best_loss=50.48113
Epoch 49/80: current_loss=50.30229 | best_loss=50.30229
Epoch 50/80: current_loss=50.12189 | best_loss=50.12189
Epoch 51/80: current_loss=49.94544 | best_loss=49.94544
Epoch 52/80: current_loss=49.77087 | best_loss=49.77087
Epoch 53/80: current_loss=49.59919 | best_loss=49.59919
Epoch 54/80: current_loss=49.42770 | best_loss=49.42770
Epoch 55/80: current_loss=49.25556 | best_loss=49.25556
Epoch 56/80: current_loss=49.08368 | best_loss=49.08368
Epoch 57/80: current_loss=48.91494 | best_loss=48.91494
Epoch 58/80: current_loss=48.74884 | best_loss=48.74884
Epoch 59/80: current_loss=48.58311 | best_loss=48.58311
Epoch 60/80: current_loss=48.41768 | best_loss=48.41768
Epoch 61/80: current_loss=48.24962 | best_loss=48.24962
Epoch 62/80: current_loss=48.08356 | best_loss=48.08356
Epoch 63/80: current_loss=47.91944 | best_loss=47.91944
Epoch 64/80: current_loss=47.75483 | best_loss=47.75483
Epoch 65/80: current_loss=47.59037 | best_loss=47.59037
Epoch 66/80: current_loss=47.43213 | best_loss=47.43213
Epoch 67/80: current_loss=47.27293 | best_loss=47.27293
Epoch 68/80: current_loss=47.11110 | best_loss=47.11110
Epoch 69/80: current_loss=46.95104 | best_loss=46.95104
Epoch 70/80: current_loss=46.79758 | best_loss=46.79758
Epoch 71/80: current_loss=46.64330 | best_loss=46.64330
Epoch 72/80: current_loss=46.48889 | best_loss=46.48889
Epoch 73/80: current_loss=46.33526 | best_loss=46.33526
Epoch 74/80: current_loss=46.18045 | best_loss=46.18045
Epoch 75/80: current_loss=46.03025 | best_loss=46.03025
Epoch 76/80: current_loss=45.88079 | best_loss=45.88079
Epoch 77/80: current_loss=45.73368 | best_loss=45.73368
Epoch 78/80: current_loss=45.58595 | best_loss=45.58595
Epoch 79/80: current_loss=45.43635 | best_loss=45.43635
      explained_var=-0.12745 | mse_loss=45.39698
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=40.99881 | best_loss=40.99881
Epoch 1/80: current_loss=40.84037 | best_loss=40.84037
Epoch 2/80: current_loss=40.68341 | best_loss=40.68341
Epoch 3/80: current_loss=40.52841 | best_loss=40.52841
Epoch 4/80: current_loss=40.37412 | best_loss=40.37412
Epoch 5/80: current_loss=40.22044 | best_loss=40.22044
Epoch 6/80: current_loss=40.06914 | best_loss=40.06914
Epoch 7/80: current_loss=39.91786 | best_loss=39.91786
Epoch 8/80: current_loss=39.76848 | best_loss=39.76848
Epoch 9/80: current_loss=39.61890 | best_loss=39.61890
Epoch 10/80: current_loss=39.47284 | best_loss=39.47284
Epoch 11/80: current_loss=39.32700 | best_loss=39.32700
Epoch 12/80: current_loss=39.18331 | best_loss=39.18331
Epoch 13/80: current_loss=39.04351 | best_loss=39.04351
Epoch 14/80: current_loss=38.90236 | best_loss=38.90236
Epoch 15/80: current_loss=38.76374 | best_loss=38.76374
Epoch 16/80: current_loss=38.62435 | best_loss=38.62435
Epoch 17/80: current_loss=38.48790 | best_loss=38.48790
Epoch 18/80: current_loss=38.35019 | best_loss=38.35019
Epoch 19/80: current_loss=38.21434 | best_loss=38.21434
Epoch 20/80: current_loss=38.08056 | best_loss=38.08056
Epoch 21/80: current_loss=37.94734 | best_loss=37.94734
Epoch 22/80: current_loss=37.81459 | best_loss=37.81459
Epoch 23/80: current_loss=37.68304 | best_loss=37.68304
Epoch 24/80: current_loss=37.55155 | best_loss=37.55155
Epoch 25/80: current_loss=37.42465 | best_loss=37.42465
Epoch 26/80: current_loss=37.30049 | best_loss=37.30049
Epoch 27/80: current_loss=37.17595 | best_loss=37.17595
Epoch 28/80: current_loss=37.04975 | best_loss=37.04975
Epoch 29/80: current_loss=36.92434 | best_loss=36.92434
Epoch 30/80: current_loss=36.79784 | best_loss=36.79784
Epoch 31/80: current_loss=36.67495 | best_loss=36.67495
Epoch 32/80: current_loss=36.55147 | best_loss=36.55147
Epoch 33/80: current_loss=36.42876 | best_loss=36.42876
Epoch 34/80: current_loss=36.30614 | best_loss=36.30614
Epoch 35/80: current_loss=36.18634 | best_loss=36.18634
Epoch 36/80: current_loss=36.06766 | best_loss=36.06766
Epoch 37/80: current_loss=35.94677 | best_loss=35.94677
Epoch 38/80: current_loss=35.83022 | best_loss=35.83022
Epoch 39/80: current_loss=35.71330 | best_loss=35.71330
Epoch 40/80: current_loss=35.59599 | best_loss=35.59599
Epoch 41/80: current_loss=35.48053 | best_loss=35.48053
Epoch 42/80: current_loss=35.36750 | best_loss=35.36750
Epoch 43/80: current_loss=35.25285 | best_loss=35.25285
Epoch 44/80: current_loss=35.13911 | best_loss=35.13911
Epoch 45/80: current_loss=35.02423 | best_loss=35.02423
Epoch 46/80: current_loss=34.91536 | best_loss=34.91536
Epoch 47/80: current_loss=34.80628 | best_loss=34.80628
Epoch 48/80: current_loss=34.69436 | best_loss=34.69436
Epoch 49/80: current_loss=34.58449 | best_loss=34.58449
Epoch 50/80: current_loss=34.47504 | best_loss=34.47504
Epoch 51/80: current_loss=34.36522 | best_loss=34.36522
Epoch 52/80: current_loss=34.26067 | best_loss=34.26067
Epoch 53/80: current_loss=34.15251 | best_loss=34.15251
Epoch 54/80: current_loss=34.04720 | best_loss=34.04720
Epoch 55/80: current_loss=33.93950 | best_loss=33.93950
Epoch 56/80: current_loss=33.83369 | best_loss=33.83369
Epoch 57/80: current_loss=33.73099 | best_loss=33.73099
Epoch 58/80: current_loss=33.62825 | best_loss=33.62825
Epoch 59/80: current_loss=33.52257 | best_loss=33.52257
Epoch 60/80: current_loss=33.42060 | best_loss=33.42060
Epoch 61/80: current_loss=33.31824 | best_loss=33.31824
Epoch 62/80: current_loss=33.21710 | best_loss=33.21710
Epoch 63/80: current_loss=33.11535 | best_loss=33.11535
Epoch 64/80: current_loss=33.01279 | best_loss=33.01279
Epoch 65/80: current_loss=32.91150 | best_loss=32.91150
Epoch 66/80: current_loss=32.81244 | best_loss=32.81244
Epoch 67/80: current_loss=32.71280 | best_loss=32.71280
Epoch 68/80: current_loss=32.61750 | best_loss=32.61750
Epoch 69/80: current_loss=32.51984 | best_loss=32.51984
Epoch 70/80: current_loss=32.42236 | best_loss=32.42236
Epoch 71/80: current_loss=32.32584 | best_loss=32.32584
Epoch 72/80: current_loss=32.22856 | best_loss=32.22856
Epoch 73/80: current_loss=32.13070 | best_loss=32.13070
Epoch 74/80: current_loss=32.03374 | best_loss=32.03374
Epoch 75/80: current_loss=31.93904 | best_loss=31.93904
Epoch 76/80: current_loss=31.84411 | best_loss=31.84411
Epoch 77/80: current_loss=31.74947 | best_loss=31.74947
Epoch 78/80: current_loss=31.65648 | best_loss=31.65648
Epoch 79/80: current_loss=31.56593 | best_loss=31.56593
      explained_var=-0.15908 | mse_loss=31.50005
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=30.83034 | best_loss=30.83034
Epoch 1/80: current_loss=30.72412 | best_loss=30.72412
Epoch 2/80: current_loss=30.61927 | best_loss=30.61927
Epoch 3/80: current_loss=30.51317 | best_loss=30.51317
Epoch 4/80: current_loss=30.40953 | best_loss=30.40953
Epoch 5/80: current_loss=30.30749 | best_loss=30.30749
Epoch 6/80: current_loss=30.20318 | best_loss=30.20318
Epoch 7/80: current_loss=30.10275 | best_loss=30.10275
Epoch 8/80: current_loss=30.00435 | best_loss=30.00435
Epoch 9/80: current_loss=29.90552 | best_loss=29.90552
Epoch 10/80: current_loss=29.80851 | best_loss=29.80851
Epoch 11/80: current_loss=29.71339 | best_loss=29.71339
Epoch 12/80: current_loss=29.61827 | best_loss=29.61827
Epoch 13/80: current_loss=29.52376 | best_loss=29.52376
Epoch 14/80: current_loss=29.42984 | best_loss=29.42984
Epoch 15/80: current_loss=29.33642 | best_loss=29.33642
Epoch 16/80: current_loss=29.24266 | best_loss=29.24266
Epoch 17/80: current_loss=29.15017 | best_loss=29.15017
Epoch 18/80: current_loss=29.05937 | best_loss=29.05937
Epoch 19/80: current_loss=28.97238 | best_loss=28.97238
Epoch 20/80: current_loss=28.88373 | best_loss=28.88373
Epoch 21/80: current_loss=28.79512 | best_loss=28.79512
Epoch 22/80: current_loss=28.70540 | best_loss=28.70540
Epoch 23/80: current_loss=28.61650 | best_loss=28.61650
Epoch 24/80: current_loss=28.52826 | best_loss=28.52826
Epoch 25/80: current_loss=28.43858 | best_loss=28.43858
Epoch 26/80: current_loss=28.35231 | best_loss=28.35231
Epoch 27/80: current_loss=28.26428 | best_loss=28.26428
Epoch 28/80: current_loss=28.17746 | best_loss=28.17746
Epoch 29/80: current_loss=28.09150 | best_loss=28.09150
Epoch 30/80: current_loss=28.00636 | best_loss=28.00636
Epoch 31/80: current_loss=27.92461 | best_loss=27.92461
Epoch 32/80: current_loss=27.84264 | best_loss=27.84264
Epoch 33/80: current_loss=27.76174 | best_loss=27.76174
Epoch 34/80: current_loss=27.68255 | best_loss=27.68255
Epoch 35/80: current_loss=27.60227 | best_loss=27.60227
Epoch 36/80: current_loss=27.52130 | best_loss=27.52130
Epoch 37/80: current_loss=27.44129 | best_loss=27.44129
Epoch 38/80: current_loss=27.36097 | best_loss=27.36097
Epoch 39/80: current_loss=27.28215 | best_loss=27.28215
Epoch 40/80: current_loss=27.20458 | best_loss=27.20458
Epoch 41/80: current_loss=27.12625 | best_loss=27.12625
Epoch 42/80: current_loss=27.04666 | best_loss=27.04666
Epoch 43/80: current_loss=26.96828 | best_loss=26.96828
Epoch 44/80: current_loss=26.89314 | best_loss=26.89314
Epoch 45/80: current_loss=26.81558 | best_loss=26.81558
Epoch 46/80: current_loss=26.74032 | best_loss=26.74032
Epoch 47/80: current_loss=26.66471 | best_loss=26.66471
Epoch 48/80: current_loss=26.59025 | best_loss=26.59025
Epoch 49/80: current_loss=26.51608 | best_loss=26.51608
Epoch 50/80: current_loss=26.43998 | best_loss=26.43998
Epoch 51/80: current_loss=26.36675 | best_loss=26.36675
Epoch 52/80: current_loss=26.29306 | best_loss=26.29306
Epoch 53/80: current_loss=26.21953 | best_loss=26.21953
Epoch 54/80: current_loss=26.14547 | best_loss=26.14547
Epoch 55/80: current_loss=26.07181 | best_loss=26.07181
Epoch 56/80: current_loss=25.99782 | best_loss=25.99782
Epoch 57/80: current_loss=25.92471 | best_loss=25.92471
Epoch 58/80: current_loss=25.85217 | best_loss=25.85217
Epoch 59/80: current_loss=25.78166 | best_loss=25.78166
Epoch 60/80: current_loss=25.70932 | best_loss=25.70932
Epoch 61/80: current_loss=25.63889 | best_loss=25.63889
Epoch 62/80: current_loss=25.56624 | best_loss=25.56624
Epoch 63/80: current_loss=25.49580 | best_loss=25.49580
Epoch 64/80: current_loss=25.42755 | best_loss=25.42755
Epoch 65/80: current_loss=25.35963 | best_loss=25.35963
Epoch 66/80: current_loss=25.29147 | best_loss=25.29147
Epoch 67/80: current_loss=25.22378 | best_loss=25.22378
Epoch 68/80: current_loss=25.15537 | best_loss=25.15537
Epoch 69/80: current_loss=25.08724 | best_loss=25.08724
Epoch 70/80: current_loss=25.02032 | best_loss=25.02032
Epoch 71/80: current_loss=24.95380 | best_loss=24.95380
Epoch 72/80: current_loss=24.88582 | best_loss=24.88582
Epoch 73/80: current_loss=24.81865 | best_loss=24.81865
Epoch 74/80: current_loss=24.75170 | best_loss=24.75170
Epoch 75/80: current_loss=24.68459 | best_loss=24.68459
Epoch 76/80: current_loss=24.61765 | best_loss=24.61765
Epoch 77/80: current_loss=24.55186 | best_loss=24.55186
Epoch 78/80: current_loss=24.48694 | best_loss=24.48694
Epoch 79/80: current_loss=24.42315 | best_loss=24.42315
      explained_var=-0.17431 | mse_loss=24.68341
----------------------------------------------
Average early_stopping_point: 80| avg_exp_var=-0.11670| avg_loss=45.65935
----------------------------------------------


----------------------------------------------
Params for Trial 9
{'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=228.35434 | best_loss=228.35434
Epoch 1/80: current_loss=58.44475 | best_loss=58.44475
Epoch 2/80: current_loss=11.49946 | best_loss=11.49946
Epoch 3/80: current_loss=8.59042 | best_loss=8.59042
Epoch 4/80: current_loss=23.55559 | best_loss=8.59042
Epoch 5/80: current_loss=10.50974 | best_loss=8.59042
Epoch 6/80: current_loss=7.94220 | best_loss=7.94220
Epoch 7/80: current_loss=68.14002 | best_loss=7.94220
Epoch 8/80: current_loss=52.68926 | best_loss=7.94220
Epoch 9/80: current_loss=41.16843 | best_loss=7.94220
Epoch 10/80: current_loss=46.65477 | best_loss=7.94220
Epoch 11/80: current_loss=20.58605 | best_loss=7.94220
Epoch 12/80: current_loss=17.13670 | best_loss=7.94220
Epoch 13/80: current_loss=19.76299 | best_loss=7.94220
Epoch 14/80: current_loss=11.00584 | best_loss=7.94220
Epoch 15/80: current_loss=10.42993 | best_loss=7.94220
Epoch 16/80: current_loss=10.64582 | best_loss=7.94220
Epoch 17/80: current_loss=10.40228 | best_loss=7.94220
Epoch 18/80: current_loss=22.93113 | best_loss=7.94220
Epoch 19/80: current_loss=9.38289 | best_loss=7.94220
Epoch 20/80: current_loss=10.20537 | best_loss=7.94220
Epoch 21/80: current_loss=13.63221 | best_loss=7.94220
Epoch 22/80: current_loss=11.54391 | best_loss=7.94220
Epoch 23/80: current_loss=9.00932 | best_loss=7.94220
Epoch 24/80: current_loss=8.64613 | best_loss=7.94220
Epoch 25/80: current_loss=9.82251 | best_loss=7.94220
Epoch 26/80: current_loss=10.33998 | best_loss=7.94220
Early Stopping at epoch 26
      explained_var=-0.01653 | mse_loss=7.80284
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=28.99680 | best_loss=28.99680
Epoch 1/80: current_loss=47.46902 | best_loss=28.99680
Epoch 2/80: current_loss=13.88419 | best_loss=13.88419
Epoch 3/80: current_loss=21.20980 | best_loss=13.88419
Epoch 4/80: current_loss=23.89958 | best_loss=13.88419
Epoch 5/80: current_loss=12.33397 | best_loss=12.33397
Epoch 6/80: current_loss=20.30233 | best_loss=12.33397
Epoch 7/80: current_loss=21.59197 | best_loss=12.33397
Epoch 8/80: current_loss=15.80480 | best_loss=12.33397
Epoch 9/80: current_loss=8.23119 | best_loss=8.23119
Epoch 10/80: current_loss=25.66271 | best_loss=8.23119
Epoch 11/80: current_loss=7.95785 | best_loss=7.95785
Epoch 12/80: current_loss=9.22866 | best_loss=7.95785
Epoch 13/80: current_loss=10.03658 | best_loss=7.95785
Epoch 14/80: current_loss=9.16205 | best_loss=7.95785
Epoch 15/80: current_loss=18.62367 | best_loss=7.95785
Epoch 16/80: current_loss=13.81385 | best_loss=7.95785
Epoch 17/80: current_loss=19.24170 | best_loss=7.95785
Epoch 18/80: current_loss=11.92122 | best_loss=7.95785
Epoch 19/80: current_loss=16.57580 | best_loss=7.95785
Epoch 20/80: current_loss=15.93417 | best_loss=7.95785
Epoch 21/80: current_loss=14.67551 | best_loss=7.95785
Epoch 22/80: current_loss=9.87759 | best_loss=7.95785
Epoch 23/80: current_loss=19.88845 | best_loss=7.95785
Epoch 24/80: current_loss=18.96601 | best_loss=7.95785
Epoch 25/80: current_loss=8.64665 | best_loss=7.95785
Epoch 26/80: current_loss=25.56863 | best_loss=7.95785
Epoch 27/80: current_loss=63.32638 | best_loss=7.95785
Epoch 28/80: current_loss=9.34562 | best_loss=7.95785
Epoch 29/80: current_loss=9.60539 | best_loss=7.95785
Epoch 30/80: current_loss=12.85843 | best_loss=7.95785
Epoch 31/80: current_loss=51.66186 | best_loss=7.95785
Early Stopping at epoch 31
      explained_var=0.03589 | mse_loss=7.90235
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.53625 | best_loss=12.53625
Epoch 1/80: current_loss=14.61762 | best_loss=12.53625
Epoch 2/80: current_loss=9.86463 | best_loss=9.86463
Epoch 3/80: current_loss=8.96499 | best_loss=8.96499
Epoch 4/80: current_loss=17.51868 | best_loss=8.96499
Epoch 5/80: current_loss=9.19181 | best_loss=8.96499
Epoch 6/80: current_loss=10.50362 | best_loss=8.96499
Epoch 7/80: current_loss=9.51884 | best_loss=8.96499
Epoch 8/80: current_loss=43.31316 | best_loss=8.96499
Epoch 9/80: current_loss=16.27726 | best_loss=8.96499
Epoch 10/80: current_loss=24.17719 | best_loss=8.96499
Epoch 11/80: current_loss=9.53853 | best_loss=8.96499
Epoch 12/80: current_loss=22.00056 | best_loss=8.96499
Epoch 13/80: current_loss=16.28805 | best_loss=8.96499
Epoch 14/80: current_loss=14.50475 | best_loss=8.96499
Epoch 15/80: current_loss=11.07352 | best_loss=8.96499
Epoch 16/80: current_loss=12.36827 | best_loss=8.96499
Epoch 17/80: current_loss=10.16574 | best_loss=8.96499
Epoch 18/80: current_loss=17.55856 | best_loss=8.96499
Epoch 19/80: current_loss=9.53015 | best_loss=8.96499
Epoch 20/80: current_loss=13.83693 | best_loss=8.96499
Epoch 21/80: current_loss=16.15378 | best_loss=8.96499
Epoch 22/80: current_loss=19.36712 | best_loss=8.96499
Epoch 23/80: current_loss=10.37291 | best_loss=8.96499
Early Stopping at epoch 23
      explained_var=-0.00560 | mse_loss=8.74273
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.25599 | best_loss=13.25599
Epoch 1/80: current_loss=40.65112 | best_loss=13.25599
Epoch 2/80: current_loss=39.82078 | best_loss=13.25599
Epoch 3/80: current_loss=9.42106 | best_loss=9.42106
Epoch 4/80: current_loss=29.84089 | best_loss=9.42106
Epoch 5/80: current_loss=10.37887 | best_loss=9.42106
Epoch 6/80: current_loss=25.50003 | best_loss=9.42106
Epoch 7/80: current_loss=29.95785 | best_loss=9.42106
Epoch 8/80: current_loss=11.21380 | best_loss=9.42106
Epoch 9/80: current_loss=18.88269 | best_loss=9.42106
Epoch 10/80: current_loss=16.59905 | best_loss=9.42106
Epoch 11/80: current_loss=10.54601 | best_loss=9.42106
Epoch 12/80: current_loss=15.29163 | best_loss=9.42106
Epoch 13/80: current_loss=13.96882 | best_loss=9.42106
Epoch 14/80: current_loss=10.54326 | best_loss=9.42106
Epoch 15/80: current_loss=9.64271 | best_loss=9.42106
Epoch 16/80: current_loss=9.10808 | best_loss=9.10808
Epoch 17/80: current_loss=14.58697 | best_loss=9.10808
Epoch 18/80: current_loss=10.95855 | best_loss=9.10808
Epoch 19/80: current_loss=12.12280 | best_loss=9.10808
Epoch 20/80: current_loss=34.93209 | best_loss=9.10808
Epoch 21/80: current_loss=9.24955 | best_loss=9.10808
Epoch 22/80: current_loss=8.70483 | best_loss=8.70483
Epoch 23/80: current_loss=12.27342 | best_loss=8.70483
Epoch 24/80: current_loss=10.80783 | best_loss=8.70483
Epoch 25/80: current_loss=9.20345 | best_loss=8.70483
Epoch 26/80: current_loss=25.17923 | best_loss=8.70483
Epoch 27/80: current_loss=9.46344 | best_loss=8.70483
Epoch 28/80: current_loss=16.16842 | best_loss=8.70483
Epoch 29/80: current_loss=61.40986 | best_loss=8.70483
Epoch 30/80: current_loss=23.54860 | best_loss=8.70483
Epoch 31/80: current_loss=32.32542 | best_loss=8.70483
Epoch 32/80: current_loss=9.58001 | best_loss=8.70483
Epoch 33/80: current_loss=12.66703 | best_loss=8.70483
Epoch 34/80: current_loss=10.04450 | best_loss=8.70483
Epoch 35/80: current_loss=13.49938 | best_loss=8.70483
Epoch 36/80: current_loss=9.70509 | best_loss=8.70483
Epoch 37/80: current_loss=8.44604 | best_loss=8.44604
Epoch 38/80: current_loss=9.65476 | best_loss=8.44604
Epoch 39/80: current_loss=11.87372 | best_loss=8.44604
Epoch 40/80: current_loss=17.38908 | best_loss=8.44604
Epoch 41/80: current_loss=12.08401 | best_loss=8.44604
Epoch 42/80: current_loss=15.13805 | best_loss=8.44604
Epoch 43/80: current_loss=11.25610 | best_loss=8.44604
Epoch 44/80: current_loss=9.03584 | best_loss=8.44604
Epoch 45/80: current_loss=11.20872 | best_loss=8.44604
Epoch 46/80: current_loss=10.26259 | best_loss=8.44604
Epoch 47/80: current_loss=10.86883 | best_loss=8.44604
Epoch 48/80: current_loss=10.40626 | best_loss=8.44604
Epoch 49/80: current_loss=11.79360 | best_loss=8.44604
Epoch 50/80: current_loss=10.32977 | best_loss=8.44604
Epoch 51/80: current_loss=20.99955 | best_loss=8.44604
Epoch 52/80: current_loss=17.04862 | best_loss=8.44604
Epoch 53/80: current_loss=11.13409 | best_loss=8.44604
Epoch 54/80: current_loss=9.32502 | best_loss=8.44604
Epoch 55/80: current_loss=22.60452 | best_loss=8.44604
Epoch 56/80: current_loss=9.67059 | best_loss=8.44604
Epoch 57/80: current_loss=10.79217 | best_loss=8.44604
Early Stopping at epoch 57
      explained_var=-0.01776 | mse_loss=8.48409
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=17.64262 | best_loss=17.64262
Epoch 1/80: current_loss=22.37857 | best_loss=17.64262
Epoch 2/80: current_loss=18.04573 | best_loss=17.64262
Epoch 3/80: current_loss=14.56260 | best_loss=14.56260
Epoch 4/80: current_loss=10.49693 | best_loss=10.49693
Epoch 5/80: current_loss=11.50479 | best_loss=10.49693
Epoch 6/80: current_loss=11.25218 | best_loss=10.49693
Epoch 7/80: current_loss=8.68545 | best_loss=8.68545
Epoch 8/80: current_loss=9.10941 | best_loss=8.68545
Epoch 9/80: current_loss=19.60160 | best_loss=8.68545
Epoch 10/80: current_loss=8.72923 | best_loss=8.68545
Epoch 11/80: current_loss=44.83161 | best_loss=8.68545
Epoch 12/80: current_loss=10.13978 | best_loss=8.68545
Epoch 13/80: current_loss=14.32554 | best_loss=8.68545
Epoch 14/80: current_loss=12.22276 | best_loss=8.68545
Epoch 15/80: current_loss=13.91288 | best_loss=8.68545
Epoch 16/80: current_loss=26.45610 | best_loss=8.68545
Epoch 17/80: current_loss=14.79156 | best_loss=8.68545
Epoch 18/80: current_loss=15.37076 | best_loss=8.68545
Epoch 19/80: current_loss=20.60369 | best_loss=8.68545
Epoch 20/80: current_loss=23.91280 | best_loss=8.68545
Epoch 21/80: current_loss=18.81382 | best_loss=8.68545
Epoch 22/80: current_loss=10.42880 | best_loss=8.68545
Epoch 23/80: current_loss=21.22748 | best_loss=8.68545
Epoch 24/80: current_loss=19.70418 | best_loss=8.68545
Epoch 25/80: current_loss=21.45584 | best_loss=8.68545
Epoch 26/80: current_loss=47.15599 | best_loss=8.68545
Epoch 27/80: current_loss=9.64727 | best_loss=8.68545
Early Stopping at epoch 27
      explained_var=-0.12389 | mse_loss=8.84138
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=-0.02558| avg_loss=8.35468
----------------------------------------------


----------------------------------------------
Params for Trial 10
{'learning_rate': 0.01, 'weight_decay': 0.009325991845748673, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=18.16000 | best_loss=18.16000
Epoch 1/80: current_loss=7.76530 | best_loss=7.76530
Epoch 2/80: current_loss=9.90775 | best_loss=7.76530
Epoch 3/80: current_loss=9.08041 | best_loss=7.76530
Epoch 4/80: current_loss=7.96709 | best_loss=7.76530
Epoch 5/80: current_loss=7.85537 | best_loss=7.76530
Epoch 6/80: current_loss=8.05294 | best_loss=7.76530
Epoch 7/80: current_loss=8.17542 | best_loss=7.76530
Epoch 8/80: current_loss=8.08829 | best_loss=7.76530
Epoch 9/80: current_loss=8.09239 | best_loss=7.76530
Epoch 10/80: current_loss=8.03449 | best_loss=7.76530
Epoch 11/80: current_loss=7.99282 | best_loss=7.76530
Epoch 12/80: current_loss=8.12596 | best_loss=7.76530
Epoch 13/80: current_loss=8.13036 | best_loss=7.76530
Epoch 14/80: current_loss=8.04944 | best_loss=7.76530
Epoch 15/80: current_loss=7.97048 | best_loss=7.76530
Epoch 16/80: current_loss=8.16885 | best_loss=7.76530
Epoch 17/80: current_loss=8.17332 | best_loss=7.76530
Epoch 18/80: current_loss=7.94779 | best_loss=7.76530
Epoch 19/80: current_loss=7.90587 | best_loss=7.76530
Epoch 20/80: current_loss=7.96723 | best_loss=7.76530
Epoch 21/80: current_loss=8.00803 | best_loss=7.76530
Early Stopping at epoch 21
      explained_var=0.00077 | mse_loss=7.58642
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.32026 | best_loss=8.32026
Epoch 1/80: current_loss=8.45550 | best_loss=8.32026
Epoch 2/80: current_loss=8.32489 | best_loss=8.32026
Epoch 3/80: current_loss=8.34546 | best_loss=8.32026
Epoch 4/80: current_loss=8.36192 | best_loss=8.32026
Epoch 5/80: current_loss=8.33834 | best_loss=8.32026
Epoch 6/80: current_loss=8.57795 | best_loss=8.32026
Epoch 7/80: current_loss=8.34484 | best_loss=8.32026
Epoch 8/80: current_loss=8.49162 | best_loss=8.32026
Epoch 9/80: current_loss=8.32890 | best_loss=8.32026
Epoch 10/80: current_loss=8.37091 | best_loss=8.32026
Epoch 11/80: current_loss=8.38389 | best_loss=8.32026
Epoch 12/80: current_loss=8.39516 | best_loss=8.32026
Epoch 13/80: current_loss=8.50293 | best_loss=8.32026
Epoch 14/80: current_loss=8.34126 | best_loss=8.32026
Epoch 15/80: current_loss=8.49834 | best_loss=8.32026
Epoch 16/80: current_loss=8.32482 | best_loss=8.32026
Epoch 17/80: current_loss=8.34062 | best_loss=8.32026
Epoch 18/80: current_loss=8.35015 | best_loss=8.32026
Epoch 19/80: current_loss=8.43967 | best_loss=8.32026
Epoch 20/80: current_loss=8.32943 | best_loss=8.32026
Early Stopping at epoch 20
      explained_var=-0.00100 | mse_loss=8.17360
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.34179 | best_loss=9.34179
Epoch 1/80: current_loss=8.86926 | best_loss=8.86926
Epoch 2/80: current_loss=9.20826 | best_loss=8.86926
Epoch 3/80: current_loss=8.87920 | best_loss=8.86926
Epoch 4/80: current_loss=8.88773 | best_loss=8.86926
Epoch 5/80: current_loss=9.05208 | best_loss=8.86926
Epoch 6/80: current_loss=8.93030 | best_loss=8.86926
Epoch 7/80: current_loss=8.93842 | best_loss=8.86926
Epoch 8/80: current_loss=8.97438 | best_loss=8.86926
Epoch 9/80: current_loss=9.00295 | best_loss=8.86926
Epoch 10/80: current_loss=9.07205 | best_loss=8.86926
Epoch 11/80: current_loss=9.22377 | best_loss=8.86926
Epoch 12/80: current_loss=8.97883 | best_loss=8.86926
Epoch 13/80: current_loss=8.99753 | best_loss=8.86926
Epoch 14/80: current_loss=8.95314 | best_loss=8.86926
Epoch 15/80: current_loss=9.01168 | best_loss=8.86926
Epoch 16/80: current_loss=9.02609 | best_loss=8.86926
Epoch 17/80: current_loss=8.85535 | best_loss=8.85535
Epoch 18/80: current_loss=9.51216 | best_loss=8.85535
Epoch 19/80: current_loss=9.05209 | best_loss=8.85535
Epoch 20/80: current_loss=9.03657 | best_loss=8.85535
Epoch 21/80: current_loss=9.14326 | best_loss=8.85535
Epoch 22/80: current_loss=8.95137 | best_loss=8.85535
Epoch 23/80: current_loss=8.92829 | best_loss=8.85535
Epoch 24/80: current_loss=9.10293 | best_loss=8.85535
Epoch 25/80: current_loss=8.83531 | best_loss=8.83531
Epoch 26/80: current_loss=9.06974 | best_loss=8.83531
Epoch 27/80: current_loss=8.91889 | best_loss=8.83531
Epoch 28/80: current_loss=9.20317 | best_loss=8.83531
Epoch 29/80: current_loss=8.87940 | best_loss=8.83531
Epoch 30/80: current_loss=8.85104 | best_loss=8.83531
Epoch 31/80: current_loss=9.15604 | best_loss=8.83531
Epoch 32/80: current_loss=8.90070 | best_loss=8.83531
Epoch 33/80: current_loss=9.24935 | best_loss=8.83531
Epoch 34/80: current_loss=8.82517 | best_loss=8.82517
Epoch 35/80: current_loss=9.11159 | best_loss=8.82517
Epoch 36/80: current_loss=8.84335 | best_loss=8.82517
Epoch 37/80: current_loss=9.65022 | best_loss=8.82517
Epoch 38/80: current_loss=8.84757 | best_loss=8.82517
Epoch 39/80: current_loss=9.02817 | best_loss=8.82517
Epoch 40/80: current_loss=8.82047 | best_loss=8.82047
Epoch 41/80: current_loss=9.28910 | best_loss=8.82047
Epoch 42/80: current_loss=8.84833 | best_loss=8.82047
Epoch 43/80: current_loss=9.11292 | best_loss=8.82047
Epoch 44/80: current_loss=8.89923 | best_loss=8.82047
Epoch 45/80: current_loss=9.34184 | best_loss=8.82047
Epoch 46/80: current_loss=8.95597 | best_loss=8.82047
Epoch 47/80: current_loss=8.79519 | best_loss=8.79519
Epoch 48/80: current_loss=9.07781 | best_loss=8.79519
Epoch 49/80: current_loss=9.00464 | best_loss=8.79519
Epoch 50/80: current_loss=9.00040 | best_loss=8.79519
Epoch 51/80: current_loss=8.87033 | best_loss=8.79519
Epoch 52/80: current_loss=8.89991 | best_loss=8.79519
Epoch 53/80: current_loss=9.10364 | best_loss=8.79519
Epoch 54/80: current_loss=8.95706 | best_loss=8.79519
Epoch 55/80: current_loss=9.52560 | best_loss=8.79519
Epoch 56/80: current_loss=8.82076 | best_loss=8.79519
Epoch 57/80: current_loss=9.21099 | best_loss=8.79519
Epoch 58/80: current_loss=8.90781 | best_loss=8.79519
Epoch 59/80: current_loss=8.98484 | best_loss=8.79519
Epoch 60/80: current_loss=9.20367 | best_loss=8.79519
Epoch 61/80: current_loss=8.88491 | best_loss=8.79519
Epoch 62/80: current_loss=9.19916 | best_loss=8.79519
Epoch 63/80: current_loss=8.95989 | best_loss=8.79519
Epoch 64/80: current_loss=9.07361 | best_loss=8.79519
Epoch 65/80: current_loss=8.85502 | best_loss=8.79519
Epoch 66/80: current_loss=9.26141 | best_loss=8.79519
Epoch 67/80: current_loss=8.94981 | best_loss=8.79519
Early Stopping at epoch 67
      explained_var=-0.00172 | mse_loss=8.57222
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.23520 | best_loss=8.23520
Epoch 1/80: current_loss=8.28092 | best_loss=8.23520
Epoch 2/80: current_loss=8.21822 | best_loss=8.21822
Epoch 3/80: current_loss=8.36572 | best_loss=8.21822
Epoch 4/80: current_loss=8.22155 | best_loss=8.21822
Epoch 5/80: current_loss=8.26581 | best_loss=8.21822
Epoch 6/80: current_loss=8.23800 | best_loss=8.21822
Epoch 7/80: current_loss=8.21659 | best_loss=8.21659
Epoch 8/80: current_loss=8.21545 | best_loss=8.21545
Epoch 9/80: current_loss=8.29825 | best_loss=8.21545
Epoch 10/80: current_loss=8.22778 | best_loss=8.21545
Epoch 11/80: current_loss=8.21987 | best_loss=8.21545
Epoch 12/80: current_loss=8.26963 | best_loss=8.21545
Epoch 13/80: current_loss=8.22081 | best_loss=8.21545
Epoch 14/80: current_loss=8.22647 | best_loss=8.21545
Epoch 15/80: current_loss=8.22337 | best_loss=8.21545
Epoch 16/80: current_loss=8.33599 | best_loss=8.21545
Epoch 17/80: current_loss=8.22074 | best_loss=8.21545
Epoch 18/80: current_loss=8.22167 | best_loss=8.21545
Epoch 19/80: current_loss=8.21934 | best_loss=8.21545
Epoch 20/80: current_loss=8.25443 | best_loss=8.21545
Epoch 21/80: current_loss=8.27180 | best_loss=8.21545
Epoch 22/80: current_loss=8.23891 | best_loss=8.21545
Epoch 23/80: current_loss=8.27255 | best_loss=8.21545
Epoch 24/80: current_loss=8.22592 | best_loss=8.21545
Epoch 25/80: current_loss=8.22422 | best_loss=8.21545
Epoch 26/80: current_loss=8.31155 | best_loss=8.21545
Epoch 27/80: current_loss=8.21787 | best_loss=8.21545
Epoch 28/80: current_loss=8.22406 | best_loss=8.21545
Early Stopping at epoch 28
      explained_var=0.00221 | mse_loss=8.31357
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.08667 | best_loss=8.08667
Epoch 1/80: current_loss=7.74057 | best_loss=7.74057
Epoch 2/80: current_loss=7.73959 | best_loss=7.73959
Epoch 3/80: current_loss=7.69905 | best_loss=7.69905
Epoch 4/80: current_loss=7.70158 | best_loss=7.69905
Epoch 5/80: current_loss=7.72822 | best_loss=7.69905
Epoch 6/80: current_loss=7.71266 | best_loss=7.69905
Epoch 7/80: current_loss=7.73261 | best_loss=7.69905
Epoch 8/80: current_loss=7.70767 | best_loss=7.69905
Epoch 9/80: current_loss=7.69076 | best_loss=7.69076
Epoch 10/80: current_loss=7.70823 | best_loss=7.69076
Epoch 11/80: current_loss=7.70287 | best_loss=7.69076
Epoch 12/80: current_loss=7.72216 | best_loss=7.69076
Epoch 13/80: current_loss=7.81473 | best_loss=7.69076
Epoch 14/80: current_loss=7.91624 | best_loss=7.69076
Epoch 15/80: current_loss=7.70893 | best_loss=7.69076
Epoch 16/80: current_loss=7.68554 | best_loss=7.68554
Epoch 17/80: current_loss=7.69484 | best_loss=7.68554
Epoch 18/80: current_loss=7.68894 | best_loss=7.68554
Epoch 19/80: current_loss=7.81370 | best_loss=7.68554
Epoch 20/80: current_loss=7.76046 | best_loss=7.68554
Epoch 21/80: current_loss=7.77091 | best_loss=7.68554
Epoch 22/80: current_loss=7.69892 | best_loss=7.68554
Epoch 23/80: current_loss=7.69705 | best_loss=7.68554
Epoch 24/80: current_loss=7.69863 | best_loss=7.68554
Epoch 25/80: current_loss=7.82673 | best_loss=7.68554
Epoch 26/80: current_loss=7.79625 | best_loss=7.68554
Epoch 27/80: current_loss=7.73824 | best_loss=7.68554
Epoch 28/80: current_loss=7.84517 | best_loss=7.68554
Epoch 29/80: current_loss=7.75591 | best_loss=7.68554
Epoch 30/80: current_loss=7.80347 | best_loss=7.68554
Epoch 31/80: current_loss=7.68918 | best_loss=7.68554
Epoch 32/80: current_loss=7.70124 | best_loss=7.68554
Epoch 33/80: current_loss=7.79667 | best_loss=7.68554
Epoch 34/80: current_loss=7.78624 | best_loss=7.68554
Epoch 35/80: current_loss=8.01647 | best_loss=7.68554
Epoch 36/80: current_loss=7.77586 | best_loss=7.68554
Early Stopping at epoch 36
      explained_var=-0.00065 | mse_loss=7.86093
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=-0.00008| avg_loss=8.10135
----------------------------------------------


----------------------------------------------
Params for Trial 11
{'learning_rate': 0.01, 'weight_decay': 0.009853936657752537, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=15.24239 | best_loss=15.24239
Epoch 1/80: current_loss=7.91508 | best_loss=7.91508
Epoch 2/80: current_loss=9.94984 | best_loss=7.91508
Epoch 3/80: current_loss=8.66603 | best_loss=7.91508
Epoch 4/80: current_loss=7.95585 | best_loss=7.91508
Epoch 5/80: current_loss=7.98627 | best_loss=7.91508
Epoch 6/80: current_loss=8.12786 | best_loss=7.91508
Epoch 7/80: current_loss=8.00943 | best_loss=7.91508
Epoch 8/80: current_loss=8.06170 | best_loss=7.91508
Epoch 9/80: current_loss=8.02457 | best_loss=7.91508
Epoch 10/80: current_loss=8.12687 | best_loss=7.91508
Epoch 11/80: current_loss=8.02130 | best_loss=7.91508
Epoch 12/80: current_loss=8.16613 | best_loss=7.91508
Epoch 13/80: current_loss=7.82748 | best_loss=7.82748
Epoch 14/80: current_loss=7.90512 | best_loss=7.82748
Epoch 15/80: current_loss=8.19011 | best_loss=7.82748
Epoch 16/80: current_loss=7.99993 | best_loss=7.82748
Epoch 17/80: current_loss=8.00284 | best_loss=7.82748
Epoch 18/80: current_loss=8.12165 | best_loss=7.82748
Epoch 19/80: current_loss=7.84089 | best_loss=7.82748
Epoch 20/80: current_loss=8.15437 | best_loss=7.82748
Epoch 21/80: current_loss=8.07173 | best_loss=7.82748
Epoch 22/80: current_loss=7.97492 | best_loss=7.82748
Epoch 23/80: current_loss=8.09108 | best_loss=7.82748
Epoch 24/80: current_loss=7.90722 | best_loss=7.82748
Epoch 25/80: current_loss=8.14884 | best_loss=7.82748
Epoch 26/80: current_loss=8.07555 | best_loss=7.82748
Epoch 27/80: current_loss=8.00321 | best_loss=7.82748
Epoch 28/80: current_loss=8.32176 | best_loss=7.82748
Epoch 29/80: current_loss=7.88261 | best_loss=7.82748
Epoch 30/80: current_loss=8.16863 | best_loss=7.82748
Epoch 31/80: current_loss=7.95716 | best_loss=7.82748
Epoch 32/80: current_loss=8.19143 | best_loss=7.82748
Epoch 33/80: current_loss=8.02066 | best_loss=7.82748
Early Stopping at epoch 33
      explained_var=0.00335 | mse_loss=7.67647
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.34716 | best_loss=8.34716
Epoch 1/80: current_loss=8.46386 | best_loss=8.34716
Epoch 2/80: current_loss=8.42171 | best_loss=8.34716
Epoch 3/80: current_loss=8.35430 | best_loss=8.34716
Epoch 4/80: current_loss=8.32630 | best_loss=8.32630
Epoch 5/80: current_loss=8.43615 | best_loss=8.32630
Epoch 6/80: current_loss=8.32816 | best_loss=8.32630
Epoch 7/80: current_loss=8.46349 | best_loss=8.32630
Epoch 8/80: current_loss=8.36242 | best_loss=8.32630
Epoch 9/80: current_loss=8.37600 | best_loss=8.32630
Epoch 10/80: current_loss=8.50523 | best_loss=8.32630
Epoch 11/80: current_loss=8.39441 | best_loss=8.32630
Epoch 12/80: current_loss=8.33286 | best_loss=8.32630
Epoch 13/80: current_loss=8.35466 | best_loss=8.32630
Epoch 14/80: current_loss=8.32340 | best_loss=8.32340
Epoch 15/80: current_loss=8.41738 | best_loss=8.32340
Epoch 16/80: current_loss=8.32801 | best_loss=8.32340
Epoch 17/80: current_loss=8.33043 | best_loss=8.32340
Epoch 18/80: current_loss=8.63433 | best_loss=8.32340
Epoch 19/80: current_loss=9.47268 | best_loss=8.32340
Epoch 20/80: current_loss=8.53064 | best_loss=8.32340
Epoch 21/80: current_loss=8.37402 | best_loss=8.32340
Epoch 22/80: current_loss=8.33085 | best_loss=8.32340
Epoch 23/80: current_loss=8.38196 | best_loss=8.32340
Epoch 24/80: current_loss=8.38495 | best_loss=8.32340
Epoch 25/80: current_loss=8.38751 | best_loss=8.32340
Epoch 26/80: current_loss=8.51608 | best_loss=8.32340
Epoch 27/80: current_loss=8.34582 | best_loss=8.32340
Epoch 28/80: current_loss=8.35029 | best_loss=8.32340
Epoch 29/80: current_loss=8.33755 | best_loss=8.32340
Epoch 30/80: current_loss=8.50592 | best_loss=8.32340
Epoch 31/80: current_loss=8.37333 | best_loss=8.32340
Epoch 32/80: current_loss=8.40853 | best_loss=8.32340
Epoch 33/80: current_loss=8.42301 | best_loss=8.32340
Epoch 34/80: current_loss=8.39779 | best_loss=8.32340
Early Stopping at epoch 34
      explained_var=-0.00020 | mse_loss=8.17546
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.95792 | best_loss=8.95792
Epoch 1/80: current_loss=8.91737 | best_loss=8.91737
Epoch 2/80: current_loss=9.14248 | best_loss=8.91737
Epoch 3/80: current_loss=8.96368 | best_loss=8.91737
Epoch 4/80: current_loss=9.01145 | best_loss=8.91737
Epoch 5/80: current_loss=8.88074 | best_loss=8.88074
Epoch 6/80: current_loss=8.85035 | best_loss=8.85035
Epoch 7/80: current_loss=9.20433 | best_loss=8.85035
Epoch 8/80: current_loss=8.83032 | best_loss=8.83032
Epoch 9/80: current_loss=8.92079 | best_loss=8.83032
Epoch 10/80: current_loss=8.96175 | best_loss=8.83032
Epoch 11/80: current_loss=8.80734 | best_loss=8.80734
Epoch 12/80: current_loss=9.16492 | best_loss=8.80734
Epoch 13/80: current_loss=9.34247 | best_loss=8.80734
Epoch 14/80: current_loss=8.86897 | best_loss=8.80734
Epoch 15/80: current_loss=9.07551 | best_loss=8.80734
Epoch 16/80: current_loss=8.93582 | best_loss=8.80734
Epoch 17/80: current_loss=9.05376 | best_loss=8.80734
Epoch 18/80: current_loss=8.98458 | best_loss=8.80734
Epoch 19/80: current_loss=9.04028 | best_loss=8.80734
Epoch 20/80: current_loss=9.07468 | best_loss=8.80734
Epoch 21/80: current_loss=9.05175 | best_loss=8.80734
Epoch 22/80: current_loss=9.24570 | best_loss=8.80734
Epoch 23/80: current_loss=8.82359 | best_loss=8.80734
Epoch 24/80: current_loss=9.13868 | best_loss=8.80734
Epoch 25/80: current_loss=11.34058 | best_loss=8.80734
Epoch 26/80: current_loss=8.82885 | best_loss=8.80734
Epoch 27/80: current_loss=9.20326 | best_loss=8.80734
Epoch 28/80: current_loss=8.94519 | best_loss=8.80734
Epoch 29/80: current_loss=9.00544 | best_loss=8.80734
Epoch 30/80: current_loss=9.14329 | best_loss=8.80734
Epoch 31/80: current_loss=8.85175 | best_loss=8.80734
Early Stopping at epoch 31
      explained_var=-0.00103 | mse_loss=8.58427
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.58330 | best_loss=8.58330
Epoch 1/80: current_loss=8.23999 | best_loss=8.23999
Epoch 2/80: current_loss=8.31608 | best_loss=8.23999
Epoch 3/80: current_loss=8.21678 | best_loss=8.21678
Epoch 4/80: current_loss=8.35780 | best_loss=8.21678
Epoch 5/80: current_loss=8.22069 | best_loss=8.21678
Epoch 6/80: current_loss=8.23063 | best_loss=8.21678
Epoch 7/80: current_loss=8.25541 | best_loss=8.21678
Epoch 8/80: current_loss=8.21771 | best_loss=8.21678
Epoch 9/80: current_loss=8.43904 | best_loss=8.21678
Epoch 10/80: current_loss=8.22159 | best_loss=8.21678
Epoch 11/80: current_loss=8.22305 | best_loss=8.21678
Epoch 12/80: current_loss=8.25442 | best_loss=8.21678
Epoch 13/80: current_loss=8.22913 | best_loss=8.21678
Epoch 14/80: current_loss=8.25049 | best_loss=8.21678
Epoch 15/80: current_loss=8.21656 | best_loss=8.21656
Epoch 16/80: current_loss=8.30823 | best_loss=8.21656
Epoch 17/80: current_loss=8.22003 | best_loss=8.21656
Epoch 18/80: current_loss=8.26086 | best_loss=8.21656
Epoch 19/80: current_loss=8.30096 | best_loss=8.21656
Epoch 20/80: current_loss=8.23193 | best_loss=8.21656
Epoch 21/80: current_loss=8.23138 | best_loss=8.21656
Epoch 22/80: current_loss=8.37385 | best_loss=8.21656
Epoch 23/80: current_loss=8.23209 | best_loss=8.21656
Epoch 24/80: current_loss=8.22753 | best_loss=8.21656
Epoch 25/80: current_loss=8.21565 | best_loss=8.21565
Epoch 26/80: current_loss=8.22853 | best_loss=8.21565
Epoch 27/80: current_loss=8.22307 | best_loss=8.21565
Epoch 28/80: current_loss=8.21988 | best_loss=8.21565
Epoch 29/80: current_loss=8.40178 | best_loss=8.21565
Epoch 30/80: current_loss=8.21722 | best_loss=8.21565
Epoch 31/80: current_loss=8.22245 | best_loss=8.21565
Epoch 32/80: current_loss=8.21811 | best_loss=8.21565
Epoch 33/80: current_loss=8.38205 | best_loss=8.21565
Epoch 34/80: current_loss=8.22499 | best_loss=8.21565
Epoch 35/80: current_loss=8.22582 | best_loss=8.21565
Epoch 36/80: current_loss=8.25441 | best_loss=8.21565
Epoch 37/80: current_loss=8.22405 | best_loss=8.21565
Epoch 38/80: current_loss=8.22685 | best_loss=8.21565
Epoch 39/80: current_loss=8.23258 | best_loss=8.21565
Epoch 40/80: current_loss=8.22405 | best_loss=8.21565
Epoch 41/80: current_loss=8.31534 | best_loss=8.21565
Epoch 42/80: current_loss=8.22780 | best_loss=8.21565
Epoch 43/80: current_loss=8.29860 | best_loss=8.21565
Epoch 44/80: current_loss=8.22809 | best_loss=8.21565
Epoch 45/80: current_loss=8.22464 | best_loss=8.21565
Early Stopping at epoch 45
      explained_var=0.00220 | mse_loss=8.31355
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.76031 | best_loss=7.76031
Epoch 1/80: current_loss=7.78638 | best_loss=7.76031
Epoch 2/80: current_loss=8.32755 | best_loss=7.76031
Epoch 3/80: current_loss=7.73231 | best_loss=7.73231
Epoch 4/80: current_loss=7.70782 | best_loss=7.70782
Epoch 5/80: current_loss=7.76927 | best_loss=7.70782
Epoch 6/80: current_loss=7.73159 | best_loss=7.70782
Epoch 7/80: current_loss=7.74901 | best_loss=7.70782
Epoch 8/80: current_loss=7.73745 | best_loss=7.70782
Epoch 9/80: current_loss=7.69572 | best_loss=7.69572
Epoch 10/80: current_loss=7.87351 | best_loss=7.69572
Epoch 11/80: current_loss=7.69431 | best_loss=7.69431
Epoch 12/80: current_loss=7.76386 | best_loss=7.69431
Epoch 13/80: current_loss=7.75806 | best_loss=7.69431
Epoch 14/80: current_loss=7.95881 | best_loss=7.69431
Epoch 15/80: current_loss=7.70421 | best_loss=7.69431
Epoch 16/80: current_loss=7.74922 | best_loss=7.69431
Epoch 17/80: current_loss=7.71570 | best_loss=7.69431
Epoch 18/80: current_loss=7.77528 | best_loss=7.69431
Epoch 19/80: current_loss=7.79242 | best_loss=7.69431
Epoch 20/80: current_loss=7.69824 | best_loss=7.69431
Epoch 21/80: current_loss=7.70866 | best_loss=7.69431
Epoch 22/80: current_loss=7.70134 | best_loss=7.69431
Epoch 23/80: current_loss=7.76603 | best_loss=7.69431
Epoch 24/80: current_loss=7.72942 | best_loss=7.69431
Epoch 25/80: current_loss=7.68961 | best_loss=7.68961
Epoch 26/80: current_loss=7.69436 | best_loss=7.68961
Epoch 27/80: current_loss=7.74291 | best_loss=7.68961
Epoch 28/80: current_loss=7.73186 | best_loss=7.68961
Epoch 29/80: current_loss=7.69366 | best_loss=7.68961
Epoch 30/80: current_loss=7.79158 | best_loss=7.68961
Epoch 31/80: current_loss=7.73341 | best_loss=7.68961
Epoch 32/80: current_loss=7.69813 | best_loss=7.68961
Epoch 33/80: current_loss=7.70658 | best_loss=7.68961
Epoch 34/80: current_loss=7.68745 | best_loss=7.68745
Epoch 35/80: current_loss=7.70111 | best_loss=7.68745
Epoch 36/80: current_loss=7.73295 | best_loss=7.68745
Epoch 37/80: current_loss=7.68995 | best_loss=7.68745
Epoch 38/80: current_loss=7.70082 | best_loss=7.68745
Epoch 39/80: current_loss=7.78426 | best_loss=7.68745
Epoch 40/80: current_loss=7.76296 | best_loss=7.68745
Epoch 41/80: current_loss=7.96870 | best_loss=7.68745
Epoch 42/80: current_loss=7.72720 | best_loss=7.68745
Epoch 43/80: current_loss=7.74784 | best_loss=7.68745
Epoch 44/80: current_loss=7.70055 | best_loss=7.68745
Epoch 45/80: current_loss=7.69202 | best_loss=7.68745
Epoch 46/80: current_loss=7.78878 | best_loss=7.68745
Epoch 47/80: current_loss=7.68995 | best_loss=7.68745
Epoch 48/80: current_loss=7.76119 | best_loss=7.68745
Epoch 49/80: current_loss=7.72354 | best_loss=7.68745
Epoch 50/80: current_loss=8.06591 | best_loss=7.68745
Epoch 51/80: current_loss=7.68996 | best_loss=7.68745
Epoch 52/80: current_loss=7.69702 | best_loss=7.68745
Epoch 53/80: current_loss=7.76876 | best_loss=7.68745
Epoch 54/80: current_loss=7.68984 | best_loss=7.68745
Early Stopping at epoch 54
      explained_var=-0.00106 | mse_loss=7.86458
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00065| avg_loss=8.12286
----------------------------------------------


----------------------------------------------
Params for Trial 12
{'learning_rate': 0.01, 'weight_decay': 0.009685524867852728, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=15.83925 | best_loss=15.83925
Epoch 1/80: current_loss=7.90060 | best_loss=7.90060
Epoch 2/80: current_loss=10.11864 | best_loss=7.90060
Epoch 3/80: current_loss=8.73014 | best_loss=7.90060
Epoch 4/80: current_loss=7.92637 | best_loss=7.90060
Epoch 5/80: current_loss=7.87807 | best_loss=7.87807
Epoch 6/80: current_loss=8.09663 | best_loss=7.87807
Epoch 7/80: current_loss=8.21270 | best_loss=7.87807
Epoch 8/80: current_loss=7.94888 | best_loss=7.87807
Epoch 9/80: current_loss=8.06082 | best_loss=7.87807
Epoch 10/80: current_loss=8.14498 | best_loss=7.87807
Epoch 11/80: current_loss=7.99686 | best_loss=7.87807
Epoch 12/80: current_loss=8.12338 | best_loss=7.87807
Epoch 13/80: current_loss=8.04429 | best_loss=7.87807
Epoch 14/80: current_loss=8.20374 | best_loss=7.87807
Epoch 15/80: current_loss=8.05648 | best_loss=7.87807
Epoch 16/80: current_loss=7.96149 | best_loss=7.87807
Epoch 17/80: current_loss=8.13429 | best_loss=7.87807
Epoch 18/80: current_loss=7.91981 | best_loss=7.87807
Epoch 19/80: current_loss=8.29452 | best_loss=7.87807
Epoch 20/80: current_loss=8.09374 | best_loss=7.87807
Epoch 21/80: current_loss=7.95162 | best_loss=7.87807
Epoch 22/80: current_loss=8.21688 | best_loss=7.87807
Epoch 23/80: current_loss=7.99670 | best_loss=7.87807
Epoch 24/80: current_loss=8.04368 | best_loss=7.87807
Epoch 25/80: current_loss=8.05248 | best_loss=7.87807
Early Stopping at epoch 25
      explained_var=0.00031 | mse_loss=7.72984
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.54621 | best_loss=8.54621
Epoch 1/80: current_loss=8.34001 | best_loss=8.34001
Epoch 2/80: current_loss=8.32458 | best_loss=8.32458
Epoch 3/80: current_loss=8.35231 | best_loss=8.32458
Epoch 4/80: current_loss=8.35535 | best_loss=8.32458
Epoch 5/80: current_loss=8.33542 | best_loss=8.32458
Epoch 6/80: current_loss=8.33992 | best_loss=8.32458
Epoch 7/80: current_loss=8.35380 | best_loss=8.32458
Epoch 8/80: current_loss=8.33270 | best_loss=8.32458
Epoch 9/80: current_loss=8.43077 | best_loss=8.32458
Epoch 10/80: current_loss=8.52421 | best_loss=8.32458
Epoch 11/80: current_loss=8.42886 | best_loss=8.32458
Epoch 12/80: current_loss=8.33823 | best_loss=8.32458
Epoch 13/80: current_loss=8.34277 | best_loss=8.32458
Epoch 14/80: current_loss=8.34434 | best_loss=8.32458
Epoch 15/80: current_loss=8.34425 | best_loss=8.32458
Epoch 16/80: current_loss=8.38809 | best_loss=8.32458
Epoch 17/80: current_loss=8.32530 | best_loss=8.32458
Epoch 18/80: current_loss=9.17767 | best_loss=8.32458
Epoch 19/80: current_loss=8.35572 | best_loss=8.32458
Epoch 20/80: current_loss=8.45365 | best_loss=8.32458
Epoch 21/80: current_loss=8.36595 | best_loss=8.32458
Epoch 22/80: current_loss=8.32354 | best_loss=8.32354
Epoch 23/80: current_loss=8.47430 | best_loss=8.32354
Epoch 24/80: current_loss=8.33965 | best_loss=8.32354
Epoch 25/80: current_loss=8.35527 | best_loss=8.32354
Epoch 26/80: current_loss=8.39246 | best_loss=8.32354
Epoch 27/80: current_loss=8.36243 | best_loss=8.32354
Epoch 28/80: current_loss=8.36517 | best_loss=8.32354
Epoch 29/80: current_loss=8.46707 | best_loss=8.32354
Epoch 30/80: current_loss=8.36661 | best_loss=8.32354
Epoch 31/80: current_loss=8.38173 | best_loss=8.32354
Epoch 32/80: current_loss=8.37065 | best_loss=8.32354
Epoch 33/80: current_loss=8.36693 | best_loss=8.32354
Epoch 34/80: current_loss=8.34929 | best_loss=8.32354
Epoch 35/80: current_loss=8.33774 | best_loss=8.32354
Epoch 36/80: current_loss=8.36323 | best_loss=8.32354
Epoch 37/80: current_loss=8.39585 | best_loss=8.32354
Epoch 38/80: current_loss=8.32973 | best_loss=8.32354
Epoch 39/80: current_loss=8.32020 | best_loss=8.32020
Epoch 40/80: current_loss=8.37874 | best_loss=8.32020
Epoch 41/80: current_loss=8.34379 | best_loss=8.32020
Epoch 42/80: current_loss=8.33332 | best_loss=8.32020
Epoch 43/80: current_loss=8.73015 | best_loss=8.32020
Epoch 44/80: current_loss=8.32684 | best_loss=8.32020
Epoch 45/80: current_loss=8.34105 | best_loss=8.32020
Epoch 46/80: current_loss=8.38290 | best_loss=8.32020
Epoch 47/80: current_loss=8.35105 | best_loss=8.32020
Epoch 48/80: current_loss=8.35404 | best_loss=8.32020
Epoch 49/80: current_loss=8.47620 | best_loss=8.32020
Epoch 50/80: current_loss=8.35160 | best_loss=8.32020
Epoch 51/80: current_loss=8.43171 | best_loss=8.32020
Epoch 52/80: current_loss=8.37241 | best_loss=8.32020
Epoch 53/80: current_loss=8.42745 | best_loss=8.32020
Epoch 54/80: current_loss=8.32668 | best_loss=8.32020
Epoch 55/80: current_loss=8.31956 | best_loss=8.31956
Epoch 56/80: current_loss=8.43211 | best_loss=8.31956
Epoch 57/80: current_loss=8.38331 | best_loss=8.31956
Epoch 58/80: current_loss=8.41109 | best_loss=8.31956
Epoch 59/80: current_loss=8.35721 | best_loss=8.31956
Epoch 60/80: current_loss=8.35441 | best_loss=8.31956
Epoch 61/80: current_loss=8.35573 | best_loss=8.31956
Epoch 62/80: current_loss=8.39469 | best_loss=8.31956
Epoch 63/80: current_loss=8.35620 | best_loss=8.31956
Epoch 64/80: current_loss=8.33369 | best_loss=8.31956
Epoch 65/80: current_loss=8.43178 | best_loss=8.31956
Epoch 66/80: current_loss=8.32961 | best_loss=8.31956
Epoch 67/80: current_loss=8.42093 | best_loss=8.31956
Epoch 68/80: current_loss=8.31612 | best_loss=8.31612
Epoch 69/80: current_loss=8.44024 | best_loss=8.31612
Epoch 70/80: current_loss=8.32895 | best_loss=8.31612
Epoch 71/80: current_loss=8.39684 | best_loss=8.31612
Epoch 72/80: current_loss=8.32491 | best_loss=8.31612
Epoch 73/80: current_loss=8.46629 | best_loss=8.31612
Epoch 74/80: current_loss=8.32623 | best_loss=8.31612
Epoch 75/80: current_loss=8.54347 | best_loss=8.31612
Epoch 76/80: current_loss=8.35433 | best_loss=8.31612
Epoch 77/80: current_loss=8.47742 | best_loss=8.31612
Epoch 78/80: current_loss=8.41682 | best_loss=8.31612
Epoch 79/80: current_loss=8.52443 | best_loss=8.31612
      explained_var=-0.00063 | mse_loss=8.16889
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.90561 | best_loss=8.90561
Epoch 1/80: current_loss=9.03980 | best_loss=8.90561
Epoch 2/80: current_loss=8.86670 | best_loss=8.86670
Epoch 3/80: current_loss=9.28993 | best_loss=8.86670
Epoch 4/80: current_loss=8.86762 | best_loss=8.86670
Epoch 5/80: current_loss=9.18564 | best_loss=8.86670
Epoch 6/80: current_loss=8.79852 | best_loss=8.79852
Epoch 7/80: current_loss=8.82679 | best_loss=8.79852
Epoch 8/80: current_loss=8.98414 | best_loss=8.79852
Epoch 9/80: current_loss=9.22836 | best_loss=8.79852
Epoch 10/80: current_loss=9.06577 | best_loss=8.79852
Epoch 11/80: current_loss=9.02372 | best_loss=8.79852
Epoch 12/80: current_loss=8.80787 | best_loss=8.79852
Epoch 13/80: current_loss=9.27626 | best_loss=8.79852
Epoch 14/80: current_loss=8.88701 | best_loss=8.79852
Epoch 15/80: current_loss=8.91999 | best_loss=8.79852
Epoch 16/80: current_loss=9.22153 | best_loss=8.79852
Epoch 17/80: current_loss=8.97521 | best_loss=8.79852
Epoch 18/80: current_loss=8.83998 | best_loss=8.79852
Epoch 19/80: current_loss=9.02111 | best_loss=8.79852
Epoch 20/80: current_loss=9.10842 | best_loss=8.79852
Epoch 21/80: current_loss=9.04132 | best_loss=8.79852
Epoch 22/80: current_loss=8.92775 | best_loss=8.79852
Epoch 23/80: current_loss=8.86964 | best_loss=8.79852
Epoch 24/80: current_loss=9.39040 | best_loss=8.79852
Epoch 25/80: current_loss=9.07938 | best_loss=8.79852
Epoch 26/80: current_loss=8.96271 | best_loss=8.79852
Early Stopping at epoch 26
      explained_var=-0.00250 | mse_loss=8.57589
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.53731 | best_loss=8.53731
Epoch 1/80: current_loss=8.23285 | best_loss=8.23285
Epoch 2/80: current_loss=8.22333 | best_loss=8.22333
Epoch 3/80: current_loss=8.21956 | best_loss=8.21956
Epoch 4/80: current_loss=8.22193 | best_loss=8.21956
Epoch 5/80: current_loss=8.22110 | best_loss=8.21956
Epoch 6/80: current_loss=8.21720 | best_loss=8.21720
Epoch 7/80: current_loss=8.24882 | best_loss=8.21720
Epoch 8/80: current_loss=8.23427 | best_loss=8.21720
Epoch 9/80: current_loss=8.22017 | best_loss=8.21720
Epoch 10/80: current_loss=8.24808 | best_loss=8.21720
Epoch 11/80: current_loss=8.28869 | best_loss=8.21720
Epoch 12/80: current_loss=8.22506 | best_loss=8.21720
Epoch 13/80: current_loss=8.26414 | best_loss=8.21720
Epoch 14/80: current_loss=8.21986 | best_loss=8.21720
Epoch 15/80: current_loss=8.24951 | best_loss=8.21720
Epoch 16/80: current_loss=8.26712 | best_loss=8.21720
Epoch 17/80: current_loss=8.25484 | best_loss=8.21720
Epoch 18/80: current_loss=8.22449 | best_loss=8.21720
Epoch 19/80: current_loss=8.24276 | best_loss=8.21720
Epoch 20/80: current_loss=8.22166 | best_loss=8.21720
Epoch 21/80: current_loss=8.24965 | best_loss=8.21720
Epoch 22/80: current_loss=8.22944 | best_loss=8.21720
Epoch 23/80: current_loss=8.23457 | best_loss=8.21720
Epoch 24/80: current_loss=8.21846 | best_loss=8.21720
Epoch 25/80: current_loss=8.23847 | best_loss=8.21720
Epoch 26/80: current_loss=8.24945 | best_loss=8.21720
Early Stopping at epoch 26
      explained_var=0.00196 | mse_loss=8.31622
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.58934 | best_loss=8.58934
Epoch 1/80: current_loss=7.71822 | best_loss=7.71822
Epoch 2/80: current_loss=7.69192 | best_loss=7.69192
Epoch 3/80: current_loss=7.70275 | best_loss=7.69192
Epoch 4/80: current_loss=7.70961 | best_loss=7.69192
Epoch 5/80: current_loss=7.73399 | best_loss=7.69192
Epoch 6/80: current_loss=7.72441 | best_loss=7.69192
Epoch 7/80: current_loss=7.72377 | best_loss=7.69192
Epoch 8/80: current_loss=7.70258 | best_loss=7.69192
Epoch 9/80: current_loss=7.70925 | best_loss=7.69192
Epoch 10/80: current_loss=7.72623 | best_loss=7.69192
Epoch 11/80: current_loss=7.72430 | best_loss=7.69192
Epoch 12/80: current_loss=7.71054 | best_loss=7.69192
Epoch 13/80: current_loss=7.80086 | best_loss=7.69192
Epoch 14/80: current_loss=7.69593 | best_loss=7.69192
Epoch 15/80: current_loss=7.68868 | best_loss=7.68868
Epoch 16/80: current_loss=7.72223 | best_loss=7.68868
Epoch 17/80: current_loss=7.70632 | best_loss=7.68868
Epoch 18/80: current_loss=7.69927 | best_loss=7.68868
Epoch 19/80: current_loss=7.70953 | best_loss=7.68868
Epoch 20/80: current_loss=7.74573 | best_loss=7.68868
Epoch 21/80: current_loss=7.74549 | best_loss=7.68868
Epoch 22/80: current_loss=7.70521 | best_loss=7.68868
Epoch 23/80: current_loss=7.71265 | best_loss=7.68868
Epoch 24/80: current_loss=7.72200 | best_loss=7.68868
Epoch 25/80: current_loss=7.69680 | best_loss=7.68868
Epoch 26/80: current_loss=7.78869 | best_loss=7.68868
Epoch 27/80: current_loss=7.70830 | best_loss=7.68868
Epoch 28/80: current_loss=7.71018 | best_loss=7.68868
Epoch 29/80: current_loss=7.75602 | best_loss=7.68868
Epoch 30/80: current_loss=7.78136 | best_loss=7.68868
Epoch 31/80: current_loss=7.69729 | best_loss=7.68868
Epoch 32/80: current_loss=7.68955 | best_loss=7.68868
Epoch 33/80: current_loss=7.70481 | best_loss=7.68868
Epoch 34/80: current_loss=7.71828 | best_loss=7.68868
Epoch 35/80: current_loss=7.70615 | best_loss=7.68868
Early Stopping at epoch 35
      explained_var=-0.00115 | mse_loss=7.86455
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=-0.00040| avg_loss=8.13108
----------------------------------------------


----------------------------------------------
Params for Trial 13
{'learning_rate': 0.01, 'weight_decay': 0.008251881902137582, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=16.95302 | best_loss=16.95302
Epoch 1/80: current_loss=7.81495 | best_loss=7.81495
Epoch 2/80: current_loss=9.85009 | best_loss=7.81495
Epoch 3/80: current_loss=9.05275 | best_loss=7.81495
Epoch 4/80: current_loss=7.96894 | best_loss=7.81495
Epoch 5/80: current_loss=7.85590 | best_loss=7.81495
Epoch 6/80: current_loss=7.98539 | best_loss=7.81495
Epoch 7/80: current_loss=8.34885 | best_loss=7.81495
Epoch 8/80: current_loss=8.08243 | best_loss=7.81495
Epoch 9/80: current_loss=8.15703 | best_loss=7.81495
Epoch 10/80: current_loss=8.01864 | best_loss=7.81495
Epoch 11/80: current_loss=8.21630 | best_loss=7.81495
Epoch 12/80: current_loss=8.33834 | best_loss=7.81495
Epoch 13/80: current_loss=8.02335 | best_loss=7.81495
Epoch 14/80: current_loss=7.87268 | best_loss=7.81495
Epoch 15/80: current_loss=8.10175 | best_loss=7.81495
Epoch 16/80: current_loss=8.45503 | best_loss=7.81495
Epoch 17/80: current_loss=7.95048 | best_loss=7.81495
Epoch 18/80: current_loss=8.15019 | best_loss=7.81495
Epoch 19/80: current_loss=8.14820 | best_loss=7.81495
Epoch 20/80: current_loss=8.08606 | best_loss=7.81495
Epoch 21/80: current_loss=7.87594 | best_loss=7.81495
Early Stopping at epoch 21
      explained_var=0.00066 | mse_loss=7.65579
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.34825 | best_loss=8.34825
Epoch 1/80: current_loss=9.10976 | best_loss=8.34825
Epoch 2/80: current_loss=8.33271 | best_loss=8.33271
Epoch 3/80: current_loss=8.34373 | best_loss=8.33271
Epoch 4/80: current_loss=8.31897 | best_loss=8.31897
Epoch 5/80: current_loss=8.33804 | best_loss=8.31897
Epoch 6/80: current_loss=8.44143 | best_loss=8.31897
Epoch 7/80: current_loss=8.35973 | best_loss=8.31897
Epoch 8/80: current_loss=8.34678 | best_loss=8.31897
Epoch 9/80: current_loss=8.34767 | best_loss=8.31897
Epoch 10/80: current_loss=8.32877 | best_loss=8.31897
Epoch 11/80: current_loss=8.32971 | best_loss=8.31897
Epoch 12/80: current_loss=8.37149 | best_loss=8.31897
Epoch 13/80: current_loss=8.45373 | best_loss=8.31897
Epoch 14/80: current_loss=8.32337 | best_loss=8.31897
Epoch 15/80: current_loss=8.33148 | best_loss=8.31897
Epoch 16/80: current_loss=8.33069 | best_loss=8.31897
Epoch 17/80: current_loss=8.36477 | best_loss=8.31897
Epoch 18/80: current_loss=8.38303 | best_loss=8.31897
Epoch 19/80: current_loss=8.36358 | best_loss=8.31897
Epoch 20/80: current_loss=8.41582 | best_loss=8.31897
Epoch 21/80: current_loss=8.38130 | best_loss=8.31897
Epoch 22/80: current_loss=8.32061 | best_loss=8.31897
Epoch 23/80: current_loss=8.38007 | best_loss=8.31897
Epoch 24/80: current_loss=8.32409 | best_loss=8.31897
Early Stopping at epoch 24
      explained_var=-0.00080 | mse_loss=8.17209
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.10515 | best_loss=9.10515
Epoch 1/80: current_loss=8.97745 | best_loss=8.97745
Epoch 2/80: current_loss=8.98417 | best_loss=8.97745
Epoch 3/80: current_loss=8.90731 | best_loss=8.90731
Epoch 4/80: current_loss=8.92926 | best_loss=8.90731
Epoch 5/80: current_loss=9.06062 | best_loss=8.90731
Epoch 6/80: current_loss=8.90992 | best_loss=8.90731
Epoch 7/80: current_loss=9.20663 | best_loss=8.90731
Epoch 8/80: current_loss=8.84702 | best_loss=8.84702
Epoch 9/80: current_loss=9.10132 | best_loss=8.84702
Epoch 10/80: current_loss=8.88292 | best_loss=8.84702
Epoch 11/80: current_loss=8.88554 | best_loss=8.84702
Epoch 12/80: current_loss=9.04282 | best_loss=8.84702
Epoch 13/80: current_loss=8.90045 | best_loss=8.84702
Epoch 14/80: current_loss=9.15198 | best_loss=8.84702
Epoch 15/80: current_loss=8.96059 | best_loss=8.84702
Epoch 16/80: current_loss=9.00323 | best_loss=8.84702
Epoch 17/80: current_loss=8.93510 | best_loss=8.84702
Epoch 18/80: current_loss=9.11182 | best_loss=8.84702
Epoch 19/80: current_loss=8.84484 | best_loss=8.84484
Epoch 20/80: current_loss=9.05032 | best_loss=8.84484
Epoch 21/80: current_loss=8.88927 | best_loss=8.84484
Epoch 22/80: current_loss=8.93135 | best_loss=8.84484
Epoch 23/80: current_loss=9.22436 | best_loss=8.84484
Epoch 24/80: current_loss=8.84587 | best_loss=8.84484
Epoch 25/80: current_loss=8.80083 | best_loss=8.80083
Epoch 26/80: current_loss=9.15465 | best_loss=8.80083
Epoch 27/80: current_loss=8.90564 | best_loss=8.80083
Epoch 28/80: current_loss=8.97474 | best_loss=8.80083
Epoch 29/80: current_loss=9.10988 | best_loss=8.80083
Epoch 30/80: current_loss=8.97453 | best_loss=8.80083
Epoch 31/80: current_loss=8.96074 | best_loss=8.80083
Epoch 32/80: current_loss=9.02907 | best_loss=8.80083
Epoch 33/80: current_loss=9.00993 | best_loss=8.80083
Epoch 34/80: current_loss=9.00058 | best_loss=8.80083
Epoch 35/80: current_loss=9.00075 | best_loss=8.80083
Epoch 36/80: current_loss=8.86209 | best_loss=8.80083
Epoch 37/80: current_loss=8.98987 | best_loss=8.80083
Epoch 38/80: current_loss=8.98352 | best_loss=8.80083
Epoch 39/80: current_loss=8.96482 | best_loss=8.80083
Epoch 40/80: current_loss=8.88639 | best_loss=8.80083
Epoch 41/80: current_loss=9.27825 | best_loss=8.80083
Epoch 42/80: current_loss=8.90851 | best_loss=8.80083
Epoch 43/80: current_loss=9.26974 | best_loss=8.80083
Epoch 44/80: current_loss=8.76613 | best_loss=8.76613
Epoch 45/80: current_loss=9.09634 | best_loss=8.76613
Epoch 46/80: current_loss=9.13020 | best_loss=8.76613
Epoch 47/80: current_loss=9.61554 | best_loss=8.76613
Epoch 48/80: current_loss=9.22000 | best_loss=8.76613
Epoch 49/80: current_loss=8.87623 | best_loss=8.76613
Epoch 50/80: current_loss=9.17602 | best_loss=8.76613
Epoch 51/80: current_loss=8.82943 | best_loss=8.76613
Epoch 52/80: current_loss=9.24688 | best_loss=8.76613
Epoch 53/80: current_loss=8.75049 | best_loss=8.75049
Epoch 54/80: current_loss=9.46873 | best_loss=8.75049
Epoch 55/80: current_loss=8.82695 | best_loss=8.75049
Epoch 56/80: current_loss=8.95965 | best_loss=8.75049
Epoch 57/80: current_loss=8.86952 | best_loss=8.75049
Epoch 58/80: current_loss=9.23542 | best_loss=8.75049
Epoch 59/80: current_loss=9.08383 | best_loss=8.75049
Epoch 60/80: current_loss=8.81578 | best_loss=8.75049
Epoch 61/80: current_loss=9.58838 | best_loss=8.75049
Epoch 62/80: current_loss=8.86987 | best_loss=8.75049
Epoch 63/80: current_loss=9.02265 | best_loss=8.75049
Epoch 64/80: current_loss=8.92462 | best_loss=8.75049
Epoch 65/80: current_loss=9.14578 | best_loss=8.75049
Epoch 66/80: current_loss=9.02232 | best_loss=8.75049
Epoch 67/80: current_loss=9.03703 | best_loss=8.75049
Epoch 68/80: current_loss=9.02247 | best_loss=8.75049
Epoch 69/80: current_loss=8.97935 | best_loss=8.75049
Epoch 70/80: current_loss=9.04135 | best_loss=8.75049
Epoch 71/80: current_loss=8.89354 | best_loss=8.75049
Epoch 72/80: current_loss=9.04817 | best_loss=8.75049
Epoch 73/80: current_loss=8.96305 | best_loss=8.75049
Early Stopping at epoch 73
      explained_var=-0.00379 | mse_loss=8.52492
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.24887 | best_loss=8.24887
Epoch 1/80: current_loss=8.28785 | best_loss=8.24887
Epoch 2/80: current_loss=8.21902 | best_loss=8.21902
Epoch 3/80: current_loss=8.34637 | best_loss=8.21902
Epoch 4/80: current_loss=8.21425 | best_loss=8.21425
Epoch 5/80: current_loss=8.24379 | best_loss=8.21425
Epoch 6/80: current_loss=8.22929 | best_loss=8.21425
Epoch 7/80: current_loss=8.27452 | best_loss=8.21425
Epoch 8/80: current_loss=8.35793 | best_loss=8.21425
Epoch 9/80: current_loss=8.34247 | best_loss=8.21425
Epoch 10/80: current_loss=8.34638 | best_loss=8.21425
Epoch 11/80: current_loss=8.23067 | best_loss=8.21425
Epoch 12/80: current_loss=8.23205 | best_loss=8.21425
Epoch 13/80: current_loss=8.23353 | best_loss=8.21425
Epoch 14/80: current_loss=8.24814 | best_loss=8.21425
Epoch 15/80: current_loss=8.30402 | best_loss=8.21425
Epoch 16/80: current_loss=8.22097 | best_loss=8.21425
Epoch 17/80: current_loss=8.22546 | best_loss=8.21425
Epoch 18/80: current_loss=8.26781 | best_loss=8.21425
Epoch 19/80: current_loss=8.22024 | best_loss=8.21425
Epoch 20/80: current_loss=8.25466 | best_loss=8.21425
Epoch 21/80: current_loss=8.23528 | best_loss=8.21425
Epoch 22/80: current_loss=8.24660 | best_loss=8.21425
Epoch 23/80: current_loss=8.26146 | best_loss=8.21425
Epoch 24/80: current_loss=8.23012 | best_loss=8.21425
Early Stopping at epoch 24
      explained_var=0.00257 | mse_loss=8.31055
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.71407 | best_loss=7.71407
Epoch 1/80: current_loss=7.69521 | best_loss=7.69521
Epoch 2/80: current_loss=7.68944 | best_loss=7.68944
Epoch 3/80: current_loss=7.69212 | best_loss=7.68944
Epoch 4/80: current_loss=7.68770 | best_loss=7.68770
Epoch 5/80: current_loss=7.73047 | best_loss=7.68770
Epoch 6/80: current_loss=7.70861 | best_loss=7.68770
Epoch 7/80: current_loss=7.74620 | best_loss=7.68770
Epoch 8/80: current_loss=7.69189 | best_loss=7.68770
Epoch 9/80: current_loss=7.72993 | best_loss=7.68770
Epoch 10/80: current_loss=7.78506 | best_loss=7.68770
Epoch 11/80: current_loss=7.69406 | best_loss=7.68770
Epoch 12/80: current_loss=7.69914 | best_loss=7.68770
Epoch 13/80: current_loss=7.70626 | best_loss=7.68770
Epoch 14/80: current_loss=7.70302 | best_loss=7.68770
Epoch 15/80: current_loss=7.73314 | best_loss=7.68770
Epoch 16/80: current_loss=7.78220 | best_loss=7.68770
Epoch 17/80: current_loss=7.69522 | best_loss=7.68770
Epoch 18/80: current_loss=7.69408 | best_loss=7.68770
Epoch 19/80: current_loss=7.70727 | best_loss=7.68770
Epoch 20/80: current_loss=7.75842 | best_loss=7.68770
Epoch 21/80: current_loss=7.69583 | best_loss=7.68770
Epoch 22/80: current_loss=7.68440 | best_loss=7.68440
Epoch 23/80: current_loss=7.69461 | best_loss=7.68440
Epoch 24/80: current_loss=7.71448 | best_loss=7.68440
Epoch 25/80: current_loss=7.75140 | best_loss=7.68440
Epoch 26/80: current_loss=7.72546 | best_loss=7.68440
Epoch 27/80: current_loss=7.73085 | best_loss=7.68440
Epoch 28/80: current_loss=7.73496 | best_loss=7.68440
Epoch 29/80: current_loss=7.77324 | best_loss=7.68440
Epoch 30/80: current_loss=7.73306 | best_loss=7.68440
Epoch 31/80: current_loss=7.69656 | best_loss=7.68440
Epoch 32/80: current_loss=7.71589 | best_loss=7.68440
Epoch 33/80: current_loss=7.73135 | best_loss=7.68440
Epoch 34/80: current_loss=7.79176 | best_loss=7.68440
Epoch 35/80: current_loss=8.10804 | best_loss=7.68440
Epoch 36/80: current_loss=7.71286 | best_loss=7.68440
Epoch 37/80: current_loss=7.74989 | best_loss=7.68440
Epoch 38/80: current_loss=7.69311 | best_loss=7.68440
Epoch 39/80: current_loss=7.74209 | best_loss=7.68440
Epoch 40/80: current_loss=7.71493 | best_loss=7.68440
Epoch 41/80: current_loss=7.74663 | best_loss=7.68440
Epoch 42/80: current_loss=7.68950 | best_loss=7.68440
Early Stopping at epoch 42
      explained_var=-0.00054 | mse_loss=7.86382
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=-0.00038| avg_loss=8.10543
----------------------------------------------


----------------------------------------------
Params for Trial 14
{'learning_rate': 0.01, 'weight_decay': 0.007908358003129208, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.38059 | best_loss=11.38059
Epoch 1/80: current_loss=7.77887 | best_loss=7.77887
Epoch 2/80: current_loss=8.72034 | best_loss=7.77887
Epoch 3/80: current_loss=8.09266 | best_loss=7.77887
Epoch 4/80: current_loss=8.50380 | best_loss=7.77887
Epoch 5/80: current_loss=8.20746 | best_loss=7.77887
Epoch 6/80: current_loss=8.29964 | best_loss=7.77887
Epoch 7/80: current_loss=8.02826 | best_loss=7.77887
Epoch 8/80: current_loss=7.79560 | best_loss=7.77887
Epoch 9/80: current_loss=8.15903 | best_loss=7.77887
Epoch 10/80: current_loss=9.24351 | best_loss=7.77887
Epoch 11/80: current_loss=8.45215 | best_loss=7.77887
Epoch 12/80: current_loss=7.86577 | best_loss=7.77887
Epoch 13/80: current_loss=7.77251 | best_loss=7.77251
Epoch 14/80: current_loss=8.12706 | best_loss=7.77251
Epoch 15/80: current_loss=9.80232 | best_loss=7.77251
Epoch 16/80: current_loss=7.97848 | best_loss=7.77251
Epoch 17/80: current_loss=8.02458 | best_loss=7.77251
Epoch 18/80: current_loss=7.88397 | best_loss=7.77251
Epoch 19/80: current_loss=7.74829 | best_loss=7.74829
Epoch 20/80: current_loss=8.39246 | best_loss=7.74829
Epoch 21/80: current_loss=8.32359 | best_loss=7.74829
Epoch 22/80: current_loss=8.23118 | best_loss=7.74829
Epoch 23/80: current_loss=8.09848 | best_loss=7.74829
Epoch 24/80: current_loss=8.32809 | best_loss=7.74829
Epoch 25/80: current_loss=7.83064 | best_loss=7.74829
Epoch 26/80: current_loss=7.77195 | best_loss=7.74829
Epoch 27/80: current_loss=8.58966 | best_loss=7.74829
Epoch 28/80: current_loss=8.69327 | best_loss=7.74829
Epoch 29/80: current_loss=9.52567 | best_loss=7.74829
Epoch 30/80: current_loss=8.00323 | best_loss=7.74829
Epoch 31/80: current_loss=7.86222 | best_loss=7.74829
Epoch 32/80: current_loss=8.74568 | best_loss=7.74829
Epoch 33/80: current_loss=8.53105 | best_loss=7.74829
Epoch 34/80: current_loss=8.66889 | best_loss=7.74829
Epoch 35/80: current_loss=7.77674 | best_loss=7.74829
Epoch 36/80: current_loss=7.90030 | best_loss=7.74829
Epoch 37/80: current_loss=8.02829 | best_loss=7.74829
Epoch 38/80: current_loss=9.87204 | best_loss=7.74829
Epoch 39/80: current_loss=8.77695 | best_loss=7.74829
Early Stopping at epoch 39
      explained_var=0.00346 | mse_loss=7.57526
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.44095 | best_loss=9.44095
Epoch 1/80: current_loss=8.44165 | best_loss=8.44165
Epoch 2/80: current_loss=8.32664 | best_loss=8.32664
Epoch 3/80: current_loss=8.35875 | best_loss=8.32664
Epoch 4/80: current_loss=8.36589 | best_loss=8.32664
Epoch 5/80: current_loss=8.45487 | best_loss=8.32664
Epoch 6/80: current_loss=8.33231 | best_loss=8.32664
Epoch 7/80: current_loss=8.75671 | best_loss=8.32664
Epoch 8/80: current_loss=8.34769 | best_loss=8.32664
Epoch 9/80: current_loss=9.29780 | best_loss=8.32664
Epoch 10/80: current_loss=8.44700 | best_loss=8.32664
Epoch 11/80: current_loss=8.33504 | best_loss=8.32664
Epoch 12/80: current_loss=8.35722 | best_loss=8.32664
Epoch 13/80: current_loss=8.37843 | best_loss=8.32664
Epoch 14/80: current_loss=8.46732 | best_loss=8.32664
Epoch 15/80: current_loss=8.50674 | best_loss=8.32664
Epoch 16/80: current_loss=8.63753 | best_loss=8.32664
Epoch 17/80: current_loss=8.56573 | best_loss=8.32664
Epoch 18/80: current_loss=8.74454 | best_loss=8.32664
Epoch 19/80: current_loss=8.86151 | best_loss=8.32664
Epoch 20/80: current_loss=8.31336 | best_loss=8.31336
Epoch 21/80: current_loss=8.50222 | best_loss=8.31336
Epoch 22/80: current_loss=8.48327 | best_loss=8.31336
Epoch 23/80: current_loss=8.32883 | best_loss=8.31336
Epoch 24/80: current_loss=8.32160 | best_loss=8.31336
Epoch 25/80: current_loss=9.10756 | best_loss=8.31336
Epoch 26/80: current_loss=8.41157 | best_loss=8.31336
Epoch 27/80: current_loss=8.97938 | best_loss=8.31336
Epoch 28/80: current_loss=9.78353 | best_loss=8.31336
Epoch 29/80: current_loss=8.63713 | best_loss=8.31336
Epoch 30/80: current_loss=8.58765 | best_loss=8.31336
Epoch 31/80: current_loss=8.70054 | best_loss=8.31336
Epoch 32/80: current_loss=8.59001 | best_loss=8.31336
Epoch 33/80: current_loss=8.35891 | best_loss=8.31336
Epoch 34/80: current_loss=8.32922 | best_loss=8.31336
Epoch 35/80: current_loss=9.89990 | best_loss=8.31336
Epoch 36/80: current_loss=9.53454 | best_loss=8.31336
Epoch 37/80: current_loss=8.51526 | best_loss=8.31336
Epoch 38/80: current_loss=8.64224 | best_loss=8.31336
Epoch 39/80: current_loss=8.38882 | best_loss=8.31336
Epoch 40/80: current_loss=8.37231 | best_loss=8.31336
Early Stopping at epoch 40
      explained_var=0.00012 | mse_loss=8.16483
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.16749 | best_loss=9.16749
Epoch 1/80: current_loss=8.74157 | best_loss=8.74157
Epoch 2/80: current_loss=9.25567 | best_loss=8.74157
Epoch 3/80: current_loss=9.17054 | best_loss=8.74157
Epoch 4/80: current_loss=9.94821 | best_loss=8.74157
Epoch 5/80: current_loss=8.73668 | best_loss=8.73668
Epoch 6/80: current_loss=8.89070 | best_loss=8.73668
Epoch 7/80: current_loss=8.82088 | best_loss=8.73668
Epoch 8/80: current_loss=8.92355 | best_loss=8.73668
Epoch 9/80: current_loss=8.73527 | best_loss=8.73527
Epoch 10/80: current_loss=9.43820 | best_loss=8.73527
Epoch 11/80: current_loss=9.01635 | best_loss=8.73527
Epoch 12/80: current_loss=8.97774 | best_loss=8.73527
Epoch 13/80: current_loss=9.64104 | best_loss=8.73527
Epoch 14/80: current_loss=8.75079 | best_loss=8.73527
Epoch 15/80: current_loss=8.75282 | best_loss=8.73527
Epoch 16/80: current_loss=9.12969 | best_loss=8.73527
Epoch 17/80: current_loss=9.26836 | best_loss=8.73527
Epoch 18/80: current_loss=8.73110 | best_loss=8.73110
Epoch 19/80: current_loss=9.52768 | best_loss=8.73110
Epoch 20/80: current_loss=8.88146 | best_loss=8.73110
Epoch 21/80: current_loss=9.40060 | best_loss=8.73110
Epoch 22/80: current_loss=10.28075 | best_loss=8.73110
Epoch 23/80: current_loss=8.93558 | best_loss=8.73110
Epoch 24/80: current_loss=8.77436 | best_loss=8.73110
Epoch 25/80: current_loss=9.78420 | best_loss=8.73110
Epoch 26/80: current_loss=10.01727 | best_loss=8.73110
Epoch 27/80: current_loss=11.51460 | best_loss=8.73110
Epoch 28/80: current_loss=8.75172 | best_loss=8.73110
Epoch 29/80: current_loss=8.73676 | best_loss=8.73110
Epoch 30/80: current_loss=8.83003 | best_loss=8.73110
Epoch 31/80: current_loss=8.84937 | best_loss=8.73110
Epoch 32/80: current_loss=9.66352 | best_loss=8.73110
Epoch 33/80: current_loss=9.12527 | best_loss=8.73110
Epoch 34/80: current_loss=9.15338 | best_loss=8.73110
Epoch 35/80: current_loss=8.78930 | best_loss=8.73110
Epoch 36/80: current_loss=8.73375 | best_loss=8.73110
Epoch 37/80: current_loss=8.73961 | best_loss=8.73110
Epoch 38/80: current_loss=8.73649 | best_loss=8.73110
Early Stopping at epoch 38
      explained_var=-0.00221 | mse_loss=8.50151
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.40732 | best_loss=8.40732
Epoch 1/80: current_loss=8.23845 | best_loss=8.23845
Epoch 2/80: current_loss=8.34138 | best_loss=8.23845
Epoch 3/80: current_loss=8.50600 | best_loss=8.23845
Epoch 4/80: current_loss=8.28459 | best_loss=8.23845
Epoch 5/80: current_loss=8.30418 | best_loss=8.23845
Epoch 6/80: current_loss=8.21927 | best_loss=8.21927
Epoch 7/80: current_loss=8.42491 | best_loss=8.21927
Epoch 8/80: current_loss=8.22960 | best_loss=8.21927
Epoch 9/80: current_loss=8.29680 | best_loss=8.21927
Epoch 10/80: current_loss=8.79894 | best_loss=8.21927
Epoch 11/80: current_loss=8.23605 | best_loss=8.21927
Epoch 12/80: current_loss=8.91222 | best_loss=8.21927
Epoch 13/80: current_loss=8.29558 | best_loss=8.21927
Epoch 14/80: current_loss=8.22890 | best_loss=8.21927
Epoch 15/80: current_loss=8.23068 | best_loss=8.21927
Epoch 16/80: current_loss=8.35940 | best_loss=8.21927
Epoch 17/80: current_loss=8.24524 | best_loss=8.21927
Epoch 18/80: current_loss=8.24907 | best_loss=8.21927
Epoch 19/80: current_loss=8.25531 | best_loss=8.21927
Epoch 20/80: current_loss=8.23949 | best_loss=8.21927
Epoch 21/80: current_loss=8.35977 | best_loss=8.21927
Epoch 22/80: current_loss=8.81830 | best_loss=8.21927
Epoch 23/80: current_loss=8.24711 | best_loss=8.21927
Epoch 24/80: current_loss=8.53210 | best_loss=8.21927
Epoch 25/80: current_loss=8.29948 | best_loss=8.21927
Epoch 26/80: current_loss=8.26359 | best_loss=8.21927
Early Stopping at epoch 26
      explained_var=0.00247 | mse_loss=8.31465
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.78180 | best_loss=7.78180
Epoch 1/80: current_loss=8.60334 | best_loss=7.78180
Epoch 2/80: current_loss=7.67207 | best_loss=7.67207
Epoch 3/80: current_loss=9.05959 | best_loss=7.67207
Epoch 4/80: current_loss=7.74280 | best_loss=7.67207
Epoch 5/80: current_loss=7.68824 | best_loss=7.67207
Epoch 6/80: current_loss=8.19082 | best_loss=7.67207
Epoch 7/80: current_loss=7.84904 | best_loss=7.67207
Epoch 8/80: current_loss=8.05455 | best_loss=7.67207
Epoch 9/80: current_loss=7.82421 | best_loss=7.67207
Epoch 10/80: current_loss=7.72175 | best_loss=7.67207
Epoch 11/80: current_loss=7.69988 | best_loss=7.67207
Epoch 12/80: current_loss=8.22101 | best_loss=7.67207
Epoch 13/80: current_loss=7.73805 | best_loss=7.67207
Epoch 14/80: current_loss=7.73115 | best_loss=7.67207
Epoch 15/80: current_loss=7.91920 | best_loss=7.67207
Epoch 16/80: current_loss=7.82662 | best_loss=7.67207
Epoch 17/80: current_loss=7.68820 | best_loss=7.67207
Epoch 18/80: current_loss=7.73566 | best_loss=7.67207
Epoch 19/80: current_loss=7.79681 | best_loss=7.67207
Epoch 20/80: current_loss=17.66237 | best_loss=7.67207
Epoch 21/80: current_loss=9.20040 | best_loss=7.67207
Epoch 22/80: current_loss=7.96461 | best_loss=7.67207
Early Stopping at epoch 22
      explained_var=0.00117 | mse_loss=7.84837
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.00100| avg_loss=8.08092
----------------------------------------------


----------------------------------------------
Params for Trial 15
{'learning_rate': 0.01, 'weight_decay': 0.007041146666322683, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.50994 | best_loss=10.50994
Epoch 1/80: current_loss=7.92556 | best_loss=7.92556
Epoch 2/80: current_loss=7.77132 | best_loss=7.77132
Epoch 3/80: current_loss=9.95465 | best_loss=7.77132
Epoch 4/80: current_loss=7.82939 | best_loss=7.77132
Epoch 5/80: current_loss=9.64126 | best_loss=7.77132
Epoch 6/80: current_loss=7.87711 | best_loss=7.77132
Epoch 7/80: current_loss=8.28550 | best_loss=7.77132
Epoch 8/80: current_loss=8.29849 | best_loss=7.77132
Epoch 9/80: current_loss=9.41453 | best_loss=7.77132
Epoch 10/80: current_loss=7.86326 | best_loss=7.77132
Epoch 11/80: current_loss=7.77806 | best_loss=7.77132
Epoch 12/80: current_loss=9.86235 | best_loss=7.77132
Epoch 13/80: current_loss=7.90426 | best_loss=7.77132
Epoch 14/80: current_loss=9.14324 | best_loss=7.77132
Epoch 15/80: current_loss=8.12490 | best_loss=7.77132
Epoch 16/80: current_loss=7.83886 | best_loss=7.77132
Epoch 17/80: current_loss=7.80849 | best_loss=7.77132
Epoch 18/80: current_loss=8.19550 | best_loss=7.77132
Epoch 19/80: current_loss=9.74602 | best_loss=7.77132
Epoch 20/80: current_loss=7.94319 | best_loss=7.77132
Epoch 21/80: current_loss=7.87668 | best_loss=7.77132
Epoch 22/80: current_loss=9.37076 | best_loss=7.77132
Early Stopping at epoch 22
      explained_var=-0.00002 | mse_loss=7.58708
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.30394 | best_loss=8.30394
Epoch 1/80: current_loss=8.31203 | best_loss=8.30394
Epoch 2/80: current_loss=9.59672 | best_loss=8.30394
Epoch 3/80: current_loss=8.39719 | best_loss=8.30394
Epoch 4/80: current_loss=8.52381 | best_loss=8.30394
Epoch 5/80: current_loss=8.30917 | best_loss=8.30394
Epoch 6/80: current_loss=8.29829 | best_loss=8.29829
Epoch 7/80: current_loss=8.41124 | best_loss=8.29829
Epoch 8/80: current_loss=8.31446 | best_loss=8.29829
Epoch 9/80: current_loss=8.60008 | best_loss=8.29829
Epoch 10/80: current_loss=8.44851 | best_loss=8.29829
Epoch 11/80: current_loss=8.23821 | best_loss=8.23821
Epoch 12/80: current_loss=8.63754 | best_loss=8.23821
Epoch 13/80: current_loss=8.75921 | best_loss=8.23821
Epoch 14/80: current_loss=8.37822 | best_loss=8.23821
Epoch 15/80: current_loss=8.51850 | best_loss=8.23821
Epoch 16/80: current_loss=8.70692 | best_loss=8.23821
Epoch 17/80: current_loss=8.63852 | best_loss=8.23821
Epoch 18/80: current_loss=9.00331 | best_loss=8.23821
Epoch 19/80: current_loss=8.29993 | best_loss=8.23821
Epoch 20/80: current_loss=9.64271 | best_loss=8.23821
Epoch 21/80: current_loss=8.48837 | best_loss=8.23821
Epoch 22/80: current_loss=8.46039 | best_loss=8.23821
Epoch 23/80: current_loss=8.53548 | best_loss=8.23821
Epoch 24/80: current_loss=10.98052 | best_loss=8.23821
Epoch 25/80: current_loss=8.56219 | best_loss=8.23821
Epoch 26/80: current_loss=9.40623 | best_loss=8.23821
Epoch 27/80: current_loss=8.35452 | best_loss=8.23821
Epoch 28/80: current_loss=8.46206 | best_loss=8.23821
Epoch 29/80: current_loss=8.47228 | best_loss=8.23821
Epoch 30/80: current_loss=8.55706 | best_loss=8.23821
Epoch 31/80: current_loss=8.37454 | best_loss=8.23821
Early Stopping at epoch 31
      explained_var=0.01771 | mse_loss=8.06364
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.89901 | best_loss=8.89901
Epoch 1/80: current_loss=8.73186 | best_loss=8.73186
Epoch 2/80: current_loss=8.92428 | best_loss=8.73186
Epoch 3/80: current_loss=9.83337 | best_loss=8.73186
Epoch 4/80: current_loss=10.66101 | best_loss=8.73186
Epoch 5/80: current_loss=9.06595 | best_loss=8.73186
Epoch 6/80: current_loss=8.73839 | best_loss=8.73186
Epoch 7/80: current_loss=8.78523 | best_loss=8.73186
Epoch 8/80: current_loss=9.06106 | best_loss=8.73186
Epoch 9/80: current_loss=9.61517 | best_loss=8.73186
Epoch 10/80: current_loss=8.75188 | best_loss=8.73186
Epoch 11/80: current_loss=8.77496 | best_loss=8.73186
Epoch 12/80: current_loss=8.88109 | best_loss=8.73186
Epoch 13/80: current_loss=9.06968 | best_loss=8.73186
Epoch 14/80: current_loss=8.74221 | best_loss=8.73186
Epoch 15/80: current_loss=9.24099 | best_loss=8.73186
Epoch 16/80: current_loss=8.94630 | best_loss=8.73186
Epoch 17/80: current_loss=8.71650 | best_loss=8.71650
Epoch 18/80: current_loss=9.44957 | best_loss=8.71650
Epoch 19/80: current_loss=10.62362 | best_loss=8.71650
Epoch 20/80: current_loss=9.20548 | best_loss=8.71650
Epoch 21/80: current_loss=8.81719 | best_loss=8.71650
Epoch 22/80: current_loss=9.31977 | best_loss=8.71650
Epoch 23/80: current_loss=8.79779 | best_loss=8.71650
Epoch 24/80: current_loss=9.17351 | best_loss=8.71650
Epoch 25/80: current_loss=9.50015 | best_loss=8.71650
Epoch 26/80: current_loss=10.23845 | best_loss=8.71650
Epoch 27/80: current_loss=9.42888 | best_loss=8.71650
Epoch 28/80: current_loss=8.69973 | best_loss=8.69973
Epoch 29/80: current_loss=8.79116 | best_loss=8.69973
Epoch 30/80: current_loss=9.08178 | best_loss=8.69973
Epoch 31/80: current_loss=8.99402 | best_loss=8.69973
Epoch 32/80: current_loss=8.83286 | best_loss=8.69973
Epoch 33/80: current_loss=8.86113 | best_loss=8.69973
Epoch 34/80: current_loss=9.50124 | best_loss=8.69973
Epoch 35/80: current_loss=10.03038 | best_loss=8.69973
Epoch 36/80: current_loss=9.71278 | best_loss=8.69973
Epoch 37/80: current_loss=9.52031 | best_loss=8.69973
Epoch 38/80: current_loss=9.82314 | best_loss=8.69973
Epoch 39/80: current_loss=8.87674 | best_loss=8.69973
Epoch 40/80: current_loss=9.02299 | best_loss=8.69973
Epoch 41/80: current_loss=10.72726 | best_loss=8.69973
Epoch 42/80: current_loss=8.88813 | best_loss=8.69973
Epoch 43/80: current_loss=8.95791 | best_loss=8.69973
Epoch 44/80: current_loss=8.74847 | best_loss=8.69973
Epoch 45/80: current_loss=8.74575 | best_loss=8.69973
Epoch 46/80: current_loss=8.88060 | best_loss=8.69973
Epoch 47/80: current_loss=9.24554 | best_loss=8.69973
Epoch 48/80: current_loss=9.75410 | best_loss=8.69973
Early Stopping at epoch 48
      explained_var=0.00264 | mse_loss=8.46250
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.35466 | best_loss=8.35466
Epoch 1/80: current_loss=8.48833 | best_loss=8.35466
Epoch 2/80: current_loss=8.32499 | best_loss=8.32499
Epoch 3/80: current_loss=8.34853 | best_loss=8.32499
Epoch 4/80: current_loss=8.36254 | best_loss=8.32499
Epoch 5/80: current_loss=8.46665 | best_loss=8.32499
Epoch 6/80: current_loss=8.22215 | best_loss=8.22215
Epoch 7/80: current_loss=8.42510 | best_loss=8.22215
Epoch 8/80: current_loss=8.23903 | best_loss=8.22215
Epoch 9/80: current_loss=8.22534 | best_loss=8.22215
Epoch 10/80: current_loss=8.52363 | best_loss=8.22215
Epoch 11/80: current_loss=8.21757 | best_loss=8.21757
Epoch 12/80: current_loss=8.26838 | best_loss=8.21757
Epoch 13/80: current_loss=8.65079 | best_loss=8.21757
Epoch 14/80: current_loss=8.55499 | best_loss=8.21757
Epoch 15/80: current_loss=8.24173 | best_loss=8.21757
Epoch 16/80: current_loss=8.70495 | best_loss=8.21757
Epoch 17/80: current_loss=8.37082 | best_loss=8.21757
Epoch 18/80: current_loss=9.06307 | best_loss=8.21757
Epoch 19/80: current_loss=8.86457 | best_loss=8.21757
Epoch 20/80: current_loss=8.22427 | best_loss=8.21757
Epoch 21/80: current_loss=8.23654 | best_loss=8.21757
Epoch 22/80: current_loss=8.69497 | best_loss=8.21757
Epoch 23/80: current_loss=8.56487 | best_loss=8.21757
Epoch 24/80: current_loss=8.28753 | best_loss=8.21757
Epoch 25/80: current_loss=9.04933 | best_loss=8.21757
Epoch 26/80: current_loss=8.49647 | best_loss=8.21757
Epoch 27/80: current_loss=8.22732 | best_loss=8.21757
Epoch 28/80: current_loss=8.98954 | best_loss=8.21757
Epoch 29/80: current_loss=8.85576 | best_loss=8.21757
Epoch 30/80: current_loss=8.25987 | best_loss=8.21757
Epoch 31/80: current_loss=8.28851 | best_loss=8.21757
Early Stopping at epoch 31
      explained_var=0.00153 | mse_loss=8.31945
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.33193 | best_loss=8.33193
Epoch 1/80: current_loss=7.76243 | best_loss=7.76243
Epoch 2/80: current_loss=7.90831 | best_loss=7.76243
Epoch 3/80: current_loss=7.79888 | best_loss=7.76243
Epoch 4/80: current_loss=7.71372 | best_loss=7.71372
Epoch 5/80: current_loss=7.78910 | best_loss=7.71372
Epoch 6/80: current_loss=7.81549 | best_loss=7.71372
Epoch 7/80: current_loss=9.02251 | best_loss=7.71372
Epoch 8/80: current_loss=8.73858 | best_loss=7.71372
Epoch 9/80: current_loss=7.85162 | best_loss=7.71372
Epoch 10/80: current_loss=7.77023 | best_loss=7.71372
Epoch 11/80: current_loss=7.68844 | best_loss=7.68844
Epoch 12/80: current_loss=7.79900 | best_loss=7.68844
Epoch 13/80: current_loss=7.97672 | best_loss=7.68844
Epoch 14/80: current_loss=7.79435 | best_loss=7.68844
Epoch 15/80: current_loss=8.00605 | best_loss=7.68844
Epoch 16/80: current_loss=7.69152 | best_loss=7.68844
Epoch 17/80: current_loss=7.97569 | best_loss=7.68844
Epoch 18/80: current_loss=7.67201 | best_loss=7.67201
Epoch 19/80: current_loss=7.72510 | best_loss=7.67201
Epoch 20/80: current_loss=7.72403 | best_loss=7.67201
Epoch 21/80: current_loss=7.79348 | best_loss=7.67201
Epoch 22/80: current_loss=7.92338 | best_loss=7.67201
Epoch 23/80: current_loss=9.36859 | best_loss=7.67201
Epoch 24/80: current_loss=8.44175 | best_loss=7.67201
Epoch 25/80: current_loss=7.99884 | best_loss=7.67201
Epoch 26/80: current_loss=7.73479 | best_loss=7.67201
Epoch 27/80: current_loss=7.67649 | best_loss=7.67201
Epoch 28/80: current_loss=7.74452 | best_loss=7.67201
Epoch 29/80: current_loss=7.91252 | best_loss=7.67201
Epoch 30/80: current_loss=7.76302 | best_loss=7.67201
Epoch 31/80: current_loss=8.03319 | best_loss=7.67201
Epoch 32/80: current_loss=7.83896 | best_loss=7.67201
Epoch 33/80: current_loss=8.01629 | best_loss=7.67201
Epoch 34/80: current_loss=7.97790 | best_loss=7.67201
Epoch 35/80: current_loss=7.68716 | best_loss=7.67201
Epoch 36/80: current_loss=7.69966 | best_loss=7.67201
Epoch 37/80: current_loss=8.31824 | best_loss=7.67201
Epoch 38/80: current_loss=7.69125 | best_loss=7.67201
Early Stopping at epoch 38
      explained_var=0.00088 | mse_loss=7.84943
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.00455| avg_loss=8.05642
----------------------------------------------


----------------------------------------------
Params for Trial 16
{'learning_rate': 0.01, 'weight_decay': 0.006893789237175351, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.02568 | best_loss=13.02568
Epoch 1/80: current_loss=7.77632 | best_loss=7.77632
Epoch 2/80: current_loss=8.02134 | best_loss=7.77632
Epoch 3/80: current_loss=8.57831 | best_loss=7.77632
Epoch 4/80: current_loss=7.81046 | best_loss=7.77632
Epoch 5/80: current_loss=8.34056 | best_loss=7.77632
Epoch 6/80: current_loss=7.92776 | best_loss=7.77632
Epoch 7/80: current_loss=8.12736 | best_loss=7.77632
Epoch 8/80: current_loss=7.96067 | best_loss=7.77632
Epoch 9/80: current_loss=8.68452 | best_loss=7.77632
Epoch 10/80: current_loss=8.13951 | best_loss=7.77632
Epoch 11/80: current_loss=8.09472 | best_loss=7.77632
Epoch 12/80: current_loss=7.79230 | best_loss=7.77632
Epoch 13/80: current_loss=9.02220 | best_loss=7.77632
Epoch 14/80: current_loss=8.11665 | best_loss=7.77632
Epoch 15/80: current_loss=7.74345 | best_loss=7.74345
Epoch 16/80: current_loss=7.75302 | best_loss=7.74345
Epoch 17/80: current_loss=8.55289 | best_loss=7.74345
Epoch 18/80: current_loss=7.76449 | best_loss=7.74345
Epoch 19/80: current_loss=8.39306 | best_loss=7.74345
Epoch 20/80: current_loss=9.10261 | best_loss=7.74345
Epoch 21/80: current_loss=8.67739 | best_loss=7.74345
Epoch 22/80: current_loss=7.82851 | best_loss=7.74345
Epoch 23/80: current_loss=7.91191 | best_loss=7.74345
Epoch 24/80: current_loss=8.60853 | best_loss=7.74345
Epoch 25/80: current_loss=7.87918 | best_loss=7.74345
Epoch 26/80: current_loss=9.23299 | best_loss=7.74345
Epoch 27/80: current_loss=10.01138 | best_loss=7.74345
Epoch 28/80: current_loss=8.15706 | best_loss=7.74345
Epoch 29/80: current_loss=7.75741 | best_loss=7.74345
Epoch 30/80: current_loss=8.05859 | best_loss=7.74345
Epoch 31/80: current_loss=9.17035 | best_loss=7.74345
Epoch 32/80: current_loss=8.52829 | best_loss=7.74345
Epoch 33/80: current_loss=7.94386 | best_loss=7.74345
Epoch 34/80: current_loss=7.76346 | best_loss=7.74345
Epoch 35/80: current_loss=7.80502 | best_loss=7.74345
Early Stopping at epoch 35
      explained_var=0.00343 | mse_loss=7.56526
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.75239 | best_loss=8.75239
Epoch 1/80: current_loss=8.57687 | best_loss=8.57687
Epoch 2/80: current_loss=8.48668 | best_loss=8.48668
Epoch 3/80: current_loss=8.32265 | best_loss=8.32265
Epoch 4/80: current_loss=8.93460 | best_loss=8.32265
Epoch 5/80: current_loss=8.73198 | best_loss=8.32265
Epoch 6/80: current_loss=8.30510 | best_loss=8.30510
Epoch 7/80: current_loss=8.49925 | best_loss=8.30510
Epoch 8/80: current_loss=8.31797 | best_loss=8.30510
Epoch 9/80: current_loss=8.41733 | best_loss=8.30510
Epoch 10/80: current_loss=8.49835 | best_loss=8.30510
Epoch 11/80: current_loss=8.50442 | best_loss=8.30510
Epoch 12/80: current_loss=9.54715 | best_loss=8.30510
Epoch 13/80: current_loss=8.30991 | best_loss=8.30510
Epoch 14/80: current_loss=8.31541 | best_loss=8.30510
Epoch 15/80: current_loss=8.31652 | best_loss=8.30510
Epoch 16/80: current_loss=8.33472 | best_loss=8.30510
Epoch 17/80: current_loss=8.43424 | best_loss=8.30510
Epoch 18/80: current_loss=8.74276 | best_loss=8.30510
Epoch 19/80: current_loss=8.40861 | best_loss=8.30510
Epoch 20/80: current_loss=8.37769 | best_loss=8.30510
Epoch 21/80: current_loss=8.56558 | best_loss=8.30510
Epoch 22/80: current_loss=8.59583 | best_loss=8.30510
Epoch 23/80: current_loss=8.61794 | best_loss=8.30510
Epoch 24/80: current_loss=8.40814 | best_loss=8.30510
Epoch 25/80: current_loss=8.47157 | best_loss=8.30510
Epoch 26/80: current_loss=9.39470 | best_loss=8.30510
Early Stopping at epoch 26
      explained_var=0.00315 | mse_loss=8.15021
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.84211 | best_loss=9.84211
Epoch 1/80: current_loss=8.80791 | best_loss=8.80791
Epoch 2/80: current_loss=8.98737 | best_loss=8.80791
Epoch 3/80: current_loss=9.35595 | best_loss=8.80791
Epoch 4/80: current_loss=8.79490 | best_loss=8.79490
Epoch 5/80: current_loss=9.22330 | best_loss=8.79490
Epoch 6/80: current_loss=8.79733 | best_loss=8.79490
Epoch 7/80: current_loss=10.46954 | best_loss=8.79490
Epoch 8/80: current_loss=8.78690 | best_loss=8.78690
Epoch 9/80: current_loss=8.72882 | best_loss=8.72882
Epoch 10/80: current_loss=9.58648 | best_loss=8.72882
Epoch 11/80: current_loss=8.98515 | best_loss=8.72882
Epoch 12/80: current_loss=9.10778 | best_loss=8.72882
Epoch 13/80: current_loss=8.84628 | best_loss=8.72882
Epoch 14/80: current_loss=9.51960 | best_loss=8.72882
Epoch 15/80: current_loss=10.13223 | best_loss=8.72882
Epoch 16/80: current_loss=8.98878 | best_loss=8.72882
Epoch 17/80: current_loss=8.92550 | best_loss=8.72882
Epoch 18/80: current_loss=9.11751 | best_loss=8.72882
Epoch 19/80: current_loss=9.07564 | best_loss=8.72882
Epoch 20/80: current_loss=9.19600 | best_loss=8.72882
Epoch 21/80: current_loss=8.74188 | best_loss=8.72882
Epoch 22/80: current_loss=9.30110 | best_loss=8.72882
Epoch 23/80: current_loss=9.75694 | best_loss=8.72882
Epoch 24/80: current_loss=8.75104 | best_loss=8.72882
Epoch 25/80: current_loss=9.28763 | best_loss=8.72882
Epoch 26/80: current_loss=8.86434 | best_loss=8.72882
Epoch 27/80: current_loss=8.98665 | best_loss=8.72882
Epoch 28/80: current_loss=9.33847 | best_loss=8.72882
Epoch 29/80: current_loss=8.87522 | best_loss=8.72882
Early Stopping at epoch 29
      explained_var=-0.00195 | mse_loss=8.49977
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.24529 | best_loss=8.24529
Epoch 1/80: current_loss=8.22620 | best_loss=8.22620
Epoch 2/80: current_loss=8.22418 | best_loss=8.22418
Epoch 3/80: current_loss=8.24395 | best_loss=8.22418
Epoch 4/80: current_loss=8.45520 | best_loss=8.22418
Epoch 5/80: current_loss=8.41809 | best_loss=8.22418
Epoch 6/80: current_loss=8.57226 | best_loss=8.22418
Epoch 7/80: current_loss=8.41790 | best_loss=8.22418
Epoch 8/80: current_loss=8.23027 | best_loss=8.22418
Epoch 9/80: current_loss=8.49895 | best_loss=8.22418
Epoch 10/80: current_loss=8.29983 | best_loss=8.22418
Epoch 11/80: current_loss=8.58060 | best_loss=8.22418
Epoch 12/80: current_loss=8.49992 | best_loss=8.22418
Epoch 13/80: current_loss=8.58346 | best_loss=8.22418
Epoch 14/80: current_loss=8.84690 | best_loss=8.22418
Epoch 15/80: current_loss=8.80484 | best_loss=8.22418
Epoch 16/80: current_loss=8.25103 | best_loss=8.22418
Epoch 17/80: current_loss=8.60872 | best_loss=8.22418
Epoch 18/80: current_loss=8.22340 | best_loss=8.22340
Epoch 19/80: current_loss=8.21668 | best_loss=8.21668
Epoch 20/80: current_loss=8.63829 | best_loss=8.21668
Epoch 21/80: current_loss=8.94230 | best_loss=8.21668
Epoch 22/80: current_loss=9.14886 | best_loss=8.21668
Epoch 23/80: current_loss=8.66096 | best_loss=8.21668
Epoch 24/80: current_loss=8.21511 | best_loss=8.21511
Epoch 25/80: current_loss=8.22175 | best_loss=8.21511
Epoch 26/80: current_loss=8.90023 | best_loss=8.21511
Epoch 27/80: current_loss=9.42002 | best_loss=8.21511
Epoch 28/80: current_loss=8.40917 | best_loss=8.21511
Epoch 29/80: current_loss=8.79308 | best_loss=8.21511
Epoch 30/80: current_loss=8.33476 | best_loss=8.21511
Epoch 31/80: current_loss=8.27464 | best_loss=8.21511
Epoch 32/80: current_loss=8.59488 | best_loss=8.21511
Epoch 33/80: current_loss=8.23824 | best_loss=8.21511
Epoch 34/80: current_loss=8.22943 | best_loss=8.21511
Epoch 35/80: current_loss=8.21801 | best_loss=8.21511
Epoch 36/80: current_loss=9.09487 | best_loss=8.21511
Epoch 37/80: current_loss=8.26922 | best_loss=8.21511
Epoch 38/80: current_loss=8.42377 | best_loss=8.21511
Epoch 39/80: current_loss=8.77143 | best_loss=8.21511
Epoch 40/80: current_loss=9.50487 | best_loss=8.21511
Epoch 41/80: current_loss=8.24437 | best_loss=8.21511
Epoch 42/80: current_loss=9.51056 | best_loss=8.21511
Epoch 43/80: current_loss=8.49677 | best_loss=8.21511
Epoch 44/80: current_loss=8.61636 | best_loss=8.21511
Early Stopping at epoch 44
      explained_var=0.00213 | mse_loss=8.31415
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.73880 | best_loss=7.73880
Epoch 1/80: current_loss=7.78809 | best_loss=7.73880
Epoch 2/80: current_loss=8.24625 | best_loss=7.73880
Epoch 3/80: current_loss=11.81528 | best_loss=7.73880
Epoch 4/80: current_loss=8.81571 | best_loss=7.73880
Epoch 5/80: current_loss=7.79733 | best_loss=7.73880
Epoch 6/80: current_loss=7.71139 | best_loss=7.71139
Epoch 7/80: current_loss=8.30262 | best_loss=7.71139
Epoch 8/80: current_loss=7.79338 | best_loss=7.71139
Epoch 9/80: current_loss=7.70213 | best_loss=7.70213
Epoch 10/80: current_loss=7.71799 | best_loss=7.70213
Epoch 11/80: current_loss=7.70690 | best_loss=7.70213
Epoch 12/80: current_loss=7.85767 | best_loss=7.70213
Epoch 13/80: current_loss=7.68581 | best_loss=7.68581
Epoch 14/80: current_loss=7.81089 | best_loss=7.68581
Epoch 15/80: current_loss=7.74361 | best_loss=7.68581
Epoch 16/80: current_loss=7.77158 | best_loss=7.68581
Epoch 17/80: current_loss=7.70279 | best_loss=7.68581
Epoch 18/80: current_loss=7.68895 | best_loss=7.68581
Epoch 19/80: current_loss=7.83898 | best_loss=7.68581
Epoch 20/80: current_loss=7.69779 | best_loss=7.68581
Epoch 21/80: current_loss=7.67127 | best_loss=7.67127
Epoch 22/80: current_loss=7.76884 | best_loss=7.67127
Epoch 23/80: current_loss=7.87750 | best_loss=7.67127
Epoch 24/80: current_loss=7.72046 | best_loss=7.67127
Epoch 25/80: current_loss=7.74197 | best_loss=7.67127
Epoch 26/80: current_loss=7.71408 | best_loss=7.67127
Epoch 27/80: current_loss=7.71471 | best_loss=7.67127
Epoch 28/80: current_loss=7.72154 | best_loss=7.67127
Epoch 29/80: current_loss=7.65086 | best_loss=7.65086
Epoch 30/80: current_loss=7.72052 | best_loss=7.65086
Epoch 31/80: current_loss=7.75847 | best_loss=7.65086
Epoch 32/80: current_loss=7.91526 | best_loss=7.65086
Epoch 33/80: current_loss=8.15984 | best_loss=7.65086
Epoch 34/80: current_loss=7.95976 | best_loss=7.65086
Epoch 35/80: current_loss=7.74958 | best_loss=7.65086
Epoch 36/80: current_loss=7.67846 | best_loss=7.65086
Epoch 37/80: current_loss=7.68107 | best_loss=7.65086
Epoch 38/80: current_loss=8.65275 | best_loss=7.65086
Epoch 39/80: current_loss=7.71067 | best_loss=7.65086
Epoch 40/80: current_loss=8.40086 | best_loss=7.65086
Epoch 41/80: current_loss=7.88581 | best_loss=7.65086
Epoch 42/80: current_loss=7.83305 | best_loss=7.65086
Epoch 43/80: current_loss=7.71885 | best_loss=7.65086
Epoch 44/80: current_loss=7.82934 | best_loss=7.65086
Epoch 45/80: current_loss=8.09773 | best_loss=7.65086
Epoch 46/80: current_loss=7.82726 | best_loss=7.65086
Epoch 47/80: current_loss=7.68849 | best_loss=7.65086
Epoch 48/80: current_loss=7.78920 | best_loss=7.65086
Epoch 49/80: current_loss=7.93074 | best_loss=7.65086
Early Stopping at epoch 49
      explained_var=0.00361 | mse_loss=7.82911
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.00207| avg_loss=8.07170
----------------------------------------------


----------------------------------------------
Params for Trial 17
{'learning_rate': 0.01, 'weight_decay': 0.003309128484976863, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.33433 | best_loss=10.33433
Epoch 1/80: current_loss=8.02673 | best_loss=8.02673
Epoch 2/80: current_loss=8.23661 | best_loss=8.02673
Epoch 3/80: current_loss=8.10398 | best_loss=8.02673
Epoch 4/80: current_loss=8.59812 | best_loss=8.02673
Epoch 5/80: current_loss=8.44802 | best_loss=8.02673
Epoch 6/80: current_loss=7.78642 | best_loss=7.78642
Epoch 7/80: current_loss=7.89429 | best_loss=7.78642
Epoch 8/80: current_loss=9.81821 | best_loss=7.78642
Epoch 9/80: current_loss=8.10825 | best_loss=7.78642
Epoch 10/80: current_loss=7.78925 | best_loss=7.78642
Epoch 11/80: current_loss=8.27932 | best_loss=7.78642
Epoch 12/80: current_loss=8.31236 | best_loss=7.78642
Epoch 13/80: current_loss=8.25671 | best_loss=7.78642
Epoch 14/80: current_loss=8.36730 | best_loss=7.78642
Epoch 15/80: current_loss=8.65645 | best_loss=7.78642
Epoch 16/80: current_loss=7.77333 | best_loss=7.77333
Epoch 17/80: current_loss=7.91877 | best_loss=7.77333
Epoch 18/80: current_loss=9.08652 | best_loss=7.77333
Epoch 19/80: current_loss=7.79350 | best_loss=7.77333
Epoch 20/80: current_loss=7.76517 | best_loss=7.76517
Epoch 21/80: current_loss=8.52409 | best_loss=7.76517
Epoch 22/80: current_loss=8.10314 | best_loss=7.76517
Epoch 23/80: current_loss=7.91297 | best_loss=7.76517
Epoch 24/80: current_loss=8.49161 | best_loss=7.76517
Epoch 25/80: current_loss=7.75298 | best_loss=7.75298
Epoch 26/80: current_loss=7.76824 | best_loss=7.75298
Epoch 27/80: current_loss=9.83450 | best_loss=7.75298
Epoch 28/80: current_loss=7.92014 | best_loss=7.75298
Epoch 29/80: current_loss=8.09669 | best_loss=7.75298
Epoch 30/80: current_loss=8.37386 | best_loss=7.75298
Epoch 31/80: current_loss=7.77405 | best_loss=7.75298
Epoch 32/80: current_loss=8.47735 | best_loss=7.75298
Epoch 33/80: current_loss=10.62652 | best_loss=7.75298
Epoch 34/80: current_loss=7.92497 | best_loss=7.75298
Epoch 35/80: current_loss=7.79467 | best_loss=7.75298
Epoch 36/80: current_loss=7.80022 | best_loss=7.75298
Epoch 37/80: current_loss=7.75964 | best_loss=7.75298
Epoch 38/80: current_loss=8.28061 | best_loss=7.75298
Epoch 39/80: current_loss=8.24372 | best_loss=7.75298
Epoch 40/80: current_loss=9.08838 | best_loss=7.75298
Epoch 41/80: current_loss=9.37453 | best_loss=7.75298
Epoch 42/80: current_loss=7.76466 | best_loss=7.75298
Epoch 43/80: current_loss=7.76607 | best_loss=7.75298
Epoch 44/80: current_loss=9.17481 | best_loss=7.75298
Epoch 45/80: current_loss=10.25842 | best_loss=7.75298
Early Stopping at epoch 45
      explained_var=0.00230 | mse_loss=7.57454
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.34967 | best_loss=8.34967
Epoch 1/80: current_loss=8.65775 | best_loss=8.34967
Epoch 2/80: current_loss=8.32337 | best_loss=8.32337
Epoch 3/80: current_loss=8.39004 | best_loss=8.32337
Epoch 4/80: current_loss=8.36198 | best_loss=8.32337
Epoch 5/80: current_loss=8.33894 | best_loss=8.32337
Epoch 6/80: current_loss=8.38552 | best_loss=8.32337
Epoch 7/80: current_loss=8.48293 | best_loss=8.32337
Epoch 8/80: current_loss=8.51549 | best_loss=8.32337
Epoch 9/80: current_loss=8.50461 | best_loss=8.32337
Epoch 10/80: current_loss=8.51551 | best_loss=8.32337
Epoch 11/80: current_loss=9.10977 | best_loss=8.32337
Epoch 12/80: current_loss=8.36480 | best_loss=8.32337
Epoch 13/80: current_loss=8.48834 | best_loss=8.32337
Epoch 14/80: current_loss=8.33942 | best_loss=8.32337
Epoch 15/80: current_loss=8.37851 | best_loss=8.32337
Epoch 16/80: current_loss=8.32935 | best_loss=8.32337
Epoch 17/80: current_loss=8.33164 | best_loss=8.32337
Epoch 18/80: current_loss=8.32527 | best_loss=8.32337
Epoch 19/80: current_loss=8.31704 | best_loss=8.31704
Epoch 20/80: current_loss=8.63531 | best_loss=8.31704
Epoch 21/80: current_loss=8.72585 | best_loss=8.31704
Epoch 22/80: current_loss=89.51302 | best_loss=8.31704
Epoch 23/80: current_loss=76.79376 | best_loss=8.31704
Epoch 24/80: current_loss=12.73378 | best_loss=8.31704
Epoch 25/80: current_loss=8.72008 | best_loss=8.31704
Epoch 26/80: current_loss=8.71242 | best_loss=8.31704
Epoch 27/80: current_loss=8.78062 | best_loss=8.31704
Epoch 28/80: current_loss=8.44820 | best_loss=8.31704
Epoch 29/80: current_loss=8.37818 | best_loss=8.31704
Epoch 30/80: current_loss=8.58520 | best_loss=8.31704
Epoch 31/80: current_loss=8.67832 | best_loss=8.31704
Epoch 32/80: current_loss=8.53819 | best_loss=8.31704
Epoch 33/80: current_loss=8.57005 | best_loss=8.31704
Epoch 34/80: current_loss=8.69311 | best_loss=8.31704
Epoch 35/80: current_loss=8.48180 | best_loss=8.31704
Epoch 36/80: current_loss=8.60658 | best_loss=8.31704
Epoch 37/80: current_loss=8.51031 | best_loss=8.31704
Epoch 38/80: current_loss=8.43071 | best_loss=8.31704
Epoch 39/80: current_loss=8.41391 | best_loss=8.31704
Early Stopping at epoch 39
      explained_var=-0.00009 | mse_loss=8.16860
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.58941 | best_loss=8.58941
Epoch 1/80: current_loss=10.69754 | best_loss=8.58941
Epoch 2/80: current_loss=9.85399 | best_loss=8.58941
Epoch 3/80: current_loss=10.88096 | best_loss=8.58941
Epoch 4/80: current_loss=8.87038 | best_loss=8.58941
Epoch 5/80: current_loss=8.80543 | best_loss=8.58941
Epoch 6/80: current_loss=8.92478 | best_loss=8.58941
Epoch 7/80: current_loss=8.74343 | best_loss=8.58941
Epoch 8/80: current_loss=54.05736 | best_loss=8.58941
Epoch 9/80: current_loss=16.41362 | best_loss=8.58941
Epoch 10/80: current_loss=9.21075 | best_loss=8.58941
Epoch 11/80: current_loss=10.12461 | best_loss=8.58941
Epoch 12/80: current_loss=8.76403 | best_loss=8.58941
Epoch 13/80: current_loss=9.11299 | best_loss=8.58941
Epoch 14/80: current_loss=9.01860 | best_loss=8.58941
Epoch 15/80: current_loss=8.79300 | best_loss=8.58941
Epoch 16/80: current_loss=9.14575 | best_loss=8.58941
Epoch 17/80: current_loss=8.93042 | best_loss=8.58941
Epoch 18/80: current_loss=8.78569 | best_loss=8.58941
Epoch 19/80: current_loss=8.90975 | best_loss=8.58941
Epoch 20/80: current_loss=8.86513 | best_loss=8.58941
Early Stopping at epoch 20
      explained_var=0.01789 | mse_loss=8.38592
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.52103 | best_loss=8.52103
Epoch 1/80: current_loss=8.47394 | best_loss=8.47394
Epoch 2/80: current_loss=9.31486 | best_loss=8.47394
Epoch 3/80: current_loss=8.28206 | best_loss=8.28206
Epoch 4/80: current_loss=8.25874 | best_loss=8.25874
Epoch 5/80: current_loss=8.35418 | best_loss=8.25874
Epoch 6/80: current_loss=8.26710 | best_loss=8.25874
Epoch 7/80: current_loss=8.51858 | best_loss=8.25874
Epoch 8/80: current_loss=8.45256 | best_loss=8.25874
Epoch 9/80: current_loss=8.35106 | best_loss=8.25874
Epoch 10/80: current_loss=8.24983 | best_loss=8.24983
Epoch 11/80: current_loss=8.23358 | best_loss=8.23358
Epoch 12/80: current_loss=9.23922 | best_loss=8.23358
Epoch 13/80: current_loss=8.63729 | best_loss=8.23358
Epoch 14/80: current_loss=8.24802 | best_loss=8.23358
Epoch 15/80: current_loss=8.51927 | best_loss=8.23358
Epoch 16/80: current_loss=8.23685 | best_loss=8.23358
Epoch 17/80: current_loss=8.61265 | best_loss=8.23358
Epoch 18/80: current_loss=8.23095 | best_loss=8.23095
Epoch 19/80: current_loss=8.39685 | best_loss=8.23095
Epoch 20/80: current_loss=8.23458 | best_loss=8.23095
Epoch 21/80: current_loss=8.33559 | best_loss=8.23095
Epoch 22/80: current_loss=8.23594 | best_loss=8.23095
Epoch 23/80: current_loss=8.29474 | best_loss=8.23095
Epoch 24/80: current_loss=8.22760 | best_loss=8.22760
Epoch 25/80: current_loss=8.33145 | best_loss=8.22760
Epoch 26/80: current_loss=8.29434 | best_loss=8.22760
Epoch 27/80: current_loss=8.27966 | best_loss=8.22760
Epoch 28/80: current_loss=8.30685 | best_loss=8.22760
Epoch 29/80: current_loss=8.27852 | best_loss=8.22760
Epoch 30/80: current_loss=8.80202 | best_loss=8.22760
Epoch 31/80: current_loss=8.78781 | best_loss=8.22760
Epoch 32/80: current_loss=8.22245 | best_loss=8.22245
Epoch 33/80: current_loss=8.23167 | best_loss=8.22245
Epoch 34/80: current_loss=8.32082 | best_loss=8.22245
Epoch 35/80: current_loss=8.28171 | best_loss=8.22245
Epoch 36/80: current_loss=8.42076 | best_loss=8.22245
Epoch 37/80: current_loss=9.13581 | best_loss=8.22245
Epoch 38/80: current_loss=8.51892 | best_loss=8.22245
Epoch 39/80: current_loss=8.27374 | best_loss=8.22245
Epoch 40/80: current_loss=8.36164 | best_loss=8.22245
Epoch 41/80: current_loss=8.56448 | best_loss=8.22245
Epoch 42/80: current_loss=8.38107 | best_loss=8.22245
Epoch 43/80: current_loss=8.20921 | best_loss=8.20921
Epoch 44/80: current_loss=8.64623 | best_loss=8.20921
Epoch 45/80: current_loss=8.13476 | best_loss=8.13476
Epoch 46/80: current_loss=8.24971 | best_loss=8.13476
Epoch 47/80: current_loss=8.46826 | best_loss=8.13476
Epoch 48/80: current_loss=8.19736 | best_loss=8.13476
Epoch 49/80: current_loss=8.36237 | best_loss=8.13476
Epoch 50/80: current_loss=8.70983 | best_loss=8.13476
Epoch 51/80: current_loss=8.40779 | best_loss=8.13476
Epoch 52/80: current_loss=8.25234 | best_loss=8.13476
Epoch 53/80: current_loss=8.45511 | best_loss=8.13476
Epoch 54/80: current_loss=8.34122 | best_loss=8.13476
Epoch 55/80: current_loss=9.42646 | best_loss=8.13476
Epoch 56/80: current_loss=8.31133 | best_loss=8.13476
Epoch 57/80: current_loss=8.25849 | best_loss=8.13476
Epoch 58/80: current_loss=8.21462 | best_loss=8.13476
Epoch 59/80: current_loss=8.46747 | best_loss=8.13476
Epoch 60/80: current_loss=8.29934 | best_loss=8.13476
Epoch 61/80: current_loss=11.93294 | best_loss=8.13476
Epoch 62/80: current_loss=8.42391 | best_loss=8.13476
Epoch 63/80: current_loss=9.02745 | best_loss=8.13476
Epoch 64/80: current_loss=8.53589 | best_loss=8.13476
Epoch 65/80: current_loss=8.23587 | best_loss=8.13476
Early Stopping at epoch 65
      explained_var=0.01363 | mse_loss=8.23618
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.16922 | best_loss=10.16922
Epoch 1/80: current_loss=7.89430 | best_loss=7.89430
Epoch 2/80: current_loss=7.64991 | best_loss=7.64991
Epoch 3/80: current_loss=7.83467 | best_loss=7.64991
Epoch 4/80: current_loss=8.53376 | best_loss=7.64991
Epoch 5/80: current_loss=7.75249 | best_loss=7.64991
Epoch 6/80: current_loss=8.62403 | best_loss=7.64991
Epoch 7/80: current_loss=8.82833 | best_loss=7.64991
Epoch 8/80: current_loss=7.97794 | best_loss=7.64991
Epoch 9/80: current_loss=7.63880 | best_loss=7.63880
Epoch 10/80: current_loss=8.10073 | best_loss=7.63880
Epoch 11/80: current_loss=7.86623 | best_loss=7.63880
Epoch 12/80: current_loss=7.98466 | best_loss=7.63880
Epoch 13/80: current_loss=8.27605 | best_loss=7.63880
Epoch 14/80: current_loss=7.68674 | best_loss=7.63880
Epoch 15/80: current_loss=8.02676 | best_loss=7.63880
Epoch 16/80: current_loss=7.85318 | best_loss=7.63880
Epoch 17/80: current_loss=7.79946 | best_loss=7.63880
Epoch 18/80: current_loss=8.07765 | best_loss=7.63880
Epoch 19/80: current_loss=8.00701 | best_loss=7.63880
Epoch 20/80: current_loss=7.77408 | best_loss=7.63880
Epoch 21/80: current_loss=7.91968 | best_loss=7.63880
Epoch 22/80: current_loss=8.46550 | best_loss=7.63880
Epoch 23/80: current_loss=7.97732 | best_loss=7.63880
Epoch 24/80: current_loss=8.05181 | best_loss=7.63880
Epoch 25/80: current_loss=7.89873 | best_loss=7.63880
Epoch 26/80: current_loss=7.77635 | best_loss=7.63880
Epoch 27/80: current_loss=7.70702 | best_loss=7.63880
Epoch 28/80: current_loss=8.40098 | best_loss=7.63880
Epoch 29/80: current_loss=10.20636 | best_loss=7.63880
Early Stopping at epoch 29
      explained_var=0.00681 | mse_loss=7.81279
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00811| avg_loss=8.03560
----------------------------------------------


----------------------------------------------
Params for Trial 18
{'learning_rate': 0.01, 'weight_decay': 0.0031521680053919715, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.82738 | best_loss=12.82738
Epoch 1/80: current_loss=7.76802 | best_loss=7.76802
Epoch 2/80: current_loss=8.52468 | best_loss=7.76802
Epoch 3/80: current_loss=8.13236 | best_loss=7.76802
Epoch 4/80: current_loss=8.07337 | best_loss=7.76802
Epoch 5/80: current_loss=9.19959 | best_loss=7.76802
Epoch 6/80: current_loss=7.80101 | best_loss=7.76802
Epoch 7/80: current_loss=9.79355 | best_loss=7.76802
Epoch 8/80: current_loss=7.76798 | best_loss=7.76798
Epoch 9/80: current_loss=8.20311 | best_loss=7.76798
Epoch 10/80: current_loss=9.01684 | best_loss=7.76798
Epoch 11/80: current_loss=7.82018 | best_loss=7.76798
Epoch 12/80: current_loss=8.83129 | best_loss=7.76798
Epoch 13/80: current_loss=8.25034 | best_loss=7.76798
Epoch 14/80: current_loss=8.32714 | best_loss=7.76798
Epoch 15/80: current_loss=7.96423 | best_loss=7.76798
Epoch 16/80: current_loss=7.95638 | best_loss=7.76798
Epoch 17/80: current_loss=8.27257 | best_loss=7.76798
Epoch 18/80: current_loss=8.72282 | best_loss=7.76798
Epoch 19/80: current_loss=7.92126 | best_loss=7.76798
Epoch 20/80: current_loss=8.21749 | best_loss=7.76798
Epoch 21/80: current_loss=8.29723 | best_loss=7.76798
Epoch 22/80: current_loss=7.77256 | best_loss=7.76798
Epoch 23/80: current_loss=7.77552 | best_loss=7.76798
Epoch 24/80: current_loss=8.19422 | best_loss=7.76798
Epoch 25/80: current_loss=7.76676 | best_loss=7.76676
Epoch 26/80: current_loss=7.93474 | best_loss=7.76676
Epoch 27/80: current_loss=8.65379 | best_loss=7.76676
Epoch 28/80: current_loss=7.98936 | best_loss=7.76676
Epoch 29/80: current_loss=8.89708 | best_loss=7.76676
Epoch 30/80: current_loss=7.78699 | best_loss=7.76676
Epoch 31/80: current_loss=7.92174 | best_loss=7.76676
Epoch 32/80: current_loss=8.01803 | best_loss=7.76676
Epoch 33/80: current_loss=8.07589 | best_loss=7.76676
Epoch 34/80: current_loss=7.78599 | best_loss=7.76676
Epoch 35/80: current_loss=9.58656 | best_loss=7.76676
Epoch 36/80: current_loss=9.16953 | best_loss=7.76676
Epoch 37/80: current_loss=7.76560 | best_loss=7.76560
Epoch 38/80: current_loss=8.04148 | best_loss=7.76560
Epoch 39/80: current_loss=9.34961 | best_loss=7.76560
Epoch 40/80: current_loss=7.75509 | best_loss=7.75509
Epoch 41/80: current_loss=7.76970 | best_loss=7.75509
Epoch 42/80: current_loss=7.78064 | best_loss=7.75509
Epoch 43/80: current_loss=9.81021 | best_loss=7.75509
Epoch 44/80: current_loss=8.62369 | best_loss=7.75509
Epoch 45/80: current_loss=8.45000 | best_loss=7.75509
Epoch 46/80: current_loss=7.81110 | best_loss=7.75509
Epoch 47/80: current_loss=8.37324 | best_loss=7.75509
Epoch 48/80: current_loss=11.09293 | best_loss=7.75509
Epoch 49/80: current_loss=8.05782 | best_loss=7.75509
Epoch 50/80: current_loss=8.33731 | best_loss=7.75509
Epoch 51/80: current_loss=8.04353 | best_loss=7.75509
Epoch 52/80: current_loss=8.37482 | best_loss=7.75509
Epoch 53/80: current_loss=9.09975 | best_loss=7.75509
Epoch 54/80: current_loss=7.78113 | best_loss=7.75509
Epoch 55/80: current_loss=7.97435 | best_loss=7.75509
Epoch 56/80: current_loss=8.31053 | best_loss=7.75509
Epoch 57/80: current_loss=7.76043 | best_loss=7.75509
Epoch 58/80: current_loss=7.75698 | best_loss=7.75509
Epoch 59/80: current_loss=9.23311 | best_loss=7.75509
Epoch 60/80: current_loss=7.94837 | best_loss=7.75509
Early Stopping at epoch 60
      explained_var=0.00197 | mse_loss=7.57499
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.35669 | best_loss=8.35669
Epoch 1/80: current_loss=8.34459 | best_loss=8.34459
Epoch 2/80: current_loss=8.38604 | best_loss=8.34459
Epoch 3/80: current_loss=8.41250 | best_loss=8.34459
Epoch 4/80: current_loss=8.38333 | best_loss=8.34459
Epoch 5/80: current_loss=9.26040 | best_loss=8.34459
Epoch 6/80: current_loss=8.42998 | best_loss=8.34459
Epoch 7/80: current_loss=8.46739 | best_loss=8.34459
Epoch 8/80: current_loss=8.47378 | best_loss=8.34459
Epoch 9/80: current_loss=8.66892 | best_loss=8.34459
Epoch 10/80: current_loss=8.57590 | best_loss=8.34459
Epoch 11/80: current_loss=8.41112 | best_loss=8.34459
Epoch 12/80: current_loss=10.18369 | best_loss=8.34459
Epoch 13/80: current_loss=8.64411 | best_loss=8.34459
Epoch 14/80: current_loss=8.45415 | best_loss=8.34459
Epoch 15/80: current_loss=8.34118 | best_loss=8.34118
Epoch 16/80: current_loss=8.93112 | best_loss=8.34118
Epoch 17/80: current_loss=8.50188 | best_loss=8.34118
Epoch 18/80: current_loss=9.18080 | best_loss=8.34118
Epoch 19/80: current_loss=9.26135 | best_loss=8.34118
Epoch 20/80: current_loss=8.87284 | best_loss=8.34118
Epoch 21/80: current_loss=8.49178 | best_loss=8.34118
Epoch 22/80: current_loss=8.59463 | best_loss=8.34118
Epoch 23/80: current_loss=8.36368 | best_loss=8.34118
Epoch 24/80: current_loss=8.32741 | best_loss=8.32741
Epoch 25/80: current_loss=9.21796 | best_loss=8.32741
Epoch 26/80: current_loss=8.55179 | best_loss=8.32741
Epoch 27/80: current_loss=8.34067 | best_loss=8.32741
Epoch 28/80: current_loss=8.34642 | best_loss=8.32741
Epoch 29/80: current_loss=8.37257 | best_loss=8.32741
Epoch 30/80: current_loss=8.40973 | best_loss=8.32741
Epoch 31/80: current_loss=8.77056 | best_loss=8.32741
Epoch 32/80: current_loss=8.31867 | best_loss=8.31867
Epoch 33/80: current_loss=8.36447 | best_loss=8.31867
Epoch 34/80: current_loss=8.97155 | best_loss=8.31867
Epoch 35/80: current_loss=8.49549 | best_loss=8.31867
Epoch 36/80: current_loss=8.36470 | best_loss=8.31867
Epoch 37/80: current_loss=8.63603 | best_loss=8.31867
Epoch 38/80: current_loss=8.58455 | best_loss=8.31867
Epoch 39/80: current_loss=9.69988 | best_loss=8.31867
Epoch 40/80: current_loss=8.32952 | best_loss=8.31867
Epoch 41/80: current_loss=8.33549 | best_loss=8.31867
Epoch 42/80: current_loss=9.04981 | best_loss=8.31867
Epoch 43/80: current_loss=8.64328 | best_loss=8.31867
Epoch 44/80: current_loss=8.32001 | best_loss=8.31867
Epoch 45/80: current_loss=8.31710 | best_loss=8.31710
Epoch 46/80: current_loss=8.77019 | best_loss=8.31710
Epoch 47/80: current_loss=8.94537 | best_loss=8.31710
Epoch 48/80: current_loss=8.30970 | best_loss=8.30970
Epoch 49/80: current_loss=8.59920 | best_loss=8.30970
Epoch 50/80: current_loss=8.92292 | best_loss=8.30970
Epoch 51/80: current_loss=8.42779 | best_loss=8.30970
Epoch 52/80: current_loss=8.75211 | best_loss=8.30970
Epoch 53/80: current_loss=8.45312 | best_loss=8.30970
Epoch 54/80: current_loss=8.50122 | best_loss=8.30970
Epoch 55/80: current_loss=8.32540 | best_loss=8.30970
Epoch 56/80: current_loss=8.31191 | best_loss=8.30970
Epoch 57/80: current_loss=8.30853 | best_loss=8.30853
Epoch 58/80: current_loss=8.63523 | best_loss=8.30853
Epoch 59/80: current_loss=8.39178 | best_loss=8.30853
Epoch 60/80: current_loss=8.31502 | best_loss=8.30853
Epoch 61/80: current_loss=8.56595 | best_loss=8.30853
Epoch 62/80: current_loss=8.32483 | best_loss=8.30853
Epoch 63/80: current_loss=8.65648 | best_loss=8.30853
Epoch 64/80: current_loss=9.87175 | best_loss=8.30853
Epoch 65/80: current_loss=8.35893 | best_loss=8.30853
Epoch 66/80: current_loss=8.57005 | best_loss=8.30853
Epoch 67/80: current_loss=8.42095 | best_loss=8.30853
Epoch 68/80: current_loss=8.42050 | best_loss=8.30853
Epoch 69/80: current_loss=8.36645 | best_loss=8.30853
Epoch 70/80: current_loss=8.32154 | best_loss=8.30853
Epoch 71/80: current_loss=9.01854 | best_loss=8.30853
Epoch 72/80: current_loss=8.34119 | best_loss=8.30853
Epoch 73/80: current_loss=8.33134 | best_loss=8.30853
Epoch 74/80: current_loss=8.32754 | best_loss=8.30853
Epoch 75/80: current_loss=8.33123 | best_loss=8.30853
Epoch 76/80: current_loss=9.75615 | best_loss=8.30853
Epoch 77/80: current_loss=8.60764 | best_loss=8.30853
Early Stopping at epoch 77
      explained_var=0.00068 | mse_loss=8.15891
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.68468 | best_loss=9.68468
Epoch 1/80: current_loss=8.91550 | best_loss=8.91550
Epoch 2/80: current_loss=8.73835 | best_loss=8.73835
Epoch 3/80: current_loss=8.79221 | best_loss=8.73835
Epoch 4/80: current_loss=8.96598 | best_loss=8.73835
Epoch 5/80: current_loss=8.70832 | best_loss=8.70832
Epoch 6/80: current_loss=9.02688 | best_loss=8.70832
Epoch 7/80: current_loss=8.78822 | best_loss=8.70832
Epoch 8/80: current_loss=9.09041 | best_loss=8.70832
Epoch 9/80: current_loss=8.86133 | best_loss=8.70832
Epoch 10/80: current_loss=9.43269 | best_loss=8.70832
Epoch 11/80: current_loss=8.76838 | best_loss=8.70832
Epoch 12/80: current_loss=8.96712 | best_loss=8.70832
Epoch 13/80: current_loss=8.81890 | best_loss=8.70832
Epoch 14/80: current_loss=9.79860 | best_loss=8.70832
Epoch 15/80: current_loss=8.92847 | best_loss=8.70832
Epoch 16/80: current_loss=9.10062 | best_loss=8.70832
Epoch 17/80: current_loss=8.72425 | best_loss=8.70832
Epoch 18/80: current_loss=8.84313 | best_loss=8.70832
Epoch 19/80: current_loss=9.94835 | best_loss=8.70832
Epoch 20/80: current_loss=10.45342 | best_loss=8.70832
Epoch 21/80: current_loss=8.76315 | best_loss=8.70832
Epoch 22/80: current_loss=8.76794 | best_loss=8.70832
Epoch 23/80: current_loss=8.80935 | best_loss=8.70832
Epoch 24/80: current_loss=8.79476 | best_loss=8.70832
Epoch 25/80: current_loss=9.70298 | best_loss=8.70832
Early Stopping at epoch 25
      explained_var=0.00087 | mse_loss=8.47532
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.77943 | best_loss=8.77943
Epoch 1/80: current_loss=8.32245 | best_loss=8.32245
Epoch 2/80: current_loss=8.26903 | best_loss=8.26903
Epoch 3/80: current_loss=8.30504 | best_loss=8.26903
Epoch 4/80: current_loss=8.45015 | best_loss=8.26903
Epoch 5/80: current_loss=8.53256 | best_loss=8.26903
Epoch 6/80: current_loss=8.23028 | best_loss=8.23028
Epoch 7/80: current_loss=8.60042 | best_loss=8.23028
Epoch 8/80: current_loss=8.61800 | best_loss=8.23028
Epoch 9/80: current_loss=8.22655 | best_loss=8.22655
Epoch 10/80: current_loss=9.52168 | best_loss=8.22655
Epoch 11/80: current_loss=8.23701 | best_loss=8.22655
Epoch 12/80: current_loss=8.23387 | best_loss=8.22655
Epoch 13/80: current_loss=9.71083 | best_loss=8.22655
Epoch 14/80: current_loss=8.53207 | best_loss=8.22655
Epoch 15/80: current_loss=8.49665 | best_loss=8.22655
Epoch 16/80: current_loss=8.42361 | best_loss=8.22655
Epoch 17/80: current_loss=8.47943 | best_loss=8.22655
Epoch 18/80: current_loss=8.31489 | best_loss=8.22655
Epoch 19/80: current_loss=8.42530 | best_loss=8.22655
Epoch 20/80: current_loss=8.51405 | best_loss=8.22655
Epoch 21/80: current_loss=9.94647 | best_loss=8.22655
Epoch 22/80: current_loss=8.26752 | best_loss=8.22655
Epoch 23/80: current_loss=8.49205 | best_loss=8.22655
Epoch 24/80: current_loss=8.19593 | best_loss=8.19593
Epoch 25/80: current_loss=8.22636 | best_loss=8.19593
Epoch 26/80: current_loss=8.35223 | best_loss=8.19593
Epoch 27/80: current_loss=8.38893 | best_loss=8.19593
Epoch 28/80: current_loss=8.93036 | best_loss=8.19593
Epoch 29/80: current_loss=8.51063 | best_loss=8.19593
Epoch 30/80: current_loss=8.65338 | best_loss=8.19593
Epoch 31/80: current_loss=9.24891 | best_loss=8.19593
Epoch 32/80: current_loss=8.35434 | best_loss=8.19593
Epoch 33/80: current_loss=8.36678 | best_loss=8.19593
Epoch 34/80: current_loss=8.27432 | best_loss=8.19593
Epoch 35/80: current_loss=8.34215 | best_loss=8.19593
Epoch 36/80: current_loss=8.70931 | best_loss=8.19593
Epoch 37/80: current_loss=8.23953 | best_loss=8.19593
Epoch 38/80: current_loss=8.52537 | best_loss=8.19593
Epoch 39/80: current_loss=8.25898 | best_loss=8.19593
Epoch 40/80: current_loss=8.28082 | best_loss=8.19593
Epoch 41/80: current_loss=8.41462 | best_loss=8.19593
Epoch 42/80: current_loss=8.26615 | best_loss=8.19593
Epoch 43/80: current_loss=8.37701 | best_loss=8.19593
Epoch 44/80: current_loss=8.55245 | best_loss=8.19593
Early Stopping at epoch 44
      explained_var=0.00357 | mse_loss=8.30220
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.77937 | best_loss=7.77937
Epoch 1/80: current_loss=9.24124 | best_loss=7.77937
Epoch 2/80: current_loss=7.82619 | best_loss=7.77937
Epoch 3/80: current_loss=8.67925 | best_loss=7.77937
Epoch 4/80: current_loss=7.77522 | best_loss=7.77522
Epoch 5/80: current_loss=7.82760 | best_loss=7.77522
Epoch 6/80: current_loss=8.09471 | best_loss=7.77522
Epoch 7/80: current_loss=7.71525 | best_loss=7.71525
Epoch 8/80: current_loss=7.87055 | best_loss=7.71525
Epoch 9/80: current_loss=7.73681 | best_loss=7.71525
Epoch 10/80: current_loss=8.11048 | best_loss=7.71525
Epoch 11/80: current_loss=7.70465 | best_loss=7.70465
Epoch 12/80: current_loss=7.97445 | best_loss=7.70465
Epoch 13/80: current_loss=7.93261 | best_loss=7.70465
Epoch 14/80: current_loss=7.71737 | best_loss=7.70465
Epoch 15/80: current_loss=7.81140 | best_loss=7.70465
Epoch 16/80: current_loss=8.00497 | best_loss=7.70465
Epoch 17/80: current_loss=8.04070 | best_loss=7.70465
Epoch 18/80: current_loss=8.07139 | best_loss=7.70465
Epoch 19/80: current_loss=7.77766 | best_loss=7.70465
Epoch 20/80: current_loss=7.84461 | best_loss=7.70465
Epoch 21/80: current_loss=8.12771 | best_loss=7.70465
Epoch 22/80: current_loss=7.77788 | best_loss=7.70465
Epoch 23/80: current_loss=7.78859 | best_loss=7.70465
Epoch 24/80: current_loss=7.76870 | best_loss=7.70465
Epoch 25/80: current_loss=8.47630 | best_loss=7.70465
Epoch 26/80: current_loss=8.31636 | best_loss=7.70465
Epoch 27/80: current_loss=7.80553 | best_loss=7.70465
Epoch 28/80: current_loss=8.51799 | best_loss=7.70465
Epoch 29/80: current_loss=7.76980 | best_loss=7.70465
Epoch 30/80: current_loss=8.16328 | best_loss=7.70465
Epoch 31/80: current_loss=8.66250 | best_loss=7.70465
Early Stopping at epoch 31
      explained_var=-0.00277 | mse_loss=7.87755
----------------------------------------------
Average early_stopping_point: 27| avg_exp_var=0.00086| avg_loss=8.07780
----------------------------------------------


----------------------------------------------
Params for Trial 19
{'learning_rate': 0.01, 'weight_decay': 0.0031199700465094505, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.60439 | best_loss=12.60439
Epoch 1/80: current_loss=7.77162 | best_loss=7.77162
Epoch 2/80: current_loss=8.24196 | best_loss=7.77162
Epoch 3/80: current_loss=7.86726 | best_loss=7.77162
Epoch 4/80: current_loss=8.55135 | best_loss=7.77162
Epoch 5/80: current_loss=7.80849 | best_loss=7.77162
Epoch 6/80: current_loss=8.47965 | best_loss=7.77162
Epoch 7/80: current_loss=8.27546 | best_loss=7.77162
Epoch 8/80: current_loss=8.22139 | best_loss=7.77162
Epoch 9/80: current_loss=7.80242 | best_loss=7.77162
Epoch 10/80: current_loss=8.33330 | best_loss=7.77162
Epoch 11/80: current_loss=7.98424 | best_loss=7.77162
Epoch 12/80: current_loss=8.47293 | best_loss=7.77162
Epoch 13/80: current_loss=7.78729 | best_loss=7.77162
Epoch 14/80: current_loss=8.62965 | best_loss=7.77162
Epoch 15/80: current_loss=8.43805 | best_loss=7.77162
Epoch 16/80: current_loss=7.95048 | best_loss=7.77162
Epoch 17/80: current_loss=7.97793 | best_loss=7.77162
Epoch 18/80: current_loss=7.83859 | best_loss=7.77162
Epoch 19/80: current_loss=8.15652 | best_loss=7.77162
Epoch 20/80: current_loss=8.55788 | best_loss=7.77162
Epoch 21/80: current_loss=8.54992 | best_loss=7.77162
Early Stopping at epoch 21
      explained_var=-0.00010 | mse_loss=7.59086
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.50582 | best_loss=8.50582
Epoch 1/80: current_loss=8.59723 | best_loss=8.50582
Epoch 2/80: current_loss=8.31753 | best_loss=8.31753
Epoch 3/80: current_loss=8.48890 | best_loss=8.31753
Epoch 4/80: current_loss=8.40675 | best_loss=8.31753
Epoch 5/80: current_loss=8.69569 | best_loss=8.31753
Epoch 6/80: current_loss=8.36048 | best_loss=8.31753
Epoch 7/80: current_loss=8.50205 | best_loss=8.31753
Epoch 8/80: current_loss=8.58065 | best_loss=8.31753
Epoch 9/80: current_loss=8.43894 | best_loss=8.31753
Epoch 10/80: current_loss=8.33069 | best_loss=8.31753
Epoch 11/80: current_loss=8.85321 | best_loss=8.31753
Epoch 12/80: current_loss=8.34571 | best_loss=8.31753
Epoch 13/80: current_loss=8.36157 | best_loss=8.31753
Epoch 14/80: current_loss=8.38898 | best_loss=8.31753
Epoch 15/80: current_loss=8.69999 | best_loss=8.31753
Epoch 16/80: current_loss=8.58976 | best_loss=8.31753
Epoch 17/80: current_loss=8.43882 | best_loss=8.31753
Epoch 18/80: current_loss=9.09325 | best_loss=8.31753
Epoch 19/80: current_loss=8.70061 | best_loss=8.31753
Epoch 20/80: current_loss=8.30781 | best_loss=8.30781
Epoch 21/80: current_loss=8.35428 | best_loss=8.30781
Epoch 22/80: current_loss=8.31179 | best_loss=8.30781
Epoch 23/80: current_loss=8.31903 | best_loss=8.30781
Epoch 24/80: current_loss=8.34033 | best_loss=8.30781
Epoch 25/80: current_loss=8.87091 | best_loss=8.30781
Epoch 26/80: current_loss=8.49516 | best_loss=8.30781
Epoch 27/80: current_loss=8.33476 | best_loss=8.30781
Epoch 28/80: current_loss=8.32484 | best_loss=8.30781
Epoch 29/80: current_loss=8.31635 | best_loss=8.30781
Epoch 30/80: current_loss=8.48523 | best_loss=8.30781
Epoch 31/80: current_loss=9.20337 | best_loss=8.30781
Epoch 32/80: current_loss=10.67946 | best_loss=8.30781
Epoch 33/80: current_loss=8.66387 | best_loss=8.30781
Epoch 34/80: current_loss=8.31065 | best_loss=8.30781
Epoch 35/80: current_loss=8.37705 | best_loss=8.30781
Epoch 36/80: current_loss=8.55771 | best_loss=8.30781
Epoch 37/80: current_loss=8.35297 | best_loss=8.30781
Epoch 38/80: current_loss=8.51291 | best_loss=8.30781
Epoch 39/80: current_loss=8.33862 | best_loss=8.30781
Epoch 40/80: current_loss=9.29988 | best_loss=8.30781
Early Stopping at epoch 40
      explained_var=0.00066 | mse_loss=8.15829
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.21786 | best_loss=9.21786
Epoch 1/80: current_loss=9.06657 | best_loss=9.06657
Epoch 2/80: current_loss=9.28174 | best_loss=9.06657
Epoch 3/80: current_loss=9.20412 | best_loss=9.06657
Epoch 4/80: current_loss=9.31609 | best_loss=9.06657
Epoch 5/80: current_loss=10.28061 | best_loss=9.06657
Epoch 6/80: current_loss=8.84462 | best_loss=8.84462
Epoch 7/80: current_loss=8.74136 | best_loss=8.74136
Epoch 8/80: current_loss=9.80880 | best_loss=8.74136
Epoch 9/80: current_loss=8.71439 | best_loss=8.71439
Epoch 10/80: current_loss=9.14309 | best_loss=8.71439
Epoch 11/80: current_loss=8.72626 | best_loss=8.71439
Epoch 12/80: current_loss=9.43044 | best_loss=8.71439
Epoch 13/80: current_loss=9.11818 | best_loss=8.71439
Epoch 14/80: current_loss=8.90471 | best_loss=8.71439
Epoch 15/80: current_loss=8.94325 | best_loss=8.71439
Epoch 16/80: current_loss=9.92279 | best_loss=8.71439
Epoch 17/80: current_loss=8.84132 | best_loss=8.71439
Epoch 18/80: current_loss=9.06037 | best_loss=8.71439
Epoch 19/80: current_loss=9.29642 | best_loss=8.71439
Epoch 20/80: current_loss=9.07191 | best_loss=8.71439
Epoch 21/80: current_loss=9.55583 | best_loss=8.71439
Epoch 22/80: current_loss=9.14361 | best_loss=8.71439
Epoch 23/80: current_loss=9.10014 | best_loss=8.71439
Epoch 24/80: current_loss=8.89610 | best_loss=8.71439
Epoch 25/80: current_loss=9.16360 | best_loss=8.71439
Epoch 26/80: current_loss=9.75154 | best_loss=8.71439
Epoch 27/80: current_loss=9.24025 | best_loss=8.71439
Epoch 28/80: current_loss=8.83804 | best_loss=8.71439
Epoch 29/80: current_loss=8.77390 | best_loss=8.71439
Early Stopping at epoch 29
      explained_var=0.00320 | mse_loss=8.48161
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.42549 | best_loss=8.42549
Epoch 1/80: current_loss=8.23870 | best_loss=8.23870
Epoch 2/80: current_loss=8.20758 | best_loss=8.20758
Epoch 3/80: current_loss=8.42840 | best_loss=8.20758
Epoch 4/80: current_loss=8.25078 | best_loss=8.20758
Epoch 5/80: current_loss=8.25022 | best_loss=8.20758
Epoch 6/80: current_loss=8.30619 | best_loss=8.20758
Epoch 7/80: current_loss=9.44488 | best_loss=8.20758
Epoch 8/80: current_loss=8.35380 | best_loss=8.20758
Epoch 9/80: current_loss=9.76058 | best_loss=8.20758
Epoch 10/80: current_loss=8.26441 | best_loss=8.20758
Epoch 11/80: current_loss=8.54614 | best_loss=8.20758
Epoch 12/80: current_loss=8.31297 | best_loss=8.20758
Epoch 13/80: current_loss=8.24790 | best_loss=8.20758
Epoch 14/80: current_loss=8.26312 | best_loss=8.20758
Epoch 15/80: current_loss=8.47886 | best_loss=8.20758
Epoch 16/80: current_loss=8.82274 | best_loss=8.20758
Epoch 17/80: current_loss=9.01466 | best_loss=8.20758
Epoch 18/80: current_loss=8.34763 | best_loss=8.20758
Epoch 19/80: current_loss=8.73743 | best_loss=8.20758
Epoch 20/80: current_loss=8.40344 | best_loss=8.20758
Epoch 21/80: current_loss=8.22212 | best_loss=8.20758
Epoch 22/80: current_loss=8.25991 | best_loss=8.20758
Early Stopping at epoch 22
      explained_var=0.00286 | mse_loss=8.30879
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.84405 | best_loss=7.84405
Epoch 1/80: current_loss=8.03108 | best_loss=7.84405
Epoch 2/80: current_loss=8.53241 | best_loss=7.84405
Epoch 3/80: current_loss=8.06665 | best_loss=7.84405
Epoch 4/80: current_loss=7.79296 | best_loss=7.79296
Epoch 5/80: current_loss=7.76019 | best_loss=7.76019
Epoch 6/80: current_loss=8.10052 | best_loss=7.76019
Epoch 7/80: current_loss=7.67631 | best_loss=7.67631
Epoch 8/80: current_loss=8.25229 | best_loss=7.67631
Epoch 9/80: current_loss=7.68437 | best_loss=7.67631
Epoch 10/80: current_loss=8.57529 | best_loss=7.67631
Epoch 11/80: current_loss=7.84802 | best_loss=7.67631
Epoch 12/80: current_loss=7.70724 | best_loss=7.67631
Epoch 13/80: current_loss=7.81550 | best_loss=7.67631
Epoch 14/80: current_loss=8.26143 | best_loss=7.67631
Epoch 15/80: current_loss=7.69338 | best_loss=7.67631
Epoch 16/80: current_loss=7.82825 | best_loss=7.67631
Epoch 17/80: current_loss=7.80514 | best_loss=7.67631
Epoch 18/80: current_loss=8.05009 | best_loss=7.67631
Epoch 19/80: current_loss=7.96209 | best_loss=7.67631
Epoch 20/80: current_loss=7.78106 | best_loss=7.67631
Epoch 21/80: current_loss=9.76272 | best_loss=7.67631
Epoch 22/80: current_loss=7.73899 | best_loss=7.67631
Epoch 23/80: current_loss=7.69405 | best_loss=7.67631
Epoch 24/80: current_loss=7.71463 | best_loss=7.67631
Epoch 25/80: current_loss=8.69835 | best_loss=7.67631
Epoch 26/80: current_loss=7.67868 | best_loss=7.67631
Epoch 27/80: current_loss=7.72128 | best_loss=7.67631
Early Stopping at epoch 27
      explained_var=0.00072 | mse_loss=7.85858
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=0.00147| avg_loss=8.07963
----------------------------------------------


----------------------------------------------
Params for Trial 20
{'learning_rate': 0.1, 'weight_decay': 0.003494494880264359, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=24.00253 | best_loss=24.00253
Epoch 1/80: current_loss=13.03006 | best_loss=13.03006
Epoch 2/80: current_loss=9.40753 | best_loss=9.40753
Epoch 3/80: current_loss=7.84671 | best_loss=7.84671
Epoch 4/80: current_loss=9.32272 | best_loss=7.84671
Epoch 5/80: current_loss=8.19144 | best_loss=7.84671
Epoch 6/80: current_loss=8.88086 | best_loss=7.84671
Epoch 7/80: current_loss=8.84353 | best_loss=7.84671
Epoch 8/80: current_loss=10.04360 | best_loss=7.84671
Epoch 9/80: current_loss=30.33459 | best_loss=7.84671
Epoch 10/80: current_loss=12.90622 | best_loss=7.84671
Epoch 11/80: current_loss=9.82652 | best_loss=7.84671
Epoch 12/80: current_loss=9.56783 | best_loss=7.84671
Epoch 13/80: current_loss=8.10329 | best_loss=7.84671
Epoch 14/80: current_loss=7.98576 | best_loss=7.84671
Epoch 15/80: current_loss=9.66131 | best_loss=7.84671
Epoch 16/80: current_loss=8.31372 | best_loss=7.84671
Epoch 17/80: current_loss=9.29111 | best_loss=7.84671
Epoch 18/80: current_loss=7.87460 | best_loss=7.84671
Epoch 19/80: current_loss=9.33702 | best_loss=7.84671
Epoch 20/80: current_loss=12.06691 | best_loss=7.84671
Epoch 21/80: current_loss=7.87600 | best_loss=7.84671
Epoch 22/80: current_loss=9.67372 | best_loss=7.84671
Epoch 23/80: current_loss=11.75628 | best_loss=7.84671
Early Stopping at epoch 23
      explained_var=-0.00428 | mse_loss=7.68484
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.04944 | best_loss=9.04944
Epoch 1/80: current_loss=8.35597 | best_loss=8.35597
Epoch 2/80: current_loss=10.48354 | best_loss=8.35597
Epoch 3/80: current_loss=8.67389 | best_loss=8.35597
Epoch 4/80: current_loss=8.70711 | best_loss=8.35597
Epoch 5/80: current_loss=10.42513 | best_loss=8.35597
Epoch 6/80: current_loss=8.28078 | best_loss=8.28078
Epoch 7/80: current_loss=12.11341 | best_loss=8.28078
Epoch 8/80: current_loss=10.23926 | best_loss=8.28078
Epoch 9/80: current_loss=8.93677 | best_loss=8.28078
Epoch 10/80: current_loss=8.58208 | best_loss=8.28078
Epoch 11/80: current_loss=8.43753 | best_loss=8.28078
Epoch 12/80: current_loss=8.38382 | best_loss=8.28078
Epoch 13/80: current_loss=10.21484 | best_loss=8.28078
Epoch 14/80: current_loss=9.86196 | best_loss=8.28078
Epoch 15/80: current_loss=9.16511 | best_loss=8.28078
Epoch 16/80: current_loss=9.06124 | best_loss=8.28078
Epoch 17/80: current_loss=16.74269 | best_loss=8.28078
Epoch 18/80: current_loss=8.49109 | best_loss=8.28078
Epoch 19/80: current_loss=9.87399 | best_loss=8.28078
Epoch 20/80: current_loss=13.09842 | best_loss=8.28078
Epoch 21/80: current_loss=8.45285 | best_loss=8.28078
Epoch 22/80: current_loss=8.84509 | best_loss=8.28078
Epoch 23/80: current_loss=11.72415 | best_loss=8.28078
Epoch 24/80: current_loss=8.72580 | best_loss=8.28078
Epoch 25/80: current_loss=8.63794 | best_loss=8.28078
Epoch 26/80: current_loss=10.53599 | best_loss=8.28078
Early Stopping at epoch 26
      explained_var=0.00671 | mse_loss=8.11271
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=20.43101 | best_loss=20.43101
Epoch 1/80: current_loss=8.38261 | best_loss=8.38261
Epoch 2/80: current_loss=9.69374 | best_loss=8.38261
Epoch 3/80: current_loss=12.39362 | best_loss=8.38261
Epoch 4/80: current_loss=9.52962 | best_loss=8.38261
Epoch 5/80: current_loss=14.36380 | best_loss=8.38261
Epoch 6/80: current_loss=8.69037 | best_loss=8.38261
Epoch 7/80: current_loss=29.42221 | best_loss=8.38261
Epoch 8/80: current_loss=31.41697 | best_loss=8.38261
Epoch 9/80: current_loss=12.85128 | best_loss=8.38261
Epoch 10/80: current_loss=9.47154 | best_loss=8.38261
Epoch 11/80: current_loss=9.15414 | best_loss=8.38261
Epoch 12/80: current_loss=11.11294 | best_loss=8.38261
Epoch 13/80: current_loss=8.74674 | best_loss=8.38261
Epoch 14/80: current_loss=8.80820 | best_loss=8.38261
Epoch 15/80: current_loss=10.69502 | best_loss=8.38261
Epoch 16/80: current_loss=11.42106 | best_loss=8.38261
Epoch 17/80: current_loss=10.98490 | best_loss=8.38261
Epoch 18/80: current_loss=8.73704 | best_loss=8.38261
Epoch 19/80: current_loss=9.58450 | best_loss=8.38261
Epoch 20/80: current_loss=9.63779 | best_loss=8.38261
Epoch 21/80: current_loss=9.27747 | best_loss=8.38261
Early Stopping at epoch 21
      explained_var=0.08073 | mse_loss=8.12635
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.06246 | best_loss=9.06246
Epoch 1/80: current_loss=8.59159 | best_loss=8.59159
Epoch 2/80: current_loss=8.79627 | best_loss=8.59159
Epoch 3/80: current_loss=16.37741 | best_loss=8.59159
Epoch 4/80: current_loss=8.56923 | best_loss=8.56923
Epoch 5/80: current_loss=8.62721 | best_loss=8.56923
Epoch 6/80: current_loss=8.16004 | best_loss=8.16004
Epoch 7/80: current_loss=8.29733 | best_loss=8.16004
Epoch 8/80: current_loss=8.63771 | best_loss=8.16004
Epoch 9/80: current_loss=12.36334 | best_loss=8.16004
Epoch 10/80: current_loss=10.92708 | best_loss=8.16004
Epoch 11/80: current_loss=8.30816 | best_loss=8.16004
Epoch 12/80: current_loss=9.16949 | best_loss=8.16004
Epoch 13/80: current_loss=8.24433 | best_loss=8.16004
Epoch 14/80: current_loss=8.82913 | best_loss=8.16004
Epoch 15/80: current_loss=9.97036 | best_loss=8.16004
Epoch 16/80: current_loss=8.24032 | best_loss=8.16004
Epoch 17/80: current_loss=8.31835 | best_loss=8.16004
Epoch 18/80: current_loss=9.24312 | best_loss=8.16004
Epoch 19/80: current_loss=12.39812 | best_loss=8.16004
Epoch 20/80: current_loss=8.28892 | best_loss=8.16004
Epoch 21/80: current_loss=18.09456 | best_loss=8.16004
Epoch 22/80: current_loss=16.76461 | best_loss=8.16004
Epoch 23/80: current_loss=9.63548 | best_loss=8.16004
Epoch 24/80: current_loss=8.49439 | best_loss=8.16004
Epoch 25/80: current_loss=9.02177 | best_loss=8.16004
Epoch 26/80: current_loss=13.71219 | best_loss=8.16004
Early Stopping at epoch 26
      explained_var=0.00810 | mse_loss=8.26447
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.70808 | best_loss=7.70808
Epoch 1/80: current_loss=7.94886 | best_loss=7.70808
Epoch 2/80: current_loss=9.07106 | best_loss=7.70808
Epoch 3/80: current_loss=8.04100 | best_loss=7.70808
Epoch 4/80: current_loss=11.89981 | best_loss=7.70808
Epoch 5/80: current_loss=7.74095 | best_loss=7.70808
Epoch 6/80: current_loss=14.39174 | best_loss=7.70808
Epoch 7/80: current_loss=8.45159 | best_loss=7.70808
Epoch 8/80: current_loss=7.97547 | best_loss=7.70808
Epoch 9/80: current_loss=21.55547 | best_loss=7.70808
Epoch 10/80: current_loss=13.90579 | best_loss=7.70808
Epoch 11/80: current_loss=7.68425 | best_loss=7.68425
Epoch 12/80: current_loss=7.71948 | best_loss=7.68425
Epoch 13/80: current_loss=8.73872 | best_loss=7.68425
Epoch 14/80: current_loss=8.48596 | best_loss=7.68425
Epoch 15/80: current_loss=7.85829 | best_loss=7.68425
Epoch 16/80: current_loss=9.41299 | best_loss=7.68425
Epoch 17/80: current_loss=11.90518 | best_loss=7.68425
Epoch 18/80: current_loss=7.74135 | best_loss=7.68425
Epoch 19/80: current_loss=12.07385 | best_loss=7.68425
Epoch 20/80: current_loss=8.53922 | best_loss=7.68425
Epoch 21/80: current_loss=8.00240 | best_loss=7.68425
Epoch 22/80: current_loss=10.74159 | best_loss=7.68425
Epoch 23/80: current_loss=9.17652 | best_loss=7.68425
Epoch 24/80: current_loss=7.73899 | best_loss=7.68425
Epoch 25/80: current_loss=8.34402 | best_loss=7.68425
Epoch 26/80: current_loss=8.71867 | best_loss=7.68425
Epoch 27/80: current_loss=7.99791 | best_loss=7.68425
Epoch 28/80: current_loss=8.61777 | best_loss=7.68425
Epoch 29/80: current_loss=12.17550 | best_loss=7.68425
Epoch 30/80: current_loss=8.83543 | best_loss=7.68425
Epoch 31/80: current_loss=10.10591 | best_loss=7.68425
Early Stopping at epoch 31
      explained_var=-0.00145 | mse_loss=7.87123
----------------------------------------------
Average early_stopping_point: 5| avg_exp_var=0.01796| avg_loss=8.01192
----------------------------------------------


----------------------------------------------
Params for Trial 21
{'learning_rate': 0.1, 'weight_decay': 0.0033061301698202017, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.48034 | best_loss=11.48034
Epoch 1/80: current_loss=7.93938 | best_loss=7.93938
Epoch 2/80: current_loss=9.11531 | best_loss=7.93938
Epoch 3/80: current_loss=8.35390 | best_loss=7.93938
Epoch 4/80: current_loss=13.18745 | best_loss=7.93938
Epoch 5/80: current_loss=9.29600 | best_loss=7.93938
Epoch 6/80: current_loss=10.89958 | best_loss=7.93938
Epoch 7/80: current_loss=12.55048 | best_loss=7.93938
Epoch 8/80: current_loss=9.60887 | best_loss=7.93938
Epoch 9/80: current_loss=10.56955 | best_loss=7.93938
Epoch 10/80: current_loss=7.75936 | best_loss=7.75936
Epoch 11/80: current_loss=7.76339 | best_loss=7.75936
Epoch 12/80: current_loss=8.67150 | best_loss=7.75936
Epoch 13/80: current_loss=7.82645 | best_loss=7.75936
Epoch 14/80: current_loss=7.78271 | best_loss=7.75936
Epoch 15/80: current_loss=9.94048 | best_loss=7.75936
Epoch 16/80: current_loss=8.05334 | best_loss=7.75936
Epoch 17/80: current_loss=8.93274 | best_loss=7.75936
Epoch 18/80: current_loss=7.84641 | best_loss=7.75936
Epoch 19/80: current_loss=7.82958 | best_loss=7.75936
Epoch 20/80: current_loss=16.26770 | best_loss=7.75936
Epoch 21/80: current_loss=7.79599 | best_loss=7.75936
Epoch 22/80: current_loss=9.52464 | best_loss=7.75936
Epoch 23/80: current_loss=7.79129 | best_loss=7.75936
Epoch 24/80: current_loss=7.76774 | best_loss=7.75936
Epoch 25/80: current_loss=8.26225 | best_loss=7.75936
Epoch 26/80: current_loss=9.10090 | best_loss=7.75936
Epoch 27/80: current_loss=7.91065 | best_loss=7.75936
Epoch 28/80: current_loss=10.08626 | best_loss=7.75936
Epoch 29/80: current_loss=11.38605 | best_loss=7.75936
Epoch 30/80: current_loss=9.87338 | best_loss=7.75936
Early Stopping at epoch 30
      explained_var=0.00417 | mse_loss=7.59425
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.44751 | best_loss=8.44751
Epoch 1/80: current_loss=8.59268 | best_loss=8.44751
Epoch 2/80: current_loss=8.34433 | best_loss=8.34433
Epoch 3/80: current_loss=8.90154 | best_loss=8.34433
Epoch 4/80: current_loss=8.83439 | best_loss=8.34433
Epoch 5/80: current_loss=10.21590 | best_loss=8.34433
Epoch 6/80: current_loss=9.53763 | best_loss=8.34433
Epoch 7/80: current_loss=8.60291 | best_loss=8.34433
Epoch 8/80: current_loss=14.60321 | best_loss=8.34433
Epoch 9/80: current_loss=13.46794 | best_loss=8.34433
Epoch 10/80: current_loss=8.50841 | best_loss=8.34433
Epoch 11/80: current_loss=12.84976 | best_loss=8.34433
Epoch 12/80: current_loss=13.88984 | best_loss=8.34433
Epoch 13/80: current_loss=11.35848 | best_loss=8.34433
Epoch 14/80: current_loss=10.76006 | best_loss=8.34433
Epoch 15/80: current_loss=12.81487 | best_loss=8.34433
Epoch 16/80: current_loss=9.23384 | best_loss=8.34433
Epoch 17/80: current_loss=19.50327 | best_loss=8.34433
Epoch 18/80: current_loss=10.33464 | best_loss=8.34433
Epoch 19/80: current_loss=8.92856 | best_loss=8.34433
Epoch 20/80: current_loss=8.19017 | best_loss=8.19017
Epoch 21/80: current_loss=9.89904 | best_loss=8.19017
Epoch 22/80: current_loss=8.34368 | best_loss=8.19017
Epoch 23/80: current_loss=11.06912 | best_loss=8.19017
Epoch 24/80: current_loss=18.09262 | best_loss=8.19017
Epoch 25/80: current_loss=8.40883 | best_loss=8.19017
Epoch 26/80: current_loss=10.30375 | best_loss=8.19017
Epoch 27/80: current_loss=8.67804 | best_loss=8.19017
Epoch 28/80: current_loss=8.61496 | best_loss=8.19017
Epoch 29/80: current_loss=8.48568 | best_loss=8.19017
Epoch 30/80: current_loss=8.48312 | best_loss=8.19017
Epoch 31/80: current_loss=8.35593 | best_loss=8.19017
Epoch 32/80: current_loss=8.81889 | best_loss=8.19017
Epoch 33/80: current_loss=8.30374 | best_loss=8.19017
Epoch 34/80: current_loss=8.49042 | best_loss=8.19017
Epoch 35/80: current_loss=8.41462 | best_loss=8.19017
Epoch 36/80: current_loss=10.26149 | best_loss=8.19017
Epoch 37/80: current_loss=8.31311 | best_loss=8.19017
Epoch 38/80: current_loss=9.46472 | best_loss=8.19017
Epoch 39/80: current_loss=9.34479 | best_loss=8.19017
Epoch 40/80: current_loss=8.35861 | best_loss=8.19017
Early Stopping at epoch 40
      explained_var=0.01840 | mse_loss=8.05753
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.88711 | best_loss=8.88711
Epoch 1/80: current_loss=11.63640 | best_loss=8.88711
Epoch 2/80: current_loss=9.08858 | best_loss=8.88711
Epoch 3/80: current_loss=10.82407 | best_loss=8.88711
Epoch 4/80: current_loss=9.95705 | best_loss=8.88711
Epoch 5/80: current_loss=14.87257 | best_loss=8.88711
Epoch 6/80: current_loss=16.92318 | best_loss=8.88711
Epoch 7/80: current_loss=9.28049 | best_loss=8.88711
Epoch 8/80: current_loss=9.52791 | best_loss=8.88711
Epoch 9/80: current_loss=12.56565 | best_loss=8.88711
Epoch 10/80: current_loss=10.83421 | best_loss=8.88711
Epoch 11/80: current_loss=9.73818 | best_loss=8.88711
Epoch 12/80: current_loss=8.83645 | best_loss=8.83645
Epoch 13/80: current_loss=8.41794 | best_loss=8.41794
Epoch 14/80: current_loss=12.17913 | best_loss=8.41794
Epoch 15/80: current_loss=12.96112 | best_loss=8.41794
Epoch 16/80: current_loss=8.91542 | best_loss=8.41794
Epoch 17/80: current_loss=12.25424 | best_loss=8.41794
Epoch 18/80: current_loss=8.47651 | best_loss=8.41794
Epoch 19/80: current_loss=11.64276 | best_loss=8.41794
Epoch 20/80: current_loss=12.54709 | best_loss=8.41794
Epoch 21/80: current_loss=8.93577 | best_loss=8.41794
Epoch 22/80: current_loss=10.48971 | best_loss=8.41794
Epoch 23/80: current_loss=9.16028 | best_loss=8.41794
Epoch 24/80: current_loss=12.48294 | best_loss=8.41794
Epoch 25/80: current_loss=9.59700 | best_loss=8.41794
Epoch 26/80: current_loss=8.91848 | best_loss=8.41794
Epoch 27/80: current_loss=9.06737 | best_loss=8.41794
Epoch 28/80: current_loss=12.24203 | best_loss=8.41794
Epoch 29/80: current_loss=25.85771 | best_loss=8.41794
Epoch 30/80: current_loss=15.82105 | best_loss=8.41794
Epoch 31/80: current_loss=13.46548 | best_loss=8.41794
Epoch 32/80: current_loss=9.20142 | best_loss=8.41794
Epoch 33/80: current_loss=16.90061 | best_loss=8.41794
Early Stopping at epoch 33
      explained_var=0.03423 | mse_loss=8.19940
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.53070 | best_loss=8.53070
Epoch 1/80: current_loss=7.89498 | best_loss=7.89498
Epoch 2/80: current_loss=7.85403 | best_loss=7.85403
Epoch 3/80: current_loss=8.81110 | best_loss=7.85403
Epoch 4/80: current_loss=9.06877 | best_loss=7.85403
Epoch 5/80: current_loss=8.75945 | best_loss=7.85403
Epoch 6/80: current_loss=9.31490 | best_loss=7.85403
Epoch 7/80: current_loss=9.96394 | best_loss=7.85403
Epoch 8/80: current_loss=10.74432 | best_loss=7.85403
Epoch 9/80: current_loss=10.46535 | best_loss=7.85403
Epoch 10/80: current_loss=9.19702 | best_loss=7.85403
Epoch 11/80: current_loss=8.39354 | best_loss=7.85403
Epoch 12/80: current_loss=8.18210 | best_loss=7.85403
Epoch 13/80: current_loss=9.60513 | best_loss=7.85403
Epoch 14/80: current_loss=11.02034 | best_loss=7.85403
Epoch 15/80: current_loss=8.62289 | best_loss=7.85403
Epoch 16/80: current_loss=9.89150 | best_loss=7.85403
Epoch 17/80: current_loss=10.26187 | best_loss=7.85403
Epoch 18/80: current_loss=12.76067 | best_loss=7.85403
Epoch 19/80: current_loss=9.06566 | best_loss=7.85403
Epoch 20/80: current_loss=8.73315 | best_loss=7.85403
Epoch 21/80: current_loss=9.02054 | best_loss=7.85403
Epoch 22/80: current_loss=9.47732 | best_loss=7.85403
Early Stopping at epoch 22
      explained_var=0.03938 | mse_loss=8.00683
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.97502 | best_loss=10.97502
Epoch 1/80: current_loss=12.42324 | best_loss=10.97502
Epoch 2/80: current_loss=8.90344 | best_loss=8.90344
Epoch 3/80: current_loss=10.12605 | best_loss=8.90344
Epoch 4/80: current_loss=9.25259 | best_loss=8.90344
Epoch 5/80: current_loss=8.76984 | best_loss=8.76984
Epoch 6/80: current_loss=11.10822 | best_loss=8.76984
Epoch 7/80: current_loss=8.35174 | best_loss=8.35174
Epoch 8/80: current_loss=8.59790 | best_loss=8.35174
Epoch 9/80: current_loss=7.86380 | best_loss=7.86380
Epoch 10/80: current_loss=7.49317 | best_loss=7.49317
Epoch 11/80: current_loss=11.51868 | best_loss=7.49317
Epoch 12/80: current_loss=8.08073 | best_loss=7.49317
Epoch 13/80: current_loss=10.24857 | best_loss=7.49317
Epoch 14/80: current_loss=9.49335 | best_loss=7.49317
Epoch 15/80: current_loss=12.12269 | best_loss=7.49317
Epoch 16/80: current_loss=8.15892 | best_loss=7.49317
Epoch 17/80: current_loss=8.53328 | best_loss=7.49317
Epoch 18/80: current_loss=10.32561 | best_loss=7.49317
Epoch 19/80: current_loss=8.93207 | best_loss=7.49317
Epoch 20/80: current_loss=15.26082 | best_loss=7.49317
Epoch 21/80: current_loss=8.06262 | best_loss=7.49317
Epoch 22/80: current_loss=10.44452 | best_loss=7.49317
Epoch 23/80: current_loss=10.84886 | best_loss=7.49317
Epoch 24/80: current_loss=12.09280 | best_loss=7.49317
Epoch 25/80: current_loss=7.58338 | best_loss=7.49317
Epoch 26/80: current_loss=14.36011 | best_loss=7.49317
Epoch 27/80: current_loss=11.82678 | best_loss=7.49317
Epoch 28/80: current_loss=9.70760 | best_loss=7.49317
Epoch 29/80: current_loss=11.13952 | best_loss=7.49317
Epoch 30/80: current_loss=7.91667 | best_loss=7.49317
Early Stopping at epoch 30
      explained_var=0.03256 | mse_loss=7.62075
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.02575| avg_loss=7.89575
----------------------------------------------


----------------------------------------------
Params for Trial 22
{'learning_rate': 0.1, 'weight_decay': 0.0034099644136067657, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.53404 | best_loss=8.53404
Epoch 1/80: current_loss=11.92685 | best_loss=8.53404
Epoch 2/80: current_loss=8.12137 | best_loss=8.12137
Epoch 3/80: current_loss=10.95096 | best_loss=8.12137
Epoch 4/80: current_loss=9.60256 | best_loss=8.12137
Epoch 5/80: current_loss=12.15426 | best_loss=8.12137
Epoch 6/80: current_loss=15.13868 | best_loss=8.12137
Epoch 7/80: current_loss=7.92933 | best_loss=7.92933
Epoch 8/80: current_loss=8.36247 | best_loss=7.92933
Epoch 9/80: current_loss=21.01364 | best_loss=7.92933
Epoch 10/80: current_loss=9.62559 | best_loss=7.92933
Epoch 11/80: current_loss=8.11872 | best_loss=7.92933
Epoch 12/80: current_loss=9.43593 | best_loss=7.92933
Epoch 13/80: current_loss=9.25974 | best_loss=7.92933
Epoch 14/80: current_loss=8.41285 | best_loss=7.92933
Epoch 15/80: current_loss=9.19832 | best_loss=7.92933
Epoch 16/80: current_loss=11.34532 | best_loss=7.92933
Epoch 17/80: current_loss=7.96435 | best_loss=7.92933
Epoch 18/80: current_loss=8.24662 | best_loss=7.92933
Epoch 19/80: current_loss=8.81618 | best_loss=7.92933
Epoch 20/80: current_loss=8.60751 | best_loss=7.92933
Epoch 21/80: current_loss=12.79252 | best_loss=7.92933
Epoch 22/80: current_loss=9.82989 | best_loss=7.92933
Epoch 23/80: current_loss=8.17400 | best_loss=7.92933
Epoch 24/80: current_loss=7.78857 | best_loss=7.78857
Epoch 25/80: current_loss=9.96444 | best_loss=7.78857
Epoch 26/80: current_loss=18.50117 | best_loss=7.78857
Epoch 27/80: current_loss=10.12996 | best_loss=7.78857
Epoch 28/80: current_loss=13.65367 | best_loss=7.78857
Epoch 29/80: current_loss=21.75580 | best_loss=7.78857
Epoch 30/80: current_loss=20.25594 | best_loss=7.78857
Epoch 31/80: current_loss=13.25899 | best_loss=7.78857
Epoch 32/80: current_loss=25.14927 | best_loss=7.78857
Epoch 33/80: current_loss=18.40057 | best_loss=7.78857
Epoch 34/80: current_loss=8.08284 | best_loss=7.78857
Epoch 35/80: current_loss=9.29470 | best_loss=7.78857
Epoch 36/80: current_loss=8.22366 | best_loss=7.78857
Epoch 37/80: current_loss=9.04324 | best_loss=7.78857
Epoch 38/80: current_loss=10.34620 | best_loss=7.78857
Epoch 39/80: current_loss=9.56518 | best_loss=7.78857
Epoch 40/80: current_loss=7.76292 | best_loss=7.76292
Epoch 41/80: current_loss=7.84781 | best_loss=7.76292
Epoch 42/80: current_loss=8.58017 | best_loss=7.76292
Epoch 43/80: current_loss=8.23682 | best_loss=7.76292
Epoch 44/80: current_loss=7.98359 | best_loss=7.76292
Epoch 45/80: current_loss=11.21404 | best_loss=7.76292
Epoch 46/80: current_loss=8.00317 | best_loss=7.76292
Epoch 47/80: current_loss=9.62232 | best_loss=7.76292
Epoch 48/80: current_loss=7.98864 | best_loss=7.76292
Epoch 49/80: current_loss=16.27117 | best_loss=7.76292
Epoch 50/80: current_loss=7.92254 | best_loss=7.76292
Epoch 51/80: current_loss=8.27478 | best_loss=7.76292
Epoch 52/80: current_loss=8.41943 | best_loss=7.76292
Epoch 53/80: current_loss=8.87487 | best_loss=7.76292
Epoch 54/80: current_loss=7.83929 | best_loss=7.76292
Epoch 55/80: current_loss=7.89194 | best_loss=7.76292
Epoch 56/80: current_loss=13.15981 | best_loss=7.76292
Epoch 57/80: current_loss=7.84652 | best_loss=7.76292
Epoch 58/80: current_loss=9.72455 | best_loss=7.76292
Epoch 59/80: current_loss=8.02819 | best_loss=7.76292
Epoch 60/80: current_loss=11.00730 | best_loss=7.76292
Early Stopping at epoch 60
      explained_var=0.00158 | mse_loss=7.57447
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.24189 | best_loss=12.24189
Epoch 1/80: current_loss=13.64007 | best_loss=12.24189
Epoch 2/80: current_loss=9.78277 | best_loss=9.78277
Epoch 3/80: current_loss=12.03488 | best_loss=9.78277
Epoch 4/80: current_loss=10.55781 | best_loss=9.78277
Epoch 5/80: current_loss=36.68994 | best_loss=9.78277
Epoch 6/80: current_loss=9.88654 | best_loss=9.78277
Epoch 7/80: current_loss=60.11465 | best_loss=9.78277
Epoch 8/80: current_loss=16.98953 | best_loss=9.78277
Epoch 9/80: current_loss=10.56938 | best_loss=9.78277
Epoch 10/80: current_loss=9.46357 | best_loss=9.46357
Epoch 11/80: current_loss=9.74509 | best_loss=9.46357
Epoch 12/80: current_loss=8.69634 | best_loss=8.69634
Epoch 13/80: current_loss=9.12890 | best_loss=8.69634
Epoch 14/80: current_loss=9.00001 | best_loss=8.69634
Epoch 15/80: current_loss=10.24577 | best_loss=8.69634
Epoch 16/80: current_loss=9.23997 | best_loss=8.69634
Epoch 17/80: current_loss=8.90058 | best_loss=8.69634
Epoch 18/80: current_loss=16.77147 | best_loss=8.69634
Epoch 19/80: current_loss=17.68991 | best_loss=8.69634
Epoch 20/80: current_loss=29.62484 | best_loss=8.69634
Epoch 21/80: current_loss=8.65938 | best_loss=8.65938
Epoch 22/80: current_loss=12.89456 | best_loss=8.65938
Epoch 23/80: current_loss=18.04275 | best_loss=8.65938
Epoch 24/80: current_loss=8.74224 | best_loss=8.65938
Epoch 25/80: current_loss=11.21583 | best_loss=8.65938
Epoch 26/80: current_loss=8.45751 | best_loss=8.45751
Epoch 27/80: current_loss=9.69133 | best_loss=8.45751
Epoch 28/80: current_loss=8.52727 | best_loss=8.45751
Epoch 29/80: current_loss=9.18646 | best_loss=8.45751
Epoch 30/80: current_loss=8.31090 | best_loss=8.31090
Epoch 31/80: current_loss=8.73352 | best_loss=8.31090
Epoch 32/80: current_loss=8.26294 | best_loss=8.26294
Epoch 33/80: current_loss=9.06281 | best_loss=8.26294
Epoch 34/80: current_loss=8.45377 | best_loss=8.26294
Epoch 35/80: current_loss=7.95276 | best_loss=7.95276
Epoch 36/80: current_loss=8.66866 | best_loss=7.95276
Epoch 37/80: current_loss=9.45733 | best_loss=7.95276
Epoch 38/80: current_loss=7.87258 | best_loss=7.87258
Epoch 39/80: current_loss=9.94443 | best_loss=7.87258
Epoch 40/80: current_loss=10.61419 | best_loss=7.87258
Epoch 41/80: current_loss=9.22802 | best_loss=7.87258
Epoch 42/80: current_loss=9.85152 | best_loss=7.87258
Epoch 43/80: current_loss=12.24979 | best_loss=7.87258
Epoch 44/80: current_loss=9.18387 | best_loss=7.87258
Epoch 45/80: current_loss=10.06745 | best_loss=7.87258
Epoch 46/80: current_loss=8.10972 | best_loss=7.87258
Epoch 47/80: current_loss=8.54683 | best_loss=7.87258
Epoch 48/80: current_loss=10.12447 | best_loss=7.87258
Epoch 49/80: current_loss=8.34551 | best_loss=7.87258
Epoch 50/80: current_loss=8.93761 | best_loss=7.87258
Epoch 51/80: current_loss=9.44109 | best_loss=7.87258
Epoch 52/80: current_loss=10.32160 | best_loss=7.87258
Epoch 53/80: current_loss=8.58774 | best_loss=7.87258
Epoch 54/80: current_loss=9.07326 | best_loss=7.87258
Epoch 55/80: current_loss=20.22369 | best_loss=7.87258
Epoch 56/80: current_loss=12.40882 | best_loss=7.87258
Epoch 57/80: current_loss=9.40962 | best_loss=7.87258
Epoch 58/80: current_loss=10.60535 | best_loss=7.87258
Early Stopping at epoch 58
      explained_var=0.04440 | mse_loss=7.80102
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.11101 | best_loss=12.11101
Epoch 1/80: current_loss=8.90804 | best_loss=8.90804
Epoch 2/80: current_loss=9.68762 | best_loss=8.90804
Epoch 3/80: current_loss=10.48126 | best_loss=8.90804
Epoch 4/80: current_loss=9.42037 | best_loss=8.90804
Epoch 5/80: current_loss=15.65458 | best_loss=8.90804
Epoch 6/80: current_loss=9.49456 | best_loss=8.90804
Epoch 7/80: current_loss=11.62835 | best_loss=8.90804
Epoch 8/80: current_loss=8.77674 | best_loss=8.77674
Epoch 9/80: current_loss=11.34904 | best_loss=8.77674
Epoch 10/80: current_loss=14.43349 | best_loss=8.77674
Epoch 11/80: current_loss=9.78004 | best_loss=8.77674
Epoch 12/80: current_loss=12.12998 | best_loss=8.77674
Epoch 13/80: current_loss=12.50399 | best_loss=8.77674
Epoch 14/80: current_loss=12.69246 | best_loss=8.77674
Epoch 15/80: current_loss=9.83505 | best_loss=8.77674
Epoch 16/80: current_loss=8.88761 | best_loss=8.77674
Epoch 17/80: current_loss=11.20532 | best_loss=8.77674
Epoch 18/80: current_loss=8.73490 | best_loss=8.73490
Epoch 19/80: current_loss=8.87029 | best_loss=8.73490
Epoch 20/80: current_loss=13.72323 | best_loss=8.73490
Epoch 21/80: current_loss=9.54785 | best_loss=8.73490
Epoch 22/80: current_loss=24.42002 | best_loss=8.73490
Epoch 23/80: current_loss=9.33488 | best_loss=8.73490
Epoch 24/80: current_loss=9.90369 | best_loss=8.73490
Epoch 25/80: current_loss=9.22880 | best_loss=8.73490
Epoch 26/80: current_loss=8.86481 | best_loss=8.73490
Epoch 27/80: current_loss=8.62224 | best_loss=8.62224
Epoch 28/80: current_loss=10.19282 | best_loss=8.62224
Epoch 29/80: current_loss=11.66074 | best_loss=8.62224
Epoch 30/80: current_loss=12.67063 | best_loss=8.62224
Epoch 31/80: current_loss=8.69576 | best_loss=8.62224
Epoch 32/80: current_loss=8.61410 | best_loss=8.61410
Epoch 33/80: current_loss=10.19042 | best_loss=8.61410
Epoch 34/80: current_loss=12.36689 | best_loss=8.61410
Epoch 35/80: current_loss=13.34312 | best_loss=8.61410
Epoch 36/80: current_loss=10.11286 | best_loss=8.61410
Epoch 37/80: current_loss=12.20294 | best_loss=8.61410
Epoch 38/80: current_loss=8.92927 | best_loss=8.61410
Epoch 39/80: current_loss=9.89951 | best_loss=8.61410
Epoch 40/80: current_loss=11.69238 | best_loss=8.61410
Epoch 41/80: current_loss=12.10216 | best_loss=8.61410
Epoch 42/80: current_loss=8.96616 | best_loss=8.61410
Epoch 43/80: current_loss=12.52461 | best_loss=8.61410
Epoch 44/80: current_loss=8.72433 | best_loss=8.61410
Epoch 45/80: current_loss=8.94575 | best_loss=8.61410
Epoch 46/80: current_loss=24.17970 | best_loss=8.61410
Epoch 47/80: current_loss=21.61148 | best_loss=8.61410
Epoch 48/80: current_loss=8.96975 | best_loss=8.61410
Epoch 49/80: current_loss=15.03811 | best_loss=8.61410
Epoch 50/80: current_loss=13.85731 | best_loss=8.61410
Epoch 51/80: current_loss=14.67339 | best_loss=8.61410
Epoch 52/80: current_loss=9.01300 | best_loss=8.61410
Early Stopping at epoch 52
      explained_var=0.03998 | mse_loss=8.40360
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.61709 | best_loss=10.61709
Epoch 1/80: current_loss=8.69777 | best_loss=8.69777
Epoch 2/80: current_loss=8.35982 | best_loss=8.35982
Epoch 3/80: current_loss=8.71503 | best_loss=8.35982
Epoch 4/80: current_loss=8.28958 | best_loss=8.28958
Epoch 5/80: current_loss=11.58587 | best_loss=8.28958
Epoch 6/80: current_loss=15.37354 | best_loss=8.28958
Epoch 7/80: current_loss=8.34347 | best_loss=8.28958
Epoch 8/80: current_loss=13.24710 | best_loss=8.28958
Epoch 9/80: current_loss=8.38543 | best_loss=8.28958
Epoch 10/80: current_loss=8.33739 | best_loss=8.28958
Epoch 11/80: current_loss=8.96776 | best_loss=8.28958
Epoch 12/80: current_loss=8.33447 | best_loss=8.28958
Epoch 13/80: current_loss=21.56486 | best_loss=8.28958
Epoch 14/80: current_loss=9.08151 | best_loss=8.28958
Epoch 15/80: current_loss=9.12677 | best_loss=8.28958
Epoch 16/80: current_loss=8.47526 | best_loss=8.28958
Epoch 17/80: current_loss=8.50249 | best_loss=8.28958
Epoch 18/80: current_loss=8.08607 | best_loss=8.08607
Epoch 19/80: current_loss=8.20680 | best_loss=8.08607
Epoch 20/80: current_loss=9.00354 | best_loss=8.08607
Epoch 21/80: current_loss=9.02484 | best_loss=8.08607
Epoch 22/80: current_loss=9.09337 | best_loss=8.08607
Epoch 23/80: current_loss=11.45978 | best_loss=8.08607
Epoch 24/80: current_loss=15.73716 | best_loss=8.08607
Epoch 25/80: current_loss=11.58034 | best_loss=8.08607
Epoch 26/80: current_loss=10.76883 | best_loss=8.08607
Epoch 27/80: current_loss=8.48555 | best_loss=8.08607
Epoch 28/80: current_loss=17.69039 | best_loss=8.08607
Epoch 29/80: current_loss=8.86960 | best_loss=8.08607
Epoch 30/80: current_loss=10.96176 | best_loss=8.08607
Epoch 31/80: current_loss=8.18740 | best_loss=8.08607
Epoch 32/80: current_loss=8.81116 | best_loss=8.08607
Epoch 33/80: current_loss=9.85759 | best_loss=8.08607
Epoch 34/80: current_loss=8.32147 | best_loss=8.08607
Epoch 35/80: current_loss=10.56010 | best_loss=8.08607
Epoch 36/80: current_loss=15.28307 | best_loss=8.08607
Epoch 37/80: current_loss=8.43219 | best_loss=8.08607
Epoch 38/80: current_loss=9.72445 | best_loss=8.08607
Early Stopping at epoch 38
      explained_var=0.01755 | mse_loss=8.18567
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.88162 | best_loss=8.88162
Epoch 1/80: current_loss=8.25582 | best_loss=8.25582
Epoch 2/80: current_loss=7.70922 | best_loss=7.70922
Epoch 3/80: current_loss=8.56400 | best_loss=7.70922
Epoch 4/80: current_loss=9.19019 | best_loss=7.70922
Epoch 5/80: current_loss=8.58680 | best_loss=7.70922
Epoch 6/80: current_loss=9.21869 | best_loss=7.70922
Epoch 7/80: current_loss=7.81928 | best_loss=7.70922
Epoch 8/80: current_loss=9.04470 | best_loss=7.70922
Epoch 9/80: current_loss=9.82320 | best_loss=7.70922
Epoch 10/80: current_loss=10.67368 | best_loss=7.70922
Epoch 11/80: current_loss=10.38307 | best_loss=7.70922
Epoch 12/80: current_loss=11.23550 | best_loss=7.70922
Epoch 13/80: current_loss=10.55836 | best_loss=7.70922
Epoch 14/80: current_loss=8.61372 | best_loss=7.70922
Epoch 15/80: current_loss=8.96392 | best_loss=7.70922
Epoch 16/80: current_loss=8.48923 | best_loss=7.70922
Epoch 17/80: current_loss=10.01733 | best_loss=7.70922
Epoch 18/80: current_loss=9.21896 | best_loss=7.70922
Epoch 19/80: current_loss=9.50560 | best_loss=7.70922
Epoch 20/80: current_loss=11.83489 | best_loss=7.70922
Epoch 21/80: current_loss=10.52067 | best_loss=7.70922
Epoch 22/80: current_loss=9.70784 | best_loss=7.70922
Early Stopping at epoch 22
      explained_var=-0.00171 | mse_loss=7.87894
----------------------------------------------
Average early_stopping_point: 26| avg_exp_var=0.02036| avg_loss=7.96874
----------------------------------------------


----------------------------------------------
Params for Trial 23
{'learning_rate': 0.1, 'weight_decay': 0.002064595745559742, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.36922 | best_loss=9.36922
Epoch 1/80: current_loss=8.33018 | best_loss=8.33018
Epoch 2/80: current_loss=8.04132 | best_loss=8.04132
Epoch 3/80: current_loss=12.55535 | best_loss=8.04132
Epoch 4/80: current_loss=9.93500 | best_loss=8.04132
Epoch 5/80: current_loss=12.52186 | best_loss=8.04132
Epoch 6/80: current_loss=12.34798 | best_loss=8.04132
Epoch 7/80: current_loss=10.06753 | best_loss=8.04132
Epoch 8/80: current_loss=10.57840 | best_loss=8.04132
Epoch 9/80: current_loss=7.85338 | best_loss=7.85338
Epoch 10/80: current_loss=9.31500 | best_loss=7.85338
Epoch 11/80: current_loss=12.19880 | best_loss=7.85338
Epoch 12/80: current_loss=9.45387 | best_loss=7.85338
Epoch 13/80: current_loss=7.83805 | best_loss=7.83805
Epoch 14/80: current_loss=11.58878 | best_loss=7.83805
Epoch 15/80: current_loss=12.77740 | best_loss=7.83805
Epoch 16/80: current_loss=8.14937 | best_loss=7.83805
Epoch 17/80: current_loss=8.73594 | best_loss=7.83805
Epoch 18/80: current_loss=16.86938 | best_loss=7.83805
Epoch 19/80: current_loss=10.29878 | best_loss=7.83805
Epoch 20/80: current_loss=9.90927 | best_loss=7.83805
Epoch 21/80: current_loss=45.65384 | best_loss=7.83805
Epoch 22/80: current_loss=9.53811 | best_loss=7.83805
Epoch 23/80: current_loss=8.15067 | best_loss=7.83805
Epoch 24/80: current_loss=9.46442 | best_loss=7.83805
Epoch 25/80: current_loss=9.39641 | best_loss=7.83805
Epoch 26/80: current_loss=9.16012 | best_loss=7.83805
Epoch 27/80: current_loss=13.83834 | best_loss=7.83805
Epoch 28/80: current_loss=9.68641 | best_loss=7.83805
Epoch 29/80: current_loss=10.21596 | best_loss=7.83805
Epoch 30/80: current_loss=8.33564 | best_loss=7.83805
Epoch 31/80: current_loss=11.35196 | best_loss=7.83805
Epoch 32/80: current_loss=10.82366 | best_loss=7.83805
Epoch 33/80: current_loss=9.80981 | best_loss=7.83805
Early Stopping at epoch 33
      explained_var=-0.01053 | mse_loss=7.66643
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=17.84147 | best_loss=17.84147
Epoch 1/80: current_loss=9.55112 | best_loss=9.55112
Epoch 2/80: current_loss=11.69114 | best_loss=9.55112
Epoch 3/80: current_loss=10.16997 | best_loss=9.55112
Epoch 4/80: current_loss=9.48067 | best_loss=9.48067
Epoch 5/80: current_loss=11.21706 | best_loss=9.48067
Epoch 6/80: current_loss=12.22831 | best_loss=9.48067
Epoch 7/80: current_loss=10.56087 | best_loss=9.48067
Epoch 8/80: current_loss=9.74491 | best_loss=9.48067
Epoch 9/80: current_loss=12.70778 | best_loss=9.48067
Epoch 10/80: current_loss=10.11336 | best_loss=9.48067
Epoch 11/80: current_loss=8.93997 | best_loss=8.93997
Epoch 12/80: current_loss=8.92251 | best_loss=8.92251
Epoch 13/80: current_loss=9.48450 | best_loss=8.92251
Epoch 14/80: current_loss=9.21520 | best_loss=8.92251
Epoch 15/80: current_loss=11.59195 | best_loss=8.92251
Epoch 16/80: current_loss=8.17228 | best_loss=8.17228
Epoch 17/80: current_loss=10.08536 | best_loss=8.17228
Epoch 18/80: current_loss=10.16826 | best_loss=8.17228
Epoch 19/80: current_loss=9.11984 | best_loss=8.17228
Epoch 20/80: current_loss=10.45830 | best_loss=8.17228
Epoch 21/80: current_loss=10.31922 | best_loss=8.17228
Epoch 22/80: current_loss=11.68213 | best_loss=8.17228
Epoch 23/80: current_loss=21.68850 | best_loss=8.17228
Epoch 24/80: current_loss=9.88763 | best_loss=8.17228
Epoch 25/80: current_loss=14.08760 | best_loss=8.17228
Epoch 26/80: current_loss=8.96592 | best_loss=8.17228
Epoch 27/80: current_loss=10.54672 | best_loss=8.17228
Epoch 28/80: current_loss=10.44994 | best_loss=8.17228
Epoch 29/80: current_loss=9.48541 | best_loss=8.17228
Epoch 30/80: current_loss=8.67799 | best_loss=8.17228
Epoch 31/80: current_loss=10.31214 | best_loss=8.17228
Epoch 32/80: current_loss=8.24665 | best_loss=8.17228
Epoch 33/80: current_loss=11.08378 | best_loss=8.17228
Epoch 34/80: current_loss=10.32360 | best_loss=8.17228
Epoch 35/80: current_loss=12.03809 | best_loss=8.17228
Epoch 36/80: current_loss=9.25192 | best_loss=8.17228
Early Stopping at epoch 36
      explained_var=0.01203 | mse_loss=8.07211
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.00125 | best_loss=9.00125
Epoch 1/80: current_loss=10.24988 | best_loss=9.00125
Epoch 2/80: current_loss=11.06295 | best_loss=9.00125
Epoch 3/80: current_loss=11.31226 | best_loss=9.00125
Epoch 4/80: current_loss=10.82718 | best_loss=9.00125
Epoch 5/80: current_loss=9.74815 | best_loss=9.00125
Epoch 6/80: current_loss=12.36559 | best_loss=9.00125
Epoch 7/80: current_loss=9.46097 | best_loss=9.00125
Epoch 8/80: current_loss=12.86914 | best_loss=9.00125
Epoch 9/80: current_loss=8.90310 | best_loss=8.90310
Epoch 10/80: current_loss=11.36495 | best_loss=8.90310
Epoch 11/80: current_loss=13.38859 | best_loss=8.90310
Epoch 12/80: current_loss=8.77780 | best_loss=8.77780
Epoch 13/80: current_loss=8.85830 | best_loss=8.77780
Epoch 14/80: current_loss=13.57513 | best_loss=8.77780
Epoch 15/80: current_loss=12.79765 | best_loss=8.77780
Epoch 16/80: current_loss=9.00900 | best_loss=8.77780
Epoch 17/80: current_loss=9.29526 | best_loss=8.77780
Epoch 18/80: current_loss=10.37864 | best_loss=8.77780
Epoch 19/80: current_loss=8.88917 | best_loss=8.77780
Epoch 20/80: current_loss=10.73954 | best_loss=8.77780
Epoch 21/80: current_loss=8.65980 | best_loss=8.65980
Epoch 22/80: current_loss=10.20312 | best_loss=8.65980
Epoch 23/80: current_loss=9.78571 | best_loss=8.65980
Epoch 24/80: current_loss=9.00499 | best_loss=8.65980
Epoch 25/80: current_loss=9.66379 | best_loss=8.65980
Epoch 26/80: current_loss=8.78131 | best_loss=8.65980
Epoch 27/80: current_loss=11.64411 | best_loss=8.65980
Epoch 28/80: current_loss=11.24851 | best_loss=8.65980
Epoch 29/80: current_loss=10.98886 | best_loss=8.65980
Epoch 30/80: current_loss=8.98187 | best_loss=8.65980
Epoch 31/80: current_loss=22.72411 | best_loss=8.65980
Epoch 32/80: current_loss=12.79021 | best_loss=8.65980
Epoch 33/80: current_loss=10.08376 | best_loss=8.65980
Epoch 34/80: current_loss=10.85123 | best_loss=8.65980
Epoch 35/80: current_loss=8.93612 | best_loss=8.65980
Epoch 36/80: current_loss=10.33727 | best_loss=8.65980
Epoch 37/80: current_loss=10.99013 | best_loss=8.65980
Epoch 38/80: current_loss=9.10401 | best_loss=8.65980
Epoch 39/80: current_loss=9.19327 | best_loss=8.65980
Epoch 40/80: current_loss=9.92302 | best_loss=8.65980
Epoch 41/80: current_loss=9.06556 | best_loss=8.65980
Early Stopping at epoch 41
      explained_var=0.00954 | mse_loss=8.41273
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.40418 | best_loss=11.40418
Epoch 1/80: current_loss=8.97854 | best_loss=8.97854
Epoch 2/80: current_loss=10.06873 | best_loss=8.97854
Epoch 3/80: current_loss=9.68349 | best_loss=8.97854
Epoch 4/80: current_loss=8.84813 | best_loss=8.84813
Epoch 5/80: current_loss=8.79857 | best_loss=8.79857
Epoch 6/80: current_loss=8.32941 | best_loss=8.32941
Epoch 7/80: current_loss=9.95081 | best_loss=8.32941
Epoch 8/80: current_loss=8.70698 | best_loss=8.32941
Epoch 9/80: current_loss=8.58543 | best_loss=8.32941
Epoch 10/80: current_loss=8.64940 | best_loss=8.32941
Epoch 11/80: current_loss=9.40038 | best_loss=8.32941
Epoch 12/80: current_loss=8.80680 | best_loss=8.32941
Epoch 13/80: current_loss=21.60635 | best_loss=8.32941
Epoch 14/80: current_loss=12.84797 | best_loss=8.32941
Epoch 15/80: current_loss=10.28710 | best_loss=8.32941
Epoch 16/80: current_loss=11.64616 | best_loss=8.32941
Epoch 17/80: current_loss=12.07921 | best_loss=8.32941
Epoch 18/80: current_loss=8.69097 | best_loss=8.32941
Epoch 19/80: current_loss=9.32208 | best_loss=8.32941
Epoch 20/80: current_loss=8.48026 | best_loss=8.32941
Epoch 21/80: current_loss=14.19587 | best_loss=8.32941
Epoch 22/80: current_loss=14.26863 | best_loss=8.32941
Epoch 23/80: current_loss=28.36651 | best_loss=8.32941
Epoch 24/80: current_loss=9.89290 | best_loss=8.32941
Epoch 25/80: current_loss=13.45797 | best_loss=8.32941
Epoch 26/80: current_loss=8.78474 | best_loss=8.32941
Early Stopping at epoch 26
      explained_var=-0.01023 | mse_loss=8.42269
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.79783 | best_loss=7.79783
Epoch 1/80: current_loss=7.83770 | best_loss=7.79783
Epoch 2/80: current_loss=9.01733 | best_loss=7.79783
Epoch 3/80: current_loss=8.55987 | best_loss=7.79783
Epoch 4/80: current_loss=11.59198 | best_loss=7.79783
Epoch 5/80: current_loss=8.75640 | best_loss=7.79783
Epoch 6/80: current_loss=7.84351 | best_loss=7.79783
Epoch 7/80: current_loss=9.11874 | best_loss=7.79783
Epoch 8/80: current_loss=7.70537 | best_loss=7.70537
Epoch 9/80: current_loss=10.06007 | best_loss=7.70537
Epoch 10/80: current_loss=7.79605 | best_loss=7.70537
Epoch 11/80: current_loss=8.44427 | best_loss=7.70537
Epoch 12/80: current_loss=7.83745 | best_loss=7.70537
Epoch 13/80: current_loss=7.75037 | best_loss=7.70537
Epoch 14/80: current_loss=7.66109 | best_loss=7.66109
Epoch 15/80: current_loss=8.21912 | best_loss=7.66109
Epoch 16/80: current_loss=9.45950 | best_loss=7.66109
Epoch 17/80: current_loss=21.06258 | best_loss=7.66109
Epoch 18/80: current_loss=7.89836 | best_loss=7.66109
Epoch 19/80: current_loss=9.35373 | best_loss=7.66109
Epoch 20/80: current_loss=7.71643 | best_loss=7.66109
Epoch 21/80: current_loss=7.73963 | best_loss=7.66109
Epoch 22/80: current_loss=23.97162 | best_loss=7.66109
Epoch 23/80: current_loss=18.69327 | best_loss=7.66109
Epoch 24/80: current_loss=8.09507 | best_loss=7.66109
Epoch 25/80: current_loss=8.44866 | best_loss=7.66109
Epoch 26/80: current_loss=8.38445 | best_loss=7.66109
Epoch 27/80: current_loss=9.02820 | best_loss=7.66109
Epoch 28/80: current_loss=11.86990 | best_loss=7.66109
Epoch 29/80: current_loss=8.42230 | best_loss=7.66109
Epoch 30/80: current_loss=7.76103 | best_loss=7.66109
Epoch 31/80: current_loss=15.41178 | best_loss=7.66109
Epoch 32/80: current_loss=9.13662 | best_loss=7.66109
Epoch 33/80: current_loss=8.20897 | best_loss=7.66109
Epoch 34/80: current_loss=17.44258 | best_loss=7.66109
Early Stopping at epoch 34
      explained_var=0.00186 | mse_loss=7.84279
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.00053| avg_loss=8.08335
----------------------------------------------


----------------------------------------------
Params for Trial 24
{'learning_rate': 0.1, 'weight_decay': 0.0038983467781305396, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=211.50485 | best_loss=211.50485
Epoch 1/80: current_loss=11.31646 | best_loss=11.31646
Epoch 2/80: current_loss=9.21381 | best_loss=9.21381
Epoch 3/80: current_loss=13.90155 | best_loss=9.21381
Epoch 4/80: current_loss=11.06493 | best_loss=9.21381
Epoch 5/80: current_loss=8.63198 | best_loss=8.63198
Epoch 6/80: current_loss=11.52666 | best_loss=8.63198
Epoch 7/80: current_loss=10.41029 | best_loss=8.63198
Epoch 8/80: current_loss=7.93656 | best_loss=7.93656
Epoch 9/80: current_loss=9.26015 | best_loss=7.93656
Epoch 10/80: current_loss=8.87887 | best_loss=7.93656
Epoch 11/80: current_loss=11.89099 | best_loss=7.93656
Epoch 12/80: current_loss=9.69327 | best_loss=7.93656
Epoch 13/80: current_loss=7.76188 | best_loss=7.76188
Epoch 14/80: current_loss=10.47655 | best_loss=7.76188
Epoch 15/80: current_loss=7.77426 | best_loss=7.76188
Epoch 16/80: current_loss=7.89901 | best_loss=7.76188
Epoch 17/80: current_loss=9.24788 | best_loss=7.76188
Epoch 18/80: current_loss=11.65778 | best_loss=7.76188
Epoch 19/80: current_loss=7.91874 | best_loss=7.76188
Epoch 20/80: current_loss=8.51899 | best_loss=7.76188
Epoch 21/80: current_loss=7.75173 | best_loss=7.75173
Epoch 22/80: current_loss=10.44525 | best_loss=7.75173
Epoch 23/80: current_loss=26.12196 | best_loss=7.75173
Epoch 24/80: current_loss=19.65346 | best_loss=7.75173
Epoch 25/80: current_loss=18.83499 | best_loss=7.75173
Epoch 26/80: current_loss=7.94268 | best_loss=7.75173
Epoch 27/80: current_loss=12.06505 | best_loss=7.75173
Epoch 28/80: current_loss=8.12653 | best_loss=7.75173
Epoch 29/80: current_loss=8.05555 | best_loss=7.75173
Epoch 30/80: current_loss=10.80909 | best_loss=7.75173
Epoch 31/80: current_loss=12.51419 | best_loss=7.75173
Epoch 32/80: current_loss=8.51794 | best_loss=7.75173
Epoch 33/80: current_loss=8.76595 | best_loss=7.75173
Epoch 34/80: current_loss=8.03333 | best_loss=7.75173
Epoch 35/80: current_loss=10.63716 | best_loss=7.75173
Epoch 36/80: current_loss=13.74885 | best_loss=7.75173
Epoch 37/80: current_loss=13.43698 | best_loss=7.75173
Epoch 38/80: current_loss=8.69235 | best_loss=7.75173
Epoch 39/80: current_loss=8.34954 | best_loss=7.75173
Epoch 40/80: current_loss=8.92300 | best_loss=7.75173
Epoch 41/80: current_loss=10.78547 | best_loss=7.75173
Early Stopping at epoch 41
      explained_var=0.00465 | mse_loss=7.57550
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=31.31692 | best_loss=31.31692
Epoch 1/80: current_loss=16.93584 | best_loss=16.93584
Epoch 2/80: current_loss=15.10688 | best_loss=15.10688
Epoch 3/80: current_loss=29.67155 | best_loss=15.10688
Epoch 4/80: current_loss=21.68924 | best_loss=15.10688
Epoch 5/80: current_loss=20.06062 | best_loss=15.10688
Epoch 6/80: current_loss=15.27764 | best_loss=15.10688
Epoch 7/80: current_loss=10.64201 | best_loss=10.64201
Epoch 8/80: current_loss=9.08661 | best_loss=9.08661
Epoch 9/80: current_loss=8.32516 | best_loss=8.32516
Epoch 10/80: current_loss=9.41883 | best_loss=8.32516
Epoch 11/80: current_loss=10.78241 | best_loss=8.32516
Epoch 12/80: current_loss=8.59543 | best_loss=8.32516
Epoch 13/80: current_loss=8.62781 | best_loss=8.32516
Epoch 14/80: current_loss=8.97811 | best_loss=8.32516
Epoch 15/80: current_loss=8.46260 | best_loss=8.32516
Epoch 16/80: current_loss=15.12988 | best_loss=8.32516
Epoch 17/80: current_loss=8.49477 | best_loss=8.32516
Epoch 18/80: current_loss=10.57412 | best_loss=8.32516
Epoch 19/80: current_loss=8.57584 | best_loss=8.32516
Epoch 20/80: current_loss=10.03250 | best_loss=8.32516
Epoch 21/80: current_loss=8.57937 | best_loss=8.32516
Epoch 22/80: current_loss=10.62858 | best_loss=8.32516
Epoch 23/80: current_loss=11.79044 | best_loss=8.32516
Epoch 24/80: current_loss=24.98882 | best_loss=8.32516
Epoch 25/80: current_loss=8.37037 | best_loss=8.32516
Epoch 26/80: current_loss=8.84583 | best_loss=8.32516
Epoch 27/80: current_loss=8.66067 | best_loss=8.32516
Epoch 28/80: current_loss=11.33039 | best_loss=8.32516
Epoch 29/80: current_loss=8.61654 | best_loss=8.32516
Early Stopping at epoch 29
      explained_var=0.00857 | mse_loss=8.15462
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.93669 | best_loss=13.93669
Epoch 1/80: current_loss=9.99748 | best_loss=9.99748
Epoch 2/80: current_loss=8.47240 | best_loss=8.47240
Epoch 3/80: current_loss=12.01004 | best_loss=8.47240
Epoch 4/80: current_loss=10.11766 | best_loss=8.47240
Epoch 5/80: current_loss=9.80666 | best_loss=8.47240
Epoch 6/80: current_loss=9.64963 | best_loss=8.47240
Epoch 7/80: current_loss=10.48448 | best_loss=8.47240
Epoch 8/80: current_loss=8.79494 | best_loss=8.47240
Epoch 9/80: current_loss=8.75027 | best_loss=8.47240
Epoch 10/80: current_loss=8.49482 | best_loss=8.47240
Epoch 11/80: current_loss=10.36070 | best_loss=8.47240
Epoch 12/80: current_loss=9.42028 | best_loss=8.47240
Epoch 13/80: current_loss=15.41813 | best_loss=8.47240
Epoch 14/80: current_loss=10.61334 | best_loss=8.47240
Epoch 15/80: current_loss=9.63951 | best_loss=8.47240
Epoch 16/80: current_loss=13.97122 | best_loss=8.47240
Epoch 17/80: current_loss=19.18695 | best_loss=8.47240
Epoch 18/80: current_loss=8.81529 | best_loss=8.47240
Epoch 19/80: current_loss=9.12921 | best_loss=8.47240
Epoch 20/80: current_loss=11.38654 | best_loss=8.47240
Epoch 21/80: current_loss=8.71336 | best_loss=8.47240
Epoch 22/80: current_loss=8.70293 | best_loss=8.47240
Early Stopping at epoch 22
      explained_var=0.04167 | mse_loss=8.24307
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.20823 | best_loss=9.20823
Epoch 1/80: current_loss=8.43142 | best_loss=8.43142
Epoch 2/80: current_loss=8.36412 | best_loss=8.36412
Epoch 3/80: current_loss=16.17765 | best_loss=8.36412
Epoch 4/80: current_loss=11.58591 | best_loss=8.36412
Epoch 5/80: current_loss=15.47133 | best_loss=8.36412
Epoch 6/80: current_loss=9.33156 | best_loss=8.36412
Epoch 7/80: current_loss=10.96604 | best_loss=8.36412
Epoch 8/80: current_loss=11.56418 | best_loss=8.36412
Epoch 9/80: current_loss=10.67394 | best_loss=8.36412
Epoch 10/80: current_loss=9.75092 | best_loss=8.36412
Epoch 11/80: current_loss=7.93858 | best_loss=7.93858
Epoch 12/80: current_loss=10.71074 | best_loss=7.93858
Epoch 13/80: current_loss=12.40572 | best_loss=7.93858
Epoch 14/80: current_loss=8.83739 | best_loss=7.93858
Epoch 15/80: current_loss=8.93979 | best_loss=7.93858
Epoch 16/80: current_loss=11.54331 | best_loss=7.93858
Epoch 17/80: current_loss=8.68463 | best_loss=7.93858
Epoch 18/80: current_loss=9.50494 | best_loss=7.93858
Epoch 19/80: current_loss=13.67130 | best_loss=7.93858
Epoch 20/80: current_loss=9.20159 | best_loss=7.93858
Epoch 21/80: current_loss=11.60783 | best_loss=7.93858
Epoch 22/80: current_loss=9.89566 | best_loss=7.93858
Epoch 23/80: current_loss=9.45760 | best_loss=7.93858
Epoch 24/80: current_loss=9.55446 | best_loss=7.93858
Epoch 25/80: current_loss=9.62366 | best_loss=7.93858
Epoch 26/80: current_loss=9.83394 | best_loss=7.93858
Epoch 27/80: current_loss=13.35411 | best_loss=7.93858
Epoch 28/80: current_loss=8.65183 | best_loss=7.93858
Epoch 29/80: current_loss=9.34246 | best_loss=7.93858
Epoch 30/80: current_loss=7.86074 | best_loss=7.86074
Epoch 31/80: current_loss=8.34337 | best_loss=7.86074
Epoch 32/80: current_loss=12.61931 | best_loss=7.86074
Epoch 33/80: current_loss=10.77048 | best_loss=7.86074
Epoch 34/80: current_loss=10.15155 | best_loss=7.86074
Epoch 35/80: current_loss=8.45537 | best_loss=7.86074
Epoch 36/80: current_loss=10.61483 | best_loss=7.86074
Epoch 37/80: current_loss=8.62217 | best_loss=7.86074
Epoch 38/80: current_loss=8.57622 | best_loss=7.86074
Epoch 39/80: current_loss=10.67878 | best_loss=7.86074
Epoch 40/80: current_loss=11.70357 | best_loss=7.86074
Epoch 41/80: current_loss=10.27365 | best_loss=7.86074
Epoch 42/80: current_loss=8.51342 | best_loss=7.86074
Epoch 43/80: current_loss=8.52851 | best_loss=7.86074
Epoch 44/80: current_loss=8.41004 | best_loss=7.86074
Epoch 45/80: current_loss=14.99087 | best_loss=7.86074
Epoch 46/80: current_loss=19.10795 | best_loss=7.86074
Epoch 47/80: current_loss=10.94214 | best_loss=7.86074
Epoch 48/80: current_loss=9.72046 | best_loss=7.86074
Epoch 49/80: current_loss=8.22271 | best_loss=7.86074
Epoch 50/80: current_loss=8.23105 | best_loss=7.86074
Early Stopping at epoch 50
      explained_var=0.04733 | mse_loss=7.94173
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.15756 | best_loss=11.15756
Epoch 1/80: current_loss=7.87785 | best_loss=7.87785
Epoch 2/80: current_loss=10.86579 | best_loss=7.87785
Epoch 3/80: current_loss=10.25402 | best_loss=7.87785
Epoch 4/80: current_loss=7.98322 | best_loss=7.87785
Epoch 5/80: current_loss=7.77507 | best_loss=7.77507
Epoch 6/80: current_loss=8.14126 | best_loss=7.77507
Epoch 7/80: current_loss=11.22953 | best_loss=7.77507
Epoch 8/80: current_loss=8.08576 | best_loss=7.77507
Epoch 9/80: current_loss=9.65643 | best_loss=7.77507
Epoch 10/80: current_loss=8.89094 | best_loss=7.77507
Epoch 11/80: current_loss=8.04954 | best_loss=7.77507
Epoch 12/80: current_loss=8.92097 | best_loss=7.77507
Epoch 13/80: current_loss=9.36974 | best_loss=7.77507
Epoch 14/80: current_loss=9.61705 | best_loss=7.77507
Epoch 15/80: current_loss=16.70909 | best_loss=7.77507
Epoch 16/80: current_loss=10.28573 | best_loss=7.77507
Epoch 17/80: current_loss=8.69832 | best_loss=7.77507
Epoch 18/80: current_loss=9.00363 | best_loss=7.77507
Epoch 19/80: current_loss=11.13950 | best_loss=7.77507
Epoch 20/80: current_loss=17.13944 | best_loss=7.77507
Epoch 21/80: current_loss=7.73987 | best_loss=7.73987
Epoch 22/80: current_loss=20.50395 | best_loss=7.73987
Epoch 23/80: current_loss=9.11700 | best_loss=7.73987
Epoch 24/80: current_loss=8.05569 | best_loss=7.73987
Epoch 25/80: current_loss=7.80172 | best_loss=7.73987
Epoch 26/80: current_loss=7.98179 | best_loss=7.73987
Epoch 27/80: current_loss=7.69642 | best_loss=7.69642
Epoch 28/80: current_loss=12.08949 | best_loss=7.69642
Epoch 29/80: current_loss=11.03669 | best_loss=7.69642
Epoch 30/80: current_loss=7.71040 | best_loss=7.69642
Epoch 31/80: current_loss=7.84307 | best_loss=7.69642
Epoch 32/80: current_loss=7.95076 | best_loss=7.69642
Epoch 33/80: current_loss=8.21363 | best_loss=7.69642
Epoch 34/80: current_loss=7.71650 | best_loss=7.69642
Epoch 35/80: current_loss=8.97889 | best_loss=7.69642
Epoch 36/80: current_loss=11.46927 | best_loss=7.69642
Epoch 37/80: current_loss=8.38646 | best_loss=7.69642
Epoch 38/80: current_loss=8.22982 | best_loss=7.69642
Epoch 39/80: current_loss=8.38210 | best_loss=7.69642
Epoch 40/80: current_loss=7.94561 | best_loss=7.69642
Epoch 41/80: current_loss=7.75207 | best_loss=7.69642
Epoch 42/80: current_loss=11.58922 | best_loss=7.69642
Epoch 43/80: current_loss=16.51943 | best_loss=7.69642
Epoch 44/80: current_loss=12.21031 | best_loss=7.69642
Epoch 45/80: current_loss=8.31162 | best_loss=7.69642
Epoch 46/80: current_loss=8.28564 | best_loss=7.69642
Epoch 47/80: current_loss=8.29253 | best_loss=7.69642
Early Stopping at epoch 47
      explained_var=0.00247 | mse_loss=7.86952
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.02094| avg_loss=7.95689
----------------------------------------------


----------------------------------------------
Params for Trial 25
{'learning_rate': 0.1, 'weight_decay': 0.004283058043447493, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.83199 | best_loss=9.83199
Epoch 1/80: current_loss=13.39412 | best_loss=9.83199
Epoch 2/80: current_loss=8.64782 | best_loss=8.64782
Epoch 3/80: current_loss=8.11713 | best_loss=8.11713
Epoch 4/80: current_loss=8.92890 | best_loss=8.11713
Epoch 5/80: current_loss=9.07616 | best_loss=8.11713
Epoch 6/80: current_loss=10.13814 | best_loss=8.11713
Epoch 7/80: current_loss=10.26129 | best_loss=8.11713
Epoch 8/80: current_loss=11.96352 | best_loss=8.11713
Epoch 9/80: current_loss=9.50686 | best_loss=8.11713
Epoch 10/80: current_loss=9.65944 | best_loss=8.11713
Epoch 11/80: current_loss=10.36677 | best_loss=8.11713
Epoch 12/80: current_loss=8.59544 | best_loss=8.11713
Epoch 13/80: current_loss=9.07429 | best_loss=8.11713
Epoch 14/80: current_loss=13.57949 | best_loss=8.11713
Epoch 15/80: current_loss=12.58171 | best_loss=8.11713
Epoch 16/80: current_loss=8.96330 | best_loss=8.11713
Epoch 17/80: current_loss=8.14084 | best_loss=8.11713
Epoch 18/80: current_loss=8.68803 | best_loss=8.11713
Epoch 19/80: current_loss=11.31019 | best_loss=8.11713
Epoch 20/80: current_loss=8.08930 | best_loss=8.08930
Epoch 21/80: current_loss=8.59225 | best_loss=8.08930
Epoch 22/80: current_loss=12.02797 | best_loss=8.08930
Epoch 23/80: current_loss=7.79566 | best_loss=7.79566
Epoch 24/80: current_loss=9.36290 | best_loss=7.79566
Epoch 25/80: current_loss=24.30783 | best_loss=7.79566
Epoch 26/80: current_loss=10.52978 | best_loss=7.79566
Epoch 27/80: current_loss=10.52202 | best_loss=7.79566
Epoch 28/80: current_loss=9.29490 | best_loss=7.79566
Epoch 29/80: current_loss=14.02662 | best_loss=7.79566
Epoch 30/80: current_loss=8.72667 | best_loss=7.79566
Epoch 31/80: current_loss=9.45437 | best_loss=7.79566
Epoch 32/80: current_loss=12.57428 | best_loss=7.79566
Epoch 33/80: current_loss=11.49534 | best_loss=7.79566
Epoch 34/80: current_loss=14.00672 | best_loss=7.79566
Epoch 35/80: current_loss=12.56017 | best_loss=7.79566
Epoch 36/80: current_loss=10.69593 | best_loss=7.79566
Epoch 37/80: current_loss=10.75957 | best_loss=7.79566
Epoch 38/80: current_loss=12.20792 | best_loss=7.79566
Epoch 39/80: current_loss=9.48744 | best_loss=7.79566
Epoch 40/80: current_loss=11.02060 | best_loss=7.79566
Epoch 41/80: current_loss=8.73741 | best_loss=7.79566
Epoch 42/80: current_loss=11.78314 | best_loss=7.79566
Epoch 43/80: current_loss=8.01411 | best_loss=7.79566
Early Stopping at epoch 43
      explained_var=-0.00555 | mse_loss=7.63332
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.89700 | best_loss=8.89700
Epoch 1/80: current_loss=12.32581 | best_loss=8.89700
Epoch 2/80: current_loss=8.39436 | best_loss=8.39436
Epoch 3/80: current_loss=10.01118 | best_loss=8.39436
Epoch 4/80: current_loss=16.38042 | best_loss=8.39436
Epoch 5/80: current_loss=8.56163 | best_loss=8.39436
Epoch 6/80: current_loss=11.02557 | best_loss=8.39436
Epoch 7/80: current_loss=8.62117 | best_loss=8.39436
Epoch 8/80: current_loss=14.85351 | best_loss=8.39436
Epoch 9/80: current_loss=8.35164 | best_loss=8.35164
Epoch 10/80: current_loss=14.77187 | best_loss=8.35164
Epoch 11/80: current_loss=15.06423 | best_loss=8.35164
Epoch 12/80: current_loss=9.79125 | best_loss=8.35164
Epoch 13/80: current_loss=9.41277 | best_loss=8.35164
Epoch 14/80: current_loss=9.78958 | best_loss=8.35164
Epoch 15/80: current_loss=13.97739 | best_loss=8.35164
Epoch 16/80: current_loss=10.90208 | best_loss=8.35164
Epoch 17/80: current_loss=9.43409 | best_loss=8.35164
Epoch 18/80: current_loss=9.66471 | best_loss=8.35164
Epoch 19/80: current_loss=9.29780 | best_loss=8.35164
Epoch 20/80: current_loss=8.39037 | best_loss=8.35164
Epoch 21/80: current_loss=9.15640 | best_loss=8.35164
Epoch 22/80: current_loss=10.76785 | best_loss=8.35164
Epoch 23/80: current_loss=8.59499 | best_loss=8.35164
Epoch 24/80: current_loss=16.55667 | best_loss=8.35164
Epoch 25/80: current_loss=10.90836 | best_loss=8.35164
Epoch 26/80: current_loss=8.40788 | best_loss=8.35164
Epoch 27/80: current_loss=8.90796 | best_loss=8.35164
Epoch 28/80: current_loss=8.76950 | best_loss=8.35164
Epoch 29/80: current_loss=8.58546 | best_loss=8.35164
Early Stopping at epoch 29
      explained_var=-0.00576 | mse_loss=8.21072
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.84344 | best_loss=11.84344
Epoch 1/80: current_loss=16.94536 | best_loss=11.84344
Epoch 2/80: current_loss=9.11044 | best_loss=9.11044
Epoch 3/80: current_loss=12.10467 | best_loss=9.11044
Epoch 4/80: current_loss=9.22683 | best_loss=9.11044
Epoch 5/80: current_loss=10.87247 | best_loss=9.11044
Epoch 6/80: current_loss=9.87701 | best_loss=9.11044
Epoch 7/80: current_loss=10.73574 | best_loss=9.11044
Epoch 8/80: current_loss=9.94870 | best_loss=9.11044
Epoch 9/80: current_loss=12.40906 | best_loss=9.11044
Epoch 10/80: current_loss=10.20507 | best_loss=9.11044
Epoch 11/80: current_loss=14.13391 | best_loss=9.11044
Epoch 12/80: current_loss=9.18143 | best_loss=9.11044
Epoch 13/80: current_loss=8.62606 | best_loss=8.62606
Epoch 14/80: current_loss=9.79016 | best_loss=8.62606
Epoch 15/80: current_loss=8.99106 | best_loss=8.62606
Epoch 16/80: current_loss=8.91531 | best_loss=8.62606
Epoch 17/80: current_loss=9.67816 | best_loss=8.62606
Epoch 18/80: current_loss=9.57949 | best_loss=8.62606
Epoch 19/80: current_loss=9.33594 | best_loss=8.62606
Epoch 20/80: current_loss=13.13635 | best_loss=8.62606
Epoch 21/80: current_loss=9.07569 | best_loss=8.62606
Epoch 22/80: current_loss=13.21411 | best_loss=8.62606
Epoch 23/80: current_loss=8.99414 | best_loss=8.62606
Epoch 24/80: current_loss=9.06377 | best_loss=8.62606
Epoch 25/80: current_loss=9.04680 | best_loss=8.62606
Epoch 26/80: current_loss=8.90406 | best_loss=8.62606
Epoch 27/80: current_loss=9.53389 | best_loss=8.62606
Epoch 28/80: current_loss=9.80826 | best_loss=8.62606
Epoch 29/80: current_loss=20.28846 | best_loss=8.62606
Epoch 30/80: current_loss=23.77082 | best_loss=8.62606
Epoch 31/80: current_loss=8.95017 | best_loss=8.62606
Epoch 32/80: current_loss=13.24876 | best_loss=8.62606
Epoch 33/80: current_loss=9.58605 | best_loss=8.62606
Early Stopping at epoch 33
      explained_var=0.00785 | mse_loss=8.41723
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.82630 | best_loss=9.82630
Epoch 1/80: current_loss=8.96497 | best_loss=8.96497
Epoch 2/80: current_loss=8.72786 | best_loss=8.72786
Epoch 3/80: current_loss=8.59128 | best_loss=8.59128
Epoch 4/80: current_loss=11.35391 | best_loss=8.59128
Epoch 5/80: current_loss=11.17172 | best_loss=8.59128
Epoch 6/80: current_loss=11.83551 | best_loss=8.59128
Epoch 7/80: current_loss=11.43115 | best_loss=8.59128
Epoch 8/80: current_loss=8.98677 | best_loss=8.59128
Epoch 9/80: current_loss=8.66773 | best_loss=8.59128
Epoch 10/80: current_loss=10.54663 | best_loss=8.59128
Epoch 11/80: current_loss=9.17570 | best_loss=8.59128
Epoch 12/80: current_loss=12.26866 | best_loss=8.59128
Epoch 13/80: current_loss=8.36187 | best_loss=8.36187
Epoch 14/80: current_loss=8.92603 | best_loss=8.36187
Epoch 15/80: current_loss=8.34224 | best_loss=8.34224
Epoch 16/80: current_loss=8.85596 | best_loss=8.34224
Epoch 17/80: current_loss=12.38834 | best_loss=8.34224
Epoch 18/80: current_loss=11.15659 | best_loss=8.34224
Epoch 19/80: current_loss=10.33502 | best_loss=8.34224
Epoch 20/80: current_loss=7.96648 | best_loss=7.96648
Epoch 21/80: current_loss=10.63350 | best_loss=7.96648
Epoch 22/80: current_loss=9.87145 | best_loss=7.96648
Epoch 23/80: current_loss=9.47512 | best_loss=7.96648
Epoch 24/80: current_loss=9.91951 | best_loss=7.96648
Epoch 25/80: current_loss=9.53335 | best_loss=7.96648
Epoch 26/80: current_loss=12.42134 | best_loss=7.96648
Epoch 27/80: current_loss=8.72114 | best_loss=7.96648
Epoch 28/80: current_loss=10.70003 | best_loss=7.96648
Epoch 29/80: current_loss=10.52757 | best_loss=7.96648
Epoch 30/80: current_loss=8.25670 | best_loss=7.96648
Epoch 31/80: current_loss=10.68518 | best_loss=7.96648
Epoch 32/80: current_loss=10.23460 | best_loss=7.96648
Epoch 33/80: current_loss=18.65338 | best_loss=7.96648
Epoch 34/80: current_loss=9.04591 | best_loss=7.96648
Epoch 35/80: current_loss=9.05193 | best_loss=7.96648
Epoch 36/80: current_loss=7.94692 | best_loss=7.94692
Epoch 37/80: current_loss=8.05800 | best_loss=7.94692
Epoch 38/80: current_loss=8.42180 | best_loss=7.94692
Epoch 39/80: current_loss=13.02317 | best_loss=7.94692
Epoch 40/80: current_loss=8.55190 | best_loss=7.94692
Epoch 41/80: current_loss=12.81717 | best_loss=7.94692
Epoch 42/80: current_loss=19.43192 | best_loss=7.94692
Epoch 43/80: current_loss=13.20273 | best_loss=7.94692
Epoch 44/80: current_loss=9.94152 | best_loss=7.94692
Epoch 45/80: current_loss=8.88654 | best_loss=7.94692
Epoch 46/80: current_loss=8.91397 | best_loss=7.94692
Epoch 47/80: current_loss=15.47993 | best_loss=7.94692
Epoch 48/80: current_loss=9.12091 | best_loss=7.94692
Epoch 49/80: current_loss=11.82339 | best_loss=7.94692
Epoch 50/80: current_loss=11.64166 | best_loss=7.94692
Epoch 51/80: current_loss=8.59644 | best_loss=7.94692
Epoch 52/80: current_loss=9.12624 | best_loss=7.94692
Epoch 53/80: current_loss=9.23388 | best_loss=7.94692
Epoch 54/80: current_loss=12.05960 | best_loss=7.94692
Epoch 55/80: current_loss=8.30767 | best_loss=7.94692
Epoch 56/80: current_loss=8.34906 | best_loss=7.94692
Early Stopping at epoch 56
      explained_var=0.04101 | mse_loss=7.99294
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.76242 | best_loss=10.76242
Epoch 1/80: current_loss=8.24606 | best_loss=8.24606
Epoch 2/80: current_loss=8.43145 | best_loss=8.24606
Epoch 3/80: current_loss=11.50411 | best_loss=8.24606
Epoch 4/80: current_loss=7.97353 | best_loss=7.97353
Epoch 5/80: current_loss=8.29102 | best_loss=7.97353
Epoch 6/80: current_loss=11.61753 | best_loss=7.97353
Epoch 7/80: current_loss=8.20907 | best_loss=7.97353
Epoch 8/80: current_loss=8.40265 | best_loss=7.97353
Epoch 9/80: current_loss=10.85942 | best_loss=7.97353
Epoch 10/80: current_loss=12.86109 | best_loss=7.97353
Epoch 11/80: current_loss=11.24286 | best_loss=7.97353
Epoch 12/80: current_loss=8.58966 | best_loss=7.97353
Epoch 13/80: current_loss=8.23453 | best_loss=7.97353
Epoch 14/80: current_loss=8.22580 | best_loss=7.97353
Epoch 15/80: current_loss=7.95442 | best_loss=7.95442
Epoch 16/80: current_loss=8.00117 | best_loss=7.95442
Epoch 17/80: current_loss=10.45464 | best_loss=7.95442
Epoch 18/80: current_loss=8.26733 | best_loss=7.95442
Epoch 19/80: current_loss=8.08159 | best_loss=7.95442
Epoch 20/80: current_loss=7.79438 | best_loss=7.79438
Epoch 21/80: current_loss=8.38778 | best_loss=7.79438
Epoch 22/80: current_loss=9.36094 | best_loss=7.79438
Epoch 23/80: current_loss=8.86202 | best_loss=7.79438
Epoch 24/80: current_loss=9.59704 | best_loss=7.79438
Epoch 25/80: current_loss=8.72458 | best_loss=7.79438
Epoch 26/80: current_loss=7.69365 | best_loss=7.69365
Epoch 27/80: current_loss=8.04687 | best_loss=7.69365
Epoch 28/80: current_loss=8.69534 | best_loss=7.69365
Epoch 29/80: current_loss=8.59122 | best_loss=7.69365
Epoch 30/80: current_loss=9.73816 | best_loss=7.69365
Epoch 31/80: current_loss=8.80952 | best_loss=7.69365
Epoch 32/80: current_loss=10.49190 | best_loss=7.69365
Epoch 33/80: current_loss=8.47153 | best_loss=7.69365
Epoch 34/80: current_loss=10.47945 | best_loss=7.69365
Epoch 35/80: current_loss=8.59316 | best_loss=7.69365
Epoch 36/80: current_loss=7.82312 | best_loss=7.69365
Epoch 37/80: current_loss=9.53518 | best_loss=7.69365
Epoch 38/80: current_loss=8.84590 | best_loss=7.69365
Epoch 39/80: current_loss=8.88315 | best_loss=7.69365
Epoch 40/80: current_loss=9.86454 | best_loss=7.69365
Epoch 41/80: current_loss=9.72424 | best_loss=7.69365
Epoch 42/80: current_loss=12.06596 | best_loss=7.69365
Epoch 43/80: current_loss=14.77747 | best_loss=7.69365
Epoch 44/80: current_loss=8.22513 | best_loss=7.69365
Epoch 45/80: current_loss=10.25241 | best_loss=7.69365
Epoch 46/80: current_loss=7.72365 | best_loss=7.69365
Early Stopping at epoch 46
      explained_var=-0.00118 | mse_loss=7.87366
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.00727| avg_loss=8.02557
----------------------------------------------


----------------------------------------------
Params for Trial 26
{'learning_rate': 0.1, 'weight_decay': 0.002103762622531458, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.28387 | best_loss=10.28387
Epoch 1/80: current_loss=14.39072 | best_loss=10.28387
Epoch 2/80: current_loss=9.34849 | best_loss=9.34849
Epoch 3/80: current_loss=18.57887 | best_loss=9.34849
Epoch 4/80: current_loss=10.64940 | best_loss=9.34849
Epoch 5/80: current_loss=9.08633 | best_loss=9.08633
Epoch 6/80: current_loss=8.95255 | best_loss=8.95255
Epoch 7/80: current_loss=8.64457 | best_loss=8.64457
Epoch 8/80: current_loss=8.55890 | best_loss=8.55890
Epoch 9/80: current_loss=9.34959 | best_loss=8.55890
Epoch 10/80: current_loss=11.77930 | best_loss=8.55890
Epoch 11/80: current_loss=8.16152 | best_loss=8.16152
Epoch 12/80: current_loss=9.43116 | best_loss=8.16152
Epoch 13/80: current_loss=8.91702 | best_loss=8.16152
Epoch 14/80: current_loss=9.31700 | best_loss=8.16152
Epoch 15/80: current_loss=8.50875 | best_loss=8.16152
Epoch 16/80: current_loss=10.32635 | best_loss=8.16152
Epoch 17/80: current_loss=10.19442 | best_loss=8.16152
Epoch 18/80: current_loss=9.52792 | best_loss=8.16152
Epoch 19/80: current_loss=8.15043 | best_loss=8.15043
Epoch 20/80: current_loss=7.84854 | best_loss=7.84854
Epoch 21/80: current_loss=9.28392 | best_loss=7.84854
Epoch 22/80: current_loss=7.82293 | best_loss=7.82293
Epoch 23/80: current_loss=8.46116 | best_loss=7.82293
Epoch 24/80: current_loss=14.95147 | best_loss=7.82293
Epoch 25/80: current_loss=19.00677 | best_loss=7.82293
Epoch 26/80: current_loss=7.82507 | best_loss=7.82293
Epoch 27/80: current_loss=13.34314 | best_loss=7.82293
Epoch 28/80: current_loss=8.41228 | best_loss=7.82293
Epoch 29/80: current_loss=7.82274 | best_loss=7.82274
Epoch 30/80: current_loss=7.91595 | best_loss=7.82274
Epoch 31/80: current_loss=7.99364 | best_loss=7.82274
Epoch 32/80: current_loss=8.87240 | best_loss=7.82274
Epoch 33/80: current_loss=8.94401 | best_loss=7.82274
Epoch 34/80: current_loss=9.59715 | best_loss=7.82274
Epoch 35/80: current_loss=7.98978 | best_loss=7.82274
Epoch 36/80: current_loss=7.82891 | best_loss=7.82274
Epoch 37/80: current_loss=9.62223 | best_loss=7.82274
Epoch 38/80: current_loss=12.51690 | best_loss=7.82274
Epoch 39/80: current_loss=17.14980 | best_loss=7.82274
Epoch 40/80: current_loss=12.06181 | best_loss=7.82274
Epoch 41/80: current_loss=11.78017 | best_loss=7.82274
Epoch 42/80: current_loss=9.06849 | best_loss=7.82274
Epoch 43/80: current_loss=7.73469 | best_loss=7.73469
Epoch 44/80: current_loss=19.81799 | best_loss=7.73469
Epoch 45/80: current_loss=11.14881 | best_loss=7.73469
Epoch 46/80: current_loss=8.04356 | best_loss=7.73469
Epoch 47/80: current_loss=8.86053 | best_loss=7.73469
Epoch 48/80: current_loss=12.18096 | best_loss=7.73469
Epoch 49/80: current_loss=11.83846 | best_loss=7.73469
Epoch 50/80: current_loss=11.07520 | best_loss=7.73469
Epoch 51/80: current_loss=8.66207 | best_loss=7.73469
Epoch 52/80: current_loss=8.59668 | best_loss=7.73469
Epoch 53/80: current_loss=11.01838 | best_loss=7.73469
Epoch 54/80: current_loss=12.40306 | best_loss=7.73469
Epoch 55/80: current_loss=9.53740 | best_loss=7.73469
Epoch 56/80: current_loss=12.69394 | best_loss=7.73469
Epoch 57/80: current_loss=8.05645 | best_loss=7.73469
Epoch 58/80: current_loss=8.52124 | best_loss=7.73469
Epoch 59/80: current_loss=8.76118 | best_loss=7.73469
Epoch 60/80: current_loss=10.60881 | best_loss=7.73469
Epoch 61/80: current_loss=8.30963 | best_loss=7.73469
Epoch 62/80: current_loss=7.86500 | best_loss=7.73469
Epoch 63/80: current_loss=13.00895 | best_loss=7.73469
Early Stopping at epoch 63
      explained_var=0.00494 | mse_loss=7.54887
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=37.36235 | best_loss=37.36235
Epoch 1/80: current_loss=9.43940 | best_loss=9.43940
Epoch 2/80: current_loss=10.43924 | best_loss=9.43940
Epoch 3/80: current_loss=8.85949 | best_loss=8.85949
Epoch 4/80: current_loss=8.85223 | best_loss=8.85223
Epoch 5/80: current_loss=8.74048 | best_loss=8.74048
Epoch 6/80: current_loss=9.02277 | best_loss=8.74048
Epoch 7/80: current_loss=8.74956 | best_loss=8.74048
Epoch 8/80: current_loss=9.11994 | best_loss=8.74048
Epoch 9/80: current_loss=8.95215 | best_loss=8.74048
Epoch 10/80: current_loss=9.71372 | best_loss=8.74048
Epoch 11/80: current_loss=9.00692 | best_loss=8.74048
Epoch 12/80: current_loss=9.13468 | best_loss=8.74048
Epoch 13/80: current_loss=8.58607 | best_loss=8.58607
Epoch 14/80: current_loss=9.00268 | best_loss=8.58607
Epoch 15/80: current_loss=9.24112 | best_loss=8.58607
Epoch 16/80: current_loss=8.87706 | best_loss=8.58607
Epoch 17/80: current_loss=8.54625 | best_loss=8.54625
Epoch 18/80: current_loss=9.52600 | best_loss=8.54625
Epoch 19/80: current_loss=8.21417 | best_loss=8.21417
Epoch 20/80: current_loss=13.12109 | best_loss=8.21417
Epoch 21/80: current_loss=8.72366 | best_loss=8.21417
Epoch 22/80: current_loss=24.59189 | best_loss=8.21417
Epoch 23/80: current_loss=14.20951 | best_loss=8.21417
Epoch 24/80: current_loss=9.22003 | best_loss=8.21417
Epoch 25/80: current_loss=10.50478 | best_loss=8.21417
Epoch 26/80: current_loss=9.16445 | best_loss=8.21417
Epoch 27/80: current_loss=9.71303 | best_loss=8.21417
Epoch 28/80: current_loss=16.97590 | best_loss=8.21417
Epoch 29/80: current_loss=8.44788 | best_loss=8.21417
Epoch 30/80: current_loss=8.89085 | best_loss=8.21417
Epoch 31/80: current_loss=10.73403 | best_loss=8.21417
Epoch 32/80: current_loss=11.00015 | best_loss=8.21417
Epoch 33/80: current_loss=8.78376 | best_loss=8.21417
Epoch 34/80: current_loss=17.16943 | best_loss=8.21417
Epoch 35/80: current_loss=8.29755 | best_loss=8.21417
Epoch 36/80: current_loss=10.15540 | best_loss=8.21417
Epoch 37/80: current_loss=10.00110 | best_loss=8.21417
Epoch 38/80: current_loss=8.78717 | best_loss=8.21417
Epoch 39/80: current_loss=12.29292 | best_loss=8.21417
Early Stopping at epoch 39
      explained_var=0.01143 | mse_loss=8.12048
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.44128 | best_loss=9.44128
Epoch 1/80: current_loss=10.73962 | best_loss=9.44128
Epoch 2/80: current_loss=17.86833 | best_loss=9.44128
Epoch 3/80: current_loss=9.09644 | best_loss=9.09644
Epoch 4/80: current_loss=9.41145 | best_loss=9.09644
Epoch 5/80: current_loss=16.52560 | best_loss=9.09644
Epoch 6/80: current_loss=11.40669 | best_loss=9.09644
Epoch 7/80: current_loss=9.59954 | best_loss=9.09644
Epoch 8/80: current_loss=12.58068 | best_loss=9.09644
Epoch 9/80: current_loss=9.40545 | best_loss=9.09644
Epoch 10/80: current_loss=13.51332 | best_loss=9.09644
Epoch 11/80: current_loss=10.96072 | best_loss=9.09644
Epoch 12/80: current_loss=10.59146 | best_loss=9.09644
Epoch 13/80: current_loss=8.85937 | best_loss=8.85937
Epoch 14/80: current_loss=10.89972 | best_loss=8.85937
Epoch 15/80: current_loss=18.34747 | best_loss=8.85937
Epoch 16/80: current_loss=8.71575 | best_loss=8.71575
Epoch 17/80: current_loss=8.96287 | best_loss=8.71575
Epoch 18/80: current_loss=19.28322 | best_loss=8.71575
Epoch 19/80: current_loss=9.49656 | best_loss=8.71575
Epoch 20/80: current_loss=8.70187 | best_loss=8.70187
Epoch 21/80: current_loss=9.27863 | best_loss=8.70187
Epoch 22/80: current_loss=8.71027 | best_loss=8.70187
Epoch 23/80: current_loss=11.23197 | best_loss=8.70187
Epoch 24/80: current_loss=8.80487 | best_loss=8.70187
Epoch 25/80: current_loss=8.70591 | best_loss=8.70187
Epoch 26/80: current_loss=8.90281 | best_loss=8.70187
Epoch 27/80: current_loss=9.65039 | best_loss=8.70187
Epoch 28/80: current_loss=8.90918 | best_loss=8.70187
Epoch 29/80: current_loss=10.34560 | best_loss=8.70187
Epoch 30/80: current_loss=8.81194 | best_loss=8.70187
Epoch 31/80: current_loss=8.58010 | best_loss=8.58010
Epoch 32/80: current_loss=9.38650 | best_loss=8.58010
Epoch 33/80: current_loss=14.45077 | best_loss=8.58010
Epoch 34/80: current_loss=9.57073 | best_loss=8.58010
Epoch 35/80: current_loss=11.87213 | best_loss=8.58010
Epoch 36/80: current_loss=8.78466 | best_loss=8.58010
Epoch 37/80: current_loss=9.93732 | best_loss=8.58010
Epoch 38/80: current_loss=10.45281 | best_loss=8.58010
Epoch 39/80: current_loss=10.37744 | best_loss=8.58010
Epoch 40/80: current_loss=13.64986 | best_loss=8.58010
Epoch 41/80: current_loss=9.29946 | best_loss=8.58010
Epoch 42/80: current_loss=11.93915 | best_loss=8.58010
Epoch 43/80: current_loss=9.09549 | best_loss=8.58010
Epoch 44/80: current_loss=10.87935 | best_loss=8.58010
Epoch 45/80: current_loss=13.62636 | best_loss=8.58010
Epoch 46/80: current_loss=10.62583 | best_loss=8.58010
Epoch 47/80: current_loss=9.26506 | best_loss=8.58010
Epoch 48/80: current_loss=10.09622 | best_loss=8.58010
Epoch 49/80: current_loss=10.34153 | best_loss=8.58010
Epoch 50/80: current_loss=11.00877 | best_loss=8.58010
Epoch 51/80: current_loss=9.90288 | best_loss=8.58010
Early Stopping at epoch 51
      explained_var=0.03239 | mse_loss=8.40230
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.12182 | best_loss=13.12182
Epoch 1/80: current_loss=9.51343 | best_loss=9.51343
Epoch 2/80: current_loss=9.58151 | best_loss=9.51343
Epoch 3/80: current_loss=8.87313 | best_loss=8.87313
Epoch 4/80: current_loss=9.29282 | best_loss=8.87313
Epoch 5/80: current_loss=13.42095 | best_loss=8.87313
Epoch 6/80: current_loss=13.83508 | best_loss=8.87313
Epoch 7/80: current_loss=11.59295 | best_loss=8.87313
Epoch 8/80: current_loss=8.34748 | best_loss=8.34748
Epoch 9/80: current_loss=9.89524 | best_loss=8.34748
Epoch 10/80: current_loss=8.45971 | best_loss=8.34748
Epoch 11/80: current_loss=12.27257 | best_loss=8.34748
Epoch 12/80: current_loss=8.79121 | best_loss=8.34748
Epoch 13/80: current_loss=8.29601 | best_loss=8.29601
Epoch 14/80: current_loss=8.24526 | best_loss=8.24526
Epoch 15/80: current_loss=12.12118 | best_loss=8.24526
Epoch 16/80: current_loss=8.10982 | best_loss=8.10982
Epoch 17/80: current_loss=10.43774 | best_loss=8.10982
Epoch 18/80: current_loss=9.69349 | best_loss=8.10982
Epoch 19/80: current_loss=13.31287 | best_loss=8.10982
Epoch 20/80: current_loss=10.78319 | best_loss=8.10982
Epoch 21/80: current_loss=12.07426 | best_loss=8.10982
Epoch 22/80: current_loss=10.15994 | best_loss=8.10982
Epoch 23/80: current_loss=9.27650 | best_loss=8.10982
Epoch 24/80: current_loss=14.26944 | best_loss=8.10982
Epoch 25/80: current_loss=8.92117 | best_loss=8.10982
Epoch 26/80: current_loss=10.46186 | best_loss=8.10982
Epoch 27/80: current_loss=10.82076 | best_loss=8.10982
Epoch 28/80: current_loss=14.22017 | best_loss=8.10982
Epoch 29/80: current_loss=9.95149 | best_loss=8.10982
Epoch 30/80: current_loss=9.19399 | best_loss=8.10982
Epoch 31/80: current_loss=8.11843 | best_loss=8.10982
Epoch 32/80: current_loss=14.71712 | best_loss=8.10982
Epoch 33/80: current_loss=12.04662 | best_loss=8.10982
Epoch 34/80: current_loss=9.36590 | best_loss=8.10982
Epoch 35/80: current_loss=9.71492 | best_loss=8.10982
Epoch 36/80: current_loss=8.20049 | best_loss=8.10982
Early Stopping at epoch 36
      explained_var=0.01524 | mse_loss=8.20521
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.86705 | best_loss=9.86705
Epoch 1/80: current_loss=11.60076 | best_loss=9.86705
Epoch 2/80: current_loss=8.95689 | best_loss=8.95689
Epoch 3/80: current_loss=9.52519 | best_loss=8.95689
Epoch 4/80: current_loss=11.02953 | best_loss=8.95689
Epoch 5/80: current_loss=8.48283 | best_loss=8.48283
Epoch 6/80: current_loss=11.32721 | best_loss=8.48283
Epoch 7/80: current_loss=8.20319 | best_loss=8.20319
Epoch 8/80: current_loss=9.08292 | best_loss=8.20319
Epoch 9/80: current_loss=8.29430 | best_loss=8.20319
Epoch 10/80: current_loss=7.87748 | best_loss=7.87748
Epoch 11/80: current_loss=10.70094 | best_loss=7.87748
Epoch 12/80: current_loss=12.19972 | best_loss=7.87748
Epoch 13/80: current_loss=10.60984 | best_loss=7.87748
Epoch 14/80: current_loss=9.70652 | best_loss=7.87748
Epoch 15/80: current_loss=10.68848 | best_loss=7.87748
Epoch 16/80: current_loss=9.84151 | best_loss=7.87748
Epoch 17/80: current_loss=8.72654 | best_loss=7.87748
Epoch 18/80: current_loss=9.09941 | best_loss=7.87748
Epoch 19/80: current_loss=12.61424 | best_loss=7.87748
Epoch 20/80: current_loss=15.48266 | best_loss=7.87748
Epoch 21/80: current_loss=11.97031 | best_loss=7.87748
Epoch 22/80: current_loss=10.30517 | best_loss=7.87748
Epoch 23/80: current_loss=11.02180 | best_loss=7.87748
Epoch 24/80: current_loss=12.16736 | best_loss=7.87748
Epoch 25/80: current_loss=10.71285 | best_loss=7.87748
Epoch 26/80: current_loss=8.07613 | best_loss=7.87748
Epoch 27/80: current_loss=9.13101 | best_loss=7.87748
Epoch 28/80: current_loss=8.50166 | best_loss=7.87748
Epoch 29/80: current_loss=11.30034 | best_loss=7.87748
Epoch 30/80: current_loss=8.00948 | best_loss=7.87748
Early Stopping at epoch 30
      explained_var=-0.02283 | mse_loss=8.03948
----------------------------------------------
Average early_stopping_point: 23| avg_exp_var=0.00823| avg_loss=8.06327
----------------------------------------------


----------------------------------------------
Params for Trial 27
{'learning_rate': 0.1, 'weight_decay': 0.004030402743807104, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=16.47651 | best_loss=16.47651
Epoch 1/80: current_loss=10.15190 | best_loss=10.15190
Epoch 2/80: current_loss=9.74376 | best_loss=9.74376
Epoch 3/80: current_loss=8.21456 | best_loss=8.21456
Epoch 4/80: current_loss=11.22342 | best_loss=8.21456
Epoch 5/80: current_loss=10.11887 | best_loss=8.21456
Epoch 6/80: current_loss=10.78520 | best_loss=8.21456
Epoch 7/80: current_loss=8.91299 | best_loss=8.21456
Epoch 8/80: current_loss=8.02660 | best_loss=8.02660
Epoch 9/80: current_loss=8.25796 | best_loss=8.02660
Epoch 10/80: current_loss=8.79464 | best_loss=8.02660
Epoch 11/80: current_loss=10.25005 | best_loss=8.02660
Epoch 12/80: current_loss=8.32209 | best_loss=8.02660
Epoch 13/80: current_loss=8.81818 | best_loss=8.02660
Epoch 14/80: current_loss=8.24392 | best_loss=8.02660
Epoch 15/80: current_loss=9.05867 | best_loss=8.02660
Epoch 16/80: current_loss=9.62906 | best_loss=8.02660
Epoch 17/80: current_loss=7.73128 | best_loss=7.73128
Epoch 18/80: current_loss=8.42160 | best_loss=7.73128
Epoch 19/80: current_loss=7.86291 | best_loss=7.73128
Epoch 20/80: current_loss=7.96371 | best_loss=7.73128
Epoch 21/80: current_loss=7.91866 | best_loss=7.73128
Epoch 22/80: current_loss=8.49458 | best_loss=7.73128
Epoch 23/80: current_loss=11.87937 | best_loss=7.73128
Epoch 24/80: current_loss=8.17992 | best_loss=7.73128
Epoch 25/80: current_loss=9.51940 | best_loss=7.73128
Epoch 26/80: current_loss=7.97332 | best_loss=7.73128
Epoch 27/80: current_loss=8.44905 | best_loss=7.73128
Epoch 28/80: current_loss=15.58095 | best_loss=7.73128
Epoch 29/80: current_loss=7.78906 | best_loss=7.73128
Epoch 30/80: current_loss=7.80032 | best_loss=7.73128
Epoch 31/80: current_loss=8.23888 | best_loss=7.73128
Epoch 32/80: current_loss=7.86463 | best_loss=7.73128
Epoch 33/80: current_loss=8.09250 | best_loss=7.73128
Epoch 34/80: current_loss=8.57226 | best_loss=7.73128
Epoch 35/80: current_loss=11.31581 | best_loss=7.73128
Epoch 36/80: current_loss=9.26617 | best_loss=7.73128
Epoch 37/80: current_loss=8.21660 | best_loss=7.73128
Early Stopping at epoch 37
      explained_var=0.00713 | mse_loss=7.54872
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.98728 | best_loss=8.98728
Epoch 1/80: current_loss=11.13803 | best_loss=8.98728
Epoch 2/80: current_loss=17.83922 | best_loss=8.98728
Epoch 3/80: current_loss=8.43207 | best_loss=8.43207
Epoch 4/80: current_loss=9.01377 | best_loss=8.43207
Epoch 5/80: current_loss=8.66718 | best_loss=8.43207
Epoch 6/80: current_loss=8.50730 | best_loss=8.43207
Epoch 7/80: current_loss=8.38445 | best_loss=8.38445
Epoch 8/80: current_loss=8.30960 | best_loss=8.30960
Epoch 9/80: current_loss=8.84670 | best_loss=8.30960
Epoch 10/80: current_loss=8.82348 | best_loss=8.30960
Epoch 11/80: current_loss=8.32496 | best_loss=8.30960
Epoch 12/80: current_loss=8.65086 | best_loss=8.30960
Epoch 13/80: current_loss=8.35475 | best_loss=8.30960
Epoch 14/80: current_loss=8.31841 | best_loss=8.30960
Epoch 15/80: current_loss=10.10164 | best_loss=8.30960
Epoch 16/80: current_loss=14.82654 | best_loss=8.30960
Epoch 17/80: current_loss=8.60611 | best_loss=8.30960
Epoch 18/80: current_loss=8.34757 | best_loss=8.30960
Epoch 19/80: current_loss=8.63335 | best_loss=8.30960
Epoch 20/80: current_loss=9.05371 | best_loss=8.30960
Epoch 21/80: current_loss=8.39726 | best_loss=8.30960
Epoch 22/80: current_loss=8.66339 | best_loss=8.30960
Epoch 23/80: current_loss=9.37858 | best_loss=8.30960
Epoch 24/80: current_loss=13.98567 | best_loss=8.30960
Epoch 25/80: current_loss=9.71658 | best_loss=8.30960
Epoch 26/80: current_loss=10.14839 | best_loss=8.30960
Epoch 27/80: current_loss=38.41509 | best_loss=8.30960
Epoch 28/80: current_loss=19.29536 | best_loss=8.30960
Early Stopping at epoch 28
      explained_var=0.00496 | mse_loss=8.14964
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.32062 | best_loss=14.32062
Epoch 1/80: current_loss=11.78890 | best_loss=11.78890
Epoch 2/80: current_loss=9.72834 | best_loss=9.72834
Epoch 3/80: current_loss=10.74011 | best_loss=9.72834
Epoch 4/80: current_loss=13.63897 | best_loss=9.72834
Epoch 5/80: current_loss=11.43888 | best_loss=9.72834
Epoch 6/80: current_loss=9.39786 | best_loss=9.39786
Epoch 7/80: current_loss=9.40825 | best_loss=9.39786
Epoch 8/80: current_loss=14.21874 | best_loss=9.39786
Epoch 9/80: current_loss=10.14431 | best_loss=9.39786
Epoch 10/80: current_loss=12.05635 | best_loss=9.39786
Epoch 11/80: current_loss=11.65119 | best_loss=9.39786
Epoch 12/80: current_loss=9.00392 | best_loss=9.00392
Epoch 13/80: current_loss=11.77646 | best_loss=9.00392
Epoch 14/80: current_loss=9.51123 | best_loss=9.00392
Epoch 15/80: current_loss=8.35735 | best_loss=8.35735
Epoch 16/80: current_loss=10.91778 | best_loss=8.35735
Epoch 17/80: current_loss=11.37204 | best_loss=8.35735
Epoch 18/80: current_loss=11.62817 | best_loss=8.35735
Epoch 19/80: current_loss=11.18198 | best_loss=8.35735
Epoch 20/80: current_loss=10.89011 | best_loss=8.35735
Epoch 21/80: current_loss=11.88153 | best_loss=8.35735
Epoch 22/80: current_loss=10.11862 | best_loss=8.35735
Epoch 23/80: current_loss=10.40757 | best_loss=8.35735
Epoch 24/80: current_loss=9.13313 | best_loss=8.35735
Epoch 25/80: current_loss=9.97945 | best_loss=8.35735
Epoch 26/80: current_loss=10.39022 | best_loss=8.35735
Epoch 27/80: current_loss=17.99202 | best_loss=8.35735
Epoch 28/80: current_loss=13.05783 | best_loss=8.35735
Epoch 29/80: current_loss=10.50842 | best_loss=8.35735
Epoch 30/80: current_loss=19.78900 | best_loss=8.35735
Epoch 31/80: current_loss=10.12340 | best_loss=8.35735
Epoch 32/80: current_loss=9.71996 | best_loss=8.35735
Epoch 33/80: current_loss=9.87398 | best_loss=8.35735
Epoch 34/80: current_loss=9.22432 | best_loss=8.35735
Epoch 35/80: current_loss=9.15775 | best_loss=8.35735
Early Stopping at epoch 35
      explained_var=0.07566 | mse_loss=8.08627
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.83339 | best_loss=8.83339
Epoch 1/80: current_loss=9.44082 | best_loss=8.83339
Epoch 2/80: current_loss=8.63532 | best_loss=8.63532
Epoch 3/80: current_loss=9.89319 | best_loss=8.63532
Epoch 4/80: current_loss=8.98335 | best_loss=8.63532
Epoch 5/80: current_loss=8.32302 | best_loss=8.32302
Epoch 6/80: current_loss=9.30153 | best_loss=8.32302
Epoch 7/80: current_loss=8.21372 | best_loss=8.21372
Epoch 8/80: current_loss=9.94091 | best_loss=8.21372
Epoch 9/80: current_loss=10.44879 | best_loss=8.21372
Epoch 10/80: current_loss=16.34305 | best_loss=8.21372
Epoch 11/80: current_loss=21.35886 | best_loss=8.21372
Epoch 12/80: current_loss=25.51197 | best_loss=8.21372
Epoch 13/80: current_loss=8.87132 | best_loss=8.21372
Epoch 14/80: current_loss=9.02562 | best_loss=8.21372
Epoch 15/80: current_loss=9.87137 | best_loss=8.21372
Epoch 16/80: current_loss=8.31506 | best_loss=8.21372
Epoch 17/80: current_loss=8.70494 | best_loss=8.21372
Epoch 18/80: current_loss=13.88549 | best_loss=8.21372
Epoch 19/80: current_loss=10.20463 | best_loss=8.21372
Epoch 20/80: current_loss=8.73935 | best_loss=8.21372
Epoch 21/80: current_loss=8.19486 | best_loss=8.19486
Epoch 22/80: current_loss=9.82792 | best_loss=8.19486
Epoch 23/80: current_loss=9.54426 | best_loss=8.19486
Epoch 24/80: current_loss=9.90407 | best_loss=8.19486
Epoch 25/80: current_loss=8.98657 | best_loss=8.19486
Epoch 26/80: current_loss=8.36521 | best_loss=8.19486
Epoch 27/80: current_loss=8.48547 | best_loss=8.19486
Epoch 28/80: current_loss=8.24609 | best_loss=8.19486
Epoch 29/80: current_loss=12.91856 | best_loss=8.19486
Epoch 30/80: current_loss=8.29855 | best_loss=8.19486
Epoch 31/80: current_loss=8.97774 | best_loss=8.19486
Epoch 32/80: current_loss=24.90650 | best_loss=8.19486
Epoch 33/80: current_loss=18.74198 | best_loss=8.19486
Epoch 34/80: current_loss=8.75787 | best_loss=8.19486
Epoch 35/80: current_loss=29.61057 | best_loss=8.19486
Epoch 36/80: current_loss=13.67858 | best_loss=8.19486
Epoch 37/80: current_loss=8.23291 | best_loss=8.19486
Epoch 38/80: current_loss=10.32908 | best_loss=8.19486
Epoch 39/80: current_loss=9.13956 | best_loss=8.19486
Epoch 40/80: current_loss=11.76235 | best_loss=8.19486
Epoch 41/80: current_loss=11.46970 | best_loss=8.19486
Early Stopping at epoch 41
      explained_var=0.01643 | mse_loss=8.26137
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.48565 | best_loss=12.48565
Epoch 1/80: current_loss=7.77932 | best_loss=7.77932
Epoch 2/80: current_loss=7.81379 | best_loss=7.77932
Epoch 3/80: current_loss=7.87552 | best_loss=7.77932
Epoch 4/80: current_loss=7.81503 | best_loss=7.77932
Epoch 5/80: current_loss=7.74672 | best_loss=7.74672
Epoch 6/80: current_loss=8.52030 | best_loss=7.74672
Epoch 7/80: current_loss=7.99487 | best_loss=7.74672
Epoch 8/80: current_loss=7.82734 | best_loss=7.74672
Epoch 9/80: current_loss=7.73423 | best_loss=7.73423
Epoch 10/80: current_loss=9.63727 | best_loss=7.73423
Epoch 11/80: current_loss=7.75706 | best_loss=7.73423
Epoch 12/80: current_loss=8.71382 | best_loss=7.73423
Epoch 13/80: current_loss=8.14127 | best_loss=7.73423
Epoch 14/80: current_loss=10.09970 | best_loss=7.73423
Epoch 15/80: current_loss=12.04680 | best_loss=7.73423
Epoch 16/80: current_loss=9.96658 | best_loss=7.73423
Epoch 17/80: current_loss=10.36081 | best_loss=7.73423
Epoch 18/80: current_loss=7.71166 | best_loss=7.71166
Epoch 19/80: current_loss=10.54972 | best_loss=7.71166
Epoch 20/80: current_loss=7.75046 | best_loss=7.71166
Epoch 21/80: current_loss=12.04978 | best_loss=7.71166
Epoch 22/80: current_loss=12.87835 | best_loss=7.71166
Epoch 23/80: current_loss=8.57039 | best_loss=7.71166
Epoch 24/80: current_loss=9.42737 | best_loss=7.71166
Epoch 25/80: current_loss=8.70142 | best_loss=7.71166
Epoch 26/80: current_loss=8.66065 | best_loss=7.71166
Epoch 27/80: current_loss=8.81169 | best_loss=7.71166
Epoch 28/80: current_loss=15.60671 | best_loss=7.71166
Epoch 29/80: current_loss=12.06276 | best_loss=7.71166
Epoch 30/80: current_loss=10.87876 | best_loss=7.71166
Epoch 31/80: current_loss=7.72479 | best_loss=7.71166
Epoch 32/80: current_loss=9.50694 | best_loss=7.71166
Epoch 33/80: current_loss=10.76560 | best_loss=7.71166
Epoch 34/80: current_loss=7.69133 | best_loss=7.69133
Epoch 35/80: current_loss=11.54172 | best_loss=7.69133
Epoch 36/80: current_loss=7.69635 | best_loss=7.69133
Epoch 37/80: current_loss=8.06884 | best_loss=7.69133
Epoch 38/80: current_loss=7.69344 | best_loss=7.69133
Epoch 39/80: current_loss=7.95211 | best_loss=7.69133
Epoch 40/80: current_loss=9.58998 | best_loss=7.69133
Epoch 41/80: current_loss=7.87170 | best_loss=7.69133
Epoch 42/80: current_loss=8.07875 | best_loss=7.69133
Epoch 43/80: current_loss=8.67247 | best_loss=7.69133
Epoch 44/80: current_loss=11.61504 | best_loss=7.69133
Epoch 45/80: current_loss=9.70702 | best_loss=7.69133
Epoch 46/80: current_loss=7.68422 | best_loss=7.68422
Epoch 47/80: current_loss=10.12235 | best_loss=7.68422
Epoch 48/80: current_loss=7.93486 | best_loss=7.68422
Epoch 49/80: current_loss=8.81733 | best_loss=7.68422
Epoch 50/80: current_loss=8.57361 | best_loss=7.68422
Epoch 51/80: current_loss=7.79616 | best_loss=7.68422
Epoch 52/80: current_loss=7.71393 | best_loss=7.68422
Epoch 53/80: current_loss=15.15214 | best_loss=7.68422
Epoch 54/80: current_loss=12.50816 | best_loss=7.68422
Epoch 55/80: current_loss=8.66365 | best_loss=7.68422
Epoch 56/80: current_loss=8.76594 | best_loss=7.68422
Epoch 57/80: current_loss=7.85154 | best_loss=7.68422
Epoch 58/80: current_loss=8.07994 | best_loss=7.68422
Epoch 59/80: current_loss=8.22640 | best_loss=7.68422
Epoch 60/80: current_loss=18.20021 | best_loss=7.68422
Epoch 61/80: current_loss=7.80724 | best_loss=7.68422
Epoch 62/80: current_loss=12.25102 | best_loss=7.68422
Epoch 63/80: current_loss=8.14120 | best_loss=7.68422
Epoch 64/80: current_loss=11.62168 | best_loss=7.68422
Epoch 65/80: current_loss=15.94572 | best_loss=7.68422
Epoch 66/80: current_loss=8.04023 | best_loss=7.68422
Early Stopping at epoch 66
      explained_var=-0.00069 | mse_loss=7.86105
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.02070| avg_loss=7.98141
----------------------------------------------


----------------------------------------------
Params for Trial 28
{'learning_rate': 0.1, 'weight_decay': 0.0024322036542309957, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.79851 | best_loss=11.79851
Epoch 1/80: current_loss=9.72630 | best_loss=9.72630
Epoch 2/80: current_loss=10.31721 | best_loss=9.72630
Epoch 3/80: current_loss=9.75026 | best_loss=9.72630
Epoch 4/80: current_loss=9.74171 | best_loss=9.72630
Epoch 5/80: current_loss=13.34887 | best_loss=9.72630
Epoch 6/80: current_loss=10.39076 | best_loss=9.72630
Epoch 7/80: current_loss=8.26479 | best_loss=8.26479
Epoch 8/80: current_loss=10.90204 | best_loss=8.26479
Epoch 9/80: current_loss=10.28458 | best_loss=8.26479
Epoch 10/80: current_loss=13.26775 | best_loss=8.26479
Epoch 11/80: current_loss=8.45306 | best_loss=8.26479
Epoch 12/80: current_loss=8.84750 | best_loss=8.26479
Epoch 13/80: current_loss=8.52789 | best_loss=8.26479
Epoch 14/80: current_loss=9.39514 | best_loss=8.26479
Epoch 15/80: current_loss=11.56883 | best_loss=8.26479
Epoch 16/80: current_loss=11.28338 | best_loss=8.26479
Epoch 17/80: current_loss=8.67892 | best_loss=8.26479
Epoch 18/80: current_loss=9.40392 | best_loss=8.26479
Epoch 19/80: current_loss=21.18872 | best_loss=8.26479
Epoch 20/80: current_loss=12.06264 | best_loss=8.26479
Epoch 21/80: current_loss=12.60776 | best_loss=8.26479
Epoch 22/80: current_loss=8.94191 | best_loss=8.26479
Epoch 23/80: current_loss=9.65736 | best_loss=8.26479
Epoch 24/80: current_loss=13.00979 | best_loss=8.26479
Epoch 25/80: current_loss=9.80387 | best_loss=8.26479
Epoch 26/80: current_loss=7.82168 | best_loss=7.82168
Epoch 27/80: current_loss=8.44093 | best_loss=7.82168
Epoch 28/80: current_loss=11.74287 | best_loss=7.82168
Epoch 29/80: current_loss=8.15898 | best_loss=7.82168
Epoch 30/80: current_loss=8.80676 | best_loss=7.82168
Epoch 31/80: current_loss=11.23084 | best_loss=7.82168
Epoch 32/80: current_loss=7.90519 | best_loss=7.82168
Epoch 33/80: current_loss=20.24769 | best_loss=7.82168
Epoch 34/80: current_loss=26.84293 | best_loss=7.82168
Epoch 35/80: current_loss=18.07807 | best_loss=7.82168
Epoch 36/80: current_loss=8.13089 | best_loss=7.82168
Epoch 37/80: current_loss=8.34953 | best_loss=7.82168
Epoch 38/80: current_loss=10.43689 | best_loss=7.82168
Epoch 39/80: current_loss=8.09065 | best_loss=7.82168
Epoch 40/80: current_loss=11.70654 | best_loss=7.82168
Epoch 41/80: current_loss=23.70957 | best_loss=7.82168
Epoch 42/80: current_loss=22.57346 | best_loss=7.82168
Epoch 43/80: current_loss=10.08358 | best_loss=7.82168
Epoch 44/80: current_loss=9.44820 | best_loss=7.82168
Epoch 45/80: current_loss=37.38644 | best_loss=7.82168
Epoch 46/80: current_loss=13.64014 | best_loss=7.82168
Early Stopping at epoch 46
      explained_var=-0.00358 | mse_loss=7.62429
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.40564 | best_loss=8.40564
Epoch 1/80: current_loss=9.28728 | best_loss=8.40564
Epoch 2/80: current_loss=10.02914 | best_loss=8.40564
Epoch 3/80: current_loss=8.62653 | best_loss=8.40564
Epoch 4/80: current_loss=11.42040 | best_loss=8.40564
Epoch 5/80: current_loss=10.74523 | best_loss=8.40564
Epoch 6/80: current_loss=15.57883 | best_loss=8.40564
Epoch 7/80: current_loss=8.49747 | best_loss=8.40564
Epoch 8/80: current_loss=11.55977 | best_loss=8.40564
Epoch 9/80: current_loss=8.25124 | best_loss=8.25124
Epoch 10/80: current_loss=8.35562 | best_loss=8.25124
Epoch 11/80: current_loss=9.92598 | best_loss=8.25124
Epoch 12/80: current_loss=9.49109 | best_loss=8.25124
Epoch 13/80: current_loss=15.23527 | best_loss=8.25124
Epoch 14/80: current_loss=9.72495 | best_loss=8.25124
Epoch 15/80: current_loss=8.32332 | best_loss=8.25124
Epoch 16/80: current_loss=14.84481 | best_loss=8.25124
Epoch 17/80: current_loss=11.37135 | best_loss=8.25124
Epoch 18/80: current_loss=8.35886 | best_loss=8.25124
Epoch 19/80: current_loss=12.52337 | best_loss=8.25124
Epoch 20/80: current_loss=9.51028 | best_loss=8.25124
Epoch 21/80: current_loss=17.41151 | best_loss=8.25124
Epoch 22/80: current_loss=12.44954 | best_loss=8.25124
Epoch 23/80: current_loss=8.31305 | best_loss=8.25124
Epoch 24/80: current_loss=9.05647 | best_loss=8.25124
Epoch 25/80: current_loss=10.74013 | best_loss=8.25124
Epoch 26/80: current_loss=8.23760 | best_loss=8.23760
Epoch 27/80: current_loss=8.26335 | best_loss=8.23760
Epoch 28/80: current_loss=8.38062 | best_loss=8.23760
Epoch 29/80: current_loss=8.37822 | best_loss=8.23760
Epoch 30/80: current_loss=8.36333 | best_loss=8.23760
Epoch 31/80: current_loss=9.27182 | best_loss=8.23760
Epoch 32/80: current_loss=9.79230 | best_loss=8.23760
Epoch 33/80: current_loss=15.66813 | best_loss=8.23760
Epoch 34/80: current_loss=9.09312 | best_loss=8.23760
Epoch 35/80: current_loss=9.15602 | best_loss=8.23760
Epoch 36/80: current_loss=8.62020 | best_loss=8.23760
Epoch 37/80: current_loss=8.63855 | best_loss=8.23760
Epoch 38/80: current_loss=8.33773 | best_loss=8.23760
Epoch 39/80: current_loss=8.45894 | best_loss=8.23760
Epoch 40/80: current_loss=8.55584 | best_loss=8.23760
Epoch 41/80: current_loss=8.83017 | best_loss=8.23760
Epoch 42/80: current_loss=12.93296 | best_loss=8.23760
Epoch 43/80: current_loss=9.63212 | best_loss=8.23760
Epoch 44/80: current_loss=8.60758 | best_loss=8.23760
Epoch 45/80: current_loss=9.79137 | best_loss=8.23760
Epoch 46/80: current_loss=8.41005 | best_loss=8.23760
Early Stopping at epoch 46
      explained_var=0.01634 | mse_loss=8.10755
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.74474 | best_loss=9.74474
Epoch 1/80: current_loss=10.81654 | best_loss=9.74474
Epoch 2/80: current_loss=9.41696 | best_loss=9.41696
Epoch 3/80: current_loss=8.73204 | best_loss=8.73204
Epoch 4/80: current_loss=8.60111 | best_loss=8.60111
Epoch 5/80: current_loss=10.24540 | best_loss=8.60111
Epoch 6/80: current_loss=10.78400 | best_loss=8.60111
Epoch 7/80: current_loss=8.82439 | best_loss=8.60111
Epoch 8/80: current_loss=12.51493 | best_loss=8.60111
Epoch 9/80: current_loss=14.83678 | best_loss=8.60111
Epoch 10/80: current_loss=8.85433 | best_loss=8.60111
Epoch 11/80: current_loss=11.11068 | best_loss=8.60111
Epoch 12/80: current_loss=9.00650 | best_loss=8.60111
Epoch 13/80: current_loss=15.09860 | best_loss=8.60111
Epoch 14/80: current_loss=10.32223 | best_loss=8.60111
Epoch 15/80: current_loss=14.38706 | best_loss=8.60111
Epoch 16/80: current_loss=10.80060 | best_loss=8.60111
Epoch 17/80: current_loss=13.87963 | best_loss=8.60111
Epoch 18/80: current_loss=11.77925 | best_loss=8.60111
Epoch 19/80: current_loss=10.07921 | best_loss=8.60111
Epoch 20/80: current_loss=8.44740 | best_loss=8.44740
Epoch 21/80: current_loss=23.46082 | best_loss=8.44740
Epoch 22/80: current_loss=13.14833 | best_loss=8.44740
Epoch 23/80: current_loss=18.54211 | best_loss=8.44740
Epoch 24/80: current_loss=15.00154 | best_loss=8.44740
Epoch 25/80: current_loss=23.30943 | best_loss=8.44740
Epoch 26/80: current_loss=10.14681 | best_loss=8.44740
Epoch 27/80: current_loss=9.20650 | best_loss=8.44740
Epoch 28/80: current_loss=11.78392 | best_loss=8.44740
Epoch 29/80: current_loss=9.34487 | best_loss=8.44740
Epoch 30/80: current_loss=11.75736 | best_loss=8.44740
Epoch 31/80: current_loss=8.81837 | best_loss=8.44740
Epoch 32/80: current_loss=9.08396 | best_loss=8.44740
Epoch 33/80: current_loss=11.40407 | best_loss=8.44740
Epoch 34/80: current_loss=10.87907 | best_loss=8.44740
Epoch 35/80: current_loss=15.20420 | best_loss=8.44740
Epoch 36/80: current_loss=8.62247 | best_loss=8.44740
Epoch 37/80: current_loss=18.83900 | best_loss=8.44740
Epoch 38/80: current_loss=9.76454 | best_loss=8.44740
Epoch 39/80: current_loss=9.00951 | best_loss=8.44740
Epoch 40/80: current_loss=11.39302 | best_loss=8.44740
Early Stopping at epoch 40
      explained_var=0.03597 | mse_loss=8.19692
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.78868 | best_loss=10.78868
Epoch 1/80: current_loss=9.85159 | best_loss=9.85159
Epoch 2/80: current_loss=9.07983 | best_loss=9.07983
Epoch 3/80: current_loss=8.54441 | best_loss=8.54441
Epoch 4/80: current_loss=8.89759 | best_loss=8.54441
Epoch 5/80: current_loss=8.35450 | best_loss=8.35450
Epoch 6/80: current_loss=10.72836 | best_loss=8.35450
Epoch 7/80: current_loss=8.33963 | best_loss=8.33963
Epoch 8/80: current_loss=16.17415 | best_loss=8.33963
Epoch 9/80: current_loss=15.62999 | best_loss=8.33963
Epoch 10/80: current_loss=8.26542 | best_loss=8.26542
Epoch 11/80: current_loss=10.62478 | best_loss=8.26542
Epoch 12/80: current_loss=8.49350 | best_loss=8.26542
Epoch 13/80: current_loss=8.81128 | best_loss=8.26542
Epoch 14/80: current_loss=13.37137 | best_loss=8.26542
Epoch 15/80: current_loss=8.45820 | best_loss=8.26542
Epoch 16/80: current_loss=8.26535 | best_loss=8.26535
Epoch 17/80: current_loss=10.79057 | best_loss=8.26535
Epoch 18/80: current_loss=8.78771 | best_loss=8.26535
Epoch 19/80: current_loss=8.29813 | best_loss=8.26535
Epoch 20/80: current_loss=8.34320 | best_loss=8.26535
Epoch 21/80: current_loss=8.39014 | best_loss=8.26535
Epoch 22/80: current_loss=11.57988 | best_loss=8.26535
Epoch 23/80: current_loss=9.24010 | best_loss=8.26535
Epoch 24/80: current_loss=22.96238 | best_loss=8.26535
Epoch 25/80: current_loss=19.84749 | best_loss=8.26535
Epoch 26/80: current_loss=9.91583 | best_loss=8.26535
Epoch 27/80: current_loss=13.18850 | best_loss=8.26535
Epoch 28/80: current_loss=15.22641 | best_loss=8.26535
Epoch 29/80: current_loss=8.73668 | best_loss=8.26535
Epoch 30/80: current_loss=8.64203 | best_loss=8.26535
Epoch 31/80: current_loss=11.70149 | best_loss=8.26535
Epoch 32/80: current_loss=8.73336 | best_loss=8.26535
Epoch 33/80: current_loss=8.61598 | best_loss=8.26535
Epoch 34/80: current_loss=8.80530 | best_loss=8.26535
Epoch 35/80: current_loss=8.22763 | best_loss=8.22763
Epoch 36/80: current_loss=11.05516 | best_loss=8.22763
Epoch 37/80: current_loss=12.24244 | best_loss=8.22763
Epoch 38/80: current_loss=9.49936 | best_loss=8.22763
Epoch 39/80: current_loss=8.42764 | best_loss=8.22763
Epoch 40/80: current_loss=10.14916 | best_loss=8.22763
Epoch 41/80: current_loss=8.31758 | best_loss=8.22763
Epoch 42/80: current_loss=9.08254 | best_loss=8.22763
Epoch 43/80: current_loss=9.51707 | best_loss=8.22763
Epoch 44/80: current_loss=11.90795 | best_loss=8.22763
Epoch 45/80: current_loss=9.17195 | best_loss=8.22763
Epoch 46/80: current_loss=11.59621 | best_loss=8.22763
Epoch 47/80: current_loss=8.43998 | best_loss=8.22763
Epoch 48/80: current_loss=8.40582 | best_loss=8.22763
Epoch 49/80: current_loss=8.54424 | best_loss=8.22763
Epoch 50/80: current_loss=8.95003 | best_loss=8.22763
Epoch 51/80: current_loss=9.33033 | best_loss=8.22763
Epoch 52/80: current_loss=10.05471 | best_loss=8.22763
Epoch 53/80: current_loss=9.41796 | best_loss=8.22763
Epoch 54/80: current_loss=8.72915 | best_loss=8.22763
Epoch 55/80: current_loss=8.37609 | best_loss=8.22763
Early Stopping at epoch 55
      explained_var=0.00237 | mse_loss=8.32813
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.34156 | best_loss=9.34156
Epoch 1/80: current_loss=8.77496 | best_loss=8.77496
Epoch 2/80: current_loss=8.60474 | best_loss=8.60474
Epoch 3/80: current_loss=8.95300 | best_loss=8.60474
Epoch 4/80: current_loss=10.71940 | best_loss=8.60474
Epoch 5/80: current_loss=17.72588 | best_loss=8.60474
Epoch 6/80: current_loss=8.06305 | best_loss=8.06305
Epoch 7/80: current_loss=8.20072 | best_loss=8.06305
Epoch 8/80: current_loss=7.81280 | best_loss=7.81280
Epoch 9/80: current_loss=8.23898 | best_loss=7.81280
Epoch 10/80: current_loss=8.89551 | best_loss=7.81280
Epoch 11/80: current_loss=12.47278 | best_loss=7.81280
Epoch 12/80: current_loss=8.63592 | best_loss=7.81280
Epoch 13/80: current_loss=8.48176 | best_loss=7.81280
Epoch 14/80: current_loss=16.20081 | best_loss=7.81280
Epoch 15/80: current_loss=11.60858 | best_loss=7.81280
Epoch 16/80: current_loss=10.47881 | best_loss=7.81280
Epoch 17/80: current_loss=8.67656 | best_loss=7.81280
Epoch 18/80: current_loss=8.21050 | best_loss=7.81280
Epoch 19/80: current_loss=7.71042 | best_loss=7.71042
Epoch 20/80: current_loss=12.55078 | best_loss=7.71042
Epoch 21/80: current_loss=18.47258 | best_loss=7.71042
Epoch 22/80: current_loss=17.94402 | best_loss=7.71042
Epoch 23/80: current_loss=13.93703 | best_loss=7.71042
Epoch 24/80: current_loss=9.93783 | best_loss=7.71042
Epoch 25/80: current_loss=8.86702 | best_loss=7.71042
Epoch 26/80: current_loss=9.42148 | best_loss=7.71042
Epoch 27/80: current_loss=8.19833 | best_loss=7.71042
Epoch 28/80: current_loss=8.32881 | best_loss=7.71042
Epoch 29/80: current_loss=7.76267 | best_loss=7.71042
Epoch 30/80: current_loss=11.74708 | best_loss=7.71042
Epoch 31/80: current_loss=15.19859 | best_loss=7.71042
Epoch 32/80: current_loss=11.16066 | best_loss=7.71042
Epoch 33/80: current_loss=8.13694 | best_loss=7.71042
Epoch 34/80: current_loss=8.13761 | best_loss=7.71042
Epoch 35/80: current_loss=7.73904 | best_loss=7.71042
Epoch 36/80: current_loss=9.15988 | best_loss=7.71042
Epoch 37/80: current_loss=7.74041 | best_loss=7.71042
Epoch 38/80: current_loss=8.71218 | best_loss=7.71042
Epoch 39/80: current_loss=10.04187 | best_loss=7.71042
Early Stopping at epoch 39
      explained_var=-0.00010 | mse_loss=7.89695
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=0.01020| avg_loss=8.03077
----------------------------------------------


----------------------------------------------
Params for Trial 29
{'learning_rate': 0.1, 'weight_decay': 0.0013876994804289905, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.02209 | best_loss=8.02209
Epoch 1/80: current_loss=8.96253 | best_loss=8.02209
Epoch 2/80: current_loss=12.37254 | best_loss=8.02209
Epoch 3/80: current_loss=8.96548 | best_loss=8.02209
Epoch 4/80: current_loss=10.85076 | best_loss=8.02209
Epoch 5/80: current_loss=9.13648 | best_loss=8.02209
Epoch 6/80: current_loss=9.18947 | best_loss=8.02209
Epoch 7/80: current_loss=8.39216 | best_loss=8.02209
Epoch 8/80: current_loss=8.12162 | best_loss=8.02209
Epoch 9/80: current_loss=11.22898 | best_loss=8.02209
Epoch 10/80: current_loss=8.29687 | best_loss=8.02209
Epoch 11/80: current_loss=10.88809 | best_loss=8.02209
Epoch 12/80: current_loss=10.85156 | best_loss=8.02209
Epoch 13/80: current_loss=9.06817 | best_loss=8.02209
Epoch 14/80: current_loss=13.19708 | best_loss=8.02209
Epoch 15/80: current_loss=8.99402 | best_loss=8.02209
Epoch 16/80: current_loss=11.92690 | best_loss=8.02209
Epoch 17/80: current_loss=7.90257 | best_loss=7.90257
Epoch 18/80: current_loss=7.79519 | best_loss=7.79519
Epoch 19/80: current_loss=8.57512 | best_loss=7.79519
Epoch 20/80: current_loss=9.03601 | best_loss=7.79519
Epoch 21/80: current_loss=7.79312 | best_loss=7.79312
Epoch 22/80: current_loss=8.62826 | best_loss=7.79312
Epoch 23/80: current_loss=17.26017 | best_loss=7.79312
Epoch 24/80: current_loss=10.51993 | best_loss=7.79312
Epoch 25/80: current_loss=10.10038 | best_loss=7.79312
Epoch 26/80: current_loss=20.52762 | best_loss=7.79312
Epoch 27/80: current_loss=9.71229 | best_loss=7.79312
Epoch 28/80: current_loss=9.54614 | best_loss=7.79312
Epoch 29/80: current_loss=8.84045 | best_loss=7.79312
Epoch 30/80: current_loss=7.79901 | best_loss=7.79312
Epoch 31/80: current_loss=14.88296 | best_loss=7.79312
Epoch 32/80: current_loss=7.85156 | best_loss=7.79312
Epoch 33/80: current_loss=7.86004 | best_loss=7.79312
Epoch 34/80: current_loss=7.83978 | best_loss=7.79312
Epoch 35/80: current_loss=7.76861 | best_loss=7.76861
Epoch 36/80: current_loss=8.15071 | best_loss=7.76861
Epoch 37/80: current_loss=7.76412 | best_loss=7.76412
Epoch 38/80: current_loss=8.25798 | best_loss=7.76412
Epoch 39/80: current_loss=9.20945 | best_loss=7.76412
Epoch 40/80: current_loss=7.94010 | best_loss=7.76412
Epoch 41/80: current_loss=7.80743 | best_loss=7.76412
Epoch 42/80: current_loss=9.30437 | best_loss=7.76412
Epoch 43/80: current_loss=8.05597 | best_loss=7.76412
Epoch 44/80: current_loss=8.02921 | best_loss=7.76412
Epoch 45/80: current_loss=9.82179 | best_loss=7.76412
Epoch 46/80: current_loss=7.97365 | best_loss=7.76412
Epoch 47/80: current_loss=14.42314 | best_loss=7.76412
Epoch 48/80: current_loss=7.80236 | best_loss=7.76412
Epoch 49/80: current_loss=8.92233 | best_loss=7.76412
Epoch 50/80: current_loss=9.10157 | best_loss=7.76412
Epoch 51/80: current_loss=8.02706 | best_loss=7.76412
Epoch 52/80: current_loss=8.91652 | best_loss=7.76412
Epoch 53/80: current_loss=11.45289 | best_loss=7.76412
Epoch 54/80: current_loss=7.83269 | best_loss=7.76412
Epoch 55/80: current_loss=8.78139 | best_loss=7.76412
Epoch 56/80: current_loss=7.76844 | best_loss=7.76412
Epoch 57/80: current_loss=11.67739 | best_loss=7.76412
Early Stopping at epoch 57
      explained_var=0.00446 | mse_loss=7.60417
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.09316 | best_loss=13.09316
Epoch 1/80: current_loss=9.89965 | best_loss=9.89965
Epoch 2/80: current_loss=11.60117 | best_loss=9.89965
Epoch 3/80: current_loss=12.59545 | best_loss=9.89965
Epoch 4/80: current_loss=9.23459 | best_loss=9.23459
Epoch 5/80: current_loss=8.56287 | best_loss=8.56287
Epoch 6/80: current_loss=9.72131 | best_loss=8.56287
Epoch 7/80: current_loss=10.41896 | best_loss=8.56287
Epoch 8/80: current_loss=9.71302 | best_loss=8.56287
Epoch 9/80: current_loss=13.49773 | best_loss=8.56287
Epoch 10/80: current_loss=9.12406 | best_loss=8.56287
Epoch 11/80: current_loss=8.54321 | best_loss=8.54321
Epoch 12/80: current_loss=10.22210 | best_loss=8.54321
Epoch 13/80: current_loss=11.17977 | best_loss=8.54321
Epoch 14/80: current_loss=12.62819 | best_loss=8.54321
Epoch 15/80: current_loss=9.19078 | best_loss=8.54321
Epoch 16/80: current_loss=11.69888 | best_loss=8.54321
Epoch 17/80: current_loss=21.02720 | best_loss=8.54321
Epoch 18/80: current_loss=9.83034 | best_loss=8.54321
Epoch 19/80: current_loss=14.09041 | best_loss=8.54321
Epoch 20/80: current_loss=10.49708 | best_loss=8.54321
Epoch 21/80: current_loss=9.06733 | best_loss=8.54321
Epoch 22/80: current_loss=10.17686 | best_loss=8.54321
Epoch 23/80: current_loss=10.20533 | best_loss=8.54321
Epoch 24/80: current_loss=8.47059 | best_loss=8.47059
Epoch 25/80: current_loss=11.09370 | best_loss=8.47059
Epoch 26/80: current_loss=9.22773 | best_loss=8.47059
Epoch 27/80: current_loss=13.21945 | best_loss=8.47059
Epoch 28/80: current_loss=9.39274 | best_loss=8.47059
Epoch 29/80: current_loss=9.16934 | best_loss=8.47059
Epoch 30/80: current_loss=10.27283 | best_loss=8.47059
Epoch 31/80: current_loss=8.24009 | best_loss=8.24009
Epoch 32/80: current_loss=10.68227 | best_loss=8.24009
Epoch 33/80: current_loss=10.18092 | best_loss=8.24009
Epoch 34/80: current_loss=11.07756 | best_loss=8.24009
Epoch 35/80: current_loss=10.21424 | best_loss=8.24009
Epoch 36/80: current_loss=11.18894 | best_loss=8.24009
Epoch 37/80: current_loss=10.77868 | best_loss=8.24009
Epoch 38/80: current_loss=18.36516 | best_loss=8.24009
Epoch 39/80: current_loss=9.68727 | best_loss=8.24009
Epoch 40/80: current_loss=12.07206 | best_loss=8.24009
Epoch 41/80: current_loss=22.59680 | best_loss=8.24009
Epoch 42/80: current_loss=10.75520 | best_loss=8.24009
Epoch 43/80: current_loss=8.88613 | best_loss=8.24009
Epoch 44/80: current_loss=9.08303 | best_loss=8.24009
Epoch 45/80: current_loss=11.79007 | best_loss=8.24009
Epoch 46/80: current_loss=12.60939 | best_loss=8.24009
Epoch 47/80: current_loss=10.68226 | best_loss=8.24009
Epoch 48/80: current_loss=9.35084 | best_loss=8.24009
Epoch 49/80: current_loss=8.05137 | best_loss=8.05137
Epoch 50/80: current_loss=12.36645 | best_loss=8.05137
Epoch 51/80: current_loss=9.63922 | best_loss=8.05137
Epoch 52/80: current_loss=9.77414 | best_loss=8.05137
Epoch 53/80: current_loss=10.97236 | best_loss=8.05137
Epoch 54/80: current_loss=17.61890 | best_loss=8.05137
Epoch 55/80: current_loss=12.44438 | best_loss=8.05137
Epoch 56/80: current_loss=9.90204 | best_loss=8.05137
Epoch 57/80: current_loss=11.21806 | best_loss=8.05137
Epoch 58/80: current_loss=10.83033 | best_loss=8.05137
Epoch 59/80: current_loss=8.49692 | best_loss=8.05137
Epoch 60/80: current_loss=9.48212 | best_loss=8.05137
Epoch 61/80: current_loss=9.09493 | best_loss=8.05137
Epoch 62/80: current_loss=10.36765 | best_loss=8.05137
Epoch 63/80: current_loss=7.22386 | best_loss=7.22386
Epoch 64/80: current_loss=9.33345 | best_loss=7.22386
Epoch 65/80: current_loss=11.90070 | best_loss=7.22386
Epoch 66/80: current_loss=8.11731 | best_loss=7.22386
Epoch 67/80: current_loss=7.51412 | best_loss=7.22386
Epoch 68/80: current_loss=9.23643 | best_loss=7.22386
Epoch 69/80: current_loss=9.62059 | best_loss=7.22386
Epoch 70/80: current_loss=8.97428 | best_loss=7.22386
Epoch 71/80: current_loss=11.73478 | best_loss=7.22386
Epoch 72/80: current_loss=7.71794 | best_loss=7.22386
Epoch 73/80: current_loss=9.75722 | best_loss=7.22386
Epoch 74/80: current_loss=12.89014 | best_loss=7.22386
Epoch 75/80: current_loss=12.26678 | best_loss=7.22386
Epoch 76/80: current_loss=8.18291 | best_loss=7.22386
Epoch 77/80: current_loss=7.60577 | best_loss=7.22386
Epoch 78/80: current_loss=9.04662 | best_loss=7.22386
Epoch 79/80: current_loss=11.54716 | best_loss=7.22386
      explained_var=0.11470 | mse_loss=7.22722
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.84935 | best_loss=8.84935
Epoch 1/80: current_loss=11.47384 | best_loss=8.84935
Epoch 2/80: current_loss=8.97123 | best_loss=8.84935
Epoch 3/80: current_loss=18.54296 | best_loss=8.84935
Epoch 4/80: current_loss=9.65472 | best_loss=8.84935
Epoch 5/80: current_loss=10.19363 | best_loss=8.84935
Epoch 6/80: current_loss=8.77955 | best_loss=8.77955
Epoch 7/80: current_loss=10.63310 | best_loss=8.77955
Epoch 8/80: current_loss=13.79337 | best_loss=8.77955
Epoch 9/80: current_loss=12.71019 | best_loss=8.77955
Epoch 10/80: current_loss=9.96131 | best_loss=8.77955
Epoch 11/80: current_loss=9.32127 | best_loss=8.77955
Epoch 12/80: current_loss=13.36878 | best_loss=8.77955
Epoch 13/80: current_loss=12.85744 | best_loss=8.77955
Epoch 14/80: current_loss=9.64333 | best_loss=8.77955
Epoch 15/80: current_loss=8.67942 | best_loss=8.67942
Epoch 16/80: current_loss=12.40681 | best_loss=8.67942
Epoch 17/80: current_loss=9.72982 | best_loss=8.67942
Epoch 18/80: current_loss=12.90188 | best_loss=8.67942
Epoch 19/80: current_loss=11.89735 | best_loss=8.67942
Epoch 20/80: current_loss=12.08546 | best_loss=8.67942
Epoch 21/80: current_loss=8.56446 | best_loss=8.56446
Epoch 22/80: current_loss=10.37430 | best_loss=8.56446
Epoch 23/80: current_loss=12.11774 | best_loss=8.56446
Epoch 24/80: current_loss=12.30664 | best_loss=8.56446
Epoch 25/80: current_loss=11.33885 | best_loss=8.56446
Epoch 26/80: current_loss=9.71840 | best_loss=8.56446
Epoch 27/80: current_loss=11.19271 | best_loss=8.56446
Epoch 28/80: current_loss=24.36299 | best_loss=8.56446
Epoch 29/80: current_loss=10.61694 | best_loss=8.56446
Epoch 30/80: current_loss=18.20507 | best_loss=8.56446
Epoch 31/80: current_loss=8.14758 | best_loss=8.14758
Epoch 32/80: current_loss=19.59519 | best_loss=8.14758
Epoch 33/80: current_loss=10.07204 | best_loss=8.14758
Epoch 34/80: current_loss=7.96031 | best_loss=7.96031
Epoch 35/80: current_loss=11.27996 | best_loss=7.96031
Epoch 36/80: current_loss=8.31584 | best_loss=7.96031
Epoch 37/80: current_loss=17.77034 | best_loss=7.96031
Epoch 38/80: current_loss=9.94416 | best_loss=7.96031
Epoch 39/80: current_loss=11.56915 | best_loss=7.96031
Epoch 40/80: current_loss=8.95964 | best_loss=7.96031
Epoch 41/80: current_loss=7.80648 | best_loss=7.80648
Epoch 42/80: current_loss=11.85295 | best_loss=7.80648
Epoch 43/80: current_loss=14.15994 | best_loss=7.80648
Epoch 44/80: current_loss=8.10282 | best_loss=7.80648
Epoch 45/80: current_loss=8.61527 | best_loss=7.80648
Epoch 46/80: current_loss=8.54274 | best_loss=7.80648
Epoch 47/80: current_loss=9.53573 | best_loss=7.80648
Epoch 48/80: current_loss=9.98489 | best_loss=7.80648
Epoch 49/80: current_loss=12.12588 | best_loss=7.80648
Epoch 50/80: current_loss=13.24699 | best_loss=7.80648
Epoch 51/80: current_loss=9.97227 | best_loss=7.80648
Epoch 52/80: current_loss=13.05888 | best_loss=7.80648
Epoch 53/80: current_loss=11.83376 | best_loss=7.80648
Epoch 54/80: current_loss=11.20860 | best_loss=7.80648
Epoch 55/80: current_loss=9.84210 | best_loss=7.80648
Epoch 56/80: current_loss=9.16343 | best_loss=7.80648
Epoch 57/80: current_loss=9.32325 | best_loss=7.80648
Epoch 58/80: current_loss=10.46738 | best_loss=7.80648
Epoch 59/80: current_loss=10.64116 | best_loss=7.80648
Epoch 60/80: current_loss=8.37563 | best_loss=7.80648
Epoch 61/80: current_loss=14.46742 | best_loss=7.80648
Early Stopping at epoch 61
      explained_var=0.10768 | mse_loss=7.58235
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.18150 | best_loss=9.18150
Epoch 1/80: current_loss=10.03067 | best_loss=9.18150
Epoch 2/80: current_loss=9.02618 | best_loss=9.02618
Epoch 3/80: current_loss=9.80119 | best_loss=9.02618
Epoch 4/80: current_loss=12.71236 | best_loss=9.02618
Epoch 5/80: current_loss=10.44617 | best_loss=9.02618
Epoch 6/80: current_loss=11.95119 | best_loss=9.02618
Epoch 7/80: current_loss=9.58207 | best_loss=9.02618
Epoch 8/80: current_loss=10.08815 | best_loss=9.02618
Epoch 9/80: current_loss=10.53355 | best_loss=9.02618
Epoch 10/80: current_loss=10.65216 | best_loss=9.02618
Epoch 11/80: current_loss=8.26226 | best_loss=8.26226
Epoch 12/80: current_loss=11.47131 | best_loss=8.26226
Epoch 13/80: current_loss=8.47933 | best_loss=8.26226
Epoch 14/80: current_loss=10.50783 | best_loss=8.26226
Epoch 15/80: current_loss=8.34026 | best_loss=8.26226
Epoch 16/80: current_loss=10.76459 | best_loss=8.26226
Epoch 17/80: current_loss=8.49637 | best_loss=8.26226
Epoch 18/80: current_loss=8.33772 | best_loss=8.26226
Epoch 19/80: current_loss=9.55043 | best_loss=8.26226
Epoch 20/80: current_loss=8.31748 | best_loss=8.26226
Epoch 21/80: current_loss=8.26182 | best_loss=8.26182
Epoch 22/80: current_loss=10.63990 | best_loss=8.26182
Epoch 23/80: current_loss=17.67013 | best_loss=8.26182
Epoch 24/80: current_loss=14.94143 | best_loss=8.26182
Epoch 25/80: current_loss=9.39209 | best_loss=8.26182
Epoch 26/80: current_loss=8.19136 | best_loss=8.19136
Epoch 27/80: current_loss=8.52878 | best_loss=8.19136
Epoch 28/80: current_loss=12.36219 | best_loss=8.19136
Epoch 29/80: current_loss=10.58611 | best_loss=8.19136
Epoch 30/80: current_loss=11.26891 | best_loss=8.19136
Epoch 31/80: current_loss=9.27181 | best_loss=8.19136
Epoch 32/80: current_loss=10.60171 | best_loss=8.19136
Epoch 33/80: current_loss=10.93873 | best_loss=8.19136
Epoch 34/80: current_loss=9.49007 | best_loss=8.19136
Epoch 35/80: current_loss=8.24454 | best_loss=8.19136
Epoch 36/80: current_loss=9.53460 | best_loss=8.19136
Epoch 37/80: current_loss=9.68236 | best_loss=8.19136
Epoch 38/80: current_loss=8.67025 | best_loss=8.19136
Epoch 39/80: current_loss=17.95081 | best_loss=8.19136
Epoch 40/80: current_loss=9.23528 | best_loss=8.19136
Epoch 41/80: current_loss=8.73647 | best_loss=8.19136
Epoch 42/80: current_loss=11.29984 | best_loss=8.19136
Epoch 43/80: current_loss=11.27895 | best_loss=8.19136
Epoch 44/80: current_loss=26.99067 | best_loss=8.19136
Epoch 45/80: current_loss=8.63836 | best_loss=8.19136
Epoch 46/80: current_loss=8.22148 | best_loss=8.19136
Early Stopping at epoch 46
      explained_var=0.00709 | mse_loss=8.29452
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.82238 | best_loss=7.82238
Epoch 1/80: current_loss=9.16554 | best_loss=7.82238
Epoch 2/80: current_loss=8.48947 | best_loss=7.82238
Epoch 3/80: current_loss=12.58183 | best_loss=7.82238
Epoch 4/80: current_loss=7.66716 | best_loss=7.66716
Epoch 5/80: current_loss=10.55032 | best_loss=7.66716
Epoch 6/80: current_loss=7.94734 | best_loss=7.66716
Epoch 7/80: current_loss=19.75567 | best_loss=7.66716
Epoch 8/80: current_loss=7.91204 | best_loss=7.66716
Epoch 9/80: current_loss=7.74301 | best_loss=7.66716
Epoch 10/80: current_loss=7.73576 | best_loss=7.66716
Epoch 11/80: current_loss=8.09885 | best_loss=7.66716
Epoch 12/80: current_loss=9.87676 | best_loss=7.66716
Epoch 13/80: current_loss=9.34615 | best_loss=7.66716
Epoch 14/80: current_loss=11.41456 | best_loss=7.66716
Epoch 15/80: current_loss=8.14917 | best_loss=7.66716
Epoch 16/80: current_loss=7.64895 | best_loss=7.64895
Epoch 17/80: current_loss=9.88578 | best_loss=7.64895
Epoch 18/80: current_loss=8.23952 | best_loss=7.64895
Epoch 19/80: current_loss=8.00885 | best_loss=7.64895
Epoch 20/80: current_loss=8.53785 | best_loss=7.64895
Epoch 21/80: current_loss=10.57607 | best_loss=7.64895
Epoch 22/80: current_loss=9.52601 | best_loss=7.64895
Epoch 23/80: current_loss=7.87502 | best_loss=7.64895
Epoch 24/80: current_loss=7.77108 | best_loss=7.64895
Epoch 25/80: current_loss=7.96997 | best_loss=7.64895
Epoch 26/80: current_loss=14.03850 | best_loss=7.64895
Epoch 27/80: current_loss=12.75874 | best_loss=7.64895
Epoch 28/80: current_loss=11.00032 | best_loss=7.64895
Epoch 29/80: current_loss=7.68530 | best_loss=7.64895
Epoch 30/80: current_loss=7.94246 | best_loss=7.64895
Epoch 31/80: current_loss=12.27024 | best_loss=7.64895
Epoch 32/80: current_loss=8.29605 | best_loss=7.64895
Epoch 33/80: current_loss=8.74100 | best_loss=7.64895
Epoch 34/80: current_loss=8.39068 | best_loss=7.64895
Epoch 35/80: current_loss=9.15442 | best_loss=7.64895
Epoch 36/80: current_loss=7.78435 | best_loss=7.64895
Early Stopping at epoch 36
      explained_var=0.00386 | mse_loss=7.82683
----------------------------------------------
Average early_stopping_point: 40| avg_exp_var=0.04756| avg_loss=7.70702
----------------------------------------------


----------------------------------------------
Params for Trial 30
{'learning_rate': 0.1, 'weight_decay': 0.0010126588970531215, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.82000 | best_loss=7.82000
Epoch 1/80: current_loss=7.90565 | best_loss=7.82000
Epoch 2/80: current_loss=9.57948 | best_loss=7.82000
Epoch 3/80: current_loss=8.11083 | best_loss=7.82000
Epoch 4/80: current_loss=9.60213 | best_loss=7.82000
Epoch 5/80: current_loss=11.45321 | best_loss=7.82000
Epoch 6/80: current_loss=9.83209 | best_loss=7.82000
Epoch 7/80: current_loss=8.60141 | best_loss=7.82000
Epoch 8/80: current_loss=8.31082 | best_loss=7.82000
Epoch 9/80: current_loss=9.68359 | best_loss=7.82000
Epoch 10/80: current_loss=8.29600 | best_loss=7.82000
Epoch 11/80: current_loss=8.01109 | best_loss=7.82000
Epoch 12/80: current_loss=7.96737 | best_loss=7.82000
Epoch 13/80: current_loss=8.67956 | best_loss=7.82000
Epoch 14/80: current_loss=9.91232 | best_loss=7.82000
Epoch 15/80: current_loss=9.95666 | best_loss=7.82000
Epoch 16/80: current_loss=7.93660 | best_loss=7.82000
Epoch 17/80: current_loss=8.09846 | best_loss=7.82000
Epoch 18/80: current_loss=7.76819 | best_loss=7.76819
Epoch 19/80: current_loss=7.92110 | best_loss=7.76819
Epoch 20/80: current_loss=8.56915 | best_loss=7.76819
Epoch 21/80: current_loss=8.42901 | best_loss=7.76819
Epoch 22/80: current_loss=10.12806 | best_loss=7.76819
Epoch 23/80: current_loss=10.06731 | best_loss=7.76819
Epoch 24/80: current_loss=7.86251 | best_loss=7.76819
Epoch 25/80: current_loss=8.31681 | best_loss=7.76819
Epoch 26/80: current_loss=9.18725 | best_loss=7.76819
Epoch 27/80: current_loss=12.31948 | best_loss=7.76819
Epoch 28/80: current_loss=10.59905 | best_loss=7.76819
Epoch 29/80: current_loss=11.36928 | best_loss=7.76819
Epoch 30/80: current_loss=13.67654 | best_loss=7.76819
Epoch 31/80: current_loss=7.88233 | best_loss=7.76819
Epoch 32/80: current_loss=7.80027 | best_loss=7.76819
Epoch 33/80: current_loss=8.62532 | best_loss=7.76819
Epoch 34/80: current_loss=7.82880 | best_loss=7.76819
Epoch 35/80: current_loss=8.21381 | best_loss=7.76819
Epoch 36/80: current_loss=8.17243 | best_loss=7.76819
Epoch 37/80: current_loss=8.68620 | best_loss=7.76819
Epoch 38/80: current_loss=7.75782 | best_loss=7.75782
Epoch 39/80: current_loss=8.12084 | best_loss=7.75782
Epoch 40/80: current_loss=9.18621 | best_loss=7.75782
Epoch 41/80: current_loss=7.86251 | best_loss=7.75782
Epoch 42/80: current_loss=8.64963 | best_loss=7.75782
Epoch 43/80: current_loss=8.78271 | best_loss=7.75782
Epoch 44/80: current_loss=7.96108 | best_loss=7.75782
Epoch 45/80: current_loss=9.43794 | best_loss=7.75782
Epoch 46/80: current_loss=8.80872 | best_loss=7.75782
Epoch 47/80: current_loss=9.13881 | best_loss=7.75782
Epoch 48/80: current_loss=7.82651 | best_loss=7.75782
Epoch 49/80: current_loss=8.71363 | best_loss=7.75782
Epoch 50/80: current_loss=7.80938 | best_loss=7.75782
Epoch 51/80: current_loss=11.54668 | best_loss=7.75782
Epoch 52/80: current_loss=12.19411 | best_loss=7.75782
Epoch 53/80: current_loss=7.81328 | best_loss=7.75782
Epoch 54/80: current_loss=9.26665 | best_loss=7.75782
Epoch 55/80: current_loss=10.74308 | best_loss=7.75782
Epoch 56/80: current_loss=9.30338 | best_loss=7.75782
Epoch 57/80: current_loss=7.97326 | best_loss=7.75782
Epoch 58/80: current_loss=7.84116 | best_loss=7.75782
Early Stopping at epoch 58
      explained_var=0.00167 | mse_loss=7.57857
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.16865 | best_loss=10.16865
Epoch 1/80: current_loss=8.42771 | best_loss=8.42771
Epoch 2/80: current_loss=12.67166 | best_loss=8.42771
Epoch 3/80: current_loss=9.88752 | best_loss=8.42771
Epoch 4/80: current_loss=9.19595 | best_loss=8.42771
Epoch 5/80: current_loss=8.85489 | best_loss=8.42771
Epoch 6/80: current_loss=9.08409 | best_loss=8.42771
Epoch 7/80: current_loss=8.82344 | best_loss=8.42771
Epoch 8/80: current_loss=8.84413 | best_loss=8.42771
Epoch 9/80: current_loss=8.28210 | best_loss=8.28210
Epoch 10/80: current_loss=8.38866 | best_loss=8.28210
Epoch 11/80: current_loss=8.48649 | best_loss=8.28210
Epoch 12/80: current_loss=10.00638 | best_loss=8.28210
Epoch 13/80: current_loss=12.67400 | best_loss=8.28210
Epoch 14/80: current_loss=10.23562 | best_loss=8.28210
Epoch 15/80: current_loss=11.86914 | best_loss=8.28210
Epoch 16/80: current_loss=9.30122 | best_loss=8.28210
Epoch 17/80: current_loss=8.40091 | best_loss=8.28210
Epoch 18/80: current_loss=8.81029 | best_loss=8.28210
Epoch 19/80: current_loss=11.65005 | best_loss=8.28210
Epoch 20/80: current_loss=10.76463 | best_loss=8.28210
Epoch 21/80: current_loss=9.79421 | best_loss=8.28210
Epoch 22/80: current_loss=8.57710 | best_loss=8.28210
Epoch 23/80: current_loss=10.09056 | best_loss=8.28210
Epoch 24/80: current_loss=8.24394 | best_loss=8.24394
Epoch 25/80: current_loss=11.03831 | best_loss=8.24394
Epoch 26/80: current_loss=10.98324 | best_loss=8.24394
Epoch 27/80: current_loss=8.51042 | best_loss=8.24394
Epoch 28/80: current_loss=8.32181 | best_loss=8.24394
Epoch 29/80: current_loss=8.48732 | best_loss=8.24394
Epoch 30/80: current_loss=8.57150 | best_loss=8.24394
Epoch 31/80: current_loss=8.32062 | best_loss=8.24394
Epoch 32/80: current_loss=8.47482 | best_loss=8.24394
Epoch 33/80: current_loss=8.85466 | best_loss=8.24394
Epoch 34/80: current_loss=9.01860 | best_loss=8.24394
Epoch 35/80: current_loss=8.48519 | best_loss=8.24394
Epoch 36/80: current_loss=8.86576 | best_loss=8.24394
Epoch 37/80: current_loss=8.55320 | best_loss=8.24394
Epoch 38/80: current_loss=9.31309 | best_loss=8.24394
Epoch 39/80: current_loss=9.39599 | best_loss=8.24394
Epoch 40/80: current_loss=8.97723 | best_loss=8.24394
Epoch 41/80: current_loss=9.88045 | best_loss=8.24394
Epoch 42/80: current_loss=8.33223 | best_loss=8.24394
Epoch 43/80: current_loss=11.57768 | best_loss=8.24394
Epoch 44/80: current_loss=8.45129 | best_loss=8.24394
Early Stopping at epoch 44
      explained_var=0.00977 | mse_loss=8.08616
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.79852 | best_loss=9.79852
Epoch 1/80: current_loss=11.90915 | best_loss=9.79852
Epoch 2/80: current_loss=8.71487 | best_loss=8.71487
Epoch 3/80: current_loss=9.41865 | best_loss=8.71487
Epoch 4/80: current_loss=9.33030 | best_loss=8.71487
Epoch 5/80: current_loss=10.10722 | best_loss=8.71487
Epoch 6/80: current_loss=9.43287 | best_loss=8.71487
Epoch 7/80: current_loss=8.70546 | best_loss=8.70546
Epoch 8/80: current_loss=9.16604 | best_loss=8.70546
Epoch 9/80: current_loss=15.69856 | best_loss=8.70546
Epoch 10/80: current_loss=9.92058 | best_loss=8.70546
Epoch 11/80: current_loss=8.92874 | best_loss=8.70546
Epoch 12/80: current_loss=8.99210 | best_loss=8.70546
Epoch 13/80: current_loss=8.71625 | best_loss=8.70546
Epoch 14/80: current_loss=11.69584 | best_loss=8.70546
Epoch 15/80: current_loss=9.96467 | best_loss=8.70546
Epoch 16/80: current_loss=9.27077 | best_loss=8.70546
Epoch 17/80: current_loss=14.98333 | best_loss=8.70546
Epoch 18/80: current_loss=9.25376 | best_loss=8.70546
Epoch 19/80: current_loss=9.21165 | best_loss=8.70546
Epoch 20/80: current_loss=10.56195 | best_loss=8.70546
Epoch 21/80: current_loss=9.35667 | best_loss=8.70546
Epoch 22/80: current_loss=9.42373 | best_loss=8.70546
Epoch 23/80: current_loss=11.25565 | best_loss=8.70546
Epoch 24/80: current_loss=9.41128 | best_loss=8.70546
Epoch 25/80: current_loss=9.17606 | best_loss=8.70546
Epoch 26/80: current_loss=10.84740 | best_loss=8.70546
Epoch 27/80: current_loss=10.88193 | best_loss=8.70546
Early Stopping at epoch 27
      explained_var=0.00388 | mse_loss=8.46442
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.73459 | best_loss=9.73459
Epoch 1/80: current_loss=11.40171 | best_loss=9.73459
Epoch 2/80: current_loss=8.23901 | best_loss=8.23901
Epoch 3/80: current_loss=13.36624 | best_loss=8.23901
Epoch 4/80: current_loss=12.45255 | best_loss=8.23901
Epoch 5/80: current_loss=8.40915 | best_loss=8.23901
Epoch 6/80: current_loss=10.56340 | best_loss=8.23901
Epoch 7/80: current_loss=9.16205 | best_loss=8.23901
Epoch 8/80: current_loss=8.29100 | best_loss=8.23901
Epoch 9/80: current_loss=8.27733 | best_loss=8.23901
Epoch 10/80: current_loss=10.43872 | best_loss=8.23901
Epoch 11/80: current_loss=8.62018 | best_loss=8.23901
Epoch 12/80: current_loss=10.40748 | best_loss=8.23901
Epoch 13/80: current_loss=8.14243 | best_loss=8.14243
Epoch 14/80: current_loss=8.77267 | best_loss=8.14243
Epoch 15/80: current_loss=8.78586 | best_loss=8.14243
Epoch 16/80: current_loss=8.58583 | best_loss=8.14243
Epoch 17/80: current_loss=8.52804 | best_loss=8.14243
Epoch 18/80: current_loss=8.64694 | best_loss=8.14243
Epoch 19/80: current_loss=8.78266 | best_loss=8.14243
Epoch 20/80: current_loss=8.36283 | best_loss=8.14243
Epoch 21/80: current_loss=8.16389 | best_loss=8.14243
Epoch 22/80: current_loss=8.49042 | best_loss=8.14243
Epoch 23/80: current_loss=8.44106 | best_loss=8.14243
Epoch 24/80: current_loss=8.34156 | best_loss=8.14243
Epoch 25/80: current_loss=8.22464 | best_loss=8.14243
Epoch 26/80: current_loss=8.21867 | best_loss=8.14243
Epoch 27/80: current_loss=9.48453 | best_loss=8.14243
Epoch 28/80: current_loss=8.21977 | best_loss=8.14243
Epoch 29/80: current_loss=8.22774 | best_loss=8.14243
Epoch 30/80: current_loss=8.34565 | best_loss=8.14243
Epoch 31/80: current_loss=8.83707 | best_loss=8.14243
Epoch 32/80: current_loss=8.40154 | best_loss=8.14243
Epoch 33/80: current_loss=8.21423 | best_loss=8.14243
Early Stopping at epoch 33
      explained_var=0.01025 | mse_loss=8.24707
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.77285 | best_loss=8.77285
Epoch 1/80: current_loss=7.90250 | best_loss=7.90250
Epoch 2/80: current_loss=8.46253 | best_loss=7.90250
Epoch 3/80: current_loss=9.31975 | best_loss=7.90250
Epoch 4/80: current_loss=9.63891 | best_loss=7.90250
Epoch 5/80: current_loss=7.74430 | best_loss=7.74430
Epoch 6/80: current_loss=10.13769 | best_loss=7.74430
Epoch 7/80: current_loss=7.99445 | best_loss=7.74430
Epoch 8/80: current_loss=7.85865 | best_loss=7.74430
Epoch 9/80: current_loss=8.77372 | best_loss=7.74430
Epoch 10/80: current_loss=10.31695 | best_loss=7.74430
Epoch 11/80: current_loss=9.33205 | best_loss=7.74430
Epoch 12/80: current_loss=8.10304 | best_loss=7.74430
Epoch 13/80: current_loss=10.03663 | best_loss=7.74430
Epoch 14/80: current_loss=11.54547 | best_loss=7.74430
Epoch 15/80: current_loss=10.53713 | best_loss=7.74430
Epoch 16/80: current_loss=8.61273 | best_loss=7.74430
Epoch 17/80: current_loss=8.72558 | best_loss=7.74430
Epoch 18/80: current_loss=8.00096 | best_loss=7.74430
Epoch 19/80: current_loss=7.88379 | best_loss=7.74430
Epoch 20/80: current_loss=8.04340 | best_loss=7.74430
Epoch 21/80: current_loss=8.23509 | best_loss=7.74430
Epoch 22/80: current_loss=8.42426 | best_loss=7.74430
Epoch 23/80: current_loss=8.09033 | best_loss=7.74430
Epoch 24/80: current_loss=9.61848 | best_loss=7.74430
Epoch 25/80: current_loss=11.87486 | best_loss=7.74430
Early Stopping at epoch 25
      explained_var=-0.01152 | mse_loss=7.94717
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00281| avg_loss=8.06468
----------------------------------------------


----------------------------------------------
Params for Trial 31
{'learning_rate': 0.1, 'weight_decay': 0.0013793194077344807, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.93557 | best_loss=8.93557
Epoch 1/80: current_loss=9.06432 | best_loss=8.93557
Epoch 2/80: current_loss=11.56105 | best_loss=8.93557
Epoch 3/80: current_loss=9.26051 | best_loss=8.93557
Epoch 4/80: current_loss=8.62599 | best_loss=8.62599
Epoch 5/80: current_loss=8.07485 | best_loss=8.07485
Epoch 6/80: current_loss=8.37827 | best_loss=8.07485
Epoch 7/80: current_loss=8.93218 | best_loss=8.07485
Epoch 8/80: current_loss=9.95929 | best_loss=8.07485
Epoch 9/80: current_loss=10.64646 | best_loss=8.07485
Epoch 10/80: current_loss=7.86726 | best_loss=7.86726
Epoch 11/80: current_loss=8.08864 | best_loss=7.86726
Epoch 12/80: current_loss=8.18500 | best_loss=7.86726
Epoch 13/80: current_loss=8.25413 | best_loss=7.86726
Epoch 14/80: current_loss=8.36767 | best_loss=7.86726
Epoch 15/80: current_loss=9.87251 | best_loss=7.86726
Epoch 16/80: current_loss=8.87217 | best_loss=7.86726
Epoch 17/80: current_loss=7.86556 | best_loss=7.86556
Epoch 18/80: current_loss=10.23620 | best_loss=7.86556
Epoch 19/80: current_loss=14.26031 | best_loss=7.86556
Epoch 20/80: current_loss=10.19876 | best_loss=7.86556
Epoch 21/80: current_loss=8.85217 | best_loss=7.86556
Epoch 22/80: current_loss=10.14983 | best_loss=7.86556
Epoch 23/80: current_loss=8.55598 | best_loss=7.86556
Epoch 24/80: current_loss=8.25520 | best_loss=7.86556
Epoch 25/80: current_loss=11.97094 | best_loss=7.86556
Epoch 26/80: current_loss=25.18628 | best_loss=7.86556
Epoch 27/80: current_loss=9.48636 | best_loss=7.86556
Epoch 28/80: current_loss=10.37932 | best_loss=7.86556
Epoch 29/80: current_loss=9.75890 | best_loss=7.86556
Epoch 30/80: current_loss=17.44848 | best_loss=7.86556
Epoch 31/80: current_loss=9.25801 | best_loss=7.86556
Epoch 32/80: current_loss=8.64952 | best_loss=7.86556
Epoch 33/80: current_loss=15.20081 | best_loss=7.86556
Epoch 34/80: current_loss=8.17843 | best_loss=7.86556
Epoch 35/80: current_loss=9.29915 | best_loss=7.86556
Epoch 36/80: current_loss=10.18961 | best_loss=7.86556
Epoch 37/80: current_loss=11.99825 | best_loss=7.86556
Early Stopping at epoch 37
      explained_var=-0.01272 | mse_loss=7.69682

----------------------------------------------
Params for Trial 32
{'learning_rate': 0.1, 'weight_decay': 0.0026926034906923652, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=16.59669 | best_loss=16.59669
Epoch 1/80: current_loss=9.12911 | best_loss=9.12911
Epoch 2/80: current_loss=9.49062 | best_loss=9.12911
Epoch 3/80: current_loss=10.73789 | best_loss=9.12911
Epoch 4/80: current_loss=8.84961 | best_loss=8.84961
Epoch 5/80: current_loss=15.70601 | best_loss=8.84961
Epoch 6/80: current_loss=9.19976 | best_loss=8.84961
Epoch 7/80: current_loss=11.01069 | best_loss=8.84961
Epoch 8/80: current_loss=9.51547 | best_loss=8.84961
Epoch 9/80: current_loss=8.02726 | best_loss=8.02726
Epoch 10/80: current_loss=8.46783 | best_loss=8.02726
Epoch 11/80: current_loss=9.68045 | best_loss=8.02726
Epoch 12/80: current_loss=8.34174 | best_loss=8.02726
Epoch 13/80: current_loss=9.54386 | best_loss=8.02726
Epoch 14/80: current_loss=17.11423 | best_loss=8.02726
Epoch 15/80: current_loss=8.38621 | best_loss=8.02726
Epoch 16/80: current_loss=7.79805 | best_loss=7.79805
Epoch 17/80: current_loss=8.26434 | best_loss=7.79805
Epoch 18/80: current_loss=10.11288 | best_loss=7.79805
Epoch 19/80: current_loss=12.31856 | best_loss=7.79805
Epoch 20/80: current_loss=10.27476 | best_loss=7.79805
Epoch 21/80: current_loss=9.37151 | best_loss=7.79805
Epoch 22/80: current_loss=8.33671 | best_loss=7.79805
Epoch 23/80: current_loss=11.27707 | best_loss=7.79805
Epoch 24/80: current_loss=7.82771 | best_loss=7.79805
Epoch 25/80: current_loss=7.93520 | best_loss=7.79805
Epoch 26/80: current_loss=8.53981 | best_loss=7.79805
Epoch 27/80: current_loss=12.09946 | best_loss=7.79805
Epoch 28/80: current_loss=7.82211 | best_loss=7.79805
Epoch 29/80: current_loss=8.66849 | best_loss=7.79805
Epoch 30/80: current_loss=9.42565 | best_loss=7.79805
Epoch 31/80: current_loss=8.29075 | best_loss=7.79805
Epoch 32/80: current_loss=7.93667 | best_loss=7.79805
Epoch 33/80: current_loss=8.53591 | best_loss=7.79805
Epoch 34/80: current_loss=16.18776 | best_loss=7.79805
Epoch 35/80: current_loss=9.00113 | best_loss=7.79805
Epoch 36/80: current_loss=9.15359 | best_loss=7.79805
Early Stopping at epoch 36
      explained_var=-0.00122 | mse_loss=7.61557
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.81502 | best_loss=9.81502
Epoch 1/80: current_loss=8.46533 | best_loss=8.46533
Epoch 2/80: current_loss=9.84673 | best_loss=8.46533
Epoch 3/80: current_loss=8.58527 | best_loss=8.46533
Epoch 4/80: current_loss=10.26593 | best_loss=8.46533
Epoch 5/80: current_loss=9.90085 | best_loss=8.46533
Epoch 6/80: current_loss=8.27760 | best_loss=8.27760
Epoch 7/80: current_loss=8.62368 | best_loss=8.27760
Epoch 8/80: current_loss=22.06002 | best_loss=8.27760
Epoch 9/80: current_loss=14.46527 | best_loss=8.27760
Epoch 10/80: current_loss=8.92060 | best_loss=8.27760
Epoch 11/80: current_loss=9.50308 | best_loss=8.27760
Epoch 12/80: current_loss=10.54005 | best_loss=8.27760
Epoch 13/80: current_loss=8.57834 | best_loss=8.27760
Epoch 14/80: current_loss=9.56804 | best_loss=8.27760
Epoch 15/80: current_loss=9.12041 | best_loss=8.27760
Epoch 16/80: current_loss=10.86743 | best_loss=8.27760
Epoch 17/80: current_loss=8.67565 | best_loss=8.27760
Epoch 18/80: current_loss=8.36149 | best_loss=8.27760
Epoch 19/80: current_loss=11.46076 | best_loss=8.27760
Epoch 20/80: current_loss=10.65639 | best_loss=8.27760
Epoch 21/80: current_loss=8.67976 | best_loss=8.27760
Epoch 22/80: current_loss=12.26819 | best_loss=8.27760
Epoch 23/80: current_loss=8.66523 | best_loss=8.27760
Epoch 24/80: current_loss=9.96724 | best_loss=8.27760
Epoch 25/80: current_loss=8.61896 | best_loss=8.27760
Epoch 26/80: current_loss=13.09285 | best_loss=8.27760
Early Stopping at epoch 26
      explained_var=0.00626 | mse_loss=8.11238
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=44.09544 | best_loss=44.09544
Epoch 1/80: current_loss=25.86590 | best_loss=25.86590
Epoch 2/80: current_loss=22.10080 | best_loss=22.10080
Epoch 3/80: current_loss=18.70860 | best_loss=18.70860
Epoch 4/80: current_loss=11.65036 | best_loss=11.65036
Epoch 5/80: current_loss=14.90839 | best_loss=11.65036
Epoch 6/80: current_loss=10.21378 | best_loss=10.21378
Epoch 7/80: current_loss=10.39481 | best_loss=10.21378
Epoch 8/80: current_loss=9.43253 | best_loss=9.43253
Epoch 9/80: current_loss=9.77429 | best_loss=9.43253
Epoch 10/80: current_loss=10.16797 | best_loss=9.43253
Epoch 11/80: current_loss=9.43881 | best_loss=9.43253
Epoch 12/80: current_loss=11.49245 | best_loss=9.43253
Epoch 13/80: current_loss=12.72649 | best_loss=9.43253
Epoch 14/80: current_loss=10.96286 | best_loss=9.43253
Epoch 15/80: current_loss=10.49542 | best_loss=9.43253
Epoch 16/80: current_loss=10.72650 | best_loss=9.43253
Epoch 17/80: current_loss=11.27755 | best_loss=9.43253
Epoch 18/80: current_loss=11.04663 | best_loss=9.43253
Epoch 19/80: current_loss=12.91422 | best_loss=9.43253
Epoch 20/80: current_loss=11.05154 | best_loss=9.43253
Epoch 21/80: current_loss=10.07714 | best_loss=9.43253
Epoch 22/80: current_loss=8.74719 | best_loss=8.74719
Epoch 23/80: current_loss=8.58743 | best_loss=8.58743
Epoch 24/80: current_loss=9.51403 | best_loss=8.58743
Epoch 25/80: current_loss=9.14231 | best_loss=8.58743
Epoch 26/80: current_loss=9.72220 | best_loss=8.58743
Epoch 27/80: current_loss=10.84718 | best_loss=8.58743
Epoch 28/80: current_loss=10.12392 | best_loss=8.58743
Epoch 29/80: current_loss=15.31552 | best_loss=8.58743
Epoch 30/80: current_loss=9.26879 | best_loss=8.58743
Epoch 31/80: current_loss=13.23176 | best_loss=8.58743
Epoch 32/80: current_loss=8.75545 | best_loss=8.58743
Epoch 33/80: current_loss=9.42367 | best_loss=8.58743
Epoch 34/80: current_loss=14.57963 | best_loss=8.58743
Epoch 35/80: current_loss=10.90045 | best_loss=8.58743
Epoch 36/80: current_loss=9.73750 | best_loss=8.58743
Epoch 37/80: current_loss=10.55483 | best_loss=8.58743
Epoch 38/80: current_loss=11.44740 | best_loss=8.58743
Epoch 39/80: current_loss=9.31092 | best_loss=8.58743
Epoch 40/80: current_loss=9.37164 | best_loss=8.58743
Epoch 41/80: current_loss=9.95883 | best_loss=8.58743
Epoch 42/80: current_loss=9.73215 | best_loss=8.58743
Epoch 43/80: current_loss=14.85597 | best_loss=8.58743
Early Stopping at epoch 43
      explained_var=0.05380 | mse_loss=8.39750
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.48216 | best_loss=9.48216
Epoch 1/80: current_loss=9.44977 | best_loss=9.44977
Epoch 2/80: current_loss=9.17483 | best_loss=9.17483
Epoch 3/80: current_loss=9.95651 | best_loss=9.17483
Epoch 4/80: current_loss=9.86509 | best_loss=9.17483
Epoch 5/80: current_loss=10.93981 | best_loss=9.17483
Epoch 6/80: current_loss=11.46499 | best_loss=9.17483
Epoch 7/80: current_loss=8.74109 | best_loss=8.74109
Epoch 8/80: current_loss=9.35869 | best_loss=8.74109
Epoch 9/80: current_loss=8.72616 | best_loss=8.72616
Epoch 10/80: current_loss=13.03224 | best_loss=8.72616
Epoch 11/80: current_loss=10.12903 | best_loss=8.72616
Epoch 12/80: current_loss=9.56460 | best_loss=8.72616
Epoch 13/80: current_loss=8.59538 | best_loss=8.59538
Epoch 14/80: current_loss=8.80090 | best_loss=8.59538
Epoch 15/80: current_loss=8.82126 | best_loss=8.59538
Epoch 16/80: current_loss=10.59125 | best_loss=8.59538
Epoch 17/80: current_loss=9.97278 | best_loss=8.59538
Epoch 18/80: current_loss=9.06641 | best_loss=8.59538
Epoch 19/80: current_loss=12.31849 | best_loss=8.59538
Epoch 20/80: current_loss=9.34316 | best_loss=8.59538
Epoch 21/80: current_loss=10.70049 | best_loss=8.59538
Epoch 22/80: current_loss=9.55725 | best_loss=8.59538
Epoch 23/80: current_loss=10.10166 | best_loss=8.59538
Epoch 24/80: current_loss=9.39220 | best_loss=8.59538
Epoch 25/80: current_loss=8.97610 | best_loss=8.59538
Epoch 26/80: current_loss=8.59008 | best_loss=8.59008
Epoch 27/80: current_loss=9.55180 | best_loss=8.59008
Epoch 28/80: current_loss=11.16711 | best_loss=8.59008
Epoch 29/80: current_loss=8.83013 | best_loss=8.59008
Epoch 30/80: current_loss=9.03976 | best_loss=8.59008
Epoch 31/80: current_loss=8.30657 | best_loss=8.30657
Epoch 32/80: current_loss=8.22100 | best_loss=8.22100
Epoch 33/80: current_loss=8.82507 | best_loss=8.22100
Epoch 34/80: current_loss=13.77228 | best_loss=8.22100
Epoch 35/80: current_loss=8.75836 | best_loss=8.22100
Epoch 36/80: current_loss=12.60691 | best_loss=8.22100
Epoch 37/80: current_loss=12.74258 | best_loss=8.22100
Epoch 38/80: current_loss=8.87305 | best_loss=8.22100
Epoch 39/80: current_loss=8.47682 | best_loss=8.22100
Epoch 40/80: current_loss=8.34485 | best_loss=8.22100
Epoch 41/80: current_loss=8.55979 | best_loss=8.22100
Epoch 42/80: current_loss=8.96673 | best_loss=8.22100
Epoch 43/80: current_loss=8.79464 | best_loss=8.22100
Epoch 44/80: current_loss=11.03614 | best_loss=8.22100
Epoch 45/80: current_loss=8.55110 | best_loss=8.22100
Epoch 46/80: current_loss=13.52387 | best_loss=8.22100
Epoch 47/80: current_loss=10.71265 | best_loss=8.22100
Epoch 48/80: current_loss=17.12742 | best_loss=8.22100
Epoch 49/80: current_loss=18.60235 | best_loss=8.22100
Epoch 50/80: current_loss=11.60517 | best_loss=8.22100
Epoch 51/80: current_loss=8.62847 | best_loss=8.22100
Epoch 52/80: current_loss=8.40051 | best_loss=8.22100
Early Stopping at epoch 52
      explained_var=0.00143 | mse_loss=8.32125
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=15.92412 | best_loss=15.92412
Epoch 1/80: current_loss=12.82828 | best_loss=12.82828
Epoch 2/80: current_loss=7.82569 | best_loss=7.82569
Epoch 3/80: current_loss=8.67672 | best_loss=7.82569
Epoch 4/80: current_loss=8.25362 | best_loss=7.82569
Epoch 5/80: current_loss=7.72584 | best_loss=7.72584
Epoch 6/80: current_loss=13.42016 | best_loss=7.72584
Epoch 7/80: current_loss=9.02630 | best_loss=7.72584
Epoch 8/80: current_loss=7.94445 | best_loss=7.72584
Epoch 9/80: current_loss=17.71251 | best_loss=7.72584
Epoch 10/80: current_loss=7.73610 | best_loss=7.72584
Epoch 11/80: current_loss=12.24633 | best_loss=7.72584
Epoch 12/80: current_loss=8.05497 | best_loss=7.72584
Epoch 13/80: current_loss=13.79305 | best_loss=7.72584
Epoch 14/80: current_loss=7.66724 | best_loss=7.66724
Epoch 15/80: current_loss=9.97257 | best_loss=7.66724
Epoch 16/80: current_loss=8.89563 | best_loss=7.66724
Epoch 17/80: current_loss=11.11082 | best_loss=7.66724
Epoch 18/80: current_loss=8.83564 | best_loss=7.66724
Epoch 19/80: current_loss=12.69042 | best_loss=7.66724
Epoch 20/80: current_loss=7.99165 | best_loss=7.66724
Epoch 21/80: current_loss=7.69692 | best_loss=7.66724
Epoch 22/80: current_loss=9.39483 | best_loss=7.66724
Epoch 23/80: current_loss=12.40737 | best_loss=7.66724
Epoch 24/80: current_loss=9.66221 | best_loss=7.66724
Epoch 25/80: current_loss=9.09042 | best_loss=7.66724
Epoch 26/80: current_loss=7.73873 | best_loss=7.66724
Epoch 27/80: current_loss=7.76222 | best_loss=7.66724
Epoch 28/80: current_loss=8.20242 | best_loss=7.66724
Epoch 29/80: current_loss=9.89091 | best_loss=7.66724
Epoch 30/80: current_loss=11.39960 | best_loss=7.66724
Epoch 31/80: current_loss=7.72625 | best_loss=7.66724
Epoch 32/80: current_loss=8.99340 | best_loss=7.66724
Epoch 33/80: current_loss=8.17522 | best_loss=7.66724
Epoch 34/80: current_loss=7.92572 | best_loss=7.66724
Early Stopping at epoch 34
      explained_var=0.00247 | mse_loss=7.85666
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.01255| avg_loss=8.06067
----------------------------------------------


----------------------------------------------
Params for Trial 33
{'learning_rate': 1e-05, 'weight_decay': 0.004038221223610376, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=81.93558 | best_loss=81.93558
Epoch 1/80: current_loss=81.40057 | best_loss=81.40057
Epoch 2/80: current_loss=80.84960 | best_loss=80.84960
Epoch 3/80: current_loss=80.27731 | best_loss=80.27731
Epoch 4/80: current_loss=79.67703 | best_loss=79.67703
Epoch 5/80: current_loss=79.04571 | best_loss=79.04571
Epoch 6/80: current_loss=78.36601 | best_loss=78.36601
Epoch 7/80: current_loss=77.63497 | best_loss=77.63497
Epoch 8/80: current_loss=76.83891 | best_loss=76.83891
Epoch 9/80: current_loss=75.96799 | best_loss=75.96799
Epoch 10/80: current_loss=74.99835 | best_loss=74.99835
Epoch 11/80: current_loss=73.93620 | best_loss=73.93620
Epoch 12/80: current_loss=72.74924 | best_loss=72.74924
Epoch 13/80: current_loss=71.41348 | best_loss=71.41348
Epoch 14/80: current_loss=69.91983 | best_loss=69.91983
Epoch 15/80: current_loss=68.27486 | best_loss=68.27486
Epoch 16/80: current_loss=66.49051 | best_loss=66.49051
Epoch 17/80: current_loss=64.55133 | best_loss=64.55133
Epoch 18/80: current_loss=62.49773 | best_loss=62.49773
Epoch 19/80: current_loss=60.38527 | best_loss=60.38527
Epoch 20/80: current_loss=58.23697 | best_loss=58.23697
Epoch 21/80: current_loss=56.11360 | best_loss=56.11360
Epoch 22/80: current_loss=54.01105 | best_loss=54.01105
Epoch 23/80: current_loss=51.98317 | best_loss=51.98317
Epoch 24/80: current_loss=50.04384 | best_loss=50.04384
Epoch 25/80: current_loss=48.19764 | best_loss=48.19764
Epoch 26/80: current_loss=46.45517 | best_loss=46.45517
Epoch 27/80: current_loss=44.82254 | best_loss=44.82254
Epoch 28/80: current_loss=43.28995 | best_loss=43.28995
Epoch 29/80: current_loss=41.84981 | best_loss=41.84981
Epoch 30/80: current_loss=40.50250 | best_loss=40.50250
Epoch 31/80: current_loss=39.22506 | best_loss=39.22506
Epoch 32/80: current_loss=38.02894 | best_loss=38.02894
Epoch 33/80: current_loss=36.90136 | best_loss=36.90136
Epoch 34/80: current_loss=35.84372 | best_loss=35.84372
Epoch 35/80: current_loss=34.83507 | best_loss=34.83507
Epoch 36/80: current_loss=33.90403 | best_loss=33.90403
Epoch 37/80: current_loss=33.02497 | best_loss=33.02497
Epoch 38/80: current_loss=32.20561 | best_loss=32.20561
Epoch 39/80: current_loss=31.41258 | best_loss=31.41258
Epoch 40/80: current_loss=30.66966 | best_loss=30.66966
Epoch 41/80: current_loss=29.98037 | best_loss=29.98037
Epoch 42/80: current_loss=29.32125 | best_loss=29.32125
Epoch 43/80: current_loss=28.70318 | best_loss=28.70318
Epoch 44/80: current_loss=28.09185 | best_loss=28.09185
Epoch 45/80: current_loss=27.51780 | best_loss=27.51780
Epoch 46/80: current_loss=26.97118 | best_loss=26.97118
Epoch 47/80: current_loss=26.46326 | best_loss=26.46326
Epoch 48/80: current_loss=25.98141 | best_loss=25.98141
Epoch 49/80: current_loss=25.51999 | best_loss=25.51999
Epoch 50/80: current_loss=25.08217 | best_loss=25.08217
Epoch 51/80: current_loss=24.66376 | best_loss=24.66376
Epoch 52/80: current_loss=24.26174 | best_loss=24.26174
Epoch 53/80: current_loss=23.87229 | best_loss=23.87229
Epoch 54/80: current_loss=23.51004 | best_loss=23.51004
Epoch 55/80: current_loss=23.16313 | best_loss=23.16313
Epoch 56/80: current_loss=22.83386 | best_loss=22.83386
Epoch 57/80: current_loss=22.51122 | best_loss=22.51122
Epoch 58/80: current_loss=22.20807 | best_loss=22.20807
Epoch 59/80: current_loss=21.91049 | best_loss=21.91049
Epoch 60/80: current_loss=21.62576 | best_loss=21.62576
Epoch 61/80: current_loss=21.35344 | best_loss=21.35344
Epoch 62/80: current_loss=21.08629 | best_loss=21.08629
Epoch 63/80: current_loss=20.82907 | best_loss=20.82907
Epoch 64/80: current_loss=20.58435 | best_loss=20.58435
Epoch 65/80: current_loss=20.34236 | best_loss=20.34236
Epoch 66/80: current_loss=20.11591 | best_loss=20.11591
Epoch 67/80: current_loss=19.89316 | best_loss=19.89316
Epoch 68/80: current_loss=19.67513 | best_loss=19.67513
Epoch 69/80: current_loss=19.46390 | best_loss=19.46390
Epoch 70/80: current_loss=19.26108 | best_loss=19.26108
Epoch 71/80: current_loss=19.06676 | best_loss=19.06676
Epoch 72/80: current_loss=18.87746 | best_loss=18.87746
Epoch 73/80: current_loss=18.68756 | best_loss=18.68756
Epoch 74/80: current_loss=18.50450 | best_loss=18.50450
Epoch 75/80: current_loss=18.32826 | best_loss=18.32826
Epoch 76/80: current_loss=18.15830 | best_loss=18.15830
Epoch 77/80: current_loss=17.99780 | best_loss=17.99780
Epoch 78/80: current_loss=17.84359 | best_loss=17.84359
Epoch 79/80: current_loss=17.68671 | best_loss=17.68671
      explained_var=-0.20679 | mse_loss=17.26281

----------------------------------------------
Params for Trial 34
{'learning_rate': 0.0001, 'weight_decay': 0.005736768451867422, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=26.68216 | best_loss=26.68216
Epoch 1/80: current_loss=11.60946 | best_loss=11.60946
Epoch 2/80: current_loss=11.68381 | best_loss=11.60946
Epoch 3/80: current_loss=10.47515 | best_loss=10.47515
Epoch 4/80: current_loss=9.82473 | best_loss=9.82473
Epoch 5/80: current_loss=9.50780 | best_loss=9.50780
Epoch 6/80: current_loss=9.47932 | best_loss=9.47932
Epoch 7/80: current_loss=9.30532 | best_loss=9.30532
Epoch 8/80: current_loss=9.07257 | best_loss=9.07257
Epoch 9/80: current_loss=8.95030 | best_loss=8.95030
Epoch 10/80: current_loss=8.82532 | best_loss=8.82532
Epoch 11/80: current_loss=8.77561 | best_loss=8.77561
Epoch 12/80: current_loss=8.69049 | best_loss=8.69049
Epoch 13/80: current_loss=8.62602 | best_loss=8.62602
Epoch 14/80: current_loss=8.42431 | best_loss=8.42431
Epoch 15/80: current_loss=8.43535 | best_loss=8.42431
Epoch 16/80: current_loss=8.30838 | best_loss=8.30838
Epoch 17/80: current_loss=8.40953 | best_loss=8.30838
Epoch 18/80: current_loss=8.33129 | best_loss=8.30838
Epoch 19/80: current_loss=8.17546 | best_loss=8.17546
Epoch 20/80: current_loss=8.14468 | best_loss=8.14468
Epoch 21/80: current_loss=8.06824 | best_loss=8.06824
Epoch 22/80: current_loss=8.15257 | best_loss=8.06824
Epoch 23/80: current_loss=8.14631 | best_loss=8.06824
Epoch 24/80: current_loss=8.13725 | best_loss=8.06824
Epoch 25/80: current_loss=8.06630 | best_loss=8.06630
Epoch 26/80: current_loss=8.17561 | best_loss=8.06630
Epoch 27/80: current_loss=8.15228 | best_loss=8.06630
Epoch 28/80: current_loss=8.05059 | best_loss=8.05059
Epoch 29/80: current_loss=8.01747 | best_loss=8.01747
Epoch 30/80: current_loss=8.03062 | best_loss=8.01747
Epoch 31/80: current_loss=8.06905 | best_loss=8.01747
Epoch 32/80: current_loss=7.99528 | best_loss=7.99528
Epoch 33/80: current_loss=7.97864 | best_loss=7.97864
Epoch 34/80: current_loss=8.00286 | best_loss=7.97864
Epoch 35/80: current_loss=8.01064 | best_loss=7.97864
Epoch 36/80: current_loss=8.14120 | best_loss=7.97864
Epoch 37/80: current_loss=8.05971 | best_loss=7.97864
Epoch 38/80: current_loss=8.05888 | best_loss=7.97864
Epoch 39/80: current_loss=7.94948 | best_loss=7.94948
Epoch 40/80: current_loss=8.02996 | best_loss=7.94948
Epoch 41/80: current_loss=8.07864 | best_loss=7.94948
Epoch 42/80: current_loss=8.04587 | best_loss=7.94948
Epoch 43/80: current_loss=8.01237 | best_loss=7.94948
Epoch 44/80: current_loss=8.03562 | best_loss=7.94948
Epoch 45/80: current_loss=8.00126 | best_loss=7.94948
Epoch 46/80: current_loss=7.92692 | best_loss=7.92692
Epoch 47/80: current_loss=8.12229 | best_loss=7.92692
Epoch 48/80: current_loss=8.22455 | best_loss=7.92692
Epoch 49/80: current_loss=8.08524 | best_loss=7.92692
Epoch 50/80: current_loss=8.00078 | best_loss=7.92692
Epoch 51/80: current_loss=8.09151 | best_loss=7.92692
Epoch 52/80: current_loss=7.95029 | best_loss=7.92692
Epoch 53/80: current_loss=8.10687 | best_loss=7.92692
Epoch 54/80: current_loss=8.18223 | best_loss=7.92692
Epoch 55/80: current_loss=8.08623 | best_loss=7.92692
Epoch 56/80: current_loss=7.93232 | best_loss=7.92692
Epoch 57/80: current_loss=8.10161 | best_loss=7.92692
Epoch 58/80: current_loss=8.30150 | best_loss=7.92692
Epoch 59/80: current_loss=8.17137 | best_loss=7.92692
Epoch 60/80: current_loss=8.15185 | best_loss=7.92692
Epoch 61/80: current_loss=8.19873 | best_loss=7.92692
Epoch 62/80: current_loss=8.06530 | best_loss=7.92692
Epoch 63/80: current_loss=8.15637 | best_loss=7.92692
Epoch 64/80: current_loss=8.12776 | best_loss=7.92692
Epoch 65/80: current_loss=8.17900 | best_loss=7.92692
Epoch 66/80: current_loss=8.02341 | best_loss=7.92692
Early Stopping at epoch 66
      explained_var=0.00550 | mse_loss=7.79111

----------------------------------------------
Params for Trial 35
{'learning_rate': 0.1, 'weight_decay': 9.926968902753612e-05, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=21.79221 | best_loss=21.79221
Epoch 1/80: current_loss=19.27610 | best_loss=19.27610
Epoch 2/80: current_loss=8.68682 | best_loss=8.68682
Epoch 3/80: current_loss=11.84953 | best_loss=8.68682
Epoch 4/80: current_loss=10.01478 | best_loss=8.68682
Epoch 5/80: current_loss=8.05675 | best_loss=8.05675
Epoch 6/80: current_loss=9.18854 | best_loss=8.05675
Epoch 7/80: current_loss=9.19820 | best_loss=8.05675
Epoch 8/80: current_loss=9.66503 | best_loss=8.05675
Epoch 9/80: current_loss=8.15209 | best_loss=8.05675
Epoch 10/80: current_loss=9.85660 | best_loss=8.05675
Epoch 11/80: current_loss=10.76645 | best_loss=8.05675
Epoch 12/80: current_loss=9.61828 | best_loss=8.05675
Epoch 13/80: current_loss=10.55239 | best_loss=8.05675
Epoch 14/80: current_loss=10.90899 | best_loss=8.05675
Epoch 15/80: current_loss=10.65958 | best_loss=8.05675
Epoch 16/80: current_loss=11.30455 | best_loss=8.05675
Epoch 17/80: current_loss=8.00685 | best_loss=8.00685
Epoch 18/80: current_loss=8.43841 | best_loss=8.00685
Epoch 19/80: current_loss=9.33042 | best_loss=8.00685
Epoch 20/80: current_loss=9.31415 | best_loss=8.00685
Epoch 21/80: current_loss=7.92907 | best_loss=7.92907
Epoch 22/80: current_loss=8.37387 | best_loss=7.92907
Epoch 23/80: current_loss=8.23275 | best_loss=7.92907
Epoch 24/80: current_loss=9.52102 | best_loss=7.92907
Epoch 25/80: current_loss=8.61000 | best_loss=7.92907
Epoch 26/80: current_loss=8.74547 | best_loss=7.92907
Epoch 27/80: current_loss=9.31281 | best_loss=7.92907
Epoch 28/80: current_loss=13.95159 | best_loss=7.92907
Epoch 29/80: current_loss=9.97268 | best_loss=7.92907
Epoch 30/80: current_loss=9.33582 | best_loss=7.92907
Epoch 31/80: current_loss=9.26210 | best_loss=7.92907
Epoch 32/80: current_loss=8.26203 | best_loss=7.92907
Epoch 33/80: current_loss=9.96208 | best_loss=7.92907
Epoch 34/80: current_loss=8.56819 | best_loss=7.92907
Epoch 35/80: current_loss=10.52690 | best_loss=7.92907
Epoch 36/80: current_loss=10.16403 | best_loss=7.92907
Epoch 37/80: current_loss=8.47009 | best_loss=7.92907
Epoch 38/80: current_loss=8.10058 | best_loss=7.92907
Epoch 39/80: current_loss=8.88834 | best_loss=7.92907
Epoch 40/80: current_loss=9.19175 | best_loss=7.92907
Epoch 41/80: current_loss=8.04553 | best_loss=7.92907
Early Stopping at epoch 41
      explained_var=-0.02457 | mse_loss=7.77434

----------------------------------------------
Params for Trial 36
{'learning_rate': 0.1, 'weight_decay': 0.0016166365393631337, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.01287 | best_loss=12.01287
Epoch 1/80: current_loss=8.33905 | best_loss=8.33905
Epoch 2/80: current_loss=8.63832 | best_loss=8.33905
Epoch 3/80: current_loss=12.31594 | best_loss=8.33905
Epoch 4/80: current_loss=7.77165 | best_loss=7.77165
Epoch 5/80: current_loss=10.09699 | best_loss=7.77165
Epoch 6/80: current_loss=7.83323 | best_loss=7.77165
Epoch 7/80: current_loss=7.89626 | best_loss=7.77165
Epoch 8/80: current_loss=9.70465 | best_loss=7.77165
Epoch 9/80: current_loss=9.62478 | best_loss=7.77165
Epoch 10/80: current_loss=9.61773 | best_loss=7.77165
Epoch 11/80: current_loss=16.32378 | best_loss=7.77165
Epoch 12/80: current_loss=9.70156 | best_loss=7.77165
Epoch 13/80: current_loss=7.93447 | best_loss=7.77165
Epoch 14/80: current_loss=8.39435 | best_loss=7.77165
Epoch 15/80: current_loss=17.56254 | best_loss=7.77165
Epoch 16/80: current_loss=10.99437 | best_loss=7.77165
Epoch 17/80: current_loss=8.43397 | best_loss=7.77165
Epoch 18/80: current_loss=8.41731 | best_loss=7.77165
Epoch 19/80: current_loss=9.56526 | best_loss=7.77165
Epoch 20/80: current_loss=11.68871 | best_loss=7.77165
Epoch 21/80: current_loss=7.80595 | best_loss=7.77165
Epoch 22/80: current_loss=15.30242 | best_loss=7.77165
Epoch 23/80: current_loss=9.60151 | best_loss=7.77165
Epoch 24/80: current_loss=7.87532 | best_loss=7.77165
Early Stopping at epoch 24
      explained_var=0.00104 | mse_loss=7.59872
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=59.66591 | best_loss=59.66591
Epoch 1/80: current_loss=32.83328 | best_loss=32.83328
Epoch 2/80: current_loss=29.49368 | best_loss=29.49368
Epoch 3/80: current_loss=12.57589 | best_loss=12.57589
Epoch 4/80: current_loss=14.60840 | best_loss=12.57589
Epoch 5/80: current_loss=12.58471 | best_loss=12.57589
Epoch 6/80: current_loss=12.32142 | best_loss=12.32142
Epoch 7/80: current_loss=9.62694 | best_loss=9.62694
Epoch 8/80: current_loss=14.72299 | best_loss=9.62694
Epoch 9/80: current_loss=10.10955 | best_loss=9.62694
Epoch 10/80: current_loss=9.67803 | best_loss=9.62694
Epoch 11/80: current_loss=9.08735 | best_loss=9.08735
Epoch 12/80: current_loss=10.81341 | best_loss=9.08735
Epoch 13/80: current_loss=9.70954 | best_loss=9.08735
Epoch 14/80: current_loss=8.06470 | best_loss=8.06470
Epoch 15/80: current_loss=13.75666 | best_loss=8.06470
Epoch 16/80: current_loss=10.00153 | best_loss=8.06470
Epoch 17/80: current_loss=8.80514 | best_loss=8.06470
Epoch 18/80: current_loss=9.02547 | best_loss=8.06470
Epoch 19/80: current_loss=9.99119 | best_loss=8.06470
Epoch 20/80: current_loss=12.36863 | best_loss=8.06470
Epoch 21/80: current_loss=19.08744 | best_loss=8.06470
Epoch 22/80: current_loss=9.60543 | best_loss=8.06470
Epoch 23/80: current_loss=10.85084 | best_loss=8.06470
Epoch 24/80: current_loss=12.23018 | best_loss=8.06470
Epoch 25/80: current_loss=8.44767 | best_loss=8.06470
Epoch 26/80: current_loss=8.56195 | best_loss=8.06470
Epoch 27/80: current_loss=9.80228 | best_loss=8.06470
Epoch 28/80: current_loss=8.68050 | best_loss=8.06470
Epoch 29/80: current_loss=9.07013 | best_loss=8.06470
Epoch 30/80: current_loss=14.13153 | best_loss=8.06470
Epoch 31/80: current_loss=8.33284 | best_loss=8.06470
Epoch 32/80: current_loss=8.53390 | best_loss=8.06470
Epoch 33/80: current_loss=8.76352 | best_loss=8.06470
Epoch 34/80: current_loss=8.86172 | best_loss=8.06470
Early Stopping at epoch 34
      explained_var=0.02850 | mse_loss=7.93126
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=43.09693 | best_loss=43.09693
Epoch 1/80: current_loss=9.29123 | best_loss=9.29123
Epoch 2/80: current_loss=14.75105 | best_loss=9.29123
Epoch 3/80: current_loss=10.78575 | best_loss=9.29123
Epoch 4/80: current_loss=9.33319 | best_loss=9.29123
Epoch 5/80: current_loss=10.42196 | best_loss=9.29123
Epoch 6/80: current_loss=8.89751 | best_loss=8.89751
Epoch 7/80: current_loss=9.67714 | best_loss=8.89751
Epoch 8/80: current_loss=10.42701 | best_loss=8.89751
Epoch 9/80: current_loss=8.94816 | best_loss=8.89751
Epoch 10/80: current_loss=9.82623 | best_loss=8.89751
Epoch 11/80: current_loss=12.56910 | best_loss=8.89751
Epoch 12/80: current_loss=10.04416 | best_loss=8.89751
Epoch 13/80: current_loss=9.46256 | best_loss=8.89751
Epoch 14/80: current_loss=8.67844 | best_loss=8.67844
Epoch 15/80: current_loss=9.53916 | best_loss=8.67844
Epoch 16/80: current_loss=8.73073 | best_loss=8.67844
Epoch 17/80: current_loss=10.67920 | best_loss=8.67844
Epoch 18/80: current_loss=9.05549 | best_loss=8.67844
Epoch 19/80: current_loss=9.80357 | best_loss=8.67844
Epoch 20/80: current_loss=8.78922 | best_loss=8.67844
Epoch 21/80: current_loss=8.77218 | best_loss=8.67844
Epoch 22/80: current_loss=14.69541 | best_loss=8.67844
Epoch 23/80: current_loss=8.68282 | best_loss=8.67844
Epoch 24/80: current_loss=9.34214 | best_loss=8.67844
Epoch 25/80: current_loss=8.88452 | best_loss=8.67844
Epoch 26/80: current_loss=16.11972 | best_loss=8.67844
Epoch 27/80: current_loss=10.98219 | best_loss=8.67844
Epoch 28/80: current_loss=12.95332 | best_loss=8.67844
Epoch 29/80: current_loss=11.72939 | best_loss=8.67844
Epoch 30/80: current_loss=12.38818 | best_loss=8.67844
Epoch 31/80: current_loss=9.81645 | best_loss=8.67844
Epoch 32/80: current_loss=9.70900 | best_loss=8.67844
Epoch 33/80: current_loss=10.67864 | best_loss=8.67844
Epoch 34/80: current_loss=8.66779 | best_loss=8.66779
Epoch 35/80: current_loss=10.29128 | best_loss=8.66779
Epoch 36/80: current_loss=9.85679 | best_loss=8.66779
Epoch 37/80: current_loss=9.75381 | best_loss=8.66779
Epoch 38/80: current_loss=9.78754 | best_loss=8.66779
Epoch 39/80: current_loss=11.52262 | best_loss=8.66779
Epoch 40/80: current_loss=14.03416 | best_loss=8.66779
Epoch 41/80: current_loss=9.01174 | best_loss=8.66779
Epoch 42/80: current_loss=10.83101 | best_loss=8.66779
Epoch 43/80: current_loss=11.42070 | best_loss=8.66779
Epoch 44/80: current_loss=10.37505 | best_loss=8.66779
Epoch 45/80: current_loss=13.61906 | best_loss=8.66779
Epoch 46/80: current_loss=9.87297 | best_loss=8.66779
Epoch 47/80: current_loss=9.72577 | best_loss=8.66779
Epoch 48/80: current_loss=9.73575 | best_loss=8.66779
Epoch 49/80: current_loss=11.14408 | best_loss=8.66779
Epoch 50/80: current_loss=10.56735 | best_loss=8.66779
Epoch 51/80: current_loss=11.05340 | best_loss=8.66779
Epoch 52/80: current_loss=9.64447 | best_loss=8.66779
Epoch 53/80: current_loss=9.65388 | best_loss=8.66779
Epoch 54/80: current_loss=9.31925 | best_loss=8.66779
Early Stopping at epoch 54
      explained_var=-0.00508 | mse_loss=8.52755
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.39403 | best_loss=10.39403
Epoch 1/80: current_loss=8.66008 | best_loss=8.66008
Epoch 2/80: current_loss=9.90101 | best_loss=8.66008
Epoch 3/80: current_loss=13.28030 | best_loss=8.66008
Epoch 4/80: current_loss=9.00304 | best_loss=8.66008
Epoch 5/80: current_loss=12.60454 | best_loss=8.66008
Epoch 6/80: current_loss=8.31116 | best_loss=8.31116
Epoch 7/80: current_loss=8.78817 | best_loss=8.31116
Epoch 8/80: current_loss=8.50514 | best_loss=8.31116
Epoch 9/80: current_loss=11.08625 | best_loss=8.31116
Epoch 10/80: current_loss=10.78407 | best_loss=8.31116
Epoch 11/80: current_loss=21.40084 | best_loss=8.31116
Epoch 12/80: current_loss=23.30879 | best_loss=8.31116
Epoch 13/80: current_loss=10.11182 | best_loss=8.31116
Epoch 14/80: current_loss=8.44626 | best_loss=8.31116
Epoch 15/80: current_loss=18.14210 | best_loss=8.31116
Epoch 16/80: current_loss=10.32769 | best_loss=8.31116
Epoch 17/80: current_loss=9.73201 | best_loss=8.31116
Epoch 18/80: current_loss=11.87482 | best_loss=8.31116
Epoch 19/80: current_loss=10.13323 | best_loss=8.31116
Epoch 20/80: current_loss=10.14053 | best_loss=8.31116
Epoch 21/80: current_loss=8.21394 | best_loss=8.21394
Epoch 22/80: current_loss=10.90600 | best_loss=8.21394
Epoch 23/80: current_loss=10.63855 | best_loss=8.21394
Epoch 24/80: current_loss=8.66049 | best_loss=8.21394
Epoch 25/80: current_loss=8.31536 | best_loss=8.21394
Epoch 26/80: current_loss=8.84033 | best_loss=8.21394
Epoch 27/80: current_loss=8.54525 | best_loss=8.21394
Epoch 28/80: current_loss=10.03473 | best_loss=8.21394
Epoch 29/80: current_loss=8.72065 | best_loss=8.21394
Epoch 30/80: current_loss=9.03405 | best_loss=8.21394
Epoch 31/80: current_loss=9.39387 | best_loss=8.21394
Epoch 32/80: current_loss=9.57690 | best_loss=8.21394
Epoch 33/80: current_loss=9.13402 | best_loss=8.21394
Epoch 34/80: current_loss=11.92058 | best_loss=8.21394
Epoch 35/80: current_loss=17.65514 | best_loss=8.21394
Epoch 36/80: current_loss=8.23322 | best_loss=8.21394
Epoch 37/80: current_loss=8.69285 | best_loss=8.21394
Epoch 38/80: current_loss=9.13863 | best_loss=8.21394
Epoch 39/80: current_loss=9.20402 | best_loss=8.21394
Epoch 40/80: current_loss=9.44884 | best_loss=8.21394
Epoch 41/80: current_loss=10.21706 | best_loss=8.21394
Early Stopping at epoch 41
      explained_var=0.00243 | mse_loss=8.31183
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.73155 | best_loss=7.73155
Epoch 1/80: current_loss=7.68788 | best_loss=7.68788
Epoch 2/80: current_loss=10.52218 | best_loss=7.68788
Epoch 3/80: current_loss=7.91066 | best_loss=7.68788
Epoch 4/80: current_loss=9.29426 | best_loss=7.68788
Epoch 5/80: current_loss=9.01736 | best_loss=7.68788
Epoch 6/80: current_loss=9.65560 | best_loss=7.68788
Epoch 7/80: current_loss=9.60966 | best_loss=7.68788
Epoch 8/80: current_loss=7.84863 | best_loss=7.68788
Epoch 9/80: current_loss=8.38181 | best_loss=7.68788
Epoch 10/80: current_loss=7.75884 | best_loss=7.68788
Epoch 11/80: current_loss=7.94515 | best_loss=7.68788
Epoch 12/80: current_loss=7.91918 | best_loss=7.68788
Epoch 13/80: current_loss=7.65794 | best_loss=7.65794
Epoch 14/80: current_loss=8.40039 | best_loss=7.65794
Epoch 15/80: current_loss=8.15819 | best_loss=7.65794
Epoch 16/80: current_loss=8.95255 | best_loss=7.65794
Epoch 17/80: current_loss=10.38498 | best_loss=7.65794
Epoch 18/80: current_loss=16.45537 | best_loss=7.65794
Epoch 19/80: current_loss=16.61130 | best_loss=7.65794
Epoch 20/80: current_loss=8.77752 | best_loss=7.65794
Epoch 21/80: current_loss=7.95249 | best_loss=7.65794
Epoch 22/80: current_loss=8.46956 | best_loss=7.65794
Epoch 23/80: current_loss=8.04523 | best_loss=7.65794
Epoch 24/80: current_loss=10.59986 | best_loss=7.65794
Epoch 25/80: current_loss=8.55533 | best_loss=7.65794
Epoch 26/80: current_loss=8.04853 | best_loss=7.65794
Epoch 27/80: current_loss=7.77832 | best_loss=7.65794
Epoch 28/80: current_loss=7.91583 | best_loss=7.65794
Epoch 29/80: current_loss=7.69170 | best_loss=7.65794
Epoch 30/80: current_loss=10.85704 | best_loss=7.65794
Epoch 31/80: current_loss=8.57093 | best_loss=7.65794
Epoch 32/80: current_loss=9.60913 | best_loss=7.65794
Epoch 33/80: current_loss=7.66857 | best_loss=7.65794
Early Stopping at epoch 33
      explained_var=0.00151 | mse_loss=7.84405
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00568| avg_loss=8.04268
----------------------------------------------


----------------------------------------------
Params for Trial 37
{'learning_rate': 0.001, 'weight_decay': 0.004693052923663803, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=41.10881 | best_loss=41.10881
Epoch 1/80: current_loss=23.62427 | best_loss=23.62427
Epoch 2/80: current_loss=17.04987 | best_loss=17.04987
Epoch 3/80: current_loss=12.92890 | best_loss=12.92890
Epoch 4/80: current_loss=10.35008 | best_loss=10.35008
Epoch 5/80: current_loss=8.86981 | best_loss=8.86981
Epoch 6/80: current_loss=8.08726 | best_loss=8.08726
Epoch 7/80: current_loss=7.79268 | best_loss=7.79268
Epoch 8/80: current_loss=7.74674 | best_loss=7.74674
Epoch 9/80: current_loss=7.79209 | best_loss=7.74674
Epoch 10/80: current_loss=7.87060 | best_loss=7.74674
Epoch 11/80: current_loss=7.95668 | best_loss=7.74674
Epoch 12/80: current_loss=8.01113 | best_loss=7.74674
Epoch 13/80: current_loss=8.04981 | best_loss=7.74674
Epoch 14/80: current_loss=8.05478 | best_loss=7.74674
Epoch 15/80: current_loss=8.06635 | best_loss=7.74674
Epoch 16/80: current_loss=8.08104 | best_loss=7.74674
Epoch 17/80: current_loss=8.07897 | best_loss=7.74674
Epoch 18/80: current_loss=8.07301 | best_loss=7.74674
Epoch 19/80: current_loss=8.06693 | best_loss=7.74674
Epoch 20/80: current_loss=8.05218 | best_loss=7.74674
Epoch 21/80: current_loss=8.09841 | best_loss=7.74674
Epoch 22/80: current_loss=8.14630 | best_loss=7.74674
Epoch 23/80: current_loss=8.13321 | best_loss=7.74674
Epoch 24/80: current_loss=8.09010 | best_loss=7.74674
Epoch 25/80: current_loss=8.04865 | best_loss=7.74674
Epoch 26/80: current_loss=8.04890 | best_loss=7.74674
Epoch 27/80: current_loss=8.00465 | best_loss=7.74674
Epoch 28/80: current_loss=8.02846 | best_loss=7.74674
Early Stopping at epoch 28
      explained_var=0.00306 | mse_loss=7.57625
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.42830 | best_loss=8.42830
Epoch 1/80: current_loss=8.40457 | best_loss=8.40457
Epoch 2/80: current_loss=8.34212 | best_loss=8.34212
Epoch 3/80: current_loss=8.33338 | best_loss=8.33338
Epoch 4/80: current_loss=8.32642 | best_loss=8.32642
Epoch 5/80: current_loss=8.33314 | best_loss=8.32642
Epoch 6/80: current_loss=8.34530 | best_loss=8.32642
Epoch 7/80: current_loss=8.36856 | best_loss=8.32642
Epoch 8/80: current_loss=8.34813 | best_loss=8.32642
Epoch 9/80: current_loss=8.32992 | best_loss=8.32642
Epoch 10/80: current_loss=8.33658 | best_loss=8.32642
Epoch 11/80: current_loss=8.33574 | best_loss=8.32642
Epoch 12/80: current_loss=8.36458 | best_loss=8.32642
Epoch 13/80: current_loss=8.36424 | best_loss=8.32642
Epoch 14/80: current_loss=8.41372 | best_loss=8.32642
Epoch 15/80: current_loss=8.40402 | best_loss=8.32642
Epoch 16/80: current_loss=8.35719 | best_loss=8.32642
Epoch 17/80: current_loss=8.36895 | best_loss=8.32642
Epoch 18/80: current_loss=8.36243 | best_loss=8.32642
Epoch 19/80: current_loss=8.35135 | best_loss=8.32642
Epoch 20/80: current_loss=8.33817 | best_loss=8.32642
Epoch 21/80: current_loss=8.34321 | best_loss=8.32642
Epoch 22/80: current_loss=8.36010 | best_loss=8.32642
Epoch 23/80: current_loss=8.33739 | best_loss=8.32642
Epoch 24/80: current_loss=8.33994 | best_loss=8.32642
Early Stopping at epoch 24
      explained_var=-0.00224 | mse_loss=8.18178
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.97760 | best_loss=8.97760
Epoch 1/80: current_loss=8.95259 | best_loss=8.95259
Epoch 2/80: current_loss=8.99076 | best_loss=8.95259
Epoch 3/80: current_loss=8.99755 | best_loss=8.95259
Epoch 4/80: current_loss=9.05972 | best_loss=8.95259
Epoch 5/80: current_loss=8.94507 | best_loss=8.94507
Epoch 6/80: current_loss=8.93475 | best_loss=8.93475
Epoch 7/80: current_loss=8.92702 | best_loss=8.92702
Epoch 8/80: current_loss=9.01552 | best_loss=8.92702
Epoch 9/80: current_loss=8.95677 | best_loss=8.92702
Epoch 10/80: current_loss=8.87087 | best_loss=8.87087
Epoch 11/80: current_loss=8.97574 | best_loss=8.87087
Epoch 12/80: current_loss=9.05057 | best_loss=8.87087
Epoch 13/80: current_loss=8.94466 | best_loss=8.87087
Epoch 14/80: current_loss=8.98205 | best_loss=8.87087
Epoch 15/80: current_loss=8.89719 | best_loss=8.87087
Epoch 16/80: current_loss=9.00372 | best_loss=8.87087
Epoch 17/80: current_loss=8.97378 | best_loss=8.87087
Epoch 18/80: current_loss=9.00137 | best_loss=8.87087
Epoch 19/80: current_loss=9.01584 | best_loss=8.87087
Epoch 20/80: current_loss=8.96708 | best_loss=8.87087
Epoch 21/80: current_loss=8.86798 | best_loss=8.86798
Epoch 22/80: current_loss=8.90908 | best_loss=8.86798
Epoch 23/80: current_loss=8.88679 | best_loss=8.86798
Epoch 24/80: current_loss=8.89822 | best_loss=8.86798
Epoch 25/80: current_loss=9.13360 | best_loss=8.86798
Epoch 26/80: current_loss=8.97924 | best_loss=8.86798
Epoch 27/80: current_loss=8.88020 | best_loss=8.86798
Epoch 28/80: current_loss=8.92598 | best_loss=8.86798
Epoch 29/80: current_loss=8.96873 | best_loss=8.86798
Epoch 30/80: current_loss=8.99549 | best_loss=8.86798
Epoch 31/80: current_loss=9.05095 | best_loss=8.86798
Epoch 32/80: current_loss=8.97155 | best_loss=8.86798
Epoch 33/80: current_loss=8.98204 | best_loss=8.86798
Epoch 34/80: current_loss=8.97093 | best_loss=8.86798
Epoch 35/80: current_loss=8.90233 | best_loss=8.86798
Epoch 36/80: current_loss=8.98216 | best_loss=8.86798
Epoch 37/80: current_loss=8.93728 | best_loss=8.86798
Epoch 38/80: current_loss=8.99061 | best_loss=8.86798
Epoch 39/80: current_loss=8.97936 | best_loss=8.86798
Epoch 40/80: current_loss=8.91430 | best_loss=8.86798
Epoch 41/80: current_loss=8.98422 | best_loss=8.86798
Early Stopping at epoch 41
      explained_var=-0.00276 | mse_loss=8.64885
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.22028 | best_loss=8.22028
Epoch 1/80: current_loss=8.22190 | best_loss=8.22028
Epoch 2/80: current_loss=8.22507 | best_loss=8.22028
Epoch 3/80: current_loss=8.24573 | best_loss=8.22028
Epoch 4/80: current_loss=8.22289 | best_loss=8.22028
Epoch 5/80: current_loss=8.24138 | best_loss=8.22028
Epoch 6/80: current_loss=8.22313 | best_loss=8.22028
Epoch 7/80: current_loss=8.21603 | best_loss=8.21603
Epoch 8/80: current_loss=8.21615 | best_loss=8.21603
Epoch 9/80: current_loss=8.22345 | best_loss=8.21603
Epoch 10/80: current_loss=8.22728 | best_loss=8.21603
Epoch 11/80: current_loss=8.21841 | best_loss=8.21603
Epoch 12/80: current_loss=8.24185 | best_loss=8.21603
Epoch 13/80: current_loss=8.21836 | best_loss=8.21603
Epoch 14/80: current_loss=8.23130 | best_loss=8.21603
Epoch 15/80: current_loss=8.22444 | best_loss=8.21603
Epoch 16/80: current_loss=8.22326 | best_loss=8.21603
Epoch 17/80: current_loss=8.21998 | best_loss=8.21603
Epoch 18/80: current_loss=8.22603 | best_loss=8.21603
Epoch 19/80: current_loss=8.22023 | best_loss=8.21603
Epoch 20/80: current_loss=8.21884 | best_loss=8.21603
Epoch 21/80: current_loss=8.25048 | best_loss=8.21603
Epoch 22/80: current_loss=8.22385 | best_loss=8.21603
Epoch 23/80: current_loss=8.22322 | best_loss=8.21603
Epoch 24/80: current_loss=8.22367 | best_loss=8.21603
Epoch 25/80: current_loss=8.23100 | best_loss=8.21603
Epoch 26/80: current_loss=8.22626 | best_loss=8.21603
Epoch 27/80: current_loss=8.22383 | best_loss=8.21603
Early Stopping at epoch 27
      explained_var=0.00221 | mse_loss=8.31381
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.74875 | best_loss=7.74875
Epoch 1/80: current_loss=7.70000 | best_loss=7.70000
Epoch 2/80: current_loss=7.69798 | best_loss=7.69798
Epoch 3/80: current_loss=7.70401 | best_loss=7.69798
Epoch 4/80: current_loss=7.69284 | best_loss=7.69284
Epoch 5/80: current_loss=7.69560 | best_loss=7.69284
Epoch 6/80: current_loss=7.69866 | best_loss=7.69284
Epoch 7/80: current_loss=7.69557 | best_loss=7.69284
Epoch 8/80: current_loss=7.70147 | best_loss=7.69284
Epoch 9/80: current_loss=7.71468 | best_loss=7.69284
Epoch 10/80: current_loss=7.71349 | best_loss=7.69284
Epoch 11/80: current_loss=7.70331 | best_loss=7.69284
Epoch 12/80: current_loss=7.69241 | best_loss=7.69241
Epoch 13/80: current_loss=7.69681 | best_loss=7.69241
Epoch 14/80: current_loss=7.69519 | best_loss=7.69241
Epoch 15/80: current_loss=7.69433 | best_loss=7.69241
Epoch 16/80: current_loss=7.72085 | best_loss=7.69241
Epoch 17/80: current_loss=7.70368 | best_loss=7.69241
Epoch 18/80: current_loss=7.69401 | best_loss=7.69241
Epoch 19/80: current_loss=7.71048 | best_loss=7.69241
Epoch 20/80: current_loss=7.69466 | best_loss=7.69241
Epoch 21/80: current_loss=7.69363 | best_loss=7.69241
Epoch 22/80: current_loss=7.68833 | best_loss=7.68833
Epoch 23/80: current_loss=7.69776 | best_loss=7.68833
Epoch 24/80: current_loss=7.71127 | best_loss=7.68833
Epoch 25/80: current_loss=7.69884 | best_loss=7.68833
Epoch 26/80: current_loss=7.69847 | best_loss=7.68833
Epoch 27/80: current_loss=7.68571 | best_loss=7.68571
Epoch 28/80: current_loss=7.70254 | best_loss=7.68571
Epoch 29/80: current_loss=7.70684 | best_loss=7.68571
Epoch 30/80: current_loss=7.70754 | best_loss=7.68571
Epoch 31/80: current_loss=7.68783 | best_loss=7.68571
Epoch 32/80: current_loss=7.72092 | best_loss=7.68571
Epoch 33/80: current_loss=7.70455 | best_loss=7.68571
Epoch 34/80: current_loss=7.71985 | best_loss=7.68571
Epoch 35/80: current_loss=7.71905 | best_loss=7.68571
Epoch 36/80: current_loss=7.70802 | best_loss=7.68571
Epoch 37/80: current_loss=7.71089 | best_loss=7.68571
Epoch 38/80: current_loss=7.72649 | best_loss=7.68571
Epoch 39/80: current_loss=7.77327 | best_loss=7.68571
Epoch 40/80: current_loss=7.80135 | best_loss=7.68571
Epoch 41/80: current_loss=7.79804 | best_loss=7.68571
Epoch 42/80: current_loss=7.70950 | best_loss=7.68571
Epoch 43/80: current_loss=7.70140 | best_loss=7.68571
Epoch 44/80: current_loss=7.70513 | best_loss=7.68571
Epoch 45/80: current_loss=7.69848 | best_loss=7.68571
Epoch 46/80: current_loss=7.69838 | best_loss=7.68571
Epoch 47/80: current_loss=7.69344 | best_loss=7.68571
Early Stopping at epoch 47
      explained_var=-0.00085 | mse_loss=7.86332
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=-0.00012| avg_loss=8.11680
----------------------------------------------


----------------------------------------------
Params for Trial 38
{'learning_rate': 0.0001, 'weight_decay': 0.0026805551965060678, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=72.32722 | best_loss=72.32722
Epoch 1/80: current_loss=32.04428 | best_loss=32.04428
Epoch 2/80: current_loss=17.34096 | best_loss=17.34096
Epoch 3/80: current_loss=14.58235 | best_loss=14.58235
Epoch 4/80: current_loss=14.35717 | best_loss=14.35717
Epoch 5/80: current_loss=14.25893 | best_loss=14.25893
Epoch 6/80: current_loss=13.96517 | best_loss=13.96517
Epoch 7/80: current_loss=13.60708 | best_loss=13.60708
Epoch 8/80: current_loss=13.35670 | best_loss=13.35670
Epoch 9/80: current_loss=13.14441 | best_loss=13.14441
Epoch 10/80: current_loss=12.93964 | best_loss=12.93964
Epoch 11/80: current_loss=12.78207 | best_loss=12.78207
Epoch 12/80: current_loss=12.68442 | best_loss=12.68442
Epoch 13/80: current_loss=12.59007 | best_loss=12.59007
Epoch 14/80: current_loss=12.44552 | best_loss=12.44552
Epoch 15/80: current_loss=12.34940 | best_loss=12.34940
Epoch 16/80: current_loss=12.21598 | best_loss=12.21598
Epoch 17/80: current_loss=12.10725 | best_loss=12.10725
Epoch 18/80: current_loss=11.98833 | best_loss=11.98833
Epoch 19/80: current_loss=11.78381 | best_loss=11.78381
Epoch 20/80: current_loss=11.79294 | best_loss=11.78381
Epoch 21/80: current_loss=11.68395 | best_loss=11.68395
Epoch 22/80: current_loss=11.60490 | best_loss=11.60490
Epoch 23/80: current_loss=11.59145 | best_loss=11.59145
Epoch 24/80: current_loss=11.52164 | best_loss=11.52164
Epoch 25/80: current_loss=11.40940 | best_loss=11.40940
Epoch 26/80: current_loss=11.32443 | best_loss=11.32443
Epoch 27/80: current_loss=11.27586 | best_loss=11.27586
Epoch 28/80: current_loss=11.23882 | best_loss=11.23882
Epoch 29/80: current_loss=11.16747 | best_loss=11.16747
Epoch 30/80: current_loss=11.09523 | best_loss=11.09523
Epoch 31/80: current_loss=11.07308 | best_loss=11.07308
Epoch 32/80: current_loss=11.00486 | best_loss=11.00486
Epoch 33/80: current_loss=10.94219 | best_loss=10.94219
Epoch 34/80: current_loss=10.89660 | best_loss=10.89660
Epoch 35/80: current_loss=10.82294 | best_loss=10.82294
Epoch 36/80: current_loss=10.85465 | best_loss=10.82294
Epoch 37/80: current_loss=10.75809 | best_loss=10.75809
Epoch 38/80: current_loss=10.69007 | best_loss=10.69007
Epoch 39/80: current_loss=10.74015 | best_loss=10.69007
Epoch 40/80: current_loss=10.75480 | best_loss=10.69007
Epoch 41/80: current_loss=10.61732 | best_loss=10.61732
Epoch 42/80: current_loss=10.58121 | best_loss=10.58121
Epoch 43/80: current_loss=10.43315 | best_loss=10.43315
Epoch 44/80: current_loss=10.41080 | best_loss=10.41080
Epoch 45/80: current_loss=10.43280 | best_loss=10.41080
Epoch 46/80: current_loss=10.46921 | best_loss=10.41080
Epoch 47/80: current_loss=10.46662 | best_loss=10.41080
Epoch 48/80: current_loss=10.46253 | best_loss=10.41080
Epoch 49/80: current_loss=10.40235 | best_loss=10.40235
Epoch 50/80: current_loss=10.37219 | best_loss=10.37219
Epoch 51/80: current_loss=10.31121 | best_loss=10.31121
Epoch 52/80: current_loss=10.30760 | best_loss=10.30760
Epoch 53/80: current_loss=10.24526 | best_loss=10.24526
Epoch 54/80: current_loss=10.17438 | best_loss=10.17438
Epoch 55/80: current_loss=10.20565 | best_loss=10.17438
Epoch 56/80: current_loss=10.14074 | best_loss=10.14074
Epoch 57/80: current_loss=10.04187 | best_loss=10.04187
Epoch 58/80: current_loss=10.04894 | best_loss=10.04187
Epoch 59/80: current_loss=10.08973 | best_loss=10.04187
Epoch 60/80: current_loss=10.05145 | best_loss=10.04187
Epoch 61/80: current_loss=9.99814 | best_loss=9.99814
Epoch 62/80: current_loss=9.98520 | best_loss=9.98520
Epoch 63/80: current_loss=10.01956 | best_loss=9.98520
Epoch 64/80: current_loss=10.12328 | best_loss=9.98520
Epoch 65/80: current_loss=9.98189 | best_loss=9.98189
Epoch 66/80: current_loss=9.90995 | best_loss=9.90995
Epoch 67/80: current_loss=9.96021 | best_loss=9.90995
Epoch 68/80: current_loss=9.87572 | best_loss=9.87572
Epoch 69/80: current_loss=9.85118 | best_loss=9.85118
Epoch 70/80: current_loss=9.82517 | best_loss=9.82517
Epoch 71/80: current_loss=9.81631 | best_loss=9.81631
Epoch 72/80: current_loss=9.90533 | best_loss=9.81631
Epoch 73/80: current_loss=9.89202 | best_loss=9.81631
Epoch 74/80: current_loss=9.87003 | best_loss=9.81631
Epoch 75/80: current_loss=9.89257 | best_loss=9.81631
Epoch 76/80: current_loss=9.79928 | best_loss=9.79928
Epoch 77/80: current_loss=9.75148 | best_loss=9.75148
Epoch 78/80: current_loss=9.75143 | best_loss=9.75143
Epoch 79/80: current_loss=9.70073 | best_loss=9.70073
      explained_var=-0.24799 | mse_loss=9.60823

----------------------------------------------
Params for Trial 39
{'learning_rate': 1e-05, 'weight_decay': 0.003807223469893579, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=81.28914 | best_loss=81.28914
Epoch 1/80: current_loss=80.85149 | best_loss=80.85149
Epoch 2/80: current_loss=80.40469 | best_loss=80.40469
Epoch 3/80: current_loss=79.94154 | best_loss=79.94154
Epoch 4/80: current_loss=79.46393 | best_loss=79.46393
Epoch 5/80: current_loss=78.96037 | best_loss=78.96037
Epoch 6/80: current_loss=78.43297 | best_loss=78.43297
Epoch 7/80: current_loss=77.86369 | best_loss=77.86369
Epoch 8/80: current_loss=77.25242 | best_loss=77.25242
Epoch 9/80: current_loss=76.58719 | best_loss=76.58719
Epoch 10/80: current_loss=75.86003 | best_loss=75.86003
Epoch 11/80: current_loss=75.05892 | best_loss=75.05892
Epoch 12/80: current_loss=74.17430 | best_loss=74.17430
Epoch 13/80: current_loss=73.18711 | best_loss=73.18711
Epoch 14/80: current_loss=72.10373 | best_loss=72.10373
Epoch 15/80: current_loss=70.88752 | best_loss=70.88752
Epoch 16/80: current_loss=69.54092 | best_loss=69.54092
Epoch 17/80: current_loss=68.06284 | best_loss=68.06284
Epoch 18/80: current_loss=66.46425 | best_loss=66.46425
Epoch 19/80: current_loss=64.75249 | best_loss=64.75249
Epoch 20/80: current_loss=62.93909 | best_loss=62.93909
Epoch 21/80: current_loss=61.06690 | best_loss=61.06690
Epoch 22/80: current_loss=59.14930 | best_loss=59.14930
Epoch 23/80: current_loss=57.24319 | best_loss=57.24319
Epoch 24/80: current_loss=55.36718 | best_loss=55.36718
Epoch 25/80: current_loss=53.54793 | best_loss=53.54793
Epoch 26/80: current_loss=51.78947 | best_loss=51.78947
Epoch 27/80: current_loss=50.11457 | best_loss=50.11457
Epoch 28/80: current_loss=48.52729 | best_loss=48.52729
Epoch 29/80: current_loss=47.02701 | best_loss=47.02701
Epoch 30/80: current_loss=45.61239 | best_loss=45.61239
Epoch 31/80: current_loss=44.32224 | best_loss=44.32224
Epoch 32/80: current_loss=43.09696 | best_loss=43.09696
Epoch 33/80: current_loss=41.95519 | best_loss=41.95519
Epoch 34/80: current_loss=40.89577 | best_loss=40.89577
Epoch 35/80: current_loss=39.88226 | best_loss=39.88226
Epoch 36/80: current_loss=38.96246 | best_loss=38.96246
Epoch 37/80: current_loss=38.09132 | best_loss=38.09132
Epoch 38/80: current_loss=37.27830 | best_loss=37.27830
Epoch 39/80: current_loss=36.49478 | best_loss=36.49478
Epoch 40/80: current_loss=35.76187 | best_loss=35.76187
Epoch 41/80: current_loss=35.05993 | best_loss=35.05993
Epoch 42/80: current_loss=34.39454 | best_loss=34.39454
Epoch 43/80: current_loss=33.75883 | best_loss=33.75883
Epoch 44/80: current_loss=33.16497 | best_loss=33.16497
Epoch 45/80: current_loss=32.60702 | best_loss=32.60702
Epoch 46/80: current_loss=32.06848 | best_loss=32.06848
Epoch 47/80: current_loss=31.55351 | best_loss=31.55351
Epoch 48/80: current_loss=31.05864 | best_loss=31.05864
Epoch 49/80: current_loss=30.59126 | best_loss=30.59126
Epoch 50/80: current_loss=30.13513 | best_loss=30.13513
Epoch 51/80: current_loss=29.70104 | best_loss=29.70104
Epoch 52/80: current_loss=29.28493 | best_loss=29.28493
Epoch 53/80: current_loss=28.88867 | best_loss=28.88867
Epoch 54/80: current_loss=28.50201 | best_loss=28.50201
Epoch 55/80: current_loss=28.12821 | best_loss=28.12821
Epoch 56/80: current_loss=27.76355 | best_loss=27.76355
Epoch 57/80: current_loss=27.41739 | best_loss=27.41739
Epoch 58/80: current_loss=27.07881 | best_loss=27.07881
Epoch 59/80: current_loss=26.75247 | best_loss=26.75247
Epoch 60/80: current_loss=26.43562 | best_loss=26.43562
Epoch 61/80: current_loss=26.13374 | best_loss=26.13374
Epoch 62/80: current_loss=25.83178 | best_loss=25.83178
Epoch 63/80: current_loss=25.53299 | best_loss=25.53299
Epoch 64/80: current_loss=25.24803 | best_loss=25.24803
Epoch 65/80: current_loss=24.97152 | best_loss=24.97152
Epoch 66/80: current_loss=24.70083 | best_loss=24.70083
Epoch 67/80: current_loss=24.43004 | best_loss=24.43004
Epoch 68/80: current_loss=24.17455 | best_loss=24.17455
Epoch 69/80: current_loss=23.92337 | best_loss=23.92337
Epoch 70/80: current_loss=23.67716 | best_loss=23.67716
Epoch 71/80: current_loss=23.43608 | best_loss=23.43608
Epoch 72/80: current_loss=23.19997 | best_loss=23.19997
Epoch 73/80: current_loss=22.97345 | best_loss=22.97345
Epoch 74/80: current_loss=22.75112 | best_loss=22.75112
Epoch 75/80: current_loss=22.53259 | best_loss=22.53259
Epoch 76/80: current_loss=22.31825 | best_loss=22.31825
Epoch 77/80: current_loss=22.10983 | best_loss=22.10983
Epoch 78/80: current_loss=21.90565 | best_loss=21.90565
Epoch 79/80: current_loss=21.71176 | best_loss=21.71176
      explained_var=-0.15169 | mse_loss=21.20733

----------------------------------------------
Params for Trial 40
{'learning_rate': 0.1, 'weight_decay': 0.005285767319840878, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=34.20308 | best_loss=34.20308
Epoch 1/80: current_loss=18.67959 | best_loss=18.67959
Epoch 2/80: current_loss=52.53417 | best_loss=18.67959
Epoch 3/80: current_loss=24.04478 | best_loss=18.67959
Epoch 4/80: current_loss=10.03770 | best_loss=10.03770
Epoch 5/80: current_loss=8.25864 | best_loss=8.25864
Epoch 6/80: current_loss=8.26002 | best_loss=8.25864
Epoch 7/80: current_loss=8.86144 | best_loss=8.25864
Epoch 8/80: current_loss=10.18935 | best_loss=8.25864
Epoch 9/80: current_loss=8.50059 | best_loss=8.25864
Epoch 10/80: current_loss=8.73638 | best_loss=8.25864
Epoch 11/80: current_loss=8.00791 | best_loss=8.00791
Epoch 12/80: current_loss=9.36674 | best_loss=8.00791
Epoch 13/80: current_loss=7.94170 | best_loss=7.94170
Epoch 14/80: current_loss=8.15382 | best_loss=7.94170
Epoch 15/80: current_loss=8.22562 | best_loss=7.94170
Epoch 16/80: current_loss=7.91553 | best_loss=7.91553
Epoch 17/80: current_loss=8.57384 | best_loss=7.91553
Epoch 18/80: current_loss=9.32599 | best_loss=7.91553
Epoch 19/80: current_loss=8.52148 | best_loss=7.91553
Epoch 20/80: current_loss=8.66581 | best_loss=7.91553
Epoch 21/80: current_loss=7.90078 | best_loss=7.90078
Epoch 22/80: current_loss=8.51962 | best_loss=7.90078
Epoch 23/80: current_loss=8.47099 | best_loss=7.90078
Epoch 24/80: current_loss=7.90875 | best_loss=7.90078
Epoch 25/80: current_loss=11.00856 | best_loss=7.90078
Epoch 26/80: current_loss=8.15296 | best_loss=7.90078
Epoch 27/80: current_loss=8.62949 | best_loss=7.90078
Epoch 28/80: current_loss=9.56634 | best_loss=7.90078
Epoch 29/80: current_loss=9.29420 | best_loss=7.90078
Epoch 30/80: current_loss=12.69834 | best_loss=7.90078
Epoch 31/80: current_loss=8.13133 | best_loss=7.90078
Epoch 32/80: current_loss=9.03695 | best_loss=7.90078
Epoch 33/80: current_loss=10.03831 | best_loss=7.90078
Epoch 34/80: current_loss=8.05889 | best_loss=7.90078
Epoch 35/80: current_loss=8.75052 | best_loss=7.90078
Epoch 36/80: current_loss=8.64754 | best_loss=7.90078
Epoch 37/80: current_loss=8.32609 | best_loss=7.90078
Epoch 38/80: current_loss=8.30819 | best_loss=7.90078
Epoch 39/80: current_loss=7.94556 | best_loss=7.90078
Epoch 40/80: current_loss=8.69987 | best_loss=7.90078
Epoch 41/80: current_loss=8.66225 | best_loss=7.90078
Early Stopping at epoch 41
      explained_var=-0.01459 | mse_loss=7.72297

----------------------------------------------
Params for Trial 41
{'learning_rate': 0.1, 'weight_decay': 0.004547582847824539, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=19.67291 | best_loss=19.67291
Epoch 1/80: current_loss=8.93933 | best_loss=8.93933
Epoch 2/80: current_loss=8.00032 | best_loss=8.00032
Epoch 3/80: current_loss=8.15735 | best_loss=8.00032
Epoch 4/80: current_loss=9.41270 | best_loss=8.00032
Epoch 5/80: current_loss=11.74907 | best_loss=8.00032
Epoch 6/80: current_loss=9.63388 | best_loss=8.00032
Epoch 7/80: current_loss=8.58900 | best_loss=8.00032
Epoch 8/80: current_loss=9.25204 | best_loss=8.00032
Epoch 9/80: current_loss=10.29520 | best_loss=8.00032
Epoch 10/80: current_loss=8.33899 | best_loss=8.00032
Epoch 11/80: current_loss=9.10678 | best_loss=8.00032
Epoch 12/80: current_loss=7.98594 | best_loss=7.98594
Epoch 13/80: current_loss=7.60846 | best_loss=7.60846
Epoch 14/80: current_loss=11.59715 | best_loss=7.60846
Epoch 15/80: current_loss=7.69101 | best_loss=7.60846
Epoch 16/80: current_loss=7.80973 | best_loss=7.60846
Epoch 17/80: current_loss=10.65820 | best_loss=7.60846
Epoch 18/80: current_loss=10.28718 | best_loss=7.60846
Epoch 19/80: current_loss=13.48973 | best_loss=7.60846
Epoch 20/80: current_loss=8.77305 | best_loss=7.60846
Epoch 21/80: current_loss=8.09494 | best_loss=7.60846
Epoch 22/80: current_loss=9.94720 | best_loss=7.60846
Epoch 23/80: current_loss=9.00784 | best_loss=7.60846
Epoch 24/80: current_loss=9.20450 | best_loss=7.60846
Epoch 25/80: current_loss=9.38186 | best_loss=7.60846
Epoch 26/80: current_loss=9.72363 | best_loss=7.60846
Epoch 27/80: current_loss=8.24433 | best_loss=7.60846
Epoch 28/80: current_loss=10.10852 | best_loss=7.60846
Epoch 29/80: current_loss=12.10328 | best_loss=7.60846
Epoch 30/80: current_loss=8.11832 | best_loss=7.60846
Epoch 31/80: current_loss=12.78582 | best_loss=7.60846
Epoch 32/80: current_loss=7.84382 | best_loss=7.60846
Epoch 33/80: current_loss=7.88916 | best_loss=7.60846
Early Stopping at epoch 33
      explained_var=0.02397 | mse_loss=7.42263
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=26.26601 | best_loss=26.26601
Epoch 1/80: current_loss=9.55697 | best_loss=9.55697
Epoch 2/80: current_loss=9.60150 | best_loss=9.55697
Epoch 3/80: current_loss=10.57640 | best_loss=9.55697
Epoch 4/80: current_loss=14.26819 | best_loss=9.55697
Epoch 5/80: current_loss=10.76241 | best_loss=9.55697
Epoch 6/80: current_loss=8.80719 | best_loss=8.80719
Epoch 7/80: current_loss=13.16154 | best_loss=8.80719
Epoch 8/80: current_loss=12.76072 | best_loss=8.80719
Epoch 9/80: current_loss=11.72890 | best_loss=8.80719
Epoch 10/80: current_loss=8.65385 | best_loss=8.65385
Epoch 11/80: current_loss=8.78035 | best_loss=8.65385
Epoch 12/80: current_loss=10.44950 | best_loss=8.65385
Epoch 13/80: current_loss=9.47365 | best_loss=8.65385
Epoch 14/80: current_loss=14.68590 | best_loss=8.65385
Epoch 15/80: current_loss=8.43291 | best_loss=8.43291
Epoch 16/80: current_loss=10.00213 | best_loss=8.43291
Epoch 17/80: current_loss=9.35485 | best_loss=8.43291
Epoch 18/80: current_loss=8.39293 | best_loss=8.39293
Epoch 19/80: current_loss=8.47201 | best_loss=8.39293
Epoch 20/80: current_loss=14.18317 | best_loss=8.39293
Epoch 21/80: current_loss=8.36122 | best_loss=8.36122
Epoch 22/80: current_loss=17.56675 | best_loss=8.36122
Epoch 23/80: current_loss=8.33658 | best_loss=8.33658
Epoch 24/80: current_loss=12.16762 | best_loss=8.33658
Epoch 25/80: current_loss=8.32722 | best_loss=8.32722
Epoch 26/80: current_loss=8.66355 | best_loss=8.32722
Epoch 27/80: current_loss=8.33972 | best_loss=8.32722
Epoch 28/80: current_loss=11.00381 | best_loss=8.32722
Epoch 29/80: current_loss=9.71413 | best_loss=8.32722
Epoch 30/80: current_loss=9.96132 | best_loss=8.32722
Epoch 31/80: current_loss=15.51005 | best_loss=8.32722
Epoch 32/80: current_loss=9.08545 | best_loss=8.32722
Epoch 33/80: current_loss=8.50012 | best_loss=8.32722
Epoch 34/80: current_loss=8.92047 | best_loss=8.32722
Epoch 35/80: current_loss=11.57951 | best_loss=8.32722
Epoch 36/80: current_loss=8.79736 | best_loss=8.32722
Epoch 37/80: current_loss=8.80004 | best_loss=8.32722
Epoch 38/80: current_loss=8.32385 | best_loss=8.32385
Epoch 39/80: current_loss=8.39258 | best_loss=8.32385
Epoch 40/80: current_loss=47.71866 | best_loss=8.32385
Epoch 41/80: current_loss=25.27786 | best_loss=8.32385
Epoch 42/80: current_loss=13.35964 | best_loss=8.32385
Epoch 43/80: current_loss=10.33857 | best_loss=8.32385
Epoch 44/80: current_loss=8.74749 | best_loss=8.32385
Epoch 45/80: current_loss=9.32384 | best_loss=8.32385
Epoch 46/80: current_loss=8.29212 | best_loss=8.29212
Epoch 47/80: current_loss=8.40808 | best_loss=8.29212
Epoch 48/80: current_loss=8.29211 | best_loss=8.29211
Epoch 49/80: current_loss=8.37355 | best_loss=8.29211
Epoch 50/80: current_loss=8.76832 | best_loss=8.29211
Epoch 51/80: current_loss=12.65516 | best_loss=8.29211
Epoch 52/80: current_loss=8.60688 | best_loss=8.29211
Epoch 53/80: current_loss=8.50446 | best_loss=8.29211
Epoch 54/80: current_loss=10.16742 | best_loss=8.29211
Epoch 55/80: current_loss=8.69945 | best_loss=8.29211
Epoch 56/80: current_loss=10.67317 | best_loss=8.29211
Epoch 57/80: current_loss=9.47479 | best_loss=8.29211
Epoch 58/80: current_loss=9.75700 | best_loss=8.29211
Epoch 59/80: current_loss=8.30632 | best_loss=8.29211
Epoch 60/80: current_loss=11.34799 | best_loss=8.29211
Epoch 61/80: current_loss=9.03056 | best_loss=8.29211
Epoch 62/80: current_loss=9.61684 | best_loss=8.29211
Epoch 63/80: current_loss=8.45303 | best_loss=8.29211
Epoch 64/80: current_loss=12.41934 | best_loss=8.29211
Epoch 65/80: current_loss=15.93001 | best_loss=8.29211
Epoch 66/80: current_loss=8.51225 | best_loss=8.29211
Epoch 67/80: current_loss=9.04909 | best_loss=8.29211
Epoch 68/80: current_loss=12.68145 | best_loss=8.29211
Early Stopping at epoch 68
      explained_var=0.00176 | mse_loss=8.15338
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.63722 | best_loss=10.63722
Epoch 1/80: current_loss=18.55555 | best_loss=10.63722
Epoch 2/80: current_loss=9.93842 | best_loss=9.93842
Epoch 3/80: current_loss=16.09426 | best_loss=9.93842
Epoch 4/80: current_loss=8.55126 | best_loss=8.55126
Epoch 5/80: current_loss=9.82348 | best_loss=8.55126
Epoch 6/80: current_loss=12.13488 | best_loss=8.55126
Epoch 7/80: current_loss=10.57840 | best_loss=8.55126
Epoch 8/80: current_loss=8.68452 | best_loss=8.55126
Epoch 9/80: current_loss=8.99662 | best_loss=8.55126
Epoch 10/80: current_loss=13.75623 | best_loss=8.55126
Epoch 11/80: current_loss=18.63464 | best_loss=8.55126
Epoch 12/80: current_loss=9.86630 | best_loss=8.55126
Epoch 13/80: current_loss=9.75311 | best_loss=8.55126
Epoch 14/80: current_loss=11.42101 | best_loss=8.55126
Epoch 15/80: current_loss=13.34758 | best_loss=8.55126
Epoch 16/80: current_loss=10.73817 | best_loss=8.55126
Epoch 17/80: current_loss=11.00099 | best_loss=8.55126
Epoch 18/80: current_loss=8.78320 | best_loss=8.55126
Epoch 19/80: current_loss=9.74796 | best_loss=8.55126
Epoch 20/80: current_loss=10.71627 | best_loss=8.55126
Epoch 21/80: current_loss=9.80686 | best_loss=8.55126
Epoch 22/80: current_loss=10.98300 | best_loss=8.55126
Epoch 23/80: current_loss=8.52727 | best_loss=8.52727
Epoch 24/80: current_loss=12.18502 | best_loss=8.52727
Epoch 25/80: current_loss=9.82252 | best_loss=8.52727
Epoch 26/80: current_loss=10.95640 | best_loss=8.52727
Epoch 27/80: current_loss=12.62495 | best_loss=8.52727
Epoch 28/80: current_loss=12.00866 | best_loss=8.52727
Epoch 29/80: current_loss=9.30308 | best_loss=8.52727
Epoch 30/80: current_loss=8.98067 | best_loss=8.52727
Epoch 31/80: current_loss=8.43266 | best_loss=8.43266
Epoch 32/80: current_loss=11.27397 | best_loss=8.43266
Epoch 33/80: current_loss=8.90067 | best_loss=8.43266
Epoch 34/80: current_loss=9.87030 | best_loss=8.43266
Epoch 35/80: current_loss=11.06595 | best_loss=8.43266
Epoch 36/80: current_loss=9.34718 | best_loss=8.43266
Epoch 37/80: current_loss=8.42993 | best_loss=8.42993
Epoch 38/80: current_loss=12.86581 | best_loss=8.42993
Epoch 39/80: current_loss=10.39100 | best_loss=8.42993
Epoch 40/80: current_loss=11.64374 | best_loss=8.42993
Epoch 41/80: current_loss=14.53447 | best_loss=8.42993
Epoch 42/80: current_loss=9.38688 | best_loss=8.42993
Epoch 43/80: current_loss=8.57628 | best_loss=8.42993
Epoch 44/80: current_loss=10.39726 | best_loss=8.42993
Epoch 45/80: current_loss=10.27334 | best_loss=8.42993
Epoch 46/80: current_loss=11.36435 | best_loss=8.42993
Epoch 47/80: current_loss=12.23356 | best_loss=8.42993
Epoch 48/80: current_loss=8.59880 | best_loss=8.42993
Epoch 49/80: current_loss=13.23818 | best_loss=8.42993
Epoch 50/80: current_loss=9.02265 | best_loss=8.42993
Epoch 51/80: current_loss=10.88083 | best_loss=8.42993
Epoch 52/80: current_loss=9.86121 | best_loss=8.42993
Epoch 53/80: current_loss=10.51370 | best_loss=8.42993
Epoch 54/80: current_loss=10.18808 | best_loss=8.42993
Epoch 55/80: current_loss=9.04805 | best_loss=8.42993
Epoch 56/80: current_loss=9.86930 | best_loss=8.42993
Epoch 57/80: current_loss=10.94974 | best_loss=8.42993
Early Stopping at epoch 57
      explained_var=0.05189 | mse_loss=8.22308
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.80432 | best_loss=8.80432
Epoch 1/80: current_loss=16.76820 | best_loss=8.80432
Epoch 2/80: current_loss=9.14833 | best_loss=8.80432
Epoch 3/80: current_loss=10.31368 | best_loss=8.80432
Epoch 4/80: current_loss=9.40902 | best_loss=8.80432
Epoch 5/80: current_loss=11.31499 | best_loss=8.80432
Epoch 6/80: current_loss=9.09222 | best_loss=8.80432
Epoch 7/80: current_loss=10.15422 | best_loss=8.80432
Epoch 8/80: current_loss=8.36341 | best_loss=8.36341
Epoch 9/80: current_loss=9.53813 | best_loss=8.36341
Epoch 10/80: current_loss=8.85953 | best_loss=8.36341
Epoch 11/80: current_loss=8.35981 | best_loss=8.35981
Epoch 12/80: current_loss=9.56314 | best_loss=8.35981
Epoch 13/80: current_loss=9.85706 | best_loss=8.35981
Epoch 14/80: current_loss=9.75321 | best_loss=8.35981
Epoch 15/80: current_loss=11.67110 | best_loss=8.35981
Epoch 16/80: current_loss=8.98695 | best_loss=8.35981
Epoch 17/80: current_loss=14.43059 | best_loss=8.35981
Epoch 18/80: current_loss=8.65795 | best_loss=8.35981
Epoch 19/80: current_loss=11.40154 | best_loss=8.35981
Epoch 20/80: current_loss=10.07299 | best_loss=8.35981
Epoch 21/80: current_loss=9.57522 | best_loss=8.35981
Epoch 22/80: current_loss=8.89373 | best_loss=8.35981
Epoch 23/80: current_loss=9.29323 | best_loss=8.35981
Epoch 24/80: current_loss=11.46924 | best_loss=8.35981
Epoch 25/80: current_loss=8.61577 | best_loss=8.35981
Epoch 26/80: current_loss=10.67259 | best_loss=8.35981
Epoch 27/80: current_loss=9.84570 | best_loss=8.35981
Epoch 28/80: current_loss=9.53666 | best_loss=8.35981
Epoch 29/80: current_loss=11.45235 | best_loss=8.35981
Epoch 30/80: current_loss=8.45673 | best_loss=8.35981
Epoch 31/80: current_loss=14.39639 | best_loss=8.35981
Early Stopping at epoch 31
      explained_var=-0.01449 | mse_loss=8.45425
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.19746 | best_loss=9.19746
Epoch 1/80: current_loss=11.89326 | best_loss=9.19746
Epoch 2/80: current_loss=14.48782 | best_loss=9.19746
Epoch 3/80: current_loss=12.13660 | best_loss=9.19746
Epoch 4/80: current_loss=10.00936 | best_loss=9.19746
Epoch 5/80: current_loss=8.06905 | best_loss=8.06905
Epoch 6/80: current_loss=9.91187 | best_loss=8.06905
Epoch 7/80: current_loss=7.83450 | best_loss=7.83450
Epoch 8/80: current_loss=10.05390 | best_loss=7.83450
Epoch 9/80: current_loss=10.64388 | best_loss=7.83450
Epoch 10/80: current_loss=10.50989 | best_loss=7.83450
Epoch 11/80: current_loss=9.21605 | best_loss=7.83450
Epoch 12/80: current_loss=9.74173 | best_loss=7.83450
Epoch 13/80: current_loss=8.58427 | best_loss=7.83450
Epoch 14/80: current_loss=9.37117 | best_loss=7.83450
Epoch 15/80: current_loss=8.29074 | best_loss=7.83450
Epoch 16/80: current_loss=7.72358 | best_loss=7.72358
Epoch 17/80: current_loss=8.05632 | best_loss=7.72358
Epoch 18/80: current_loss=10.60814 | best_loss=7.72358
Epoch 19/80: current_loss=8.42262 | best_loss=7.72358
Epoch 20/80: current_loss=9.23623 | best_loss=7.72358
Epoch 21/80: current_loss=11.12479 | best_loss=7.72358
Epoch 22/80: current_loss=7.91259 | best_loss=7.72358
Epoch 23/80: current_loss=8.08054 | best_loss=7.72358
Epoch 24/80: current_loss=13.25421 | best_loss=7.72358
Epoch 25/80: current_loss=8.44855 | best_loss=7.72358
Epoch 26/80: current_loss=9.10511 | best_loss=7.72358
Epoch 27/80: current_loss=10.86015 | best_loss=7.72358
Epoch 28/80: current_loss=14.38247 | best_loss=7.72358
Epoch 29/80: current_loss=12.65014 | best_loss=7.72358
Epoch 30/80: current_loss=8.06416 | best_loss=7.72358
Epoch 31/80: current_loss=8.86779 | best_loss=7.72358
Epoch 32/80: current_loss=8.21869 | best_loss=7.72358
Epoch 33/80: current_loss=9.84267 | best_loss=7.72358
Epoch 34/80: current_loss=8.20217 | best_loss=7.72358
Epoch 35/80: current_loss=8.64716 | best_loss=7.72358
Epoch 36/80: current_loss=9.10730 | best_loss=7.72358
Early Stopping at epoch 36
      explained_var=-0.00725 | mse_loss=7.91314
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=0.01117| avg_loss=8.03329
----------------------------------------------


----------------------------------------------
Params for Trial 42
{'learning_rate': 0.1, 'weight_decay': 0.0037665179576084427, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=60.80296 | best_loss=60.80296
Epoch 1/80: current_loss=33.54849 | best_loss=33.54849
Epoch 2/80: current_loss=34.95454 | best_loss=33.54849
Epoch 3/80: current_loss=24.94331 | best_loss=24.94331
Epoch 4/80: current_loss=19.04373 | best_loss=19.04373
Epoch 5/80: current_loss=13.29601 | best_loss=13.29601
Epoch 6/80: current_loss=12.34965 | best_loss=12.34965
Epoch 7/80: current_loss=10.06930 | best_loss=10.06930
Epoch 8/80: current_loss=9.11792 | best_loss=9.11792
Epoch 9/80: current_loss=10.72370 | best_loss=9.11792
Epoch 10/80: current_loss=8.57303 | best_loss=8.57303
Epoch 11/80: current_loss=9.09831 | best_loss=8.57303
Epoch 12/80: current_loss=7.88039 | best_loss=7.88039
Epoch 13/80: current_loss=12.75136 | best_loss=7.88039
Epoch 14/80: current_loss=8.27618 | best_loss=7.88039
Epoch 15/80: current_loss=8.39955 | best_loss=7.88039
Epoch 16/80: current_loss=10.06009 | best_loss=7.88039
Epoch 17/80: current_loss=11.13076 | best_loss=7.88039
Epoch 18/80: current_loss=10.87699 | best_loss=7.88039
Epoch 19/80: current_loss=7.78202 | best_loss=7.78202
Epoch 20/80: current_loss=16.07338 | best_loss=7.78202
Epoch 21/80: current_loss=21.91367 | best_loss=7.78202
Epoch 22/80: current_loss=9.99200 | best_loss=7.78202
Epoch 23/80: current_loss=13.46734 | best_loss=7.78202
Epoch 24/80: current_loss=12.97271 | best_loss=7.78202
Epoch 25/80: current_loss=8.62129 | best_loss=7.78202
Epoch 26/80: current_loss=10.73773 | best_loss=7.78202
Epoch 27/80: current_loss=9.69408 | best_loss=7.78202
Epoch 28/80: current_loss=8.01821 | best_loss=7.78202
Epoch 29/80: current_loss=8.71161 | best_loss=7.78202
Epoch 30/80: current_loss=8.03706 | best_loss=7.78202
Epoch 31/80: current_loss=7.93318 | best_loss=7.78202
Epoch 32/80: current_loss=11.52179 | best_loss=7.78202
Epoch 33/80: current_loss=11.64553 | best_loss=7.78202
Epoch 34/80: current_loss=8.31592 | best_loss=7.78202
Epoch 35/80: current_loss=13.11684 | best_loss=7.78202
Epoch 36/80: current_loss=10.33101 | best_loss=7.78202
Epoch 37/80: current_loss=7.76868 | best_loss=7.76868
Epoch 38/80: current_loss=8.16296 | best_loss=7.76868
Epoch 39/80: current_loss=16.25804 | best_loss=7.76868
Epoch 40/80: current_loss=7.75082 | best_loss=7.75082
Epoch 41/80: current_loss=12.33337 | best_loss=7.75082
Epoch 42/80: current_loss=14.46776 | best_loss=7.75082
Epoch 43/80: current_loss=18.85691 | best_loss=7.75082
Epoch 44/80: current_loss=11.32807 | best_loss=7.75082
Epoch 45/80: current_loss=8.28509 | best_loss=7.75082
Epoch 46/80: current_loss=7.77417 | best_loss=7.75082
Epoch 47/80: current_loss=13.13076 | best_loss=7.75082
Epoch 48/80: current_loss=8.62579 | best_loss=7.75082
Epoch 49/80: current_loss=10.88831 | best_loss=7.75082
Epoch 50/80: current_loss=15.87260 | best_loss=7.75082
Epoch 51/80: current_loss=8.73648 | best_loss=7.75082
Epoch 52/80: current_loss=9.23581 | best_loss=7.75082
Epoch 53/80: current_loss=8.53905 | best_loss=7.75082
Epoch 54/80: current_loss=7.79442 | best_loss=7.75082
Epoch 55/80: current_loss=9.48722 | best_loss=7.75082
Epoch 56/80: current_loss=8.47471 | best_loss=7.75082
Epoch 57/80: current_loss=13.67795 | best_loss=7.75082
Epoch 58/80: current_loss=8.81566 | best_loss=7.75082
Epoch 59/80: current_loss=7.86450 | best_loss=7.75082
Epoch 60/80: current_loss=15.08881 | best_loss=7.75082
Early Stopping at epoch 60
      explained_var=0.00323 | mse_loss=7.58765
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.11245 | best_loss=12.11245
Epoch 1/80: current_loss=15.24800 | best_loss=12.11245
Epoch 2/80: current_loss=8.48779 | best_loss=8.48779
Epoch 3/80: current_loss=8.71778 | best_loss=8.48779
Epoch 4/80: current_loss=8.87393 | best_loss=8.48779
Epoch 5/80: current_loss=9.19502 | best_loss=8.48779
Epoch 6/80: current_loss=9.00644 | best_loss=8.48779
Epoch 7/80: current_loss=9.16580 | best_loss=8.48779
Epoch 8/80: current_loss=10.32299 | best_loss=8.48779
Epoch 9/80: current_loss=10.48035 | best_loss=8.48779
Epoch 10/80: current_loss=9.30641 | best_loss=8.48779
Epoch 11/80: current_loss=8.63159 | best_loss=8.48779
Epoch 12/80: current_loss=12.21181 | best_loss=8.48779
Epoch 13/80: current_loss=8.41461 | best_loss=8.41461
Epoch 14/80: current_loss=9.68777 | best_loss=8.41461
Epoch 15/80: current_loss=8.16243 | best_loss=8.16243
Epoch 16/80: current_loss=11.94808 | best_loss=8.16243
Epoch 17/80: current_loss=14.00543 | best_loss=8.16243
Epoch 18/80: current_loss=14.06458 | best_loss=8.16243
Epoch 19/80: current_loss=17.72152 | best_loss=8.16243
Epoch 20/80: current_loss=10.19622 | best_loss=8.16243
Epoch 21/80: current_loss=8.57515 | best_loss=8.16243
Epoch 22/80: current_loss=10.24139 | best_loss=8.16243
Epoch 23/80: current_loss=12.43590 | best_loss=8.16243
Epoch 24/80: current_loss=8.37790 | best_loss=8.16243
Epoch 25/80: current_loss=9.49962 | best_loss=8.16243
Epoch 26/80: current_loss=9.21831 | best_loss=8.16243
Epoch 27/80: current_loss=9.28387 | best_loss=8.16243
Epoch 28/80: current_loss=8.62564 | best_loss=8.16243
Epoch 29/80: current_loss=9.32306 | best_loss=8.16243
Epoch 30/80: current_loss=8.24322 | best_loss=8.16243
Epoch 31/80: current_loss=8.41050 | best_loss=8.16243
Epoch 32/80: current_loss=9.54751 | best_loss=8.16243
Epoch 33/80: current_loss=8.64897 | best_loss=8.16243
Epoch 34/80: current_loss=8.45478 | best_loss=8.16243
Epoch 35/80: current_loss=8.32999 | best_loss=8.16243
Early Stopping at epoch 35
      explained_var=0.02586 | mse_loss=8.02921
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.06227 | best_loss=10.06227
Epoch 1/80: current_loss=9.56181 | best_loss=9.56181
Epoch 2/80: current_loss=10.08751 | best_loss=9.56181
Epoch 3/80: current_loss=9.93732 | best_loss=9.56181
Epoch 4/80: current_loss=12.77773 | best_loss=9.56181
Epoch 5/80: current_loss=11.12653 | best_loss=9.56181
Epoch 6/80: current_loss=9.92505 | best_loss=9.56181
Epoch 7/80: current_loss=9.39742 | best_loss=9.39742
Epoch 8/80: current_loss=9.00043 | best_loss=9.00043
Epoch 9/80: current_loss=11.07110 | best_loss=9.00043
Epoch 10/80: current_loss=11.48708 | best_loss=9.00043
Epoch 11/80: current_loss=8.86081 | best_loss=8.86081
Epoch 12/80: current_loss=11.14650 | best_loss=8.86081
Epoch 13/80: current_loss=9.56874 | best_loss=8.86081
Epoch 14/80: current_loss=9.41449 | best_loss=8.86081
Epoch 15/80: current_loss=10.44099 | best_loss=8.86081
Epoch 16/80: current_loss=10.75098 | best_loss=8.86081
Epoch 17/80: current_loss=8.77115 | best_loss=8.77115
Epoch 18/80: current_loss=11.59640 | best_loss=8.77115
Epoch 19/80: current_loss=14.20788 | best_loss=8.77115
Epoch 20/80: current_loss=11.41135 | best_loss=8.77115
Epoch 21/80: current_loss=11.76619 | best_loss=8.77115
Epoch 22/80: current_loss=9.75189 | best_loss=8.77115
Epoch 23/80: current_loss=11.23948 | best_loss=8.77115
Epoch 24/80: current_loss=18.43535 | best_loss=8.77115
Epoch 25/80: current_loss=8.92699 | best_loss=8.77115
Epoch 26/80: current_loss=9.19521 | best_loss=8.77115
Epoch 27/80: current_loss=10.05980 | best_loss=8.77115
Epoch 28/80: current_loss=9.84060 | best_loss=8.77115
Epoch 29/80: current_loss=9.78381 | best_loss=8.77115
Epoch 30/80: current_loss=14.60884 | best_loss=8.77115
Epoch 31/80: current_loss=9.15622 | best_loss=8.77115
Epoch 32/80: current_loss=12.33901 | best_loss=8.77115
Epoch 33/80: current_loss=10.86657 | best_loss=8.77115
Epoch 34/80: current_loss=10.41589 | best_loss=8.77115
Epoch 35/80: current_loss=9.53273 | best_loss=8.77115
Epoch 36/80: current_loss=9.18532 | best_loss=8.77115
Epoch 37/80: current_loss=11.31099 | best_loss=8.77115
Early Stopping at epoch 37
      explained_var=0.01675 | mse_loss=8.60985
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.24409 | best_loss=9.24409
Epoch 1/80: current_loss=8.58333 | best_loss=8.58333
Epoch 2/80: current_loss=9.17845 | best_loss=8.58333
Epoch 3/80: current_loss=12.89050 | best_loss=8.58333
Epoch 4/80: current_loss=11.69097 | best_loss=8.58333
Epoch 5/80: current_loss=9.63220 | best_loss=8.58333
Epoch 6/80: current_loss=9.08500 | best_loss=8.58333
Epoch 7/80: current_loss=9.25745 | best_loss=8.58333
Epoch 8/80: current_loss=10.25928 | best_loss=8.58333
Epoch 9/80: current_loss=9.59834 | best_loss=8.58333
Epoch 10/80: current_loss=18.15443 | best_loss=8.58333
Epoch 11/80: current_loss=8.45918 | best_loss=8.45918
Epoch 12/80: current_loss=12.75012 | best_loss=8.45918
Epoch 13/80: current_loss=10.27583 | best_loss=8.45918
Epoch 14/80: current_loss=14.49429 | best_loss=8.45918
Epoch 15/80: current_loss=10.60255 | best_loss=8.45918
Epoch 16/80: current_loss=16.52758 | best_loss=8.45918
Epoch 17/80: current_loss=9.28998 | best_loss=8.45918
Epoch 18/80: current_loss=8.67038 | best_loss=8.45918
Epoch 19/80: current_loss=13.63660 | best_loss=8.45918
Epoch 20/80: current_loss=12.09465 | best_loss=8.45918
Epoch 21/80: current_loss=8.17739 | best_loss=8.17739
Epoch 22/80: current_loss=9.62371 | best_loss=8.17739
Epoch 23/80: current_loss=8.54122 | best_loss=8.17739
Epoch 24/80: current_loss=14.74237 | best_loss=8.17739
Epoch 25/80: current_loss=9.21088 | best_loss=8.17739
Epoch 26/80: current_loss=9.44586 | best_loss=8.17739
Epoch 27/80: current_loss=8.53439 | best_loss=8.17739
Epoch 28/80: current_loss=9.03349 | best_loss=8.17739
Epoch 29/80: current_loss=9.40777 | best_loss=8.17739
Epoch 30/80: current_loss=9.20267 | best_loss=8.17739
Epoch 31/80: current_loss=8.97488 | best_loss=8.17739
Epoch 32/80: current_loss=8.51638 | best_loss=8.17739
Epoch 33/80: current_loss=8.56721 | best_loss=8.17739
Epoch 34/80: current_loss=12.75853 | best_loss=8.17739
Epoch 35/80: current_loss=13.54672 | best_loss=8.17739
Epoch 36/80: current_loss=8.57550 | best_loss=8.17739
Epoch 37/80: current_loss=9.31562 | best_loss=8.17739
Epoch 38/80: current_loss=9.79074 | best_loss=8.17739
Epoch 39/80: current_loss=9.07314 | best_loss=8.17739
Epoch 40/80: current_loss=12.92962 | best_loss=8.17739
Epoch 41/80: current_loss=9.69017 | best_loss=8.17739
Early Stopping at epoch 41
      explained_var=0.00633 | mse_loss=8.28853
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.31354 | best_loss=11.31354
Epoch 1/80: current_loss=22.81203 | best_loss=11.31354
Epoch 2/80: current_loss=8.33320 | best_loss=8.33320
Epoch 3/80: current_loss=11.40644 | best_loss=8.33320
Epoch 4/80: current_loss=7.95114 | best_loss=7.95114
Epoch 5/80: current_loss=7.93960 | best_loss=7.93960
Epoch 6/80: current_loss=8.76834 | best_loss=7.93960
Epoch 7/80: current_loss=10.85341 | best_loss=7.93960
Epoch 8/80: current_loss=13.38445 | best_loss=7.93960
Epoch 9/80: current_loss=11.95370 | best_loss=7.93960
Epoch 10/80: current_loss=8.28449 | best_loss=7.93960
Epoch 11/80: current_loss=8.92488 | best_loss=7.93960
Epoch 12/80: current_loss=7.73032 | best_loss=7.73032
Epoch 13/80: current_loss=9.27636 | best_loss=7.73032
Epoch 14/80: current_loss=12.49668 | best_loss=7.73032
Epoch 15/80: current_loss=8.12073 | best_loss=7.73032
Epoch 16/80: current_loss=9.38776 | best_loss=7.73032
Epoch 17/80: current_loss=11.33931 | best_loss=7.73032
Epoch 18/80: current_loss=9.11962 | best_loss=7.73032
Epoch 19/80: current_loss=8.15234 | best_loss=7.73032
Epoch 20/80: current_loss=14.51246 | best_loss=7.73032
Epoch 21/80: current_loss=8.79286 | best_loss=7.73032
Epoch 22/80: current_loss=9.44966 | best_loss=7.73032
Epoch 23/80: current_loss=9.65333 | best_loss=7.73032
Epoch 24/80: current_loss=8.36221 | best_loss=7.73032
Epoch 25/80: current_loss=13.47701 | best_loss=7.73032
Epoch 26/80: current_loss=10.26517 | best_loss=7.73032
Epoch 27/80: current_loss=9.09797 | best_loss=7.73032
Epoch 28/80: current_loss=7.76693 | best_loss=7.73032
Epoch 29/80: current_loss=9.41706 | best_loss=7.73032
Epoch 30/80: current_loss=11.39830 | best_loss=7.73032
Epoch 31/80: current_loss=7.94559 | best_loss=7.73032
Epoch 32/80: current_loss=11.39863 | best_loss=7.73032
Early Stopping at epoch 32
      explained_var=-0.00723 | mse_loss=7.91291
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.00899| avg_loss=8.08563
----------------------------------------------


----------------------------------------------
Params for Trial 43
{'learning_rate': 0.1, 'weight_decay': 0.005933897454538834, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.55024 | best_loss=10.55024
Epoch 1/80: current_loss=9.00231 | best_loss=9.00231
Epoch 2/80: current_loss=9.82976 | best_loss=9.00231
Epoch 3/80: current_loss=8.50027 | best_loss=8.50027
Epoch 4/80: current_loss=11.61818 | best_loss=8.50027
Epoch 5/80: current_loss=9.15123 | best_loss=8.50027
Epoch 6/80: current_loss=11.20257 | best_loss=8.50027
Epoch 7/80: current_loss=13.80923 | best_loss=8.50027
Epoch 8/80: current_loss=14.82705 | best_loss=8.50027
Epoch 9/80: current_loss=8.20183 | best_loss=8.20183
Epoch 10/80: current_loss=10.89495 | best_loss=8.20183
Epoch 11/80: current_loss=9.32105 | best_loss=8.20183
Epoch 12/80: current_loss=10.45759 | best_loss=8.20183
Epoch 13/80: current_loss=8.00571 | best_loss=8.00571
Epoch 14/80: current_loss=9.09209 | best_loss=8.00571
Epoch 15/80: current_loss=13.64573 | best_loss=8.00571
Epoch 16/80: current_loss=8.94569 | best_loss=8.00571
Epoch 17/80: current_loss=9.82110 | best_loss=8.00571
Epoch 18/80: current_loss=12.32667 | best_loss=8.00571
Epoch 19/80: current_loss=9.80224 | best_loss=8.00571
Epoch 20/80: current_loss=9.08381 | best_loss=8.00571
Epoch 21/80: current_loss=7.97866 | best_loss=7.97866
Epoch 22/80: current_loss=8.00322 | best_loss=7.97866
Epoch 23/80: current_loss=10.85761 | best_loss=7.97866
Epoch 24/80: current_loss=12.26498 | best_loss=7.97866
Epoch 25/80: current_loss=8.17452 | best_loss=7.97866
Epoch 26/80: current_loss=19.14140 | best_loss=7.97866
Epoch 27/80: current_loss=9.66300 | best_loss=7.97866
Epoch 28/80: current_loss=7.79841 | best_loss=7.79841
Epoch 29/80: current_loss=14.28495 | best_loss=7.79841
Epoch 30/80: current_loss=7.77142 | best_loss=7.77142
Epoch 31/80: current_loss=9.79576 | best_loss=7.77142
Epoch 32/80: current_loss=8.66071 | best_loss=7.77142
Epoch 33/80: current_loss=8.31131 | best_loss=7.77142
Epoch 34/80: current_loss=8.20229 | best_loss=7.77142
Epoch 35/80: current_loss=10.71367 | best_loss=7.77142
Epoch 36/80: current_loss=11.59896 | best_loss=7.77142
Epoch 37/80: current_loss=8.59502 | best_loss=7.77142
Epoch 38/80: current_loss=7.77973 | best_loss=7.77142
Epoch 39/80: current_loss=12.96485 | best_loss=7.77142
Epoch 40/80: current_loss=7.82636 | best_loss=7.77142
Epoch 41/80: current_loss=11.71361 | best_loss=7.77142
Epoch 42/80: current_loss=8.41192 | best_loss=7.77142
Epoch 43/80: current_loss=10.25910 | best_loss=7.77142
Epoch 44/80: current_loss=7.83316 | best_loss=7.77142
Epoch 45/80: current_loss=8.40499 | best_loss=7.77142
Epoch 46/80: current_loss=7.83305 | best_loss=7.77142
Epoch 47/80: current_loss=7.96691 | best_loss=7.77142
Epoch 48/80: current_loss=26.49598 | best_loss=7.77142
Epoch 49/80: current_loss=10.75283 | best_loss=7.77142
Epoch 50/80: current_loss=12.26776 | best_loss=7.77142
Early Stopping at epoch 50
      explained_var=0.00209 | mse_loss=7.60297
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.62230 | best_loss=9.62230
Epoch 1/80: current_loss=8.93522 | best_loss=8.93522
Epoch 2/80: current_loss=23.40710 | best_loss=8.93522
Epoch 3/80: current_loss=8.59609 | best_loss=8.59609
Epoch 4/80: current_loss=8.78768 | best_loss=8.59609
Epoch 5/80: current_loss=9.44193 | best_loss=8.59609
Epoch 6/80: current_loss=10.35046 | best_loss=8.59609
Epoch 7/80: current_loss=10.55252 | best_loss=8.59609
Epoch 8/80: current_loss=11.01930 | best_loss=8.59609
Epoch 9/80: current_loss=8.69928 | best_loss=8.59609
Epoch 10/80: current_loss=11.61664 | best_loss=8.59609
Epoch 11/80: current_loss=10.64763 | best_loss=8.59609
Epoch 12/80: current_loss=9.57958 | best_loss=8.59609
Epoch 13/80: current_loss=8.28193 | best_loss=8.28193
Epoch 14/80: current_loss=18.68909 | best_loss=8.28193
Epoch 15/80: current_loss=9.83872 | best_loss=8.28193
Epoch 16/80: current_loss=8.54184 | best_loss=8.28193
Epoch 17/80: current_loss=8.51233 | best_loss=8.28193
Epoch 18/80: current_loss=11.33304 | best_loss=8.28193
Epoch 19/80: current_loss=11.69947 | best_loss=8.28193
Epoch 20/80: current_loss=9.23938 | best_loss=8.28193
Epoch 21/80: current_loss=8.53042 | best_loss=8.28193
Epoch 22/80: current_loss=13.10425 | best_loss=8.28193
Epoch 23/80: current_loss=8.53467 | best_loss=8.28193
Epoch 24/80: current_loss=10.26918 | best_loss=8.28193
Epoch 25/80: current_loss=8.34639 | best_loss=8.28193
Epoch 26/80: current_loss=7.93330 | best_loss=7.93330
Epoch 27/80: current_loss=8.91611 | best_loss=7.93330
Epoch 28/80: current_loss=9.52154 | best_loss=7.93330
Epoch 29/80: current_loss=8.37152 | best_loss=7.93330
Epoch 30/80: current_loss=8.82371 | best_loss=7.93330
Epoch 31/80: current_loss=9.70430 | best_loss=7.93330
Epoch 32/80: current_loss=9.89015 | best_loss=7.93330
Epoch 33/80: current_loss=10.52785 | best_loss=7.93330
Epoch 34/80: current_loss=8.85991 | best_loss=7.93330
Epoch 35/80: current_loss=9.78088 | best_loss=7.93330
Epoch 36/80: current_loss=10.03006 | best_loss=7.93330
Epoch 37/80: current_loss=17.93309 | best_loss=7.93330
Epoch 38/80: current_loss=23.04279 | best_loss=7.93330
Epoch 39/80: current_loss=12.28662 | best_loss=7.93330
Epoch 40/80: current_loss=12.52962 | best_loss=7.93330
Epoch 41/80: current_loss=9.50080 | best_loss=7.93330
Epoch 42/80: current_loss=9.21315 | best_loss=7.93330
Epoch 43/80: current_loss=9.58510 | best_loss=7.93330
Epoch 44/80: current_loss=8.65491 | best_loss=7.93330
Epoch 45/80: current_loss=10.45793 | best_loss=7.93330
Epoch 46/80: current_loss=8.62877 | best_loss=7.93330
Early Stopping at epoch 46
      explained_var=0.04927 | mse_loss=7.76847
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.23069 | best_loss=11.23069
Epoch 1/80: current_loss=18.42043 | best_loss=11.23069
Epoch 2/80: current_loss=7.88886 | best_loss=7.88886
Epoch 3/80: current_loss=8.65498 | best_loss=7.88886
Epoch 4/80: current_loss=10.71480 | best_loss=7.88886
Epoch 5/80: current_loss=17.15287 | best_loss=7.88886
Epoch 6/80: current_loss=10.22497 | best_loss=7.88886
Epoch 7/80: current_loss=8.68242 | best_loss=7.88886
Epoch 8/80: current_loss=12.65992 | best_loss=7.88886
Epoch 9/80: current_loss=8.56208 | best_loss=7.88886
Epoch 10/80: current_loss=11.19402 | best_loss=7.88886
Epoch 11/80: current_loss=10.09426 | best_loss=7.88886
Epoch 12/80: current_loss=10.00757 | best_loss=7.88886
Epoch 13/80: current_loss=10.16239 | best_loss=7.88886
Epoch 14/80: current_loss=8.79743 | best_loss=7.88886
Epoch 15/80: current_loss=17.24056 | best_loss=7.88886
Epoch 16/80: current_loss=18.43926 | best_loss=7.88886
Epoch 17/80: current_loss=9.70465 | best_loss=7.88886
Epoch 18/80: current_loss=8.31206 | best_loss=7.88886
Epoch 19/80: current_loss=10.04786 | best_loss=7.88886
Epoch 20/80: current_loss=9.45200 | best_loss=7.88886
Epoch 21/80: current_loss=13.22787 | best_loss=7.88886
Epoch 22/80: current_loss=10.23957 | best_loss=7.88886
Early Stopping at epoch 22
      explained_var=0.11405 | mse_loss=7.71751
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.45476 | best_loss=10.45476
Epoch 1/80: current_loss=10.50731 | best_loss=10.45476
Epoch 2/80: current_loss=12.97333 | best_loss=10.45476
Epoch 3/80: current_loss=9.54452 | best_loss=9.54452
Epoch 4/80: current_loss=8.69722 | best_loss=8.69722
Epoch 5/80: current_loss=11.49601 | best_loss=8.69722
Epoch 6/80: current_loss=8.11123 | best_loss=8.11123
Epoch 7/80: current_loss=9.84998 | best_loss=8.11123
Epoch 8/80: current_loss=9.17479 | best_loss=8.11123
Epoch 9/80: current_loss=13.35999 | best_loss=8.11123
Epoch 10/80: current_loss=10.19869 | best_loss=8.11123
Epoch 11/80: current_loss=9.58430 | best_loss=8.11123
Epoch 12/80: current_loss=8.58886 | best_loss=8.11123
Epoch 13/80: current_loss=19.46491 | best_loss=8.11123
Epoch 14/80: current_loss=10.25789 | best_loss=8.11123
Epoch 15/80: current_loss=10.90022 | best_loss=8.11123
Epoch 16/80: current_loss=9.00824 | best_loss=8.11123
Epoch 17/80: current_loss=9.19843 | best_loss=8.11123
Epoch 18/80: current_loss=15.77164 | best_loss=8.11123
Epoch 19/80: current_loss=12.03592 | best_loss=8.11123
Epoch 20/80: current_loss=10.70798 | best_loss=8.11123
Epoch 21/80: current_loss=10.04115 | best_loss=8.11123
Epoch 22/80: current_loss=10.35532 | best_loss=8.11123
Epoch 23/80: current_loss=9.00550 | best_loss=8.11123
Epoch 24/80: current_loss=10.69293 | best_loss=8.11123
Epoch 25/80: current_loss=7.94219 | best_loss=7.94219
Epoch 26/80: current_loss=8.53611 | best_loss=7.94219
Epoch 27/80: current_loss=9.35445 | best_loss=7.94219
Epoch 28/80: current_loss=19.44149 | best_loss=7.94219
Epoch 29/80: current_loss=12.96837 | best_loss=7.94219
Epoch 30/80: current_loss=10.26336 | best_loss=7.94219
Epoch 31/80: current_loss=8.34428 | best_loss=7.94219
Epoch 32/80: current_loss=10.33336 | best_loss=7.94219
Epoch 33/80: current_loss=8.53043 | best_loss=7.94219
Epoch 34/80: current_loss=8.43361 | best_loss=7.94219
Epoch 35/80: current_loss=8.78417 | best_loss=7.94219
Epoch 36/80: current_loss=9.46310 | best_loss=7.94219
Epoch 37/80: current_loss=15.75020 | best_loss=7.94219
Epoch 38/80: current_loss=8.22119 | best_loss=7.94219
Epoch 39/80: current_loss=9.32497 | best_loss=7.94219
Epoch 40/80: current_loss=9.58374 | best_loss=7.94219
Epoch 41/80: current_loss=9.45452 | best_loss=7.94219
Epoch 42/80: current_loss=11.02647 | best_loss=7.94219
Epoch 43/80: current_loss=8.47600 | best_loss=7.94219
Epoch 44/80: current_loss=11.54608 | best_loss=7.94219
Epoch 45/80: current_loss=10.29480 | best_loss=7.94219
Early Stopping at epoch 45
      explained_var=0.03040 | mse_loss=8.10241
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.56611 | best_loss=9.56611
Epoch 1/80: current_loss=8.26086 | best_loss=8.26086
Epoch 2/80: current_loss=7.95801 | best_loss=7.95801
Epoch 3/80: current_loss=10.85250 | best_loss=7.95801
Epoch 4/80: current_loss=9.38215 | best_loss=7.95801
Epoch 5/80: current_loss=8.78005 | best_loss=7.95801
Epoch 6/80: current_loss=8.47709 | best_loss=7.95801
Epoch 7/80: current_loss=7.67048 | best_loss=7.67048
Epoch 8/80: current_loss=13.19988 | best_loss=7.67048
Epoch 9/80: current_loss=13.33145 | best_loss=7.67048
Epoch 10/80: current_loss=8.06827 | best_loss=7.67048
Epoch 11/80: current_loss=8.00128 | best_loss=7.67048
Epoch 12/80: current_loss=7.81733 | best_loss=7.67048
Epoch 13/80: current_loss=7.67712 | best_loss=7.67048
Epoch 14/80: current_loss=12.11078 | best_loss=7.67048
Epoch 15/80: current_loss=9.20682 | best_loss=7.67048
Epoch 16/80: current_loss=13.09864 | best_loss=7.67048
Epoch 17/80: current_loss=13.71510 | best_loss=7.67048
Epoch 18/80: current_loss=8.34321 | best_loss=7.67048
Epoch 19/80: current_loss=8.91648 | best_loss=7.67048
Epoch 20/80: current_loss=8.15321 | best_loss=7.67048
Epoch 21/80: current_loss=9.11493 | best_loss=7.67048
Epoch 22/80: current_loss=8.41228 | best_loss=7.67048
Epoch 23/80: current_loss=7.84724 | best_loss=7.67048
Epoch 24/80: current_loss=10.96681 | best_loss=7.67048
Epoch 25/80: current_loss=13.17776 | best_loss=7.67048
Epoch 26/80: current_loss=7.69676 | best_loss=7.67048
Epoch 27/80: current_loss=7.74707 | best_loss=7.67048
Early Stopping at epoch 27
      explained_var=-0.00051 | mse_loss=7.86043
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.03906| avg_loss=7.81036
----------------------------------------------


----------------------------------------------
Params for Trial 44
{'learning_rate': 0.001, 'weight_decay': 0.005922044700713671, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=20.02898 | best_loss=20.02898
Epoch 1/80: current_loss=10.64674 | best_loss=10.64674
Epoch 2/80: current_loss=7.95788 | best_loss=7.95788
Epoch 3/80: current_loss=7.92780 | best_loss=7.92780
Epoch 4/80: current_loss=8.27066 | best_loss=7.92780
Epoch 5/80: current_loss=8.23842 | best_loss=7.92780
Epoch 6/80: current_loss=8.15300 | best_loss=7.92780
Epoch 7/80: current_loss=8.10801 | best_loss=7.92780
Epoch 8/80: current_loss=8.13889 | best_loss=7.92780
Epoch 9/80: current_loss=7.98030 | best_loss=7.92780
Epoch 10/80: current_loss=7.97167 | best_loss=7.92780
Epoch 11/80: current_loss=8.11225 | best_loss=7.92780
Epoch 12/80: current_loss=8.16723 | best_loss=7.92780
Epoch 13/80: current_loss=8.10884 | best_loss=7.92780
Epoch 14/80: current_loss=8.07197 | best_loss=7.92780
Epoch 15/80: current_loss=8.01564 | best_loss=7.92780
Epoch 16/80: current_loss=8.03825 | best_loss=7.92780
Epoch 17/80: current_loss=7.98820 | best_loss=7.92780
Epoch 18/80: current_loss=7.93317 | best_loss=7.92780
Epoch 19/80: current_loss=8.05188 | best_loss=7.92780
Epoch 20/80: current_loss=8.14216 | best_loss=7.92780
Epoch 21/80: current_loss=8.04153 | best_loss=7.92780
Epoch 22/80: current_loss=7.98487 | best_loss=7.92780
Epoch 23/80: current_loss=8.11892 | best_loss=7.92780
Early Stopping at epoch 23
      explained_var=0.00011 | mse_loss=7.79309

----------------------------------------------
Params for Trial 45
{'learning_rate': 0.1, 'weight_decay': 0.005157219947816699, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=50.65339 | best_loss=50.65339
Epoch 1/80: current_loss=19.41709 | best_loss=19.41709
Epoch 2/80: current_loss=12.60324 | best_loss=12.60324
Epoch 3/80: current_loss=9.83176 | best_loss=9.83176
Epoch 4/80: current_loss=12.20828 | best_loss=9.83176
Epoch 5/80: current_loss=14.90346 | best_loss=9.83176
Epoch 6/80: current_loss=16.34515 | best_loss=9.83176
Epoch 7/80: current_loss=20.84587 | best_loss=9.83176
Epoch 8/80: current_loss=16.80788 | best_loss=9.83176
Epoch 9/80: current_loss=66.63186 | best_loss=9.83176
Epoch 10/80: current_loss=41.22866 | best_loss=9.83176
Epoch 11/80: current_loss=17.93346 | best_loss=9.83176
Epoch 12/80: current_loss=19.79029 | best_loss=9.83176
Epoch 13/80: current_loss=11.40624 | best_loss=9.83176
Epoch 14/80: current_loss=18.20378 | best_loss=9.83176
Epoch 15/80: current_loss=16.05946 | best_loss=9.83176
Epoch 16/80: current_loss=12.21372 | best_loss=9.83176
Epoch 17/80: current_loss=9.93918 | best_loss=9.83176
Epoch 18/80: current_loss=9.39155 | best_loss=9.39155
Epoch 19/80: current_loss=14.65679 | best_loss=9.39155
Epoch 20/80: current_loss=20.49027 | best_loss=9.39155
Epoch 21/80: current_loss=11.94706 | best_loss=9.39155
Epoch 22/80: current_loss=11.48245 | best_loss=9.39155
Epoch 23/80: current_loss=11.08259 | best_loss=9.39155
Epoch 24/80: current_loss=12.36280 | best_loss=9.39155
Epoch 25/80: current_loss=11.19395 | best_loss=9.39155
Epoch 26/80: current_loss=8.17607 | best_loss=8.17607
Epoch 27/80: current_loss=10.08777 | best_loss=8.17607
Epoch 28/80: current_loss=14.07350 | best_loss=8.17607
Epoch 29/80: current_loss=16.29461 | best_loss=8.17607
Epoch 30/80: current_loss=16.02558 | best_loss=8.17607
Epoch 31/80: current_loss=10.87328 | best_loss=8.17607
Epoch 32/80: current_loss=11.92424 | best_loss=8.17607
Epoch 33/80: current_loss=12.63701 | best_loss=8.17607
Epoch 34/80: current_loss=9.60058 | best_loss=8.17607
Epoch 35/80: current_loss=12.19560 | best_loss=8.17607
Epoch 36/80: current_loss=9.83242 | best_loss=8.17607
Epoch 37/80: current_loss=14.61811 | best_loss=8.17607
Epoch 38/80: current_loss=11.43840 | best_loss=8.17607
Epoch 39/80: current_loss=13.70870 | best_loss=8.17607
Epoch 40/80: current_loss=16.80862 | best_loss=8.17607
Epoch 41/80: current_loss=12.91035 | best_loss=8.17607
Epoch 42/80: current_loss=9.27307 | best_loss=8.17607
Epoch 43/80: current_loss=8.66636 | best_loss=8.17607
Epoch 44/80: current_loss=10.19609 | best_loss=8.17607
Epoch 45/80: current_loss=17.97081 | best_loss=8.17607
Epoch 46/80: current_loss=13.18600 | best_loss=8.17607
Early Stopping at epoch 46
      explained_var=-0.00694 | mse_loss=8.10077

----------------------------------------------
Params for Trial 46
{'learning_rate': 0.1, 'weight_decay': 0.006412961772115979, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.94251 | best_loss=10.94251
Epoch 1/80: current_loss=13.45424 | best_loss=10.94251
Epoch 2/80: current_loss=7.75886 | best_loss=7.75886
Epoch 3/80: current_loss=8.52944 | best_loss=7.75886
Epoch 4/80: current_loss=8.75748 | best_loss=7.75886
Epoch 5/80: current_loss=17.50167 | best_loss=7.75886
Epoch 6/80: current_loss=52.05712 | best_loss=7.75886
Epoch 7/80: current_loss=10.37229 | best_loss=7.75886
Epoch 8/80: current_loss=11.63084 | best_loss=7.75886
Epoch 9/80: current_loss=17.74826 | best_loss=7.75886
Epoch 10/80: current_loss=14.02201 | best_loss=7.75886
Epoch 11/80: current_loss=11.87032 | best_loss=7.75886
Epoch 12/80: current_loss=12.98896 | best_loss=7.75886
Epoch 13/80: current_loss=10.03537 | best_loss=7.75886
Epoch 14/80: current_loss=9.92460 | best_loss=7.75886
Epoch 15/80: current_loss=17.09884 | best_loss=7.75886
Epoch 16/80: current_loss=9.93550 | best_loss=7.75886
Epoch 17/80: current_loss=8.98279 | best_loss=7.75886
Epoch 18/80: current_loss=9.32375 | best_loss=7.75886
Epoch 19/80: current_loss=12.47791 | best_loss=7.75886
Epoch 20/80: current_loss=8.55588 | best_loss=7.75886
Epoch 21/80: current_loss=7.98028 | best_loss=7.75886
Epoch 22/80: current_loss=16.18490 | best_loss=7.75886
Early Stopping at epoch 22
      explained_var=0.00476 | mse_loss=7.56273
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.15560 | best_loss=12.15560
Epoch 1/80: current_loss=14.16176 | best_loss=12.15560
Epoch 2/80: current_loss=8.55450 | best_loss=8.55450
Epoch 3/80: current_loss=8.37570 | best_loss=8.37570
Epoch 4/80: current_loss=9.15305 | best_loss=8.37570
Epoch 5/80: current_loss=8.65201 | best_loss=8.37570
Epoch 6/80: current_loss=8.97053 | best_loss=8.37570
Epoch 7/80: current_loss=11.57802 | best_loss=8.37570
Epoch 8/80: current_loss=8.52803 | best_loss=8.37570
Epoch 9/80: current_loss=8.54609 | best_loss=8.37570
Epoch 10/80: current_loss=15.29512 | best_loss=8.37570
Epoch 11/80: current_loss=10.18326 | best_loss=8.37570
Epoch 12/80: current_loss=11.03569 | best_loss=8.37570
Epoch 13/80: current_loss=8.60053 | best_loss=8.37570
Epoch 14/80: current_loss=8.96443 | best_loss=8.37570
Epoch 15/80: current_loss=9.30623 | best_loss=8.37570
Epoch 16/80: current_loss=10.66260 | best_loss=8.37570
Epoch 17/80: current_loss=9.41765 | best_loss=8.37570
Epoch 18/80: current_loss=8.37061 | best_loss=8.37061
Epoch 19/80: current_loss=12.55002 | best_loss=8.37061
Epoch 20/80: current_loss=9.19500 | best_loss=8.37061
Epoch 21/80: current_loss=19.25934 | best_loss=8.37061
Epoch 22/80: current_loss=15.57097 | best_loss=8.37061
Epoch 23/80: current_loss=9.56787 | best_loss=8.37061
Epoch 24/80: current_loss=8.51897 | best_loss=8.37061
Epoch 25/80: current_loss=8.59308 | best_loss=8.37061
Epoch 26/80: current_loss=9.70178 | best_loss=8.37061
Epoch 27/80: current_loss=11.31296 | best_loss=8.37061
Epoch 28/80: current_loss=9.00831 | best_loss=8.37061
Epoch 29/80: current_loss=8.72561 | best_loss=8.37061
Epoch 30/80: current_loss=11.22067 | best_loss=8.37061
Epoch 31/80: current_loss=8.53254 | best_loss=8.37061
Epoch 32/80: current_loss=13.25349 | best_loss=8.37061
Epoch 33/80: current_loss=9.75951 | best_loss=8.37061
Epoch 34/80: current_loss=11.17547 | best_loss=8.37061
Epoch 35/80: current_loss=8.47562 | best_loss=8.37061
Epoch 36/80: current_loss=8.31455 | best_loss=8.31455
Epoch 37/80: current_loss=10.50597 | best_loss=8.31455
Epoch 38/80: current_loss=8.31157 | best_loss=8.31157
Epoch 39/80: current_loss=8.67884 | best_loss=8.31157
Epoch 40/80: current_loss=10.58671 | best_loss=8.31157
Epoch 41/80: current_loss=8.43163 | best_loss=8.31157
Epoch 42/80: current_loss=8.31025 | best_loss=8.31025
Epoch 43/80: current_loss=8.38215 | best_loss=8.31025
Epoch 44/80: current_loss=8.64578 | best_loss=8.31025
Epoch 45/80: current_loss=8.81466 | best_loss=8.31025
Epoch 46/80: current_loss=14.07185 | best_loss=8.31025
Epoch 47/80: current_loss=9.04443 | best_loss=8.31025
Epoch 48/80: current_loss=13.06929 | best_loss=8.31025
Epoch 49/80: current_loss=9.19868 | best_loss=8.31025
Epoch 50/80: current_loss=12.79690 | best_loss=8.31025
Epoch 51/80: current_loss=8.51560 | best_loss=8.31025
Epoch 52/80: current_loss=12.81363 | best_loss=8.31025
Epoch 53/80: current_loss=8.38978 | best_loss=8.31025
Epoch 54/80: current_loss=10.79253 | best_loss=8.31025
Epoch 55/80: current_loss=8.86171 | best_loss=8.31025
Epoch 56/80: current_loss=11.05859 | best_loss=8.31025
Epoch 57/80: current_loss=14.14108 | best_loss=8.31025
Epoch 58/80: current_loss=9.05964 | best_loss=8.31025
Epoch 59/80: current_loss=8.84343 | best_loss=8.31025
Epoch 60/80: current_loss=13.34667 | best_loss=8.31025
Epoch 61/80: current_loss=13.67530 | best_loss=8.31025
Epoch 62/80: current_loss=9.09205 | best_loss=8.31025
Early Stopping at epoch 62
      explained_var=0.00111 | mse_loss=8.16059
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.44008 | best_loss=10.44008
Epoch 1/80: current_loss=10.63023 | best_loss=10.44008
Epoch 2/80: current_loss=15.77651 | best_loss=10.44008
Epoch 3/80: current_loss=8.77719 | best_loss=8.77719
Epoch 4/80: current_loss=10.03210 | best_loss=8.77719
Epoch 5/80: current_loss=12.14647 | best_loss=8.77719
Epoch 6/80: current_loss=12.85140 | best_loss=8.77719
Epoch 7/80: current_loss=8.97284 | best_loss=8.77719
Epoch 8/80: current_loss=10.16695 | best_loss=8.77719
Epoch 9/80: current_loss=9.12628 | best_loss=8.77719
Epoch 10/80: current_loss=11.08771 | best_loss=8.77719
Epoch 11/80: current_loss=16.87856 | best_loss=8.77719
Epoch 12/80: current_loss=10.45269 | best_loss=8.77719
Epoch 13/80: current_loss=10.29731 | best_loss=8.77719
Epoch 14/80: current_loss=16.56467 | best_loss=8.77719
Epoch 15/80: current_loss=8.61602 | best_loss=8.61602
Epoch 16/80: current_loss=13.27212 | best_loss=8.61602
Epoch 17/80: current_loss=8.79400 | best_loss=8.61602
Epoch 18/80: current_loss=14.68411 | best_loss=8.61602
Epoch 19/80: current_loss=8.56219 | best_loss=8.56219
Epoch 20/80: current_loss=9.44394 | best_loss=8.56219
Epoch 21/80: current_loss=8.56276 | best_loss=8.56219
Epoch 22/80: current_loss=9.04703 | best_loss=8.56219
Epoch 23/80: current_loss=9.08034 | best_loss=8.56219
Epoch 24/80: current_loss=8.70935 | best_loss=8.56219
Epoch 25/80: current_loss=8.56760 | best_loss=8.56219
Epoch 26/80: current_loss=9.98841 | best_loss=8.56219
Epoch 27/80: current_loss=12.28806 | best_loss=8.56219
Epoch 28/80: current_loss=11.39413 | best_loss=8.56219
Epoch 29/80: current_loss=8.74125 | best_loss=8.56219
Epoch 30/80: current_loss=11.85247 | best_loss=8.56219
Epoch 31/80: current_loss=9.77780 | best_loss=8.56219
Epoch 32/80: current_loss=9.20244 | best_loss=8.56219
Epoch 33/80: current_loss=8.79418 | best_loss=8.56219
Epoch 34/80: current_loss=10.91382 | best_loss=8.56219
Epoch 35/80: current_loss=8.77117 | best_loss=8.56219
Epoch 36/80: current_loss=11.82493 | best_loss=8.56219
Epoch 37/80: current_loss=10.89922 | best_loss=8.56219
Epoch 38/80: current_loss=8.71483 | best_loss=8.56219
Epoch 39/80: current_loss=12.13157 | best_loss=8.56219
Early Stopping at epoch 39
      explained_var=0.03943 | mse_loss=8.34862
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.27722 | best_loss=9.27722
Epoch 1/80: current_loss=11.04459 | best_loss=9.27722
Epoch 2/80: current_loss=9.00057 | best_loss=9.00057
Epoch 3/80: current_loss=12.03789 | best_loss=9.00057
Epoch 4/80: current_loss=14.14954 | best_loss=9.00057
Epoch 5/80: current_loss=12.63938 | best_loss=9.00057
Epoch 6/80: current_loss=11.88715 | best_loss=9.00057
Epoch 7/80: current_loss=7.95013 | best_loss=7.95013
Epoch 8/80: current_loss=10.45949 | best_loss=7.95013
Epoch 9/80: current_loss=8.92945 | best_loss=7.95013
Epoch 10/80: current_loss=11.56997 | best_loss=7.95013
Epoch 11/80: current_loss=16.27602 | best_loss=7.95013
Epoch 12/80: current_loss=8.93199 | best_loss=7.95013
Epoch 13/80: current_loss=8.14491 | best_loss=7.95013
Epoch 14/80: current_loss=9.04045 | best_loss=7.95013
Epoch 15/80: current_loss=9.02899 | best_loss=7.95013
Epoch 16/80: current_loss=9.09850 | best_loss=7.95013
Epoch 17/80: current_loss=10.85857 | best_loss=7.95013
Epoch 18/80: current_loss=8.05397 | best_loss=7.95013
Epoch 19/80: current_loss=10.39605 | best_loss=7.95013
Epoch 20/80: current_loss=8.99600 | best_loss=7.95013
Epoch 21/80: current_loss=10.48703 | best_loss=7.95013
Epoch 22/80: current_loss=9.95330 | best_loss=7.95013
Epoch 23/80: current_loss=10.79423 | best_loss=7.95013
Epoch 24/80: current_loss=8.43193 | best_loss=7.95013
Epoch 25/80: current_loss=10.04952 | best_loss=7.95013
Epoch 26/80: current_loss=9.70555 | best_loss=7.95013
Epoch 27/80: current_loss=12.95114 | best_loss=7.95013
Early Stopping at epoch 27
      explained_var=0.03824 | mse_loss=8.05624
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.10581 | best_loss=10.10581
Epoch 1/80: current_loss=9.03791 | best_loss=9.03791
Epoch 2/80: current_loss=8.43972 | best_loss=8.43972
Epoch 3/80: current_loss=9.18250 | best_loss=8.43972
Epoch 4/80: current_loss=8.12261 | best_loss=8.12261
Epoch 5/80: current_loss=9.70035 | best_loss=8.12261
Epoch 6/80: current_loss=9.99001 | best_loss=8.12261
Epoch 7/80: current_loss=8.41879 | best_loss=8.12261
Epoch 8/80: current_loss=11.06081 | best_loss=8.12261
Epoch 9/80: current_loss=8.36544 | best_loss=8.12261
Epoch 10/80: current_loss=9.60779 | best_loss=8.12261
Epoch 11/80: current_loss=8.57844 | best_loss=8.12261
Epoch 12/80: current_loss=13.15192 | best_loss=8.12261
Epoch 13/80: current_loss=9.63058 | best_loss=8.12261
Epoch 14/80: current_loss=11.77917 | best_loss=8.12261
Epoch 15/80: current_loss=8.55509 | best_loss=8.12261
Epoch 16/80: current_loss=10.04060 | best_loss=8.12261
Epoch 17/80: current_loss=10.19458 | best_loss=8.12261
Epoch 18/80: current_loss=8.30441 | best_loss=8.12261
Epoch 19/80: current_loss=9.24223 | best_loss=8.12261
Epoch 20/80: current_loss=9.12659 | best_loss=8.12261
Epoch 21/80: current_loss=9.03005 | best_loss=8.12261
Epoch 22/80: current_loss=10.71073 | best_loss=8.12261
Epoch 23/80: current_loss=9.59487 | best_loss=8.12261
Epoch 24/80: current_loss=9.02661 | best_loss=8.12261
Early Stopping at epoch 24
      explained_var=-0.05555 | mse_loss=8.31024
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.00560| avg_loss=8.08768
----------------------------------------------


----------------------------------------------
Params for Trial 47
{'learning_rate': 0.1, 'weight_decay': 0.0007137446078104952, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.78523 | best_loss=7.78523
Epoch 1/80: current_loss=9.11143 | best_loss=7.78523
Epoch 2/80: current_loss=8.48758 | best_loss=7.78523
Epoch 3/80: current_loss=9.51110 | best_loss=7.78523
Epoch 4/80: current_loss=7.82048 | best_loss=7.78523
Epoch 5/80: current_loss=8.19865 | best_loss=7.78523
Epoch 6/80: current_loss=8.53738 | best_loss=7.78523
Epoch 7/80: current_loss=8.04280 | best_loss=7.78523
Epoch 8/80: current_loss=7.81485 | best_loss=7.78523
Epoch 9/80: current_loss=8.08131 | best_loss=7.78523
Epoch 10/80: current_loss=7.83395 | best_loss=7.78523
Epoch 11/80: current_loss=7.83824 | best_loss=7.78523
Epoch 12/80: current_loss=7.78417 | best_loss=7.78417
Epoch 13/80: current_loss=8.12893 | best_loss=7.78417
Epoch 14/80: current_loss=8.34466 | best_loss=7.78417
Epoch 15/80: current_loss=7.97158 | best_loss=7.78417
Epoch 16/80: current_loss=8.59260 | best_loss=7.78417
Epoch 17/80: current_loss=11.06481 | best_loss=7.78417
Epoch 18/80: current_loss=11.23912 | best_loss=7.78417
Epoch 19/80: current_loss=12.63017 | best_loss=7.78417
Epoch 20/80: current_loss=14.11167 | best_loss=7.78417
Epoch 21/80: current_loss=12.69787 | best_loss=7.78417
Epoch 22/80: current_loss=8.41109 | best_loss=7.78417
Epoch 23/80: current_loss=7.78593 | best_loss=7.78417
Epoch 24/80: current_loss=7.92127 | best_loss=7.78417
Epoch 25/80: current_loss=7.79143 | best_loss=7.78417
Epoch 26/80: current_loss=8.09894 | best_loss=7.78417
Epoch 27/80: current_loss=7.77467 | best_loss=7.77467
Epoch 28/80: current_loss=7.94428 | best_loss=7.77467
Epoch 29/80: current_loss=7.77484 | best_loss=7.77467
Epoch 30/80: current_loss=8.04939 | best_loss=7.77467
Epoch 31/80: current_loss=7.83442 | best_loss=7.77467
Epoch 32/80: current_loss=8.30430 | best_loss=7.77467
Epoch 33/80: current_loss=7.98556 | best_loss=7.77467
Epoch 34/80: current_loss=7.85601 | best_loss=7.77467
Epoch 35/80: current_loss=7.87936 | best_loss=7.77467
Epoch 36/80: current_loss=8.22225 | best_loss=7.77467
Epoch 37/80: current_loss=8.33306 | best_loss=7.77467
Epoch 38/80: current_loss=7.81465 | best_loss=7.77467
Epoch 39/80: current_loss=7.77222 | best_loss=7.77222
Epoch 40/80: current_loss=8.50641 | best_loss=7.77222
Epoch 41/80: current_loss=9.19495 | best_loss=7.77222
Epoch 42/80: current_loss=7.84167 | best_loss=7.77222
Epoch 43/80: current_loss=8.09864 | best_loss=7.77222
Epoch 44/80: current_loss=8.41388 | best_loss=7.77222
Epoch 45/80: current_loss=8.58731 | best_loss=7.77222
Epoch 46/80: current_loss=8.07637 | best_loss=7.77222
Epoch 47/80: current_loss=7.88276 | best_loss=7.77222
Epoch 48/80: current_loss=7.82691 | best_loss=7.77222
Epoch 49/80: current_loss=8.24211 | best_loss=7.77222
Epoch 50/80: current_loss=7.78171 | best_loss=7.77222
Epoch 51/80: current_loss=7.77407 | best_loss=7.77222
Epoch 52/80: current_loss=9.86967 | best_loss=7.77222
Epoch 53/80: current_loss=7.89473 | best_loss=7.77222
Epoch 54/80: current_loss=7.96172 | best_loss=7.77222
Epoch 55/80: current_loss=8.99938 | best_loss=7.77222
Epoch 56/80: current_loss=11.69926 | best_loss=7.77222
Epoch 57/80: current_loss=9.30841 | best_loss=7.77222
Epoch 58/80: current_loss=8.28862 | best_loss=7.77222
Epoch 59/80: current_loss=8.05448 | best_loss=7.77222
Early Stopping at epoch 59
      explained_var=0.00002 | mse_loss=7.58601
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.37083 | best_loss=8.37083
Epoch 1/80: current_loss=8.32998 | best_loss=8.32998
Epoch 2/80: current_loss=8.34942 | best_loss=8.32998
Epoch 3/80: current_loss=8.61498 | best_loss=8.32998
Epoch 4/80: current_loss=8.56276 | best_loss=8.32998
Epoch 5/80: current_loss=9.47770 | best_loss=8.32998
Epoch 6/80: current_loss=9.23001 | best_loss=8.32998
Epoch 7/80: current_loss=8.29682 | best_loss=8.29682
Epoch 8/80: current_loss=8.54896 | best_loss=8.29682
Epoch 9/80: current_loss=8.45770 | best_loss=8.29682
Epoch 10/80: current_loss=8.41163 | best_loss=8.29682
Epoch 11/80: current_loss=8.59403 | best_loss=8.29682
Epoch 12/80: current_loss=8.70315 | best_loss=8.29682
Epoch 13/80: current_loss=8.76352 | best_loss=8.29682
Epoch 14/80: current_loss=9.68813 | best_loss=8.29682
Epoch 15/80: current_loss=9.03483 | best_loss=8.29682
Epoch 16/80: current_loss=9.57311 | best_loss=8.29682
Epoch 17/80: current_loss=8.46083 | best_loss=8.29682
Epoch 18/80: current_loss=8.72583 | best_loss=8.29682
Epoch 19/80: current_loss=8.38181 | best_loss=8.29682
Epoch 20/80: current_loss=8.39632 | best_loss=8.29682
Epoch 21/80: current_loss=8.43614 | best_loss=8.29682
Epoch 22/80: current_loss=8.44389 | best_loss=8.29682
Epoch 23/80: current_loss=10.30187 | best_loss=8.29682
Epoch 24/80: current_loss=8.32185 | best_loss=8.29682
Epoch 25/80: current_loss=9.72469 | best_loss=8.29682
Epoch 26/80: current_loss=11.95103 | best_loss=8.29682
Epoch 27/80: current_loss=9.91305 | best_loss=8.29682
Early Stopping at epoch 27
      explained_var=0.00264 | mse_loss=8.14324
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.07328 | best_loss=10.07328
Epoch 1/80: current_loss=10.80668 | best_loss=10.07328
Epoch 2/80: current_loss=8.87381 | best_loss=8.87381
Epoch 3/80: current_loss=8.95114 | best_loss=8.87381
Epoch 4/80: current_loss=8.78729 | best_loss=8.78729
Epoch 5/80: current_loss=12.37023 | best_loss=8.78729
Epoch 6/80: current_loss=8.88395 | best_loss=8.78729
Epoch 7/80: current_loss=8.89756 | best_loss=8.78729
Epoch 8/80: current_loss=8.72124 | best_loss=8.72124
Epoch 9/80: current_loss=9.26013 | best_loss=8.72124
Epoch 10/80: current_loss=10.46828 | best_loss=8.72124
Epoch 11/80: current_loss=11.94339 | best_loss=8.72124
Epoch 12/80: current_loss=9.21270 | best_loss=8.72124
Epoch 13/80: current_loss=8.75729 | best_loss=8.72124
Epoch 14/80: current_loss=9.17481 | best_loss=8.72124
Epoch 15/80: current_loss=8.74801 | best_loss=8.72124
Epoch 16/80: current_loss=9.03346 | best_loss=8.72124
Epoch 17/80: current_loss=8.70070 | best_loss=8.70070
Epoch 18/80: current_loss=12.17047 | best_loss=8.70070
Epoch 19/80: current_loss=8.88941 | best_loss=8.70070
Epoch 20/80: current_loss=8.69271 | best_loss=8.69271
Epoch 21/80: current_loss=10.38492 | best_loss=8.69271
Epoch 22/80: current_loss=10.01992 | best_loss=8.69271
Epoch 23/80: current_loss=10.33858 | best_loss=8.69271
Epoch 24/80: current_loss=9.10905 | best_loss=8.69271
Epoch 25/80: current_loss=9.55885 | best_loss=8.69271
Epoch 26/80: current_loss=12.33421 | best_loss=8.69271
Epoch 27/80: current_loss=8.73618 | best_loss=8.69271
Epoch 28/80: current_loss=9.78191 | best_loss=8.69271
Epoch 29/80: current_loss=8.75994 | best_loss=8.69271
Epoch 30/80: current_loss=10.61908 | best_loss=8.69271
Epoch 31/80: current_loss=9.04300 | best_loss=8.69271
Epoch 32/80: current_loss=8.80181 | best_loss=8.69271
Epoch 33/80: current_loss=10.85363 | best_loss=8.69271
Epoch 34/80: current_loss=9.08204 | best_loss=8.69271
Epoch 35/80: current_loss=8.92593 | best_loss=8.69271
Epoch 36/80: current_loss=10.35252 | best_loss=8.69271
Epoch 37/80: current_loss=9.17208 | best_loss=8.69271
Epoch 38/80: current_loss=8.75082 | best_loss=8.69271
Epoch 39/80: current_loss=9.10801 | best_loss=8.69271
Epoch 40/80: current_loss=8.77255 | best_loss=8.69271
Early Stopping at epoch 40
      explained_var=0.00383 | mse_loss=8.45159
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.47473 | best_loss=8.47473
Epoch 1/80: current_loss=12.46719 | best_loss=8.47473
Epoch 2/80: current_loss=10.09011 | best_loss=8.47473
Epoch 3/80: current_loss=9.34294 | best_loss=8.47473
Epoch 4/80: current_loss=8.81487 | best_loss=8.47473
Epoch 5/80: current_loss=33.10252 | best_loss=8.47473
Epoch 6/80: current_loss=9.10526 | best_loss=8.47473
Epoch 7/80: current_loss=8.69251 | best_loss=8.47473
Epoch 8/80: current_loss=8.33717 | best_loss=8.33717
Epoch 9/80: current_loss=9.24662 | best_loss=8.33717
Epoch 10/80: current_loss=8.36174 | best_loss=8.33717
Epoch 11/80: current_loss=8.33430 | best_loss=8.33430
Epoch 12/80: current_loss=8.70907 | best_loss=8.33430
Epoch 13/80: current_loss=9.51854 | best_loss=8.33430
Epoch 14/80: current_loss=9.33948 | best_loss=8.33430
Epoch 15/80: current_loss=8.82519 | best_loss=8.33430
Epoch 16/80: current_loss=8.36582 | best_loss=8.33430
Epoch 17/80: current_loss=9.50053 | best_loss=8.33430
Epoch 18/80: current_loss=8.99991 | best_loss=8.33430
Epoch 19/80: current_loss=8.34664 | best_loss=8.33430
Epoch 20/80: current_loss=8.27873 | best_loss=8.27873
Epoch 21/80: current_loss=9.85095 | best_loss=8.27873
Epoch 22/80: current_loss=8.77104 | best_loss=8.27873
Epoch 23/80: current_loss=8.70816 | best_loss=8.27873
Epoch 24/80: current_loss=8.42794 | best_loss=8.27873
Epoch 25/80: current_loss=8.52035 | best_loss=8.27873
Epoch 26/80: current_loss=8.23415 | best_loss=8.23415
Epoch 27/80: current_loss=8.42866 | best_loss=8.23415
Epoch 28/80: current_loss=8.66842 | best_loss=8.23415
Epoch 29/80: current_loss=9.34592 | best_loss=8.23415
Epoch 30/80: current_loss=8.41099 | best_loss=8.23415
Epoch 31/80: current_loss=9.02211 | best_loss=8.23415
Epoch 32/80: current_loss=8.78066 | best_loss=8.23415
Epoch 33/80: current_loss=8.51530 | best_loss=8.23415
Epoch 34/80: current_loss=8.38324 | best_loss=8.23415
Epoch 35/80: current_loss=8.38471 | best_loss=8.23415
Epoch 36/80: current_loss=8.47335 | best_loss=8.23415
Epoch 37/80: current_loss=8.32114 | best_loss=8.23415
Epoch 38/80: current_loss=9.67783 | best_loss=8.23415
Epoch 39/80: current_loss=8.25241 | best_loss=8.23415
Epoch 40/80: current_loss=9.28313 | best_loss=8.23415
Epoch 41/80: current_loss=9.30167 | best_loss=8.23415
Epoch 42/80: current_loss=8.51590 | best_loss=8.23415
Epoch 43/80: current_loss=8.23994 | best_loss=8.23415
Epoch 44/80: current_loss=8.29748 | best_loss=8.23415
Epoch 45/80: current_loss=11.39920 | best_loss=8.23415
Epoch 46/80: current_loss=9.48999 | best_loss=8.23415
Early Stopping at epoch 46
      explained_var=0.00207 | mse_loss=8.33635
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.87348 | best_loss=7.87348
Epoch 1/80: current_loss=7.83419 | best_loss=7.83419
Epoch 2/80: current_loss=7.67901 | best_loss=7.67901
Epoch 3/80: current_loss=7.97021 | best_loss=7.67901
Epoch 4/80: current_loss=8.10456 | best_loss=7.67901
Epoch 5/80: current_loss=15.78304 | best_loss=7.67901
Epoch 6/80: current_loss=9.66679 | best_loss=7.67901
Epoch 7/80: current_loss=7.65330 | best_loss=7.65330
Epoch 8/80: current_loss=8.42928 | best_loss=7.65330
Epoch 9/80: current_loss=9.30385 | best_loss=7.65330
Epoch 10/80: current_loss=8.02256 | best_loss=7.65330
Epoch 11/80: current_loss=7.81295 | best_loss=7.65330
Epoch 12/80: current_loss=8.20500 | best_loss=7.65330
Epoch 13/80: current_loss=8.55441 | best_loss=7.65330
Epoch 14/80: current_loss=8.18861 | best_loss=7.65330
Epoch 15/80: current_loss=8.04408 | best_loss=7.65330
Epoch 16/80: current_loss=7.76391 | best_loss=7.65330
Epoch 17/80: current_loss=8.85861 | best_loss=7.65330
Epoch 18/80: current_loss=10.46481 | best_loss=7.65330
Epoch 19/80: current_loss=8.21726 | best_loss=7.65330
Epoch 20/80: current_loss=8.46421 | best_loss=7.65330
Epoch 21/80: current_loss=8.05215 | best_loss=7.65330
Epoch 22/80: current_loss=8.44668 | best_loss=7.65330
Epoch 23/80: current_loss=8.54072 | best_loss=7.65330
Epoch 24/80: current_loss=7.63974 | best_loss=7.63974
Epoch 25/80: current_loss=9.13640 | best_loss=7.63974
Epoch 26/80: current_loss=8.45928 | best_loss=7.63974
Epoch 27/80: current_loss=8.34343 | best_loss=7.63974
Epoch 28/80: current_loss=7.72941 | best_loss=7.63974
Epoch 29/80: current_loss=7.68503 | best_loss=7.63974
Epoch 30/80: current_loss=7.71541 | best_loss=7.63974
Epoch 31/80: current_loss=8.35701 | best_loss=7.63974
Epoch 32/80: current_loss=7.95882 | best_loss=7.63974
Epoch 33/80: current_loss=7.80840 | best_loss=7.63974
Epoch 34/80: current_loss=8.33920 | best_loss=7.63974
Epoch 35/80: current_loss=8.16611 | best_loss=7.63974
Epoch 36/80: current_loss=7.98750 | best_loss=7.63974
Epoch 37/80: current_loss=7.80144 | best_loss=7.63974
Epoch 38/80: current_loss=8.10613 | best_loss=7.63974
Epoch 39/80: current_loss=7.66277 | best_loss=7.63974
Epoch 40/80: current_loss=7.81145 | best_loss=7.63974
Epoch 41/80: current_loss=7.89491 | best_loss=7.63974
Epoch 42/80: current_loss=7.89706 | best_loss=7.63974
Epoch 43/80: current_loss=7.69027 | best_loss=7.63974
Epoch 44/80: current_loss=8.22762 | best_loss=7.63974
Early Stopping at epoch 44
      explained_var=0.00489 | mse_loss=7.81771
----------------------------------------------
Average early_stopping_point: 23| avg_exp_var=0.00269| avg_loss=8.06698
----------------------------------------------


----------------------------------------------
Params for Trial 48
{'learning_rate': 0.0001, 'weight_decay': 0.0018816411938204188, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=82.11623 | best_loss=82.11623
Epoch 1/80: current_loss=80.01055 | best_loss=80.01055
Epoch 2/80: current_loss=77.23879 | best_loss=77.23879
Epoch 3/80: current_loss=72.40187 | best_loss=72.40187
Epoch 4/80: current_loss=62.73021 | best_loss=62.73021
Epoch 5/80: current_loss=50.85605 | best_loss=50.85605
Epoch 6/80: current_loss=41.83631 | best_loss=41.83631
Epoch 7/80: current_loss=35.41272 | best_loss=35.41272
Epoch 8/80: current_loss=30.69536 | best_loss=30.69536
Epoch 9/80: current_loss=27.12586 | best_loss=27.12586
Epoch 10/80: current_loss=24.31332 | best_loss=24.31332
Epoch 11/80: current_loss=22.08942 | best_loss=22.08942
Epoch 12/80: current_loss=20.29069 | best_loss=20.29069
Epoch 13/80: current_loss=18.84623 | best_loss=18.84623
Epoch 14/80: current_loss=17.64997 | best_loss=17.64997
Epoch 15/80: current_loss=16.60684 | best_loss=16.60684
Epoch 16/80: current_loss=15.72061 | best_loss=15.72061
Epoch 17/80: current_loss=14.99435 | best_loss=14.99435
Epoch 18/80: current_loss=14.35688 | best_loss=14.35688
Epoch 19/80: current_loss=13.83135 | best_loss=13.83135
Epoch 20/80: current_loss=13.35971 | best_loss=13.35971
Epoch 21/80: current_loss=12.97011 | best_loss=12.97011
Epoch 22/80: current_loss=12.63052 | best_loss=12.63052
Epoch 23/80: current_loss=12.34846 | best_loss=12.34846
Epoch 24/80: current_loss=12.10252 | best_loss=12.10252
Epoch 25/80: current_loss=11.88739 | best_loss=11.88739
Epoch 26/80: current_loss=11.70983 | best_loss=11.70983
Epoch 27/80: current_loss=11.55940 | best_loss=11.55940
Epoch 28/80: current_loss=11.42770 | best_loss=11.42770
Epoch 29/80: current_loss=11.31443 | best_loss=11.31443
Epoch 30/80: current_loss=11.21591 | best_loss=11.21591
Epoch 31/80: current_loss=11.13640 | best_loss=11.13640
Epoch 32/80: current_loss=11.07362 | best_loss=11.07362
Epoch 33/80: current_loss=11.02139 | best_loss=11.02139
Epoch 34/80: current_loss=10.97467 | best_loss=10.97467
Epoch 35/80: current_loss=10.92950 | best_loss=10.92950
Epoch 36/80: current_loss=10.89434 | best_loss=10.89434
Epoch 37/80: current_loss=10.86407 | best_loss=10.86407
Epoch 38/80: current_loss=10.83556 | best_loss=10.83556
Epoch 39/80: current_loss=10.81309 | best_loss=10.81309
Epoch 40/80: current_loss=10.79251 | best_loss=10.79251
Epoch 41/80: current_loss=10.77069 | best_loss=10.77069
Epoch 42/80: current_loss=10.75630 | best_loss=10.75630
Epoch 43/80: current_loss=10.74690 | best_loss=10.74690
Epoch 44/80: current_loss=10.73767 | best_loss=10.73767
Epoch 45/80: current_loss=10.72281 | best_loss=10.72281
Epoch 46/80: current_loss=10.70835 | best_loss=10.70835
Epoch 47/80: current_loss=10.69745 | best_loss=10.69745
Epoch 48/80: current_loss=10.68648 | best_loss=10.68648
Epoch 49/80: current_loss=10.67855 | best_loss=10.67855
Epoch 50/80: current_loss=10.66078 | best_loss=10.66078
Epoch 51/80: current_loss=10.65268 | best_loss=10.65268
Epoch 52/80: current_loss=10.63595 | best_loss=10.63595
Epoch 53/80: current_loss=10.62560 | best_loss=10.62560
Epoch 54/80: current_loss=10.61540 | best_loss=10.61540
Epoch 55/80: current_loss=10.60487 | best_loss=10.60487
Epoch 56/80: current_loss=10.59608 | best_loss=10.59608
Epoch 57/80: current_loss=10.57457 | best_loss=10.57457
Epoch 58/80: current_loss=10.56534 | best_loss=10.56534
Epoch 59/80: current_loss=10.54611 | best_loss=10.54611
Epoch 60/80: current_loss=10.53205 | best_loss=10.53205
Epoch 61/80: current_loss=10.51059 | best_loss=10.51059
Epoch 62/80: current_loss=10.48221 | best_loss=10.48221
Epoch 63/80: current_loss=10.46063 | best_loss=10.46063
Epoch 64/80: current_loss=10.45157 | best_loss=10.45157
Epoch 65/80: current_loss=10.44242 | best_loss=10.44242
Epoch 66/80: current_loss=10.42435 | best_loss=10.42435
Epoch 67/80: current_loss=10.40785 | best_loss=10.40785
Epoch 68/80: current_loss=10.39042 | best_loss=10.39042
Epoch 69/80: current_loss=10.37526 | best_loss=10.37526
Epoch 70/80: current_loss=10.36256 | best_loss=10.36256
Epoch 71/80: current_loss=10.34413 | best_loss=10.34413
Epoch 72/80: current_loss=10.33152 | best_loss=10.33152
Epoch 73/80: current_loss=10.32491 | best_loss=10.32491
Epoch 74/80: current_loss=10.31093 | best_loss=10.31093
Epoch 75/80: current_loss=10.30243 | best_loss=10.30243
Epoch 76/80: current_loss=10.28461 | best_loss=10.28461
Epoch 77/80: current_loss=10.26026 | best_loss=10.26026
Epoch 78/80: current_loss=10.24178 | best_loss=10.24178
Epoch 79/80: current_loss=10.22712 | best_loss=10.22712
      explained_var=-0.32386 | mse_loss=10.15612

----------------------------------------------
Params for Trial 49
{'learning_rate': 1e-05, 'weight_decay': 0.0011812485304707437, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=82.09888 | best_loss=82.09888
Epoch 1/80: current_loss=81.47938 | best_loss=81.47938
Epoch 2/80: current_loss=80.85639 | best_loss=80.85639
Epoch 3/80: current_loss=80.22701 | best_loss=80.22701
Epoch 4/80: current_loss=79.56892 | best_loss=79.56892
Epoch 5/80: current_loss=78.88005 | best_loss=78.88005
Epoch 6/80: current_loss=78.14979 | best_loss=78.14979
Epoch 7/80: current_loss=77.35102 | best_loss=77.35102
Epoch 8/80: current_loss=76.47503 | best_loss=76.47503
Epoch 9/80: current_loss=75.53060 | best_loss=75.53060
Epoch 10/80: current_loss=74.47041 | best_loss=74.47041
Epoch 11/80: current_loss=73.25779 | best_loss=73.25779
Epoch 12/80: current_loss=71.90628 | best_loss=71.90628
Epoch 13/80: current_loss=70.36991 | best_loss=70.36991
Epoch 14/80: current_loss=68.61475 | best_loss=68.61475
Epoch 15/80: current_loss=66.62326 | best_loss=66.62326
Epoch 16/80: current_loss=64.45624 | best_loss=64.45624
Epoch 17/80: current_loss=62.13445 | best_loss=62.13445
Epoch 18/80: current_loss=59.66941 | best_loss=59.66941
Epoch 19/80: current_loss=57.12228 | best_loss=57.12228
Epoch 20/80: current_loss=54.59314 | best_loss=54.59314
Epoch 21/80: current_loss=52.14135 | best_loss=52.14135
Epoch 22/80: current_loss=49.79177 | best_loss=49.79177
Epoch 23/80: current_loss=47.57483 | best_loss=47.57483
Epoch 24/80: current_loss=45.46503 | best_loss=45.46503
Epoch 25/80: current_loss=43.51459 | best_loss=43.51459
Epoch 26/80: current_loss=41.71488 | best_loss=41.71488
Epoch 27/80: current_loss=40.05396 | best_loss=40.05396
Epoch 28/80: current_loss=38.50913 | best_loss=38.50913
Epoch 29/80: current_loss=37.09874 | best_loss=37.09874
Epoch 30/80: current_loss=35.78511 | best_loss=35.78511
Epoch 31/80: current_loss=34.55653 | best_loss=34.55653
Epoch 32/80: current_loss=33.41985 | best_loss=33.41985
Epoch 33/80: current_loss=32.36607 | best_loss=32.36607
Epoch 34/80: current_loss=31.41639 | best_loss=31.41639
Epoch 35/80: current_loss=30.50857 | best_loss=30.50857
Epoch 36/80: current_loss=29.67721 | best_loss=29.67721
Epoch 37/80: current_loss=28.88558 | best_loss=28.88558
Epoch 38/80: current_loss=28.14548 | best_loss=28.14548
Epoch 39/80: current_loss=27.45074 | best_loss=27.45074
Epoch 40/80: current_loss=26.80596 | best_loss=26.80596
Epoch 41/80: current_loss=26.19290 | best_loss=26.19290
Epoch 42/80: current_loss=25.62203 | best_loss=25.62203
Epoch 43/80: current_loss=25.08474 | best_loss=25.08474
Epoch 44/80: current_loss=24.57869 | best_loss=24.57869
Epoch 45/80: current_loss=24.09574 | best_loss=24.09574
Epoch 46/80: current_loss=23.63959 | best_loss=23.63959
Epoch 47/80: current_loss=23.20662 | best_loss=23.20662
Epoch 48/80: current_loss=22.79081 | best_loss=22.79081
Epoch 49/80: current_loss=22.40436 | best_loss=22.40436
Epoch 50/80: current_loss=22.02621 | best_loss=22.02621
Epoch 51/80: current_loss=21.66883 | best_loss=21.66883
Epoch 52/80: current_loss=21.32165 | best_loss=21.32165
Epoch 53/80: current_loss=20.99266 | best_loss=20.99266
Epoch 54/80: current_loss=20.66721 | best_loss=20.66721
Epoch 55/80: current_loss=20.35488 | best_loss=20.35488
Epoch 56/80: current_loss=20.07048 | best_loss=20.07048
Epoch 57/80: current_loss=19.78919 | best_loss=19.78919
Epoch 58/80: current_loss=19.51311 | best_loss=19.51311
Epoch 59/80: current_loss=19.24920 | best_loss=19.24920
Epoch 60/80: current_loss=18.99441 | best_loss=18.99441
Epoch 61/80: current_loss=18.74731 | best_loss=18.74731
Epoch 62/80: current_loss=18.51630 | best_loss=18.51630
Epoch 63/80: current_loss=18.28181 | best_loss=18.28181
Epoch 64/80: current_loss=18.06625 | best_loss=18.06625
Epoch 65/80: current_loss=17.86055 | best_loss=17.86055
Epoch 66/80: current_loss=17.65011 | best_loss=17.65011
Epoch 67/80: current_loss=17.45326 | best_loss=17.45326
Epoch 68/80: current_loss=17.26047 | best_loss=17.26047
Epoch 69/80: current_loss=17.07404 | best_loss=17.07404
Epoch 70/80: current_loss=16.88735 | best_loss=16.88735
Epoch 71/80: current_loss=16.71182 | best_loss=16.71182
Epoch 72/80: current_loss=16.54105 | best_loss=16.54105
Epoch 73/80: current_loss=16.37125 | best_loss=16.37125
Epoch 74/80: current_loss=16.21082 | best_loss=16.21082
Epoch 75/80: current_loss=16.05486 | best_loss=16.05486
Epoch 76/80: current_loss=15.90630 | best_loss=15.90630
Epoch 77/80: current_loss=15.75729 | best_loss=15.75729
Epoch 78/80: current_loss=15.61526 | best_loss=15.61526
Epoch 79/80: current_loss=15.47659 | best_loss=15.47659
      explained_var=-0.14146 | mse_loss=15.07675

----------------------------------------------
Params for Trial 50
{'learning_rate': 0.1, 'weight_decay': 0.007382303183224713, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.92973 | best_loss=9.92973
Epoch 1/80: current_loss=8.09245 | best_loss=8.09245
Epoch 2/80: current_loss=7.78237 | best_loss=7.78237
Epoch 3/80: current_loss=7.80247 | best_loss=7.78237
Epoch 4/80: current_loss=8.45875 | best_loss=7.78237
Epoch 5/80: current_loss=11.09903 | best_loss=7.78237
Epoch 6/80: current_loss=13.73120 | best_loss=7.78237
Epoch 7/80: current_loss=10.08121 | best_loss=7.78237
Epoch 8/80: current_loss=8.43268 | best_loss=7.78237
Epoch 9/80: current_loss=7.73088 | best_loss=7.73088
Epoch 10/80: current_loss=7.77221 | best_loss=7.73088
Epoch 11/80: current_loss=8.48324 | best_loss=7.73088
Epoch 12/80: current_loss=9.21092 | best_loss=7.73088
Epoch 13/80: current_loss=7.99536 | best_loss=7.73088
Epoch 14/80: current_loss=9.42663 | best_loss=7.73088
Epoch 15/80: current_loss=13.44525 | best_loss=7.73088
Epoch 16/80: current_loss=10.23935 | best_loss=7.73088
Epoch 17/80: current_loss=8.47680 | best_loss=7.73088
Epoch 18/80: current_loss=7.93806 | best_loss=7.73088
Epoch 19/80: current_loss=8.01282 | best_loss=7.73088
Epoch 20/80: current_loss=7.84209 | best_loss=7.73088
Epoch 21/80: current_loss=10.72817 | best_loss=7.73088
Epoch 22/80: current_loss=8.45519 | best_loss=7.73088
Epoch 23/80: current_loss=8.02176 | best_loss=7.73088
Epoch 24/80: current_loss=9.13047 | best_loss=7.73088
Epoch 25/80: current_loss=9.15305 | best_loss=7.73088
Epoch 26/80: current_loss=8.40230 | best_loss=7.73088
Epoch 27/80: current_loss=9.01048 | best_loss=7.73088
Epoch 28/80: current_loss=8.86882 | best_loss=7.73088
Epoch 29/80: current_loss=11.52637 | best_loss=7.73088
Early Stopping at epoch 29
      explained_var=0.00489 | mse_loss=7.55305
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.75123 | best_loss=9.75123
Epoch 1/80: current_loss=8.66741 | best_loss=8.66741
Epoch 2/80: current_loss=8.86932 | best_loss=8.66741
Epoch 3/80: current_loss=8.33792 | best_loss=8.33792
Epoch 4/80: current_loss=39.26460 | best_loss=8.33792
Epoch 5/80: current_loss=13.51484 | best_loss=8.33792
Epoch 6/80: current_loss=8.50443 | best_loss=8.33792
Epoch 7/80: current_loss=12.03207 | best_loss=8.33792
Epoch 8/80: current_loss=12.26852 | best_loss=8.33792
Epoch 9/80: current_loss=8.45893 | best_loss=8.33792
Epoch 10/80: current_loss=8.75913 | best_loss=8.33792
Epoch 11/80: current_loss=8.70237 | best_loss=8.33792
Epoch 12/80: current_loss=9.24376 | best_loss=8.33792
Epoch 13/80: current_loss=8.52207 | best_loss=8.33792
Epoch 14/80: current_loss=9.09182 | best_loss=8.33792
Epoch 15/80: current_loss=8.68971 | best_loss=8.33792
Epoch 16/80: current_loss=9.26401 | best_loss=8.33792
Epoch 17/80: current_loss=8.25301 | best_loss=8.25301
Epoch 18/80: current_loss=8.75250 | best_loss=8.25301
Epoch 19/80: current_loss=9.25514 | best_loss=8.25301
Epoch 20/80: current_loss=10.99828 | best_loss=8.25301
Epoch 21/80: current_loss=8.59594 | best_loss=8.25301
Epoch 22/80: current_loss=8.47029 | best_loss=8.25301
Epoch 23/80: current_loss=8.15404 | best_loss=8.15404
Epoch 24/80: current_loss=8.55117 | best_loss=8.15404
Epoch 25/80: current_loss=8.61971 | best_loss=8.15404
Epoch 26/80: current_loss=9.23587 | best_loss=8.15404
Epoch 27/80: current_loss=8.95509 | best_loss=8.15404
Epoch 28/80: current_loss=9.39059 | best_loss=8.15404
Epoch 29/80: current_loss=8.64325 | best_loss=8.15404
Epoch 30/80: current_loss=8.35220 | best_loss=8.15404
Epoch 31/80: current_loss=8.58686 | best_loss=8.15404
Epoch 32/80: current_loss=10.22113 | best_loss=8.15404
Epoch 33/80: current_loss=8.10848 | best_loss=8.10848
Epoch 34/80: current_loss=8.39068 | best_loss=8.10848
Epoch 35/80: current_loss=8.23425 | best_loss=8.10848
Epoch 36/80: current_loss=8.33665 | best_loss=8.10848
Epoch 37/80: current_loss=10.76179 | best_loss=8.10848
Epoch 38/80: current_loss=8.78890 | best_loss=8.10848
Epoch 39/80: current_loss=11.14966 | best_loss=8.10848
Epoch 40/80: current_loss=8.73935 | best_loss=8.10848
Epoch 41/80: current_loss=8.12293 | best_loss=8.10848
Epoch 42/80: current_loss=9.71613 | best_loss=8.10848
Epoch 43/80: current_loss=9.32514 | best_loss=8.10848
Epoch 44/80: current_loss=9.37287 | best_loss=8.10848
Epoch 45/80: current_loss=8.56446 | best_loss=8.10848
Epoch 46/80: current_loss=8.36254 | best_loss=8.10848
Epoch 47/80: current_loss=8.81074 | best_loss=8.10848
Epoch 48/80: current_loss=9.49124 | best_loss=8.10848
Epoch 49/80: current_loss=8.34522 | best_loss=8.10848
Epoch 50/80: current_loss=8.62854 | best_loss=8.10848
Epoch 51/80: current_loss=9.24470 | best_loss=8.10848
Epoch 52/80: current_loss=8.47689 | best_loss=8.10848
Epoch 53/80: current_loss=9.75580 | best_loss=8.10848
Early Stopping at epoch 53
      explained_var=0.02384 | mse_loss=7.97910
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.42733 | best_loss=9.42733
Epoch 1/80: current_loss=9.44447 | best_loss=9.42733
Epoch 2/80: current_loss=10.52691 | best_loss=9.42733
Epoch 3/80: current_loss=9.85088 | best_loss=9.42733
Epoch 4/80: current_loss=8.83175 | best_loss=8.83175
Epoch 5/80: current_loss=10.06662 | best_loss=8.83175
Epoch 6/80: current_loss=11.11623 | best_loss=8.83175
Epoch 7/80: current_loss=10.55930 | best_loss=8.83175
Epoch 8/80: current_loss=11.81149 | best_loss=8.83175
Epoch 9/80: current_loss=9.43084 | best_loss=8.83175
Epoch 10/80: current_loss=16.06057 | best_loss=8.83175
Epoch 11/80: current_loss=10.17876 | best_loss=8.83175
Epoch 12/80: current_loss=7.56447 | best_loss=7.56447
Epoch 13/80: current_loss=9.02318 | best_loss=7.56447
Epoch 14/80: current_loss=10.64436 | best_loss=7.56447
Epoch 15/80: current_loss=10.70128 | best_loss=7.56447
Epoch 16/80: current_loss=9.43407 | best_loss=7.56447
Epoch 17/80: current_loss=9.47629 | best_loss=7.56447
Epoch 18/80: current_loss=8.87333 | best_loss=7.56447
Epoch 19/80: current_loss=8.88367 | best_loss=7.56447
Epoch 20/80: current_loss=9.04127 | best_loss=7.56447
Epoch 21/80: current_loss=8.71918 | best_loss=7.56447
Epoch 22/80: current_loss=8.95855 | best_loss=7.56447
Epoch 23/80: current_loss=10.71004 | best_loss=7.56447
Epoch 24/80: current_loss=9.31600 | best_loss=7.56447
Epoch 25/80: current_loss=11.33762 | best_loss=7.56447
Epoch 26/80: current_loss=9.98739 | best_loss=7.56447
Epoch 27/80: current_loss=12.66434 | best_loss=7.56447
Epoch 28/80: current_loss=8.73089 | best_loss=7.56447
Epoch 29/80: current_loss=8.70491 | best_loss=7.56447
Epoch 30/80: current_loss=9.05357 | best_loss=7.56447
Epoch 31/80: current_loss=13.30836 | best_loss=7.56447
Epoch 32/80: current_loss=10.17568 | best_loss=7.56447
Early Stopping at epoch 32
      explained_var=0.12063 | mse_loss=7.50828
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.13736 | best_loss=8.13736
Epoch 1/80: current_loss=7.97476 | best_loss=7.97476
Epoch 2/80: current_loss=8.05053 | best_loss=7.97476
Epoch 3/80: current_loss=10.95535 | best_loss=7.97476
Epoch 4/80: current_loss=7.82219 | best_loss=7.82219
Epoch 5/80: current_loss=9.06559 | best_loss=7.82219
Epoch 6/80: current_loss=7.95119 | best_loss=7.82219
Epoch 7/80: current_loss=7.99413 | best_loss=7.82219
Epoch 8/80: current_loss=9.13626 | best_loss=7.82219
Epoch 9/80: current_loss=13.02137 | best_loss=7.82219
Epoch 10/80: current_loss=7.79351 | best_loss=7.79351
Epoch 11/80: current_loss=8.29684 | best_loss=7.79351
Epoch 12/80: current_loss=8.90488 | best_loss=7.79351
Epoch 13/80: current_loss=11.45294 | best_loss=7.79351
Epoch 14/80: current_loss=10.66126 | best_loss=7.79351
Epoch 15/80: current_loss=8.99678 | best_loss=7.79351
Epoch 16/80: current_loss=8.20227 | best_loss=7.79351
Epoch 17/80: current_loss=8.61165 | best_loss=7.79351
Epoch 18/80: current_loss=9.16483 | best_loss=7.79351
Epoch 19/80: current_loss=9.07974 | best_loss=7.79351
Epoch 20/80: current_loss=8.39499 | best_loss=7.79351
Epoch 21/80: current_loss=8.84499 | best_loss=7.79351
Epoch 22/80: current_loss=9.27271 | best_loss=7.79351
Epoch 23/80: current_loss=8.88628 | best_loss=7.79351
Epoch 24/80: current_loss=8.51364 | best_loss=7.79351
Epoch 25/80: current_loss=12.12912 | best_loss=7.79351
Epoch 26/80: current_loss=8.41589 | best_loss=7.79351
Epoch 27/80: current_loss=8.10489 | best_loss=7.79351
Epoch 28/80: current_loss=10.00141 | best_loss=7.79351
Epoch 29/80: current_loss=8.74840 | best_loss=7.79351
Epoch 30/80: current_loss=11.02433 | best_loss=7.79351
Early Stopping at epoch 30
      explained_var=0.06015 | mse_loss=7.83677
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.35730 | best_loss=9.35730
Epoch 1/80: current_loss=7.66211 | best_loss=7.66211
Epoch 2/80: current_loss=7.65541 | best_loss=7.65541
Epoch 3/80: current_loss=8.82260 | best_loss=7.65541
Epoch 4/80: current_loss=9.97909 | best_loss=7.65541
Epoch 5/80: current_loss=7.92601 | best_loss=7.65541
Epoch 6/80: current_loss=7.81117 | best_loss=7.65541
Epoch 7/80: current_loss=9.27940 | best_loss=7.65541
Epoch 8/80: current_loss=8.87200 | best_loss=7.65541
Epoch 9/80: current_loss=7.70107 | best_loss=7.65541
Epoch 10/80: current_loss=7.94990 | best_loss=7.65541
Epoch 11/80: current_loss=8.60045 | best_loss=7.65541
Epoch 12/80: current_loss=7.76914 | best_loss=7.65541
Epoch 13/80: current_loss=9.22025 | best_loss=7.65541
Epoch 14/80: current_loss=7.96241 | best_loss=7.65541
Epoch 15/80: current_loss=7.93884 | best_loss=7.65541
Epoch 16/80: current_loss=8.60119 | best_loss=7.65541
Epoch 17/80: current_loss=7.66262 | best_loss=7.65541
Epoch 18/80: current_loss=11.39405 | best_loss=7.65541
Epoch 19/80: current_loss=11.77732 | best_loss=7.65541
Epoch 20/80: current_loss=7.70948 | best_loss=7.65541
Epoch 21/80: current_loss=8.25757 | best_loss=7.65541
Epoch 22/80: current_loss=7.88068 | best_loss=7.65541
Early Stopping at epoch 22
      explained_var=0.00195 | mse_loss=7.84025
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.04229| avg_loss=7.74349
----------------------------------------------


----------------------------------------------
Params for Trial 51
{'learning_rate': 0.1, 'weight_decay': 0.007549957762672246, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.48092 | best_loss=14.48092
Epoch 1/80: current_loss=14.52019 | best_loss=14.48092
Epoch 2/80: current_loss=95.49445 | best_loss=14.48092
Epoch 3/80: current_loss=8.51178 | best_loss=8.51178
Epoch 4/80: current_loss=9.35420 | best_loss=8.51178
Epoch 5/80: current_loss=7.82009 | best_loss=7.82009
Epoch 6/80: current_loss=7.89889 | best_loss=7.82009
Epoch 7/80: current_loss=8.21696 | best_loss=7.82009
Epoch 8/80: current_loss=7.92949 | best_loss=7.82009
Epoch 9/80: current_loss=8.18822 | best_loss=7.82009
Epoch 10/80: current_loss=7.77800 | best_loss=7.77800
Epoch 11/80: current_loss=9.50513 | best_loss=7.77800
Epoch 12/80: current_loss=7.75392 | best_loss=7.75392
Epoch 13/80: current_loss=8.01435 | best_loss=7.75392
Epoch 14/80: current_loss=8.52864 | best_loss=7.75392
Epoch 15/80: current_loss=7.82732 | best_loss=7.75392
Epoch 16/80: current_loss=7.85597 | best_loss=7.75392
Epoch 17/80: current_loss=8.19826 | best_loss=7.75392
Epoch 18/80: current_loss=7.93464 | best_loss=7.75392
Epoch 19/80: current_loss=9.60469 | best_loss=7.75392
Epoch 20/80: current_loss=8.13904 | best_loss=7.75392
Epoch 21/80: current_loss=8.18836 | best_loss=7.75392
Epoch 22/80: current_loss=9.06156 | best_loss=7.75392
Epoch 23/80: current_loss=9.32896 | best_loss=7.75392
Epoch 24/80: current_loss=7.80754 | best_loss=7.75392
Epoch 25/80: current_loss=8.81744 | best_loss=7.75392
Epoch 26/80: current_loss=7.77732 | best_loss=7.75392
Epoch 27/80: current_loss=8.08366 | best_loss=7.75392
Epoch 28/80: current_loss=9.13714 | best_loss=7.75392
Epoch 29/80: current_loss=12.25729 | best_loss=7.75392
Epoch 30/80: current_loss=14.52322 | best_loss=7.75392
Epoch 31/80: current_loss=8.00626 | best_loss=7.75392
Epoch 32/80: current_loss=9.63787 | best_loss=7.75392
Early Stopping at epoch 32
      explained_var=0.00488 | mse_loss=7.55925
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.51135 | best_loss=8.51135
Epoch 1/80: current_loss=8.61117 | best_loss=8.51135
Epoch 2/80: current_loss=8.28291 | best_loss=8.28291
Epoch 3/80: current_loss=8.34188 | best_loss=8.28291
Epoch 4/80: current_loss=8.84966 | best_loss=8.28291
Epoch 5/80: current_loss=8.38034 | best_loss=8.28291
Epoch 6/80: current_loss=8.47042 | best_loss=8.28291
Epoch 7/80: current_loss=9.99134 | best_loss=8.28291
Epoch 8/80: current_loss=10.38687 | best_loss=8.28291
Epoch 9/80: current_loss=10.73580 | best_loss=8.28291
Epoch 10/80: current_loss=12.94361 | best_loss=8.28291
Epoch 11/80: current_loss=8.30992 | best_loss=8.28291
Epoch 12/80: current_loss=8.58502 | best_loss=8.28291
Epoch 13/80: current_loss=8.62406 | best_loss=8.28291
Epoch 14/80: current_loss=15.00052 | best_loss=8.28291
Epoch 15/80: current_loss=11.97597 | best_loss=8.28291
Epoch 16/80: current_loss=8.91049 | best_loss=8.28291
Epoch 17/80: current_loss=9.27039 | best_loss=8.28291
Epoch 18/80: current_loss=9.65644 | best_loss=8.28291
Epoch 19/80: current_loss=8.43584 | best_loss=8.28291
Epoch 20/80: current_loss=8.50941 | best_loss=8.28291
Epoch 21/80: current_loss=9.41292 | best_loss=8.28291
Epoch 22/80: current_loss=8.36126 | best_loss=8.28291
Early Stopping at epoch 22
      explained_var=0.00495 | mse_loss=8.12303
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.05048 | best_loss=10.05048
Epoch 1/80: current_loss=9.06527 | best_loss=9.06527
Epoch 2/80: current_loss=10.04657 | best_loss=9.06527
Epoch 3/80: current_loss=9.04945 | best_loss=9.04945
Epoch 4/80: current_loss=9.24883 | best_loss=9.04945
Epoch 5/80: current_loss=8.70652 | best_loss=8.70652
Epoch 6/80: current_loss=11.17547 | best_loss=8.70652
Epoch 7/80: current_loss=11.37038 | best_loss=8.70652
Epoch 8/80: current_loss=9.57707 | best_loss=8.70652
Epoch 9/80: current_loss=11.44864 | best_loss=8.70652
Epoch 10/80: current_loss=8.78666 | best_loss=8.70652
Epoch 11/80: current_loss=9.23540 | best_loss=8.70652
Epoch 12/80: current_loss=9.09467 | best_loss=8.70652
Epoch 13/80: current_loss=10.58645 | best_loss=8.70652
Epoch 14/80: current_loss=11.89359 | best_loss=8.70652
Epoch 15/80: current_loss=9.81977 | best_loss=8.70652
Epoch 16/80: current_loss=9.46836 | best_loss=8.70652
Epoch 17/80: current_loss=8.76624 | best_loss=8.70652
Epoch 18/80: current_loss=9.47396 | best_loss=8.70652
Epoch 19/80: current_loss=9.16082 | best_loss=8.70652
Epoch 20/80: current_loss=9.71454 | best_loss=8.70652
Epoch 21/80: current_loss=11.21741 | best_loss=8.70652
Epoch 22/80: current_loss=10.69954 | best_loss=8.70652
Epoch 23/80: current_loss=10.24311 | best_loss=8.70652
Epoch 24/80: current_loss=10.64848 | best_loss=8.70652
Epoch 25/80: current_loss=11.04660 | best_loss=8.70652
Early Stopping at epoch 25
      explained_var=0.00577 | mse_loss=8.45808
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.24657 | best_loss=9.24657
Epoch 1/80: current_loss=11.22011 | best_loss=9.24657
Epoch 2/80: current_loss=8.41794 | best_loss=8.41794
Epoch 3/80: current_loss=8.51617 | best_loss=8.41794
Epoch 4/80: current_loss=10.61604 | best_loss=8.41794
Epoch 5/80: current_loss=8.51778 | best_loss=8.41794
Epoch 6/80: current_loss=8.21494 | best_loss=8.21494
Epoch 7/80: current_loss=17.42828 | best_loss=8.21494
Epoch 8/80: current_loss=8.67587 | best_loss=8.21494
Epoch 9/80: current_loss=8.94872 | best_loss=8.21494
Epoch 10/80: current_loss=11.71554 | best_loss=8.21494
Epoch 11/80: current_loss=8.30252 | best_loss=8.21494
Epoch 12/80: current_loss=8.70561 | best_loss=8.21494
Epoch 13/80: current_loss=12.14552 | best_loss=8.21494
Epoch 14/80: current_loss=8.74498 | best_loss=8.21494
Epoch 15/80: current_loss=8.64495 | best_loss=8.21494
Epoch 16/80: current_loss=12.87924 | best_loss=8.21494
Epoch 17/80: current_loss=8.93329 | best_loss=8.21494
Epoch 18/80: current_loss=8.26597 | best_loss=8.21494
Epoch 19/80: current_loss=9.53509 | best_loss=8.21494
Epoch 20/80: current_loss=8.46186 | best_loss=8.21494
Epoch 21/80: current_loss=8.56418 | best_loss=8.21494
Epoch 22/80: current_loss=9.61233 | best_loss=8.21494
Epoch 23/80: current_loss=8.23242 | best_loss=8.21494
Epoch 24/80: current_loss=9.08409 | best_loss=8.21494
Epoch 25/80: current_loss=11.31393 | best_loss=8.21494
Epoch 26/80: current_loss=8.57531 | best_loss=8.21494
Early Stopping at epoch 26
      explained_var=0.00230 | mse_loss=8.31350
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.32741 | best_loss=8.32741
Epoch 1/80: current_loss=7.87232 | best_loss=7.87232
Epoch 2/80: current_loss=9.33320 | best_loss=7.87232
Epoch 3/80: current_loss=8.13469 | best_loss=7.87232
Epoch 4/80: current_loss=7.79835 | best_loss=7.79835
Epoch 5/80: current_loss=8.95885 | best_loss=7.79835
Epoch 6/80: current_loss=7.93920 | best_loss=7.79835
Epoch 7/80: current_loss=8.25845 | best_loss=7.79835
Epoch 8/80: current_loss=7.94150 | best_loss=7.79835
Epoch 9/80: current_loss=9.42644 | best_loss=7.79835
Epoch 10/80: current_loss=7.98935 | best_loss=7.79835
Epoch 11/80: current_loss=10.87102 | best_loss=7.79835
Epoch 12/80: current_loss=11.21704 | best_loss=7.79835
Epoch 13/80: current_loss=8.21749 | best_loss=7.79835
Epoch 14/80: current_loss=10.89935 | best_loss=7.79835
Epoch 15/80: current_loss=12.19904 | best_loss=7.79835
Epoch 16/80: current_loss=7.85800 | best_loss=7.79835
Epoch 17/80: current_loss=8.85621 | best_loss=7.79835
Epoch 18/80: current_loss=9.07248 | best_loss=7.79835
Epoch 19/80: current_loss=7.98124 | best_loss=7.79835
Epoch 20/80: current_loss=10.04457 | best_loss=7.79835
Epoch 21/80: current_loss=9.82005 | best_loss=7.79835
Epoch 22/80: current_loss=8.92765 | best_loss=7.79835
Epoch 23/80: current_loss=7.68188 | best_loss=7.68188
Epoch 24/80: current_loss=13.60594 | best_loss=7.68188
Epoch 25/80: current_loss=9.81286 | best_loss=7.68188
Epoch 26/80: current_loss=7.74804 | best_loss=7.68188
Epoch 27/80: current_loss=11.29617 | best_loss=7.68188
Epoch 28/80: current_loss=12.12864 | best_loss=7.68188
Epoch 29/80: current_loss=8.53242 | best_loss=7.68188
Epoch 30/80: current_loss=7.82370 | best_loss=7.68188
Epoch 31/80: current_loss=8.66557 | best_loss=7.68188
Epoch 32/80: current_loss=9.12454 | best_loss=7.68188
Epoch 33/80: current_loss=7.65730 | best_loss=7.65730
Epoch 34/80: current_loss=11.26945 | best_loss=7.65730
Epoch 35/80: current_loss=8.38250 | best_loss=7.65730
Epoch 36/80: current_loss=7.80934 | best_loss=7.65730
Epoch 37/80: current_loss=7.68586 | best_loss=7.65730
Epoch 38/80: current_loss=11.52656 | best_loss=7.65730
Epoch 39/80: current_loss=8.07022 | best_loss=7.65730
Epoch 40/80: current_loss=12.49382 | best_loss=7.65730
Epoch 41/80: current_loss=9.58314 | best_loss=7.65730
Epoch 42/80: current_loss=11.66390 | best_loss=7.65730
Epoch 43/80: current_loss=9.53303 | best_loss=7.65730
Epoch 44/80: current_loss=8.61266 | best_loss=7.65730
Epoch 45/80: current_loss=7.66629 | best_loss=7.65730
Epoch 46/80: current_loss=7.74693 | best_loss=7.65730
Epoch 47/80: current_loss=8.96814 | best_loss=7.65730
Epoch 48/80: current_loss=8.35419 | best_loss=7.65730
Epoch 49/80: current_loss=7.94852 | best_loss=7.65730
Epoch 50/80: current_loss=8.18605 | best_loss=7.65730
Epoch 51/80: current_loss=12.06522 | best_loss=7.65730
Epoch 52/80: current_loss=8.19582 | best_loss=7.65730
Epoch 53/80: current_loss=8.03411 | best_loss=7.65730
Early Stopping at epoch 53
      explained_var=0.00241 | mse_loss=7.84271
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00406| avg_loss=8.05931
----------------------------------------------


----------------------------------------------
Params for Trial 52
{'learning_rate': 0.1, 'weight_decay': 0.006355783027331873, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.86870 | best_loss=8.86870
Epoch 1/80: current_loss=9.53275 | best_loss=8.86870
Epoch 2/80: current_loss=8.45607 | best_loss=8.45607
Epoch 3/80: current_loss=9.50191 | best_loss=8.45607
Epoch 4/80: current_loss=8.19060 | best_loss=8.19060
Epoch 5/80: current_loss=7.84653 | best_loss=7.84653
Epoch 6/80: current_loss=9.05159 | best_loss=7.84653
Epoch 7/80: current_loss=10.26770 | best_loss=7.84653
Epoch 8/80: current_loss=10.99192 | best_loss=7.84653
Epoch 9/80: current_loss=11.53725 | best_loss=7.84653
Epoch 10/80: current_loss=8.36655 | best_loss=7.84653
Epoch 11/80: current_loss=7.75280 | best_loss=7.75280
Epoch 12/80: current_loss=7.85676 | best_loss=7.75280
Epoch 13/80: current_loss=8.95053 | best_loss=7.75280
Epoch 14/80: current_loss=7.77221 | best_loss=7.75280
Epoch 15/80: current_loss=8.01141 | best_loss=7.75280
Epoch 16/80: current_loss=12.04673 | best_loss=7.75280
Epoch 17/80: current_loss=7.76040 | best_loss=7.75280
Epoch 18/80: current_loss=13.73137 | best_loss=7.75280
Epoch 19/80: current_loss=8.24198 | best_loss=7.75280
Epoch 20/80: current_loss=18.57186 | best_loss=7.75280
Epoch 21/80: current_loss=8.69406 | best_loss=7.75280
Epoch 22/80: current_loss=8.19366 | best_loss=7.75280
Epoch 23/80: current_loss=7.74058 | best_loss=7.74058
Epoch 24/80: current_loss=7.73715 | best_loss=7.73715
Epoch 25/80: current_loss=10.73481 | best_loss=7.73715
Epoch 26/80: current_loss=8.49445 | best_loss=7.73715
Epoch 27/80: current_loss=8.42139 | best_loss=7.73715
Epoch 28/80: current_loss=7.99563 | best_loss=7.73715
Epoch 29/80: current_loss=7.76986 | best_loss=7.73715
Epoch 30/80: current_loss=7.81624 | best_loss=7.73715
Epoch 31/80: current_loss=7.75976 | best_loss=7.73715
Epoch 32/80: current_loss=7.80745 | best_loss=7.73715
Epoch 33/80: current_loss=8.88349 | best_loss=7.73715
Epoch 34/80: current_loss=8.02678 | best_loss=7.73715
Epoch 35/80: current_loss=9.90533 | best_loss=7.73715
Epoch 36/80: current_loss=8.86586 | best_loss=7.73715
Epoch 37/80: current_loss=10.26343 | best_loss=7.73715
Epoch 38/80: current_loss=7.82150 | best_loss=7.73715
Epoch 39/80: current_loss=7.78123 | best_loss=7.73715
Epoch 40/80: current_loss=9.75923 | best_loss=7.73715
Epoch 41/80: current_loss=7.90904 | best_loss=7.73715
Epoch 42/80: current_loss=7.93555 | best_loss=7.73715
Epoch 43/80: current_loss=8.67161 | best_loss=7.73715
Epoch 44/80: current_loss=8.68983 | best_loss=7.73715
Early Stopping at epoch 44
      explained_var=0.00495 | mse_loss=7.54970
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.71439 | best_loss=12.71439
Epoch 1/80: current_loss=8.99845 | best_loss=8.99845
Epoch 2/80: current_loss=11.08666 | best_loss=8.99845
Epoch 3/80: current_loss=8.37089 | best_loss=8.37089
Epoch 4/80: current_loss=8.46363 | best_loss=8.37089
Epoch 5/80: current_loss=10.27353 | best_loss=8.37089
Epoch 6/80: current_loss=10.59362 | best_loss=8.37089
Epoch 7/80: current_loss=8.43228 | best_loss=8.37089
Epoch 8/80: current_loss=8.70506 | best_loss=8.37089
Epoch 9/80: current_loss=9.56856 | best_loss=8.37089
Epoch 10/80: current_loss=9.44850 | best_loss=8.37089
Epoch 11/80: current_loss=8.30651 | best_loss=8.30651
Epoch 12/80: current_loss=9.66530 | best_loss=8.30651
Epoch 13/80: current_loss=9.15917 | best_loss=8.30651
Epoch 14/80: current_loss=8.78858 | best_loss=8.30651
Epoch 15/80: current_loss=9.34031 | best_loss=8.30651
Epoch 16/80: current_loss=18.81443 | best_loss=8.30651
Epoch 17/80: current_loss=11.39196 | best_loss=8.30651
Epoch 18/80: current_loss=8.45493 | best_loss=8.30651
Epoch 19/80: current_loss=10.11645 | best_loss=8.30651
Epoch 20/80: current_loss=8.85915 | best_loss=8.30651
Epoch 21/80: current_loss=8.47120 | best_loss=8.30651
Epoch 22/80: current_loss=9.06114 | best_loss=8.30651
Epoch 23/80: current_loss=9.60727 | best_loss=8.30651
Epoch 24/80: current_loss=8.36718 | best_loss=8.30651
Epoch 25/80: current_loss=11.01141 | best_loss=8.30651
Epoch 26/80: current_loss=8.85177 | best_loss=8.30651
Epoch 27/80: current_loss=9.04988 | best_loss=8.30651
Epoch 28/80: current_loss=9.04372 | best_loss=8.30651
Epoch 29/80: current_loss=9.12663 | best_loss=8.30651
Epoch 30/80: current_loss=9.84166 | best_loss=8.30651
Epoch 31/80: current_loss=8.21670 | best_loss=8.21670
Epoch 32/80: current_loss=9.81176 | best_loss=8.21670
Epoch 33/80: current_loss=8.38950 | best_loss=8.21670
Epoch 34/80: current_loss=8.76286 | best_loss=8.21670
Epoch 35/80: current_loss=14.02240 | best_loss=8.21670
Epoch 36/80: current_loss=9.31678 | best_loss=8.21670
Epoch 37/80: current_loss=8.71374 | best_loss=8.21670
Epoch 38/80: current_loss=10.73978 | best_loss=8.21670
Epoch 39/80: current_loss=9.49999 | best_loss=8.21670
Epoch 40/80: current_loss=8.56741 | best_loss=8.21670
Epoch 41/80: current_loss=8.46913 | best_loss=8.21670
Epoch 42/80: current_loss=9.80536 | best_loss=8.21670
Epoch 43/80: current_loss=10.11990 | best_loss=8.21670
Epoch 44/80: current_loss=10.56281 | best_loss=8.21670
Epoch 45/80: current_loss=8.65936 | best_loss=8.21670
Epoch 46/80: current_loss=8.58675 | best_loss=8.21670
Epoch 47/80: current_loss=11.22449 | best_loss=8.21670
Epoch 48/80: current_loss=9.35130 | best_loss=8.21670
Epoch 49/80: current_loss=12.92149 | best_loss=8.21670
Epoch 50/80: current_loss=12.29821 | best_loss=8.21670
Epoch 51/80: current_loss=8.81783 | best_loss=8.21670
Early Stopping at epoch 51
      explained_var=0.00642 | mse_loss=8.11143
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.97760 | best_loss=11.97760
Epoch 1/80: current_loss=10.35209 | best_loss=10.35209
Epoch 2/80: current_loss=9.44604 | best_loss=9.44604
Epoch 3/80: current_loss=8.78808 | best_loss=8.78808
Epoch 4/80: current_loss=8.72732 | best_loss=8.72732
Epoch 5/80: current_loss=15.30019 | best_loss=8.72732
Epoch 6/80: current_loss=9.32486 | best_loss=8.72732
Epoch 7/80: current_loss=12.60980 | best_loss=8.72732
Epoch 8/80: current_loss=8.82080 | best_loss=8.72732
Epoch 9/80: current_loss=9.02423 | best_loss=8.72732
Epoch 10/80: current_loss=10.32838 | best_loss=8.72732
Epoch 11/80: current_loss=8.74783 | best_loss=8.72732
Epoch 12/80: current_loss=8.80350 | best_loss=8.72732
Epoch 13/80: current_loss=12.39538 | best_loss=8.72732
Epoch 14/80: current_loss=8.74933 | best_loss=8.72732
Epoch 15/80: current_loss=8.72967 | best_loss=8.72732
Epoch 16/80: current_loss=9.81923 | best_loss=8.72732
Epoch 17/80: current_loss=12.09131 | best_loss=8.72732
Epoch 18/80: current_loss=8.77619 | best_loss=8.72732
Epoch 19/80: current_loss=10.13263 | best_loss=8.72732
Epoch 20/80: current_loss=11.56165 | best_loss=8.72732
Epoch 21/80: current_loss=8.96130 | best_loss=8.72732
Epoch 22/80: current_loss=9.47660 | best_loss=8.72732
Epoch 23/80: current_loss=11.26807 | best_loss=8.72732
Epoch 24/80: current_loss=11.52962 | best_loss=8.72732
Early Stopping at epoch 24
      explained_var=-0.00121 | mse_loss=8.49720
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.12475 | best_loss=11.12475
Epoch 1/80: current_loss=10.64475 | best_loss=10.64475
Epoch 2/80: current_loss=10.19754 | best_loss=10.19754
Epoch 3/80: current_loss=8.49205 | best_loss=8.49205
Epoch 4/80: current_loss=8.56746 | best_loss=8.49205
Epoch 5/80: current_loss=8.80488 | best_loss=8.49205
Epoch 6/80: current_loss=8.58109 | best_loss=8.49205
Epoch 7/80: current_loss=8.75591 | best_loss=8.49205
Epoch 8/80: current_loss=10.02733 | best_loss=8.49205
Epoch 9/80: current_loss=9.20274 | best_loss=8.49205
Epoch 10/80: current_loss=8.32533 | best_loss=8.32533
Epoch 11/80: current_loss=8.21859 | best_loss=8.21859
Epoch 12/80: current_loss=8.30400 | best_loss=8.21859
Epoch 13/80: current_loss=8.67708 | best_loss=8.21859
Epoch 14/80: current_loss=10.69516 | best_loss=8.21859
Epoch 15/80: current_loss=8.24214 | best_loss=8.21859
Epoch 16/80: current_loss=8.31831 | best_loss=8.21859
Epoch 17/80: current_loss=8.46692 | best_loss=8.21859
Epoch 18/80: current_loss=8.38461 | best_loss=8.21859
Epoch 19/80: current_loss=8.24100 | best_loss=8.21859
Epoch 20/80: current_loss=8.22458 | best_loss=8.21859
Epoch 21/80: current_loss=8.27285 | best_loss=8.21859
Epoch 22/80: current_loss=8.69748 | best_loss=8.21859
Epoch 23/80: current_loss=9.94923 | best_loss=8.21859
Epoch 24/80: current_loss=8.33212 | best_loss=8.21859
Epoch 25/80: current_loss=8.34693 | best_loss=8.21859
Epoch 26/80: current_loss=8.58459 | best_loss=8.21859
Epoch 27/80: current_loss=9.13100 | best_loss=8.21859
Epoch 28/80: current_loss=8.62502 | best_loss=8.21859
Epoch 29/80: current_loss=8.49205 | best_loss=8.21859
Epoch 30/80: current_loss=8.66115 | best_loss=8.21859
Epoch 31/80: current_loss=8.36011 | best_loss=8.21859
Early Stopping at epoch 31
      explained_var=0.00197 | mse_loss=8.31801
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.00558 | best_loss=9.00558
Epoch 1/80: current_loss=8.14490 | best_loss=8.14490
Epoch 2/80: current_loss=7.86796 | best_loss=7.86796
Epoch 3/80: current_loss=8.83473 | best_loss=7.86796
Epoch 4/80: current_loss=10.22261 | best_loss=7.86796
Epoch 5/80: current_loss=8.25812 | best_loss=7.86796
Epoch 6/80: current_loss=7.62750 | best_loss=7.62750
Epoch 7/80: current_loss=7.83646 | best_loss=7.62750
Epoch 8/80: current_loss=9.02072 | best_loss=7.62750
Epoch 9/80: current_loss=7.98286 | best_loss=7.62750
Epoch 10/80: current_loss=10.95606 | best_loss=7.62750
Epoch 11/80: current_loss=10.77456 | best_loss=7.62750
Epoch 12/80: current_loss=10.66760 | best_loss=7.62750
Epoch 13/80: current_loss=8.28387 | best_loss=7.62750
Epoch 14/80: current_loss=8.42533 | best_loss=7.62750
Epoch 15/80: current_loss=7.65815 | best_loss=7.62750
Epoch 16/80: current_loss=10.45603 | best_loss=7.62750
Epoch 17/80: current_loss=8.77998 | best_loss=7.62750
Epoch 18/80: current_loss=7.82252 | best_loss=7.62750
Epoch 19/80: current_loss=7.75852 | best_loss=7.62750
Epoch 20/80: current_loss=7.95244 | best_loss=7.62750
Epoch 21/80: current_loss=8.67084 | best_loss=7.62750
Epoch 22/80: current_loss=7.82010 | best_loss=7.62750
Epoch 23/80: current_loss=9.32889 | best_loss=7.62750
Epoch 24/80: current_loss=8.00161 | best_loss=7.62750
Epoch 25/80: current_loss=9.67888 | best_loss=7.62750
Epoch 26/80: current_loss=8.50542 | best_loss=7.62750
Early Stopping at epoch 26
      explained_var=0.00658 | mse_loss=7.80396
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.00374| avg_loss=8.05606
----------------------------------------------


----------------------------------------------
Params for Trial 53
{'learning_rate': 0.1, 'weight_decay': 0.007378705494742956, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.67660 | best_loss=11.67660
Epoch 1/80: current_loss=10.55813 | best_loss=10.55813
Epoch 2/80: current_loss=7.96381 | best_loss=7.96381
Epoch 3/80: current_loss=7.82250 | best_loss=7.82250
Epoch 4/80: current_loss=7.75582 | best_loss=7.75582
Epoch 5/80: current_loss=7.79917 | best_loss=7.75582
Epoch 6/80: current_loss=9.53686 | best_loss=7.75582
Epoch 7/80: current_loss=8.03599 | best_loss=7.75582
Epoch 8/80: current_loss=8.44540 | best_loss=7.75582
Epoch 9/80: current_loss=8.22044 | best_loss=7.75582
Epoch 10/80: current_loss=8.12563 | best_loss=7.75582
Epoch 11/80: current_loss=8.30638 | best_loss=7.75582
Epoch 12/80: current_loss=8.34272 | best_loss=7.75582
Epoch 13/80: current_loss=7.80056 | best_loss=7.75582
Epoch 14/80: current_loss=8.05715 | best_loss=7.75582
Epoch 15/80: current_loss=8.80420 | best_loss=7.75582
Epoch 16/80: current_loss=9.67835 | best_loss=7.75582
Epoch 17/80: current_loss=7.77221 | best_loss=7.75582
Epoch 18/80: current_loss=9.00265 | best_loss=7.75582
Epoch 19/80: current_loss=9.84442 | best_loss=7.75582
Epoch 20/80: current_loss=8.37575 | best_loss=7.75582
Epoch 21/80: current_loss=7.90447 | best_loss=7.75582
Epoch 22/80: current_loss=7.98418 | best_loss=7.75582
Epoch 23/80: current_loss=7.98314 | best_loss=7.75582
Epoch 24/80: current_loss=9.20307 | best_loss=7.75582
Early Stopping at epoch 24
      explained_var=0.00283 | mse_loss=7.58399
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.42560 | best_loss=10.42560
Epoch 1/80: current_loss=8.67661 | best_loss=8.67661
Epoch 2/80: current_loss=9.11864 | best_loss=8.67661
Epoch 3/80: current_loss=8.18732 | best_loss=8.18732
Epoch 4/80: current_loss=8.42469 | best_loss=8.18732
Epoch 5/80: current_loss=12.92411 | best_loss=8.18732
Epoch 6/80: current_loss=9.37848 | best_loss=8.18732
Epoch 7/80: current_loss=8.59143 | best_loss=8.18732
Epoch 8/80: current_loss=9.10520 | best_loss=8.18732
Epoch 9/80: current_loss=12.03808 | best_loss=8.18732
Epoch 10/80: current_loss=8.24067 | best_loss=8.18732
Epoch 11/80: current_loss=8.93038 | best_loss=8.18732
Epoch 12/80: current_loss=8.22258 | best_loss=8.18732
Epoch 13/80: current_loss=8.31375 | best_loss=8.18732
Epoch 14/80: current_loss=8.23230 | best_loss=8.18732
Epoch 15/80: current_loss=9.17491 | best_loss=8.18732
Epoch 16/80: current_loss=8.51138 | best_loss=8.18732
Epoch 17/80: current_loss=9.71835 | best_loss=8.18732
Epoch 18/80: current_loss=8.88906 | best_loss=8.18732
Epoch 19/80: current_loss=10.08496 | best_loss=8.18732
Epoch 20/80: current_loss=8.34228 | best_loss=8.18732
Epoch 21/80: current_loss=9.91838 | best_loss=8.18732
Epoch 22/80: current_loss=8.52481 | best_loss=8.18732
Epoch 23/80: current_loss=10.03916 | best_loss=8.18732
Early Stopping at epoch 23
      explained_var=0.01724 | mse_loss=8.12354
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.53048 | best_loss=10.53048
Epoch 1/80: current_loss=9.11576 | best_loss=9.11576
Epoch 2/80: current_loss=8.72844 | best_loss=8.72844
Epoch 3/80: current_loss=8.78286 | best_loss=8.72844
Epoch 4/80: current_loss=13.32895 | best_loss=8.72844
Epoch 5/80: current_loss=11.32102 | best_loss=8.72844
Epoch 6/80: current_loss=13.67512 | best_loss=8.72844
Epoch 7/80: current_loss=9.45095 | best_loss=8.72844
Epoch 8/80: current_loss=9.60428 | best_loss=8.72844
Epoch 9/80: current_loss=8.74290 | best_loss=8.72844
Epoch 10/80: current_loss=9.68580 | best_loss=8.72844
Epoch 11/80: current_loss=11.70350 | best_loss=8.72844
Epoch 12/80: current_loss=8.83278 | best_loss=8.72844
Epoch 13/80: current_loss=10.94464 | best_loss=8.72844
Epoch 14/80: current_loss=8.63992 | best_loss=8.63992
Epoch 15/80: current_loss=10.94493 | best_loss=8.63992
Epoch 16/80: current_loss=9.17094 | best_loss=8.63992
Epoch 17/80: current_loss=8.96120 | best_loss=8.63992
Epoch 18/80: current_loss=12.30018 | best_loss=8.63992
Epoch 19/80: current_loss=8.58966 | best_loss=8.58966
Epoch 20/80: current_loss=10.01851 | best_loss=8.58966
Epoch 21/80: current_loss=8.83777 | best_loss=8.58966
Epoch 22/80: current_loss=8.67720 | best_loss=8.58966
Epoch 23/80: current_loss=10.37345 | best_loss=8.58966
Epoch 24/80: current_loss=8.68282 | best_loss=8.58966
Epoch 25/80: current_loss=8.94142 | best_loss=8.58966
Epoch 26/80: current_loss=13.25765 | best_loss=8.58966
Epoch 27/80: current_loss=8.96760 | best_loss=8.58966
Epoch 28/80: current_loss=10.98706 | best_loss=8.58966
Epoch 29/80: current_loss=11.86635 | best_loss=8.58966
Epoch 30/80: current_loss=9.32158 | best_loss=8.58966
Epoch 31/80: current_loss=9.44008 | best_loss=8.58966
Epoch 32/80: current_loss=9.28298 | best_loss=8.58966
Epoch 33/80: current_loss=9.57743 | best_loss=8.58966
Epoch 34/80: current_loss=11.30168 | best_loss=8.58966
Epoch 35/80: current_loss=9.04077 | best_loss=8.58966
Epoch 36/80: current_loss=8.99662 | best_loss=8.58966
Epoch 37/80: current_loss=9.06296 | best_loss=8.58966
Epoch 38/80: current_loss=10.41352 | best_loss=8.58966
Epoch 39/80: current_loss=13.86999 | best_loss=8.58966
Early Stopping at epoch 39
      explained_var=0.01580 | mse_loss=8.34959
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.89140 | best_loss=12.89140
Epoch 1/80: current_loss=9.08093 | best_loss=9.08093
Epoch 2/80: current_loss=8.25105 | best_loss=8.25105
Epoch 3/80: current_loss=8.41935 | best_loss=8.25105
Epoch 4/80: current_loss=8.36011 | best_loss=8.25105
Epoch 5/80: current_loss=8.77107 | best_loss=8.25105
Epoch 6/80: current_loss=9.19502 | best_loss=8.25105
Epoch 7/80: current_loss=9.87187 | best_loss=8.25105
Epoch 8/80: current_loss=8.59921 | best_loss=8.25105
Epoch 9/80: current_loss=8.21569 | best_loss=8.21569
Epoch 10/80: current_loss=8.30580 | best_loss=8.21569
Epoch 11/80: current_loss=8.28613 | best_loss=8.21569
Epoch 12/80: current_loss=8.83061 | best_loss=8.21569
Epoch 13/80: current_loss=8.72191 | best_loss=8.21569
Epoch 14/80: current_loss=10.26774 | best_loss=8.21569
Epoch 15/80: current_loss=8.44215 | best_loss=8.21569
Epoch 16/80: current_loss=8.31520 | best_loss=8.21569
Epoch 17/80: current_loss=8.50078 | best_loss=8.21569
Epoch 18/80: current_loss=8.40127 | best_loss=8.21569
Epoch 19/80: current_loss=10.66980 | best_loss=8.21569
Epoch 20/80: current_loss=8.76110 | best_loss=8.21569
Epoch 21/80: current_loss=8.34249 | best_loss=8.21569
Epoch 22/80: current_loss=8.77479 | best_loss=8.21569
Epoch 23/80: current_loss=8.53728 | best_loss=8.21569
Epoch 24/80: current_loss=8.44114 | best_loss=8.21569
Epoch 25/80: current_loss=8.34610 | best_loss=8.21569
Epoch 26/80: current_loss=11.74951 | best_loss=8.21569
Epoch 27/80: current_loss=9.91270 | best_loss=8.21569
Epoch 28/80: current_loss=8.40236 | best_loss=8.21569
Epoch 29/80: current_loss=8.60327 | best_loss=8.21569
Early Stopping at epoch 29
      explained_var=0.00230 | mse_loss=8.31495
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.53510 | best_loss=9.53510
Epoch 1/80: current_loss=8.30743 | best_loss=8.30743
Epoch 2/80: current_loss=8.52151 | best_loss=8.30743
Epoch 3/80: current_loss=7.81402 | best_loss=7.81402
Epoch 4/80: current_loss=9.75692 | best_loss=7.81402
Epoch 5/80: current_loss=9.87824 | best_loss=7.81402
Epoch 6/80: current_loss=9.69157 | best_loss=7.81402
Epoch 7/80: current_loss=7.80112 | best_loss=7.80112
Epoch 8/80: current_loss=8.51133 | best_loss=7.80112
Epoch 9/80: current_loss=8.21202 | best_loss=7.80112
Epoch 10/80: current_loss=7.84121 | best_loss=7.80112
Epoch 11/80: current_loss=7.86325 | best_loss=7.80112
Epoch 12/80: current_loss=8.94137 | best_loss=7.80112
Epoch 13/80: current_loss=8.11532 | best_loss=7.80112
Epoch 14/80: current_loss=7.77819 | best_loss=7.77819
Epoch 15/80: current_loss=11.81625 | best_loss=7.77819
Epoch 16/80: current_loss=8.56798 | best_loss=7.77819
Epoch 17/80: current_loss=8.10257 | best_loss=7.77819
Epoch 18/80: current_loss=7.81353 | best_loss=7.77819
Epoch 19/80: current_loss=8.65089 | best_loss=7.77819
Epoch 20/80: current_loss=8.21926 | best_loss=7.77819
Epoch 21/80: current_loss=7.83351 | best_loss=7.77819
Epoch 22/80: current_loss=7.68727 | best_loss=7.68727
Epoch 23/80: current_loss=12.56991 | best_loss=7.68727
Epoch 24/80: current_loss=9.54159 | best_loss=7.68727
Epoch 25/80: current_loss=8.05580 | best_loss=7.68727
Epoch 26/80: current_loss=7.82574 | best_loss=7.68727
Epoch 27/80: current_loss=9.25406 | best_loss=7.68727
Epoch 28/80: current_loss=8.16735 | best_loss=7.68727
Epoch 29/80: current_loss=7.96578 | best_loss=7.68727
Epoch 30/80: current_loss=9.09090 | best_loss=7.68727
Epoch 31/80: current_loss=13.20446 | best_loss=7.68727
Epoch 32/80: current_loss=8.12783 | best_loss=7.68727
Epoch 33/80: current_loss=7.82261 | best_loss=7.68727
Epoch 34/80: current_loss=11.34020 | best_loss=7.68727
Epoch 35/80: current_loss=10.64448 | best_loss=7.68727
Epoch 36/80: current_loss=8.87104 | best_loss=7.68727
Epoch 37/80: current_loss=7.70888 | best_loss=7.68727
Epoch 38/80: current_loss=8.30465 | best_loss=7.68727
Epoch 39/80: current_loss=8.24136 | best_loss=7.68727
Epoch 40/80: current_loss=8.05130 | best_loss=7.68727
Epoch 41/80: current_loss=7.74188 | best_loss=7.68727
Epoch 42/80: current_loss=9.72194 | best_loss=7.68727
Early Stopping at epoch 42
      explained_var=-0.00081 | mse_loss=7.86688
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00747| avg_loss=8.04779
----------------------------------------------


----------------------------------------------
Params for Trial 54
{'learning_rate': 0.001, 'weight_decay': 0.008879903304231945, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=41.02633 | best_loss=41.02633
Epoch 1/80: current_loss=24.04551 | best_loss=24.04551
Epoch 2/80: current_loss=17.04255 | best_loss=17.04255
Epoch 3/80: current_loss=12.94821 | best_loss=12.94821
Epoch 4/80: current_loss=10.33481 | best_loss=10.33481
Epoch 5/80: current_loss=8.84243 | best_loss=8.84243
Epoch 6/80: current_loss=8.09843 | best_loss=8.09843
Epoch 7/80: current_loss=7.80372 | best_loss=7.80372
Epoch 8/80: current_loss=7.73709 | best_loss=7.73709
Epoch 9/80: current_loss=7.76409 | best_loss=7.73709
Epoch 10/80: current_loss=7.83497 | best_loss=7.73709
Epoch 11/80: current_loss=7.90432 | best_loss=7.73709
Epoch 12/80: current_loss=7.93772 | best_loss=7.73709
Epoch 13/80: current_loss=7.96605 | best_loss=7.73709
Epoch 14/80: current_loss=8.00339 | best_loss=7.73709
Epoch 15/80: current_loss=8.03781 | best_loss=7.73709
Epoch 16/80: current_loss=8.05271 | best_loss=7.73709
Epoch 17/80: current_loss=8.07635 | best_loss=7.73709
Epoch 18/80: current_loss=8.08585 | best_loss=7.73709
Epoch 19/80: current_loss=8.03210 | best_loss=7.73709
Epoch 20/80: current_loss=8.04052 | best_loss=7.73709
Epoch 21/80: current_loss=8.07429 | best_loss=7.73709
Epoch 22/80: current_loss=8.08378 | best_loss=7.73709
Epoch 23/80: current_loss=8.06144 | best_loss=7.73709
Epoch 24/80: current_loss=8.03210 | best_loss=7.73709
Epoch 25/80: current_loss=8.04017 | best_loss=7.73709
Epoch 26/80: current_loss=8.05683 | best_loss=7.73709
Epoch 27/80: current_loss=8.10918 | best_loss=7.73709
Epoch 28/80: current_loss=8.09454 | best_loss=7.73709
Early Stopping at epoch 28
      explained_var=0.00384 | mse_loss=7.56045
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.41005 | best_loss=8.41005
Epoch 1/80: current_loss=8.41521 | best_loss=8.41005
Epoch 2/80: current_loss=8.39946 | best_loss=8.39946
Epoch 3/80: current_loss=8.37608 | best_loss=8.37608
Epoch 4/80: current_loss=8.35760 | best_loss=8.35760
Epoch 5/80: current_loss=8.34825 | best_loss=8.34825
Epoch 6/80: current_loss=8.35547 | best_loss=8.34825
Epoch 7/80: current_loss=8.36545 | best_loss=8.34825
Epoch 8/80: current_loss=8.33717 | best_loss=8.33717
Epoch 9/80: current_loss=8.34863 | best_loss=8.33717
Epoch 10/80: current_loss=8.34104 | best_loss=8.33717
Epoch 11/80: current_loss=8.35008 | best_loss=8.33717
Epoch 12/80: current_loss=8.37533 | best_loss=8.33717
Epoch 13/80: current_loss=8.35216 | best_loss=8.33717
Epoch 14/80: current_loss=8.35782 | best_loss=8.33717
Epoch 15/80: current_loss=8.43434 | best_loss=8.33717
Epoch 16/80: current_loss=8.35151 | best_loss=8.33717
Epoch 17/80: current_loss=8.34468 | best_loss=8.33717
Epoch 18/80: current_loss=8.34577 | best_loss=8.33717
Epoch 19/80: current_loss=8.41909 | best_loss=8.33717
Epoch 20/80: current_loss=8.41265 | best_loss=8.33717
Epoch 21/80: current_loss=8.37558 | best_loss=8.33717
Epoch 22/80: current_loss=8.38634 | best_loss=8.33717
Epoch 23/80: current_loss=8.34076 | best_loss=8.33717
Epoch 24/80: current_loss=8.35607 | best_loss=8.33717
Epoch 25/80: current_loss=8.35046 | best_loss=8.33717
Epoch 26/80: current_loss=8.37608 | best_loss=8.33717
Epoch 27/80: current_loss=8.41117 | best_loss=8.33717
Epoch 28/80: current_loss=8.37345 | best_loss=8.33717
Early Stopping at epoch 28
      explained_var=-0.00378 | mse_loss=8.19456
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.12070 | best_loss=9.12070
Epoch 1/80: current_loss=9.09967 | best_loss=9.09967
Epoch 2/80: current_loss=8.93012 | best_loss=8.93012
Epoch 3/80: current_loss=8.87816 | best_loss=8.87816
Epoch 4/80: current_loss=8.86905 | best_loss=8.86905
Epoch 5/80: current_loss=8.96384 | best_loss=8.86905
Epoch 6/80: current_loss=9.00324 | best_loss=8.86905
Epoch 7/80: current_loss=8.92253 | best_loss=8.86905
Epoch 8/80: current_loss=8.87080 | best_loss=8.86905
Epoch 9/80: current_loss=9.02210 | best_loss=8.86905
Epoch 10/80: current_loss=8.97451 | best_loss=8.86905
Epoch 11/80: current_loss=8.97387 | best_loss=8.86905
Epoch 12/80: current_loss=8.92483 | best_loss=8.86905
Epoch 13/80: current_loss=8.96293 | best_loss=8.86905
Epoch 14/80: current_loss=8.97007 | best_loss=8.86905
Epoch 15/80: current_loss=8.90465 | best_loss=8.86905
Epoch 16/80: current_loss=8.89023 | best_loss=8.86905
Epoch 17/80: current_loss=8.96770 | best_loss=8.86905
Epoch 18/80: current_loss=9.07523 | best_loss=8.86905
Epoch 19/80: current_loss=8.98764 | best_loss=8.86905
Epoch 20/80: current_loss=9.00462 | best_loss=8.86905
Epoch 21/80: current_loss=8.89088 | best_loss=8.86905
Epoch 22/80: current_loss=8.95986 | best_loss=8.86905
Epoch 23/80: current_loss=8.97712 | best_loss=8.86905
Epoch 24/80: current_loss=8.95221 | best_loss=8.86905
Early Stopping at epoch 24
      explained_var=-0.00572 | mse_loss=8.65192
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.22302 | best_loss=8.22302
Epoch 1/80: current_loss=8.21746 | best_loss=8.21746
Epoch 2/80: current_loss=8.21818 | best_loss=8.21746
Epoch 3/80: current_loss=8.22031 | best_loss=8.21746
Epoch 4/80: current_loss=8.21995 | best_loss=8.21746
Epoch 5/80: current_loss=8.26327 | best_loss=8.21746
Epoch 6/80: current_loss=8.21911 | best_loss=8.21746
Epoch 7/80: current_loss=8.21841 | best_loss=8.21746
Epoch 8/80: current_loss=8.23847 | best_loss=8.21746
Epoch 9/80: current_loss=8.21942 | best_loss=8.21746
Epoch 10/80: current_loss=8.22000 | best_loss=8.21746
Epoch 11/80: current_loss=8.22734 | best_loss=8.21746
Epoch 12/80: current_loss=8.23678 | best_loss=8.21746
Epoch 13/80: current_loss=8.22000 | best_loss=8.21746
Epoch 14/80: current_loss=8.21965 | best_loss=8.21746
Epoch 15/80: current_loss=8.22222 | best_loss=8.21746
Epoch 16/80: current_loss=8.23417 | best_loss=8.21746
Epoch 17/80: current_loss=8.21898 | best_loss=8.21746
Epoch 18/80: current_loss=8.22353 | best_loss=8.21746
Epoch 19/80: current_loss=8.22254 | best_loss=8.21746
Epoch 20/80: current_loss=8.23861 | best_loss=8.21746
Epoch 21/80: current_loss=8.22813 | best_loss=8.21746
Early Stopping at epoch 21
      explained_var=0.00194 | mse_loss=8.31649
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.71121 | best_loss=7.71121
Epoch 1/80: current_loss=7.72416 | best_loss=7.71121
Epoch 2/80: current_loss=7.69904 | best_loss=7.69904
Epoch 3/80: current_loss=7.70140 | best_loss=7.69904
Epoch 4/80: current_loss=7.72099 | best_loss=7.69904
Epoch 5/80: current_loss=7.69865 | best_loss=7.69865
Epoch 6/80: current_loss=7.69483 | best_loss=7.69483
Epoch 7/80: current_loss=7.72230 | best_loss=7.69483
Epoch 8/80: current_loss=7.69307 | best_loss=7.69307
Epoch 9/80: current_loss=7.68969 | best_loss=7.68969
Epoch 10/80: current_loss=7.68989 | best_loss=7.68969
Epoch 11/80: current_loss=7.69383 | best_loss=7.68969
Epoch 12/80: current_loss=7.70386 | best_loss=7.68969
Epoch 13/80: current_loss=7.71040 | best_loss=7.68969
Epoch 14/80: current_loss=7.76157 | best_loss=7.68969
Epoch 15/80: current_loss=7.75790 | best_loss=7.68969
Epoch 16/80: current_loss=7.77375 | best_loss=7.68969
Epoch 17/80: current_loss=7.81257 | best_loss=7.68969
Epoch 18/80: current_loss=7.71852 | best_loss=7.68969
Epoch 19/80: current_loss=7.69554 | best_loss=7.68969
Epoch 20/80: current_loss=7.69807 | best_loss=7.68969
Epoch 21/80: current_loss=7.72615 | best_loss=7.68969
Epoch 22/80: current_loss=7.78991 | best_loss=7.68969
Epoch 23/80: current_loss=7.73043 | best_loss=7.68969
Epoch 24/80: current_loss=7.72623 | best_loss=7.68969
Epoch 25/80: current_loss=7.75721 | best_loss=7.68969
Epoch 26/80: current_loss=7.77828 | best_loss=7.68969
Epoch 27/80: current_loss=7.70868 | best_loss=7.68969
Epoch 28/80: current_loss=7.70096 | best_loss=7.68969
Epoch 29/80: current_loss=7.70745 | best_loss=7.68969
Early Stopping at epoch 29
      explained_var=-0.00131 | mse_loss=7.86635
----------------------------------------------
Average early_stopping_point: 6| avg_exp_var=-0.00101| avg_loss=8.11796
----------------------------------------------


----------------------------------------------
Params for Trial 55
{'learning_rate': 0.1, 'weight_decay': 0.005746814082902043, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.09305 | best_loss=8.09305
Epoch 1/80: current_loss=8.31146 | best_loss=8.09305
Epoch 2/80: current_loss=9.94875 | best_loss=8.09305
Epoch 3/80: current_loss=9.29132 | best_loss=8.09305
Epoch 4/80: current_loss=8.63383 | best_loss=8.09305
Epoch 5/80: current_loss=8.33532 | best_loss=8.09305
Epoch 6/80: current_loss=9.93921 | best_loss=8.09305
Epoch 7/80: current_loss=8.71264 | best_loss=8.09305
Epoch 8/80: current_loss=9.44908 | best_loss=8.09305
Epoch 9/80: current_loss=11.01417 | best_loss=8.09305
Epoch 10/80: current_loss=7.90315 | best_loss=7.90315
Epoch 11/80: current_loss=7.76869 | best_loss=7.76869
Epoch 12/80: current_loss=7.76899 | best_loss=7.76869
Epoch 13/80: current_loss=7.94790 | best_loss=7.76869
Epoch 14/80: current_loss=10.22793 | best_loss=7.76869
Epoch 15/80: current_loss=7.74347 | best_loss=7.74347
Epoch 16/80: current_loss=7.86691 | best_loss=7.74347
Epoch 17/80: current_loss=7.79839 | best_loss=7.74347
Epoch 18/80: current_loss=9.75646 | best_loss=7.74347
Epoch 19/80: current_loss=8.09885 | best_loss=7.74347
Epoch 20/80: current_loss=8.26774 | best_loss=7.74347
Epoch 21/80: current_loss=7.80306 | best_loss=7.74347
Epoch 22/80: current_loss=8.35571 | best_loss=7.74347
Epoch 23/80: current_loss=7.99290 | best_loss=7.74347
Epoch 24/80: current_loss=7.78472 | best_loss=7.74347
Epoch 25/80: current_loss=7.78238 | best_loss=7.74347
Epoch 26/80: current_loss=8.14193 | best_loss=7.74347
Epoch 27/80: current_loss=8.58128 | best_loss=7.74347
Epoch 28/80: current_loss=9.10511 | best_loss=7.74347
Epoch 29/80: current_loss=7.96374 | best_loss=7.74347
Epoch 30/80: current_loss=9.47403 | best_loss=7.74347
Epoch 31/80: current_loss=7.86037 | best_loss=7.74347
Epoch 32/80: current_loss=7.79189 | best_loss=7.74347
Epoch 33/80: current_loss=9.39511 | best_loss=7.74347
Epoch 34/80: current_loss=12.39260 | best_loss=7.74347
Epoch 35/80: current_loss=8.63278 | best_loss=7.74347
Early Stopping at epoch 35
      explained_var=0.00474 | mse_loss=7.55378
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.56227 | best_loss=8.56227
Epoch 1/80: current_loss=8.99314 | best_loss=8.56227
Epoch 2/80: current_loss=8.31164 | best_loss=8.31164
Epoch 3/80: current_loss=8.31948 | best_loss=8.31164
Epoch 4/80: current_loss=10.15380 | best_loss=8.31164
Epoch 5/80: current_loss=8.76311 | best_loss=8.31164
Epoch 6/80: current_loss=8.89208 | best_loss=8.31164
Epoch 7/80: current_loss=9.32015 | best_loss=8.31164
Epoch 8/80: current_loss=8.61513 | best_loss=8.31164
Epoch 9/80: current_loss=8.53970 | best_loss=8.31164
Epoch 10/80: current_loss=9.36204 | best_loss=8.31164
Epoch 11/80: current_loss=9.50721 | best_loss=8.31164
Epoch 12/80: current_loss=8.32562 | best_loss=8.31164
Epoch 13/80: current_loss=8.38584 | best_loss=8.31164
Epoch 14/80: current_loss=8.47248 | best_loss=8.31164
Epoch 15/80: current_loss=8.45658 | best_loss=8.31164
Epoch 16/80: current_loss=9.67861 | best_loss=8.31164
Epoch 17/80: current_loss=8.31738 | best_loss=8.31164
Epoch 18/80: current_loss=8.76466 | best_loss=8.31164
Epoch 19/80: current_loss=8.40938 | best_loss=8.31164
Epoch 20/80: current_loss=9.24247 | best_loss=8.31164
Epoch 21/80: current_loss=11.54261 | best_loss=8.31164
Epoch 22/80: current_loss=8.35466 | best_loss=8.31164
Early Stopping at epoch 22
      explained_var=0.00040 | mse_loss=8.16253
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.45780 | best_loss=10.45780
Epoch 1/80: current_loss=9.26598 | best_loss=9.26598
Epoch 2/80: current_loss=9.71814 | best_loss=9.26598
Epoch 3/80: current_loss=8.81286 | best_loss=8.81286
Epoch 4/80: current_loss=8.70288 | best_loss=8.70288
Epoch 5/80: current_loss=8.87840 | best_loss=8.70288
Epoch 6/80: current_loss=8.75794 | best_loss=8.70288
Epoch 7/80: current_loss=8.88721 | best_loss=8.70288
Epoch 8/80: current_loss=11.33760 | best_loss=8.70288
Epoch 9/80: current_loss=8.71607 | best_loss=8.70288
Epoch 10/80: current_loss=9.52489 | best_loss=8.70288
Epoch 11/80: current_loss=10.13383 | best_loss=8.70288
Epoch 12/80: current_loss=9.46529 | best_loss=8.70288
Epoch 13/80: current_loss=8.83901 | best_loss=8.70288
Epoch 14/80: current_loss=8.77761 | best_loss=8.70288
Epoch 15/80: current_loss=11.66688 | best_loss=8.70288
Epoch 16/80: current_loss=9.41111 | best_loss=8.70288
Epoch 17/80: current_loss=10.11934 | best_loss=8.70288
Epoch 18/80: current_loss=11.06291 | best_loss=8.70288
Epoch 19/80: current_loss=11.86111 | best_loss=8.70288
Epoch 20/80: current_loss=12.32180 | best_loss=8.70288
Epoch 21/80: current_loss=8.83910 | best_loss=8.70288
Epoch 22/80: current_loss=9.03556 | best_loss=8.70288
Epoch 23/80: current_loss=9.13353 | best_loss=8.70288
Epoch 24/80: current_loss=8.73265 | best_loss=8.70288
Early Stopping at epoch 24
      explained_var=0.00552 | mse_loss=8.46104
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.44996 | best_loss=8.44996
Epoch 1/80: current_loss=8.62557 | best_loss=8.44996
Epoch 2/80: current_loss=8.36991 | best_loss=8.36991
Epoch 3/80: current_loss=8.47997 | best_loss=8.36991
Epoch 4/80: current_loss=8.27431 | best_loss=8.27431
Epoch 5/80: current_loss=8.59925 | best_loss=8.27431
Epoch 6/80: current_loss=8.22887 | best_loss=8.22887
Epoch 7/80: current_loss=9.46568 | best_loss=8.22887
Epoch 8/80: current_loss=10.59828 | best_loss=8.22887
Epoch 9/80: current_loss=8.56551 | best_loss=8.22887
Epoch 10/80: current_loss=8.69082 | best_loss=8.22887
Epoch 11/80: current_loss=8.25254 | best_loss=8.22887
Epoch 12/80: current_loss=8.28921 | best_loss=8.22887
Epoch 13/80: current_loss=8.41233 | best_loss=8.22887
Epoch 14/80: current_loss=8.56800 | best_loss=8.22887
Epoch 15/80: current_loss=8.73139 | best_loss=8.22887
Epoch 16/80: current_loss=8.30920 | best_loss=8.22887
Epoch 17/80: current_loss=8.55448 | best_loss=8.22887
Epoch 18/80: current_loss=8.49288 | best_loss=8.22887
Epoch 19/80: current_loss=11.93450 | best_loss=8.22887
Epoch 20/80: current_loss=8.81762 | best_loss=8.22887
Epoch 21/80: current_loss=9.36753 | best_loss=8.22887
Epoch 22/80: current_loss=9.66391 | best_loss=8.22887
Epoch 23/80: current_loss=14.99940 | best_loss=8.22887
Epoch 24/80: current_loss=10.34626 | best_loss=8.22887
Epoch 25/80: current_loss=8.59528 | best_loss=8.22887
Epoch 26/80: current_loss=8.26682 | best_loss=8.22887
Early Stopping at epoch 26
      explained_var=0.00027 | mse_loss=8.33309
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.00269 | best_loss=8.00269
Epoch 1/80: current_loss=7.97097 | best_loss=7.97097
Epoch 2/80: current_loss=7.67569 | best_loss=7.67569
Epoch 3/80: current_loss=7.72294 | best_loss=7.67569
Epoch 4/80: current_loss=7.76247 | best_loss=7.67569
Epoch 5/80: current_loss=8.07184 | best_loss=7.67569
Epoch 6/80: current_loss=7.69840 | best_loss=7.67569
Epoch 7/80: current_loss=8.73222 | best_loss=7.67569
Epoch 8/80: current_loss=8.83476 | best_loss=7.67569
Epoch 9/80: current_loss=8.58272 | best_loss=7.67569
Epoch 10/80: current_loss=9.81604 | best_loss=7.67569
Epoch 11/80: current_loss=7.94924 | best_loss=7.67569
Epoch 12/80: current_loss=7.73623 | best_loss=7.67569
Epoch 13/80: current_loss=7.72412 | best_loss=7.67569
Epoch 14/80: current_loss=8.32641 | best_loss=7.67569
Epoch 15/80: current_loss=7.69451 | best_loss=7.67569
Epoch 16/80: current_loss=7.84860 | best_loss=7.67569
Epoch 17/80: current_loss=10.20659 | best_loss=7.67569
Epoch 18/80: current_loss=10.27512 | best_loss=7.67569
Epoch 19/80: current_loss=7.67416 | best_loss=7.67416
Epoch 20/80: current_loss=8.36126 | best_loss=7.67416
Epoch 21/80: current_loss=9.41624 | best_loss=7.67416
Epoch 22/80: current_loss=8.23105 | best_loss=7.67416
Epoch 23/80: current_loss=7.66659 | best_loss=7.66659
Epoch 24/80: current_loss=8.18721 | best_loss=7.66659
Epoch 25/80: current_loss=8.49952 | best_loss=7.66659
Epoch 26/80: current_loss=7.90306 | best_loss=7.66659
Epoch 27/80: current_loss=7.76107 | best_loss=7.66659
Epoch 28/80: current_loss=7.78552 | best_loss=7.66659
Epoch 29/80: current_loss=7.84801 | best_loss=7.66659
Epoch 30/80: current_loss=9.75467 | best_loss=7.66659
Epoch 31/80: current_loss=7.68543 | best_loss=7.66659
Epoch 32/80: current_loss=8.12273 | best_loss=7.66659
Epoch 33/80: current_loss=8.18973 | best_loss=7.66659
Epoch 34/80: current_loss=8.70348 | best_loss=7.66659
Epoch 35/80: current_loss=7.79071 | best_loss=7.66659
Epoch 36/80: current_loss=7.68106 | best_loss=7.66659
Epoch 37/80: current_loss=7.88076 | best_loss=7.66659
Epoch 38/80: current_loss=7.89912 | best_loss=7.66659
Epoch 39/80: current_loss=8.64820 | best_loss=7.66659
Epoch 40/80: current_loss=7.71186 | best_loss=7.66659
Epoch 41/80: current_loss=10.43148 | best_loss=7.66659
Epoch 42/80: current_loss=7.71975 | best_loss=7.66659
Epoch 43/80: current_loss=8.98910 | best_loss=7.66659
Early Stopping at epoch 43
      explained_var=0.00150 | mse_loss=7.84791
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.00249| avg_loss=8.07167
----------------------------------------------


----------------------------------------------
Params for Trial 56
{'learning_rate': 0.1, 'weight_decay': 0.008632600292830538, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.20943 | best_loss=9.20943
Epoch 1/80: current_loss=9.75386 | best_loss=9.20943
Epoch 2/80: current_loss=8.79954 | best_loss=8.79954
Epoch 3/80: current_loss=8.60497 | best_loss=8.60497
Epoch 4/80: current_loss=9.44684 | best_loss=8.60497
Epoch 5/80: current_loss=8.46972 | best_loss=8.46972
Epoch 6/80: current_loss=9.64136 | best_loss=8.46972
Epoch 7/80: current_loss=8.18065 | best_loss=8.18065
Epoch 8/80: current_loss=8.79844 | best_loss=8.18065
Epoch 9/80: current_loss=8.51894 | best_loss=8.18065
Epoch 10/80: current_loss=18.31133 | best_loss=8.18065
Epoch 11/80: current_loss=13.28120 | best_loss=8.18065
Epoch 12/80: current_loss=22.73807 | best_loss=8.18065
Epoch 13/80: current_loss=13.05878 | best_loss=8.18065
Epoch 14/80: current_loss=14.45695 | best_loss=8.18065
Epoch 15/80: current_loss=11.57041 | best_loss=8.18065
Epoch 16/80: current_loss=11.52765 | best_loss=8.18065
Epoch 17/80: current_loss=8.07345 | best_loss=8.07345
Epoch 18/80: current_loss=9.17301 | best_loss=8.07345
Epoch 19/80: current_loss=7.87128 | best_loss=7.87128
Epoch 20/80: current_loss=8.39054 | best_loss=7.87128
Epoch 21/80: current_loss=8.20091 | best_loss=7.87128
Epoch 22/80: current_loss=11.10645 | best_loss=7.87128
Epoch 23/80: current_loss=10.46646 | best_loss=7.87128
Epoch 24/80: current_loss=8.93668 | best_loss=7.87128
Epoch 25/80: current_loss=8.90206 | best_loss=7.87128
Epoch 26/80: current_loss=13.58029 | best_loss=7.87128
Epoch 27/80: current_loss=7.82929 | best_loss=7.82929
Epoch 28/80: current_loss=8.27079 | best_loss=7.82929
Epoch 29/80: current_loss=11.65443 | best_loss=7.82929
Epoch 30/80: current_loss=13.64123 | best_loss=7.82929
Epoch 31/80: current_loss=7.76063 | best_loss=7.76063
Epoch 32/80: current_loss=13.85452 | best_loss=7.76063
Epoch 33/80: current_loss=7.75392 | best_loss=7.75392
Epoch 34/80: current_loss=13.23145 | best_loss=7.75392
Epoch 35/80: current_loss=10.34637 | best_loss=7.75392
Epoch 36/80: current_loss=8.07422 | best_loss=7.75392
Epoch 37/80: current_loss=8.14616 | best_loss=7.75392
Epoch 38/80: current_loss=9.25569 | best_loss=7.75392
Epoch 39/80: current_loss=9.67687 | best_loss=7.75392
Epoch 40/80: current_loss=8.19697 | best_loss=7.75392
Epoch 41/80: current_loss=7.80737 | best_loss=7.75392
Epoch 42/80: current_loss=8.50577 | best_loss=7.75392
Epoch 43/80: current_loss=7.97075 | best_loss=7.75392
Epoch 44/80: current_loss=9.29191 | best_loss=7.75392
Epoch 45/80: current_loss=13.28971 | best_loss=7.75392
Epoch 46/80: current_loss=8.98480 | best_loss=7.75392
Epoch 47/80: current_loss=7.84516 | best_loss=7.75392
Epoch 48/80: current_loss=8.23760 | best_loss=7.75392
Epoch 49/80: current_loss=7.76902 | best_loss=7.75392
Epoch 50/80: current_loss=10.81544 | best_loss=7.75392
Epoch 51/80: current_loss=61.28747 | best_loss=7.75392
Epoch 52/80: current_loss=19.21666 | best_loss=7.75392
Epoch 53/80: current_loss=9.15035 | best_loss=7.75392
Early Stopping at epoch 53
      explained_var=0.00228 | mse_loss=7.57669
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.88517 | best_loss=8.88517
Epoch 1/80: current_loss=9.00842 | best_loss=8.88517
Epoch 2/80: current_loss=11.27810 | best_loss=8.88517
Epoch 3/80: current_loss=8.84758 | best_loss=8.84758
Epoch 4/80: current_loss=8.57733 | best_loss=8.57733
Epoch 5/80: current_loss=8.32572 | best_loss=8.32572
Epoch 6/80: current_loss=11.30152 | best_loss=8.32572
Epoch 7/80: current_loss=10.08825 | best_loss=8.32572
Epoch 8/80: current_loss=8.72394 | best_loss=8.32572
Epoch 9/80: current_loss=10.82140 | best_loss=8.32572
Epoch 10/80: current_loss=8.31866 | best_loss=8.31866
Epoch 11/80: current_loss=16.16666 | best_loss=8.31866
Epoch 12/80: current_loss=9.72642 | best_loss=8.31866
Epoch 13/80: current_loss=8.87217 | best_loss=8.31866
Epoch 14/80: current_loss=9.23099 | best_loss=8.31866
Epoch 15/80: current_loss=8.74731 | best_loss=8.31866
Epoch 16/80: current_loss=9.01736 | best_loss=8.31866
Epoch 17/80: current_loss=8.81368 | best_loss=8.31866
Epoch 18/80: current_loss=9.80567 | best_loss=8.31866
Epoch 19/80: current_loss=8.45073 | best_loss=8.31866
Epoch 20/80: current_loss=8.80865 | best_loss=8.31866
Epoch 21/80: current_loss=8.64533 | best_loss=8.31866
Epoch 22/80: current_loss=9.70406 | best_loss=8.31866
Epoch 23/80: current_loss=8.33728 | best_loss=8.31866
Epoch 24/80: current_loss=15.67307 | best_loss=8.31866
Epoch 25/80: current_loss=12.15343 | best_loss=8.31866
Epoch 26/80: current_loss=9.73761 | best_loss=8.31866
Epoch 27/80: current_loss=11.79326 | best_loss=8.31866
Epoch 28/80: current_loss=12.08567 | best_loss=8.31866
Epoch 29/80: current_loss=9.18149 | best_loss=8.31866
Epoch 30/80: current_loss=8.36065 | best_loss=8.31866
Early Stopping at epoch 30
      explained_var=0.00421 | mse_loss=8.13103
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.01682 | best_loss=9.01682
Epoch 1/80: current_loss=8.57984 | best_loss=8.57984
Epoch 2/80: current_loss=11.63315 | best_loss=8.57984
Epoch 3/80: current_loss=13.93116 | best_loss=8.57984
Epoch 4/80: current_loss=14.89189 | best_loss=8.57984
Epoch 5/80: current_loss=10.78708 | best_loss=8.57984
Epoch 6/80: current_loss=9.60196 | best_loss=8.57984
Epoch 7/80: current_loss=12.75649 | best_loss=8.57984
Epoch 8/80: current_loss=9.22559 | best_loss=8.57984
Epoch 9/80: current_loss=9.85178 | best_loss=8.57984
Epoch 10/80: current_loss=15.17609 | best_loss=8.57984
Epoch 11/80: current_loss=9.70732 | best_loss=8.57984
Epoch 12/80: current_loss=8.67523 | best_loss=8.57984
Epoch 13/80: current_loss=9.81817 | best_loss=8.57984
Epoch 14/80: current_loss=12.92221 | best_loss=8.57984
Epoch 15/80: current_loss=11.50440 | best_loss=8.57984
Epoch 16/80: current_loss=11.28342 | best_loss=8.57984
Epoch 17/80: current_loss=9.34027 | best_loss=8.57984
Epoch 18/80: current_loss=8.43998 | best_loss=8.43998
Epoch 19/80: current_loss=9.83169 | best_loss=8.43998
Epoch 20/80: current_loss=9.28059 | best_loss=8.43998
Epoch 21/80: current_loss=11.52572 | best_loss=8.43998
Epoch 22/80: current_loss=11.88339 | best_loss=8.43998
Epoch 23/80: current_loss=10.35161 | best_loss=8.43998
Epoch 24/80: current_loss=9.25731 | best_loss=8.43998
Epoch 25/80: current_loss=9.23550 | best_loss=8.43998
Epoch 26/80: current_loss=9.97678 | best_loss=8.43998
Epoch 27/80: current_loss=11.67196 | best_loss=8.43998
Epoch 28/80: current_loss=8.96980 | best_loss=8.43998
Epoch 29/80: current_loss=22.38931 | best_loss=8.43998
Epoch 30/80: current_loss=8.74852 | best_loss=8.43998
Epoch 31/80: current_loss=8.67878 | best_loss=8.43998
Epoch 32/80: current_loss=10.67290 | best_loss=8.43998
Epoch 33/80: current_loss=14.09013 | best_loss=8.43998
Epoch 34/80: current_loss=15.75993 | best_loss=8.43998
Epoch 35/80: current_loss=9.33742 | best_loss=8.43998
Epoch 36/80: current_loss=10.66607 | best_loss=8.43998
Epoch 37/80: current_loss=9.27609 | best_loss=8.43998
Epoch 38/80: current_loss=9.88432 | best_loss=8.43998
Early Stopping at epoch 38
      explained_var=0.06512 | mse_loss=8.20544
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=25.84978 | best_loss=25.84978
Epoch 1/80: current_loss=10.91964 | best_loss=10.91964
Epoch 2/80: current_loss=9.31547 | best_loss=9.31547
Epoch 3/80: current_loss=11.93634 | best_loss=9.31547
Epoch 4/80: current_loss=11.60870 | best_loss=9.31547
Epoch 5/80: current_loss=9.57389 | best_loss=9.31547
Epoch 6/80: current_loss=7.92902 | best_loss=7.92902
Epoch 7/80: current_loss=8.27614 | best_loss=7.92902
Epoch 8/80: current_loss=9.71786 | best_loss=7.92902
Epoch 9/80: current_loss=13.79687 | best_loss=7.92902
Epoch 10/80: current_loss=9.85147 | best_loss=7.92902
Epoch 11/80: current_loss=11.19089 | best_loss=7.92902
Epoch 12/80: current_loss=11.14471 | best_loss=7.92902
Epoch 13/80: current_loss=10.60939 | best_loss=7.92902
Epoch 14/80: current_loss=8.87593 | best_loss=7.92902
Epoch 15/80: current_loss=9.64297 | best_loss=7.92902
Epoch 16/80: current_loss=9.69851 | best_loss=7.92902
Epoch 17/80: current_loss=8.55004 | best_loss=7.92902
Epoch 18/80: current_loss=8.11302 | best_loss=7.92902
Epoch 19/80: current_loss=9.34940 | best_loss=7.92902
Epoch 20/80: current_loss=8.37070 | best_loss=7.92902
Epoch 21/80: current_loss=9.81739 | best_loss=7.92902
Epoch 22/80: current_loss=8.80469 | best_loss=7.92902
Epoch 23/80: current_loss=8.26279 | best_loss=7.92902
Epoch 24/80: current_loss=11.70560 | best_loss=7.92902
Epoch 25/80: current_loss=8.66399 | best_loss=7.92902
Epoch 26/80: current_loss=9.16328 | best_loss=7.92902
Early Stopping at epoch 26
      explained_var=0.04774 | mse_loss=7.93632
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.59921 | best_loss=10.59921
Epoch 1/80: current_loss=8.25302 | best_loss=8.25302
Epoch 2/80: current_loss=7.96749 | best_loss=7.96749
Epoch 3/80: current_loss=8.22856 | best_loss=7.96749
Epoch 4/80: current_loss=12.45917 | best_loss=7.96749
Epoch 5/80: current_loss=10.21284 | best_loss=7.96749
Epoch 6/80: current_loss=9.50980 | best_loss=7.96749
Epoch 7/80: current_loss=8.88564 | best_loss=7.96749
Epoch 8/80: current_loss=10.10292 | best_loss=7.96749
Epoch 9/80: current_loss=12.51335 | best_loss=7.96749
Epoch 10/80: current_loss=11.33287 | best_loss=7.96749
Epoch 11/80: current_loss=15.72367 | best_loss=7.96749
Epoch 12/80: current_loss=7.69551 | best_loss=7.69551
Epoch 13/80: current_loss=13.54617 | best_loss=7.69551
Epoch 14/80: current_loss=15.32587 | best_loss=7.69551
Epoch 15/80: current_loss=14.51845 | best_loss=7.69551
Epoch 16/80: current_loss=9.56113 | best_loss=7.69551
Epoch 17/80: current_loss=7.86512 | best_loss=7.69551
Epoch 18/80: current_loss=7.75782 | best_loss=7.69551
Epoch 19/80: current_loss=10.15914 | best_loss=7.69551
Epoch 20/80: current_loss=7.83824 | best_loss=7.69551
Epoch 21/80: current_loss=8.05111 | best_loss=7.69551
Epoch 22/80: current_loss=7.82542 | best_loss=7.69551
Epoch 23/80: current_loss=8.10919 | best_loss=7.69551
Epoch 24/80: current_loss=8.58652 | best_loss=7.69551
Epoch 25/80: current_loss=12.27795 | best_loss=7.69551
Epoch 26/80: current_loss=7.65893 | best_loss=7.65893
Epoch 27/80: current_loss=8.03707 | best_loss=7.65893
Epoch 28/80: current_loss=8.59667 | best_loss=7.65893
Epoch 29/80: current_loss=7.67554 | best_loss=7.65893
Epoch 30/80: current_loss=9.16652 | best_loss=7.65893
Epoch 31/80: current_loss=8.33785 | best_loss=7.65893
Epoch 32/80: current_loss=8.30353 | best_loss=7.65893
Epoch 33/80: current_loss=8.07269 | best_loss=7.65893
Epoch 34/80: current_loss=9.19407 | best_loss=7.65893
Epoch 35/80: current_loss=8.94674 | best_loss=7.65893
Epoch 36/80: current_loss=9.20748 | best_loss=7.65893
Epoch 37/80: current_loss=13.13328 | best_loss=7.65893
Epoch 38/80: current_loss=10.02358 | best_loss=7.65893
Epoch 39/80: current_loss=14.87545 | best_loss=7.65893
Epoch 40/80: current_loss=16.70923 | best_loss=7.65893
Epoch 41/80: current_loss=8.83872 | best_loss=7.65893
Epoch 42/80: current_loss=9.15210 | best_loss=7.65893
Epoch 43/80: current_loss=13.28952 | best_loss=7.65893
Epoch 44/80: current_loss=10.70382 | best_loss=7.65893
Epoch 45/80: current_loss=12.92133 | best_loss=7.65893
Epoch 46/80: current_loss=7.65776 | best_loss=7.65776
Epoch 47/80: current_loss=7.84554 | best_loss=7.65776
Epoch 48/80: current_loss=7.67798 | best_loss=7.65776
Epoch 49/80: current_loss=7.76795 | best_loss=7.65776
Epoch 50/80: current_loss=8.09487 | best_loss=7.65776
Epoch 51/80: current_loss=8.04004 | best_loss=7.65776
Epoch 52/80: current_loss=12.29724 | best_loss=7.65776
Epoch 53/80: current_loss=12.73470 | best_loss=7.65776
Epoch 54/80: current_loss=10.26588 | best_loss=7.65776
Epoch 55/80: current_loss=14.82244 | best_loss=7.65776
Epoch 56/80: current_loss=8.56634 | best_loss=7.65776
Epoch 57/80: current_loss=8.82434 | best_loss=7.65776
Epoch 58/80: current_loss=7.69289 | best_loss=7.65776
Epoch 59/80: current_loss=12.15483 | best_loss=7.65776
Epoch 60/80: current_loss=9.06516 | best_loss=7.65776
Epoch 61/80: current_loss=8.22778 | best_loss=7.65776
Epoch 62/80: current_loss=11.43725 | best_loss=7.65776
Epoch 63/80: current_loss=8.09602 | best_loss=7.65776
Epoch 64/80: current_loss=8.12886 | best_loss=7.65776
Epoch 65/80: current_loss=12.35780 | best_loss=7.65776
Epoch 66/80: current_loss=7.91878 | best_loss=7.65776
Early Stopping at epoch 66
      explained_var=0.00294 | mse_loss=7.83363
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=0.02446| avg_loss=7.93662
----------------------------------------------


----------------------------------------------
Params for Trial 57
{'learning_rate': 0.1, 'weight_decay': 0.009165328704009157, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.91705 | best_loss=7.91705
Epoch 1/80: current_loss=8.47016 | best_loss=7.91705
Epoch 2/80: current_loss=7.86779 | best_loss=7.86779
Epoch 3/80: current_loss=10.24618 | best_loss=7.86779
Epoch 4/80: current_loss=8.26960 | best_loss=7.86779
Epoch 5/80: current_loss=8.84395 | best_loss=7.86779
Epoch 6/80: current_loss=12.54027 | best_loss=7.86779
Epoch 7/80: current_loss=8.68345 | best_loss=7.86779
Epoch 8/80: current_loss=8.03412 | best_loss=7.86779
Epoch 9/80: current_loss=10.69176 | best_loss=7.86779
Epoch 10/80: current_loss=19.24051 | best_loss=7.86779
Epoch 11/80: current_loss=7.85593 | best_loss=7.85593
Epoch 12/80: current_loss=8.77987 | best_loss=7.85593
Epoch 13/80: current_loss=25.04874 | best_loss=7.85593
Epoch 14/80: current_loss=8.57291 | best_loss=7.85593
Epoch 15/80: current_loss=10.58759 | best_loss=7.85593
Epoch 16/80: current_loss=7.84165 | best_loss=7.84165
Epoch 17/80: current_loss=12.97791 | best_loss=7.84165
Epoch 18/80: current_loss=7.84062 | best_loss=7.84062
Epoch 19/80: current_loss=8.07457 | best_loss=7.84062
Epoch 20/80: current_loss=7.82987 | best_loss=7.82987
Epoch 21/80: current_loss=7.91187 | best_loss=7.82987
Epoch 22/80: current_loss=7.93117 | best_loss=7.82987
Epoch 23/80: current_loss=7.81122 | best_loss=7.81122
Epoch 24/80: current_loss=13.00886 | best_loss=7.81122
Epoch 25/80: current_loss=7.78226 | best_loss=7.78226
Epoch 26/80: current_loss=11.11946 | best_loss=7.78226
Epoch 27/80: current_loss=15.04548 | best_loss=7.78226
Epoch 28/80: current_loss=11.52569 | best_loss=7.78226
Epoch 29/80: current_loss=8.35572 | best_loss=7.78226
Epoch 30/80: current_loss=7.87646 | best_loss=7.78226
Epoch 31/80: current_loss=7.85806 | best_loss=7.78226
Epoch 32/80: current_loss=7.80771 | best_loss=7.78226
Epoch 33/80: current_loss=7.89482 | best_loss=7.78226
Epoch 34/80: current_loss=7.69069 | best_loss=7.69069
Epoch 35/80: current_loss=8.19989 | best_loss=7.69069
Epoch 36/80: current_loss=8.37573 | best_loss=7.69069
Epoch 37/80: current_loss=9.03302 | best_loss=7.69069
Epoch 38/80: current_loss=8.15457 | best_loss=7.69069
Epoch 39/80: current_loss=8.07510 | best_loss=7.69069
Epoch 40/80: current_loss=11.19096 | best_loss=7.69069
Epoch 41/80: current_loss=7.89078 | best_loss=7.69069
Epoch 42/80: current_loss=7.98437 | best_loss=7.69069
Epoch 43/80: current_loss=8.12322 | best_loss=7.69069
Epoch 44/80: current_loss=7.81218 | best_loss=7.69069
Epoch 45/80: current_loss=24.55564 | best_loss=7.69069
Epoch 46/80: current_loss=20.51226 | best_loss=7.69069
Epoch 47/80: current_loss=8.28293 | best_loss=7.69069
Epoch 48/80: current_loss=14.31494 | best_loss=7.69069
Epoch 49/80: current_loss=9.71240 | best_loss=7.69069
Epoch 50/80: current_loss=7.85818 | best_loss=7.69069
Epoch 51/80: current_loss=10.56405 | best_loss=7.69069
Epoch 52/80: current_loss=7.82836 | best_loss=7.69069
Epoch 53/80: current_loss=8.63639 | best_loss=7.69069
Epoch 54/80: current_loss=8.93405 | best_loss=7.69069
Early Stopping at epoch 54
      explained_var=0.00923 | mse_loss=7.52390
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.50360 | best_loss=12.50360
Epoch 1/80: current_loss=9.70497 | best_loss=9.70497
Epoch 2/80: current_loss=9.57419 | best_loss=9.57419
Epoch 3/80: current_loss=9.01791 | best_loss=9.01791
Epoch 4/80: current_loss=9.27346 | best_loss=9.01791
Epoch 5/80: current_loss=8.65258 | best_loss=8.65258
Epoch 6/80: current_loss=15.27552 | best_loss=8.65258
Epoch 7/80: current_loss=8.48967 | best_loss=8.48967
Epoch 8/80: current_loss=9.34137 | best_loss=8.48967
Epoch 9/80: current_loss=10.30478 | best_loss=8.48967
Epoch 10/80: current_loss=8.64786 | best_loss=8.48967
Epoch 11/80: current_loss=11.04901 | best_loss=8.48967
Epoch 12/80: current_loss=16.49083 | best_loss=8.48967
Epoch 13/80: current_loss=10.26440 | best_loss=8.48967
Epoch 14/80: current_loss=11.52238 | best_loss=8.48967
Epoch 15/80: current_loss=11.79009 | best_loss=8.48967
Epoch 16/80: current_loss=12.01576 | best_loss=8.48967
Epoch 17/80: current_loss=9.03301 | best_loss=8.48967
Epoch 18/80: current_loss=9.21558 | best_loss=8.48967
Epoch 19/80: current_loss=11.99856 | best_loss=8.48967
Epoch 20/80: current_loss=8.23449 | best_loss=8.23449
Epoch 21/80: current_loss=8.50618 | best_loss=8.23449
Epoch 22/80: current_loss=8.62135 | best_loss=8.23449
Epoch 23/80: current_loss=11.71958 | best_loss=8.23449
Epoch 24/80: current_loss=14.77682 | best_loss=8.23449
Epoch 25/80: current_loss=9.91284 | best_loss=8.23449
Epoch 26/80: current_loss=10.90594 | best_loss=8.23449
Epoch 27/80: current_loss=11.19558 | best_loss=8.23449
Epoch 28/80: current_loss=10.00817 | best_loss=8.23449
Epoch 29/80: current_loss=13.86687 | best_loss=8.23449
Epoch 30/80: current_loss=9.70623 | best_loss=8.23449
Epoch 31/80: current_loss=9.06996 | best_loss=8.23449
Epoch 32/80: current_loss=11.32471 | best_loss=8.23449
Epoch 33/80: current_loss=10.13824 | best_loss=8.23449
Epoch 34/80: current_loss=9.61898 | best_loss=8.23449
Epoch 35/80: current_loss=9.13300 | best_loss=8.23449
Epoch 36/80: current_loss=11.08591 | best_loss=8.23449
Epoch 37/80: current_loss=9.39225 | best_loss=8.23449
Epoch 38/80: current_loss=8.45621 | best_loss=8.23449
Epoch 39/80: current_loss=8.98154 | best_loss=8.23449
Epoch 40/80: current_loss=8.68352 | best_loss=8.23449
Early Stopping at epoch 40
      explained_var=0.00791 | mse_loss=8.14467
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.86209 | best_loss=8.86209
Epoch 1/80: current_loss=10.27926 | best_loss=8.86209
Epoch 2/80: current_loss=9.30991 | best_loss=8.86209
Epoch 3/80: current_loss=11.41825 | best_loss=8.86209
Epoch 4/80: current_loss=16.28195 | best_loss=8.86209
Epoch 5/80: current_loss=16.70329 | best_loss=8.86209
Epoch 6/80: current_loss=10.98188 | best_loss=8.86209
Epoch 7/80: current_loss=11.48556 | best_loss=8.86209
Epoch 8/80: current_loss=8.66752 | best_loss=8.66752
Epoch 9/80: current_loss=16.55328 | best_loss=8.66752
Epoch 10/80: current_loss=9.49268 | best_loss=8.66752
Epoch 11/80: current_loss=9.00426 | best_loss=8.66752
Epoch 12/80: current_loss=9.07623 | best_loss=8.66752
Epoch 13/80: current_loss=10.26969 | best_loss=8.66752
Epoch 14/80: current_loss=10.32162 | best_loss=8.66752
Epoch 15/80: current_loss=9.34138 | best_loss=8.66752
Epoch 16/80: current_loss=8.94630 | best_loss=8.66752
Epoch 17/80: current_loss=9.88690 | best_loss=8.66752
Epoch 18/80: current_loss=8.73036 | best_loss=8.66752
Epoch 19/80: current_loss=8.84857 | best_loss=8.66752
Epoch 20/80: current_loss=8.73182 | best_loss=8.66752
Epoch 21/80: current_loss=8.77709 | best_loss=8.66752
Epoch 22/80: current_loss=9.73089 | best_loss=8.66752
Epoch 23/80: current_loss=8.77636 | best_loss=8.66752
Epoch 24/80: current_loss=12.86863 | best_loss=8.66752
Epoch 25/80: current_loss=9.79606 | best_loss=8.66752
Epoch 26/80: current_loss=11.54697 | best_loss=8.66752
Epoch 27/80: current_loss=8.74365 | best_loss=8.66752
Epoch 28/80: current_loss=8.74100 | best_loss=8.66752
Early Stopping at epoch 28
      explained_var=0.00627 | mse_loss=8.43536
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=22.01282 | best_loss=22.01282
Epoch 1/80: current_loss=9.09838 | best_loss=9.09838
Epoch 2/80: current_loss=8.22755 | best_loss=8.22755
Epoch 3/80: current_loss=7.84571 | best_loss=7.84571
Epoch 4/80: current_loss=9.20221 | best_loss=7.84571
Epoch 5/80: current_loss=12.45745 | best_loss=7.84571
Epoch 6/80: current_loss=11.04328 | best_loss=7.84571
Epoch 7/80: current_loss=8.31826 | best_loss=7.84571
Epoch 8/80: current_loss=8.44123 | best_loss=7.84571
Epoch 9/80: current_loss=8.08077 | best_loss=7.84571
Epoch 10/80: current_loss=10.64842 | best_loss=7.84571
Epoch 11/80: current_loss=9.32561 | best_loss=7.84571
Epoch 12/80: current_loss=9.28662 | best_loss=7.84571
Epoch 13/80: current_loss=8.21912 | best_loss=7.84571
Epoch 14/80: current_loss=10.50054 | best_loss=7.84571
Epoch 15/80: current_loss=11.85675 | best_loss=7.84571
Epoch 16/80: current_loss=8.34804 | best_loss=7.84571
Epoch 17/80: current_loss=10.45482 | best_loss=7.84571
Epoch 18/80: current_loss=8.97082 | best_loss=7.84571
Epoch 19/80: current_loss=9.37739 | best_loss=7.84571
Epoch 20/80: current_loss=8.85778 | best_loss=7.84571
Epoch 21/80: current_loss=9.45064 | best_loss=7.84571
Epoch 22/80: current_loss=11.89920 | best_loss=7.84571
Epoch 23/80: current_loss=9.09311 | best_loss=7.84571
Early Stopping at epoch 23
      explained_var=0.05269 | mse_loss=7.89798
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.73068 | best_loss=10.73068
Epoch 1/80: current_loss=8.34999 | best_loss=8.34999
Epoch 2/80: current_loss=11.07980 | best_loss=8.34999
Epoch 3/80: current_loss=9.93834 | best_loss=8.34999
Epoch 4/80: current_loss=12.82720 | best_loss=8.34999
Epoch 5/80: current_loss=9.30437 | best_loss=8.34999
Epoch 6/80: current_loss=9.21297 | best_loss=8.34999
Epoch 7/80: current_loss=8.45993 | best_loss=8.34999
Epoch 8/80: current_loss=9.69688 | best_loss=8.34999
Epoch 9/80: current_loss=10.92550 | best_loss=8.34999
Epoch 10/80: current_loss=14.54044 | best_loss=8.34999
Epoch 11/80: current_loss=9.52847 | best_loss=8.34999
Epoch 12/80: current_loss=12.87613 | best_loss=8.34999
Epoch 13/80: current_loss=10.04916 | best_loss=8.34999
Epoch 14/80: current_loss=17.32473 | best_loss=8.34999
Epoch 15/80: current_loss=8.02465 | best_loss=8.02465
Epoch 16/80: current_loss=11.60822 | best_loss=8.02465
Epoch 17/80: current_loss=8.38098 | best_loss=8.02465
Epoch 18/80: current_loss=11.03557 | best_loss=8.02465
Epoch 19/80: current_loss=8.26276 | best_loss=8.02465
Epoch 20/80: current_loss=12.10962 | best_loss=8.02465
Epoch 21/80: current_loss=18.06324 | best_loss=8.02465
Epoch 22/80: current_loss=8.18450 | best_loss=8.02465
Epoch 23/80: current_loss=8.65487 | best_loss=8.02465
Epoch 24/80: current_loss=8.22298 | best_loss=8.02465
Epoch 25/80: current_loss=15.23557 | best_loss=8.02465
Epoch 26/80: current_loss=14.82880 | best_loss=8.02465
Epoch 27/80: current_loss=8.60972 | best_loss=8.02465
Epoch 28/80: current_loss=8.39468 | best_loss=8.02465
Epoch 29/80: current_loss=9.25748 | best_loss=8.02465
Epoch 30/80: current_loss=8.26667 | best_loss=8.02465
Epoch 31/80: current_loss=9.68224 | best_loss=8.02465
Epoch 32/80: current_loss=9.72800 | best_loss=8.02465
Epoch 33/80: current_loss=8.92789 | best_loss=8.02465
Epoch 34/80: current_loss=9.73411 | best_loss=8.02465
Epoch 35/80: current_loss=11.53811 | best_loss=8.02465
Early Stopping at epoch 35
      explained_var=0.00169 | mse_loss=8.17600
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.01556| avg_loss=8.03558
----------------------------------------------


----------------------------------------------
Params for Trial 58
{'learning_rate': 0.1, 'weight_decay': 0.008287931288480549, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=575.73694 | best_loss=575.73694
Epoch 1/80: current_loss=93.83243 | best_loss=93.83243
Epoch 2/80: current_loss=54.50630 | best_loss=54.50630
Epoch 3/80: current_loss=70.19377 | best_loss=54.50630
Epoch 4/80: current_loss=14.37631 | best_loss=14.37631
Epoch 5/80: current_loss=9.01878 | best_loss=9.01878
Epoch 6/80: current_loss=9.01609 | best_loss=9.01609
Epoch 7/80: current_loss=8.86923 | best_loss=8.86923
Epoch 8/80: current_loss=12.63569 | best_loss=8.86923
Epoch 9/80: current_loss=9.89128 | best_loss=8.86923
Epoch 10/80: current_loss=9.28533 | best_loss=8.86923
Epoch 11/80: current_loss=10.93697 | best_loss=8.86923
Epoch 12/80: current_loss=11.02686 | best_loss=8.86923
Epoch 13/80: current_loss=9.79102 | best_loss=8.86923
Epoch 14/80: current_loss=8.92011 | best_loss=8.86923
Epoch 15/80: current_loss=9.05595 | best_loss=8.86923
Epoch 16/80: current_loss=10.09435 | best_loss=8.86923
Epoch 17/80: current_loss=8.57903 | best_loss=8.57903
Epoch 18/80: current_loss=13.94878 | best_loss=8.57903
Epoch 19/80: current_loss=8.80245 | best_loss=8.57903
Epoch 20/80: current_loss=8.28570 | best_loss=8.28570
Epoch 21/80: current_loss=8.59857 | best_loss=8.28570
Epoch 22/80: current_loss=25.34928 | best_loss=8.28570
Epoch 23/80: current_loss=15.67616 | best_loss=8.28570
Epoch 24/80: current_loss=16.99586 | best_loss=8.28570
Epoch 25/80: current_loss=17.18443 | best_loss=8.28570
Epoch 26/80: current_loss=9.98847 | best_loss=8.28570
Epoch 27/80: current_loss=14.47200 | best_loss=8.28570
Epoch 28/80: current_loss=15.68651 | best_loss=8.28570
Epoch 29/80: current_loss=14.44571 | best_loss=8.28570
Epoch 30/80: current_loss=9.40903 | best_loss=8.28570
Epoch 31/80: current_loss=9.00139 | best_loss=8.28570
Epoch 32/80: current_loss=14.67751 | best_loss=8.28570
Epoch 33/80: current_loss=13.70181 | best_loss=8.28570
Epoch 34/80: current_loss=16.39919 | best_loss=8.28570
Epoch 35/80: current_loss=8.94553 | best_loss=8.28570
Epoch 36/80: current_loss=11.47498 | best_loss=8.28570
Epoch 37/80: current_loss=11.04707 | best_loss=8.28570
Epoch 38/80: current_loss=12.10371 | best_loss=8.28570
Epoch 39/80: current_loss=8.89394 | best_loss=8.28570
Epoch 40/80: current_loss=25.48601 | best_loss=8.28570
Early Stopping at epoch 40
      explained_var=-0.02500 | mse_loss=8.19641

----------------------------------------------
Params for Trial 59
{'learning_rate': 0.1, 'weight_decay': 0.008828229595662063, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.06758 | best_loss=9.06758
Epoch 1/80: current_loss=8.38988 | best_loss=8.38988
Epoch 2/80: current_loss=9.14178 | best_loss=8.38988
Epoch 3/80: current_loss=8.46999 | best_loss=8.38988
Epoch 4/80: current_loss=8.19713 | best_loss=8.19713
Epoch 5/80: current_loss=8.13069 | best_loss=8.13069
Epoch 6/80: current_loss=8.12386 | best_loss=8.12386
Epoch 7/80: current_loss=9.18235 | best_loss=8.12386
Epoch 8/80: current_loss=8.97832 | best_loss=8.12386
Epoch 9/80: current_loss=9.08825 | best_loss=8.12386
Epoch 10/80: current_loss=8.14612 | best_loss=8.12386
Epoch 11/80: current_loss=8.00431 | best_loss=8.00431
Epoch 12/80: current_loss=8.53806 | best_loss=8.00431
Epoch 13/80: current_loss=8.00354 | best_loss=8.00354
Epoch 14/80: current_loss=8.92156 | best_loss=8.00354
Epoch 15/80: current_loss=7.98124 | best_loss=7.98124
Epoch 16/80: current_loss=8.69817 | best_loss=7.98124
Epoch 17/80: current_loss=13.51880 | best_loss=7.98124
Epoch 18/80: current_loss=7.97511 | best_loss=7.97511
Epoch 19/80: current_loss=15.31632 | best_loss=7.97511
Epoch 20/80: current_loss=8.62115 | best_loss=7.97511
Epoch 21/80: current_loss=9.01305 | best_loss=7.97511
Epoch 22/80: current_loss=8.17826 | best_loss=7.97511
Epoch 23/80: current_loss=8.29273 | best_loss=7.97511
Epoch 24/80: current_loss=8.60089 | best_loss=7.97511
Epoch 25/80: current_loss=9.40069 | best_loss=7.97511
Epoch 26/80: current_loss=8.27504 | best_loss=7.97511
Epoch 27/80: current_loss=8.44754 | best_loss=7.97511
Epoch 28/80: current_loss=9.18963 | best_loss=7.97511
Epoch 29/80: current_loss=10.50398 | best_loss=7.97511
Epoch 30/80: current_loss=8.76268 | best_loss=7.97511
Epoch 31/80: current_loss=8.03621 | best_loss=7.97511
Epoch 32/80: current_loss=9.48072 | best_loss=7.97511
Epoch 33/80: current_loss=9.24341 | best_loss=7.97511
Epoch 34/80: current_loss=8.03204 | best_loss=7.97511
Epoch 35/80: current_loss=9.06876 | best_loss=7.97511
Epoch 36/80: current_loss=10.17342 | best_loss=7.97511
Epoch 37/80: current_loss=11.04759 | best_loss=7.97511
Epoch 38/80: current_loss=7.83266 | best_loss=7.83266
Epoch 39/80: current_loss=10.27488 | best_loss=7.83266
Epoch 40/80: current_loss=11.79549 | best_loss=7.83266
Epoch 41/80: current_loss=8.16342 | best_loss=7.83266
Epoch 42/80: current_loss=8.35453 | best_loss=7.83266
Epoch 43/80: current_loss=8.69583 | best_loss=7.83266
Epoch 44/80: current_loss=8.56751 | best_loss=7.83266
Epoch 45/80: current_loss=9.03994 | best_loss=7.83266
Epoch 46/80: current_loss=8.39348 | best_loss=7.83266
Epoch 47/80: current_loss=9.63786 | best_loss=7.83266
Epoch 48/80: current_loss=10.50905 | best_loss=7.83266
Epoch 49/80: current_loss=8.85337 | best_loss=7.83266
Epoch 50/80: current_loss=10.19282 | best_loss=7.83266
Epoch 51/80: current_loss=8.05867 | best_loss=7.83266
Epoch 52/80: current_loss=8.45434 | best_loss=7.83266
Epoch 53/80: current_loss=11.20347 | best_loss=7.83266
Epoch 54/80: current_loss=13.41439 | best_loss=7.83266
Epoch 55/80: current_loss=8.10149 | best_loss=7.83266
Epoch 56/80: current_loss=10.28649 | best_loss=7.83266
Epoch 57/80: current_loss=8.18905 | best_loss=7.83266
Epoch 58/80: current_loss=9.98702 | best_loss=7.83266
Early Stopping at epoch 58
      explained_var=-0.00514 | mse_loss=7.63723

----------------------------------------------
Params for Trial 60
{'learning_rate': 0.1, 'weight_decay': 0.008547238148562179, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=16.83320 | best_loss=16.83320
Epoch 1/80: current_loss=7.86235 | best_loss=7.86235
Epoch 2/80: current_loss=11.69617 | best_loss=7.86235
Epoch 3/80: current_loss=8.00477 | best_loss=7.86235
Epoch 4/80: current_loss=12.73681 | best_loss=7.86235
Epoch 5/80: current_loss=8.86049 | best_loss=7.86235
Epoch 6/80: current_loss=9.01446 | best_loss=7.86235
Epoch 7/80: current_loss=8.30478 | best_loss=7.86235
Epoch 8/80: current_loss=12.96303 | best_loss=7.86235
Epoch 9/80: current_loss=10.39180 | best_loss=7.86235
Epoch 10/80: current_loss=8.14294 | best_loss=7.86235
Epoch 11/80: current_loss=12.33637 | best_loss=7.86235
Epoch 12/80: current_loss=10.51911 | best_loss=7.86235
Epoch 13/80: current_loss=12.51784 | best_loss=7.86235
Epoch 14/80: current_loss=7.98279 | best_loss=7.86235
Epoch 15/80: current_loss=8.97273 | best_loss=7.86235
Epoch 16/80: current_loss=12.22305 | best_loss=7.86235
Epoch 17/80: current_loss=7.91767 | best_loss=7.86235
Epoch 18/80: current_loss=10.63534 | best_loss=7.86235
Epoch 19/80: current_loss=8.34876 | best_loss=7.86235
Epoch 20/80: current_loss=13.08471 | best_loss=7.86235
Epoch 21/80: current_loss=9.17418 | best_loss=7.86235
Early Stopping at epoch 21
      explained_var=-0.00522 | mse_loss=7.69615

----------------------------------------------
Params for Trial 61
{'learning_rate': 0.1, 'weight_decay': 0.00294245489551131, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=20.58933 | best_loss=20.58933
Epoch 1/80: current_loss=14.92221 | best_loss=14.92221
Epoch 2/80: current_loss=10.65537 | best_loss=10.65537
Epoch 3/80: current_loss=10.01666 | best_loss=10.01666
Epoch 4/80: current_loss=7.98116 | best_loss=7.98116
Epoch 5/80: current_loss=9.65760 | best_loss=7.98116
Epoch 6/80: current_loss=9.53804 | best_loss=7.98116
Epoch 7/80: current_loss=7.85978 | best_loss=7.85978
Epoch 8/80: current_loss=7.86272 | best_loss=7.85978
Epoch 9/80: current_loss=9.64044 | best_loss=7.85978
Epoch 10/80: current_loss=12.51803 | best_loss=7.85978
Epoch 11/80: current_loss=8.01934 | best_loss=7.85978
Epoch 12/80: current_loss=8.24210 | best_loss=7.85978
Epoch 13/80: current_loss=9.24816 | best_loss=7.85978
Epoch 14/80: current_loss=12.57541 | best_loss=7.85978
Epoch 15/80: current_loss=7.94629 | best_loss=7.85978
Epoch 16/80: current_loss=9.37205 | best_loss=7.85978
Epoch 17/80: current_loss=7.75613 | best_loss=7.75613
Epoch 18/80: current_loss=10.02751 | best_loss=7.75613
Epoch 19/80: current_loss=9.99121 | best_loss=7.75613
Epoch 20/80: current_loss=12.28619 | best_loss=7.75613
Epoch 21/80: current_loss=19.19524 | best_loss=7.75613
Epoch 22/80: current_loss=7.93076 | best_loss=7.75613
Epoch 23/80: current_loss=8.36325 | best_loss=7.75613
Epoch 24/80: current_loss=11.53457 | best_loss=7.75613
Epoch 25/80: current_loss=8.54016 | best_loss=7.75613
Epoch 26/80: current_loss=10.52977 | best_loss=7.75613
Epoch 27/80: current_loss=56.31377 | best_loss=7.75613
Epoch 28/80: current_loss=19.81207 | best_loss=7.75613
Epoch 29/80: current_loss=8.79693 | best_loss=7.75613
Epoch 30/80: current_loss=10.40998 | best_loss=7.75613
Epoch 31/80: current_loss=27.13203 | best_loss=7.75613
Epoch 32/80: current_loss=8.60539 | best_loss=7.75613
Epoch 33/80: current_loss=12.03690 | best_loss=7.75613
Epoch 34/80: current_loss=7.92220 | best_loss=7.75613
Epoch 35/80: current_loss=7.98551 | best_loss=7.75613
Epoch 36/80: current_loss=8.46575 | best_loss=7.75613
Epoch 37/80: current_loss=9.35054 | best_loss=7.75613
Early Stopping at epoch 37
      explained_var=0.00441 | mse_loss=7.58069
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.44550 | best_loss=11.44550
Epoch 1/80: current_loss=9.43499 | best_loss=9.43499
Epoch 2/80: current_loss=9.70404 | best_loss=9.43499
Epoch 3/80: current_loss=11.15467 | best_loss=9.43499
Epoch 4/80: current_loss=10.86774 | best_loss=9.43499
Epoch 5/80: current_loss=8.76299 | best_loss=8.76299
Epoch 6/80: current_loss=8.97888 | best_loss=8.76299
Epoch 7/80: current_loss=9.72310 | best_loss=8.76299
Epoch 8/80: current_loss=13.09242 | best_loss=8.76299
Epoch 9/80: current_loss=9.19503 | best_loss=8.76299
Epoch 10/80: current_loss=8.75762 | best_loss=8.75762
Epoch 11/80: current_loss=12.39849 | best_loss=8.75762
Epoch 12/80: current_loss=9.62707 | best_loss=8.75762
Epoch 13/80: current_loss=22.49331 | best_loss=8.75762
Epoch 14/80: current_loss=9.50575 | best_loss=8.75762
Epoch 15/80: current_loss=9.82681 | best_loss=8.75762
Epoch 16/80: current_loss=8.99423 | best_loss=8.75762
Epoch 17/80: current_loss=8.05288 | best_loss=8.05288
Epoch 18/80: current_loss=8.19999 | best_loss=8.05288
Epoch 19/80: current_loss=10.48708 | best_loss=8.05288
Epoch 20/80: current_loss=9.25090 | best_loss=8.05288
Epoch 21/80: current_loss=8.38318 | best_loss=8.05288
Epoch 22/80: current_loss=11.72575 | best_loss=8.05288
Epoch 23/80: current_loss=11.28338 | best_loss=8.05288
Epoch 24/80: current_loss=8.98927 | best_loss=8.05288
Epoch 25/80: current_loss=8.80370 | best_loss=8.05288
Epoch 26/80: current_loss=8.49123 | best_loss=8.05288
Epoch 27/80: current_loss=9.41731 | best_loss=8.05288
Epoch 28/80: current_loss=9.11932 | best_loss=8.05288
Epoch 29/80: current_loss=10.63518 | best_loss=8.05288
Epoch 30/80: current_loss=9.12781 | best_loss=8.05288
Epoch 31/80: current_loss=8.48380 | best_loss=8.05288
Epoch 32/80: current_loss=15.31406 | best_loss=8.05288
Epoch 33/80: current_loss=8.50004 | best_loss=8.05288
Epoch 34/80: current_loss=12.32355 | best_loss=8.05288
Epoch 35/80: current_loss=8.41819 | best_loss=8.05288
Epoch 36/80: current_loss=8.69815 | best_loss=8.05288
Epoch 37/80: current_loss=11.06052 | best_loss=8.05288
Early Stopping at epoch 37
      explained_var=0.05894 | mse_loss=7.92637
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=18.14244 | best_loss=18.14244
Epoch 1/80: current_loss=11.80320 | best_loss=11.80320
Epoch 2/80: current_loss=10.95623 | best_loss=10.95623
Epoch 3/80: current_loss=10.26322 | best_loss=10.26322
Epoch 4/80: current_loss=8.86268 | best_loss=8.86268
Epoch 5/80: current_loss=9.81587 | best_loss=8.86268
Epoch 6/80: current_loss=10.30263 | best_loss=8.86268
Epoch 7/80: current_loss=14.30942 | best_loss=8.86268
Epoch 8/80: current_loss=23.82092 | best_loss=8.86268
Epoch 9/80: current_loss=9.09958 | best_loss=8.86268
Epoch 10/80: current_loss=9.12007 | best_loss=8.86268
Epoch 11/80: current_loss=10.33383 | best_loss=8.86268
Epoch 12/80: current_loss=9.10462 | best_loss=8.86268
Epoch 13/80: current_loss=9.09006 | best_loss=8.86268
Epoch 14/80: current_loss=26.86189 | best_loss=8.86268
Epoch 15/80: current_loss=12.72501 | best_loss=8.86268
Epoch 16/80: current_loss=25.19587 | best_loss=8.86268
Epoch 17/80: current_loss=18.96705 | best_loss=8.86268
Epoch 18/80: current_loss=8.88556 | best_loss=8.86268
Epoch 19/80: current_loss=8.74081 | best_loss=8.74081
Epoch 20/80: current_loss=8.82418 | best_loss=8.74081
Epoch 21/80: current_loss=8.71379 | best_loss=8.71379
Epoch 22/80: current_loss=13.40318 | best_loss=8.71379
Epoch 23/80: current_loss=12.88117 | best_loss=8.71379
Epoch 24/80: current_loss=8.77449 | best_loss=8.71379
Epoch 25/80: current_loss=9.07459 | best_loss=8.71379
Epoch 26/80: current_loss=8.55859 | best_loss=8.55859
Epoch 27/80: current_loss=11.62675 | best_loss=8.55859
Epoch 28/80: current_loss=8.75781 | best_loss=8.55859
Epoch 29/80: current_loss=8.76330 | best_loss=8.55859
Epoch 30/80: current_loss=9.09799 | best_loss=8.55859
Epoch 31/80: current_loss=8.93283 | best_loss=8.55859
Epoch 32/80: current_loss=8.77418 | best_loss=8.55859
Epoch 33/80: current_loss=9.63808 | best_loss=8.55859
Epoch 34/80: current_loss=9.65569 | best_loss=8.55859
Epoch 35/80: current_loss=10.01472 | best_loss=8.55859
Epoch 36/80: current_loss=11.65926 | best_loss=8.55859
Epoch 37/80: current_loss=8.74254 | best_loss=8.55859
Epoch 38/80: current_loss=14.55563 | best_loss=8.55859
Epoch 39/80: current_loss=10.34101 | best_loss=8.55859
Epoch 40/80: current_loss=12.70165 | best_loss=8.55859
Epoch 41/80: current_loss=10.66836 | best_loss=8.55859
Epoch 42/80: current_loss=8.49042 | best_loss=8.49042
Epoch 43/80: current_loss=16.84146 | best_loss=8.49042
Epoch 44/80: current_loss=8.74368 | best_loss=8.49042
Epoch 45/80: current_loss=11.20603 | best_loss=8.49042
Epoch 46/80: current_loss=9.23051 | best_loss=8.49042
Epoch 47/80: current_loss=11.71955 | best_loss=8.49042
Epoch 48/80: current_loss=8.83658 | best_loss=8.49042
Epoch 49/80: current_loss=8.59664 | best_loss=8.49042
Epoch 50/80: current_loss=9.72338 | best_loss=8.49042
Epoch 51/80: current_loss=9.45623 | best_loss=8.49042
Epoch 52/80: current_loss=12.36869 | best_loss=8.49042
Epoch 53/80: current_loss=8.85271 | best_loss=8.49042
Epoch 54/80: current_loss=8.73684 | best_loss=8.49042
Epoch 55/80: current_loss=10.21754 | best_loss=8.49042
Epoch 56/80: current_loss=13.39266 | best_loss=8.49042
Epoch 57/80: current_loss=9.48858 | best_loss=8.49042
Epoch 58/80: current_loss=8.43873 | best_loss=8.43873
Epoch 59/80: current_loss=9.40297 | best_loss=8.43873
Epoch 60/80: current_loss=12.19578 | best_loss=8.43873
Epoch 61/80: current_loss=12.39599 | best_loss=8.43873
Epoch 62/80: current_loss=11.80842 | best_loss=8.43873
Epoch 63/80: current_loss=9.33322 | best_loss=8.43873
Epoch 64/80: current_loss=10.01327 | best_loss=8.43873
Epoch 65/80: current_loss=9.07066 | best_loss=8.43873
Epoch 66/80: current_loss=9.02745 | best_loss=8.43873
Epoch 67/80: current_loss=15.04409 | best_loss=8.43873
Epoch 68/80: current_loss=10.69777 | best_loss=8.43873
Epoch 69/80: current_loss=14.51572 | best_loss=8.43873
Epoch 70/80: current_loss=9.01778 | best_loss=8.43873
Epoch 71/80: current_loss=8.62369 | best_loss=8.43873
Epoch 72/80: current_loss=9.86250 | best_loss=8.43873
Epoch 73/80: current_loss=9.37500 | best_loss=8.43873
Epoch 74/80: current_loss=9.21909 | best_loss=8.43873
Epoch 75/80: current_loss=13.63626 | best_loss=8.43873
Epoch 76/80: current_loss=11.05709 | best_loss=8.43873
Epoch 77/80: current_loss=16.00656 | best_loss=8.43873
Epoch 78/80: current_loss=10.86671 | best_loss=8.43873
Early Stopping at epoch 78
      explained_var=0.04978 | mse_loss=8.25807
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.51564 | best_loss=9.51564
Epoch 1/80: current_loss=8.57770 | best_loss=8.57770
Epoch 2/80: current_loss=9.45330 | best_loss=8.57770
Epoch 3/80: current_loss=8.94386 | best_loss=8.57770
Epoch 4/80: current_loss=8.68433 | best_loss=8.57770
Epoch 5/80: current_loss=9.31226 | best_loss=8.57770
Epoch 6/80: current_loss=9.98543 | best_loss=8.57770
Epoch 7/80: current_loss=10.24445 | best_loss=8.57770
Epoch 8/80: current_loss=10.16260 | best_loss=8.57770
Epoch 9/80: current_loss=12.12815 | best_loss=8.57770
Epoch 10/80: current_loss=14.99043 | best_loss=8.57770
Epoch 11/80: current_loss=10.30499 | best_loss=8.57770
Epoch 12/80: current_loss=9.48415 | best_loss=8.57770
Epoch 13/80: current_loss=11.05037 | best_loss=8.57770
Epoch 14/80: current_loss=9.52855 | best_loss=8.57770
Epoch 15/80: current_loss=11.53600 | best_loss=8.57770
Epoch 16/80: current_loss=8.01907 | best_loss=8.01907
Epoch 17/80: current_loss=12.77625 | best_loss=8.01907
Epoch 18/80: current_loss=9.38562 | best_loss=8.01907
Epoch 19/80: current_loss=8.45848 | best_loss=8.01907
Epoch 20/80: current_loss=9.24574 | best_loss=8.01907
Epoch 21/80: current_loss=8.46224 | best_loss=8.01907
Epoch 22/80: current_loss=8.70904 | best_loss=8.01907
Epoch 23/80: current_loss=11.21791 | best_loss=8.01907
Epoch 24/80: current_loss=11.47741 | best_loss=8.01907
Epoch 25/80: current_loss=10.38091 | best_loss=8.01907
Epoch 26/80: current_loss=8.49271 | best_loss=8.01907
Epoch 27/80: current_loss=9.55286 | best_loss=8.01907
Epoch 28/80: current_loss=9.97738 | best_loss=8.01907
Epoch 29/80: current_loss=8.48991 | best_loss=8.01907
Epoch 30/80: current_loss=10.14827 | best_loss=8.01907
Epoch 31/80: current_loss=8.94285 | best_loss=8.01907
Epoch 32/80: current_loss=8.93777 | best_loss=8.01907
Epoch 33/80: current_loss=14.82756 | best_loss=8.01907
Epoch 34/80: current_loss=9.51089 | best_loss=8.01907
Epoch 35/80: current_loss=9.73961 | best_loss=8.01907
Epoch 36/80: current_loss=9.08605 | best_loss=8.01907
Early Stopping at epoch 36
      explained_var=0.02976 | mse_loss=8.11096
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.77546 | best_loss=9.77546
Epoch 1/80: current_loss=11.37234 | best_loss=9.77546
Epoch 2/80: current_loss=22.19344 | best_loss=9.77546
Epoch 3/80: current_loss=11.49126 | best_loss=9.77546
Epoch 4/80: current_loss=8.39369 | best_loss=8.39369
Epoch 5/80: current_loss=8.42751 | best_loss=8.39369
Epoch 6/80: current_loss=7.75323 | best_loss=7.75323
Epoch 7/80: current_loss=10.86889 | best_loss=7.75323
Epoch 8/80: current_loss=9.08228 | best_loss=7.75323
Epoch 9/80: current_loss=8.28233 | best_loss=7.75323
Epoch 10/80: current_loss=16.65730 | best_loss=7.75323
Epoch 11/80: current_loss=9.28731 | best_loss=7.75323
Epoch 12/80: current_loss=8.17286 | best_loss=7.75323
Epoch 13/80: current_loss=8.70839 | best_loss=7.75323
Epoch 14/80: current_loss=7.71512 | best_loss=7.71512
Epoch 15/80: current_loss=12.37742 | best_loss=7.71512
Epoch 16/80: current_loss=10.41049 | best_loss=7.71512
Epoch 17/80: current_loss=15.07278 | best_loss=7.71512
Epoch 18/80: current_loss=9.15124 | best_loss=7.71512
Epoch 19/80: current_loss=8.36847 | best_loss=7.71512
Epoch 20/80: current_loss=8.39450 | best_loss=7.71512
Epoch 21/80: current_loss=7.87026 | best_loss=7.71512
Epoch 22/80: current_loss=8.03767 | best_loss=7.71512
Epoch 23/80: current_loss=8.13292 | best_loss=7.71512
Epoch 24/80: current_loss=7.98906 | best_loss=7.71512
Epoch 25/80: current_loss=8.19112 | best_loss=7.71512
Epoch 26/80: current_loss=8.05497 | best_loss=7.71512
Epoch 27/80: current_loss=8.30618 | best_loss=7.71512
Epoch 28/80: current_loss=8.65089 | best_loss=7.71512
Epoch 29/80: current_loss=12.58934 | best_loss=7.71512
Epoch 30/80: current_loss=8.52118 | best_loss=7.71512
Epoch 31/80: current_loss=12.24839 | best_loss=7.71512
Epoch 32/80: current_loss=23.56404 | best_loss=7.71512
Epoch 33/80: current_loss=8.43973 | best_loss=7.71512
Epoch 34/80: current_loss=18.20040 | best_loss=7.71512
Early Stopping at epoch 34
      explained_var=0.00025 | mse_loss=7.89664
----------------------------------------------
Average early_stopping_point: 24| avg_exp_var=0.02863| avg_loss=7.95454
----------------------------------------------


----------------------------------------------
Params for Trial 62
{'learning_rate': 0.1, 'weight_decay': 0.002986006237257322, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.44073 | best_loss=11.44073
Epoch 1/80: current_loss=11.31043 | best_loss=11.31043
Epoch 2/80: current_loss=11.56804 | best_loss=11.31043
Epoch 3/80: current_loss=8.63952 | best_loss=8.63952
Epoch 4/80: current_loss=10.09933 | best_loss=8.63952
Epoch 5/80: current_loss=7.97021 | best_loss=7.97021
Epoch 6/80: current_loss=10.01016 | best_loss=7.97021
Epoch 7/80: current_loss=8.74039 | best_loss=7.97021
Epoch 8/80: current_loss=11.75003 | best_loss=7.97021
Epoch 9/80: current_loss=11.35964 | best_loss=7.97021
Epoch 10/80: current_loss=8.01767 | best_loss=7.97021
Epoch 11/80: current_loss=9.67693 | best_loss=7.97021
Epoch 12/80: current_loss=9.60374 | best_loss=7.97021
Epoch 13/80: current_loss=12.82668 | best_loss=7.97021
Epoch 14/80: current_loss=8.86348 | best_loss=7.97021
Epoch 15/80: current_loss=10.74932 | best_loss=7.97021
Epoch 16/80: current_loss=12.94003 | best_loss=7.97021
Epoch 17/80: current_loss=10.04384 | best_loss=7.97021
Epoch 18/80: current_loss=12.58129 | best_loss=7.97021
Epoch 19/80: current_loss=7.91061 | best_loss=7.91061
Epoch 20/80: current_loss=15.07432 | best_loss=7.91061
Epoch 21/80: current_loss=8.10923 | best_loss=7.91061
Epoch 22/80: current_loss=7.77492 | best_loss=7.77492
Epoch 23/80: current_loss=7.94669 | best_loss=7.77492
Epoch 24/80: current_loss=8.70908 | best_loss=7.77492
Epoch 25/80: current_loss=7.81807 | best_loss=7.77492
Epoch 26/80: current_loss=15.55837 | best_loss=7.77492
Epoch 27/80: current_loss=7.79134 | best_loss=7.77492
Epoch 28/80: current_loss=7.83329 | best_loss=7.77492
Epoch 29/80: current_loss=11.12385 | best_loss=7.77492
Epoch 30/80: current_loss=7.90149 | best_loss=7.77492
Epoch 31/80: current_loss=8.21846 | best_loss=7.77492
Epoch 32/80: current_loss=9.72835 | best_loss=7.77492
Epoch 33/80: current_loss=7.75817 | best_loss=7.75817
Epoch 34/80: current_loss=8.45975 | best_loss=7.75817
Epoch 35/80: current_loss=13.94645 | best_loss=7.75817
Epoch 36/80: current_loss=12.64890 | best_loss=7.75817
Epoch 37/80: current_loss=10.03843 | best_loss=7.75817
Epoch 38/80: current_loss=8.91943 | best_loss=7.75817
Epoch 39/80: current_loss=14.06024 | best_loss=7.75817
Epoch 40/80: current_loss=7.79217 | best_loss=7.75817
Epoch 41/80: current_loss=13.43996 | best_loss=7.75817
Epoch 42/80: current_loss=12.06334 | best_loss=7.75817
Epoch 43/80: current_loss=8.25813 | best_loss=7.75817
Epoch 44/80: current_loss=7.87180 | best_loss=7.75817
Epoch 45/80: current_loss=8.88954 | best_loss=7.75817
Epoch 46/80: current_loss=10.08955 | best_loss=7.75817
Epoch 47/80: current_loss=7.94252 | best_loss=7.75817
Epoch 48/80: current_loss=8.64759 | best_loss=7.75817
Epoch 49/80: current_loss=53.33243 | best_loss=7.75817
Epoch 50/80: current_loss=8.04628 | best_loss=7.75817
Epoch 51/80: current_loss=8.20619 | best_loss=7.75817
Epoch 52/80: current_loss=11.10500 | best_loss=7.75817
Epoch 53/80: current_loss=9.49201 | best_loss=7.75817
Early Stopping at epoch 53
      explained_var=0.00490 | mse_loss=7.59668
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.36183 | best_loss=8.36183
Epoch 1/80: current_loss=8.91700 | best_loss=8.36183
Epoch 2/80: current_loss=9.31120 | best_loss=8.36183
Epoch 3/80: current_loss=11.93792 | best_loss=8.36183
Epoch 4/80: current_loss=9.37869 | best_loss=8.36183
Epoch 5/80: current_loss=10.25632 | best_loss=8.36183
Epoch 6/80: current_loss=8.46149 | best_loss=8.36183
Epoch 7/80: current_loss=8.45716 | best_loss=8.36183
Epoch 8/80: current_loss=10.93184 | best_loss=8.36183
Epoch 9/80: current_loss=9.94508 | best_loss=8.36183
Epoch 10/80: current_loss=15.86492 | best_loss=8.36183
Epoch 11/80: current_loss=9.39514 | best_loss=8.36183
Epoch 12/80: current_loss=9.55692 | best_loss=8.36183
Epoch 13/80: current_loss=9.28244 | best_loss=8.36183
Epoch 14/80: current_loss=8.53158 | best_loss=8.36183
Epoch 15/80: current_loss=9.45152 | best_loss=8.36183
Epoch 16/80: current_loss=9.85398 | best_loss=8.36183
Epoch 17/80: current_loss=9.57875 | best_loss=8.36183
Epoch 18/80: current_loss=8.83069 | best_loss=8.36183
Epoch 19/80: current_loss=12.76614 | best_loss=8.36183
Epoch 20/80: current_loss=8.36209 | best_loss=8.36183
Early Stopping at epoch 20
      explained_var=0.00479 | mse_loss=8.21406
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.07026 | best_loss=9.07026
Epoch 1/80: current_loss=8.63722 | best_loss=8.63722
Epoch 2/80: current_loss=10.66829 | best_loss=8.63722
Epoch 3/80: current_loss=8.77335 | best_loss=8.63722
Epoch 4/80: current_loss=8.86378 | best_loss=8.63722
Epoch 5/80: current_loss=10.67376 | best_loss=8.63722
Epoch 6/80: current_loss=9.54273 | best_loss=8.63722
Epoch 7/80: current_loss=19.71813 | best_loss=8.63722
Epoch 8/80: current_loss=14.06323 | best_loss=8.63722
Epoch 9/80: current_loss=9.52244 | best_loss=8.63722
Epoch 10/80: current_loss=11.76744 | best_loss=8.63722
Epoch 11/80: current_loss=16.16820 | best_loss=8.63722
Epoch 12/80: current_loss=12.08786 | best_loss=8.63722
Epoch 13/80: current_loss=8.70974 | best_loss=8.63722
Epoch 14/80: current_loss=8.89572 | best_loss=8.63722
Epoch 15/80: current_loss=8.85987 | best_loss=8.63722
Epoch 16/80: current_loss=8.74508 | best_loss=8.63722
Epoch 17/80: current_loss=8.92241 | best_loss=8.63722
Epoch 18/80: current_loss=9.59182 | best_loss=8.63722
Epoch 19/80: current_loss=14.23298 | best_loss=8.63722
Epoch 20/80: current_loss=11.40077 | best_loss=8.63722
Epoch 21/80: current_loss=13.40260 | best_loss=8.63722
Early Stopping at epoch 21
      explained_var=0.01193 | mse_loss=8.38763
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.44516 | best_loss=14.44516
Epoch 1/80: current_loss=10.31944 | best_loss=10.31944
Epoch 2/80: current_loss=8.50226 | best_loss=8.50226
Epoch 3/80: current_loss=9.40281 | best_loss=8.50226
Epoch 4/80: current_loss=8.35363 | best_loss=8.35363
Epoch 5/80: current_loss=29.04865 | best_loss=8.35363
Epoch 6/80: current_loss=18.92418 | best_loss=8.35363
Epoch 7/80: current_loss=10.77085 | best_loss=8.35363
Epoch 8/80: current_loss=11.96635 | best_loss=8.35363
Epoch 9/80: current_loss=10.55156 | best_loss=8.35363
Epoch 10/80: current_loss=12.53937 | best_loss=8.35363
Epoch 11/80: current_loss=9.78735 | best_loss=8.35363
Epoch 12/80: current_loss=8.44955 | best_loss=8.35363
Epoch 13/80: current_loss=8.36155 | best_loss=8.35363
Epoch 14/80: current_loss=20.43870 | best_loss=8.35363
Epoch 15/80: current_loss=9.54726 | best_loss=8.35363
Epoch 16/80: current_loss=18.08938 | best_loss=8.35363
Epoch 17/80: current_loss=16.60537 | best_loss=8.35363
Epoch 18/80: current_loss=10.00032 | best_loss=8.35363
Epoch 19/80: current_loss=8.35853 | best_loss=8.35363
Epoch 20/80: current_loss=8.35591 | best_loss=8.35363
Epoch 21/80: current_loss=16.31546 | best_loss=8.35363
Epoch 22/80: current_loss=8.26842 | best_loss=8.26842
Epoch 23/80: current_loss=8.61276 | best_loss=8.26842
Epoch 24/80: current_loss=11.11479 | best_loss=8.26842
Epoch 25/80: current_loss=8.46025 | best_loss=8.26842
Epoch 26/80: current_loss=15.22363 | best_loss=8.26842
Epoch 27/80: current_loss=9.67196 | best_loss=8.26842
Epoch 28/80: current_loss=9.26793 | best_loss=8.26842
Epoch 29/80: current_loss=8.24304 | best_loss=8.24304
Epoch 30/80: current_loss=11.07912 | best_loss=8.24304
Epoch 31/80: current_loss=9.73711 | best_loss=8.24304
Epoch 32/80: current_loss=9.99345 | best_loss=8.24304
Epoch 33/80: current_loss=8.81281 | best_loss=8.24304
Epoch 34/80: current_loss=10.18305 | best_loss=8.24304
Epoch 35/80: current_loss=12.83800 | best_loss=8.24304
Epoch 36/80: current_loss=8.91713 | best_loss=8.24304
Epoch 37/80: current_loss=8.93004 | best_loss=8.24304
Epoch 38/80: current_loss=8.41381 | best_loss=8.24304
Epoch 39/80: current_loss=9.31845 | best_loss=8.24304
Epoch 40/80: current_loss=8.33754 | best_loss=8.24304
Epoch 41/80: current_loss=8.22744 | best_loss=8.22744
Epoch 42/80: current_loss=10.58107 | best_loss=8.22744
Epoch 43/80: current_loss=8.47912 | best_loss=8.22744
Epoch 44/80: current_loss=14.72211 | best_loss=8.22744
Epoch 45/80: current_loss=9.30184 | best_loss=8.22744
Epoch 46/80: current_loss=16.62742 | best_loss=8.22744
Epoch 47/80: current_loss=9.64379 | best_loss=8.22744
Epoch 48/80: current_loss=8.81272 | best_loss=8.22744
Epoch 49/80: current_loss=8.78928 | best_loss=8.22744
Epoch 50/80: current_loss=13.05293 | best_loss=8.22744
Epoch 51/80: current_loss=12.80081 | best_loss=8.22744
Epoch 52/80: current_loss=8.26351 | best_loss=8.22744
Epoch 53/80: current_loss=8.26486 | best_loss=8.22744
Epoch 54/80: current_loss=8.28926 | best_loss=8.22744
Epoch 55/80: current_loss=8.29045 | best_loss=8.22744
Epoch 56/80: current_loss=8.34997 | best_loss=8.22744
Epoch 57/80: current_loss=8.27001 | best_loss=8.22744
Epoch 58/80: current_loss=8.67866 | best_loss=8.22744
Epoch 59/80: current_loss=9.60186 | best_loss=8.22744
Epoch 60/80: current_loss=8.23547 | best_loss=8.22744
Epoch 61/80: current_loss=8.37043 | best_loss=8.22744
Early Stopping at epoch 61
      explained_var=-0.00022 | mse_loss=8.33455
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.32786 | best_loss=9.32786
Epoch 1/80: current_loss=11.73350 | best_loss=9.32786
Epoch 2/80: current_loss=8.58020 | best_loss=8.58020
Epoch 3/80: current_loss=10.10859 | best_loss=8.58020
Epoch 4/80: current_loss=9.98509 | best_loss=8.58020
Epoch 5/80: current_loss=8.43475 | best_loss=8.43475
Epoch 6/80: current_loss=9.12893 | best_loss=8.43475
Epoch 7/80: current_loss=8.06876 | best_loss=8.06876
Epoch 8/80: current_loss=8.81097 | best_loss=8.06876
Epoch 9/80: current_loss=7.93971 | best_loss=7.93971
Epoch 10/80: current_loss=9.79334 | best_loss=7.93971
Epoch 11/80: current_loss=8.54518 | best_loss=7.93971
Epoch 12/80: current_loss=8.28109 | best_loss=7.93971
Epoch 13/80: current_loss=9.53225 | best_loss=7.93971
Epoch 14/80: current_loss=8.10994 | best_loss=7.93971
Epoch 15/80: current_loss=11.49649 | best_loss=7.93971
Epoch 16/80: current_loss=8.47574 | best_loss=7.93971
Epoch 17/80: current_loss=7.91035 | best_loss=7.91035
Epoch 18/80: current_loss=9.15573 | best_loss=7.91035
Epoch 19/80: current_loss=9.09885 | best_loss=7.91035
Epoch 20/80: current_loss=13.32511 | best_loss=7.91035
Epoch 21/80: current_loss=11.32948 | best_loss=7.91035
Epoch 22/80: current_loss=7.76360 | best_loss=7.76360
Epoch 23/80: current_loss=9.30474 | best_loss=7.76360
Epoch 24/80: current_loss=9.53554 | best_loss=7.76360
Epoch 25/80: current_loss=8.73086 | best_loss=7.76360
Epoch 26/80: current_loss=8.35098 | best_loss=7.76360
Epoch 27/80: current_loss=12.30986 | best_loss=7.76360
Epoch 28/80: current_loss=9.39759 | best_loss=7.76360
Epoch 29/80: current_loss=7.90610 | best_loss=7.76360
Epoch 30/80: current_loss=9.77716 | best_loss=7.76360
Epoch 31/80: current_loss=13.31308 | best_loss=7.76360
Epoch 32/80: current_loss=8.31215 | best_loss=7.76360
Epoch 33/80: current_loss=12.76489 | best_loss=7.76360
Epoch 34/80: current_loss=10.66442 | best_loss=7.76360
Epoch 35/80: current_loss=8.32883 | best_loss=7.76360
Epoch 36/80: current_loss=10.87070 | best_loss=7.76360
Epoch 37/80: current_loss=10.79696 | best_loss=7.76360
Epoch 38/80: current_loss=11.76739 | best_loss=7.76360
Epoch 39/80: current_loss=8.24256 | best_loss=7.76360
Epoch 40/80: current_loss=7.70762 | best_loss=7.70762
Epoch 41/80: current_loss=8.48317 | best_loss=7.70762
Epoch 42/80: current_loss=9.81527 | best_loss=7.70762
Epoch 43/80: current_loss=8.08189 | best_loss=7.70762
Epoch 44/80: current_loss=9.15319 | best_loss=7.70762
Epoch 45/80: current_loss=9.38412 | best_loss=7.70762
Epoch 46/80: current_loss=10.45853 | best_loss=7.70762
Epoch 47/80: current_loss=10.01435 | best_loss=7.70762
Epoch 48/80: current_loss=8.61660 | best_loss=7.70762
Epoch 49/80: current_loss=10.66772 | best_loss=7.70762
Epoch 50/80: current_loss=11.88542 | best_loss=7.70762
Epoch 51/80: current_loss=10.79817 | best_loss=7.70762
Epoch 52/80: current_loss=10.47255 | best_loss=7.70762
Epoch 53/80: current_loss=12.56800 | best_loss=7.70762
Epoch 54/80: current_loss=10.76767 | best_loss=7.70762
Epoch 55/80: current_loss=7.72971 | best_loss=7.70762
Epoch 56/80: current_loss=8.85594 | best_loss=7.70762
Epoch 57/80: current_loss=8.45750 | best_loss=7.70762
Epoch 58/80: current_loss=9.13668 | best_loss=7.70762
Epoch 59/80: current_loss=11.80089 | best_loss=7.70762
Epoch 60/80: current_loss=11.10407 | best_loss=7.70762
Early Stopping at epoch 60
      explained_var=-0.00188 | mse_loss=7.88980
----------------------------------------------
Average early_stopping_point: 23| avg_exp_var=0.00390| avg_loss=8.08454
----------------------------------------------


----------------------------------------------
Params for Trial 63
{'learning_rate': 0.1, 'weight_decay': 0.004931778140481958, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.33692 | best_loss=9.33692
Epoch 1/80: current_loss=40.68142 | best_loss=9.33692
Epoch 2/80: current_loss=8.03150 | best_loss=8.03150
Epoch 3/80: current_loss=10.42490 | best_loss=8.03150
Epoch 4/80: current_loss=72.09241 | best_loss=8.03150
Epoch 5/80: current_loss=22.81174 | best_loss=8.03150
Epoch 6/80: current_loss=10.77486 | best_loss=8.03150
Epoch 7/80: current_loss=15.05474 | best_loss=8.03150
Epoch 8/80: current_loss=25.78560 | best_loss=8.03150
Epoch 9/80: current_loss=18.17669 | best_loss=8.03150
Epoch 10/80: current_loss=13.05900 | best_loss=8.03150
Epoch 11/80: current_loss=10.12128 | best_loss=8.03150
Epoch 12/80: current_loss=10.69757 | best_loss=8.03150
Epoch 13/80: current_loss=10.90587 | best_loss=8.03150
Epoch 14/80: current_loss=9.40398 | best_loss=8.03150
Epoch 15/80: current_loss=7.79591 | best_loss=7.79591
Epoch 16/80: current_loss=9.60477 | best_loss=7.79591
Epoch 17/80: current_loss=7.89209 | best_loss=7.79591
Epoch 18/80: current_loss=11.52075 | best_loss=7.79591
Epoch 19/80: current_loss=8.23463 | best_loss=7.79591
Epoch 20/80: current_loss=8.43305 | best_loss=7.79591
Epoch 21/80: current_loss=8.36635 | best_loss=7.79591
Epoch 22/80: current_loss=8.92911 | best_loss=7.79591
Epoch 23/80: current_loss=9.66991 | best_loss=7.79591
Epoch 24/80: current_loss=9.05405 | best_loss=7.79591
Epoch 25/80: current_loss=18.74156 | best_loss=7.79591
Epoch 26/80: current_loss=8.51916 | best_loss=7.79591
Epoch 27/80: current_loss=8.53563 | best_loss=7.79591
Epoch 28/80: current_loss=8.10604 | best_loss=7.79591
Epoch 29/80: current_loss=8.22897 | best_loss=7.79591
Epoch 30/80: current_loss=10.24745 | best_loss=7.79591
Epoch 31/80: current_loss=15.03228 | best_loss=7.79591
Epoch 32/80: current_loss=8.57648 | best_loss=7.79591
Epoch 33/80: current_loss=8.38549 | best_loss=7.79591
Epoch 34/80: current_loss=9.16982 | best_loss=7.79591
Epoch 35/80: current_loss=8.25906 | best_loss=7.79591
Early Stopping at epoch 35
      explained_var=0.00076 | mse_loss=7.62095

----------------------------------------------
Params for Trial 64
{'learning_rate': 0.1, 'weight_decay': 0.007818873531169437, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=77.90836 | best_loss=77.90836
Epoch 1/80: current_loss=10.89034 | best_loss=10.89034
Epoch 2/80: current_loss=22.34756 | best_loss=10.89034
Epoch 3/80: current_loss=9.55378 | best_loss=9.55378
Epoch 4/80: current_loss=8.30441 | best_loss=8.30441
Epoch 5/80: current_loss=9.11659 | best_loss=8.30441
Epoch 6/80: current_loss=8.26603 | best_loss=8.26603
Epoch 7/80: current_loss=9.55792 | best_loss=8.26603
Epoch 8/80: current_loss=10.04202 | best_loss=8.26603
Epoch 9/80: current_loss=9.97052 | best_loss=8.26603
Epoch 10/80: current_loss=9.23614 | best_loss=8.26603
Epoch 11/80: current_loss=9.34595 | best_loss=8.26603
Epoch 12/80: current_loss=8.85722 | best_loss=8.26603
Epoch 13/80: current_loss=11.36673 | best_loss=8.26603
Epoch 14/80: current_loss=24.45630 | best_loss=8.26603
Epoch 15/80: current_loss=16.92733 | best_loss=8.26603
Epoch 16/80: current_loss=14.47409 | best_loss=8.26603
Epoch 17/80: current_loss=16.32895 | best_loss=8.26603
Epoch 18/80: current_loss=12.49845 | best_loss=8.26603
Epoch 19/80: current_loss=8.28375 | best_loss=8.26603
Epoch 20/80: current_loss=8.41338 | best_loss=8.26603
Epoch 21/80: current_loss=20.72988 | best_loss=8.26603
Epoch 22/80: current_loss=10.42893 | best_loss=8.26603
Epoch 23/80: current_loss=8.13138 | best_loss=8.13138
Epoch 24/80: current_loss=8.58384 | best_loss=8.13138
Epoch 25/80: current_loss=8.60514 | best_loss=8.13138
Epoch 26/80: current_loss=8.97655 | best_loss=8.13138
Epoch 27/80: current_loss=7.96660 | best_loss=7.96660
Epoch 28/80: current_loss=10.64092 | best_loss=7.96660
Epoch 29/80: current_loss=8.35695 | best_loss=7.96660
Epoch 30/80: current_loss=9.99603 | best_loss=7.96660
Epoch 31/80: current_loss=8.31790 | best_loss=7.96660
Epoch 32/80: current_loss=8.84795 | best_loss=7.96660
Epoch 33/80: current_loss=8.51448 | best_loss=7.96660
Epoch 34/80: current_loss=8.26775 | best_loss=7.96660
Epoch 35/80: current_loss=9.78122 | best_loss=7.96660
Epoch 36/80: current_loss=8.20917 | best_loss=7.96660
Epoch 37/80: current_loss=17.39844 | best_loss=7.96660
Epoch 38/80: current_loss=8.83356 | best_loss=7.96660
Epoch 39/80: current_loss=10.67172 | best_loss=7.96660
Epoch 40/80: current_loss=8.17597 | best_loss=7.96660
Epoch 41/80: current_loss=8.92724 | best_loss=7.96660
Epoch 42/80: current_loss=8.49477 | best_loss=7.96660
Epoch 43/80: current_loss=8.45721 | best_loss=7.96660
Epoch 44/80: current_loss=8.30222 | best_loss=7.96660
Epoch 45/80: current_loss=9.91659 | best_loss=7.96660
Epoch 46/80: current_loss=8.10588 | best_loss=7.96660
Epoch 47/80: current_loss=12.66340 | best_loss=7.96660
Early Stopping at epoch 47
      explained_var=0.00116 | mse_loss=7.82713

----------------------------------------------
Params for Trial 65
{'learning_rate': 0.1, 'weight_decay': 0.009510005316160676, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=15.72757 | best_loss=15.72757
Epoch 1/80: current_loss=8.07339 | best_loss=8.07339
Epoch 2/80: current_loss=8.15538 | best_loss=8.07339
Epoch 3/80: current_loss=11.05618 | best_loss=8.07339
Epoch 4/80: current_loss=7.86764 | best_loss=7.86764
Epoch 5/80: current_loss=8.70900 | best_loss=7.86764
Epoch 6/80: current_loss=7.76154 | best_loss=7.76154
Epoch 7/80: current_loss=8.61250 | best_loss=7.76154
Epoch 8/80: current_loss=14.00872 | best_loss=7.76154
Epoch 9/80: current_loss=8.84579 | best_loss=7.76154
Epoch 10/80: current_loss=8.01447 | best_loss=7.76154
Epoch 11/80: current_loss=8.01517 | best_loss=7.76154
Epoch 12/80: current_loss=8.23201 | best_loss=7.76154
Epoch 13/80: current_loss=12.17845 | best_loss=7.76154
Epoch 14/80: current_loss=7.94133 | best_loss=7.76154
Epoch 15/80: current_loss=10.00336 | best_loss=7.76154
Epoch 16/80: current_loss=11.10944 | best_loss=7.76154
Epoch 17/80: current_loss=8.23594 | best_loss=7.76154
Epoch 18/80: current_loss=24.15027 | best_loss=7.76154
Epoch 19/80: current_loss=11.36590 | best_loss=7.76154
Epoch 20/80: current_loss=13.59851 | best_loss=7.76154
Epoch 21/80: current_loss=8.65242 | best_loss=7.76154
Epoch 22/80: current_loss=8.31012 | best_loss=7.76154
Epoch 23/80: current_loss=9.22947 | best_loss=7.76154
Epoch 24/80: current_loss=8.19936 | best_loss=7.76154
Epoch 25/80: current_loss=10.04550 | best_loss=7.76154
Epoch 26/80: current_loss=31.96137 | best_loss=7.76154
Early Stopping at epoch 26
      explained_var=0.00262 | mse_loss=7.56949
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=27.55447 | best_loss=27.55447
Epoch 1/80: current_loss=12.40311 | best_loss=12.40311
Epoch 2/80: current_loss=20.92535 | best_loss=12.40311
Epoch 3/80: current_loss=14.11298 | best_loss=12.40311
Epoch 4/80: current_loss=18.52504 | best_loss=12.40311
Epoch 5/80: current_loss=13.85075 | best_loss=12.40311
Epoch 6/80: current_loss=12.77173 | best_loss=12.40311
Epoch 7/80: current_loss=11.22977 | best_loss=11.22977
Epoch 8/80: current_loss=10.10659 | best_loss=10.10659
Epoch 9/80: current_loss=13.96721 | best_loss=10.10659
Epoch 10/80: current_loss=8.77418 | best_loss=8.77418
Epoch 11/80: current_loss=9.65664 | best_loss=8.77418
Epoch 12/80: current_loss=8.48375 | best_loss=8.48375
Epoch 13/80: current_loss=11.04178 | best_loss=8.48375
Epoch 14/80: current_loss=11.56695 | best_loss=8.48375
Epoch 15/80: current_loss=8.72427 | best_loss=8.48375
Epoch 16/80: current_loss=9.60980 | best_loss=8.48375
Epoch 17/80: current_loss=9.13318 | best_loss=8.48375
Epoch 18/80: current_loss=8.92635 | best_loss=8.48375
Epoch 19/80: current_loss=10.35821 | best_loss=8.48375
Epoch 20/80: current_loss=10.29078 | best_loss=8.48375
Epoch 21/80: current_loss=8.52404 | best_loss=8.48375
Epoch 22/80: current_loss=8.72682 | best_loss=8.48375
Epoch 23/80: current_loss=9.60869 | best_loss=8.48375
Epoch 24/80: current_loss=15.71656 | best_loss=8.48375
Epoch 25/80: current_loss=9.97610 | best_loss=8.48375
Epoch 26/80: current_loss=8.99579 | best_loss=8.48375
Epoch 27/80: current_loss=8.40101 | best_loss=8.40101
Epoch 28/80: current_loss=11.87247 | best_loss=8.40101
Epoch 29/80: current_loss=9.37865 | best_loss=8.40101
Epoch 30/80: current_loss=9.81768 | best_loss=8.40101
Epoch 31/80: current_loss=9.38705 | best_loss=8.40101
Epoch 32/80: current_loss=8.96904 | best_loss=8.40101
Epoch 33/80: current_loss=8.63172 | best_loss=8.40101
Epoch 34/80: current_loss=11.25253 | best_loss=8.40101
Epoch 35/80: current_loss=12.96878 | best_loss=8.40101
Epoch 36/80: current_loss=10.84453 | best_loss=8.40101
Epoch 37/80: current_loss=8.99472 | best_loss=8.40101
Epoch 38/80: current_loss=8.54900 | best_loss=8.40101
Epoch 39/80: current_loss=9.72703 | best_loss=8.40101
Epoch 40/80: current_loss=11.04885 | best_loss=8.40101
Epoch 41/80: current_loss=8.45840 | best_loss=8.40101
Epoch 42/80: current_loss=10.21040 | best_loss=8.40101
Epoch 43/80: current_loss=8.96668 | best_loss=8.40101
Epoch 44/80: current_loss=9.95638 | best_loss=8.40101
Epoch 45/80: current_loss=9.94106 | best_loss=8.40101
Epoch 46/80: current_loss=14.98812 | best_loss=8.40101
Epoch 47/80: current_loss=8.52473 | best_loss=8.40101
Early Stopping at epoch 47
      explained_var=-0.00886 | mse_loss=8.28339
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.18935 | best_loss=9.18935
Epoch 1/80: current_loss=12.06138 | best_loss=9.18935
Epoch 2/80: current_loss=10.48373 | best_loss=9.18935
Epoch 3/80: current_loss=16.00970 | best_loss=9.18935
Epoch 4/80: current_loss=9.71171 | best_loss=9.18935
Epoch 5/80: current_loss=10.79104 | best_loss=9.18935
Epoch 6/80: current_loss=10.96608 | best_loss=9.18935
Epoch 7/80: current_loss=8.71869 | best_loss=8.71869
Epoch 8/80: current_loss=10.07274 | best_loss=8.71869
Epoch 9/80: current_loss=10.32023 | best_loss=8.71869
Epoch 10/80: current_loss=8.92473 | best_loss=8.71869
Epoch 11/80: current_loss=8.02575 | best_loss=8.02575
Epoch 12/80: current_loss=17.57156 | best_loss=8.02575
Epoch 13/80: current_loss=12.27059 | best_loss=8.02575
Epoch 14/80: current_loss=9.58054 | best_loss=8.02575
Epoch 15/80: current_loss=11.71776 | best_loss=8.02575
Epoch 16/80: current_loss=9.73234 | best_loss=8.02575
Epoch 17/80: current_loss=9.29420 | best_loss=8.02575
Epoch 18/80: current_loss=9.15506 | best_loss=8.02575
Epoch 19/80: current_loss=8.91987 | best_loss=8.02575
Epoch 20/80: current_loss=9.10760 | best_loss=8.02575
Epoch 21/80: current_loss=10.86148 | best_loss=8.02575
Epoch 22/80: current_loss=8.95255 | best_loss=8.02575
Epoch 23/80: current_loss=8.72423 | best_loss=8.02575
Epoch 24/80: current_loss=16.78430 | best_loss=8.02575
Epoch 25/80: current_loss=11.12822 | best_loss=8.02575
Epoch 26/80: current_loss=9.38719 | best_loss=8.02575
Epoch 27/80: current_loss=16.47031 | best_loss=8.02575
Epoch 28/80: current_loss=10.47626 | best_loss=8.02575
Epoch 29/80: current_loss=10.58196 | best_loss=8.02575
Epoch 30/80: current_loss=9.97729 | best_loss=8.02575
Epoch 31/80: current_loss=11.30289 | best_loss=8.02575
Early Stopping at epoch 31
      explained_var=0.09783 | mse_loss=7.90575
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=17.99263 | best_loss=17.99263
Epoch 1/80: current_loss=7.99587 | best_loss=7.99587
Epoch 2/80: current_loss=9.68570 | best_loss=7.99587
Epoch 3/80: current_loss=8.94771 | best_loss=7.99587
Epoch 4/80: current_loss=9.00458 | best_loss=7.99587
Epoch 5/80: current_loss=9.59429 | best_loss=7.99587
Epoch 6/80: current_loss=10.30468 | best_loss=7.99587
Epoch 7/80: current_loss=12.32529 | best_loss=7.99587
Epoch 8/80: current_loss=14.51457 | best_loss=7.99587
Epoch 9/80: current_loss=8.39971 | best_loss=7.99587
Epoch 10/80: current_loss=10.29894 | best_loss=7.99587
Epoch 11/80: current_loss=8.82458 | best_loss=7.99587
Epoch 12/80: current_loss=8.64987 | best_loss=7.99587
Epoch 13/80: current_loss=9.12474 | best_loss=7.99587
Epoch 14/80: current_loss=10.05137 | best_loss=7.99587
Epoch 15/80: current_loss=14.13790 | best_loss=7.99587
Epoch 16/80: current_loss=9.59884 | best_loss=7.99587
Epoch 17/80: current_loss=8.58290 | best_loss=7.99587
Epoch 18/80: current_loss=9.00719 | best_loss=7.99587
Epoch 19/80: current_loss=9.32162 | best_loss=7.99587
Epoch 20/80: current_loss=9.40553 | best_loss=7.99587
Epoch 21/80: current_loss=10.45326 | best_loss=7.99587
Early Stopping at epoch 21
      explained_var=0.02667 | mse_loss=8.11073
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.58872 | best_loss=9.58872
Epoch 1/80: current_loss=9.84363 | best_loss=9.58872
Epoch 2/80: current_loss=9.39862 | best_loss=9.39862
Epoch 3/80: current_loss=11.38237 | best_loss=9.39862
Epoch 4/80: current_loss=8.82372 | best_loss=8.82372
Epoch 5/80: current_loss=7.96575 | best_loss=7.96575
Epoch 6/80: current_loss=13.09589 | best_loss=7.96575
Epoch 7/80: current_loss=9.68216 | best_loss=7.96575
Epoch 8/80: current_loss=8.20311 | best_loss=7.96575
Epoch 9/80: current_loss=8.67767 | best_loss=7.96575
Epoch 10/80: current_loss=11.77530 | best_loss=7.96575
Epoch 11/80: current_loss=15.46631 | best_loss=7.96575
Epoch 12/80: current_loss=8.00797 | best_loss=7.96575
Epoch 13/80: current_loss=7.93886 | best_loss=7.93886
Epoch 14/80: current_loss=7.88859 | best_loss=7.88859
Epoch 15/80: current_loss=7.67527 | best_loss=7.67527
Epoch 16/80: current_loss=10.54214 | best_loss=7.67527
Epoch 17/80: current_loss=8.50441 | best_loss=7.67527
Epoch 18/80: current_loss=8.95443 | best_loss=7.67527
Epoch 19/80: current_loss=8.67345 | best_loss=7.67527
Epoch 20/80: current_loss=7.61541 | best_loss=7.61541
Epoch 21/80: current_loss=9.69572 | best_loss=7.61541
Epoch 22/80: current_loss=11.44711 | best_loss=7.61541
Epoch 23/80: current_loss=12.19483 | best_loss=7.61541
Epoch 24/80: current_loss=7.82028 | best_loss=7.61541
Epoch 25/80: current_loss=8.43444 | best_loss=7.61541
Epoch 26/80: current_loss=9.00125 | best_loss=7.61541
Epoch 27/80: current_loss=8.73753 | best_loss=7.61541
Epoch 28/80: current_loss=13.79522 | best_loss=7.61541
Epoch 29/80: current_loss=8.27257 | best_loss=7.61541
Epoch 30/80: current_loss=9.51410 | best_loss=7.61541
Epoch 31/80: current_loss=10.20145 | best_loss=7.61541
Epoch 32/80: current_loss=15.58637 | best_loss=7.61541
Epoch 33/80: current_loss=8.41612 | best_loss=7.61541
Epoch 34/80: current_loss=8.32350 | best_loss=7.61541
Epoch 35/80: current_loss=12.86550 | best_loss=7.61541
Epoch 36/80: current_loss=11.86788 | best_loss=7.61541
Epoch 37/80: current_loss=20.13869 | best_loss=7.61541
Epoch 38/80: current_loss=7.87874 | best_loss=7.61541
Epoch 39/80: current_loss=12.62243 | best_loss=7.61541
Epoch 40/80: current_loss=10.81247 | best_loss=7.61541
Early Stopping at epoch 40
      explained_var=0.01390 | mse_loss=7.77057
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.02643| avg_loss=7.92799
----------------------------------------------


----------------------------------------------
Params for Trial 66
{'learning_rate': 1e-05, 'weight_decay': 0.00964006272628599, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=83.51224 | best_loss=83.51224
Epoch 1/80: current_loss=82.93551 | best_loss=82.93551
Epoch 2/80: current_loss=82.35796 | best_loss=82.35796
Epoch 3/80: current_loss=81.76109 | best_loss=81.76109
Epoch 4/80: current_loss=81.13074 | best_loss=81.13074
Epoch 5/80: current_loss=80.45490 | best_loss=80.45490
Epoch 6/80: current_loss=79.72463 | best_loss=79.72463
Epoch 7/80: current_loss=78.91227 | best_loss=78.91227
Epoch 8/80: current_loss=77.99330 | best_loss=77.99330
Epoch 9/80: current_loss=76.94476 | best_loss=76.94476
Epoch 10/80: current_loss=75.75438 | best_loss=75.75438
Epoch 11/80: current_loss=74.39685 | best_loss=74.39685
Epoch 12/80: current_loss=72.87307 | best_loss=72.87307
Epoch 13/80: current_loss=71.16645 | best_loss=71.16645
Epoch 14/80: current_loss=69.27891 | best_loss=69.27891
Epoch 15/80: current_loss=67.23927 | best_loss=67.23927
Epoch 16/80: current_loss=65.04723 | best_loss=65.04723
Epoch 17/80: current_loss=62.80173 | best_loss=62.80173
Epoch 18/80: current_loss=60.49465 | best_loss=60.49465
Epoch 19/80: current_loss=58.18960 | best_loss=58.18960
Epoch 20/80: current_loss=55.88987 | best_loss=55.88987
Epoch 21/80: current_loss=53.67756 | best_loss=53.67756
Epoch 22/80: current_loss=51.54751 | best_loss=51.54751
Epoch 23/80: current_loss=49.50066 | best_loss=49.50066
Epoch 24/80: current_loss=47.54540 | best_loss=47.54540
Epoch 25/80: current_loss=45.71217 | best_loss=45.71217
Epoch 26/80: current_loss=43.97239 | best_loss=43.97239
Epoch 27/80: current_loss=42.34340 | best_loss=42.34340
Epoch 28/80: current_loss=40.82389 | best_loss=40.82389
Epoch 29/80: current_loss=39.36353 | best_loss=39.36353
Epoch 30/80: current_loss=37.99115 | best_loss=37.99115
Epoch 31/80: current_loss=36.72817 | best_loss=36.72817
Epoch 32/80: current_loss=35.53949 | best_loss=35.53949
Epoch 33/80: current_loss=34.41696 | best_loss=34.41696
Epoch 34/80: current_loss=33.38030 | best_loss=33.38030
Epoch 35/80: current_loss=32.43913 | best_loss=32.43913
Epoch 36/80: current_loss=31.53863 | best_loss=31.53863
Epoch 37/80: current_loss=30.67969 | best_loss=30.67969
Epoch 38/80: current_loss=29.89079 | best_loss=29.89079
Epoch 39/80: current_loss=29.13448 | best_loss=29.13448
Epoch 40/80: current_loss=28.41822 | best_loss=28.41822
Epoch 41/80: current_loss=27.75643 | best_loss=27.75643
Epoch 42/80: current_loss=27.13117 | best_loss=27.13117
Epoch 43/80: current_loss=26.54139 | best_loss=26.54139
Epoch 44/80: current_loss=25.98094 | best_loss=25.98094
Epoch 45/80: current_loss=25.45307 | best_loss=25.45307
Epoch 46/80: current_loss=24.95166 | best_loss=24.95166
Epoch 47/80: current_loss=24.47155 | best_loss=24.47155
Epoch 48/80: current_loss=24.00587 | best_loss=24.00587
Epoch 49/80: current_loss=23.57483 | best_loss=23.57483
Epoch 50/80: current_loss=23.16350 | best_loss=23.16350
Epoch 51/80: current_loss=22.77336 | best_loss=22.77336
Epoch 52/80: current_loss=22.39082 | best_loss=22.39082
Epoch 53/80: current_loss=22.02399 | best_loss=22.02399
Epoch 54/80: current_loss=21.68000 | best_loss=21.68000
Epoch 55/80: current_loss=21.34644 | best_loss=21.34644
Epoch 56/80: current_loss=21.02941 | best_loss=21.02941
Epoch 57/80: current_loss=20.73093 | best_loss=20.73093
Epoch 58/80: current_loss=20.43649 | best_loss=20.43649
Epoch 59/80: current_loss=20.15281 | best_loss=20.15281
Epoch 60/80: current_loss=19.88579 | best_loss=19.88579
Epoch 61/80: current_loss=19.62190 | best_loss=19.62190
Epoch 62/80: current_loss=19.37735 | best_loss=19.37735
Epoch 63/80: current_loss=19.13753 | best_loss=19.13753
Epoch 64/80: current_loss=18.90346 | best_loss=18.90346
Epoch 65/80: current_loss=18.67339 | best_loss=18.67339
Epoch 66/80: current_loss=18.45825 | best_loss=18.45825
Epoch 67/80: current_loss=18.24443 | best_loss=18.24443
Epoch 68/80: current_loss=18.04245 | best_loss=18.04245
Epoch 69/80: current_loss=17.84831 | best_loss=17.84831
Epoch 70/80: current_loss=17.65153 | best_loss=17.65153
Epoch 71/80: current_loss=17.46319 | best_loss=17.46319
Epoch 72/80: current_loss=17.28364 | best_loss=17.28364
Epoch 73/80: current_loss=17.10976 | best_loss=17.10976
Epoch 74/80: current_loss=16.94294 | best_loss=16.94294
Epoch 75/80: current_loss=16.78139 | best_loss=16.78139
Epoch 76/80: current_loss=16.62708 | best_loss=16.62708
Epoch 77/80: current_loss=16.47294 | best_loss=16.47294
Epoch 78/80: current_loss=16.32412 | best_loss=16.32412
Epoch 79/80: current_loss=16.17575 | best_loss=16.17575
      explained_var=-0.12884 | mse_loss=15.75806

----------------------------------------------
Params for Trial 67
{'learning_rate': 0.0001, 'weight_decay': 0.009954558540959658, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=83.85113 | best_loss=83.85113
Epoch 1/80: current_loss=82.59049 | best_loss=82.59049
Epoch 2/80: current_loss=81.32281 | best_loss=81.32281
Epoch 3/80: current_loss=79.99038 | best_loss=79.99038
Epoch 4/80: current_loss=78.48821 | best_loss=78.48821
Epoch 5/80: current_loss=76.74205 | best_loss=76.74205
Epoch 6/80: current_loss=74.65753 | best_loss=74.65753
Epoch 7/80: current_loss=72.19434 | best_loss=72.19434
Epoch 8/80: current_loss=69.31797 | best_loss=69.31797
Epoch 9/80: current_loss=66.12640 | best_loss=66.12640
Epoch 10/80: current_loss=62.81462 | best_loss=62.81462
Epoch 11/80: current_loss=59.58417 | best_loss=59.58417
Epoch 12/80: current_loss=56.55522 | best_loss=56.55522
Epoch 13/80: current_loss=53.81618 | best_loss=53.81618
Epoch 14/80: current_loss=51.42171 | best_loss=51.42171
Epoch 15/80: current_loss=49.29858 | best_loss=49.29858
Epoch 16/80: current_loss=47.44754 | best_loss=47.44754
Epoch 17/80: current_loss=45.81973 | best_loss=45.81973
Epoch 18/80: current_loss=44.40279 | best_loss=44.40279
Epoch 19/80: current_loss=43.14137 | best_loss=43.14137
Epoch 20/80: current_loss=42.00678 | best_loss=42.00678
Epoch 21/80: current_loss=40.96214 | best_loss=40.96214
Epoch 22/80: current_loss=40.01625 | best_loss=40.01625
Epoch 23/80: current_loss=39.13128 | best_loss=39.13128
Epoch 24/80: current_loss=38.30258 | best_loss=38.30258
Epoch 25/80: current_loss=37.53119 | best_loss=37.53119
Epoch 26/80: current_loss=36.80232 | best_loss=36.80232
Epoch 27/80: current_loss=36.11472 | best_loss=36.11472
Epoch 28/80: current_loss=35.45219 | best_loss=35.45219
Epoch 29/80: current_loss=34.80931 | best_loss=34.80931
Epoch 30/80: current_loss=34.20104 | best_loss=34.20104
Epoch 31/80: current_loss=33.60954 | best_loss=33.60954
Epoch 32/80: current_loss=33.04960 | best_loss=33.04960
Epoch 33/80: current_loss=32.50516 | best_loss=32.50516
Epoch 34/80: current_loss=31.98393 | best_loss=31.98393
Epoch 35/80: current_loss=31.47919 | best_loss=31.47919
Epoch 36/80: current_loss=30.98210 | best_loss=30.98210
Epoch 37/80: current_loss=30.50683 | best_loss=30.50683
Epoch 38/80: current_loss=30.03632 | best_loss=30.03632
Epoch 39/80: current_loss=29.57901 | best_loss=29.57901
Epoch 40/80: current_loss=29.13385 | best_loss=29.13385
Epoch 41/80: current_loss=28.70698 | best_loss=28.70698
Epoch 42/80: current_loss=28.28442 | best_loss=28.28442
Epoch 43/80: current_loss=27.86186 | best_loss=27.86186
Epoch 44/80: current_loss=27.46251 | best_loss=27.46251
Epoch 45/80: current_loss=27.06619 | best_loss=27.06619
Epoch 46/80: current_loss=26.68048 | best_loss=26.68048
Epoch 47/80: current_loss=26.30584 | best_loss=26.30584
Epoch 48/80: current_loss=25.93726 | best_loss=25.93726
Epoch 49/80: current_loss=25.57474 | best_loss=25.57474
Epoch 50/80: current_loss=25.22599 | best_loss=25.22599
Epoch 51/80: current_loss=24.87855 | best_loss=24.87855
Epoch 52/80: current_loss=24.53901 | best_loss=24.53901
Epoch 53/80: current_loss=24.20424 | best_loss=24.20424
Epoch 54/80: current_loss=23.88845 | best_loss=23.88845
Epoch 55/80: current_loss=23.56865 | best_loss=23.56865
Epoch 56/80: current_loss=23.24756 | best_loss=23.24756
Epoch 57/80: current_loss=22.93933 | best_loss=22.93933
Epoch 58/80: current_loss=22.63804 | best_loss=22.63804
Epoch 59/80: current_loss=22.33122 | best_loss=22.33122
Epoch 60/80: current_loss=22.04104 | best_loss=22.04104
Epoch 61/80: current_loss=21.75771 | best_loss=21.75771
Epoch 62/80: current_loss=21.47524 | best_loss=21.47524
Epoch 63/80: current_loss=21.19981 | best_loss=21.19981
Epoch 64/80: current_loss=20.92540 | best_loss=20.92540
Epoch 65/80: current_loss=20.66133 | best_loss=20.66133
Epoch 66/80: current_loss=20.39808 | best_loss=20.39808
Epoch 67/80: current_loss=20.14373 | best_loss=20.14373
Epoch 68/80: current_loss=19.89230 | best_loss=19.89230
Epoch 69/80: current_loss=19.64303 | best_loss=19.64303
Epoch 70/80: current_loss=19.40276 | best_loss=19.40276
Epoch 71/80: current_loss=19.16292 | best_loss=19.16292
Epoch 72/80: current_loss=18.92740 | best_loss=18.92740
Epoch 73/80: current_loss=18.69649 | best_loss=18.69649
Epoch 74/80: current_loss=18.46472 | best_loss=18.46472
Epoch 75/80: current_loss=18.24380 | best_loss=18.24380
Epoch 76/80: current_loss=18.02129 | best_loss=18.02129
Epoch 77/80: current_loss=17.80235 | best_loss=17.80235
Epoch 78/80: current_loss=17.58539 | best_loss=17.58539
Epoch 79/80: current_loss=17.37960 | best_loss=17.37960
      explained_var=0.00182 | mse_loss=16.88508

----------------------------------------------
Params for Trial 68
{'learning_rate': 0.1, 'weight_decay': 0.006721037980471418, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.26842 | best_loss=13.26842
Epoch 1/80: current_loss=8.19598 | best_loss=8.19598
Epoch 2/80: current_loss=9.16077 | best_loss=8.19598
Epoch 3/80: current_loss=8.30561 | best_loss=8.19598
Epoch 4/80: current_loss=9.40440 | best_loss=8.19598
Epoch 5/80: current_loss=9.85431 | best_loss=8.19598
Epoch 6/80: current_loss=8.98595 | best_loss=8.19598
Epoch 7/80: current_loss=8.00031 | best_loss=8.00031
Epoch 8/80: current_loss=7.73497 | best_loss=7.73497
Epoch 9/80: current_loss=8.14261 | best_loss=7.73497
Epoch 10/80: current_loss=10.80164 | best_loss=7.73497
Epoch 11/80: current_loss=12.21254 | best_loss=7.73497
Epoch 12/80: current_loss=13.47105 | best_loss=7.73497
Epoch 13/80: current_loss=15.39538 | best_loss=7.73497
Epoch 14/80: current_loss=9.07708 | best_loss=7.73497
Epoch 15/80: current_loss=13.29455 | best_loss=7.73497
Epoch 16/80: current_loss=7.96140 | best_loss=7.73497
Epoch 17/80: current_loss=7.95085 | best_loss=7.73497
Epoch 18/80: current_loss=7.77471 | best_loss=7.73497
Epoch 19/80: current_loss=10.04326 | best_loss=7.73497
Epoch 20/80: current_loss=12.73057 | best_loss=7.73497
Epoch 21/80: current_loss=7.80201 | best_loss=7.73497
Epoch 22/80: current_loss=8.89948 | best_loss=7.73497
Epoch 23/80: current_loss=8.78820 | best_loss=7.73497
Epoch 24/80: current_loss=7.76769 | best_loss=7.73497
Epoch 25/80: current_loss=9.17536 | best_loss=7.73497
Epoch 26/80: current_loss=9.62973 | best_loss=7.73497
Epoch 27/80: current_loss=10.98302 | best_loss=7.73497
Epoch 28/80: current_loss=7.76206 | best_loss=7.73497
Early Stopping at epoch 28
      explained_var=0.00487 | mse_loss=7.56194
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=28.44280 | best_loss=28.44280
Epoch 1/80: current_loss=11.07728 | best_loss=11.07728
Epoch 2/80: current_loss=8.93039 | best_loss=8.93039
Epoch 3/80: current_loss=8.37129 | best_loss=8.37129
Epoch 4/80: current_loss=8.67498 | best_loss=8.37129
Epoch 5/80: current_loss=8.41492 | best_loss=8.37129
Epoch 6/80: current_loss=9.92255 | best_loss=8.37129
Epoch 7/80: current_loss=9.48974 | best_loss=8.37129
Epoch 8/80: current_loss=8.29597 | best_loss=8.29597
Epoch 9/80: current_loss=8.82079 | best_loss=8.29597
Epoch 10/80: current_loss=9.16465 | best_loss=8.29597
Epoch 11/80: current_loss=9.16274 | best_loss=8.29597
Epoch 12/80: current_loss=11.52276 | best_loss=8.29597
Epoch 13/80: current_loss=17.85887 | best_loss=8.29597
Epoch 14/80: current_loss=9.54982 | best_loss=8.29597
Epoch 15/80: current_loss=8.29315 | best_loss=8.29315
Epoch 16/80: current_loss=9.70041 | best_loss=8.29315
Epoch 17/80: current_loss=9.20010 | best_loss=8.29315
Epoch 18/80: current_loss=10.06651 | best_loss=8.29315
Epoch 19/80: current_loss=8.29398 | best_loss=8.29315
Epoch 20/80: current_loss=10.39708 | best_loss=8.29315
Epoch 21/80: current_loss=11.14059 | best_loss=8.29315
Epoch 22/80: current_loss=10.90347 | best_loss=8.29315
Epoch 23/80: current_loss=9.71101 | best_loss=8.29315
Epoch 24/80: current_loss=8.69150 | best_loss=8.29315
Epoch 25/80: current_loss=9.69164 | best_loss=8.29315
Epoch 26/80: current_loss=10.12318 | best_loss=8.29315
Epoch 27/80: current_loss=11.27150 | best_loss=8.29315
Epoch 28/80: current_loss=9.78333 | best_loss=8.29315
Epoch 29/80: current_loss=8.34145 | best_loss=8.29315
Epoch 30/80: current_loss=9.17636 | best_loss=8.29315
Epoch 31/80: current_loss=9.41709 | best_loss=8.29315
Epoch 32/80: current_loss=8.36127 | best_loss=8.29315
Epoch 33/80: current_loss=13.50636 | best_loss=8.29315
Epoch 34/80: current_loss=9.71947 | best_loss=8.29315
Epoch 35/80: current_loss=9.10742 | best_loss=8.29315
Early Stopping at epoch 35
      explained_var=0.00782 | mse_loss=8.13303
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.89699 | best_loss=13.89699
Epoch 1/80: current_loss=9.12992 | best_loss=9.12992
Epoch 2/80: current_loss=10.77104 | best_loss=9.12992
Epoch 3/80: current_loss=9.79341 | best_loss=9.12992
Epoch 4/80: current_loss=9.59463 | best_loss=9.12992
Epoch 5/80: current_loss=22.98748 | best_loss=9.12992
Epoch 6/80: current_loss=13.37117 | best_loss=9.12992
Epoch 7/80: current_loss=8.65444 | best_loss=8.65444
Epoch 8/80: current_loss=16.34769 | best_loss=8.65444
Epoch 9/80: current_loss=9.58433 | best_loss=8.65444
Epoch 10/80: current_loss=9.99615 | best_loss=8.65444
Epoch 11/80: current_loss=9.21289 | best_loss=8.65444
Epoch 12/80: current_loss=8.99964 | best_loss=8.65444
Epoch 13/80: current_loss=10.89171 | best_loss=8.65444
Epoch 14/80: current_loss=9.24042 | best_loss=8.65444
Epoch 15/80: current_loss=9.21205 | best_loss=8.65444
Epoch 16/80: current_loss=12.40940 | best_loss=8.65444
Epoch 17/80: current_loss=9.39528 | best_loss=8.65444
Epoch 18/80: current_loss=9.48637 | best_loss=8.65444
Epoch 19/80: current_loss=10.09092 | best_loss=8.65444
Epoch 20/80: current_loss=8.54021 | best_loss=8.54021
Epoch 21/80: current_loss=9.86569 | best_loss=8.54021
Epoch 22/80: current_loss=12.24502 | best_loss=8.54021
Epoch 23/80: current_loss=10.37448 | best_loss=8.54021
Epoch 24/80: current_loss=8.94186 | best_loss=8.54021
Epoch 25/80: current_loss=9.11314 | best_loss=8.54021
Epoch 26/80: current_loss=9.18041 | best_loss=8.54021
Epoch 27/80: current_loss=10.05818 | best_loss=8.54021
Epoch 28/80: current_loss=10.98790 | best_loss=8.54021
Epoch 29/80: current_loss=13.23400 | best_loss=8.54021
Epoch 30/80: current_loss=9.59704 | best_loss=8.54021
Epoch 31/80: current_loss=9.32262 | best_loss=8.54021
Epoch 32/80: current_loss=9.61612 | best_loss=8.54021
Epoch 33/80: current_loss=12.05000 | best_loss=8.54021
Epoch 34/80: current_loss=8.91391 | best_loss=8.54021
Epoch 35/80: current_loss=9.20041 | best_loss=8.54021
Epoch 36/80: current_loss=9.85414 | best_loss=8.54021
Epoch 37/80: current_loss=16.16280 | best_loss=8.54021
Epoch 38/80: current_loss=8.85211 | best_loss=8.54021
Epoch 39/80: current_loss=8.84464 | best_loss=8.54021
Epoch 40/80: current_loss=12.70656 | best_loss=8.54021
Early Stopping at epoch 40
      explained_var=0.02219 | mse_loss=8.37953
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.22866 | best_loss=10.22866
Epoch 1/80: current_loss=9.04398 | best_loss=9.04398
Epoch 2/80: current_loss=8.76170 | best_loss=8.76170
Epoch 3/80: current_loss=8.28900 | best_loss=8.28900
Epoch 4/80: current_loss=90.64160 | best_loss=8.28900
Epoch 5/80: current_loss=24.62588 | best_loss=8.28900
Epoch 6/80: current_loss=15.05938 | best_loss=8.28900
Epoch 7/80: current_loss=10.01007 | best_loss=8.28900
Epoch 8/80: current_loss=10.88974 | best_loss=8.28900
Epoch 9/80: current_loss=8.45522 | best_loss=8.28900
Epoch 10/80: current_loss=11.32406 | best_loss=8.28900
Epoch 11/80: current_loss=11.92947 | best_loss=8.28900
Epoch 12/80: current_loss=9.83114 | best_loss=8.28900
Epoch 13/80: current_loss=9.40361 | best_loss=8.28900
Epoch 14/80: current_loss=9.18962 | best_loss=8.28900
Epoch 15/80: current_loss=15.13270 | best_loss=8.28900
Epoch 16/80: current_loss=8.50043 | best_loss=8.28900
Epoch 17/80: current_loss=12.73903 | best_loss=8.28900
Epoch 18/80: current_loss=8.35628 | best_loss=8.28900
Epoch 19/80: current_loss=9.55773 | best_loss=8.28900
Epoch 20/80: current_loss=9.32708 | best_loss=8.28900
Epoch 21/80: current_loss=9.02893 | best_loss=8.28900
Epoch 22/80: current_loss=13.11038 | best_loss=8.28900
Epoch 23/80: current_loss=8.25203 | best_loss=8.25203
Epoch 24/80: current_loss=9.22005 | best_loss=8.25203
Epoch 25/80: current_loss=8.49371 | best_loss=8.25203
Epoch 26/80: current_loss=8.97010 | best_loss=8.25203
Epoch 27/80: current_loss=9.05909 | best_loss=8.25203
Epoch 28/80: current_loss=10.62251 | best_loss=8.25203
Epoch 29/80: current_loss=11.14105 | best_loss=8.25203
Epoch 30/80: current_loss=11.80956 | best_loss=8.25203
Epoch 31/80: current_loss=9.41055 | best_loss=8.25203
Epoch 32/80: current_loss=8.57474 | best_loss=8.25203
Epoch 33/80: current_loss=9.63976 | best_loss=8.25203
Epoch 34/80: current_loss=8.24699 | best_loss=8.24699
Epoch 35/80: current_loss=8.66725 | best_loss=8.24699
Epoch 36/80: current_loss=8.59544 | best_loss=8.24699
Epoch 37/80: current_loss=8.24116 | best_loss=8.24116
Epoch 38/80: current_loss=8.56385 | best_loss=8.24116
Epoch 39/80: current_loss=8.33011 | best_loss=8.24116
Epoch 40/80: current_loss=8.52973 | best_loss=8.24116
Epoch 41/80: current_loss=9.30447 | best_loss=8.24116
Epoch 42/80: current_loss=8.34269 | best_loss=8.24116
Epoch 43/80: current_loss=8.28557 | best_loss=8.24116
Epoch 44/80: current_loss=8.84432 | best_loss=8.24116
Epoch 45/80: current_loss=16.25998 | best_loss=8.24116
Epoch 46/80: current_loss=17.16874 | best_loss=8.24116
Epoch 47/80: current_loss=8.38009 | best_loss=8.24116
Epoch 48/80: current_loss=12.61715 | best_loss=8.24116
Epoch 49/80: current_loss=8.34648 | best_loss=8.24116
Epoch 50/80: current_loss=9.03213 | best_loss=8.24116
Epoch 51/80: current_loss=10.72718 | best_loss=8.24116
Epoch 52/80: current_loss=11.97021 | best_loss=8.24116
Epoch 53/80: current_loss=8.72002 | best_loss=8.24116
Epoch 54/80: current_loss=8.32904 | best_loss=8.24116
Epoch 55/80: current_loss=8.92697 | best_loss=8.24116
Epoch 56/80: current_loss=8.33052 | best_loss=8.24116
Epoch 57/80: current_loss=10.40010 | best_loss=8.24116
Early Stopping at epoch 57
      explained_var=0.00298 | mse_loss=8.32971
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.35883 | best_loss=8.35883
Epoch 1/80: current_loss=8.50761 | best_loss=8.35883
Epoch 2/80: current_loss=9.59879 | best_loss=8.35883
Epoch 3/80: current_loss=15.03739 | best_loss=8.35883
Epoch 4/80: current_loss=8.55541 | best_loss=8.35883
Epoch 5/80: current_loss=9.31592 | best_loss=8.35883
Epoch 6/80: current_loss=8.55547 | best_loss=8.35883
Epoch 7/80: current_loss=7.98689 | best_loss=7.98689
Epoch 8/80: current_loss=18.37484 | best_loss=7.98689
Epoch 9/80: current_loss=8.05716 | best_loss=7.98689
Epoch 10/80: current_loss=8.65445 | best_loss=7.98689
Epoch 11/80: current_loss=11.32580 | best_loss=7.98689
Epoch 12/80: current_loss=7.61350 | best_loss=7.61350
Epoch 13/80: current_loss=9.72877 | best_loss=7.61350
Epoch 14/80: current_loss=8.48630 | best_loss=7.61350
Epoch 15/80: current_loss=9.15362 | best_loss=7.61350
Epoch 16/80: current_loss=9.66956 | best_loss=7.61350
Epoch 17/80: current_loss=8.71175 | best_loss=7.61350
Epoch 18/80: current_loss=8.55311 | best_loss=7.61350
Epoch 19/80: current_loss=7.83118 | best_loss=7.61350
Epoch 20/80: current_loss=9.36031 | best_loss=7.61350
Epoch 21/80: current_loss=14.43411 | best_loss=7.61350
Epoch 22/80: current_loss=8.20241 | best_loss=7.61350
Epoch 23/80: current_loss=7.99937 | best_loss=7.61350
Epoch 24/80: current_loss=14.61674 | best_loss=7.61350
Epoch 25/80: current_loss=10.31865 | best_loss=7.61350
Epoch 26/80: current_loss=9.37416 | best_loss=7.61350
Epoch 27/80: current_loss=9.74219 | best_loss=7.61350
Epoch 28/80: current_loss=10.64138 | best_loss=7.61350
Epoch 29/80: current_loss=11.23793 | best_loss=7.61350
Epoch 30/80: current_loss=8.36590 | best_loss=7.61350
Epoch 31/80: current_loss=8.19984 | best_loss=7.61350
Epoch 32/80: current_loss=7.96103 | best_loss=7.61350
Early Stopping at epoch 32
      explained_var=0.00499 | mse_loss=7.81643
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00857| avg_loss=8.04413
----------------------------------------------


----------------------------------------------
Params for Trial 69
{'learning_rate': 0.1, 'weight_decay': 0.008502514508871024, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.13166 | best_loss=9.13166
Epoch 1/80: current_loss=11.90479 | best_loss=9.13166
Epoch 2/80: current_loss=9.54132 | best_loss=9.13166
Epoch 3/80: current_loss=8.15696 | best_loss=8.15696
Epoch 4/80: current_loss=10.76008 | best_loss=8.15696
Epoch 5/80: current_loss=9.88076 | best_loss=8.15696
Epoch 6/80: current_loss=9.55276 | best_loss=8.15696
Epoch 7/80: current_loss=8.50646 | best_loss=8.15696
Epoch 8/80: current_loss=8.48538 | best_loss=8.15696
Epoch 9/80: current_loss=8.18630 | best_loss=8.15696
Epoch 10/80: current_loss=10.67208 | best_loss=8.15696
Epoch 11/80: current_loss=9.24276 | best_loss=8.15696
Epoch 12/80: current_loss=8.14503 | best_loss=8.14503
Epoch 13/80: current_loss=9.31322 | best_loss=8.14503
Epoch 14/80: current_loss=10.80649 | best_loss=8.14503
Epoch 15/80: current_loss=7.73045 | best_loss=7.73045
Epoch 16/80: current_loss=7.79515 | best_loss=7.73045
Epoch 17/80: current_loss=9.27737 | best_loss=7.73045
Epoch 18/80: current_loss=8.02312 | best_loss=7.73045
Epoch 19/80: current_loss=7.90848 | best_loss=7.73045
Epoch 20/80: current_loss=14.92372 | best_loss=7.73045
Epoch 21/80: current_loss=16.97368 | best_loss=7.73045
Epoch 22/80: current_loss=9.04925 | best_loss=7.73045
Epoch 23/80: current_loss=10.19114 | best_loss=7.73045
Epoch 24/80: current_loss=11.96763 | best_loss=7.73045
Epoch 25/80: current_loss=10.22794 | best_loss=7.73045
Epoch 26/80: current_loss=8.07157 | best_loss=7.73045
Epoch 27/80: current_loss=11.49763 | best_loss=7.73045
Epoch 28/80: current_loss=7.89812 | best_loss=7.73045
Epoch 29/80: current_loss=8.43001 | best_loss=7.73045
Epoch 30/80: current_loss=10.11980 | best_loss=7.73045
Epoch 31/80: current_loss=11.05378 | best_loss=7.73045
Epoch 32/80: current_loss=9.91374 | best_loss=7.73045
Epoch 33/80: current_loss=9.01776 | best_loss=7.73045
Epoch 34/80: current_loss=14.31755 | best_loss=7.73045
Epoch 35/80: current_loss=9.46802 | best_loss=7.73045
Early Stopping at epoch 35
      explained_var=0.00494 | mse_loss=7.54987
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.74999 | best_loss=8.74999
Epoch 1/80: current_loss=8.66652 | best_loss=8.66652
Epoch 2/80: current_loss=11.96673 | best_loss=8.66652
Epoch 3/80: current_loss=8.67010 | best_loss=8.66652
Epoch 4/80: current_loss=8.51208 | best_loss=8.51208
Epoch 5/80: current_loss=8.58569 | best_loss=8.51208
Epoch 6/80: current_loss=9.00764 | best_loss=8.51208
Epoch 7/80: current_loss=9.71284 | best_loss=8.51208
Epoch 8/80: current_loss=12.48602 | best_loss=8.51208
Epoch 9/80: current_loss=9.73727 | best_loss=8.51208
Epoch 10/80: current_loss=8.62557 | best_loss=8.51208
Epoch 11/80: current_loss=8.52380 | best_loss=8.51208
Epoch 12/80: current_loss=8.53075 | best_loss=8.51208
Epoch 13/80: current_loss=11.74833 | best_loss=8.51208
Epoch 14/80: current_loss=8.84787 | best_loss=8.51208
Epoch 15/80: current_loss=10.36210 | best_loss=8.51208
Epoch 16/80: current_loss=9.27156 | best_loss=8.51208
Epoch 17/80: current_loss=8.33260 | best_loss=8.33260
Epoch 18/80: current_loss=8.32900 | best_loss=8.32900
Epoch 19/80: current_loss=11.40308 | best_loss=8.32900
Epoch 20/80: current_loss=8.65023 | best_loss=8.32900
Epoch 21/80: current_loss=51.12023 | best_loss=8.32900
Epoch 22/80: current_loss=13.40303 | best_loss=8.32900
Epoch 23/80: current_loss=14.41387 | best_loss=8.32900
Epoch 24/80: current_loss=13.63052 | best_loss=8.32900
Epoch 25/80: current_loss=10.43187 | best_loss=8.32900
Epoch 26/80: current_loss=7.82189 | best_loss=7.82189
Epoch 27/80: current_loss=10.71946 | best_loss=7.82189
Epoch 28/80: current_loss=8.72959 | best_loss=7.82189
Epoch 29/80: current_loss=16.08377 | best_loss=7.82189
Epoch 30/80: current_loss=12.63245 | best_loss=7.82189
Epoch 31/80: current_loss=16.30038 | best_loss=7.82189
Epoch 32/80: current_loss=8.33687 | best_loss=7.82189
Epoch 33/80: current_loss=9.89984 | best_loss=7.82189
Epoch 34/80: current_loss=8.73768 | best_loss=7.82189
Epoch 35/80: current_loss=11.70237 | best_loss=7.82189
Epoch 36/80: current_loss=9.34718 | best_loss=7.82189
Epoch 37/80: current_loss=8.52770 | best_loss=7.82189
Epoch 38/80: current_loss=8.29649 | best_loss=7.82189
Epoch 39/80: current_loss=8.13127 | best_loss=7.82189
Epoch 40/80: current_loss=13.77563 | best_loss=7.82189
Epoch 41/80: current_loss=9.61762 | best_loss=7.82189
Epoch 42/80: current_loss=9.73588 | best_loss=7.82189
Epoch 43/80: current_loss=8.41808 | best_loss=7.82189
Epoch 44/80: current_loss=9.79644 | best_loss=7.82189
Epoch 45/80: current_loss=11.82027 | best_loss=7.82189
Epoch 46/80: current_loss=11.56972 | best_loss=7.82189
Early Stopping at epoch 46
      explained_var=0.07146 | mse_loss=7.68870
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.83969 | best_loss=11.83969
Epoch 1/80: current_loss=9.36806 | best_loss=9.36806
Epoch 2/80: current_loss=8.93638 | best_loss=8.93638
Epoch 3/80: current_loss=10.81159 | best_loss=8.93638
Epoch 4/80: current_loss=12.05321 | best_loss=8.93638
Epoch 5/80: current_loss=10.88983 | best_loss=8.93638
Epoch 6/80: current_loss=8.89443 | best_loss=8.89443
Epoch 7/80: current_loss=8.17461 | best_loss=8.17461
Epoch 8/80: current_loss=12.99278 | best_loss=8.17461
Epoch 9/80: current_loss=9.22000 | best_loss=8.17461
Epoch 10/80: current_loss=8.71464 | best_loss=8.17461
Epoch 11/80: current_loss=15.66378 | best_loss=8.17461
Epoch 12/80: current_loss=11.46462 | best_loss=8.17461
Epoch 13/80: current_loss=8.92852 | best_loss=8.17461
Epoch 14/80: current_loss=11.89879 | best_loss=8.17461
Epoch 15/80: current_loss=11.79077 | best_loss=8.17461
Epoch 16/80: current_loss=13.37971 | best_loss=8.17461
Epoch 17/80: current_loss=11.01297 | best_loss=8.17461
Epoch 18/80: current_loss=10.00934 | best_loss=8.17461
Epoch 19/80: current_loss=9.11142 | best_loss=8.17461
Epoch 20/80: current_loss=9.02818 | best_loss=8.17461
Epoch 21/80: current_loss=10.83303 | best_loss=8.17461
Epoch 22/80: current_loss=11.24839 | best_loss=8.17461
Epoch 23/80: current_loss=10.54967 | best_loss=8.17461
Epoch 24/80: current_loss=14.48903 | best_loss=8.17461
Epoch 25/80: current_loss=9.84847 | best_loss=8.17461
Epoch 26/80: current_loss=10.42181 | best_loss=8.17461
Epoch 27/80: current_loss=15.01633 | best_loss=8.17461
Early Stopping at epoch 27
      explained_var=0.06895 | mse_loss=7.89940
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.28339 | best_loss=14.28339
Epoch 1/80: current_loss=8.71992 | best_loss=8.71992
Epoch 2/80: current_loss=9.49840 | best_loss=8.71992
Epoch 3/80: current_loss=11.62338 | best_loss=8.71992
Epoch 4/80: current_loss=9.69478 | best_loss=8.71992
Epoch 5/80: current_loss=9.42281 | best_loss=8.71992
Epoch 6/80: current_loss=10.08459 | best_loss=8.71992
Epoch 7/80: current_loss=9.00510 | best_loss=8.71992
Epoch 8/80: current_loss=9.53939 | best_loss=8.71992
Epoch 9/80: current_loss=9.86824 | best_loss=8.71992
Epoch 10/80: current_loss=8.80705 | best_loss=8.71992
Epoch 11/80: current_loss=10.32120 | best_loss=8.71992
Epoch 12/80: current_loss=12.33934 | best_loss=8.71992
Epoch 13/80: current_loss=10.46424 | best_loss=8.71992
Epoch 14/80: current_loss=13.87106 | best_loss=8.71992
Epoch 15/80: current_loss=19.98080 | best_loss=8.71992
Epoch 16/80: current_loss=11.68223 | best_loss=8.71992
Epoch 17/80: current_loss=8.49047 | best_loss=8.49047
Epoch 18/80: current_loss=9.57016 | best_loss=8.49047
Epoch 19/80: current_loss=8.70659 | best_loss=8.49047
Epoch 20/80: current_loss=10.19925 | best_loss=8.49047
Epoch 21/80: current_loss=9.49453 | best_loss=8.49047
Epoch 22/80: current_loss=8.81752 | best_loss=8.49047
Epoch 23/80: current_loss=8.31926 | best_loss=8.31926
Epoch 24/80: current_loss=11.38533 | best_loss=8.31926
Epoch 25/80: current_loss=10.13878 | best_loss=8.31926
Epoch 26/80: current_loss=11.26142 | best_loss=8.31926
Epoch 27/80: current_loss=9.99299 | best_loss=8.31926
Epoch 28/80: current_loss=8.92729 | best_loss=8.31926
Epoch 29/80: current_loss=9.07705 | best_loss=8.31926
Epoch 30/80: current_loss=9.11789 | best_loss=8.31926
Epoch 31/80: current_loss=10.56823 | best_loss=8.31926
Epoch 32/80: current_loss=8.44579 | best_loss=8.31926
Epoch 33/80: current_loss=8.31480 | best_loss=8.31480
Epoch 34/80: current_loss=10.92217 | best_loss=8.31480
Epoch 35/80: current_loss=8.29071 | best_loss=8.29071
Epoch 36/80: current_loss=15.51204 | best_loss=8.29071
Epoch 37/80: current_loss=19.12561 | best_loss=8.29071
Epoch 38/80: current_loss=8.25309 | best_loss=8.25309
Epoch 39/80: current_loss=8.26407 | best_loss=8.25309
Epoch 40/80: current_loss=8.58685 | best_loss=8.25309
Epoch 41/80: current_loss=9.59781 | best_loss=8.25309
Epoch 42/80: current_loss=13.04705 | best_loss=8.25309
Epoch 43/80: current_loss=9.29732 | best_loss=8.25309
Epoch 44/80: current_loss=8.87530 | best_loss=8.25309
Epoch 45/80: current_loss=10.32889 | best_loss=8.25309
Epoch 46/80: current_loss=12.30193 | best_loss=8.25309
Epoch 47/80: current_loss=8.82298 | best_loss=8.25309
Epoch 48/80: current_loss=8.60691 | best_loss=8.25309
Epoch 49/80: current_loss=14.91603 | best_loss=8.25309
Epoch 50/80: current_loss=11.16593 | best_loss=8.25309
Epoch 51/80: current_loss=8.40292 | best_loss=8.25309
Epoch 52/80: current_loss=9.03825 | best_loss=8.25309
Epoch 53/80: current_loss=8.75334 | best_loss=8.25309
Epoch 54/80: current_loss=8.16951 | best_loss=8.16951
Epoch 55/80: current_loss=8.35946 | best_loss=8.16951
Epoch 56/80: current_loss=14.01890 | best_loss=8.16951
Epoch 57/80: current_loss=8.53268 | best_loss=8.16951
Epoch 58/80: current_loss=16.59265 | best_loss=8.16951
Epoch 59/80: current_loss=12.95453 | best_loss=8.16951
Epoch 60/80: current_loss=13.68413 | best_loss=8.16951
Epoch 61/80: current_loss=21.54306 | best_loss=8.16951
Epoch 62/80: current_loss=13.46841 | best_loss=8.16951
Epoch 63/80: current_loss=10.76580 | best_loss=8.16951
Epoch 64/80: current_loss=8.80031 | best_loss=8.16951
Epoch 65/80: current_loss=12.50277 | best_loss=8.16951
Epoch 66/80: current_loss=8.58935 | best_loss=8.16951
Epoch 67/80: current_loss=11.93087 | best_loss=8.16951
Epoch 68/80: current_loss=8.66759 | best_loss=8.16951
Epoch 69/80: current_loss=8.99614 | best_loss=8.16951
Epoch 70/80: current_loss=15.34814 | best_loss=8.16951
Epoch 71/80: current_loss=13.85729 | best_loss=8.16951
Epoch 72/80: current_loss=8.56709 | best_loss=8.16951
Epoch 73/80: current_loss=9.65837 | best_loss=8.16951
Epoch 74/80: current_loss=9.04516 | best_loss=8.16951
Early Stopping at epoch 74
      explained_var=0.00817 | mse_loss=8.27595
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.81263 | best_loss=8.81263
Epoch 1/80: current_loss=8.29354 | best_loss=8.29354
Epoch 2/80: current_loss=9.06805 | best_loss=8.29354
Epoch 3/80: current_loss=9.62020 | best_loss=8.29354
Epoch 4/80: current_loss=9.20437 | best_loss=8.29354
Epoch 5/80: current_loss=7.95280 | best_loss=7.95280
Epoch 6/80: current_loss=7.79215 | best_loss=7.79215
Epoch 7/80: current_loss=8.20366 | best_loss=7.79215
Epoch 8/80: current_loss=7.68326 | best_loss=7.68326
Epoch 9/80: current_loss=8.56721 | best_loss=7.68326
Epoch 10/80: current_loss=7.65940 | best_loss=7.65940
Epoch 11/80: current_loss=9.27525 | best_loss=7.65940
Epoch 12/80: current_loss=7.72879 | best_loss=7.65940
Epoch 13/80: current_loss=12.30584 | best_loss=7.65940
Epoch 14/80: current_loss=8.11396 | best_loss=7.65940
Epoch 15/80: current_loss=7.71652 | best_loss=7.65940
Epoch 16/80: current_loss=9.00659 | best_loss=7.65940
Epoch 17/80: current_loss=19.36765 | best_loss=7.65940
Epoch 18/80: current_loss=10.08701 | best_loss=7.65940
Epoch 19/80: current_loss=7.69965 | best_loss=7.65940
Epoch 20/80: current_loss=12.65472 | best_loss=7.65940
Epoch 21/80: current_loss=7.84739 | best_loss=7.65940
Epoch 22/80: current_loss=7.79964 | best_loss=7.65940
Epoch 23/80: current_loss=7.68551 | best_loss=7.65940
Epoch 24/80: current_loss=7.83992 | best_loss=7.65940
Epoch 25/80: current_loss=9.07411 | best_loss=7.65940
Epoch 26/80: current_loss=9.11224 | best_loss=7.65940
Epoch 27/80: current_loss=14.46953 | best_loss=7.65940
Epoch 28/80: current_loss=8.29121 | best_loss=7.65940
Epoch 29/80: current_loss=8.94583 | best_loss=7.65940
Epoch 30/80: current_loss=7.78944 | best_loss=7.65940
Early Stopping at epoch 30
      explained_var=0.00173 | mse_loss=7.84272
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=0.03105| avg_loss=7.85133
----------------------------------------------


----------------------------------------------
Params for Trial 70
{'learning_rate': 0.1, 'weight_decay': 0.00855026624295477, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=20.63273 | best_loss=20.63273
Epoch 1/80: current_loss=10.74068 | best_loss=10.74068
Epoch 2/80: current_loss=8.09615 | best_loss=8.09615
Epoch 3/80: current_loss=8.37345 | best_loss=8.09615
Epoch 4/80: current_loss=11.17073 | best_loss=8.09615
Epoch 5/80: current_loss=9.59898 | best_loss=8.09615
Epoch 6/80: current_loss=11.58628 | best_loss=8.09615
Epoch 7/80: current_loss=9.65126 | best_loss=8.09615
Epoch 8/80: current_loss=10.15635 | best_loss=8.09615
Epoch 9/80: current_loss=8.19442 | best_loss=8.09615
Epoch 10/80: current_loss=10.78061 | best_loss=8.09615
Epoch 11/80: current_loss=8.45196 | best_loss=8.09615
Epoch 12/80: current_loss=26.96144 | best_loss=8.09615
Epoch 13/80: current_loss=7.97030 | best_loss=7.97030
Epoch 14/80: current_loss=11.06276 | best_loss=7.97030
Epoch 15/80: current_loss=9.10367 | best_loss=7.97030
Epoch 16/80: current_loss=12.68752 | best_loss=7.97030
Epoch 17/80: current_loss=10.03921 | best_loss=7.97030
Epoch 18/80: current_loss=9.35985 | best_loss=7.97030
Epoch 19/80: current_loss=10.99982 | best_loss=7.97030
Epoch 20/80: current_loss=10.01879 | best_loss=7.97030
Epoch 21/80: current_loss=10.44630 | best_loss=7.97030
Epoch 22/80: current_loss=17.96034 | best_loss=7.97030
Epoch 23/80: current_loss=25.54844 | best_loss=7.97030
Epoch 24/80: current_loss=16.12311 | best_loss=7.97030
Epoch 25/80: current_loss=8.77379 | best_loss=7.97030
Epoch 26/80: current_loss=11.96959 | best_loss=7.97030
Epoch 27/80: current_loss=12.26131 | best_loss=7.97030
Epoch 28/80: current_loss=11.71983 | best_loss=7.97030
Epoch 29/80: current_loss=8.62835 | best_loss=7.97030
Epoch 30/80: current_loss=8.25913 | best_loss=7.97030
Epoch 31/80: current_loss=8.23937 | best_loss=7.97030
Epoch 32/80: current_loss=23.94010 | best_loss=7.97030
Epoch 33/80: current_loss=8.14759 | best_loss=7.97030
Early Stopping at epoch 33
      explained_var=-0.02642 | mse_loss=7.80761

----------------------------------------------
Params for Trial 71
{'learning_rate': 0.1, 'weight_decay': 0.009259713585183044, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.76619 | best_loss=13.76619
Epoch 1/80: current_loss=8.38741 | best_loss=8.38741
Epoch 2/80: current_loss=9.46000 | best_loss=8.38741
Epoch 3/80: current_loss=8.82815 | best_loss=8.38741
Epoch 4/80: current_loss=8.05984 | best_loss=8.05984
Epoch 5/80: current_loss=9.50224 | best_loss=8.05984
Epoch 6/80: current_loss=7.79923 | best_loss=7.79923
Epoch 7/80: current_loss=9.91850 | best_loss=7.79923
Epoch 8/80: current_loss=8.33306 | best_loss=7.79923
Epoch 9/80: current_loss=10.91252 | best_loss=7.79923
Epoch 10/80: current_loss=7.85980 | best_loss=7.79923
Epoch 11/80: current_loss=25.05612 | best_loss=7.79923
Epoch 12/80: current_loss=22.33409 | best_loss=7.79923
Epoch 13/80: current_loss=8.89913 | best_loss=7.79923
Epoch 14/80: current_loss=7.98649 | best_loss=7.79923
Epoch 15/80: current_loss=9.09908 | best_loss=7.79923
Epoch 16/80: current_loss=8.08451 | best_loss=7.79923
Epoch 17/80: current_loss=7.82772 | best_loss=7.79923
Epoch 18/80: current_loss=9.57705 | best_loss=7.79923
Epoch 19/80: current_loss=7.97068 | best_loss=7.79923
Epoch 20/80: current_loss=8.65972 | best_loss=7.79923
Epoch 21/80: current_loss=8.21285 | best_loss=7.79923
Epoch 22/80: current_loss=8.01860 | best_loss=7.79923
Epoch 23/80: current_loss=12.89893 | best_loss=7.79923
Epoch 24/80: current_loss=9.13481 | best_loss=7.79923
Epoch 25/80: current_loss=7.81565 | best_loss=7.79923
Epoch 26/80: current_loss=8.49063 | best_loss=7.79923
Early Stopping at epoch 26
      explained_var=-0.00529 | mse_loss=7.63559

----------------------------------------------
Params for Trial 72
{'learning_rate': 0.1, 'weight_decay': 0.007293519849191989, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=21.36266 | best_loss=21.36266
Epoch 1/80: current_loss=8.45535 | best_loss=8.45535
Epoch 2/80: current_loss=7.97551 | best_loss=7.97551
Epoch 3/80: current_loss=19.22658 | best_loss=7.97551
Epoch 4/80: current_loss=7.84709 | best_loss=7.84709
Epoch 5/80: current_loss=8.46029 | best_loss=7.84709
Epoch 6/80: current_loss=8.24000 | best_loss=7.84709
Epoch 7/80: current_loss=9.52952 | best_loss=7.84709
Epoch 8/80: current_loss=7.78105 | best_loss=7.78105
Epoch 9/80: current_loss=8.44436 | best_loss=7.78105
Epoch 10/80: current_loss=9.18766 | best_loss=7.78105
Epoch 11/80: current_loss=10.24316 | best_loss=7.78105
Epoch 12/80: current_loss=11.13526 | best_loss=7.78105
Epoch 13/80: current_loss=8.12467 | best_loss=7.78105
Epoch 14/80: current_loss=7.85331 | best_loss=7.78105
Epoch 15/80: current_loss=8.97758 | best_loss=7.78105
Epoch 16/80: current_loss=7.84339 | best_loss=7.78105
Epoch 17/80: current_loss=8.39876 | best_loss=7.78105
Epoch 18/80: current_loss=14.45013 | best_loss=7.78105
Epoch 19/80: current_loss=16.88330 | best_loss=7.78105
Epoch 20/80: current_loss=8.08036 | best_loss=7.78105
Epoch 21/80: current_loss=13.27537 | best_loss=7.78105
Epoch 22/80: current_loss=7.92415 | best_loss=7.78105
Epoch 23/80: current_loss=7.84693 | best_loss=7.78105
Epoch 24/80: current_loss=8.77533 | best_loss=7.78105
Epoch 25/80: current_loss=9.11440 | best_loss=7.78105
Epoch 26/80: current_loss=8.66856 | best_loss=7.78105
Epoch 27/80: current_loss=10.05595 | best_loss=7.78105
Epoch 28/80: current_loss=8.54692 | best_loss=7.78105
Early Stopping at epoch 28
      explained_var=0.00033 | mse_loss=7.61075

----------------------------------------------
Params for Trial 73
{'learning_rate': 0.1, 'weight_decay': 0.009628829903029086, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.11995 | best_loss=8.11995
Epoch 1/80: current_loss=7.94285 | best_loss=7.94285
Epoch 2/80: current_loss=9.52938 | best_loss=7.94285
Epoch 3/80: current_loss=9.04233 | best_loss=7.94285
Epoch 4/80: current_loss=9.80694 | best_loss=7.94285
Epoch 5/80: current_loss=13.88729 | best_loss=7.94285
Epoch 6/80: current_loss=8.02256 | best_loss=7.94285
Epoch 7/80: current_loss=8.22200 | best_loss=7.94285
Epoch 8/80: current_loss=9.52955 | best_loss=7.94285
Epoch 9/80: current_loss=8.39946 | best_loss=7.94285
Epoch 10/80: current_loss=10.77699 | best_loss=7.94285
Epoch 11/80: current_loss=10.17169 | best_loss=7.94285
Epoch 12/80: current_loss=8.77702 | best_loss=7.94285
Epoch 13/80: current_loss=8.82492 | best_loss=7.94285
Epoch 14/80: current_loss=32.87707 | best_loss=7.94285
Epoch 15/80: current_loss=22.51628 | best_loss=7.94285
Epoch 16/80: current_loss=15.23904 | best_loss=7.94285
Epoch 17/80: current_loss=9.38577 | best_loss=7.94285
Epoch 18/80: current_loss=13.26777 | best_loss=7.94285
Epoch 19/80: current_loss=8.74354 | best_loss=7.94285
Epoch 20/80: current_loss=7.79997 | best_loss=7.79997
Epoch 21/80: current_loss=8.44964 | best_loss=7.79997
Epoch 22/80: current_loss=13.28498 | best_loss=7.79997
Epoch 23/80: current_loss=8.74017 | best_loss=7.79997
Epoch 24/80: current_loss=8.52794 | best_loss=7.79997
Epoch 25/80: current_loss=9.42035 | best_loss=7.79997
Epoch 26/80: current_loss=8.67644 | best_loss=7.79997
Epoch 27/80: current_loss=8.65103 | best_loss=7.79997
Epoch 28/80: current_loss=12.33699 | best_loss=7.79997
Epoch 29/80: current_loss=13.29157 | best_loss=7.79997
Epoch 30/80: current_loss=9.63888 | best_loss=7.79997
Epoch 31/80: current_loss=7.87423 | best_loss=7.79997
Epoch 32/80: current_loss=8.72656 | best_loss=7.79997
Epoch 33/80: current_loss=10.62324 | best_loss=7.79997
Epoch 34/80: current_loss=8.83357 | best_loss=7.79997
Epoch 35/80: current_loss=7.90710 | best_loss=7.79997
Epoch 36/80: current_loss=10.49166 | best_loss=7.79997
Epoch 37/80: current_loss=10.86531 | best_loss=7.79997
Epoch 38/80: current_loss=11.25581 | best_loss=7.79997
Epoch 39/80: current_loss=9.78054 | best_loss=7.79997
Epoch 40/80: current_loss=8.52212 | best_loss=7.79997
Early Stopping at epoch 40
      explained_var=0.00568 | mse_loss=7.61468

----------------------------------------------
Params for Trial 74
{'learning_rate': 0.001, 'weight_decay': 0.008557764551919485, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=19.27305 | best_loss=19.27305
Epoch 1/80: current_loss=10.04096 | best_loss=10.04096
Epoch 2/80: current_loss=8.51443 | best_loss=8.51443
Epoch 3/80: current_loss=8.87882 | best_loss=8.51443
Epoch 4/80: current_loss=9.02872 | best_loss=8.51443
Epoch 5/80: current_loss=8.58846 | best_loss=8.51443
Epoch 6/80: current_loss=8.39104 | best_loss=8.39104
Epoch 7/80: current_loss=8.32636 | best_loss=8.32636
Epoch 8/80: current_loss=8.22394 | best_loss=8.22394
Epoch 9/80: current_loss=8.15055 | best_loss=8.15055
Epoch 10/80: current_loss=8.07769 | best_loss=8.07769
Epoch 11/80: current_loss=8.24286 | best_loss=8.07769
Epoch 12/80: current_loss=8.16911 | best_loss=8.07769
Epoch 13/80: current_loss=8.11883 | best_loss=8.07769
Epoch 14/80: current_loss=8.02392 | best_loss=8.02392
Epoch 15/80: current_loss=8.01928 | best_loss=8.01928
Epoch 16/80: current_loss=8.10156 | best_loss=8.01928
Epoch 17/80: current_loss=8.09232 | best_loss=8.01928
Epoch 18/80: current_loss=8.08499 | best_loss=8.01928
Epoch 19/80: current_loss=8.01435 | best_loss=8.01435
Epoch 20/80: current_loss=8.04541 | best_loss=8.01435
Epoch 21/80: current_loss=8.05614 | best_loss=8.01435
Epoch 22/80: current_loss=7.95443 | best_loss=7.95443
Epoch 23/80: current_loss=7.95275 | best_loss=7.95275
Epoch 24/80: current_loss=8.04052 | best_loss=7.95275
Epoch 25/80: current_loss=8.16517 | best_loss=7.95275
Epoch 26/80: current_loss=8.08178 | best_loss=7.95275
Epoch 27/80: current_loss=8.09317 | best_loss=7.95275
Epoch 28/80: current_loss=8.15650 | best_loss=7.95275
Epoch 29/80: current_loss=8.21290 | best_loss=7.95275
Epoch 30/80: current_loss=8.08891 | best_loss=7.95275
Epoch 31/80: current_loss=7.90884 | best_loss=7.90884
Epoch 32/80: current_loss=7.90908 | best_loss=7.90884
Epoch 33/80: current_loss=7.93211 | best_loss=7.90884
Epoch 34/80: current_loss=7.97330 | best_loss=7.90884
Epoch 35/80: current_loss=7.97311 | best_loss=7.90884
Epoch 36/80: current_loss=8.04188 | best_loss=7.90884
Epoch 37/80: current_loss=7.98627 | best_loss=7.90884
Epoch 38/80: current_loss=8.05473 | best_loss=7.90884
Epoch 39/80: current_loss=7.93696 | best_loss=7.90884
Epoch 40/80: current_loss=7.92074 | best_loss=7.90884
Epoch 41/80: current_loss=7.91040 | best_loss=7.90884
Epoch 42/80: current_loss=8.10000 | best_loss=7.90884
Epoch 43/80: current_loss=8.08161 | best_loss=7.90884
Epoch 44/80: current_loss=8.02281 | best_loss=7.90884
Epoch 45/80: current_loss=7.93985 | best_loss=7.90884
Epoch 46/80: current_loss=8.04867 | best_loss=7.90884
Epoch 47/80: current_loss=8.01336 | best_loss=7.90884
Epoch 48/80: current_loss=7.92615 | best_loss=7.90884
Epoch 49/80: current_loss=7.97307 | best_loss=7.90884
Epoch 50/80: current_loss=8.00523 | best_loss=7.90884
Epoch 51/80: current_loss=7.92291 | best_loss=7.90884
Early Stopping at epoch 51
      explained_var=0.00471 | mse_loss=7.77375

----------------------------------------------
Params for Trial 75
{'learning_rate': 0.1, 'weight_decay': 0.007646484681293494, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=17.55343 | best_loss=17.55343
Epoch 1/80: current_loss=19.97362 | best_loss=17.55343
Epoch 2/80: current_loss=11.03233 | best_loss=11.03233
Epoch 3/80: current_loss=7.81128 | best_loss=7.81128
Epoch 4/80: current_loss=8.37373 | best_loss=7.81128
Epoch 5/80: current_loss=7.85340 | best_loss=7.81128
Epoch 6/80: current_loss=8.78264 | best_loss=7.81128
Epoch 7/80: current_loss=8.43547 | best_loss=7.81128
Epoch 8/80: current_loss=9.94302 | best_loss=7.81128
Epoch 9/80: current_loss=8.47903 | best_loss=7.81128
Epoch 10/80: current_loss=8.09845 | best_loss=7.81128
Epoch 11/80: current_loss=8.09055 | best_loss=7.81128
Epoch 12/80: current_loss=10.51756 | best_loss=7.81128
Epoch 13/80: current_loss=9.79826 | best_loss=7.81128
Epoch 14/80: current_loss=9.45330 | best_loss=7.81128
Epoch 15/80: current_loss=10.96494 | best_loss=7.81128
Epoch 16/80: current_loss=7.84671 | best_loss=7.81128
Epoch 17/80: current_loss=11.88651 | best_loss=7.81128
Epoch 18/80: current_loss=12.31894 | best_loss=7.81128
Epoch 19/80: current_loss=11.87528 | best_loss=7.81128
Epoch 20/80: current_loss=7.80123 | best_loss=7.80123
Epoch 21/80: current_loss=10.64341 | best_loss=7.80123
Epoch 22/80: current_loss=15.70907 | best_loss=7.80123
Epoch 23/80: current_loss=8.31609 | best_loss=7.80123
Epoch 24/80: current_loss=13.75556 | best_loss=7.80123
Epoch 25/80: current_loss=11.96728 | best_loss=7.80123
Epoch 26/80: current_loss=11.84688 | best_loss=7.80123
Epoch 27/80: current_loss=12.83386 | best_loss=7.80123
Epoch 28/80: current_loss=14.18702 | best_loss=7.80123
Epoch 29/80: current_loss=9.68614 | best_loss=7.80123
Epoch 30/80: current_loss=9.11005 | best_loss=7.80123
Epoch 31/80: current_loss=10.11533 | best_loss=7.80123
Epoch 32/80: current_loss=9.42244 | best_loss=7.80123
Epoch 33/80: current_loss=10.45544 | best_loss=7.80123
Epoch 34/80: current_loss=23.27865 | best_loss=7.80123
Epoch 35/80: current_loss=10.18058 | best_loss=7.80123
Epoch 36/80: current_loss=10.83562 | best_loss=7.80123
Epoch 37/80: current_loss=10.59679 | best_loss=7.80123
Epoch 38/80: current_loss=8.45460 | best_loss=7.80123
Epoch 39/80: current_loss=9.57550 | best_loss=7.80123
Epoch 40/80: current_loss=8.38487 | best_loss=7.80123
Early Stopping at epoch 40
      explained_var=0.00345 | mse_loss=7.63565

----------------------------------------------
Params for Trial 76
{'learning_rate': 0.1, 'weight_decay': 0.008309185373577117, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.08533 | best_loss=11.08533
Epoch 1/80: current_loss=8.47622 | best_loss=8.47622
Epoch 2/80: current_loss=16.76856 | best_loss=8.47622
Epoch 3/80: current_loss=8.81933 | best_loss=8.47622
Epoch 4/80: current_loss=7.81909 | best_loss=7.81909
Epoch 5/80: current_loss=10.88040 | best_loss=7.81909
Epoch 6/80: current_loss=8.50998 | best_loss=7.81909
Epoch 7/80: current_loss=8.95013 | best_loss=7.81909
Epoch 8/80: current_loss=8.06777 | best_loss=7.81909
Epoch 9/80: current_loss=10.17150 | best_loss=7.81909
Epoch 10/80: current_loss=8.02029 | best_loss=7.81909
Epoch 11/80: current_loss=8.01206 | best_loss=7.81909
Epoch 12/80: current_loss=10.51392 | best_loss=7.81909
Epoch 13/80: current_loss=10.27425 | best_loss=7.81909
Epoch 14/80: current_loss=8.08790 | best_loss=7.81909
Epoch 15/80: current_loss=8.88883 | best_loss=7.81909
Epoch 16/80: current_loss=7.78831 | best_loss=7.78831
Epoch 17/80: current_loss=7.82803 | best_loss=7.78831
Epoch 18/80: current_loss=10.61266 | best_loss=7.78831
Epoch 19/80: current_loss=8.39898 | best_loss=7.78831
Epoch 20/80: current_loss=8.50409 | best_loss=7.78831
Epoch 21/80: current_loss=8.01130 | best_loss=7.78831
Epoch 22/80: current_loss=14.74510 | best_loss=7.78831
Epoch 23/80: current_loss=7.63606 | best_loss=7.63606
Epoch 24/80: current_loss=7.96795 | best_loss=7.63606
Epoch 25/80: current_loss=8.69918 | best_loss=7.63606
Epoch 26/80: current_loss=8.68890 | best_loss=7.63606
Epoch 27/80: current_loss=8.11145 | best_loss=7.63606
Epoch 28/80: current_loss=8.65686 | best_loss=7.63606
Epoch 29/80: current_loss=8.69667 | best_loss=7.63606
Epoch 30/80: current_loss=10.82730 | best_loss=7.63606
Epoch 31/80: current_loss=9.17279 | best_loss=7.63606
Epoch 32/80: current_loss=8.23261 | best_loss=7.63606
Epoch 33/80: current_loss=10.48454 | best_loss=7.63606
Epoch 34/80: current_loss=8.48288 | best_loss=7.63606
Epoch 35/80: current_loss=11.54251 | best_loss=7.63606
Epoch 36/80: current_loss=8.12359 | best_loss=7.63606
Epoch 37/80: current_loss=8.23373 | best_loss=7.63606
Epoch 38/80: current_loss=11.46285 | best_loss=7.63606
Epoch 39/80: current_loss=16.44389 | best_loss=7.63606
Epoch 40/80: current_loss=13.79010 | best_loss=7.63606
Epoch 41/80: current_loss=10.99668 | best_loss=7.63606
Epoch 42/80: current_loss=8.46443 | best_loss=7.63606
Epoch 43/80: current_loss=12.03930 | best_loss=7.63606
Early Stopping at epoch 43
      explained_var=0.01844 | mse_loss=7.44996
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=20.22902 | best_loss=20.22902
Epoch 1/80: current_loss=8.16679 | best_loss=8.16679
Epoch 2/80: current_loss=9.17976 | best_loss=8.16679
Epoch 3/80: current_loss=9.95124 | best_loss=8.16679
Epoch 4/80: current_loss=10.94204 | best_loss=8.16679
Epoch 5/80: current_loss=11.47591 | best_loss=8.16679
Epoch 6/80: current_loss=9.14134 | best_loss=8.16679
Epoch 7/80: current_loss=11.11476 | best_loss=8.16679
Epoch 8/80: current_loss=9.48219 | best_loss=8.16679
Epoch 9/80: current_loss=13.22766 | best_loss=8.16679
Epoch 10/80: current_loss=8.25704 | best_loss=8.16679
Epoch 11/80: current_loss=8.91592 | best_loss=8.16679
Epoch 12/80: current_loss=8.88694 | best_loss=8.16679
Epoch 13/80: current_loss=9.97348 | best_loss=8.16679
Epoch 14/80: current_loss=10.55368 | best_loss=8.16679
Epoch 15/80: current_loss=9.77958 | best_loss=8.16679
Epoch 16/80: current_loss=7.86388 | best_loss=7.86388
Epoch 17/80: current_loss=9.69557 | best_loss=7.86388
Epoch 18/80: current_loss=10.62989 | best_loss=7.86388
Epoch 19/80: current_loss=13.12866 | best_loss=7.86388
Epoch 20/80: current_loss=9.01457 | best_loss=7.86388
Epoch 21/80: current_loss=8.09494 | best_loss=7.86388
Epoch 22/80: current_loss=10.29022 | best_loss=7.86388
Epoch 23/80: current_loss=8.24807 | best_loss=7.86388
Epoch 24/80: current_loss=8.62030 | best_loss=7.86388
Epoch 25/80: current_loss=9.48309 | best_loss=7.86388
Epoch 26/80: current_loss=8.88426 | best_loss=7.86388
Epoch 27/80: current_loss=15.60039 | best_loss=7.86388
Epoch 28/80: current_loss=13.82027 | best_loss=7.86388
Epoch 29/80: current_loss=8.99907 | best_loss=7.86388
Epoch 30/80: current_loss=13.75305 | best_loss=7.86388
Epoch 31/80: current_loss=10.50311 | best_loss=7.86388
Epoch 32/80: current_loss=9.14587 | best_loss=7.86388
Epoch 33/80: current_loss=9.76791 | best_loss=7.86388
Epoch 34/80: current_loss=12.00081 | best_loss=7.86388
Epoch 35/80: current_loss=9.18320 | best_loss=7.86388
Epoch 36/80: current_loss=9.49090 | best_loss=7.86388
Early Stopping at epoch 36
      explained_var=0.05756 | mse_loss=7.71142
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=22.12571 | best_loss=22.12571
Epoch 1/80: current_loss=11.50012 | best_loss=11.50012
Epoch 2/80: current_loss=9.86031 | best_loss=9.86031
Epoch 3/80: current_loss=12.79937 | best_loss=9.86031
Epoch 4/80: current_loss=10.58941 | best_loss=9.86031
Epoch 5/80: current_loss=9.53483 | best_loss=9.53483
Epoch 6/80: current_loss=9.78780 | best_loss=9.53483
Epoch 7/80: current_loss=10.16842 | best_loss=9.53483
Epoch 8/80: current_loss=9.83354 | best_loss=9.53483
Epoch 9/80: current_loss=14.92020 | best_loss=9.53483
Epoch 10/80: current_loss=12.94782 | best_loss=9.53483
Epoch 11/80: current_loss=10.33965 | best_loss=9.53483
Epoch 12/80: current_loss=12.06702 | best_loss=9.53483
Epoch 13/80: current_loss=10.42921 | best_loss=9.53483
Epoch 14/80: current_loss=11.46755 | best_loss=9.53483
Epoch 15/80: current_loss=22.71959 | best_loss=9.53483
Epoch 16/80: current_loss=10.75756 | best_loss=9.53483
Epoch 17/80: current_loss=9.40142 | best_loss=9.40142
Epoch 18/80: current_loss=9.01933 | best_loss=9.01933
Epoch 19/80: current_loss=10.23569 | best_loss=9.01933
Epoch 20/80: current_loss=8.80345 | best_loss=8.80345
Epoch 21/80: current_loss=16.32651 | best_loss=8.80345
Epoch 22/80: current_loss=9.20291 | best_loss=8.80345
Epoch 23/80: current_loss=13.78773 | best_loss=8.80345
Epoch 24/80: current_loss=9.50456 | best_loss=8.80345
Epoch 25/80: current_loss=11.99141 | best_loss=8.80345
Epoch 26/80: current_loss=9.30040 | best_loss=8.80345
Epoch 27/80: current_loss=10.50825 | best_loss=8.80345
Epoch 28/80: current_loss=9.27966 | best_loss=8.80345
Epoch 29/80: current_loss=8.80865 | best_loss=8.80345
Epoch 30/80: current_loss=9.43416 | best_loss=8.80345
Epoch 31/80: current_loss=9.52246 | best_loss=8.80345
Epoch 32/80: current_loss=18.00130 | best_loss=8.80345
Epoch 33/80: current_loss=9.72913 | best_loss=8.80345
Epoch 34/80: current_loss=15.98874 | best_loss=8.80345
Epoch 35/80: current_loss=9.10998 | best_loss=8.80345
Epoch 36/80: current_loss=10.34720 | best_loss=8.80345
Epoch 37/80: current_loss=12.81879 | best_loss=8.80345
Epoch 38/80: current_loss=10.27052 | best_loss=8.80345
Epoch 39/80: current_loss=10.25767 | best_loss=8.80345
Epoch 40/80: current_loss=14.35053 | best_loss=8.80345
Early Stopping at epoch 40
      explained_var=0.00738 | mse_loss=8.63662
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.10877 | best_loss=11.10877
Epoch 1/80: current_loss=10.87233 | best_loss=10.87233
Epoch 2/80: current_loss=15.73103 | best_loss=10.87233
Epoch 3/80: current_loss=8.64962 | best_loss=8.64962
Epoch 4/80: current_loss=8.50516 | best_loss=8.50516
Epoch 5/80: current_loss=8.57023 | best_loss=8.50516
Epoch 6/80: current_loss=9.45158 | best_loss=8.50516
Epoch 7/80: current_loss=13.01869 | best_loss=8.50516
Epoch 8/80: current_loss=8.39872 | best_loss=8.39872
Epoch 9/80: current_loss=9.66509 | best_loss=8.39872
Epoch 10/80: current_loss=8.47650 | best_loss=8.39872
Epoch 11/80: current_loss=10.92987 | best_loss=8.39872
Epoch 12/80: current_loss=11.38888 | best_loss=8.39872
Epoch 13/80: current_loss=8.37867 | best_loss=8.37867
Epoch 14/80: current_loss=8.32441 | best_loss=8.32441
Epoch 15/80: current_loss=8.52860 | best_loss=8.32441
Epoch 16/80: current_loss=8.26339 | best_loss=8.26339
Epoch 17/80: current_loss=18.29495 | best_loss=8.26339
Epoch 18/80: current_loss=9.64934 | best_loss=8.26339
Epoch 19/80: current_loss=13.92961 | best_loss=8.26339
Epoch 20/80: current_loss=18.56648 | best_loss=8.26339
Epoch 21/80: current_loss=11.67028 | best_loss=8.26339
Epoch 22/80: current_loss=8.83007 | best_loss=8.26339
Epoch 23/80: current_loss=8.57339 | best_loss=8.26339
Epoch 24/80: current_loss=10.20837 | best_loss=8.26339
Epoch 25/80: current_loss=12.58419 | best_loss=8.26339
Epoch 26/80: current_loss=8.40054 | best_loss=8.26339
Epoch 27/80: current_loss=20.00131 | best_loss=8.26339
Epoch 28/80: current_loss=8.70239 | best_loss=8.26339
Epoch 29/80: current_loss=8.64650 | best_loss=8.26339
Epoch 30/80: current_loss=8.65251 | best_loss=8.26339
Epoch 31/80: current_loss=9.64316 | best_loss=8.26339
Epoch 32/80: current_loss=18.67735 | best_loss=8.26339
Epoch 33/80: current_loss=13.26185 | best_loss=8.26339
Epoch 34/80: current_loss=10.48963 | best_loss=8.26339
Epoch 35/80: current_loss=8.88916 | best_loss=8.26339
Epoch 36/80: current_loss=11.02593 | best_loss=8.26339
Early Stopping at epoch 36
      explained_var=0.00018 | mse_loss=8.36563
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.32761 | best_loss=11.32761
Epoch 1/80: current_loss=10.51755 | best_loss=10.51755
Epoch 2/80: current_loss=16.01419 | best_loss=10.51755
Epoch 3/80: current_loss=7.98776 | best_loss=7.98776
Epoch 4/80: current_loss=8.56536 | best_loss=7.98776
Epoch 5/80: current_loss=7.72050 | best_loss=7.72050
Epoch 6/80: current_loss=9.16131 | best_loss=7.72050
Epoch 7/80: current_loss=10.43615 | best_loss=7.72050
Epoch 8/80: current_loss=7.52530 | best_loss=7.52530
Epoch 9/80: current_loss=14.66243 | best_loss=7.52530
Epoch 10/80: current_loss=9.67417 | best_loss=7.52530
Epoch 11/80: current_loss=10.82563 | best_loss=7.52530
Epoch 12/80: current_loss=8.48327 | best_loss=7.52530
Epoch 13/80: current_loss=9.11020 | best_loss=7.52530
Epoch 14/80: current_loss=10.05743 | best_loss=7.52530
Epoch 15/80: current_loss=9.94954 | best_loss=7.52530
Epoch 16/80: current_loss=8.00445 | best_loss=7.52530
Epoch 17/80: current_loss=8.51293 | best_loss=7.52530
Epoch 18/80: current_loss=13.59240 | best_loss=7.52530
Epoch 19/80: current_loss=9.25739 | best_loss=7.52530
Epoch 20/80: current_loss=11.17664 | best_loss=7.52530
Epoch 21/80: current_loss=11.23968 | best_loss=7.52530
Epoch 22/80: current_loss=11.11313 | best_loss=7.52530
Epoch 23/80: current_loss=8.06953 | best_loss=7.52530
Epoch 24/80: current_loss=8.10828 | best_loss=7.52530
Epoch 25/80: current_loss=8.16122 | best_loss=7.52530
Epoch 26/80: current_loss=12.59494 | best_loss=7.52530
Epoch 27/80: current_loss=8.33598 | best_loss=7.52530
Epoch 28/80: current_loss=7.75991 | best_loss=7.52530
Early Stopping at epoch 28
      explained_var=0.02550 | mse_loss=7.68136
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.02181| avg_loss=7.96900
----------------------------------------------


----------------------------------------------
Params for Trial 77
{'learning_rate': 0.1, 'weight_decay': 0.009382961602135682, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.30658 | best_loss=14.30658
Epoch 1/80: current_loss=7.81709 | best_loss=7.81709
Epoch 2/80: current_loss=8.70508 | best_loss=7.81709
Epoch 3/80: current_loss=9.77213 | best_loss=7.81709
Epoch 4/80: current_loss=10.43433 | best_loss=7.81709
Epoch 5/80: current_loss=9.99424 | best_loss=7.81709
Epoch 6/80: current_loss=7.82154 | best_loss=7.81709
Epoch 7/80: current_loss=9.08773 | best_loss=7.81709
Epoch 8/80: current_loss=8.02025 | best_loss=7.81709
Epoch 9/80: current_loss=9.12475 | best_loss=7.81709
Epoch 10/80: current_loss=9.79937 | best_loss=7.81709
Epoch 11/80: current_loss=10.74917 | best_loss=7.81709
Epoch 12/80: current_loss=9.28797 | best_loss=7.81709
Epoch 13/80: current_loss=7.79319 | best_loss=7.79319
Epoch 14/80: current_loss=7.82179 | best_loss=7.79319
Epoch 15/80: current_loss=11.77709 | best_loss=7.79319
Epoch 16/80: current_loss=8.58707 | best_loss=7.79319
Epoch 17/80: current_loss=10.99777 | best_loss=7.79319
Epoch 18/80: current_loss=7.74085 | best_loss=7.74085
Epoch 19/80: current_loss=7.75745 | best_loss=7.74085
Epoch 20/80: current_loss=8.25859 | best_loss=7.74085
Epoch 21/80: current_loss=8.07713 | best_loss=7.74085
Epoch 22/80: current_loss=9.41469 | best_loss=7.74085
Epoch 23/80: current_loss=10.32029 | best_loss=7.74085
Epoch 24/80: current_loss=8.38409 | best_loss=7.74085
Epoch 25/80: current_loss=9.35425 | best_loss=7.74085
Epoch 26/80: current_loss=8.68683 | best_loss=7.74085
Epoch 27/80: current_loss=7.95308 | best_loss=7.74085
Epoch 28/80: current_loss=9.32840 | best_loss=7.74085
Epoch 29/80: current_loss=8.46700 | best_loss=7.74085
Epoch 30/80: current_loss=10.82848 | best_loss=7.74085
Epoch 31/80: current_loss=7.81862 | best_loss=7.74085
Epoch 32/80: current_loss=8.54184 | best_loss=7.74085
Epoch 33/80: current_loss=7.99653 | best_loss=7.74085
Epoch 34/80: current_loss=7.79372 | best_loss=7.74085
Epoch 35/80: current_loss=7.92363 | best_loss=7.74085
Epoch 36/80: current_loss=7.88123 | best_loss=7.74085
Epoch 37/80: current_loss=7.92123 | best_loss=7.74085
Epoch 38/80: current_loss=15.21358 | best_loss=7.74085
Early Stopping at epoch 38
      explained_var=0.00457 | mse_loss=7.55279
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.77117 | best_loss=10.77117
Epoch 1/80: current_loss=8.40150 | best_loss=8.40150
Epoch 2/80: current_loss=8.29847 | best_loss=8.29847
Epoch 3/80: current_loss=9.10769 | best_loss=8.29847
Epoch 4/80: current_loss=8.57735 | best_loss=8.29847
Epoch 5/80: current_loss=8.67794 | best_loss=8.29847
Epoch 6/80: current_loss=8.37806 | best_loss=8.29847
Epoch 7/80: current_loss=9.44412 | best_loss=8.29847
Epoch 8/80: current_loss=9.56980 | best_loss=8.29847
Epoch 9/80: current_loss=8.39282 | best_loss=8.29847
Epoch 10/80: current_loss=10.17802 | best_loss=8.29847
Epoch 11/80: current_loss=8.56963 | best_loss=8.29847
Epoch 12/80: current_loss=9.57789 | best_loss=8.29847
Epoch 13/80: current_loss=9.64251 | best_loss=8.29847
Epoch 14/80: current_loss=9.86329 | best_loss=8.29847
Epoch 15/80: current_loss=12.50191 | best_loss=8.29847
Epoch 16/80: current_loss=10.82153 | best_loss=8.29847
Epoch 17/80: current_loss=10.39129 | best_loss=8.29847
Epoch 18/80: current_loss=16.06563 | best_loss=8.29847
Epoch 19/80: current_loss=9.15529 | best_loss=8.29847
Epoch 20/80: current_loss=8.29569 | best_loss=8.29569
Epoch 21/80: current_loss=11.45016 | best_loss=8.29569
Epoch 22/80: current_loss=11.72406 | best_loss=8.29569
Epoch 23/80: current_loss=8.49294 | best_loss=8.29569
Epoch 24/80: current_loss=8.40044 | best_loss=8.29569
Epoch 25/80: current_loss=8.32202 | best_loss=8.29569
Epoch 26/80: current_loss=8.47406 | best_loss=8.29569
Epoch 27/80: current_loss=11.30554 | best_loss=8.29569
Epoch 28/80: current_loss=8.64135 | best_loss=8.29569
Epoch 29/80: current_loss=10.87771 | best_loss=8.29569
Epoch 30/80: current_loss=10.98409 | best_loss=8.29569
Epoch 31/80: current_loss=10.32835 | best_loss=8.29569
Epoch 32/80: current_loss=8.53124 | best_loss=8.29569
Epoch 33/80: current_loss=8.48009 | best_loss=8.29569
Epoch 34/80: current_loss=8.98874 | best_loss=8.29569
Epoch 35/80: current_loss=8.70796 | best_loss=8.29569
Epoch 36/80: current_loss=8.97246 | best_loss=8.29569
Epoch 37/80: current_loss=11.14530 | best_loss=8.29569
Epoch 38/80: current_loss=8.43032 | best_loss=8.29569
Epoch 39/80: current_loss=8.31342 | best_loss=8.29569
Epoch 40/80: current_loss=8.45616 | best_loss=8.29569
Early Stopping at epoch 40
      explained_var=0.00325 | mse_loss=8.14098
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.39315 | best_loss=11.39315
Epoch 1/80: current_loss=12.69591 | best_loss=11.39315
Epoch 2/80: current_loss=11.90850 | best_loss=11.39315
Epoch 3/80: current_loss=9.77408 | best_loss=9.77408
Epoch 4/80: current_loss=10.70943 | best_loss=9.77408
Epoch 5/80: current_loss=9.08495 | best_loss=9.08495
Epoch 6/80: current_loss=10.13047 | best_loss=9.08495
Epoch 7/80: current_loss=13.00005 | best_loss=9.08495
Epoch 8/80: current_loss=10.07164 | best_loss=9.08495
Epoch 9/80: current_loss=9.45907 | best_loss=9.08495
Epoch 10/80: current_loss=11.15775 | best_loss=9.08495
Epoch 11/80: current_loss=9.51104 | best_loss=9.08495
Epoch 12/80: current_loss=9.70948 | best_loss=9.08495
Epoch 13/80: current_loss=11.20780 | best_loss=9.08495
Epoch 14/80: current_loss=9.95727 | best_loss=9.08495
Epoch 15/80: current_loss=8.79190 | best_loss=8.79190
Epoch 16/80: current_loss=11.37565 | best_loss=8.79190
Epoch 17/80: current_loss=10.48447 | best_loss=8.79190
Epoch 18/80: current_loss=9.43087 | best_loss=8.79190
Epoch 19/80: current_loss=8.81516 | best_loss=8.79190
Epoch 20/80: current_loss=13.13804 | best_loss=8.79190
Epoch 21/80: current_loss=10.60272 | best_loss=8.79190
Epoch 22/80: current_loss=9.40515 | best_loss=8.79190
Epoch 23/80: current_loss=9.16937 | best_loss=8.79190
Epoch 24/80: current_loss=9.69241 | best_loss=8.79190
Epoch 25/80: current_loss=11.34834 | best_loss=8.79190
Epoch 26/80: current_loss=9.04812 | best_loss=8.79190
Epoch 27/80: current_loss=9.13534 | best_loss=8.79190
Epoch 28/80: current_loss=9.76656 | best_loss=8.79190
Epoch 29/80: current_loss=13.81816 | best_loss=8.79190
Epoch 30/80: current_loss=9.71350 | best_loss=8.79190
Epoch 31/80: current_loss=9.13342 | best_loss=8.79190
Epoch 32/80: current_loss=10.38826 | best_loss=8.79190
Epoch 33/80: current_loss=10.08957 | best_loss=8.79190
Epoch 34/80: current_loss=11.30806 | best_loss=8.79190
Epoch 35/80: current_loss=9.11568 | best_loss=8.79190
Early Stopping at epoch 35
      explained_var=-0.00255 | mse_loss=8.55694
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.63694 | best_loss=9.63694
Epoch 1/80: current_loss=9.09903 | best_loss=9.09903
Epoch 2/80: current_loss=9.90408 | best_loss=9.09903
Epoch 3/80: current_loss=9.87325 | best_loss=9.09903
Epoch 4/80: current_loss=10.16220 | best_loss=9.09903
Epoch 5/80: current_loss=8.71398 | best_loss=8.71398
Epoch 6/80: current_loss=9.71344 | best_loss=8.71398
Epoch 7/80: current_loss=11.78510 | best_loss=8.71398
Epoch 8/80: current_loss=8.76547 | best_loss=8.71398
Epoch 9/80: current_loss=10.98431 | best_loss=8.71398
Epoch 10/80: current_loss=9.72940 | best_loss=8.71398
Epoch 11/80: current_loss=8.89964 | best_loss=8.71398
Epoch 12/80: current_loss=11.66293 | best_loss=8.71398
Epoch 13/80: current_loss=8.46301 | best_loss=8.46301
Epoch 14/80: current_loss=10.35309 | best_loss=8.46301
Epoch 15/80: current_loss=8.93283 | best_loss=8.46301
Epoch 16/80: current_loss=9.15469 | best_loss=8.46301
Epoch 17/80: current_loss=8.47107 | best_loss=8.46301
Epoch 18/80: current_loss=10.07708 | best_loss=8.46301
Epoch 19/80: current_loss=10.08994 | best_loss=8.46301
Epoch 20/80: current_loss=8.23309 | best_loss=8.23309
Epoch 21/80: current_loss=10.09588 | best_loss=8.23309
Epoch 22/80: current_loss=8.24477 | best_loss=8.23309
Epoch 23/80: current_loss=8.33671 | best_loss=8.23309
Epoch 24/80: current_loss=8.32034 | best_loss=8.23309
Epoch 25/80: current_loss=9.48101 | best_loss=8.23309
Epoch 26/80: current_loss=8.31041 | best_loss=8.23309
Epoch 27/80: current_loss=9.02949 | best_loss=8.23309
Epoch 28/80: current_loss=8.27918 | best_loss=8.23309
Epoch 29/80: current_loss=8.25209 | best_loss=8.23309
Epoch 30/80: current_loss=8.95099 | best_loss=8.23309
Epoch 31/80: current_loss=8.35267 | best_loss=8.23309
Epoch 32/80: current_loss=8.57469 | best_loss=8.23309
Epoch 33/80: current_loss=8.13892 | best_loss=8.13892
Epoch 34/80: current_loss=8.47016 | best_loss=8.13892
Epoch 35/80: current_loss=8.66736 | best_loss=8.13892
Epoch 36/80: current_loss=9.53540 | best_loss=8.13892
Epoch 37/80: current_loss=9.33553 | best_loss=8.13892
Epoch 38/80: current_loss=9.90550 | best_loss=8.13892
Epoch 39/80: current_loss=8.63507 | best_loss=8.13892
Epoch 40/80: current_loss=8.40051 | best_loss=8.13892
Epoch 41/80: current_loss=11.58902 | best_loss=8.13892
Epoch 42/80: current_loss=14.84765 | best_loss=8.13892
Epoch 43/80: current_loss=11.91362 | best_loss=8.13892
Epoch 44/80: current_loss=11.42412 | best_loss=8.13892
Epoch 45/80: current_loss=8.32829 | best_loss=8.13892
Epoch 46/80: current_loss=8.20300 | best_loss=8.13892
Epoch 47/80: current_loss=8.36346 | best_loss=8.13892
Epoch 48/80: current_loss=8.36793 | best_loss=8.13892
Epoch 49/80: current_loss=8.92526 | best_loss=8.13892
Epoch 50/80: current_loss=8.20600 | best_loss=8.13892
Epoch 51/80: current_loss=8.36730 | best_loss=8.13892
Epoch 52/80: current_loss=8.28336 | best_loss=8.13892
Epoch 53/80: current_loss=10.85935 | best_loss=8.13892
Early Stopping at epoch 53
      explained_var=0.01264 | mse_loss=8.22909
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.14865 | best_loss=9.14865
Epoch 1/80: current_loss=8.78829 | best_loss=8.78829
Epoch 2/80: current_loss=7.70673 | best_loss=7.70673
Epoch 3/80: current_loss=8.56365 | best_loss=7.70673
Epoch 4/80: current_loss=9.89818 | best_loss=7.70673
Epoch 5/80: current_loss=14.13965 | best_loss=7.70673
Epoch 6/80: current_loss=10.12631 | best_loss=7.70673
Epoch 7/80: current_loss=8.00345 | best_loss=7.70673
Epoch 8/80: current_loss=7.73281 | best_loss=7.70673
Epoch 9/80: current_loss=7.99259 | best_loss=7.70673
Epoch 10/80: current_loss=8.27518 | best_loss=7.70673
Epoch 11/80: current_loss=7.96092 | best_loss=7.70673
Epoch 12/80: current_loss=8.75954 | best_loss=7.70673
Epoch 13/80: current_loss=8.48244 | best_loss=7.70673
Epoch 14/80: current_loss=8.80489 | best_loss=7.70673
Epoch 15/80: current_loss=8.17159 | best_loss=7.70673
Epoch 16/80: current_loss=8.86669 | best_loss=7.70673
Epoch 17/80: current_loss=9.58941 | best_loss=7.70673
Epoch 18/80: current_loss=10.71358 | best_loss=7.70673
Epoch 19/80: current_loss=10.18215 | best_loss=7.70673
Epoch 20/80: current_loss=8.41452 | best_loss=7.70673
Epoch 21/80: current_loss=8.21528 | best_loss=7.70673
Epoch 22/80: current_loss=8.11864 | best_loss=7.70673
Early Stopping at epoch 22
      explained_var=-0.00253 | mse_loss=7.88046
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00308| avg_loss=8.07205
----------------------------------------------


----------------------------------------------
Params for Trial 78
{'learning_rate': 0.01, 'weight_decay': 0.008072354329797861, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.77232 | best_loss=11.77232
Epoch 1/80: current_loss=7.77694 | best_loss=7.77694
Epoch 2/80: current_loss=8.76718 | best_loss=7.77694
Epoch 3/80: current_loss=7.98245 | best_loss=7.77694
Epoch 4/80: current_loss=8.23478 | best_loss=7.77694
Epoch 5/80: current_loss=8.30056 | best_loss=7.77694
Epoch 6/80: current_loss=7.77924 | best_loss=7.77694
Epoch 7/80: current_loss=8.34421 | best_loss=7.77694
Epoch 8/80: current_loss=8.52994 | best_loss=7.77694
Epoch 9/80: current_loss=7.92086 | best_loss=7.77694
Epoch 10/80: current_loss=7.78030 | best_loss=7.77694
Epoch 11/80: current_loss=8.42358 | best_loss=7.77694
Epoch 12/80: current_loss=7.90119 | best_loss=7.77694
Epoch 13/80: current_loss=7.85849 | best_loss=7.77694
Epoch 14/80: current_loss=7.82683 | best_loss=7.77694
Epoch 15/80: current_loss=8.33124 | best_loss=7.77694
Epoch 16/80: current_loss=8.60928 | best_loss=7.77694
Epoch 17/80: current_loss=7.77537 | best_loss=7.77537
Epoch 18/80: current_loss=9.06521 | best_loss=7.77537
Epoch 19/80: current_loss=8.08973 | best_loss=7.77537
Epoch 20/80: current_loss=8.02140 | best_loss=7.77537
Epoch 21/80: current_loss=8.52247 | best_loss=7.77537
Epoch 22/80: current_loss=8.94774 | best_loss=7.77537
Epoch 23/80: current_loss=7.82056 | best_loss=7.77537
Epoch 24/80: current_loss=8.76453 | best_loss=7.77537
Epoch 25/80: current_loss=8.51189 | best_loss=7.77537
Epoch 26/80: current_loss=8.47161 | best_loss=7.77537
Epoch 27/80: current_loss=7.91305 | best_loss=7.77537
Epoch 28/80: current_loss=8.14712 | best_loss=7.77537
Epoch 29/80: current_loss=7.88944 | best_loss=7.77537
Epoch 30/80: current_loss=8.29926 | best_loss=7.77537
Epoch 31/80: current_loss=8.38108 | best_loss=7.77537
Epoch 32/80: current_loss=10.04208 | best_loss=7.77537
Epoch 33/80: current_loss=7.81650 | best_loss=7.77537
Epoch 34/80: current_loss=7.81897 | best_loss=7.77537
Epoch 35/80: current_loss=8.90290 | best_loss=7.77537
Epoch 36/80: current_loss=8.32292 | best_loss=7.77537
Epoch 37/80: current_loss=8.10703 | best_loss=7.77537
Early Stopping at epoch 37
      explained_var=0.00056 | mse_loss=7.60329

----------------------------------------------
Params for Trial 79
{'learning_rate': 0.1, 'weight_decay': 0.0023238611857344217, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=90.60825 | best_loss=90.60825
Epoch 1/80: current_loss=47.39381 | best_loss=47.39381
Epoch 2/80: current_loss=35.73882 | best_loss=35.73882
Epoch 3/80: current_loss=34.16104 | best_loss=34.16104
Epoch 4/80: current_loss=21.96945 | best_loss=21.96945
Epoch 5/80: current_loss=15.91505 | best_loss=15.91505
Epoch 6/80: current_loss=14.81163 | best_loss=14.81163
Epoch 7/80: current_loss=19.09002 | best_loss=14.81163
Epoch 8/80: current_loss=12.95573 | best_loss=12.95573
Epoch 9/80: current_loss=20.28090 | best_loss=12.95573
Epoch 10/80: current_loss=13.99643 | best_loss=12.95573
Epoch 11/80: current_loss=39.46822 | best_loss=12.95573
Epoch 12/80: current_loss=7.95877 | best_loss=7.95877
Epoch 13/80: current_loss=8.35220 | best_loss=7.95877
Epoch 14/80: current_loss=8.08605 | best_loss=7.95877
Epoch 15/80: current_loss=14.75329 | best_loss=7.95877
Epoch 16/80: current_loss=15.78470 | best_loss=7.95877
Epoch 17/80: current_loss=42.62212 | best_loss=7.95877
Epoch 18/80: current_loss=8.12956 | best_loss=7.95877
Epoch 19/80: current_loss=14.79741 | best_loss=7.95877
Epoch 20/80: current_loss=20.63461 | best_loss=7.95877
Epoch 21/80: current_loss=17.92686 | best_loss=7.95877
Epoch 22/80: current_loss=8.51102 | best_loss=7.95877
Epoch 23/80: current_loss=17.25790 | best_loss=7.95877
Epoch 24/80: current_loss=8.35171 | best_loss=7.95877
Epoch 25/80: current_loss=33.23183 | best_loss=7.95877
Epoch 26/80: current_loss=15.89833 | best_loss=7.95877
Epoch 27/80: current_loss=28.48354 | best_loss=7.95877
Epoch 28/80: current_loss=26.99968 | best_loss=7.95877
Epoch 29/80: current_loss=13.75114 | best_loss=7.95877
Epoch 30/80: current_loss=35.68774 | best_loss=7.95877
Epoch 31/80: current_loss=43.78057 | best_loss=7.95877
Epoch 32/80: current_loss=8.95634 | best_loss=7.95877
Early Stopping at epoch 32
      explained_var=-0.01075 | mse_loss=7.79087

----------------------------------------------
Params for Trial 80
{'learning_rate': 0.1, 'weight_decay': 3.599585275592559e-05, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.75530 | best_loss=10.75530
Epoch 1/80: current_loss=10.24583 | best_loss=10.24583
Epoch 2/80: current_loss=9.41882 | best_loss=9.41882
Epoch 3/80: current_loss=9.85000 | best_loss=9.41882
Epoch 4/80: current_loss=9.66305 | best_loss=9.41882
Epoch 5/80: current_loss=10.62386 | best_loss=9.41882
Epoch 6/80: current_loss=9.15323 | best_loss=9.15323
Epoch 7/80: current_loss=7.94287 | best_loss=7.94287
Epoch 8/80: current_loss=8.15891 | best_loss=7.94287
Epoch 9/80: current_loss=7.99809 | best_loss=7.94287
Epoch 10/80: current_loss=10.34843 | best_loss=7.94287
Epoch 11/80: current_loss=9.52965 | best_loss=7.94287
Epoch 12/80: current_loss=8.50052 | best_loss=7.94287
Epoch 13/80: current_loss=9.70283 | best_loss=7.94287
Epoch 14/80: current_loss=10.73389 | best_loss=7.94287
Epoch 15/80: current_loss=11.65134 | best_loss=7.94287
Epoch 16/80: current_loss=9.53859 | best_loss=7.94287
Epoch 17/80: current_loss=8.01538 | best_loss=7.94287
Epoch 18/80: current_loss=21.99786 | best_loss=7.94287
Epoch 19/80: current_loss=8.66314 | best_loss=7.94287
Epoch 20/80: current_loss=8.43464 | best_loss=7.94287
Epoch 21/80: current_loss=11.21270 | best_loss=7.94287
Epoch 22/80: current_loss=8.13590 | best_loss=7.94287
Epoch 23/80: current_loss=11.52587 | best_loss=7.94287
Epoch 24/80: current_loss=12.49506 | best_loss=7.94287
Epoch 25/80: current_loss=10.08435 | best_loss=7.94287
Epoch 26/80: current_loss=15.17466 | best_loss=7.94287
Epoch 27/80: current_loss=11.46864 | best_loss=7.94287
Early Stopping at epoch 27
      explained_var=-0.02059 | mse_loss=7.74237

----------------------------------------------
Params for Trial 81
{'learning_rate': 0.1, 'weight_decay': 0.003658918306377608, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=31.85045 | best_loss=31.85045
Epoch 1/80: current_loss=14.17056 | best_loss=14.17056
Epoch 2/80: current_loss=8.77214 | best_loss=8.77214
Epoch 3/80: current_loss=8.94338 | best_loss=8.77214
Epoch 4/80: current_loss=9.14170 | best_loss=8.77214
Epoch 5/80: current_loss=8.72992 | best_loss=8.72992
Epoch 6/80: current_loss=9.43890 | best_loss=8.72992
Epoch 7/80: current_loss=8.36001 | best_loss=8.36001
Epoch 8/80: current_loss=10.95222 | best_loss=8.36001
Epoch 9/80: current_loss=10.23057 | best_loss=8.36001
Epoch 10/80: current_loss=41.31849 | best_loss=8.36001
Epoch 11/80: current_loss=31.26094 | best_loss=8.36001
Epoch 12/80: current_loss=15.11542 | best_loss=8.36001
Epoch 13/80: current_loss=11.37719 | best_loss=8.36001
Epoch 14/80: current_loss=10.65447 | best_loss=8.36001
Epoch 15/80: current_loss=8.30701 | best_loss=8.30701
Epoch 16/80: current_loss=10.81809 | best_loss=8.30701
Epoch 17/80: current_loss=8.07658 | best_loss=8.07658
Epoch 18/80: current_loss=9.88387 | best_loss=8.07658
Epoch 19/80: current_loss=12.07358 | best_loss=8.07658
Epoch 20/80: current_loss=10.34387 | best_loss=8.07658
Epoch 21/80: current_loss=10.34719 | best_loss=8.07658
Epoch 22/80: current_loss=9.39519 | best_loss=8.07658
Epoch 23/80: current_loss=8.18014 | best_loss=8.07658
Epoch 24/80: current_loss=10.69629 | best_loss=8.07658
Epoch 25/80: current_loss=8.36390 | best_loss=8.07658
Epoch 26/80: current_loss=8.33829 | best_loss=8.07658
Epoch 27/80: current_loss=15.48260 | best_loss=8.07658
Epoch 28/80: current_loss=8.47894 | best_loss=8.07658
Epoch 29/80: current_loss=12.60174 | best_loss=8.07658
Epoch 30/80: current_loss=11.47522 | best_loss=8.07658
Epoch 31/80: current_loss=9.31378 | best_loss=8.07658
Epoch 32/80: current_loss=8.10169 | best_loss=8.07658
Epoch 33/80: current_loss=9.48919 | best_loss=8.07658
Epoch 34/80: current_loss=9.57839 | best_loss=8.07658
Epoch 35/80: current_loss=10.97292 | best_loss=8.07658
Epoch 36/80: current_loss=8.12572 | best_loss=8.07658
Epoch 37/80: current_loss=9.06961 | best_loss=8.07658
Early Stopping at epoch 37
      explained_var=-0.01762 | mse_loss=7.94800

----------------------------------------------
Params for Trial 82
{'learning_rate': 0.1, 'weight_decay': 0.004419024432487524, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.00751 | best_loss=13.00751
Epoch 1/80: current_loss=25.70039 | best_loss=13.00751
Epoch 2/80: current_loss=12.14751 | best_loss=12.14751
Epoch 3/80: current_loss=9.25150 | best_loss=9.25150
Epoch 4/80: current_loss=10.32498 | best_loss=9.25150
Epoch 5/80: current_loss=8.90398 | best_loss=8.90398
Epoch 6/80: current_loss=8.89977 | best_loss=8.89977
Epoch 7/80: current_loss=8.37374 | best_loss=8.37374
Epoch 8/80: current_loss=10.17013 | best_loss=8.37374
Epoch 9/80: current_loss=8.35549 | best_loss=8.35549
Epoch 10/80: current_loss=10.23025 | best_loss=8.35549
Epoch 11/80: current_loss=8.24381 | best_loss=8.24381
Epoch 12/80: current_loss=8.95958 | best_loss=8.24381
Epoch 13/80: current_loss=7.84774 | best_loss=7.84774
Epoch 14/80: current_loss=7.83579 | best_loss=7.83579
Epoch 15/80: current_loss=8.63504 | best_loss=7.83579
Epoch 16/80: current_loss=12.95016 | best_loss=7.83579
Epoch 17/80: current_loss=8.06239 | best_loss=7.83579
Epoch 18/80: current_loss=8.22375 | best_loss=7.83579
Epoch 19/80: current_loss=8.41052 | best_loss=7.83579
Epoch 20/80: current_loss=9.20450 | best_loss=7.83579
Epoch 21/80: current_loss=9.67683 | best_loss=7.83579
Epoch 22/80: current_loss=12.39966 | best_loss=7.83579
Epoch 23/80: current_loss=9.13992 | best_loss=7.83579
Epoch 24/80: current_loss=8.11717 | best_loss=7.83579
Epoch 25/80: current_loss=7.81342 | best_loss=7.81342
Epoch 26/80: current_loss=7.77147 | best_loss=7.77147
Epoch 27/80: current_loss=15.51326 | best_loss=7.77147
Epoch 28/80: current_loss=7.86368 | best_loss=7.77147
Epoch 29/80: current_loss=8.09629 | best_loss=7.77147
Epoch 30/80: current_loss=9.09111 | best_loss=7.77147
Epoch 31/80: current_loss=7.78292 | best_loss=7.77147
Epoch 32/80: current_loss=7.84469 | best_loss=7.77147
Epoch 33/80: current_loss=10.93668 | best_loss=7.77147
Epoch 34/80: current_loss=7.89349 | best_loss=7.77147
Epoch 35/80: current_loss=7.83187 | best_loss=7.77147
Epoch 36/80: current_loss=12.72449 | best_loss=7.77147
Epoch 37/80: current_loss=14.38700 | best_loss=7.77147
Epoch 38/80: current_loss=8.68216 | best_loss=7.77147
Epoch 39/80: current_loss=8.52647 | best_loss=7.77147
Epoch 40/80: current_loss=18.43811 | best_loss=7.77147
Epoch 41/80: current_loss=7.77397 | best_loss=7.77147
Epoch 42/80: current_loss=8.86168 | best_loss=7.77147
Epoch 43/80: current_loss=8.41659 | best_loss=7.77147
Epoch 44/80: current_loss=8.71390 | best_loss=7.77147
Epoch 45/80: current_loss=9.61183 | best_loss=7.77147
Epoch 46/80: current_loss=8.70413 | best_loss=7.77147
Early Stopping at epoch 46
      explained_var=0.00427 | mse_loss=7.57265
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.53210 | best_loss=10.53210
Epoch 1/80: current_loss=8.49799 | best_loss=8.49799
Epoch 2/80: current_loss=8.40799 | best_loss=8.40799
Epoch 3/80: current_loss=9.46837 | best_loss=8.40799
Epoch 4/80: current_loss=8.84205 | best_loss=8.40799
Epoch 5/80: current_loss=10.53388 | best_loss=8.40799
Epoch 6/80: current_loss=8.80272 | best_loss=8.40799
Epoch 7/80: current_loss=9.18350 | best_loss=8.40799
Epoch 8/80: current_loss=11.71897 | best_loss=8.40799
Epoch 9/80: current_loss=11.72641 | best_loss=8.40799
Epoch 10/80: current_loss=8.31899 | best_loss=8.31899
Epoch 11/80: current_loss=8.51931 | best_loss=8.31899
Epoch 12/80: current_loss=9.46968 | best_loss=8.31899
Epoch 13/80: current_loss=9.90788 | best_loss=8.31899
Epoch 14/80: current_loss=8.82200 | best_loss=8.31899
Epoch 15/80: current_loss=8.38317 | best_loss=8.31899
Epoch 16/80: current_loss=9.12126 | best_loss=8.31899
Epoch 17/80: current_loss=8.45043 | best_loss=8.31899
Epoch 18/80: current_loss=8.34695 | best_loss=8.31899
Epoch 19/80: current_loss=8.43265 | best_loss=8.31899
Epoch 20/80: current_loss=13.42675 | best_loss=8.31899
Epoch 21/80: current_loss=8.55243 | best_loss=8.31899
Epoch 22/80: current_loss=8.33232 | best_loss=8.31899
Epoch 23/80: current_loss=12.61694 | best_loss=8.31899
Epoch 24/80: current_loss=8.48843 | best_loss=8.31899
Epoch 25/80: current_loss=10.68686 | best_loss=8.31899
Epoch 26/80: current_loss=8.31165 | best_loss=8.31165
Epoch 27/80: current_loss=12.41600 | best_loss=8.31165
Epoch 28/80: current_loss=10.21101 | best_loss=8.31165
Epoch 29/80: current_loss=10.50897 | best_loss=8.31165
Epoch 30/80: current_loss=9.22019 | best_loss=8.31165
Epoch 31/80: current_loss=9.38637 | best_loss=8.31165
Epoch 32/80: current_loss=8.37424 | best_loss=8.31165
Epoch 33/80: current_loss=9.00006 | best_loss=8.31165
Epoch 34/80: current_loss=10.84733 | best_loss=8.31165
Epoch 35/80: current_loss=12.55173 | best_loss=8.31165
Epoch 36/80: current_loss=16.54466 | best_loss=8.31165
Epoch 37/80: current_loss=10.36912 | best_loss=8.31165
Epoch 38/80: current_loss=8.93075 | best_loss=8.31165
Epoch 39/80: current_loss=9.81678 | best_loss=8.31165
Epoch 40/80: current_loss=8.68132 | best_loss=8.31165
Epoch 41/80: current_loss=8.53879 | best_loss=8.31165
Epoch 42/80: current_loss=9.88969 | best_loss=8.31165
Epoch 43/80: current_loss=9.90878 | best_loss=8.31165
Epoch 44/80: current_loss=8.45323 | best_loss=8.31165
Epoch 45/80: current_loss=8.76086 | best_loss=8.31165
Epoch 46/80: current_loss=10.44566 | best_loss=8.31165
Early Stopping at epoch 46
      explained_var=0.00002 | mse_loss=8.16334
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.07491 | best_loss=9.07491
Epoch 1/80: current_loss=16.44942 | best_loss=9.07491
Epoch 2/80: current_loss=16.18158 | best_loss=9.07491
Epoch 3/80: current_loss=9.28777 | best_loss=9.07491
Epoch 4/80: current_loss=9.17058 | best_loss=9.07491
Epoch 5/80: current_loss=14.00527 | best_loss=9.07491
Epoch 6/80: current_loss=8.76976 | best_loss=8.76976
Epoch 7/80: current_loss=9.19682 | best_loss=8.76976
Epoch 8/80: current_loss=9.72414 | best_loss=8.76976
Epoch 9/80: current_loss=9.51989 | best_loss=8.76976
Epoch 10/80: current_loss=9.16364 | best_loss=8.76976
Epoch 11/80: current_loss=11.10547 | best_loss=8.76976
Epoch 12/80: current_loss=27.14308 | best_loss=8.76976
Epoch 13/80: current_loss=21.13407 | best_loss=8.76976
Epoch 14/80: current_loss=30.52069 | best_loss=8.76976
Epoch 15/80: current_loss=14.02073 | best_loss=8.76976
Epoch 16/80: current_loss=13.74793 | best_loss=8.76976
Epoch 17/80: current_loss=10.74670 | best_loss=8.76976
Epoch 18/80: current_loss=9.33783 | best_loss=8.76976
Epoch 19/80: current_loss=8.57286 | best_loss=8.57286
Epoch 20/80: current_loss=8.95513 | best_loss=8.57286
Epoch 21/80: current_loss=9.42959 | best_loss=8.57286
Epoch 22/80: current_loss=16.85076 | best_loss=8.57286
Epoch 23/80: current_loss=9.16469 | best_loss=8.57286
Epoch 24/80: current_loss=16.62195 | best_loss=8.57286
Epoch 25/80: current_loss=8.77205 | best_loss=8.57286
Epoch 26/80: current_loss=8.76468 | best_loss=8.57286
Epoch 27/80: current_loss=9.59746 | best_loss=8.57286
Epoch 28/80: current_loss=8.69389 | best_loss=8.57286
Epoch 29/80: current_loss=9.46201 | best_loss=8.57286
Epoch 30/80: current_loss=9.60185 | best_loss=8.57286
Epoch 31/80: current_loss=8.77895 | best_loss=8.57286
Epoch 32/80: current_loss=9.27895 | best_loss=8.57286
Epoch 33/80: current_loss=9.24100 | best_loss=8.57286
Epoch 34/80: current_loss=8.83591 | best_loss=8.57286
Epoch 35/80: current_loss=11.49298 | best_loss=8.57286
Epoch 36/80: current_loss=16.13413 | best_loss=8.57286
Epoch 37/80: current_loss=14.51350 | best_loss=8.57286
Epoch 38/80: current_loss=11.95897 | best_loss=8.57286
Epoch 39/80: current_loss=15.98334 | best_loss=8.57286
Early Stopping at epoch 39
      explained_var=0.03861 | mse_loss=8.35395
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.78568 | best_loss=8.78568
Epoch 1/80: current_loss=8.78476 | best_loss=8.78476
Epoch 2/80: current_loss=9.78555 | best_loss=8.78476
Epoch 3/80: current_loss=9.26638 | best_loss=8.78476
Epoch 4/80: current_loss=9.81586 | best_loss=8.78476
Epoch 5/80: current_loss=11.53243 | best_loss=8.78476
Epoch 6/80: current_loss=10.36461 | best_loss=8.78476
Epoch 7/80: current_loss=8.03789 | best_loss=8.03789
Epoch 8/80: current_loss=9.62191 | best_loss=8.03789
Epoch 9/80: current_loss=9.44768 | best_loss=8.03789
Epoch 10/80: current_loss=8.86086 | best_loss=8.03789
Epoch 11/80: current_loss=8.11293 | best_loss=8.03789
Epoch 12/80: current_loss=9.49005 | best_loss=8.03789
Epoch 13/80: current_loss=15.14462 | best_loss=8.03789
Epoch 14/80: current_loss=8.89500 | best_loss=8.03789
Epoch 15/80: current_loss=9.40070 | best_loss=8.03789
Epoch 16/80: current_loss=8.12704 | best_loss=8.03789
Epoch 17/80: current_loss=10.86165 | best_loss=8.03789
Epoch 18/80: current_loss=8.42783 | best_loss=8.03789
Epoch 19/80: current_loss=8.85344 | best_loss=8.03789
Epoch 20/80: current_loss=9.55950 | best_loss=8.03789
Epoch 21/80: current_loss=11.64371 | best_loss=8.03789
Epoch 22/80: current_loss=8.92410 | best_loss=8.03789
Epoch 23/80: current_loss=10.20158 | best_loss=8.03789
Epoch 24/80: current_loss=9.17102 | best_loss=8.03789
Epoch 25/80: current_loss=11.89467 | best_loss=8.03789
Epoch 26/80: current_loss=8.19137 | best_loss=8.03789
Epoch 27/80: current_loss=11.38918 | best_loss=8.03789
Early Stopping at epoch 27
      explained_var=0.03293 | mse_loss=8.09471
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.36827 | best_loss=9.36827
Epoch 1/80: current_loss=8.63313 | best_loss=8.63313
Epoch 2/80: current_loss=8.79395 | best_loss=8.63313
Epoch 3/80: current_loss=8.11662 | best_loss=8.11662
Epoch 4/80: current_loss=8.17226 | best_loss=8.11662
Epoch 5/80: current_loss=7.70274 | best_loss=7.70274
Epoch 6/80: current_loss=7.77261 | best_loss=7.70274
Epoch 7/80: current_loss=8.12810 | best_loss=7.70274
Epoch 8/80: current_loss=10.73369 | best_loss=7.70274
Epoch 9/80: current_loss=7.78764 | best_loss=7.70274
Epoch 10/80: current_loss=7.82835 | best_loss=7.70274
Epoch 11/80: current_loss=13.94372 | best_loss=7.70274
Epoch 12/80: current_loss=17.49483 | best_loss=7.70274
Epoch 13/80: current_loss=7.93367 | best_loss=7.70274
Epoch 14/80: current_loss=7.83888 | best_loss=7.70274
Epoch 15/80: current_loss=7.89196 | best_loss=7.70274
Epoch 16/80: current_loss=8.17454 | best_loss=7.70274
Epoch 17/80: current_loss=8.14746 | best_loss=7.70274
Epoch 18/80: current_loss=10.86289 | best_loss=7.70274
Epoch 19/80: current_loss=18.36836 | best_loss=7.70274
Epoch 20/80: current_loss=10.44781 | best_loss=7.70274
Epoch 21/80: current_loss=7.94958 | best_loss=7.70274
Epoch 22/80: current_loss=8.08959 | best_loss=7.70274
Epoch 23/80: current_loss=8.39284 | best_loss=7.70274
Epoch 24/80: current_loss=9.90683 | best_loss=7.70274
Epoch 25/80: current_loss=8.53635 | best_loss=7.70274
Early Stopping at epoch 25
      explained_var=-0.00229 | mse_loss=7.87475
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.01471| avg_loss=8.01188
----------------------------------------------


----------------------------------------------
Params for Trial 83
{'learning_rate': 0.1, 'weight_decay': 0.0028995467352481833, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.71453 | best_loss=9.71453
Epoch 1/80: current_loss=11.45252 | best_loss=9.71453
Epoch 2/80: current_loss=12.84967 | best_loss=9.71453
Epoch 3/80: current_loss=7.98852 | best_loss=7.98852
Epoch 4/80: current_loss=7.92713 | best_loss=7.92713
Epoch 5/80: current_loss=11.43612 | best_loss=7.92713
Epoch 6/80: current_loss=12.03275 | best_loss=7.92713
Epoch 7/80: current_loss=7.95665 | best_loss=7.92713
Epoch 8/80: current_loss=11.95016 | best_loss=7.92713
Epoch 9/80: current_loss=9.55186 | best_loss=7.92713
Epoch 10/80: current_loss=8.65951 | best_loss=7.92713
Epoch 11/80: current_loss=11.59840 | best_loss=7.92713
Epoch 12/80: current_loss=14.94395 | best_loss=7.92713
Epoch 13/80: current_loss=9.48239 | best_loss=7.92713
Epoch 14/80: current_loss=16.44481 | best_loss=7.92713
Epoch 15/80: current_loss=7.85204 | best_loss=7.85204
Epoch 16/80: current_loss=8.35395 | best_loss=7.85204
Epoch 17/80: current_loss=10.78876 | best_loss=7.85204
Epoch 18/80: current_loss=9.65495 | best_loss=7.85204
Epoch 19/80: current_loss=9.19133 | best_loss=7.85204
Epoch 20/80: current_loss=7.88895 | best_loss=7.85204
Epoch 21/80: current_loss=18.97594 | best_loss=7.85204
Epoch 22/80: current_loss=13.01002 | best_loss=7.85204
Epoch 23/80: current_loss=18.44460 | best_loss=7.85204
Epoch 24/80: current_loss=16.43656 | best_loss=7.85204
Epoch 25/80: current_loss=8.33359 | best_loss=7.85204
Epoch 26/80: current_loss=8.30256 | best_loss=7.85204
Epoch 27/80: current_loss=7.97839 | best_loss=7.85204
Epoch 28/80: current_loss=7.72754 | best_loss=7.72754
Epoch 29/80: current_loss=11.26608 | best_loss=7.72754
Epoch 30/80: current_loss=7.89312 | best_loss=7.72754
Epoch 31/80: current_loss=22.31351 | best_loss=7.72754
Epoch 32/80: current_loss=14.96732 | best_loss=7.72754
Epoch 33/80: current_loss=8.20514 | best_loss=7.72754
Epoch 34/80: current_loss=13.78849 | best_loss=7.72754
Epoch 35/80: current_loss=9.94397 | best_loss=7.72754
Epoch 36/80: current_loss=10.16076 | best_loss=7.72754
Epoch 37/80: current_loss=9.41715 | best_loss=7.72754
Epoch 38/80: current_loss=9.10177 | best_loss=7.72754
Epoch 39/80: current_loss=9.04700 | best_loss=7.72754
Epoch 40/80: current_loss=8.17690 | best_loss=7.72754
Epoch 41/80: current_loss=7.73537 | best_loss=7.72754
Epoch 42/80: current_loss=9.59128 | best_loss=7.72754
Epoch 43/80: current_loss=9.88981 | best_loss=7.72754
Epoch 44/80: current_loss=8.96897 | best_loss=7.72754
Epoch 45/80: current_loss=11.48809 | best_loss=7.72754
Epoch 46/80: current_loss=8.99347 | best_loss=7.72754
Epoch 47/80: current_loss=11.38683 | best_loss=7.72754
Epoch 48/80: current_loss=11.14926 | best_loss=7.72754
Early Stopping at epoch 48
      explained_var=0.00587 | mse_loss=7.54626
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.28075 | best_loss=14.28075
Epoch 1/80: current_loss=18.91819 | best_loss=14.28075
Epoch 2/80: current_loss=9.23158 | best_loss=9.23158
Epoch 3/80: current_loss=15.19051 | best_loss=9.23158
Epoch 4/80: current_loss=11.19492 | best_loss=9.23158
Epoch 5/80: current_loss=10.08605 | best_loss=9.23158
Epoch 6/80: current_loss=14.28573 | best_loss=9.23158
Epoch 7/80: current_loss=10.39951 | best_loss=9.23158
Epoch 8/80: current_loss=11.16348 | best_loss=9.23158
Epoch 9/80: current_loss=10.15872 | best_loss=9.23158
Epoch 10/80: current_loss=10.51583 | best_loss=9.23158
Epoch 11/80: current_loss=14.90539 | best_loss=9.23158
Epoch 12/80: current_loss=8.84512 | best_loss=8.84512
Epoch 13/80: current_loss=11.90096 | best_loss=8.84512
Epoch 14/80: current_loss=15.56571 | best_loss=8.84512
Epoch 15/80: current_loss=12.71765 | best_loss=8.84512
Epoch 16/80: current_loss=9.80954 | best_loss=8.84512
Epoch 17/80: current_loss=8.55275 | best_loss=8.55275
Epoch 18/80: current_loss=10.83879 | best_loss=8.55275
Epoch 19/80: current_loss=10.69307 | best_loss=8.55275
Epoch 20/80: current_loss=8.34516 | best_loss=8.34516
Epoch 21/80: current_loss=8.43925 | best_loss=8.34516
Epoch 22/80: current_loss=11.81215 | best_loss=8.34516
Epoch 23/80: current_loss=8.65577 | best_loss=8.34516
Epoch 24/80: current_loss=8.31802 | best_loss=8.31802
Epoch 25/80: current_loss=9.07586 | best_loss=8.31802
Epoch 26/80: current_loss=13.14278 | best_loss=8.31802
Epoch 27/80: current_loss=30.42760 | best_loss=8.31802
Epoch 28/80: current_loss=10.90425 | best_loss=8.31802
Epoch 29/80: current_loss=8.70511 | best_loss=8.31802
Epoch 30/80: current_loss=8.60325 | best_loss=8.31802
Epoch 31/80: current_loss=11.79137 | best_loss=8.31802
Epoch 32/80: current_loss=23.10442 | best_loss=8.31802
Epoch 33/80: current_loss=10.50866 | best_loss=8.31802
Epoch 34/80: current_loss=20.33390 | best_loss=8.31802
Epoch 35/80: current_loss=12.71912 | best_loss=8.31802
Epoch 36/80: current_loss=11.42651 | best_loss=8.31802
Epoch 37/80: current_loss=10.29749 | best_loss=8.31802
Epoch 38/80: current_loss=8.45138 | best_loss=8.31802
Epoch 39/80: current_loss=8.55783 | best_loss=8.31802
Epoch 40/80: current_loss=13.88896 | best_loss=8.31802
Epoch 41/80: current_loss=8.74097 | best_loss=8.31802
Epoch 42/80: current_loss=8.54778 | best_loss=8.31802
Epoch 43/80: current_loss=9.49459 | best_loss=8.31802
Epoch 44/80: current_loss=8.34871 | best_loss=8.31802
Early Stopping at epoch 44
      explained_var=0.00013 | mse_loss=8.16935
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.80956 | best_loss=8.80956
Epoch 1/80: current_loss=9.35241 | best_loss=8.80956
Epoch 2/80: current_loss=16.41205 | best_loss=8.80956
Epoch 3/80: current_loss=10.21267 | best_loss=8.80956
Epoch 4/80: current_loss=9.46996 | best_loss=8.80956
Epoch 5/80: current_loss=10.81761 | best_loss=8.80956
Epoch 6/80: current_loss=12.81611 | best_loss=8.80956
Epoch 7/80: current_loss=13.72563 | best_loss=8.80956
Epoch 8/80: current_loss=10.89550 | best_loss=8.80956
Epoch 9/80: current_loss=10.77114 | best_loss=8.80956
Epoch 10/80: current_loss=9.38915 | best_loss=8.80956
Epoch 11/80: current_loss=9.84011 | best_loss=8.80956
Epoch 12/80: current_loss=8.89210 | best_loss=8.80956
Epoch 13/80: current_loss=9.76904 | best_loss=8.80956
Epoch 14/80: current_loss=10.56108 | best_loss=8.80956
Epoch 15/80: current_loss=8.00218 | best_loss=8.00218
Epoch 16/80: current_loss=8.78239 | best_loss=8.00218
Epoch 17/80: current_loss=9.61829 | best_loss=8.00218
Epoch 18/80: current_loss=10.30163 | best_loss=8.00218
Epoch 19/80: current_loss=20.80423 | best_loss=8.00218
Epoch 20/80: current_loss=9.94889 | best_loss=8.00218
Epoch 21/80: current_loss=8.74155 | best_loss=8.00218
Epoch 22/80: current_loss=10.61798 | best_loss=8.00218
Epoch 23/80: current_loss=9.76400 | best_loss=8.00218
Epoch 24/80: current_loss=9.94614 | best_loss=8.00218
Epoch 25/80: current_loss=10.74543 | best_loss=8.00218
Epoch 26/80: current_loss=8.64344 | best_loss=8.00218
Epoch 27/80: current_loss=14.79399 | best_loss=8.00218
Epoch 28/80: current_loss=9.72515 | best_loss=8.00218
Epoch 29/80: current_loss=13.02947 | best_loss=8.00218
Epoch 30/80: current_loss=8.64111 | best_loss=8.00218
Epoch 31/80: current_loss=9.97599 | best_loss=8.00218
Epoch 32/80: current_loss=12.41058 | best_loss=8.00218
Epoch 33/80: current_loss=9.58716 | best_loss=8.00218
Epoch 34/80: current_loss=8.73209 | best_loss=8.00218
Epoch 35/80: current_loss=12.12176 | best_loss=8.00218
Early Stopping at epoch 35
      explained_var=0.10301 | mse_loss=7.74952
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.68210 | best_loss=8.68210
Epoch 1/80: current_loss=11.30745 | best_loss=8.68210
Epoch 2/80: current_loss=9.16636 | best_loss=8.68210
Epoch 3/80: current_loss=10.77764 | best_loss=8.68210
Epoch 4/80: current_loss=10.44269 | best_loss=8.68210
Epoch 5/80: current_loss=10.55647 | best_loss=8.68210
Epoch 6/80: current_loss=8.75393 | best_loss=8.68210
Epoch 7/80: current_loss=15.89604 | best_loss=8.68210
Epoch 8/80: current_loss=9.16668 | best_loss=8.68210
Epoch 9/80: current_loss=8.99205 | best_loss=8.68210
Epoch 10/80: current_loss=8.67822 | best_loss=8.67822
Epoch 11/80: current_loss=9.62368 | best_loss=8.67822
Epoch 12/80: current_loss=8.20814 | best_loss=8.20814
Epoch 13/80: current_loss=8.66145 | best_loss=8.20814
Epoch 14/80: current_loss=10.03316 | best_loss=8.20814
Epoch 15/80: current_loss=8.46651 | best_loss=8.20814
Epoch 16/80: current_loss=9.19428 | best_loss=8.20814
Epoch 17/80: current_loss=8.98948 | best_loss=8.20814
Epoch 18/80: current_loss=11.47350 | best_loss=8.20814
Epoch 19/80: current_loss=8.07175 | best_loss=8.07175
Epoch 20/80: current_loss=14.11814 | best_loss=8.07175
Epoch 21/80: current_loss=8.62098 | best_loss=8.07175
Epoch 22/80: current_loss=10.14665 | best_loss=8.07175
Epoch 23/80: current_loss=10.78733 | best_loss=8.07175
Epoch 24/80: current_loss=13.09021 | best_loss=8.07175
Epoch 25/80: current_loss=8.55082 | best_loss=8.07175
Epoch 26/80: current_loss=12.14474 | best_loss=8.07175
Epoch 27/80: current_loss=8.99439 | best_loss=8.07175
Epoch 28/80: current_loss=14.89902 | best_loss=8.07175
Epoch 29/80: current_loss=8.43173 | best_loss=8.07175
Epoch 30/80: current_loss=8.48694 | best_loss=8.07175
Epoch 31/80: current_loss=12.01509 | best_loss=8.07175
Epoch 32/80: current_loss=8.00104 | best_loss=8.00104
Epoch 33/80: current_loss=10.92769 | best_loss=8.00104
Epoch 34/80: current_loss=8.24364 | best_loss=8.00104
Epoch 35/80: current_loss=8.73106 | best_loss=8.00104
Epoch 36/80: current_loss=9.14593 | best_loss=8.00104
Epoch 37/80: current_loss=14.79451 | best_loss=8.00104
Epoch 38/80: current_loss=17.16372 | best_loss=8.00104
Epoch 39/80: current_loss=9.72222 | best_loss=8.00104
Epoch 40/80: current_loss=10.47326 | best_loss=8.00104
Epoch 41/80: current_loss=10.21615 | best_loss=8.00104
Epoch 42/80: current_loss=9.28479 | best_loss=8.00104
Epoch 43/80: current_loss=9.42378 | best_loss=8.00104
Epoch 44/80: current_loss=8.30673 | best_loss=8.00104
Epoch 45/80: current_loss=9.20223 | best_loss=8.00104
Epoch 46/80: current_loss=10.99366 | best_loss=8.00104
Epoch 47/80: current_loss=8.52128 | best_loss=8.00104
Epoch 48/80: current_loss=12.19709 | best_loss=8.00104
Epoch 49/80: current_loss=8.68266 | best_loss=8.00104
Epoch 50/80: current_loss=18.31162 | best_loss=8.00104
Epoch 51/80: current_loss=12.01016 | best_loss=8.00104
Epoch 52/80: current_loss=11.72790 | best_loss=8.00104
Early Stopping at epoch 52
      explained_var=0.03575 | mse_loss=8.06212
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.16409 | best_loss=9.16409
Epoch 1/80: current_loss=7.80614 | best_loss=7.80614
Epoch 2/80: current_loss=7.84670 | best_loss=7.80614
Epoch 3/80: current_loss=9.77039 | best_loss=7.80614
Epoch 4/80: current_loss=9.83758 | best_loss=7.80614
Epoch 5/80: current_loss=15.23512 | best_loss=7.80614
Epoch 6/80: current_loss=7.80256 | best_loss=7.80256
Epoch 7/80: current_loss=8.23688 | best_loss=7.80256
Epoch 8/80: current_loss=9.21226 | best_loss=7.80256
Epoch 9/80: current_loss=10.92163 | best_loss=7.80256
Epoch 10/80: current_loss=10.15626 | best_loss=7.80256
Epoch 11/80: current_loss=7.69608 | best_loss=7.69608
Epoch 12/80: current_loss=8.33190 | best_loss=7.69608
Epoch 13/80: current_loss=7.66649 | best_loss=7.66649
Epoch 14/80: current_loss=9.23025 | best_loss=7.66649
Epoch 15/80: current_loss=7.79439 | best_loss=7.66649
Epoch 16/80: current_loss=9.39081 | best_loss=7.66649
Epoch 17/80: current_loss=7.68457 | best_loss=7.66649
Epoch 18/80: current_loss=7.76586 | best_loss=7.66649
Epoch 19/80: current_loss=10.31149 | best_loss=7.66649
Epoch 20/80: current_loss=14.75574 | best_loss=7.66649
Epoch 21/80: current_loss=7.67568 | best_loss=7.66649
Epoch 22/80: current_loss=7.78089 | best_loss=7.66649
Epoch 23/80: current_loss=7.76981 | best_loss=7.66649
Epoch 24/80: current_loss=8.07929 | best_loss=7.66649
Epoch 25/80: current_loss=7.73428 | best_loss=7.66649
Epoch 26/80: current_loss=7.72514 | best_loss=7.66649
Epoch 27/80: current_loss=20.24006 | best_loss=7.66649
Epoch 28/80: current_loss=7.89845 | best_loss=7.66649
Epoch 29/80: current_loss=7.74464 | best_loss=7.66649
Epoch 30/80: current_loss=15.19054 | best_loss=7.66649
Epoch 31/80: current_loss=11.18473 | best_loss=7.66649
Epoch 32/80: current_loss=32.31810 | best_loss=7.66649
Epoch 33/80: current_loss=9.50872 | best_loss=7.66649
Early Stopping at epoch 33
      explained_var=0.00173 | mse_loss=7.84366
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=0.02930| avg_loss=7.87418
----------------------------------------------


----------------------------------------------
Params for Trial 84
{'learning_rate': 0.1, 'weight_decay': 0.0026881299584821414, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.02455 | best_loss=9.02455
Epoch 1/80: current_loss=7.74693 | best_loss=7.74693
Epoch 2/80: current_loss=7.87426 | best_loss=7.74693
Epoch 3/80: current_loss=7.74749 | best_loss=7.74693
Epoch 4/80: current_loss=7.91993 | best_loss=7.74693
Epoch 5/80: current_loss=9.04437 | best_loss=7.74693
Epoch 6/80: current_loss=15.72824 | best_loss=7.74693
Epoch 7/80: current_loss=8.73993 | best_loss=7.74693
Epoch 8/80: current_loss=8.06290 | best_loss=7.74693
Epoch 9/80: current_loss=7.74926 | best_loss=7.74693
Epoch 10/80: current_loss=8.74062 | best_loss=7.74693
Epoch 11/80: current_loss=10.45181 | best_loss=7.74693
Epoch 12/80: current_loss=8.61086 | best_loss=7.74693
Epoch 13/80: current_loss=15.09270 | best_loss=7.74693
Epoch 14/80: current_loss=7.84964 | best_loss=7.74693
Epoch 15/80: current_loss=8.53635 | best_loss=7.74693
Epoch 16/80: current_loss=7.76898 | best_loss=7.74693
Epoch 17/80: current_loss=7.91996 | best_loss=7.74693
Epoch 18/80: current_loss=9.43120 | best_loss=7.74693
Epoch 19/80: current_loss=8.27991 | best_loss=7.74693
Epoch 20/80: current_loss=12.73636 | best_loss=7.74693
Epoch 21/80: current_loss=9.34876 | best_loss=7.74693
Early Stopping at epoch 21
      explained_var=0.00227 | mse_loss=7.57875
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.06277 | best_loss=10.06277
Epoch 1/80: current_loss=8.54392 | best_loss=8.54392
Epoch 2/80: current_loss=10.15801 | best_loss=8.54392
Epoch 3/80: current_loss=8.96463 | best_loss=8.54392
Epoch 4/80: current_loss=8.35315 | best_loss=8.35315
Epoch 5/80: current_loss=23.25058 | best_loss=8.35315
Epoch 6/80: current_loss=14.83172 | best_loss=8.35315
Epoch 7/80: current_loss=9.19712 | best_loss=8.35315
Epoch 8/80: current_loss=8.75620 | best_loss=8.35315
Epoch 9/80: current_loss=9.71952 | best_loss=8.35315
Epoch 10/80: current_loss=10.13953 | best_loss=8.35315
Epoch 11/80: current_loss=8.36179 | best_loss=8.35315
Epoch 12/80: current_loss=11.87262 | best_loss=8.35315
Epoch 13/80: current_loss=9.21029 | best_loss=8.35315
Epoch 14/80: current_loss=38.30798 | best_loss=8.35315
Epoch 15/80: current_loss=8.23886 | best_loss=8.23886
Epoch 16/80: current_loss=11.95345 | best_loss=8.23886
Epoch 17/80: current_loss=14.56507 | best_loss=8.23886
Epoch 18/80: current_loss=10.73329 | best_loss=8.23886
Epoch 19/80: current_loss=9.21317 | best_loss=8.23886
Epoch 20/80: current_loss=8.64575 | best_loss=8.23886
Epoch 21/80: current_loss=8.57320 | best_loss=8.23886
Epoch 22/80: current_loss=9.26280 | best_loss=8.23886
Epoch 23/80: current_loss=9.28194 | best_loss=8.23886
Epoch 24/80: current_loss=8.57811 | best_loss=8.23886
Epoch 25/80: current_loss=12.95071 | best_loss=8.23886
Epoch 26/80: current_loss=12.78780 | best_loss=8.23886
Epoch 27/80: current_loss=8.94886 | best_loss=8.23886
Epoch 28/80: current_loss=8.40058 | best_loss=8.23886
Epoch 29/80: current_loss=9.35174 | best_loss=8.23886
Epoch 30/80: current_loss=10.93308 | best_loss=8.23886
Epoch 31/80: current_loss=8.98963 | best_loss=8.23886
Epoch 32/80: current_loss=11.79768 | best_loss=8.23886
Epoch 33/80: current_loss=10.19270 | best_loss=8.23886
Epoch 34/80: current_loss=12.60548 | best_loss=8.23886
Epoch 35/80: current_loss=10.36122 | best_loss=8.23886
Early Stopping at epoch 35
      explained_var=0.01908 | mse_loss=8.07011
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.73352 | best_loss=9.73352
Epoch 1/80: current_loss=9.20580 | best_loss=9.20580
Epoch 2/80: current_loss=12.53543 | best_loss=9.20580
Epoch 3/80: current_loss=12.69733 | best_loss=9.20580
Epoch 4/80: current_loss=9.05247 | best_loss=9.05247
Epoch 5/80: current_loss=9.92916 | best_loss=9.05247
Epoch 6/80: current_loss=16.14770 | best_loss=9.05247
Epoch 7/80: current_loss=8.91042 | best_loss=8.91042
Epoch 8/80: current_loss=10.88376 | best_loss=8.91042
Epoch 9/80: current_loss=11.96790 | best_loss=8.91042
Epoch 10/80: current_loss=10.07307 | best_loss=8.91042
Epoch 11/80: current_loss=8.87272 | best_loss=8.87272
Epoch 12/80: current_loss=12.76687 | best_loss=8.87272
Epoch 13/80: current_loss=10.43327 | best_loss=8.87272
Epoch 14/80: current_loss=11.45649 | best_loss=8.87272
Epoch 15/80: current_loss=10.92734 | best_loss=8.87272
Epoch 16/80: current_loss=9.01370 | best_loss=8.87272
Epoch 17/80: current_loss=12.02247 | best_loss=8.87272
Epoch 18/80: current_loss=9.30511 | best_loss=8.87272
Epoch 19/80: current_loss=8.82941 | best_loss=8.82941
Epoch 20/80: current_loss=9.50707 | best_loss=8.82941
Epoch 21/80: current_loss=9.89771 | best_loss=8.82941
Epoch 22/80: current_loss=9.36899 | best_loss=8.82941
Epoch 23/80: current_loss=10.53642 | best_loss=8.82941
Epoch 24/80: current_loss=8.85923 | best_loss=8.82941
Epoch 25/80: current_loss=9.18039 | best_loss=8.82941
Epoch 26/80: current_loss=9.52417 | best_loss=8.82941
Epoch 27/80: current_loss=8.73280 | best_loss=8.73280
Epoch 28/80: current_loss=9.13113 | best_loss=8.73280
Epoch 29/80: current_loss=8.90357 | best_loss=8.73280
Epoch 30/80: current_loss=9.38526 | best_loss=8.73280
Epoch 31/80: current_loss=17.68830 | best_loss=8.73280
Epoch 32/80: current_loss=8.82550 | best_loss=8.73280
Epoch 33/80: current_loss=8.82961 | best_loss=8.73280
Epoch 34/80: current_loss=9.09412 | best_loss=8.73280
Epoch 35/80: current_loss=21.59548 | best_loss=8.73280
Epoch 36/80: current_loss=22.05393 | best_loss=8.73280
Epoch 37/80: current_loss=8.82813 | best_loss=8.73280
Epoch 38/80: current_loss=9.40894 | best_loss=8.73280
Epoch 39/80: current_loss=15.73530 | best_loss=8.73280
Epoch 40/80: current_loss=26.46407 | best_loss=8.73280
Epoch 41/80: current_loss=23.97922 | best_loss=8.73280
Epoch 42/80: current_loss=8.85578 | best_loss=8.73280
Epoch 43/80: current_loss=9.19017 | best_loss=8.73280
Epoch 44/80: current_loss=8.87758 | best_loss=8.73280
Epoch 45/80: current_loss=8.73028 | best_loss=8.73028
Epoch 46/80: current_loss=9.22384 | best_loss=8.73028
Epoch 47/80: current_loss=8.85446 | best_loss=8.73028
Epoch 48/80: current_loss=9.37279 | best_loss=8.73028
Epoch 49/80: current_loss=9.13182 | best_loss=8.73028
Epoch 50/80: current_loss=8.90356 | best_loss=8.73028
Epoch 51/80: current_loss=9.12099 | best_loss=8.73028
Epoch 52/80: current_loss=13.57938 | best_loss=8.73028
Epoch 53/80: current_loss=12.91759 | best_loss=8.73028
Epoch 54/80: current_loss=16.97676 | best_loss=8.73028
Epoch 55/80: current_loss=8.89633 | best_loss=8.73028
Epoch 56/80: current_loss=12.38720 | best_loss=8.73028
Epoch 57/80: current_loss=16.03467 | best_loss=8.73028
Epoch 58/80: current_loss=10.27904 | best_loss=8.73028
Epoch 59/80: current_loss=10.27262 | best_loss=8.73028
Epoch 60/80: current_loss=14.23049 | best_loss=8.73028
Epoch 61/80: current_loss=12.10254 | best_loss=8.73028
Epoch 62/80: current_loss=8.82375 | best_loss=8.73028
Epoch 63/80: current_loss=9.22194 | best_loss=8.73028
Epoch 64/80: current_loss=9.82339 | best_loss=8.73028
Epoch 65/80: current_loss=11.95492 | best_loss=8.73028
Early Stopping at epoch 65
      explained_var=0.00495 | mse_loss=8.50159
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.43502 | best_loss=9.43502
Epoch 1/80: current_loss=10.20295 | best_loss=9.43502
Epoch 2/80: current_loss=23.36676 | best_loss=9.43502
Epoch 3/80: current_loss=15.75554 | best_loss=9.43502
Epoch 4/80: current_loss=9.48902 | best_loss=9.43502
Epoch 5/80: current_loss=8.41492 | best_loss=8.41492
Epoch 6/80: current_loss=10.33019 | best_loss=8.41492
Epoch 7/80: current_loss=13.14489 | best_loss=8.41492
Epoch 8/80: current_loss=8.43813 | best_loss=8.41492
Epoch 9/80: current_loss=8.40914 | best_loss=8.40914
Epoch 10/80: current_loss=11.23674 | best_loss=8.40914
Epoch 11/80: current_loss=9.85794 | best_loss=8.40914
Epoch 12/80: current_loss=8.58373 | best_loss=8.40914
Epoch 13/80: current_loss=8.29295 | best_loss=8.29295
Epoch 14/80: current_loss=9.51575 | best_loss=8.29295
Epoch 15/80: current_loss=8.27368 | best_loss=8.27368
Epoch 16/80: current_loss=8.29424 | best_loss=8.27368
Epoch 17/80: current_loss=9.04062 | best_loss=8.27368
Epoch 18/80: current_loss=10.48012 | best_loss=8.27368
Epoch 19/80: current_loss=18.57103 | best_loss=8.27368
Epoch 20/80: current_loss=9.77834 | best_loss=8.27368
Epoch 21/80: current_loss=9.36520 | best_loss=8.27368
Epoch 22/80: current_loss=9.37760 | best_loss=8.27368
Epoch 23/80: current_loss=11.60766 | best_loss=8.27368
Epoch 24/80: current_loss=8.72492 | best_loss=8.27368
Epoch 25/80: current_loss=18.20339 | best_loss=8.27368
Epoch 26/80: current_loss=9.67164 | best_loss=8.27368
Epoch 27/80: current_loss=12.30237 | best_loss=8.27368
Epoch 28/80: current_loss=8.27719 | best_loss=8.27368
Epoch 29/80: current_loss=12.51364 | best_loss=8.27368
Epoch 30/80: current_loss=11.54232 | best_loss=8.27368
Epoch 31/80: current_loss=9.19003 | best_loss=8.27368
Epoch 32/80: current_loss=8.22987 | best_loss=8.22987
Epoch 33/80: current_loss=8.46536 | best_loss=8.22987
Epoch 34/80: current_loss=12.19816 | best_loss=8.22987
Epoch 35/80: current_loss=10.93164 | best_loss=8.22987
Epoch 36/80: current_loss=15.47621 | best_loss=8.22987
Epoch 37/80: current_loss=13.85215 | best_loss=8.22987
Epoch 38/80: current_loss=14.43976 | best_loss=8.22987
Epoch 39/80: current_loss=11.25253 | best_loss=8.22987
Epoch 40/80: current_loss=8.47504 | best_loss=8.22987
Epoch 41/80: current_loss=10.19273 | best_loss=8.22987
Epoch 42/80: current_loss=8.39634 | best_loss=8.22987
Epoch 43/80: current_loss=9.02987 | best_loss=8.22987
Epoch 44/80: current_loss=8.44561 | best_loss=8.22987
Epoch 45/80: current_loss=9.19471 | best_loss=8.22987
Epoch 46/80: current_loss=9.36318 | best_loss=8.22987
Epoch 47/80: current_loss=8.63933 | best_loss=8.22987
Epoch 48/80: current_loss=15.59305 | best_loss=8.22987
Epoch 49/80: current_loss=10.73841 | best_loss=8.22987
Epoch 50/80: current_loss=10.67719 | best_loss=8.22987
Epoch 51/80: current_loss=10.47985 | best_loss=8.22987
Epoch 52/80: current_loss=9.14989 | best_loss=8.22987
Early Stopping at epoch 52
      explained_var=0.00166 | mse_loss=8.32764
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.48679 | best_loss=13.48679
Epoch 1/80: current_loss=8.28376 | best_loss=8.28376
Epoch 2/80: current_loss=9.89392 | best_loss=8.28376
Epoch 3/80: current_loss=17.47520 | best_loss=8.28376
Epoch 4/80: current_loss=7.93797 | best_loss=7.93797
Epoch 5/80: current_loss=9.58370 | best_loss=7.93797
Epoch 6/80: current_loss=9.82190 | best_loss=7.93797
Epoch 7/80: current_loss=8.28981 | best_loss=7.93797
Epoch 8/80: current_loss=7.79389 | best_loss=7.79389
Epoch 9/80: current_loss=7.71932 | best_loss=7.71932
Epoch 10/80: current_loss=8.80780 | best_loss=7.71932
Epoch 11/80: current_loss=11.33390 | best_loss=7.71932
Epoch 12/80: current_loss=8.86550 | best_loss=7.71932
Epoch 13/80: current_loss=8.61913 | best_loss=7.71932
Epoch 14/80: current_loss=9.45594 | best_loss=7.71932
Epoch 15/80: current_loss=9.30373 | best_loss=7.71932
Epoch 16/80: current_loss=12.33028 | best_loss=7.71932
Epoch 17/80: current_loss=7.77165 | best_loss=7.71932
Epoch 18/80: current_loss=8.94049 | best_loss=7.71932
Epoch 19/80: current_loss=8.85688 | best_loss=7.71932
Epoch 20/80: current_loss=11.49241 | best_loss=7.71932
Epoch 21/80: current_loss=8.28980 | best_loss=7.71932
Epoch 22/80: current_loss=8.81366 | best_loss=7.71932
Epoch 23/80: current_loss=8.00737 | best_loss=7.71932
Epoch 24/80: current_loss=12.64424 | best_loss=7.71932
Epoch 25/80: current_loss=10.89897 | best_loss=7.71932
Epoch 26/80: current_loss=8.01638 | best_loss=7.71932
Epoch 27/80: current_loss=15.59338 | best_loss=7.71932
Epoch 28/80: current_loss=9.38599 | best_loss=7.71932
Epoch 29/80: current_loss=7.88950 | best_loss=7.71932
Early Stopping at epoch 29
      explained_var=-0.00585 | mse_loss=7.90658
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00442| avg_loss=8.07693
----------------------------------------------


----------------------------------------------
Params for Trial 85
{'learning_rate': 0.1, 'weight_decay': 0.003125962209276666, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=75.97290 | best_loss=75.97290
Epoch 1/80: current_loss=52.64906 | best_loss=52.64906
Epoch 2/80: current_loss=31.25936 | best_loss=31.25936
Epoch 3/80: current_loss=30.72776 | best_loss=30.72776
Epoch 4/80: current_loss=16.81146 | best_loss=16.81146
Epoch 5/80: current_loss=19.76541 | best_loss=16.81146
Epoch 6/80: current_loss=16.51492 | best_loss=16.51492
Epoch 7/80: current_loss=14.01624 | best_loss=14.01624
Epoch 8/80: current_loss=8.84646 | best_loss=8.84646
Epoch 9/80: current_loss=10.28259 | best_loss=8.84646
Epoch 10/80: current_loss=13.04876 | best_loss=8.84646
Epoch 11/80: current_loss=9.69315 | best_loss=8.84646
Epoch 12/80: current_loss=15.68986 | best_loss=8.84646
Epoch 13/80: current_loss=9.02841 | best_loss=8.84646
Epoch 14/80: current_loss=8.16797 | best_loss=8.16797
Epoch 15/80: current_loss=7.94705 | best_loss=7.94705
Epoch 16/80: current_loss=8.29750 | best_loss=7.94705
Epoch 17/80: current_loss=8.19706 | best_loss=7.94705
Epoch 18/80: current_loss=9.28243 | best_loss=7.94705
Epoch 19/80: current_loss=14.12581 | best_loss=7.94705
Epoch 20/80: current_loss=8.94672 | best_loss=7.94705
Epoch 21/80: current_loss=11.78349 | best_loss=7.94705
Epoch 22/80: current_loss=10.38804 | best_loss=7.94705
Epoch 23/80: current_loss=9.02823 | best_loss=7.94705
Epoch 24/80: current_loss=8.41693 | best_loss=7.94705
Epoch 25/80: current_loss=9.18193 | best_loss=7.94705
Epoch 26/80: current_loss=9.65453 | best_loss=7.94705
Epoch 27/80: current_loss=8.88626 | best_loss=7.94705
Epoch 28/80: current_loss=8.18238 | best_loss=7.94705
Epoch 29/80: current_loss=8.31299 | best_loss=7.94705
Epoch 30/80: current_loss=9.14436 | best_loss=7.94705
Epoch 31/80: current_loss=15.68950 | best_loss=7.94705
Epoch 32/80: current_loss=17.62084 | best_loss=7.94705
Epoch 33/80: current_loss=13.39857 | best_loss=7.94705
Epoch 34/80: current_loss=8.33957 | best_loss=7.94705
Epoch 35/80: current_loss=8.98002 | best_loss=7.94705
Early Stopping at epoch 35
      explained_var=-0.00261 | mse_loss=7.83958

----------------------------------------------
Params for Trial 86
{'learning_rate': 0.0001, 'weight_decay': 0.00159797844566313, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=75.03588 | best_loss=75.03588
Epoch 1/80: current_loss=65.45528 | best_loss=65.45528
Epoch 2/80: current_loss=50.57173 | best_loss=50.57173
Epoch 3/80: current_loss=37.17674 | best_loss=37.17674
Epoch 4/80: current_loss=28.57805 | best_loss=28.57805
Epoch 5/80: current_loss=23.41506 | best_loss=23.41506
Epoch 6/80: current_loss=20.04622 | best_loss=20.04622
Epoch 7/80: current_loss=17.78216 | best_loss=17.78216
Epoch 8/80: current_loss=16.17419 | best_loss=16.17419
Epoch 9/80: current_loss=14.93104 | best_loss=14.93104
Epoch 10/80: current_loss=13.87443 | best_loss=13.87443
Epoch 11/80: current_loss=12.99935 | best_loss=12.99935
Epoch 12/80: current_loss=12.31127 | best_loss=12.31127
Epoch 13/80: current_loss=11.72269 | best_loss=11.72269
Epoch 14/80: current_loss=11.21658 | best_loss=11.21658
Epoch 15/80: current_loss=10.77339 | best_loss=10.77339
Epoch 16/80: current_loss=10.38914 | best_loss=10.38914
Epoch 17/80: current_loss=10.04348 | best_loss=10.04348
Epoch 18/80: current_loss=9.74644 | best_loss=9.74644
Epoch 19/80: current_loss=9.46697 | best_loss=9.46697
Epoch 20/80: current_loss=9.24107 | best_loss=9.24107
Epoch 21/80: current_loss=9.04016 | best_loss=9.04016
Epoch 22/80: current_loss=8.86685 | best_loss=8.86685
Epoch 23/80: current_loss=8.71883 | best_loss=8.71883
Epoch 24/80: current_loss=8.58574 | best_loss=8.58574
Epoch 25/80: current_loss=8.47118 | best_loss=8.47118
Epoch 26/80: current_loss=8.37094 | best_loss=8.37094
Epoch 27/80: current_loss=8.29650 | best_loss=8.29650
Epoch 28/80: current_loss=8.23716 | best_loss=8.23716
Epoch 29/80: current_loss=8.18071 | best_loss=8.18071
Epoch 30/80: current_loss=8.13781 | best_loss=8.13781
Epoch 31/80: current_loss=8.10583 | best_loss=8.10583
Epoch 32/80: current_loss=8.08397 | best_loss=8.08397
Epoch 33/80: current_loss=8.06456 | best_loss=8.06456
Epoch 34/80: current_loss=8.04681 | best_loss=8.04681
Epoch 35/80: current_loss=8.03503 | best_loss=8.03503
Epoch 36/80: current_loss=8.03041 | best_loss=8.03041
Epoch 37/80: current_loss=8.02891 | best_loss=8.02891
Epoch 38/80: current_loss=8.03013 | best_loss=8.02891
Epoch 39/80: current_loss=8.03188 | best_loss=8.02891
Epoch 40/80: current_loss=8.03116 | best_loss=8.02891
Epoch 41/80: current_loss=8.02446 | best_loss=8.02446
Epoch 42/80: current_loss=8.02577 | best_loss=8.02446
Epoch 43/80: current_loss=8.02708 | best_loss=8.02446
Epoch 44/80: current_loss=8.02250 | best_loss=8.02250
Epoch 45/80: current_loss=8.02293 | best_loss=8.02250
Epoch 46/80: current_loss=8.02634 | best_loss=8.02250
Epoch 47/80: current_loss=8.03510 | best_loss=8.02250
Epoch 48/80: current_loss=8.04257 | best_loss=8.02250
Epoch 49/80: current_loss=8.05010 | best_loss=8.02250
Epoch 50/80: current_loss=8.06015 | best_loss=8.02250
Epoch 51/80: current_loss=8.07135 | best_loss=8.02250
Epoch 52/80: current_loss=8.06942 | best_loss=8.02250
Epoch 53/80: current_loss=8.06678 | best_loss=8.02250
Epoch 54/80: current_loss=8.06663 | best_loss=8.02250
Epoch 55/80: current_loss=8.07319 | best_loss=8.02250
Epoch 56/80: current_loss=8.08042 | best_loss=8.02250
Epoch 57/80: current_loss=8.07227 | best_loss=8.02250
Epoch 58/80: current_loss=8.08120 | best_loss=8.02250
Epoch 59/80: current_loss=8.08856 | best_loss=8.02250
Epoch 60/80: current_loss=8.07763 | best_loss=8.02250
Epoch 61/80: current_loss=8.07863 | best_loss=8.02250
Epoch 62/80: current_loss=8.08165 | best_loss=8.02250
Epoch 63/80: current_loss=8.08852 | best_loss=8.02250
Epoch 64/80: current_loss=8.09249 | best_loss=8.02250
Early Stopping at epoch 64
      explained_var=-0.01673 | mse_loss=7.88792

----------------------------------------------
Params for Trial 87
{'learning_rate': 1e-05, 'weight_decay': 0.004206720874971614, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=82.72580 | best_loss=82.72580
Epoch 1/80: current_loss=82.59219 | best_loss=82.59219
Epoch 2/80: current_loss=82.45978 | best_loss=82.45978
Epoch 3/80: current_loss=82.32846 | best_loss=82.32846
Epoch 4/80: current_loss=82.19687 | best_loss=82.19687
Epoch 5/80: current_loss=82.06557 | best_loss=82.06557
Epoch 6/80: current_loss=81.93214 | best_loss=81.93214
Epoch 7/80: current_loss=81.79870 | best_loss=81.79870
Epoch 8/80: current_loss=81.66639 | best_loss=81.66639
Epoch 9/80: current_loss=81.53229 | best_loss=81.53229
Epoch 10/80: current_loss=81.39684 | best_loss=81.39684
Epoch 11/80: current_loss=81.26068 | best_loss=81.26068
Epoch 12/80: current_loss=81.12614 | best_loss=81.12614
Epoch 13/80: current_loss=80.98866 | best_loss=80.98866
Epoch 14/80: current_loss=80.85175 | best_loss=80.85175
Epoch 15/80: current_loss=80.71557 | best_loss=80.71557
Epoch 16/80: current_loss=80.57743 | best_loss=80.57743
Epoch 17/80: current_loss=80.43900 | best_loss=80.43900
Epoch 18/80: current_loss=80.29838 | best_loss=80.29838
Epoch 19/80: current_loss=80.15662 | best_loss=80.15662
Epoch 20/80: current_loss=80.01189 | best_loss=80.01189
Epoch 21/80: current_loss=79.86535 | best_loss=79.86535
Epoch 22/80: current_loss=79.71798 | best_loss=79.71798
Epoch 23/80: current_loss=79.56918 | best_loss=79.56918
Epoch 24/80: current_loss=79.42015 | best_loss=79.42015
Epoch 25/80: current_loss=79.26747 | best_loss=79.26747
Epoch 26/80: current_loss=79.11393 | best_loss=79.11393
Epoch 27/80: current_loss=78.95545 | best_loss=78.95545
Epoch 28/80: current_loss=78.79775 | best_loss=78.79775
Epoch 29/80: current_loss=78.63853 | best_loss=78.63853
Epoch 30/80: current_loss=78.47674 | best_loss=78.47674
Epoch 31/80: current_loss=78.31189 | best_loss=78.31189
Epoch 32/80: current_loss=78.14413 | best_loss=78.14413
Epoch 33/80: current_loss=77.97615 | best_loss=77.97615
Epoch 34/80: current_loss=77.80407 | best_loss=77.80407
Epoch 35/80: current_loss=77.62936 | best_loss=77.62936
Epoch 36/80: current_loss=77.45364 | best_loss=77.45364
Epoch 37/80: current_loss=77.27522 | best_loss=77.27522
Epoch 38/80: current_loss=77.09561 | best_loss=77.09561
Epoch 39/80: current_loss=76.91395 | best_loss=76.91395
Epoch 40/80: current_loss=76.73169 | best_loss=76.73169
Epoch 41/80: current_loss=76.54273 | best_loss=76.54273
Epoch 42/80: current_loss=76.35330 | best_loss=76.35330
Epoch 43/80: current_loss=76.16155 | best_loss=76.16155
Epoch 44/80: current_loss=75.96461 | best_loss=75.96461
Epoch 45/80: current_loss=75.76766 | best_loss=75.76766
Epoch 46/80: current_loss=75.56646 | best_loss=75.56646
Epoch 47/80: current_loss=75.36451 | best_loss=75.36451
Epoch 48/80: current_loss=75.16273 | best_loss=75.16273
Epoch 49/80: current_loss=74.96141 | best_loss=74.96141
Epoch 50/80: current_loss=74.75521 | best_loss=74.75521
Epoch 51/80: current_loss=74.54491 | best_loss=74.54491
Epoch 52/80: current_loss=74.33197 | best_loss=74.33197
Epoch 53/80: current_loss=74.11591 | best_loss=74.11591
Epoch 54/80: current_loss=73.89876 | best_loss=73.89876
Epoch 55/80: current_loss=73.68073 | best_loss=73.68073
Epoch 56/80: current_loss=73.45922 | best_loss=73.45922
Epoch 57/80: current_loss=73.23522 | best_loss=73.23522
Epoch 58/80: current_loss=73.01204 | best_loss=73.01204
Epoch 59/80: current_loss=72.78591 | best_loss=72.78591
Epoch 60/80: current_loss=72.55764 | best_loss=72.55764
Epoch 61/80: current_loss=72.32956 | best_loss=72.32956
Epoch 62/80: current_loss=72.09579 | best_loss=72.09579
Epoch 63/80: current_loss=71.86118 | best_loss=71.86118
Epoch 64/80: current_loss=71.62259 | best_loss=71.62259
Epoch 65/80: current_loss=71.38473 | best_loss=71.38473
Epoch 66/80: current_loss=71.14436 | best_loss=71.14436
Epoch 67/80: current_loss=70.90480 | best_loss=70.90480
Epoch 68/80: current_loss=70.65933 | best_loss=70.65933
Epoch 69/80: current_loss=70.41396 | best_loss=70.41396
Epoch 70/80: current_loss=70.16823 | best_loss=70.16823
Epoch 71/80: current_loss=69.92184 | best_loss=69.92184
Epoch 72/80: current_loss=69.67307 | best_loss=69.67307
Epoch 73/80: current_loss=69.42190 | best_loss=69.42190
Epoch 74/80: current_loss=69.17132 | best_loss=69.17132
Epoch 75/80: current_loss=68.91884 | best_loss=68.91884
Epoch 76/80: current_loss=68.66341 | best_loss=68.66341
Epoch 77/80: current_loss=68.40925 | best_loss=68.40925
Epoch 78/80: current_loss=68.15123 | best_loss=68.15123
Epoch 79/80: current_loss=67.89520 | best_loss=67.89520
      explained_var=-0.00396 | mse_loss=66.91646

----------------------------------------------
Params for Trial 88
{'learning_rate': 0.1, 'weight_decay': 0.0018531758797334442, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.75518 | best_loss=12.75518
Epoch 1/80: current_loss=8.74597 | best_loss=8.74597
Epoch 2/80: current_loss=8.91429 | best_loss=8.74597
Epoch 3/80: current_loss=12.39403 | best_loss=8.74597
Epoch 4/80: current_loss=9.07820 | best_loss=8.74597
Epoch 5/80: current_loss=8.79352 | best_loss=8.74597
Epoch 6/80: current_loss=8.23375 | best_loss=8.23375
Epoch 7/80: current_loss=8.51448 | best_loss=8.23375
Epoch 8/80: current_loss=14.06949 | best_loss=8.23375
Epoch 9/80: current_loss=7.76877 | best_loss=7.76877
Epoch 10/80: current_loss=8.29798 | best_loss=7.76877
Epoch 11/80: current_loss=13.81939 | best_loss=7.76877
Epoch 12/80: current_loss=8.07448 | best_loss=7.76877
Epoch 13/80: current_loss=12.38794 | best_loss=7.76877
Epoch 14/80: current_loss=8.26218 | best_loss=7.76877
Epoch 15/80: current_loss=9.43843 | best_loss=7.76877
Epoch 16/80: current_loss=9.37880 | best_loss=7.76877
Epoch 17/80: current_loss=10.97850 | best_loss=7.76877
Epoch 18/80: current_loss=12.77296 | best_loss=7.76877
Epoch 19/80: current_loss=9.90977 | best_loss=7.76877
Epoch 20/80: current_loss=7.99779 | best_loss=7.76877
Epoch 21/80: current_loss=10.37603 | best_loss=7.76877
Epoch 22/80: current_loss=25.41236 | best_loss=7.76877
Epoch 23/80: current_loss=8.77932 | best_loss=7.76877
Epoch 24/80: current_loss=8.25525 | best_loss=7.76877
Epoch 25/80: current_loss=8.11165 | best_loss=7.76877
Epoch 26/80: current_loss=22.33706 | best_loss=7.76877
Epoch 27/80: current_loss=10.17258 | best_loss=7.76877
Epoch 28/80: current_loss=11.75307 | best_loss=7.76877
Epoch 29/80: current_loss=26.89305 | best_loss=7.76877
Early Stopping at epoch 29
      explained_var=0.00170 | mse_loss=7.60354

----------------------------------------------
Params for Trial 89
{'learning_rate': 0.1, 'weight_decay': 0.007020234536633536, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=126.67577 | best_loss=126.67577
Epoch 1/80: current_loss=44.68708 | best_loss=44.68708
Epoch 2/80: current_loss=20.16687 | best_loss=20.16687
Epoch 3/80: current_loss=7.82794 | best_loss=7.82794
Epoch 4/80: current_loss=8.00347 | best_loss=7.82794
Epoch 5/80: current_loss=7.96752 | best_loss=7.82794
Epoch 6/80: current_loss=7.82203 | best_loss=7.82203
Epoch 7/80: current_loss=8.92223 | best_loss=7.82203
Epoch 8/80: current_loss=11.70259 | best_loss=7.82203
Epoch 9/80: current_loss=7.83080 | best_loss=7.82203
Epoch 10/80: current_loss=9.07407 | best_loss=7.82203
Epoch 11/80: current_loss=8.23166 | best_loss=7.82203
Epoch 12/80: current_loss=8.22726 | best_loss=7.82203
Epoch 13/80: current_loss=7.79134 | best_loss=7.79134
Epoch 14/80: current_loss=8.66626 | best_loss=7.79134
Epoch 15/80: current_loss=7.96970 | best_loss=7.79134
Epoch 16/80: current_loss=8.53359 | best_loss=7.79134
Epoch 17/80: current_loss=7.77279 | best_loss=7.77279
Epoch 18/80: current_loss=8.38194 | best_loss=7.77279
Epoch 19/80: current_loss=8.71379 | best_loss=7.77279
Epoch 20/80: current_loss=10.27883 | best_loss=7.77279
Epoch 21/80: current_loss=9.59900 | best_loss=7.77279
Epoch 22/80: current_loss=9.45285 | best_loss=7.77279
Epoch 23/80: current_loss=9.05643 | best_loss=7.77279
Epoch 24/80: current_loss=9.24633 | best_loss=7.77279
Epoch 25/80: current_loss=10.19690 | best_loss=7.77279
Epoch 26/80: current_loss=8.01095 | best_loss=7.77279
Epoch 27/80: current_loss=7.77918 | best_loss=7.77279
Epoch 28/80: current_loss=7.88605 | best_loss=7.77279
Epoch 29/80: current_loss=7.94414 | best_loss=7.77279
Epoch 30/80: current_loss=8.15487 | best_loss=7.77279
Epoch 31/80: current_loss=9.97287 | best_loss=7.77279
Epoch 32/80: current_loss=8.34392 | best_loss=7.77279
Epoch 33/80: current_loss=7.81897 | best_loss=7.77279
Epoch 34/80: current_loss=9.50648 | best_loss=7.77279
Epoch 35/80: current_loss=32.31397 | best_loss=7.77279
Epoch 36/80: current_loss=9.83445 | best_loss=7.77279
Epoch 37/80: current_loss=8.14482 | best_loss=7.77279
Early Stopping at epoch 37
      explained_var=0.00036 | mse_loss=7.61191

----------------------------------------------
Params for Trial 90
{'learning_rate': 0.001, 'weight_decay': 0.0004714800024636888, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=20.36852 | best_loss=20.36852
Epoch 1/80: current_loss=10.56029 | best_loss=10.56029
Epoch 2/80: current_loss=8.51707 | best_loss=8.51707
Epoch 3/80: current_loss=8.68076 | best_loss=8.51707
Epoch 4/80: current_loss=8.86351 | best_loss=8.51707
Epoch 5/80: current_loss=8.82404 | best_loss=8.51707
Epoch 6/80: current_loss=8.46059 | best_loss=8.46059
Epoch 7/80: current_loss=8.26718 | best_loss=8.26718
Epoch 8/80: current_loss=8.25790 | best_loss=8.25790
Epoch 9/80: current_loss=8.24619 | best_loss=8.24619
Epoch 10/80: current_loss=8.24296 | best_loss=8.24296
Epoch 11/80: current_loss=8.05564 | best_loss=8.05564
Epoch 12/80: current_loss=8.06431 | best_loss=8.05564
Epoch 13/80: current_loss=8.11171 | best_loss=8.05564
Epoch 14/80: current_loss=8.11043 | best_loss=8.05564
Epoch 15/80: current_loss=8.11962 | best_loss=8.05564
Epoch 16/80: current_loss=8.02333 | best_loss=8.02333
Epoch 17/80: current_loss=8.06615 | best_loss=8.02333
Epoch 18/80: current_loss=8.06427 | best_loss=8.02333
Epoch 19/80: current_loss=8.00794 | best_loss=8.00794
Epoch 20/80: current_loss=8.17701 | best_loss=8.00794
Epoch 21/80: current_loss=8.12323 | best_loss=8.00794
Epoch 22/80: current_loss=8.02785 | best_loss=8.00794
Epoch 23/80: current_loss=8.05090 | best_loss=8.00794
Epoch 24/80: current_loss=7.99129 | best_loss=7.99129
Epoch 25/80: current_loss=8.05292 | best_loss=7.99129
Epoch 26/80: current_loss=8.10639 | best_loss=7.99129
Epoch 27/80: current_loss=8.24038 | best_loss=7.99129
Epoch 28/80: current_loss=8.08704 | best_loss=7.99129
Epoch 29/80: current_loss=8.10201 | best_loss=7.99129
Epoch 30/80: current_loss=8.29399 | best_loss=7.99129
Epoch 31/80: current_loss=8.20811 | best_loss=7.99129
Epoch 32/80: current_loss=8.02671 | best_loss=7.99129
Epoch 33/80: current_loss=8.13181 | best_loss=7.99129
Epoch 34/80: current_loss=8.09938 | best_loss=7.99129
Epoch 35/80: current_loss=8.04091 | best_loss=7.99129
Epoch 36/80: current_loss=8.04128 | best_loss=7.99129
Epoch 37/80: current_loss=7.89587 | best_loss=7.89587
Epoch 38/80: current_loss=7.91028 | best_loss=7.89587
Epoch 39/80: current_loss=8.20744 | best_loss=7.89587
Epoch 40/80: current_loss=8.05799 | best_loss=7.89587
Epoch 41/80: current_loss=7.96002 | best_loss=7.89587
Epoch 42/80: current_loss=7.93421 | best_loss=7.89587
Epoch 43/80: current_loss=8.22036 | best_loss=7.89587
Epoch 44/80: current_loss=8.31318 | best_loss=7.89587
Epoch 45/80: current_loss=8.06551 | best_loss=7.89587
Epoch 46/80: current_loss=8.08004 | best_loss=7.89587
Epoch 47/80: current_loss=8.14073 | best_loss=7.89587
Epoch 48/80: current_loss=8.09459 | best_loss=7.89587
Epoch 49/80: current_loss=7.98769 | best_loss=7.89587
Epoch 50/80: current_loss=8.12017 | best_loss=7.89587
Epoch 51/80: current_loss=7.91988 | best_loss=7.89587
Epoch 52/80: current_loss=8.02705 | best_loss=7.89587
Epoch 53/80: current_loss=8.25395 | best_loss=7.89587
Epoch 54/80: current_loss=8.07212 | best_loss=7.89587
Epoch 55/80: current_loss=8.01249 | best_loss=7.89587
Epoch 56/80: current_loss=8.02182 | best_loss=7.89587
Epoch 57/80: current_loss=8.15586 | best_loss=7.89587
Early Stopping at epoch 57
      explained_var=0.00491 | mse_loss=7.75831

----------------------------------------------
Params for Trial 91
{'learning_rate': 0.1, 'weight_decay': 0.0034148415160391403, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.91636 | best_loss=11.91636
Epoch 1/80: current_loss=9.18063 | best_loss=9.18063
Epoch 2/80: current_loss=9.84247 | best_loss=9.18063
Epoch 3/80: current_loss=8.05634 | best_loss=8.05634
Epoch 4/80: current_loss=8.52678 | best_loss=8.05634
Epoch 5/80: current_loss=10.65974 | best_loss=8.05634
Epoch 6/80: current_loss=19.09432 | best_loss=8.05634
Epoch 7/80: current_loss=10.30465 | best_loss=8.05634
Epoch 8/80: current_loss=9.87364 | best_loss=8.05634
Epoch 9/80: current_loss=66.91800 | best_loss=8.05634
Epoch 10/80: current_loss=17.16948 | best_loss=8.05634
Epoch 11/80: current_loss=13.01471 | best_loss=8.05634
Epoch 12/80: current_loss=7.96395 | best_loss=7.96395
Epoch 13/80: current_loss=10.16607 | best_loss=7.96395
Epoch 14/80: current_loss=8.79446 | best_loss=7.96395
Epoch 15/80: current_loss=9.61851 | best_loss=7.96395
Epoch 16/80: current_loss=8.80855 | best_loss=7.96395
Epoch 17/80: current_loss=21.92961 | best_loss=7.96395
Epoch 18/80: current_loss=13.32458 | best_loss=7.96395
Epoch 19/80: current_loss=9.65909 | best_loss=7.96395
Epoch 20/80: current_loss=8.20166 | best_loss=7.96395
Epoch 21/80: current_loss=16.29628 | best_loss=7.96395
Epoch 22/80: current_loss=8.71009 | best_loss=7.96395
Epoch 23/80: current_loss=9.51942 | best_loss=7.96395
Epoch 24/80: current_loss=10.18468 | best_loss=7.96395
Epoch 25/80: current_loss=8.57436 | best_loss=7.96395
Epoch 26/80: current_loss=10.41565 | best_loss=7.96395
Epoch 27/80: current_loss=8.09893 | best_loss=7.96395
Epoch 28/80: current_loss=10.20265 | best_loss=7.96395
Epoch 29/80: current_loss=15.66008 | best_loss=7.96395
Epoch 30/80: current_loss=20.42296 | best_loss=7.96395
Epoch 31/80: current_loss=9.72199 | best_loss=7.96395
Epoch 32/80: current_loss=8.80942 | best_loss=7.96395
Early Stopping at epoch 32
      explained_var=-0.01758 | mse_loss=7.81356

----------------------------------------------
Params for Trial 92
{'learning_rate': 0.1, 'weight_decay': 0.003954949803359157, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.50193 | best_loss=13.50193
Epoch 1/80: current_loss=8.08733 | best_loss=8.08733
Epoch 2/80: current_loss=8.26933 | best_loss=8.08733
Epoch 3/80: current_loss=9.40707 | best_loss=8.08733
Epoch 4/80: current_loss=9.53640 | best_loss=8.08733
Epoch 5/80: current_loss=14.41549 | best_loss=8.08733
Epoch 6/80: current_loss=9.35839 | best_loss=8.08733
Epoch 7/80: current_loss=11.12889 | best_loss=8.08733
Epoch 8/80: current_loss=11.97553 | best_loss=8.08733
Epoch 9/80: current_loss=12.52168 | best_loss=8.08733
Epoch 10/80: current_loss=12.74225 | best_loss=8.08733
Epoch 11/80: current_loss=9.55269 | best_loss=8.08733
Epoch 12/80: current_loss=8.31397 | best_loss=8.08733
Epoch 13/80: current_loss=9.98923 | best_loss=8.08733
Epoch 14/80: current_loss=9.19232 | best_loss=8.08733
Epoch 15/80: current_loss=7.97404 | best_loss=7.97404
Epoch 16/80: current_loss=10.73625 | best_loss=7.97404
Epoch 17/80: current_loss=9.11701 | best_loss=7.97404
Epoch 18/80: current_loss=7.93756 | best_loss=7.93756
Epoch 19/80: current_loss=9.46737 | best_loss=7.93756
Epoch 20/80: current_loss=20.38645 | best_loss=7.93756
Epoch 21/80: current_loss=14.91557 | best_loss=7.93756
Epoch 22/80: current_loss=20.84397 | best_loss=7.93756
Epoch 23/80: current_loss=10.70833 | best_loss=7.93756
Epoch 24/80: current_loss=8.21049 | best_loss=7.93756
Epoch 25/80: current_loss=8.06881 | best_loss=7.93756
Epoch 26/80: current_loss=8.84401 | best_loss=7.93756
Epoch 27/80: current_loss=8.25624 | best_loss=7.93756
Epoch 28/80: current_loss=7.92641 | best_loss=7.92641
Epoch 29/80: current_loss=9.09239 | best_loss=7.92641
Epoch 30/80: current_loss=11.14507 | best_loss=7.92641
Epoch 31/80: current_loss=16.13162 | best_loss=7.92641
Epoch 32/80: current_loss=7.77419 | best_loss=7.77419
Epoch 33/80: current_loss=9.55035 | best_loss=7.77419
Epoch 34/80: current_loss=8.46558 | best_loss=7.77419
Epoch 35/80: current_loss=7.76757 | best_loss=7.76757
Epoch 36/80: current_loss=8.31725 | best_loss=7.76757
Epoch 37/80: current_loss=19.12490 | best_loss=7.76757
Epoch 38/80: current_loss=27.43872 | best_loss=7.76757
Epoch 39/80: current_loss=8.15088 | best_loss=7.76757
Epoch 40/80: current_loss=7.75614 | best_loss=7.75614
Epoch 41/80: current_loss=7.77307 | best_loss=7.75614
Epoch 42/80: current_loss=16.87074 | best_loss=7.75614
Epoch 43/80: current_loss=7.76268 | best_loss=7.75614
Epoch 44/80: current_loss=7.77038 | best_loss=7.75614
Epoch 45/80: current_loss=9.60145 | best_loss=7.75614
Epoch 46/80: current_loss=7.95478 | best_loss=7.75614
Epoch 47/80: current_loss=7.76353 | best_loss=7.75614
Epoch 48/80: current_loss=7.78138 | best_loss=7.75614
Epoch 49/80: current_loss=8.07583 | best_loss=7.75614
Epoch 50/80: current_loss=10.06502 | best_loss=7.75614
Epoch 51/80: current_loss=11.61552 | best_loss=7.75614
Epoch 52/80: current_loss=7.77850 | best_loss=7.75614
Epoch 53/80: current_loss=7.76428 | best_loss=7.75614
Epoch 54/80: current_loss=9.47195 | best_loss=7.75614
Epoch 55/80: current_loss=7.77503 | best_loss=7.75614
Epoch 56/80: current_loss=8.77580 | best_loss=7.75614
Epoch 57/80: current_loss=14.31491 | best_loss=7.75614
Epoch 58/80: current_loss=8.53453 | best_loss=7.75614
Epoch 59/80: current_loss=8.85318 | best_loss=7.75614
Epoch 60/80: current_loss=8.54694 | best_loss=7.75614
Early Stopping at epoch 60
      explained_var=0.00220 | mse_loss=7.58062
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=38.99207 | best_loss=38.99207
Epoch 1/80: current_loss=28.73148 | best_loss=28.73148
Epoch 2/80: current_loss=18.93762 | best_loss=18.93762
Epoch 3/80: current_loss=11.11517 | best_loss=11.11517
Epoch 4/80: current_loss=10.35826 | best_loss=10.35826
Epoch 5/80: current_loss=9.71799 | best_loss=9.71799
Epoch 6/80: current_loss=8.91855 | best_loss=8.91855
Epoch 7/80: current_loss=11.15547 | best_loss=8.91855
Epoch 8/80: current_loss=11.64622 | best_loss=8.91855
Epoch 9/80: current_loss=9.58711 | best_loss=8.91855
Epoch 10/80: current_loss=12.04160 | best_loss=8.91855
Epoch 11/80: current_loss=8.81462 | best_loss=8.81462
Epoch 12/80: current_loss=9.13901 | best_loss=8.81462
Epoch 13/80: current_loss=9.67717 | best_loss=8.81462
Epoch 14/80: current_loss=12.75907 | best_loss=8.81462
Epoch 15/80: current_loss=10.20352 | best_loss=8.81462
Epoch 16/80: current_loss=12.79966 | best_loss=8.81462
Epoch 17/80: current_loss=9.42278 | best_loss=8.81462
Epoch 18/80: current_loss=8.56897 | best_loss=8.56897
Epoch 19/80: current_loss=11.72636 | best_loss=8.56897
Epoch 20/80: current_loss=10.45492 | best_loss=8.56897
Epoch 21/80: current_loss=8.82851 | best_loss=8.56897
Epoch 22/80: current_loss=8.53656 | best_loss=8.53656
Epoch 23/80: current_loss=10.11669 | best_loss=8.53656
Epoch 24/80: current_loss=8.71656 | best_loss=8.53656
Epoch 25/80: current_loss=10.56426 | best_loss=8.53656
Epoch 26/80: current_loss=13.83583 | best_loss=8.53656
Epoch 27/80: current_loss=11.80090 | best_loss=8.53656
Epoch 28/80: current_loss=15.72250 | best_loss=8.53656
Epoch 29/80: current_loss=9.92824 | best_loss=8.53656
Epoch 30/80: current_loss=8.73032 | best_loss=8.53656
Epoch 31/80: current_loss=9.51236 | best_loss=8.53656
Epoch 32/80: current_loss=10.91872 | best_loss=8.53656
Epoch 33/80: current_loss=13.00531 | best_loss=8.53656
Epoch 34/80: current_loss=11.73593 | best_loss=8.53656
Epoch 35/80: current_loss=20.73610 | best_loss=8.53656
Epoch 36/80: current_loss=13.07919 | best_loss=8.53656
Epoch 37/80: current_loss=9.83275 | best_loss=8.53656
Epoch 38/80: current_loss=17.77056 | best_loss=8.53656
Epoch 39/80: current_loss=17.32593 | best_loss=8.53656
Epoch 40/80: current_loss=12.58862 | best_loss=8.53656
Epoch 41/80: current_loss=10.58898 | best_loss=8.53656
Epoch 42/80: current_loss=10.28659 | best_loss=8.53656
Early Stopping at epoch 42
      explained_var=-0.03139 | mse_loss=8.42027
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.16441 | best_loss=9.16441
Epoch 1/80: current_loss=10.90322 | best_loss=9.16441
Epoch 2/80: current_loss=9.65790 | best_loss=9.16441
Epoch 3/80: current_loss=9.15708 | best_loss=9.15708
Epoch 4/80: current_loss=10.66201 | best_loss=9.15708
Epoch 5/80: current_loss=10.59162 | best_loss=9.15708
Epoch 6/80: current_loss=10.62213 | best_loss=9.15708
Epoch 7/80: current_loss=10.38703 | best_loss=9.15708
Epoch 8/80: current_loss=8.87973 | best_loss=8.87973
Epoch 9/80: current_loss=8.72055 | best_loss=8.72055
Epoch 10/80: current_loss=11.23173 | best_loss=8.72055
Epoch 11/80: current_loss=9.93019 | best_loss=8.72055
Epoch 12/80: current_loss=22.29295 | best_loss=8.72055
Epoch 13/80: current_loss=9.76368 | best_loss=8.72055
Epoch 14/80: current_loss=8.96532 | best_loss=8.72055
Epoch 15/80: current_loss=13.57922 | best_loss=8.72055
Epoch 16/80: current_loss=8.78633 | best_loss=8.72055
Epoch 17/80: current_loss=9.36443 | best_loss=8.72055
Epoch 18/80: current_loss=10.02149 | best_loss=8.72055
Epoch 19/80: current_loss=10.51104 | best_loss=8.72055
Epoch 20/80: current_loss=9.45682 | best_loss=8.72055
Epoch 21/80: current_loss=8.82238 | best_loss=8.72055
Epoch 22/80: current_loss=11.76986 | best_loss=8.72055
Epoch 23/80: current_loss=8.96644 | best_loss=8.72055
Epoch 24/80: current_loss=13.23698 | best_loss=8.72055
Epoch 25/80: current_loss=9.91525 | best_loss=8.72055
Epoch 26/80: current_loss=8.98732 | best_loss=8.72055
Epoch 27/80: current_loss=9.28855 | best_loss=8.72055
Epoch 28/80: current_loss=19.71808 | best_loss=8.72055
Epoch 29/80: current_loss=17.70567 | best_loss=8.72055
Early Stopping at epoch 29
      explained_var=-0.00047 | mse_loss=8.49059
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.47698 | best_loss=8.47698
Epoch 1/80: current_loss=9.61967 | best_loss=8.47698
Epoch 2/80: current_loss=9.14266 | best_loss=8.47698
Epoch 3/80: current_loss=8.28517 | best_loss=8.28517
Epoch 4/80: current_loss=8.47888 | best_loss=8.28517
Epoch 5/80: current_loss=8.31000 | best_loss=8.28517
Epoch 6/80: current_loss=8.46904 | best_loss=8.28517
Epoch 7/80: current_loss=8.27358 | best_loss=8.27358
Epoch 8/80: current_loss=8.24844 | best_loss=8.24844
Epoch 9/80: current_loss=9.74644 | best_loss=8.24844
Epoch 10/80: current_loss=8.50658 | best_loss=8.24844
Epoch 11/80: current_loss=8.81560 | best_loss=8.24844
Epoch 12/80: current_loss=10.05402 | best_loss=8.24844
Epoch 13/80: current_loss=8.31356 | best_loss=8.24844
Epoch 14/80: current_loss=8.25767 | best_loss=8.24844
Epoch 15/80: current_loss=8.26713 | best_loss=8.24844
Epoch 16/80: current_loss=10.88299 | best_loss=8.24844
Epoch 17/80: current_loss=16.19621 | best_loss=8.24844
Epoch 18/80: current_loss=10.99492 | best_loss=8.24844
Epoch 19/80: current_loss=8.50550 | best_loss=8.24844
Epoch 20/80: current_loss=8.25861 | best_loss=8.24844
Epoch 21/80: current_loss=8.66018 | best_loss=8.24844
Epoch 22/80: current_loss=8.27082 | best_loss=8.24844
Epoch 23/80: current_loss=8.87967 | best_loss=8.24844
Epoch 24/80: current_loss=9.43058 | best_loss=8.24844
Epoch 25/80: current_loss=8.91526 | best_loss=8.24844
Epoch 26/80: current_loss=8.87061 | best_loss=8.24844
Epoch 27/80: current_loss=8.31773 | best_loss=8.24844
Epoch 28/80: current_loss=12.53737 | best_loss=8.24844
Early Stopping at epoch 28
      explained_var=-0.00066 | mse_loss=8.35905
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.43867 | best_loss=13.43867
Epoch 1/80: current_loss=8.61114 | best_loss=8.61114
Epoch 2/80: current_loss=9.73594 | best_loss=8.61114
Epoch 3/80: current_loss=9.74896 | best_loss=8.61114
Epoch 4/80: current_loss=8.33519 | best_loss=8.33519
Epoch 5/80: current_loss=11.02840 | best_loss=8.33519
Epoch 6/80: current_loss=10.79609 | best_loss=8.33519
Epoch 7/80: current_loss=8.21980 | best_loss=8.21980
Epoch 8/80: current_loss=10.40967 | best_loss=8.21980
Epoch 9/80: current_loss=10.21568 | best_loss=8.21980
Epoch 10/80: current_loss=8.81138 | best_loss=8.21980
Epoch 11/80: current_loss=16.01109 | best_loss=8.21980
Epoch 12/80: current_loss=8.12390 | best_loss=8.12390
Epoch 13/80: current_loss=7.99085 | best_loss=7.99085
Epoch 14/80: current_loss=10.68267 | best_loss=7.99085
Epoch 15/80: current_loss=23.94016 | best_loss=7.99085
Epoch 16/80: current_loss=7.89946 | best_loss=7.89946
Epoch 17/80: current_loss=14.78887 | best_loss=7.89946
Epoch 18/80: current_loss=8.62127 | best_loss=7.89946
Epoch 19/80: current_loss=7.94908 | best_loss=7.89946
Epoch 20/80: current_loss=8.98094 | best_loss=7.89946
Epoch 21/80: current_loss=7.82281 | best_loss=7.82281
Epoch 22/80: current_loss=9.49991 | best_loss=7.82281
Epoch 23/80: current_loss=8.73093 | best_loss=7.82281
Epoch 24/80: current_loss=16.00116 | best_loss=7.82281
Epoch 25/80: current_loss=15.59470 | best_loss=7.82281
Epoch 26/80: current_loss=12.00440 | best_loss=7.82281
Epoch 27/80: current_loss=13.96529 | best_loss=7.82281
Epoch 28/80: current_loss=8.53186 | best_loss=7.82281
Epoch 29/80: current_loss=7.74464 | best_loss=7.74464
Epoch 30/80: current_loss=7.87948 | best_loss=7.74464
Epoch 31/80: current_loss=9.57614 | best_loss=7.74464
Epoch 32/80: current_loss=12.17196 | best_loss=7.74464
Epoch 33/80: current_loss=7.69205 | best_loss=7.69205
Epoch 34/80: current_loss=8.15714 | best_loss=7.69205
Epoch 35/80: current_loss=13.03246 | best_loss=7.69205
Epoch 36/80: current_loss=10.51941 | best_loss=7.69205
Epoch 37/80: current_loss=11.42380 | best_loss=7.69205
Epoch 38/80: current_loss=8.06927 | best_loss=7.69205
Epoch 39/80: current_loss=9.46897 | best_loss=7.69205
Epoch 40/80: current_loss=9.93831 | best_loss=7.69205
Epoch 41/80: current_loss=8.34752 | best_loss=7.69205
Epoch 42/80: current_loss=10.18693 | best_loss=7.69205
Epoch 43/80: current_loss=8.09530 | best_loss=7.69205
Epoch 44/80: current_loss=8.61405 | best_loss=7.69205
Epoch 45/80: current_loss=7.81445 | best_loss=7.69205
Epoch 46/80: current_loss=10.22363 | best_loss=7.69205
Epoch 47/80: current_loss=9.64206 | best_loss=7.69205
Epoch 48/80: current_loss=8.97017 | best_loss=7.69205
Epoch 49/80: current_loss=9.70036 | best_loss=7.69205
Epoch 50/80: current_loss=11.79934 | best_loss=7.69205
Epoch 51/80: current_loss=8.43721 | best_loss=7.69205
Epoch 52/80: current_loss=9.97196 | best_loss=7.69205
Epoch 53/80: current_loss=7.88991 | best_loss=7.69205
Early Stopping at epoch 53
      explained_var=-0.00070 | mse_loss=7.87280
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=-0.00621| avg_loss=8.14466
----------------------------------------------


----------------------------------------------
Params for Trial 93
{'learning_rate': 0.1, 'weight_decay': 0.0028857667658923804, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.50514 | best_loss=13.50514
Epoch 1/80: current_loss=9.67576 | best_loss=9.67576
Epoch 2/80: current_loss=11.02352 | best_loss=9.67576
Epoch 3/80: current_loss=11.76523 | best_loss=9.67576
Epoch 4/80: current_loss=8.38994 | best_loss=8.38994
Epoch 5/80: current_loss=10.01454 | best_loss=8.38994
Epoch 6/80: current_loss=8.31176 | best_loss=8.31176
Epoch 7/80: current_loss=9.34828 | best_loss=8.31176
Epoch 8/80: current_loss=9.89320 | best_loss=8.31176
Epoch 9/80: current_loss=9.32450 | best_loss=8.31176
Epoch 10/80: current_loss=13.12065 | best_loss=8.31176
Epoch 11/80: current_loss=8.32614 | best_loss=8.31176
Epoch 12/80: current_loss=11.34017 | best_loss=8.31176
Epoch 13/80: current_loss=11.99994 | best_loss=8.31176
Epoch 14/80: current_loss=9.48897 | best_loss=8.31176
Epoch 15/80: current_loss=8.48836 | best_loss=8.31176
Epoch 16/80: current_loss=9.51260 | best_loss=8.31176
Epoch 17/80: current_loss=9.09875 | best_loss=8.31176
Epoch 18/80: current_loss=9.09570 | best_loss=8.31176
Epoch 19/80: current_loss=12.80323 | best_loss=8.31176
Epoch 20/80: current_loss=9.21311 | best_loss=8.31176
Epoch 21/80: current_loss=8.51317 | best_loss=8.31176
Epoch 22/80: current_loss=14.19610 | best_loss=8.31176
Epoch 23/80: current_loss=8.02465 | best_loss=8.02465
Epoch 24/80: current_loss=11.51502 | best_loss=8.02465
Epoch 25/80: current_loss=26.69385 | best_loss=8.02465
Epoch 26/80: current_loss=7.73676 | best_loss=7.73676
Epoch 27/80: current_loss=9.21713 | best_loss=7.73676
Epoch 28/80: current_loss=9.21929 | best_loss=7.73676
Epoch 29/80: current_loss=8.11833 | best_loss=7.73676
Epoch 30/80: current_loss=7.92419 | best_loss=7.73676
Epoch 31/80: current_loss=7.78368 | best_loss=7.73676
Epoch 32/80: current_loss=11.42452 | best_loss=7.73676
Epoch 33/80: current_loss=9.25335 | best_loss=7.73676
Epoch 34/80: current_loss=8.87435 | best_loss=7.73676
Epoch 35/80: current_loss=14.60700 | best_loss=7.73676
Epoch 36/80: current_loss=9.75836 | best_loss=7.73676
Epoch 37/80: current_loss=8.28527 | best_loss=7.73676
Epoch 38/80: current_loss=10.42670 | best_loss=7.73676
Epoch 39/80: current_loss=8.79701 | best_loss=7.73676
Epoch 40/80: current_loss=8.85020 | best_loss=7.73676
Epoch 41/80: current_loss=8.31432 | best_loss=7.73676
Epoch 42/80: current_loss=8.87357 | best_loss=7.73676
Epoch 43/80: current_loss=9.29439 | best_loss=7.73676
Epoch 44/80: current_loss=8.32375 | best_loss=7.73676
Epoch 45/80: current_loss=9.96135 | best_loss=7.73676
Epoch 46/80: current_loss=11.45925 | best_loss=7.73676
Early Stopping at epoch 46
      explained_var=-0.00140 | mse_loss=7.60152

----------------------------------------------
Params for Trial 94
{'learning_rate': 0.1, 'weight_decay': 0.0033932131700765036, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=69.50014 | best_loss=69.50014
Epoch 1/80: current_loss=52.39612 | best_loss=52.39612
Epoch 2/80: current_loss=12.44021 | best_loss=12.44021
Epoch 3/80: current_loss=13.65797 | best_loss=12.44021
Epoch 4/80: current_loss=8.73603 | best_loss=8.73603
Epoch 5/80: current_loss=8.88147 | best_loss=8.73603
Epoch 6/80: current_loss=9.18467 | best_loss=8.73603
Epoch 7/80: current_loss=8.60212 | best_loss=8.60212
Epoch 8/80: current_loss=9.00764 | best_loss=8.60212
Epoch 9/80: current_loss=7.95931 | best_loss=7.95931
Epoch 10/80: current_loss=10.77812 | best_loss=7.95931
Epoch 11/80: current_loss=11.78367 | best_loss=7.95931
Epoch 12/80: current_loss=21.41099 | best_loss=7.95931
Epoch 13/80: current_loss=20.70599 | best_loss=7.95931
Epoch 14/80: current_loss=15.13611 | best_loss=7.95931
Epoch 15/80: current_loss=13.79265 | best_loss=7.95931
Epoch 16/80: current_loss=9.11793 | best_loss=7.95931
Epoch 17/80: current_loss=9.50190 | best_loss=7.95931
Epoch 18/80: current_loss=8.57116 | best_loss=7.95931
Epoch 19/80: current_loss=9.48325 | best_loss=7.95931
Epoch 20/80: current_loss=11.40131 | best_loss=7.95931
Epoch 21/80: current_loss=9.84079 | best_loss=7.95931
Epoch 22/80: current_loss=11.63207 | best_loss=7.95931
Epoch 23/80: current_loss=9.70564 | best_loss=7.95931
Epoch 24/80: current_loss=14.44425 | best_loss=7.95931
Epoch 25/80: current_loss=8.66867 | best_loss=7.95931
Epoch 26/80: current_loss=7.42153 | best_loss=7.42153
Epoch 27/80: current_loss=9.56616 | best_loss=7.42153
Epoch 28/80: current_loss=9.32888 | best_loss=7.42153
Epoch 29/80: current_loss=19.26597 | best_loss=7.42153
Epoch 30/80: current_loss=11.43922 | best_loss=7.42153
Epoch 31/80: current_loss=11.11661 | best_loss=7.42153
Epoch 32/80: current_loss=11.76316 | best_loss=7.42153
Epoch 33/80: current_loss=9.13103 | best_loss=7.42153
Epoch 34/80: current_loss=12.64659 | best_loss=7.42153
Epoch 35/80: current_loss=12.96604 | best_loss=7.42153
Epoch 36/80: current_loss=8.26821 | best_loss=7.42153
Epoch 37/80: current_loss=8.29856 | best_loss=7.42153
Epoch 38/80: current_loss=8.27649 | best_loss=7.42153
Epoch 39/80: current_loss=9.35477 | best_loss=7.42153
Epoch 40/80: current_loss=10.24923 | best_loss=7.42153
Epoch 41/80: current_loss=9.83649 | best_loss=7.42153
Epoch 42/80: current_loss=10.23150 | best_loss=7.42153
Epoch 43/80: current_loss=9.98289 | best_loss=7.42153
Epoch 44/80: current_loss=8.34430 | best_loss=7.42153
Epoch 45/80: current_loss=9.11150 | best_loss=7.42153
Epoch 46/80: current_loss=8.35196 | best_loss=7.42153
Early Stopping at epoch 46
      explained_var=0.06712 | mse_loss=7.33895
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.28697 | best_loss=11.28697
Epoch 1/80: current_loss=10.62765 | best_loss=10.62765
Epoch 2/80: current_loss=10.45892 | best_loss=10.45892
Epoch 3/80: current_loss=9.83100 | best_loss=9.83100
Epoch 4/80: current_loss=12.59308 | best_loss=9.83100
Epoch 5/80: current_loss=8.48833 | best_loss=8.48833
Epoch 6/80: current_loss=8.72176 | best_loss=8.48833
Epoch 7/80: current_loss=13.34408 | best_loss=8.48833
Epoch 8/80: current_loss=10.27210 | best_loss=8.48833
Epoch 9/80: current_loss=11.71526 | best_loss=8.48833
Epoch 10/80: current_loss=16.97040 | best_loss=8.48833
Epoch 11/80: current_loss=8.06480 | best_loss=8.06480
Epoch 12/80: current_loss=8.74135 | best_loss=8.06480
Epoch 13/80: current_loss=9.47210 | best_loss=8.06480
Epoch 14/80: current_loss=8.20190 | best_loss=8.06480
Epoch 15/80: current_loss=9.87902 | best_loss=8.06480
Epoch 16/80: current_loss=8.70238 | best_loss=8.06480
Epoch 17/80: current_loss=11.13872 | best_loss=8.06480
Epoch 18/80: current_loss=9.22038 | best_loss=8.06480
Epoch 19/80: current_loss=8.38344 | best_loss=8.06480
Epoch 20/80: current_loss=8.72600 | best_loss=8.06480
Epoch 21/80: current_loss=8.68855 | best_loss=8.06480
Epoch 22/80: current_loss=10.21004 | best_loss=8.06480
Epoch 23/80: current_loss=10.02759 | best_loss=8.06480
Epoch 24/80: current_loss=9.83937 | best_loss=8.06480
Epoch 25/80: current_loss=8.56031 | best_loss=8.06480
Epoch 26/80: current_loss=11.49275 | best_loss=8.06480
Epoch 27/80: current_loss=12.90319 | best_loss=8.06480
Epoch 28/80: current_loss=7.89260 | best_loss=7.89260
Epoch 29/80: current_loss=11.76222 | best_loss=7.89260
Epoch 30/80: current_loss=9.59570 | best_loss=7.89260
Epoch 31/80: current_loss=8.79684 | best_loss=7.89260
Epoch 32/80: current_loss=9.53183 | best_loss=7.89260
Epoch 33/80: current_loss=10.63104 | best_loss=7.89260
Epoch 34/80: current_loss=9.09985 | best_loss=7.89260
Epoch 35/80: current_loss=19.52128 | best_loss=7.89260
Epoch 36/80: current_loss=10.05212 | best_loss=7.89260
Epoch 37/80: current_loss=10.25871 | best_loss=7.89260
Epoch 38/80: current_loss=8.52465 | best_loss=7.89260
Epoch 39/80: current_loss=18.66154 | best_loss=7.89260
Epoch 40/80: current_loss=9.94706 | best_loss=7.89260
Epoch 41/80: current_loss=8.84720 | best_loss=7.89260
Epoch 42/80: current_loss=10.10665 | best_loss=7.89260
Epoch 43/80: current_loss=9.23720 | best_loss=7.89260
Epoch 44/80: current_loss=10.61543 | best_loss=7.89260
Epoch 45/80: current_loss=10.57865 | best_loss=7.89260
Epoch 46/80: current_loss=11.30026 | best_loss=7.89260
Epoch 47/80: current_loss=9.94794 | best_loss=7.89260
Epoch 48/80: current_loss=15.11986 | best_loss=7.89260
Early Stopping at epoch 48
      explained_var=0.05703 | mse_loss=7.72851
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.45628 | best_loss=13.45628
Epoch 1/80: current_loss=8.81010 | best_loss=8.81010
Epoch 2/80: current_loss=9.32247 | best_loss=8.81010
Epoch 3/80: current_loss=9.04229 | best_loss=8.81010
Epoch 4/80: current_loss=10.08276 | best_loss=8.81010
Epoch 5/80: current_loss=8.90704 | best_loss=8.81010
Epoch 6/80: current_loss=8.85441 | best_loss=8.81010
Epoch 7/80: current_loss=19.18035 | best_loss=8.81010
Epoch 8/80: current_loss=9.60713 | best_loss=8.81010
Epoch 9/80: current_loss=8.69763 | best_loss=8.69763
Epoch 10/80: current_loss=14.19578 | best_loss=8.69763
Epoch 11/80: current_loss=8.74955 | best_loss=8.69763
Epoch 12/80: current_loss=10.56728 | best_loss=8.69763
Epoch 13/80: current_loss=10.71570 | best_loss=8.69763
Epoch 14/80: current_loss=9.34164 | best_loss=8.69763
Epoch 15/80: current_loss=9.03578 | best_loss=8.69763
Epoch 16/80: current_loss=17.40475 | best_loss=8.69763
Epoch 17/80: current_loss=10.34235 | best_loss=8.69763
Epoch 18/80: current_loss=16.10857 | best_loss=8.69763
Epoch 19/80: current_loss=12.26768 | best_loss=8.69763
Epoch 20/80: current_loss=8.89798 | best_loss=8.69763
Epoch 21/80: current_loss=9.42557 | best_loss=8.69763
Epoch 22/80: current_loss=11.15413 | best_loss=8.69763
Epoch 23/80: current_loss=11.20191 | best_loss=8.69763
Epoch 24/80: current_loss=10.35708 | best_loss=8.69763
Epoch 25/80: current_loss=9.94920 | best_loss=8.69763
Epoch 26/80: current_loss=10.69275 | best_loss=8.69763
Epoch 27/80: current_loss=8.86738 | best_loss=8.69763
Epoch 28/80: current_loss=9.71875 | best_loss=8.69763
Epoch 29/80: current_loss=10.63144 | best_loss=8.69763
Early Stopping at epoch 29
      explained_var=0.00242 | mse_loss=8.46216
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.59691 | best_loss=14.59691
Epoch 1/80: current_loss=9.52925 | best_loss=9.52925
Epoch 2/80: current_loss=10.65487 | best_loss=9.52925
Epoch 3/80: current_loss=9.00291 | best_loss=9.00291
Epoch 4/80: current_loss=11.82791 | best_loss=9.00291
Epoch 5/80: current_loss=8.31821 | best_loss=8.31821
Epoch 6/80: current_loss=9.40768 | best_loss=8.31821
Epoch 7/80: current_loss=9.82928 | best_loss=8.31821
Epoch 8/80: current_loss=8.31957 | best_loss=8.31821
Epoch 9/80: current_loss=8.24711 | best_loss=8.24711
Epoch 10/80: current_loss=8.22595 | best_loss=8.22595
Epoch 11/80: current_loss=8.23300 | best_loss=8.22595
Epoch 12/80: current_loss=8.59838 | best_loss=8.22595
Epoch 13/80: current_loss=8.40640 | best_loss=8.22595
Epoch 14/80: current_loss=8.21913 | best_loss=8.21913
Epoch 15/80: current_loss=9.75242 | best_loss=8.21913
Epoch 16/80: current_loss=8.40777 | best_loss=8.21913
Epoch 17/80: current_loss=8.59355 | best_loss=8.21913
Epoch 18/80: current_loss=8.51154 | best_loss=8.21913
Epoch 19/80: current_loss=11.59922 | best_loss=8.21913
Epoch 20/80: current_loss=9.44678 | best_loss=8.21913
Epoch 21/80: current_loss=9.53519 | best_loss=8.21913
Epoch 22/80: current_loss=14.98670 | best_loss=8.21913
Epoch 23/80: current_loss=8.34929 | best_loss=8.21913
Epoch 24/80: current_loss=9.94102 | best_loss=8.21913
Epoch 25/80: current_loss=9.97254 | best_loss=8.21913
Epoch 26/80: current_loss=13.22247 | best_loss=8.21913
Epoch 27/80: current_loss=9.11435 | best_loss=8.21913
Epoch 28/80: current_loss=9.86428 | best_loss=8.21913
Epoch 29/80: current_loss=9.64198 | best_loss=8.21913
Epoch 30/80: current_loss=8.27289 | best_loss=8.21913
Epoch 31/80: current_loss=9.07692 | best_loss=8.21913
Epoch 32/80: current_loss=9.54886 | best_loss=8.21913
Epoch 33/80: current_loss=10.58319 | best_loss=8.21913
Epoch 34/80: current_loss=11.81704 | best_loss=8.21913
Early Stopping at epoch 34
      explained_var=0.00283 | mse_loss=8.31529
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.37521 | best_loss=13.37521
Epoch 1/80: current_loss=9.44827 | best_loss=9.44827
Epoch 2/80: current_loss=8.43952 | best_loss=8.43952
Epoch 3/80: current_loss=8.94993 | best_loss=8.43952
Epoch 4/80: current_loss=8.04389 | best_loss=8.04389
Epoch 5/80: current_loss=8.25246 | best_loss=8.04389
Epoch 6/80: current_loss=8.03518 | best_loss=8.03518
Epoch 7/80: current_loss=16.08322 | best_loss=8.03518
Epoch 8/80: current_loss=10.07178 | best_loss=8.03518
Epoch 9/80: current_loss=12.58452 | best_loss=8.03518
Epoch 10/80: current_loss=8.04124 | best_loss=8.03518
Epoch 11/80: current_loss=8.42606 | best_loss=8.03518
Epoch 12/80: current_loss=7.70782 | best_loss=7.70782
Epoch 13/80: current_loss=8.66129 | best_loss=7.70782
Epoch 14/80: current_loss=15.06247 | best_loss=7.70782
Epoch 15/80: current_loss=9.74846 | best_loss=7.70782
Epoch 16/80: current_loss=13.70425 | best_loss=7.70782
Epoch 17/80: current_loss=7.67288 | best_loss=7.67288
Epoch 18/80: current_loss=7.80755 | best_loss=7.67288
Epoch 19/80: current_loss=8.32307 | best_loss=7.67288
Epoch 20/80: current_loss=8.84653 | best_loss=7.67288
Epoch 21/80: current_loss=9.77583 | best_loss=7.67288
Epoch 22/80: current_loss=8.09151 | best_loss=7.67288
Epoch 23/80: current_loss=8.27225 | best_loss=7.67288
Epoch 24/80: current_loss=8.40474 | best_loss=7.67288
Epoch 25/80: current_loss=14.51581 | best_loss=7.67288
Epoch 26/80: current_loss=7.71523 | best_loss=7.67288
Epoch 27/80: current_loss=8.07553 | best_loss=7.67288
Epoch 28/80: current_loss=7.88421 | best_loss=7.67288
Epoch 29/80: current_loss=8.27486 | best_loss=7.67288
Epoch 30/80: current_loss=8.09548 | best_loss=7.67288
Epoch 31/80: current_loss=12.56496 | best_loss=7.67288
Epoch 32/80: current_loss=7.67573 | best_loss=7.67288
Epoch 33/80: current_loss=11.12257 | best_loss=7.67288
Epoch 34/80: current_loss=16.18321 | best_loss=7.67288
Epoch 35/80: current_loss=18.16359 | best_loss=7.67288
Epoch 36/80: current_loss=13.15545 | best_loss=7.67288
Epoch 37/80: current_loss=8.80108 | best_loss=7.67288
Early Stopping at epoch 37
      explained_var=0.00114 | mse_loss=7.84815
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.02611| avg_loss=7.93861
----------------------------------------------


----------------------------------------------
Params for Trial 95
{'learning_rate': 0.1, 'weight_decay': 0.002242239042106237, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.69827 | best_loss=9.69827
Epoch 1/80: current_loss=36.46634 | best_loss=9.69827
Epoch 2/80: current_loss=18.48149 | best_loss=9.69827
Epoch 3/80: current_loss=12.42908 | best_loss=9.69827
Epoch 4/80: current_loss=10.49874 | best_loss=9.69827
Epoch 5/80: current_loss=8.48552 | best_loss=8.48552
Epoch 6/80: current_loss=10.56250 | best_loss=8.48552
Epoch 7/80: current_loss=8.14374 | best_loss=8.14374
Epoch 8/80: current_loss=7.94126 | best_loss=7.94126
Epoch 9/80: current_loss=9.24460 | best_loss=7.94126
Epoch 10/80: current_loss=8.82831 | best_loss=7.94126
Epoch 11/80: current_loss=8.15264 | best_loss=7.94126
Epoch 12/80: current_loss=9.49958 | best_loss=7.94126
Epoch 13/80: current_loss=9.24065 | best_loss=7.94126
Epoch 14/80: current_loss=10.23740 | best_loss=7.94126
Epoch 15/80: current_loss=19.99622 | best_loss=7.94126
Epoch 16/80: current_loss=14.95930 | best_loss=7.94126
Epoch 17/80: current_loss=8.70040 | best_loss=7.94126
Epoch 18/80: current_loss=12.30517 | best_loss=7.94126
Epoch 19/80: current_loss=8.92552 | best_loss=7.94126
Epoch 20/80: current_loss=9.29241 | best_loss=7.94126
Epoch 21/80: current_loss=8.53372 | best_loss=7.94126
Epoch 22/80: current_loss=8.75206 | best_loss=7.94126
Epoch 23/80: current_loss=10.11863 | best_loss=7.94126
Epoch 24/80: current_loss=7.93763 | best_loss=7.93763
Epoch 25/80: current_loss=10.06897 | best_loss=7.93763
Epoch 26/80: current_loss=9.04854 | best_loss=7.93763
Epoch 27/80: current_loss=8.54562 | best_loss=7.93763
Epoch 28/80: current_loss=8.13023 | best_loss=7.93763
Epoch 29/80: current_loss=9.83092 | best_loss=7.93763
Epoch 30/80: current_loss=10.32698 | best_loss=7.93763
Epoch 31/80: current_loss=14.77057 | best_loss=7.93763
Epoch 32/80: current_loss=8.05993 | best_loss=7.93763
Epoch 33/80: current_loss=12.84559 | best_loss=7.93763
Epoch 34/80: current_loss=11.56806 | best_loss=7.93763
Epoch 35/80: current_loss=8.07340 | best_loss=7.93763
Epoch 36/80: current_loss=8.02502 | best_loss=7.93763
Epoch 37/80: current_loss=8.95095 | best_loss=7.93763
Epoch 38/80: current_loss=8.48697 | best_loss=7.93763
Epoch 39/80: current_loss=8.49311 | best_loss=7.93763
Epoch 40/80: current_loss=9.57792 | best_loss=7.93763
Epoch 41/80: current_loss=11.19017 | best_loss=7.93763
Epoch 42/80: current_loss=8.68549 | best_loss=7.93763
Epoch 43/80: current_loss=9.14412 | best_loss=7.93763
Epoch 44/80: current_loss=7.76771 | best_loss=7.76771
Epoch 45/80: current_loss=13.24061 | best_loss=7.76771
Epoch 46/80: current_loss=39.90113 | best_loss=7.76771
Epoch 47/80: current_loss=20.89477 | best_loss=7.76771
Epoch 48/80: current_loss=19.30237 | best_loss=7.76771
Epoch 49/80: current_loss=12.76558 | best_loss=7.76771
Epoch 50/80: current_loss=14.35916 | best_loss=7.76771
Epoch 51/80: current_loss=16.79300 | best_loss=7.76771
Epoch 52/80: current_loss=10.29456 | best_loss=7.76771
Epoch 53/80: current_loss=18.81536 | best_loss=7.76771
Epoch 54/80: current_loss=10.54992 | best_loss=7.76771
Epoch 55/80: current_loss=10.97486 | best_loss=7.76771
Epoch 56/80: current_loss=8.77890 | best_loss=7.76771
Epoch 57/80: current_loss=9.36949 | best_loss=7.76771
Epoch 58/80: current_loss=21.13397 | best_loss=7.76771
Epoch 59/80: current_loss=12.16258 | best_loss=7.76771
Epoch 60/80: current_loss=9.32443 | best_loss=7.76771
Epoch 61/80: current_loss=8.64032 | best_loss=7.76771
Epoch 62/80: current_loss=8.40895 | best_loss=7.76771
Epoch 63/80: current_loss=8.35609 | best_loss=7.76771
Epoch 64/80: current_loss=9.23036 | best_loss=7.76771
Early Stopping at epoch 64
      explained_var=0.00322 | mse_loss=7.56635
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.03372 | best_loss=9.03372
Epoch 1/80: current_loss=10.38245 | best_loss=9.03372
Epoch 2/80: current_loss=8.94695 | best_loss=8.94695
Epoch 3/80: current_loss=9.93404 | best_loss=8.94695
Epoch 4/80: current_loss=11.03985 | best_loss=8.94695
Epoch 5/80: current_loss=9.74777 | best_loss=8.94695
Epoch 6/80: current_loss=18.42941 | best_loss=8.94695
Epoch 7/80: current_loss=8.84955 | best_loss=8.84955
Epoch 8/80: current_loss=9.41172 | best_loss=8.84955
Epoch 9/80: current_loss=8.54584 | best_loss=8.54584
Epoch 10/80: current_loss=10.66971 | best_loss=8.54584
Epoch 11/80: current_loss=12.52614 | best_loss=8.54584
Epoch 12/80: current_loss=12.31936 | best_loss=8.54584
Epoch 13/80: current_loss=10.69761 | best_loss=8.54584
Epoch 14/80: current_loss=10.70661 | best_loss=8.54584
Epoch 15/80: current_loss=11.00439 | best_loss=8.54584
Epoch 16/80: current_loss=8.58311 | best_loss=8.54584
Epoch 17/80: current_loss=14.09632 | best_loss=8.54584
Epoch 18/80: current_loss=9.79494 | best_loss=8.54584
Epoch 19/80: current_loss=9.73402 | best_loss=8.54584
Epoch 20/80: current_loss=14.84096 | best_loss=8.54584
Epoch 21/80: current_loss=10.64124 | best_loss=8.54584
Epoch 22/80: current_loss=8.74932 | best_loss=8.54584
Epoch 23/80: current_loss=9.94787 | best_loss=8.54584
Epoch 24/80: current_loss=8.63661 | best_loss=8.54584
Epoch 25/80: current_loss=11.32373 | best_loss=8.54584
Epoch 26/80: current_loss=8.42594 | best_loss=8.42594
Epoch 27/80: current_loss=18.19410 | best_loss=8.42594
Epoch 28/80: current_loss=8.39146 | best_loss=8.39146
Epoch 29/80: current_loss=8.50039 | best_loss=8.39146
Epoch 30/80: current_loss=10.11073 | best_loss=8.39146
Epoch 31/80: current_loss=8.43762 | best_loss=8.39146
Epoch 32/80: current_loss=8.85285 | best_loss=8.39146
Epoch 33/80: current_loss=11.58016 | best_loss=8.39146
Epoch 34/80: current_loss=21.96618 | best_loss=8.39146
Epoch 35/80: current_loss=8.65854 | best_loss=8.39146
Epoch 36/80: current_loss=11.25910 | best_loss=8.39146
Epoch 37/80: current_loss=10.09383 | best_loss=8.39146
Epoch 38/80: current_loss=9.07385 | best_loss=8.39146
Epoch 39/80: current_loss=10.91265 | best_loss=8.39146
Epoch 40/80: current_loss=13.02704 | best_loss=8.39146
Epoch 41/80: current_loss=8.85752 | best_loss=8.39146
Epoch 42/80: current_loss=9.16323 | best_loss=8.39146
Epoch 43/80: current_loss=9.38630 | best_loss=8.39146
Epoch 44/80: current_loss=8.37795 | best_loss=8.37795
Epoch 45/80: current_loss=8.28998 | best_loss=8.28998
Epoch 46/80: current_loss=8.99808 | best_loss=8.28998
Epoch 47/80: current_loss=8.71260 | best_loss=8.28998
Epoch 48/80: current_loss=9.08421 | best_loss=8.28998
Epoch 49/80: current_loss=13.50702 | best_loss=8.28998
Epoch 50/80: current_loss=8.61538 | best_loss=8.28998
Epoch 51/80: current_loss=8.45953 | best_loss=8.28998
Epoch 52/80: current_loss=11.74041 | best_loss=8.28998
Epoch 53/80: current_loss=8.44885 | best_loss=8.28998
Epoch 54/80: current_loss=8.44865 | best_loss=8.28998
Epoch 55/80: current_loss=11.82641 | best_loss=8.28998
Epoch 56/80: current_loss=10.20251 | best_loss=8.28998
Epoch 57/80: current_loss=12.39735 | best_loss=8.28998
Epoch 58/80: current_loss=9.95502 | best_loss=8.28998
Epoch 59/80: current_loss=10.22221 | best_loss=8.28998
Epoch 60/80: current_loss=10.84426 | best_loss=8.28998
Epoch 61/80: current_loss=11.31483 | best_loss=8.28998
Epoch 62/80: current_loss=9.21603 | best_loss=8.28998
Epoch 63/80: current_loss=9.43572 | best_loss=8.28998
Epoch 64/80: current_loss=10.99327 | best_loss=8.28998
Epoch 65/80: current_loss=10.02503 | best_loss=8.28998
Early Stopping at epoch 65
      explained_var=0.00714 | mse_loss=8.11269
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.92395 | best_loss=10.92395
Epoch 1/80: current_loss=8.97777 | best_loss=8.97777
Epoch 2/80: current_loss=10.50226 | best_loss=8.97777
Epoch 3/80: current_loss=12.61411 | best_loss=8.97777
Epoch 4/80: current_loss=9.02157 | best_loss=8.97777
Epoch 5/80: current_loss=9.34768 | best_loss=8.97777
Epoch 6/80: current_loss=10.66633 | best_loss=8.97777
Epoch 7/80: current_loss=8.85529 | best_loss=8.85529
Epoch 8/80: current_loss=20.60102 | best_loss=8.85529
Epoch 9/80: current_loss=11.64984 | best_loss=8.85529
Epoch 10/80: current_loss=8.83868 | best_loss=8.83868
Epoch 11/80: current_loss=9.61510 | best_loss=8.83868
Epoch 12/80: current_loss=8.73524 | best_loss=8.73524
Epoch 13/80: current_loss=9.03561 | best_loss=8.73524
Epoch 14/80: current_loss=14.04982 | best_loss=8.73524
Epoch 15/80: current_loss=14.43040 | best_loss=8.73524
Epoch 16/80: current_loss=9.84345 | best_loss=8.73524
Epoch 17/80: current_loss=8.68022 | best_loss=8.68022
Epoch 18/80: current_loss=8.92529 | best_loss=8.68022
Epoch 19/80: current_loss=14.94249 | best_loss=8.68022
Epoch 20/80: current_loss=14.68205 | best_loss=8.68022
Epoch 21/80: current_loss=8.60210 | best_loss=8.60210
Epoch 22/80: current_loss=16.38470 | best_loss=8.60210
Epoch 23/80: current_loss=8.88292 | best_loss=8.60210
Epoch 24/80: current_loss=10.76055 | best_loss=8.60210
Epoch 25/80: current_loss=8.98960 | best_loss=8.60210
Epoch 26/80: current_loss=14.42104 | best_loss=8.60210
Epoch 27/80: current_loss=9.34251 | best_loss=8.60210
Epoch 28/80: current_loss=8.72205 | best_loss=8.60210
Epoch 29/80: current_loss=10.36982 | best_loss=8.60210
Epoch 30/80: current_loss=12.49459 | best_loss=8.60210
Epoch 31/80: current_loss=8.66410 | best_loss=8.60210
Epoch 32/80: current_loss=8.80233 | best_loss=8.60210
Epoch 33/80: current_loss=9.19717 | best_loss=8.60210
Epoch 34/80: current_loss=17.21968 | best_loss=8.60210
Epoch 35/80: current_loss=10.44766 | best_loss=8.60210
Epoch 36/80: current_loss=8.71043 | best_loss=8.60210
Epoch 37/80: current_loss=15.03343 | best_loss=8.60210
Epoch 38/80: current_loss=8.68124 | best_loss=8.60210
Epoch 39/80: current_loss=9.54785 | best_loss=8.60210
Epoch 40/80: current_loss=9.28691 | best_loss=8.60210
Epoch 41/80: current_loss=13.11088 | best_loss=8.60210
Early Stopping at epoch 41
      explained_var=0.01199 | mse_loss=8.38365
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.03902 | best_loss=14.03902
Epoch 1/80: current_loss=22.99427 | best_loss=14.03902
Epoch 2/80: current_loss=9.79519 | best_loss=9.79519
Epoch 3/80: current_loss=8.96670 | best_loss=8.96670
Epoch 4/80: current_loss=12.16610 | best_loss=8.96670
Epoch 5/80: current_loss=9.03361 | best_loss=8.96670
Epoch 6/80: current_loss=10.03316 | best_loss=8.96670
Epoch 7/80: current_loss=9.01320 | best_loss=8.96670
Epoch 8/80: current_loss=8.73304 | best_loss=8.73304
Epoch 9/80: current_loss=11.37307 | best_loss=8.73304
Epoch 10/80: current_loss=12.22068 | best_loss=8.73304
Epoch 11/80: current_loss=10.10231 | best_loss=8.73304
Epoch 12/80: current_loss=11.55858 | best_loss=8.73304
Epoch 13/80: current_loss=9.54333 | best_loss=8.73304
Epoch 14/80: current_loss=10.64287 | best_loss=8.73304
Epoch 15/80: current_loss=9.57900 | best_loss=8.73304
Epoch 16/80: current_loss=12.22302 | best_loss=8.73304
Epoch 17/80: current_loss=10.00307 | best_loss=8.73304
Epoch 18/80: current_loss=9.60933 | best_loss=8.73304
Epoch 19/80: current_loss=11.51851 | best_loss=8.73304
Epoch 20/80: current_loss=9.97230 | best_loss=8.73304
Epoch 21/80: current_loss=8.85452 | best_loss=8.73304
Epoch 22/80: current_loss=9.78068 | best_loss=8.73304
Epoch 23/80: current_loss=10.65598 | best_loss=8.73304
Epoch 24/80: current_loss=9.30310 | best_loss=8.73304
Epoch 25/80: current_loss=8.74050 | best_loss=8.73304
Epoch 26/80: current_loss=9.94503 | best_loss=8.73304
Epoch 27/80: current_loss=8.34775 | best_loss=8.34775
Epoch 28/80: current_loss=8.51658 | best_loss=8.34775
Epoch 29/80: current_loss=10.26467 | best_loss=8.34775
Epoch 30/80: current_loss=8.86665 | best_loss=8.34775
Epoch 31/80: current_loss=9.78740 | best_loss=8.34775
Epoch 32/80: current_loss=9.20099 | best_loss=8.34775
Epoch 33/80: current_loss=9.86996 | best_loss=8.34775
Epoch 34/80: current_loss=10.50218 | best_loss=8.34775
Epoch 35/80: current_loss=11.65706 | best_loss=8.34775
Epoch 36/80: current_loss=8.47058 | best_loss=8.34775
Epoch 37/80: current_loss=9.88533 | best_loss=8.34775
Epoch 38/80: current_loss=14.34698 | best_loss=8.34775
Epoch 39/80: current_loss=9.60114 | best_loss=8.34775
Epoch 40/80: current_loss=9.18919 | best_loss=8.34775
Epoch 41/80: current_loss=9.99972 | best_loss=8.34775
Epoch 42/80: current_loss=10.06053 | best_loss=8.34775
Epoch 43/80: current_loss=9.27185 | best_loss=8.34775
Epoch 44/80: current_loss=13.75910 | best_loss=8.34775
Epoch 45/80: current_loss=9.07890 | best_loss=8.34775
Epoch 46/80: current_loss=8.49658 | best_loss=8.34775
Epoch 47/80: current_loss=12.12551 | best_loss=8.34775
Early Stopping at epoch 47
      explained_var=0.00076 | mse_loss=8.46197
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.74666 | best_loss=9.74666
Epoch 1/80: current_loss=9.06069 | best_loss=9.06069
Epoch 2/80: current_loss=9.79277 | best_loss=9.06069
Epoch 3/80: current_loss=10.78024 | best_loss=9.06069
Epoch 4/80: current_loss=7.64740 | best_loss=7.64740
Epoch 5/80: current_loss=9.55052 | best_loss=7.64740
Epoch 6/80: current_loss=12.20727 | best_loss=7.64740
Epoch 7/80: current_loss=12.84953 | best_loss=7.64740
Epoch 8/80: current_loss=12.10629 | best_loss=7.64740
Epoch 9/80: current_loss=9.04695 | best_loss=7.64740
Epoch 10/80: current_loss=8.69213 | best_loss=7.64740
Epoch 11/80: current_loss=8.96890 | best_loss=7.64740
Epoch 12/80: current_loss=8.52247 | best_loss=7.64740
Epoch 13/80: current_loss=7.65014 | best_loss=7.64740
Epoch 14/80: current_loss=9.18805 | best_loss=7.64740
Epoch 15/80: current_loss=9.75820 | best_loss=7.64740
Epoch 16/80: current_loss=7.71520 | best_loss=7.64740
Epoch 17/80: current_loss=11.54959 | best_loss=7.64740
Epoch 18/80: current_loss=13.26260 | best_loss=7.64740
Epoch 19/80: current_loss=10.79355 | best_loss=7.64740
Epoch 20/80: current_loss=8.44868 | best_loss=7.64740
Epoch 21/80: current_loss=8.33298 | best_loss=7.64740
Epoch 22/80: current_loss=12.98037 | best_loss=7.64740
Epoch 23/80: current_loss=7.81801 | best_loss=7.64740
Epoch 24/80: current_loss=8.40542 | best_loss=7.64740
Early Stopping at epoch 24
      explained_var=0.00419 | mse_loss=7.82603
----------------------------------------------
Average early_stopping_point: 28| avg_exp_var=0.00546| avg_loss=8.07014
----------------------------------------------


----------------------------------------------
Params for Trial 96
{'learning_rate': 0.1, 'weight_decay': 0.009111491895869205, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.20200 | best_loss=11.20200
Epoch 1/80: current_loss=8.55426 | best_loss=8.55426
Epoch 2/80: current_loss=8.94852 | best_loss=8.55426
Epoch 3/80: current_loss=9.04771 | best_loss=8.55426
Epoch 4/80: current_loss=8.84806 | best_loss=8.55426
Epoch 5/80: current_loss=8.67671 | best_loss=8.55426
Epoch 6/80: current_loss=8.19965 | best_loss=8.19965
Epoch 7/80: current_loss=9.77206 | best_loss=8.19965
Epoch 8/80: current_loss=9.90880 | best_loss=8.19965
Epoch 9/80: current_loss=7.73728 | best_loss=7.73728
Epoch 10/80: current_loss=8.80820 | best_loss=7.73728
Epoch 11/80: current_loss=8.61139 | best_loss=7.73728
Epoch 12/80: current_loss=8.06953 | best_loss=7.73728
Epoch 13/80: current_loss=8.23288 | best_loss=7.73728
Epoch 14/80: current_loss=9.29659 | best_loss=7.73728
Epoch 15/80: current_loss=8.06958 | best_loss=7.73728
Epoch 16/80: current_loss=7.78613 | best_loss=7.73728
Epoch 17/80: current_loss=10.73400 | best_loss=7.73728
Epoch 18/80: current_loss=8.49892 | best_loss=7.73728
Epoch 19/80: current_loss=16.84016 | best_loss=7.73728
Epoch 20/80: current_loss=12.66528 | best_loss=7.73728
Epoch 21/80: current_loss=9.82003 | best_loss=7.73728
Epoch 22/80: current_loss=7.83937 | best_loss=7.73728
Epoch 23/80: current_loss=8.99817 | best_loss=7.73728
Epoch 24/80: current_loss=13.69014 | best_loss=7.73728
Epoch 25/80: current_loss=7.97275 | best_loss=7.73728
Epoch 26/80: current_loss=8.01862 | best_loss=7.73728
Epoch 27/80: current_loss=11.82505 | best_loss=7.73728
Epoch 28/80: current_loss=9.91519 | best_loss=7.73728
Epoch 29/80: current_loss=17.13727 | best_loss=7.73728
Early Stopping at epoch 29
      explained_var=0.00524 | mse_loss=7.54733
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=31.49179 | best_loss=31.49179
Epoch 1/80: current_loss=12.06919 | best_loss=12.06919
Epoch 2/80: current_loss=10.20806 | best_loss=10.20806
Epoch 3/80: current_loss=8.87349 | best_loss=8.87349
Epoch 4/80: current_loss=8.46919 | best_loss=8.46919
Epoch 5/80: current_loss=9.02096 | best_loss=8.46919
Epoch 6/80: current_loss=8.59471 | best_loss=8.46919
Epoch 7/80: current_loss=9.79328 | best_loss=8.46919
Epoch 8/80: current_loss=9.47849 | best_loss=8.46919
Epoch 9/80: current_loss=9.84403 | best_loss=8.46919
Epoch 10/80: current_loss=10.78224 | best_loss=8.46919
Epoch 11/80: current_loss=14.54983 | best_loss=8.46919
Epoch 12/80: current_loss=10.56424 | best_loss=8.46919
Epoch 13/80: current_loss=34.68518 | best_loss=8.46919
Epoch 14/80: current_loss=9.08349 | best_loss=8.46919
Epoch 15/80: current_loss=8.39295 | best_loss=8.39295
Epoch 16/80: current_loss=15.29234 | best_loss=8.39295
Epoch 17/80: current_loss=13.99073 | best_loss=8.39295
Epoch 18/80: current_loss=10.42704 | best_loss=8.39295
Epoch 19/80: current_loss=18.31043 | best_loss=8.39295
Epoch 20/80: current_loss=8.53646 | best_loss=8.39295
Epoch 21/80: current_loss=8.41033 | best_loss=8.39295
Epoch 22/80: current_loss=8.66463 | best_loss=8.39295
Epoch 23/80: current_loss=9.91996 | best_loss=8.39295
Epoch 24/80: current_loss=11.75640 | best_loss=8.39295
Epoch 25/80: current_loss=9.33333 | best_loss=8.39295
Epoch 26/80: current_loss=8.54627 | best_loss=8.39295
Epoch 27/80: current_loss=8.84687 | best_loss=8.39295
Epoch 28/80: current_loss=11.26695 | best_loss=8.39295
Epoch 29/80: current_loss=11.58890 | best_loss=8.39295
Epoch 30/80: current_loss=10.42417 | best_loss=8.39295
Epoch 31/80: current_loss=8.27408 | best_loss=8.27408
Epoch 32/80: current_loss=9.94238 | best_loss=8.27408
Epoch 33/80: current_loss=11.46436 | best_loss=8.27408
Epoch 34/80: current_loss=20.66841 | best_loss=8.27408
Epoch 35/80: current_loss=11.07759 | best_loss=8.27408
Epoch 36/80: current_loss=8.95595 | best_loss=8.27408
Epoch 37/80: current_loss=9.02811 | best_loss=8.27408
Epoch 38/80: current_loss=8.75757 | best_loss=8.27408
Epoch 39/80: current_loss=11.37964 | best_loss=8.27408
Epoch 40/80: current_loss=8.31750 | best_loss=8.27408
Epoch 41/80: current_loss=8.64220 | best_loss=8.27408
Epoch 42/80: current_loss=8.33129 | best_loss=8.27408
Epoch 43/80: current_loss=11.62889 | best_loss=8.27408
Epoch 44/80: current_loss=15.52006 | best_loss=8.27408
Epoch 45/80: current_loss=9.62269 | best_loss=8.27408
Epoch 46/80: current_loss=10.11832 | best_loss=8.27408
Epoch 47/80: current_loss=8.80324 | best_loss=8.27408
Epoch 48/80: current_loss=10.20841 | best_loss=8.27408
Epoch 49/80: current_loss=10.51500 | best_loss=8.27408
Epoch 50/80: current_loss=10.24316 | best_loss=8.27408
Epoch 51/80: current_loss=10.84144 | best_loss=8.27408
Early Stopping at epoch 51
      explained_var=0.00099 | mse_loss=8.15613
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.87345 | best_loss=13.87345
Epoch 1/80: current_loss=8.49958 | best_loss=8.49958
Epoch 2/80: current_loss=10.85411 | best_loss=8.49958
Epoch 3/80: current_loss=12.18620 | best_loss=8.49958
Epoch 4/80: current_loss=17.38084 | best_loss=8.49958
Epoch 5/80: current_loss=11.12734 | best_loss=8.49958
Epoch 6/80: current_loss=12.14936 | best_loss=8.49958
Epoch 7/80: current_loss=9.44446 | best_loss=8.49958
Epoch 8/80: current_loss=9.04273 | best_loss=8.49958
Epoch 9/80: current_loss=12.66626 | best_loss=8.49958
Epoch 10/80: current_loss=11.41279 | best_loss=8.49958
Epoch 11/80: current_loss=10.90723 | best_loss=8.49958
Epoch 12/80: current_loss=10.56244 | best_loss=8.49958
Epoch 13/80: current_loss=9.00908 | best_loss=8.49958
Epoch 14/80: current_loss=10.82663 | best_loss=8.49958
Epoch 15/80: current_loss=11.04721 | best_loss=8.49958
Epoch 16/80: current_loss=9.92866 | best_loss=8.49958
Epoch 17/80: current_loss=12.35533 | best_loss=8.49958
Epoch 18/80: current_loss=16.78437 | best_loss=8.49958
Epoch 19/80: current_loss=9.87889 | best_loss=8.49958
Epoch 20/80: current_loss=8.71827 | best_loss=8.49958
Epoch 21/80: current_loss=11.93133 | best_loss=8.49958
Early Stopping at epoch 21
      explained_var=0.03487 | mse_loss=8.28741
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.51595 | best_loss=8.51595
Epoch 1/80: current_loss=10.10078 | best_loss=8.51595
Epoch 2/80: current_loss=10.03810 | best_loss=8.51595
Epoch 3/80: current_loss=13.57307 | best_loss=8.51595
Epoch 4/80: current_loss=13.93655 | best_loss=8.51595
Epoch 5/80: current_loss=8.63267 | best_loss=8.51595
Epoch 6/80: current_loss=10.00147 | best_loss=8.51595
Epoch 7/80: current_loss=10.98099 | best_loss=8.51595
Epoch 8/80: current_loss=16.57885 | best_loss=8.51595
Epoch 9/80: current_loss=8.91209 | best_loss=8.51595
Epoch 10/80: current_loss=8.22961 | best_loss=8.22961
Epoch 11/80: current_loss=8.91588 | best_loss=8.22961
Epoch 12/80: current_loss=10.89487 | best_loss=8.22961
Epoch 13/80: current_loss=7.96643 | best_loss=7.96643
Epoch 14/80: current_loss=9.69926 | best_loss=7.96643
Epoch 15/80: current_loss=9.70592 | best_loss=7.96643
Epoch 16/80: current_loss=9.05138 | best_loss=7.96643
Epoch 17/80: current_loss=8.09165 | best_loss=7.96643
Epoch 18/80: current_loss=8.29725 | best_loss=7.96643
Epoch 19/80: current_loss=8.58133 | best_loss=7.96643
Epoch 20/80: current_loss=12.67808 | best_loss=7.96643
Epoch 21/80: current_loss=8.42506 | best_loss=7.96643
Epoch 22/80: current_loss=8.33995 | best_loss=7.96643
Epoch 23/80: current_loss=8.24256 | best_loss=7.96643
Epoch 24/80: current_loss=16.68352 | best_loss=7.96643
Epoch 25/80: current_loss=8.80171 | best_loss=7.96643
Epoch 26/80: current_loss=16.57782 | best_loss=7.96643
Epoch 27/80: current_loss=14.01100 | best_loss=7.96643
Epoch 28/80: current_loss=8.48093 | best_loss=7.96643
Epoch 29/80: current_loss=8.27871 | best_loss=7.96643
Epoch 30/80: current_loss=13.75747 | best_loss=7.96643
Epoch 31/80: current_loss=17.86103 | best_loss=7.96643
Epoch 32/80: current_loss=11.10478 | best_loss=7.96643
Epoch 33/80: current_loss=8.32320 | best_loss=7.96643
Early Stopping at epoch 33
      explained_var=0.03428 | mse_loss=8.05359
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=13.36260 | best_loss=13.36260
Epoch 1/80: current_loss=11.39776 | best_loss=11.39776
Epoch 2/80: current_loss=8.51270 | best_loss=8.51270
Epoch 3/80: current_loss=9.98251 | best_loss=8.51270
Epoch 4/80: current_loss=8.63464 | best_loss=8.51270
Epoch 5/80: current_loss=8.06246 | best_loss=8.06246
Epoch 6/80: current_loss=9.34461 | best_loss=8.06246
Epoch 7/80: current_loss=8.20768 | best_loss=8.06246
Epoch 8/80: current_loss=7.95004 | best_loss=7.95004
Epoch 9/80: current_loss=8.50941 | best_loss=7.95004
Epoch 10/80: current_loss=9.09900 | best_loss=7.95004
Epoch 11/80: current_loss=7.79036 | best_loss=7.79036
Epoch 12/80: current_loss=8.71327 | best_loss=7.79036
Epoch 13/80: current_loss=7.95460 | best_loss=7.79036
Epoch 14/80: current_loss=11.45604 | best_loss=7.79036
Epoch 15/80: current_loss=13.03336 | best_loss=7.79036
Epoch 16/80: current_loss=8.90001 | best_loss=7.79036
Epoch 17/80: current_loss=10.78001 | best_loss=7.79036
Epoch 18/80: current_loss=7.71669 | best_loss=7.71669
Epoch 19/80: current_loss=8.32767 | best_loss=7.71669
Epoch 20/80: current_loss=8.85458 | best_loss=7.71669
Epoch 21/80: current_loss=12.59809 | best_loss=7.71669
Epoch 22/80: current_loss=8.14731 | best_loss=7.71669
Epoch 23/80: current_loss=10.03583 | best_loss=7.71669
Epoch 24/80: current_loss=8.37002 | best_loss=7.71669
Epoch 25/80: current_loss=8.31265 | best_loss=7.71669
Epoch 26/80: current_loss=9.48916 | best_loss=7.71669
Epoch 27/80: current_loss=8.95094 | best_loss=7.71669
Epoch 28/80: current_loss=8.83408 | best_loss=7.71669
Epoch 29/80: current_loss=12.02395 | best_loss=7.71669
Epoch 30/80: current_loss=16.45177 | best_loss=7.71669
Epoch 31/80: current_loss=8.51073 | best_loss=7.71669
Epoch 32/80: current_loss=8.04470 | best_loss=7.71669
Epoch 33/80: current_loss=9.43983 | best_loss=7.71669
Epoch 34/80: current_loss=13.08000 | best_loss=7.71669
Epoch 35/80: current_loss=7.67997 | best_loss=7.67997
Epoch 36/80: current_loss=11.84114 | best_loss=7.67997
Epoch 37/80: current_loss=9.00186 | best_loss=7.67997
Epoch 38/80: current_loss=8.41093 | best_loss=7.67997
Epoch 39/80: current_loss=8.93067 | best_loss=7.67997
Epoch 40/80: current_loss=7.90134 | best_loss=7.67997
Epoch 41/80: current_loss=9.78228 | best_loss=7.67997
Epoch 42/80: current_loss=11.44758 | best_loss=7.67997
Epoch 43/80: current_loss=12.51056 | best_loss=7.67997
Epoch 44/80: current_loss=8.08663 | best_loss=7.67997
Epoch 45/80: current_loss=14.00003 | best_loss=7.67997
Epoch 46/80: current_loss=8.86890 | best_loss=7.67997
Epoch 47/80: current_loss=11.15665 | best_loss=7.67997
Epoch 48/80: current_loss=7.76847 | best_loss=7.67997
Epoch 49/80: current_loss=9.97677 | best_loss=7.67997
Epoch 50/80: current_loss=9.79310 | best_loss=7.67997
Epoch 51/80: current_loss=9.31154 | best_loss=7.67997
Epoch 52/80: current_loss=8.64342 | best_loss=7.67997
Epoch 53/80: current_loss=9.80581 | best_loss=7.67997
Epoch 54/80: current_loss=8.09509 | best_loss=7.67997
Epoch 55/80: current_loss=11.25027 | best_loss=7.67997
Early Stopping at epoch 55
      explained_var=0.01352 | mse_loss=7.85799
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.01778| avg_loss=7.98049
----------------------------------------------


----------------------------------------------
Params for Trial 97
{'learning_rate': 0.01, 'weight_decay': 0.0087331784886066, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.76809 | best_loss=9.76809
Epoch 1/80: current_loss=9.06092 | best_loss=9.06092
Epoch 2/80: current_loss=8.27411 | best_loss=8.27411
Epoch 3/80: current_loss=7.75827 | best_loss=7.75827
Epoch 4/80: current_loss=8.05616 | best_loss=7.75827
Epoch 5/80: current_loss=8.28834 | best_loss=7.75827
Epoch 6/80: current_loss=7.88982 | best_loss=7.75827
Epoch 7/80: current_loss=8.02903 | best_loss=7.75827
Epoch 8/80: current_loss=7.88627 | best_loss=7.75827
Epoch 9/80: current_loss=8.45735 | best_loss=7.75827
Epoch 10/80: current_loss=9.31394 | best_loss=7.75827
Epoch 11/80: current_loss=9.12792 | best_loss=7.75827
Epoch 12/80: current_loss=53.11283 | best_loss=7.75827
Epoch 13/80: current_loss=20.47163 | best_loss=7.75827
Epoch 14/80: current_loss=11.28578 | best_loss=7.75827
Epoch 15/80: current_loss=8.26908 | best_loss=7.75827
Epoch 16/80: current_loss=8.95224 | best_loss=7.75827
Epoch 17/80: current_loss=8.24160 | best_loss=7.75827
Epoch 18/80: current_loss=8.60022 | best_loss=7.75827
Epoch 19/80: current_loss=7.90055 | best_loss=7.75827
Epoch 20/80: current_loss=8.57195 | best_loss=7.75827
Epoch 21/80: current_loss=8.83425 | best_loss=7.75827
Epoch 22/80: current_loss=8.17236 | best_loss=7.75827
Epoch 23/80: current_loss=10.31614 | best_loss=7.75827
Early Stopping at epoch 23
      explained_var=0.00240 | mse_loss=7.57403
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=17.16394 | best_loss=17.16394
Epoch 1/80: current_loss=10.85497 | best_loss=10.85497
Epoch 2/80: current_loss=8.06946 | best_loss=8.06946
Epoch 3/80: current_loss=11.60193 | best_loss=8.06946
Epoch 4/80: current_loss=9.88737 | best_loss=8.06946
Epoch 5/80: current_loss=10.47724 | best_loss=8.06946
Epoch 6/80: current_loss=8.94353 | best_loss=8.06946
Epoch 7/80: current_loss=8.83009 | best_loss=8.06946
Epoch 8/80: current_loss=9.44916 | best_loss=8.06946
Epoch 9/80: current_loss=9.04174 | best_loss=8.06946
Epoch 10/80: current_loss=8.59176 | best_loss=8.06946
Epoch 11/80: current_loss=8.54908 | best_loss=8.06946
Epoch 12/80: current_loss=8.86425 | best_loss=8.06946
Epoch 13/80: current_loss=8.87189 | best_loss=8.06946
Epoch 14/80: current_loss=9.93964 | best_loss=8.06946
Epoch 15/80: current_loss=9.34648 | best_loss=8.06946
Epoch 16/80: current_loss=8.58978 | best_loss=8.06946
Epoch 17/80: current_loss=9.66687 | best_loss=8.06946
Epoch 18/80: current_loss=9.17893 | best_loss=8.06946
Epoch 19/80: current_loss=8.81092 | best_loss=8.06946
Epoch 20/80: current_loss=8.67857 | best_loss=8.06946
Epoch 21/80: current_loss=9.20680 | best_loss=8.06946
Epoch 22/80: current_loss=8.83784 | best_loss=8.06946
Early Stopping at epoch 22
      explained_var=0.02749 | mse_loss=7.93909
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=10.22539 | best_loss=10.22539
Epoch 1/80: current_loss=9.70115 | best_loss=9.70115
Epoch 2/80: current_loss=8.93077 | best_loss=8.93077
Epoch 3/80: current_loss=10.06340 | best_loss=8.93077
Epoch 4/80: current_loss=11.05380 | best_loss=8.93077
Epoch 5/80: current_loss=10.13210 | best_loss=8.93077
Epoch 6/80: current_loss=9.89232 | best_loss=8.93077
Epoch 7/80: current_loss=9.42155 | best_loss=8.93077
Epoch 8/80: current_loss=9.79926 | best_loss=8.93077
Epoch 9/80: current_loss=9.14139 | best_loss=8.93077
Epoch 10/80: current_loss=11.21118 | best_loss=8.93077
Epoch 11/80: current_loss=9.19065 | best_loss=8.93077
Epoch 12/80: current_loss=8.82509 | best_loss=8.82509
Epoch 13/80: current_loss=9.51392 | best_loss=8.82509
Epoch 14/80: current_loss=11.36349 | best_loss=8.82509
Epoch 15/80: current_loss=10.92419 | best_loss=8.82509
Epoch 16/80: current_loss=8.40821 | best_loss=8.40821
Epoch 17/80: current_loss=8.97041 | best_loss=8.40821
Epoch 18/80: current_loss=9.62917 | best_loss=8.40821
Epoch 19/80: current_loss=8.90491 | best_loss=8.40821
Epoch 20/80: current_loss=8.40298 | best_loss=8.40298
Epoch 21/80: current_loss=9.31716 | best_loss=8.40298
Epoch 22/80: current_loss=9.06753 | best_loss=8.40298
Epoch 23/80: current_loss=9.81982 | best_loss=8.40298
Epoch 24/80: current_loss=9.47006 | best_loss=8.40298
Epoch 25/80: current_loss=8.68301 | best_loss=8.40298
Epoch 26/80: current_loss=9.04459 | best_loss=8.40298
Epoch 27/80: current_loss=9.11668 | best_loss=8.40298
Epoch 28/80: current_loss=12.72586 | best_loss=8.40298
Epoch 29/80: current_loss=13.57830 | best_loss=8.40298
Epoch 30/80: current_loss=16.58247 | best_loss=8.40298
Epoch 31/80: current_loss=9.50999 | best_loss=8.40298
Epoch 32/80: current_loss=73.49409 | best_loss=8.40298
Epoch 33/80: current_loss=67.50516 | best_loss=8.40298
Epoch 34/80: current_loss=61.64478 | best_loss=8.40298
Epoch 35/80: current_loss=59.91284 | best_loss=8.40298
Epoch 36/80: current_loss=50.60709 | best_loss=8.40298
Epoch 37/80: current_loss=48.00365 | best_loss=8.40298
Epoch 38/80: current_loss=47.87654 | best_loss=8.40298
Epoch 39/80: current_loss=43.20433 | best_loss=8.40298
Epoch 40/80: current_loss=40.76210 | best_loss=8.40298
Early Stopping at epoch 40
      explained_var=0.03602 | mse_loss=8.19141
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=26.29503 | best_loss=26.29503
Epoch 1/80: current_loss=48.40539 | best_loss=26.29503
Epoch 2/80: current_loss=26.23506 | best_loss=26.23506
Epoch 3/80: current_loss=8.69760 | best_loss=8.69760
Epoch 4/80: current_loss=13.04393 | best_loss=8.69760
Epoch 5/80: current_loss=10.23141 | best_loss=8.69760
Epoch 6/80: current_loss=23.01344 | best_loss=8.69760
Epoch 7/80: current_loss=17.08109 | best_loss=8.69760
Epoch 8/80: current_loss=16.16364 | best_loss=8.69760
Epoch 9/80: current_loss=12.26786 | best_loss=8.69760
Epoch 10/80: current_loss=9.24556 | best_loss=8.69760
Epoch 11/80: current_loss=8.47029 | best_loss=8.47029
Epoch 12/80: current_loss=12.75320 | best_loss=8.47029
Epoch 13/80: current_loss=9.22035 | best_loss=8.47029
Epoch 14/80: current_loss=9.54019 | best_loss=8.47029
Epoch 15/80: current_loss=8.69110 | best_loss=8.47029
Epoch 16/80: current_loss=8.38249 | best_loss=8.38249
Epoch 17/80: current_loss=8.80015 | best_loss=8.38249
Epoch 18/80: current_loss=8.54423 | best_loss=8.38249
Epoch 19/80: current_loss=8.34433 | best_loss=8.34433
Epoch 20/80: current_loss=8.21666 | best_loss=8.21666
Epoch 21/80: current_loss=8.22798 | best_loss=8.21666
Epoch 22/80: current_loss=8.23445 | best_loss=8.21666
Epoch 23/80: current_loss=9.44758 | best_loss=8.21666
Epoch 24/80: current_loss=12.94498 | best_loss=8.21666
Epoch 25/80: current_loss=24.71914 | best_loss=8.21666
Epoch 26/80: current_loss=24.37552 | best_loss=8.21666
Epoch 27/80: current_loss=22.88822 | best_loss=8.21666
Epoch 28/80: current_loss=20.48001 | best_loss=8.21666
Epoch 29/80: current_loss=19.61225 | best_loss=8.21666
Epoch 30/80: current_loss=21.47697 | best_loss=8.21666
Epoch 31/80: current_loss=17.32497 | best_loss=8.21666
Epoch 32/80: current_loss=18.18643 | best_loss=8.21666
Epoch 33/80: current_loss=16.85782 | best_loss=8.21666
Epoch 34/80: current_loss=18.33272 | best_loss=8.21666
Epoch 35/80: current_loss=15.47567 | best_loss=8.21666
Epoch 36/80: current_loss=15.61167 | best_loss=8.21666
Epoch 37/80: current_loss=13.33442 | best_loss=8.21666
Epoch 38/80: current_loss=12.41496 | best_loss=8.21666
Epoch 39/80: current_loss=11.29339 | best_loss=8.21666
Epoch 40/80: current_loss=13.38847 | best_loss=8.21666
Early Stopping at epoch 40
      explained_var=0.00613 | mse_loss=8.32506
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=12.59889 | best_loss=12.59889
Epoch 1/80: current_loss=11.29048 | best_loss=11.29048
Epoch 2/80: current_loss=10.80112 | best_loss=10.80112
Epoch 3/80: current_loss=13.89537 | best_loss=10.80112
Epoch 4/80: current_loss=8.52670 | best_loss=8.52670
Epoch 5/80: current_loss=9.34231 | best_loss=8.52670
Epoch 6/80: current_loss=8.89201 | best_loss=8.52670
Epoch 7/80: current_loss=8.49853 | best_loss=8.49853
Epoch 8/80: current_loss=7.74735 | best_loss=7.74735
Epoch 9/80: current_loss=8.80826 | best_loss=7.74735
Epoch 10/80: current_loss=7.77429 | best_loss=7.74735
Epoch 11/80: current_loss=9.32520 | best_loss=7.74735
Epoch 12/80: current_loss=8.10152 | best_loss=7.74735
Epoch 13/80: current_loss=8.95069 | best_loss=7.74735
Epoch 14/80: current_loss=8.36510 | best_loss=7.74735
Epoch 15/80: current_loss=7.68741 | best_loss=7.68741
Epoch 16/80: current_loss=8.04018 | best_loss=7.68741
Epoch 17/80: current_loss=9.79702 | best_loss=7.68741
Epoch 18/80: current_loss=7.72286 | best_loss=7.68741
Epoch 19/80: current_loss=8.03506 | best_loss=7.68741
Epoch 20/80: current_loss=7.75135 | best_loss=7.68741
Epoch 21/80: current_loss=8.26191 | best_loss=7.68741
Epoch 22/80: current_loss=8.04523 | best_loss=7.68741
Epoch 23/80: current_loss=8.54340 | best_loss=7.68741
Epoch 24/80: current_loss=9.19101 | best_loss=7.68741
Epoch 25/80: current_loss=7.73823 | best_loss=7.68741
Epoch 26/80: current_loss=9.29351 | best_loss=7.68741
Epoch 27/80: current_loss=8.07375 | best_loss=7.68741
Epoch 28/80: current_loss=8.65326 | best_loss=7.68741
Epoch 29/80: current_loss=9.76609 | best_loss=7.68741
Epoch 30/80: current_loss=9.04457 | best_loss=7.68741
Epoch 31/80: current_loss=8.16541 | best_loss=7.68741
Epoch 32/80: current_loss=9.42722 | best_loss=7.68741
Epoch 33/80: current_loss=8.62535 | best_loss=7.68741
Epoch 34/80: current_loss=8.13177 | best_loss=7.68741
Epoch 35/80: current_loss=7.76209 | best_loss=7.68741
Early Stopping at epoch 35
      explained_var=-0.00179 | mse_loss=7.86973
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=0.01405| avg_loss=7.97986
----------------------------------------------


----------------------------------------------
Params for Trial 98
{'learning_rate': 0.1, 'weight_decay': 0.003280800347435423, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.84633 | best_loss=11.84633
Epoch 1/80: current_loss=8.06160 | best_loss=8.06160
Epoch 2/80: current_loss=10.11078 | best_loss=8.06160
Epoch 3/80: current_loss=10.17541 | best_loss=8.06160
Epoch 4/80: current_loss=8.93617 | best_loss=8.06160
Epoch 5/80: current_loss=7.86002 | best_loss=7.86002
Epoch 6/80: current_loss=9.30515 | best_loss=7.86002
Epoch 7/80: current_loss=26.68849 | best_loss=7.86002
Epoch 8/80: current_loss=9.22714 | best_loss=7.86002
Epoch 9/80: current_loss=9.58002 | best_loss=7.86002
Epoch 10/80: current_loss=9.24929 | best_loss=7.86002
Epoch 11/80: current_loss=9.86770 | best_loss=7.86002
Epoch 12/80: current_loss=8.98986 | best_loss=7.86002
Epoch 13/80: current_loss=8.19329 | best_loss=7.86002
Epoch 14/80: current_loss=8.32553 | best_loss=7.86002
Epoch 15/80: current_loss=7.86942 | best_loss=7.86002
Epoch 16/80: current_loss=8.34841 | best_loss=7.86002
Epoch 17/80: current_loss=8.62466 | best_loss=7.86002
Epoch 18/80: current_loss=8.06288 | best_loss=7.86002
Epoch 19/80: current_loss=8.94059 | best_loss=7.86002
Epoch 20/80: current_loss=8.71468 | best_loss=7.86002
Epoch 21/80: current_loss=8.95281 | best_loss=7.86002
Epoch 22/80: current_loss=8.58776 | best_loss=7.86002
Epoch 23/80: current_loss=8.33389 | best_loss=7.86002
Epoch 24/80: current_loss=7.70348 | best_loss=7.70348
Epoch 25/80: current_loss=8.97908 | best_loss=7.70348
Epoch 26/80: current_loss=8.13580 | best_loss=7.70348
Epoch 27/80: current_loss=7.83898 | best_loss=7.70348
Epoch 28/80: current_loss=8.34996 | best_loss=7.70348
Epoch 29/80: current_loss=8.16487 | best_loss=7.70348
Epoch 30/80: current_loss=7.93389 | best_loss=7.70348
Epoch 31/80: current_loss=8.81338 | best_loss=7.70348
Epoch 32/80: current_loss=15.73799 | best_loss=7.70348
Epoch 33/80: current_loss=12.61420 | best_loss=7.70348
Epoch 34/80: current_loss=8.47800 | best_loss=7.70348
Epoch 35/80: current_loss=7.94153 | best_loss=7.70348
Epoch 36/80: current_loss=8.69357 | best_loss=7.70348
Epoch 37/80: current_loss=8.23831 | best_loss=7.70348
Epoch 38/80: current_loss=8.24488 | best_loss=7.70348
Epoch 39/80: current_loss=7.97468 | best_loss=7.70348
Epoch 40/80: current_loss=8.00849 | best_loss=7.70348
Epoch 41/80: current_loss=10.74004 | best_loss=7.70348
Epoch 42/80: current_loss=9.88190 | best_loss=7.70348
Epoch 43/80: current_loss=8.66386 | best_loss=7.70348
Epoch 44/80: current_loss=9.88059 | best_loss=7.70348
Early Stopping at epoch 44
      explained_var=0.02052 | mse_loss=7.58242
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.91791 | best_loss=8.91791
Epoch 1/80: current_loss=11.31001 | best_loss=8.91791
Epoch 2/80: current_loss=9.44801 | best_loss=8.91791
Epoch 3/80: current_loss=9.60878 | best_loss=8.91791
Epoch 4/80: current_loss=9.97974 | best_loss=8.91791
Epoch 5/80: current_loss=8.93253 | best_loss=8.91791
Epoch 6/80: current_loss=9.31209 | best_loss=8.91791
Epoch 7/80: current_loss=8.65144 | best_loss=8.65144
Epoch 8/80: current_loss=9.05528 | best_loss=8.65144
Epoch 9/80: current_loss=9.15937 | best_loss=8.65144
Epoch 10/80: current_loss=9.09046 | best_loss=8.65144
Epoch 11/80: current_loss=9.41547 | best_loss=8.65144
Epoch 12/80: current_loss=9.32854 | best_loss=8.65144
Epoch 13/80: current_loss=11.07008 | best_loss=8.65144
Epoch 14/80: current_loss=8.34873 | best_loss=8.34873
Epoch 15/80: current_loss=8.60899 | best_loss=8.34873
Epoch 16/80: current_loss=8.67690 | best_loss=8.34873
Epoch 17/80: current_loss=9.34268 | best_loss=8.34873
Epoch 18/80: current_loss=8.95671 | best_loss=8.34873
Epoch 19/80: current_loss=11.47104 | best_loss=8.34873
Epoch 20/80: current_loss=8.84141 | best_loss=8.34873
Epoch 21/80: current_loss=8.88597 | best_loss=8.34873
Epoch 22/80: current_loss=8.46109 | best_loss=8.34873
Epoch 23/80: current_loss=8.40807 | best_loss=8.34873
Epoch 24/80: current_loss=8.77732 | best_loss=8.34873
Epoch 25/80: current_loss=9.22148 | best_loss=8.34873
Epoch 26/80: current_loss=9.29552 | best_loss=8.34873
Epoch 27/80: current_loss=8.56304 | best_loss=8.34873
Epoch 28/80: current_loss=10.21291 | best_loss=8.34873
Epoch 29/80: current_loss=8.16806 | best_loss=8.16806
Epoch 30/80: current_loss=10.99528 | best_loss=8.16806
Epoch 31/80: current_loss=11.39673 | best_loss=8.16806
Epoch 32/80: current_loss=8.35065 | best_loss=8.16806
Epoch 33/80: current_loss=9.17883 | best_loss=8.16806
Epoch 34/80: current_loss=9.22036 | best_loss=8.16806
Epoch 35/80: current_loss=9.28841 | best_loss=8.16806
Epoch 36/80: current_loss=11.68957 | best_loss=8.16806
Epoch 37/80: current_loss=8.49257 | best_loss=8.16806
Epoch 38/80: current_loss=8.33195 | best_loss=8.16806
Epoch 39/80: current_loss=8.54312 | best_loss=8.16806
Epoch 40/80: current_loss=8.34914 | best_loss=8.16806
Epoch 41/80: current_loss=10.80854 | best_loss=8.16806
Epoch 42/80: current_loss=8.90764 | best_loss=8.16806
Epoch 43/80: current_loss=8.29566 | best_loss=8.16806
Epoch 44/80: current_loss=8.34937 | best_loss=8.16806
Epoch 45/80: current_loss=8.71612 | best_loss=8.16806
Epoch 46/80: current_loss=8.76279 | best_loss=8.16806
Epoch 47/80: current_loss=8.50375 | best_loss=8.16806
Epoch 48/80: current_loss=10.19567 | best_loss=8.16806
Epoch 49/80: current_loss=8.35518 | best_loss=8.16806
Early Stopping at epoch 49
      explained_var=0.02367 | mse_loss=8.07661
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.03158 | best_loss=9.03158
Epoch 1/80: current_loss=9.41949 | best_loss=9.03158
Epoch 2/80: current_loss=11.30847 | best_loss=9.03158
Epoch 3/80: current_loss=8.73435 | best_loss=8.73435
Epoch 4/80: current_loss=11.28477 | best_loss=8.73435
Epoch 5/80: current_loss=10.60137 | best_loss=8.73435
Epoch 6/80: current_loss=9.53054 | best_loss=8.73435
Epoch 7/80: current_loss=9.19965 | best_loss=8.73435
Epoch 8/80: current_loss=8.90852 | best_loss=8.73435
Epoch 9/80: current_loss=9.50606 | best_loss=8.73435
Epoch 10/80: current_loss=10.83704 | best_loss=8.73435
Epoch 11/80: current_loss=8.75167 | best_loss=8.73435
Epoch 12/80: current_loss=9.08432 | best_loss=8.73435
Epoch 13/80: current_loss=8.70412 | best_loss=8.70412
Epoch 14/80: current_loss=8.73107 | best_loss=8.70412
Epoch 15/80: current_loss=9.80911 | best_loss=8.70412
Epoch 16/80: current_loss=8.72474 | best_loss=8.70412
Epoch 17/80: current_loss=8.78699 | best_loss=8.70412
Epoch 18/80: current_loss=8.76608 | best_loss=8.70412
Epoch 19/80: current_loss=9.80730 | best_loss=8.70412
Epoch 20/80: current_loss=8.73631 | best_loss=8.70412
Epoch 21/80: current_loss=8.79347 | best_loss=8.70412
Epoch 22/80: current_loss=9.13491 | best_loss=8.70412
Epoch 23/80: current_loss=8.75027 | best_loss=8.70412
Epoch 24/80: current_loss=8.90140 | best_loss=8.70412
Epoch 25/80: current_loss=8.73381 | best_loss=8.70412
Epoch 26/80: current_loss=10.21697 | best_loss=8.70412
Epoch 27/80: current_loss=17.91032 | best_loss=8.70412
Epoch 28/80: current_loss=8.94525 | best_loss=8.70412
Epoch 29/80: current_loss=9.20598 | best_loss=8.70412
Epoch 30/80: current_loss=14.59580 | best_loss=8.70412
Epoch 31/80: current_loss=9.58943 | best_loss=8.70412
Epoch 32/80: current_loss=12.90407 | best_loss=8.70412
Epoch 33/80: current_loss=10.25751 | best_loss=8.70412
Early Stopping at epoch 33
      explained_var=0.00171 | mse_loss=8.47192
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.31437 | best_loss=8.31437
Epoch 1/80: current_loss=9.00111 | best_loss=8.31437
Epoch 2/80: current_loss=11.94084 | best_loss=8.31437
Epoch 3/80: current_loss=9.24213 | best_loss=8.31437
Epoch 4/80: current_loss=8.93132 | best_loss=8.31437
Epoch 5/80: current_loss=9.30529 | best_loss=8.31437
Epoch 6/80: current_loss=8.47395 | best_loss=8.31437
Epoch 7/80: current_loss=9.26442 | best_loss=8.31437
Epoch 8/80: current_loss=8.73742 | best_loss=8.31437
Epoch 9/80: current_loss=9.79282 | best_loss=8.31437
Epoch 10/80: current_loss=9.06074 | best_loss=8.31437
Epoch 11/80: current_loss=8.88222 | best_loss=8.31437
Epoch 12/80: current_loss=8.93462 | best_loss=8.31437
Epoch 13/80: current_loss=10.02355 | best_loss=8.31437
Epoch 14/80: current_loss=9.68212 | best_loss=8.31437
Epoch 15/80: current_loss=8.11913 | best_loss=8.11913
Epoch 16/80: current_loss=8.92781 | best_loss=8.11913
Epoch 17/80: current_loss=10.35985 | best_loss=8.11913
Epoch 18/80: current_loss=8.25647 | best_loss=8.11913
Epoch 19/80: current_loss=8.65504 | best_loss=8.11913
Epoch 20/80: current_loss=8.57777 | best_loss=8.11913
Epoch 21/80: current_loss=8.64709 | best_loss=8.11913
Epoch 22/80: current_loss=8.50246 | best_loss=8.11913
Epoch 23/80: current_loss=9.29678 | best_loss=8.11913
Epoch 24/80: current_loss=8.70662 | best_loss=8.11913
Epoch 25/80: current_loss=8.81650 | best_loss=8.11913
Epoch 26/80: current_loss=8.80053 | best_loss=8.11913
Epoch 27/80: current_loss=9.72203 | best_loss=8.11913
Epoch 28/80: current_loss=8.76619 | best_loss=8.11913
Epoch 29/80: current_loss=7.95723 | best_loss=7.95723
Epoch 30/80: current_loss=10.68318 | best_loss=7.95723
Epoch 31/80: current_loss=11.26649 | best_loss=7.95723
Epoch 32/80: current_loss=8.62819 | best_loss=7.95723
Epoch 33/80: current_loss=9.47478 | best_loss=7.95723
Epoch 34/80: current_loss=8.16943 | best_loss=7.95723
Epoch 35/80: current_loss=10.95687 | best_loss=7.95723
Epoch 36/80: current_loss=10.80489 | best_loss=7.95723
Epoch 37/80: current_loss=12.47745 | best_loss=7.95723
Epoch 38/80: current_loss=9.49020 | best_loss=7.95723
Epoch 39/80: current_loss=10.36344 | best_loss=7.95723
Epoch 40/80: current_loss=9.44530 | best_loss=7.95723
Epoch 41/80: current_loss=8.73314 | best_loss=7.95723
Epoch 42/80: current_loss=10.81923 | best_loss=7.95723
Epoch 43/80: current_loss=9.33368 | best_loss=7.95723
Epoch 44/80: current_loss=8.35594 | best_loss=7.95723
Epoch 45/80: current_loss=9.80586 | best_loss=7.95723
Epoch 46/80: current_loss=11.08373 | best_loss=7.95723
Epoch 47/80: current_loss=8.38283 | best_loss=7.95723
Epoch 48/80: current_loss=12.46043 | best_loss=7.95723
Epoch 49/80: current_loss=12.36186 | best_loss=7.95723
Early Stopping at epoch 49
      explained_var=0.03791 | mse_loss=8.04309
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.01106 | best_loss=8.01106
Epoch 1/80: current_loss=10.01294 | best_loss=8.01106
Epoch 2/80: current_loss=9.20100 | best_loss=8.01106
Epoch 3/80: current_loss=8.22258 | best_loss=8.01106
Epoch 4/80: current_loss=7.95688 | best_loss=7.95688
Epoch 5/80: current_loss=8.28028 | best_loss=7.95688
Epoch 6/80: current_loss=8.12289 | best_loss=7.95688
Epoch 7/80: current_loss=8.46910 | best_loss=7.95688
Epoch 8/80: current_loss=9.69424 | best_loss=7.95688
Epoch 9/80: current_loss=10.40760 | best_loss=7.95688
Epoch 10/80: current_loss=8.45925 | best_loss=7.95688
Epoch 11/80: current_loss=7.61473 | best_loss=7.61473
Epoch 12/80: current_loss=8.92445 | best_loss=7.61473
Epoch 13/80: current_loss=8.93736 | best_loss=7.61473
Epoch 14/80: current_loss=7.58948 | best_loss=7.58948
Epoch 15/80: current_loss=8.18053 | best_loss=7.58948
Epoch 16/80: current_loss=8.05212 | best_loss=7.58948
Epoch 17/80: current_loss=9.49968 | best_loss=7.58948
Epoch 18/80: current_loss=9.36431 | best_loss=7.58948
Epoch 19/80: current_loss=9.27097 | best_loss=7.58948
Epoch 20/80: current_loss=8.29131 | best_loss=7.58948
Epoch 21/80: current_loss=8.50591 | best_loss=7.58948
Epoch 22/80: current_loss=9.33509 | best_loss=7.58948
Epoch 23/80: current_loss=7.89341 | best_loss=7.58948
Epoch 24/80: current_loss=8.13897 | best_loss=7.58948
Epoch 25/80: current_loss=8.53902 | best_loss=7.58948
Epoch 26/80: current_loss=8.13645 | best_loss=7.58948
Epoch 27/80: current_loss=8.63928 | best_loss=7.58948
Epoch 28/80: current_loss=8.64973 | best_loss=7.58948
Epoch 29/80: current_loss=8.96534 | best_loss=7.58948
Epoch 30/80: current_loss=10.95676 | best_loss=7.58948
Epoch 31/80: current_loss=8.83245 | best_loss=7.58948
Epoch 32/80: current_loss=10.44206 | best_loss=7.58948
Epoch 33/80: current_loss=10.00550 | best_loss=7.58948
Epoch 34/80: current_loss=8.97945 | best_loss=7.58948
Early Stopping at epoch 34
      explained_var=0.01764 | mse_loss=7.72068
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.02029| avg_loss=7.97894
----------------------------------------------


----------------------------------------------
Params for Trial 99
{'learning_rate': 0.1, 'weight_decay': 0.0024925442055966796, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=7.94223 | best_loss=7.94223
Epoch 1/80: current_loss=8.05120 | best_loss=7.94223
Epoch 2/80: current_loss=7.85228 | best_loss=7.85228
Epoch 3/80: current_loss=11.84255 | best_loss=7.85228
Epoch 4/80: current_loss=10.18311 | best_loss=7.85228
Epoch 5/80: current_loss=8.41105 | best_loss=7.85228
Epoch 6/80: current_loss=7.86183 | best_loss=7.85228
Epoch 7/80: current_loss=7.95329 | best_loss=7.85228
Epoch 8/80: current_loss=9.23831 | best_loss=7.85228
Epoch 9/80: current_loss=11.64601 | best_loss=7.85228
Epoch 10/80: current_loss=8.35745 | best_loss=7.85228
Epoch 11/80: current_loss=13.42929 | best_loss=7.85228
Epoch 12/80: current_loss=9.92949 | best_loss=7.85228
Epoch 13/80: current_loss=10.95400 | best_loss=7.85228
Epoch 14/80: current_loss=17.39629 | best_loss=7.85228
Epoch 15/80: current_loss=31.01809 | best_loss=7.85228
Epoch 16/80: current_loss=9.71715 | best_loss=7.85228
Epoch 17/80: current_loss=7.83217 | best_loss=7.83217
Epoch 18/80: current_loss=7.82935 | best_loss=7.82935
Epoch 19/80: current_loss=11.68136 | best_loss=7.82935
Epoch 20/80: current_loss=10.83634 | best_loss=7.82935
Epoch 21/80: current_loss=7.86031 | best_loss=7.82935
Epoch 22/80: current_loss=7.80144 | best_loss=7.80144
Epoch 23/80: current_loss=7.83959 | best_loss=7.80144
Epoch 24/80: current_loss=8.19668 | best_loss=7.80144
Epoch 25/80: current_loss=8.16187 | best_loss=7.80144
Epoch 26/80: current_loss=8.89146 | best_loss=7.80144
Epoch 27/80: current_loss=7.76933 | best_loss=7.76933
Epoch 28/80: current_loss=8.01995 | best_loss=7.76933
Epoch 29/80: current_loss=7.74909 | best_loss=7.74909
Epoch 30/80: current_loss=7.75444 | best_loss=7.74909
Epoch 31/80: current_loss=9.62220 | best_loss=7.74909
Epoch 32/80: current_loss=7.90191 | best_loss=7.74909
Epoch 33/80: current_loss=9.05046 | best_loss=7.74909
Epoch 34/80: current_loss=8.03634 | best_loss=7.74909
Epoch 35/80: current_loss=9.03400 | best_loss=7.74909
Epoch 36/80: current_loss=8.38009 | best_loss=7.74909
Epoch 37/80: current_loss=13.19661 | best_loss=7.74909
Epoch 38/80: current_loss=11.18319 | best_loss=7.74909
Epoch 39/80: current_loss=7.76581 | best_loss=7.74909
Epoch 40/80: current_loss=8.46690 | best_loss=7.74909
Epoch 41/80: current_loss=9.07812 | best_loss=7.74909
Epoch 42/80: current_loss=10.86939 | best_loss=7.74909
Epoch 43/80: current_loss=9.36058 | best_loss=7.74909
Epoch 44/80: current_loss=7.97423 | best_loss=7.74909
Epoch 45/80: current_loss=12.37515 | best_loss=7.74909
Epoch 46/80: current_loss=11.96074 | best_loss=7.74909
Epoch 47/80: current_loss=7.75893 | best_loss=7.74909
Epoch 48/80: current_loss=10.87254 | best_loss=7.74909
Epoch 49/80: current_loss=8.58884 | best_loss=7.74909
Early Stopping at epoch 49
      explained_var=0.00285 | mse_loss=7.57345
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=47.47942 | best_loss=47.47942
Epoch 1/80: current_loss=17.47783 | best_loss=17.47783
Epoch 2/80: current_loss=10.32686 | best_loss=10.32686
Epoch 3/80: current_loss=10.07676 | best_loss=10.07676
Epoch 4/80: current_loss=31.57690 | best_loss=10.07676
Epoch 5/80: current_loss=21.88946 | best_loss=10.07676
Epoch 6/80: current_loss=15.18852 | best_loss=10.07676
Epoch 7/80: current_loss=11.54358 | best_loss=10.07676
Epoch 8/80: current_loss=13.70729 | best_loss=10.07676
Epoch 9/80: current_loss=12.86747 | best_loss=10.07676
Epoch 10/80: current_loss=8.56511 | best_loss=8.56511
Epoch 11/80: current_loss=8.55527 | best_loss=8.55527
Epoch 12/80: current_loss=16.66587 | best_loss=8.55527
Epoch 13/80: current_loss=8.88734 | best_loss=8.55527
Epoch 14/80: current_loss=8.27096 | best_loss=8.27096
Epoch 15/80: current_loss=8.28349 | best_loss=8.27096
Epoch 16/80: current_loss=10.03446 | best_loss=8.27096
Epoch 17/80: current_loss=23.96028 | best_loss=8.27096
Epoch 18/80: current_loss=11.55913 | best_loss=8.27096
Epoch 19/80: current_loss=9.21484 | best_loss=8.27096
Epoch 20/80: current_loss=9.11022 | best_loss=8.27096
Epoch 21/80: current_loss=11.02993 | best_loss=8.27096
Epoch 22/80: current_loss=9.99877 | best_loss=8.27096
Epoch 23/80: current_loss=11.61848 | best_loss=8.27096
Epoch 24/80: current_loss=8.39424 | best_loss=8.27096
Epoch 25/80: current_loss=8.00977 | best_loss=8.00977
Epoch 26/80: current_loss=9.67152 | best_loss=8.00977
Epoch 27/80: current_loss=8.71349 | best_loss=8.00977
Epoch 28/80: current_loss=9.19004 | best_loss=8.00977
Epoch 29/80: current_loss=8.51913 | best_loss=8.00977
Epoch 30/80: current_loss=8.35838 | best_loss=8.00977
Epoch 31/80: current_loss=8.68425 | best_loss=8.00977
Epoch 32/80: current_loss=9.42464 | best_loss=8.00977
Epoch 33/80: current_loss=8.74650 | best_loss=8.00977
Epoch 34/80: current_loss=11.04208 | best_loss=8.00977
Epoch 35/80: current_loss=8.35921 | best_loss=8.00977
Epoch 36/80: current_loss=8.57618 | best_loss=8.00977
Epoch 37/80: current_loss=14.05888 | best_loss=8.00977
Epoch 38/80: current_loss=9.91534 | best_loss=8.00977
Epoch 39/80: current_loss=8.25329 | best_loss=8.00977
Epoch 40/80: current_loss=8.57514 | best_loss=8.00977
Epoch 41/80: current_loss=8.64805 | best_loss=8.00977
Epoch 42/80: current_loss=11.18768 | best_loss=8.00977
Epoch 43/80: current_loss=8.55419 | best_loss=8.00977
Epoch 44/80: current_loss=8.74889 | best_loss=8.00977
Epoch 45/80: current_loss=10.21438 | best_loss=8.00977
Early Stopping at epoch 45
      explained_var=0.03227 | mse_loss=7.90054
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=11.83886 | best_loss=11.83886
Epoch 1/80: current_loss=12.93312 | best_loss=11.83886
Epoch 2/80: current_loss=9.29815 | best_loss=9.29815
Epoch 3/80: current_loss=9.11879 | best_loss=9.11879
Epoch 4/80: current_loss=14.30118 | best_loss=9.11879
Epoch 5/80: current_loss=8.69729 | best_loss=8.69729
Epoch 6/80: current_loss=9.85457 | best_loss=8.69729
Epoch 7/80: current_loss=11.30744 | best_loss=8.69729
Epoch 8/80: current_loss=15.52942 | best_loss=8.69729
Epoch 9/80: current_loss=14.60332 | best_loss=8.69729
Epoch 10/80: current_loss=8.75076 | best_loss=8.69729
Epoch 11/80: current_loss=8.76628 | best_loss=8.69729
Epoch 12/80: current_loss=12.63063 | best_loss=8.69729
Epoch 13/80: current_loss=13.41151 | best_loss=8.69729
Epoch 14/80: current_loss=9.15228 | best_loss=8.69729
Epoch 15/80: current_loss=10.21237 | best_loss=8.69729
Epoch 16/80: current_loss=11.47851 | best_loss=8.69729
Epoch 17/80: current_loss=8.73282 | best_loss=8.69729
Epoch 18/80: current_loss=12.31414 | best_loss=8.69729
Epoch 19/80: current_loss=9.91160 | best_loss=8.69729
Epoch 20/80: current_loss=8.71869 | best_loss=8.69729
Epoch 21/80: current_loss=8.91434 | best_loss=8.69729
Epoch 22/80: current_loss=9.36000 | best_loss=8.69729
Epoch 23/80: current_loss=10.15905 | best_loss=8.69729
Epoch 24/80: current_loss=9.86829 | best_loss=8.69729
Epoch 25/80: current_loss=18.67815 | best_loss=8.69729
Early Stopping at epoch 25
      explained_var=0.00669 | mse_loss=8.43749
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=8.64542 | best_loss=8.64542
Epoch 1/80: current_loss=9.18411 | best_loss=8.64542
Epoch 2/80: current_loss=8.24280 | best_loss=8.24280
Epoch 3/80: current_loss=8.36266 | best_loss=8.24280
Epoch 4/80: current_loss=9.83563 | best_loss=8.24280
Epoch 5/80: current_loss=8.37579 | best_loss=8.24280
Epoch 6/80: current_loss=8.98431 | best_loss=8.24280
Epoch 7/80: current_loss=9.88755 | best_loss=8.24280
Epoch 8/80: current_loss=9.58813 | best_loss=8.24280
Epoch 9/80: current_loss=9.24390 | best_loss=8.24280
Epoch 10/80: current_loss=9.24914 | best_loss=8.24280
Epoch 11/80: current_loss=10.14563 | best_loss=8.24280
Epoch 12/80: current_loss=9.74835 | best_loss=8.24280
Epoch 13/80: current_loss=9.31830 | best_loss=8.24280
Epoch 14/80: current_loss=10.86509 | best_loss=8.24280
Epoch 15/80: current_loss=9.03300 | best_loss=8.24280
Epoch 16/80: current_loss=8.23071 | best_loss=8.23071
Epoch 17/80: current_loss=18.23304 | best_loss=8.23071
Epoch 18/80: current_loss=9.40813 | best_loss=8.23071
Epoch 19/80: current_loss=8.31716 | best_loss=8.23071
Epoch 20/80: current_loss=8.40440 | best_loss=8.23071
Epoch 21/80: current_loss=10.58098 | best_loss=8.23071
Epoch 22/80: current_loss=10.64090 | best_loss=8.23071
Epoch 23/80: current_loss=13.71509 | best_loss=8.23071
Epoch 24/80: current_loss=10.56698 | best_loss=8.23071
Epoch 25/80: current_loss=8.34871 | best_loss=8.23071
Epoch 26/80: current_loss=8.26587 | best_loss=8.23071
Epoch 27/80: current_loss=14.21170 | best_loss=8.23071
Epoch 28/80: current_loss=9.10071 | best_loss=8.23071
Epoch 29/80: current_loss=8.28584 | best_loss=8.23071
Epoch 30/80: current_loss=12.08773 | best_loss=8.23071
Epoch 31/80: current_loss=9.15176 | best_loss=8.23071
Epoch 32/80: current_loss=12.21707 | best_loss=8.23071
Epoch 33/80: current_loss=12.14041 | best_loss=8.23071
Epoch 34/80: current_loss=8.89281 | best_loss=8.23071
Epoch 35/80: current_loss=8.36934 | best_loss=8.23071
Epoch 36/80: current_loss=8.71403 | best_loss=8.23071
Early Stopping at epoch 36
      explained_var=-0.00082 | mse_loss=8.33914
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=14.49200 | best_loss=14.49200
Epoch 1/80: current_loss=11.14973 | best_loss=11.14973
Epoch 2/80: current_loss=10.86885 | best_loss=10.86885
Epoch 3/80: current_loss=8.75050 | best_loss=8.75050
Epoch 4/80: current_loss=14.52969 | best_loss=8.75050
Epoch 5/80: current_loss=9.63088 | best_loss=8.75050
Epoch 6/80: current_loss=8.34510 | best_loss=8.34510
Epoch 7/80: current_loss=8.66377 | best_loss=8.34510
Epoch 8/80: current_loss=8.38046 | best_loss=8.34510
Epoch 9/80: current_loss=7.73506 | best_loss=7.73506
Epoch 10/80: current_loss=8.89404 | best_loss=7.73506
Epoch 11/80: current_loss=9.21022 | best_loss=7.73506
Epoch 12/80: current_loss=8.33003 | best_loss=7.73506
Epoch 13/80: current_loss=7.82125 | best_loss=7.73506
Epoch 14/80: current_loss=8.19678 | best_loss=7.73506
Epoch 15/80: current_loss=8.89904 | best_loss=7.73506
Epoch 16/80: current_loss=12.93658 | best_loss=7.73506
Epoch 17/80: current_loss=10.08536 | best_loss=7.73506
Epoch 18/80: current_loss=8.50828 | best_loss=7.73506
Epoch 19/80: current_loss=10.56890 | best_loss=7.73506
Epoch 20/80: current_loss=8.16872 | best_loss=7.73506
Epoch 21/80: current_loss=8.09352 | best_loss=7.73506
Epoch 22/80: current_loss=8.84233 | best_loss=7.73506
Epoch 23/80: current_loss=17.50259 | best_loss=7.73506
Epoch 24/80: current_loss=13.82533 | best_loss=7.73506
Epoch 25/80: current_loss=12.02840 | best_loss=7.73506
Epoch 26/80: current_loss=9.21514 | best_loss=7.73506
Epoch 27/80: current_loss=7.92121 | best_loss=7.73506
Epoch 28/80: current_loss=7.82953 | best_loss=7.73506
Epoch 29/80: current_loss=8.21421 | best_loss=7.73506
Early Stopping at epoch 29
      explained_var=0.00084 | mse_loss=7.90949
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.00837| avg_loss=8.03202
----------------------------------------------

Optuna study finished, study statistics:
  Finished trials:  100
  Pruned trials:  36
  Completed trials:  64
  Best Trial:  29
  Value:  7.707018709785889
  AVG stopping:  40
  Params: 
    learning_rate: 0.1
    weight_decay: 0.0013876994804289905
    n_layers: 2
    hidden_size: 128
    dropout: 0.2
----------------------------------------------

Check best params: {'learning_rate': 0.1, 'weight_decay': 0.0013876994804289905, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2, 'avg_epochs': 40}
--------------------------------------------------------------
Test CNN results: avg_loss=17.8184, avg_expvar=-1.1431, avg_r2score=-1.1616, avg_mae=3.3736
--------------------------------------------------------------
