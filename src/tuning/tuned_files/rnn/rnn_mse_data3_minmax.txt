[I 2024-01-06 06:33:52,776] A new study created in memory with name: cnn_mseloss_data3_minmax
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:34:24,436] Trial 0 finished with value: 0.027065059633784066 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 0.027065059633784066.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:37:24,349] Trial 1 finished with value: 0.02715952467485257 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}. Best is trial 0 with value: 0.027065059633784066.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:41:12,084] Trial 2 finished with value: 0.026884004884826524 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:41:33,247] Trial 3 finished with value: 0.02695036480336126 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:42:25,983] Trial 4 finished with value: 0.02698107079877784 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:43:18,285] Trial 5 finished with value: 0.026938799615249802 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:44:49,276] Trial 6 finished with value: 0.026948208867907275 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:46:54,147] Trial 7 finished with value: 0.032065815391608514 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:47:59,156] Trial 8 finished with value: 0.02735862974044385 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:54:19,328] Trial 9 finished with value: 0.028095333659304354 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 07:03:54,200] Trial 10 finished with value: 0.026922935219063738 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0030741327184301173, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 07:14:21,918] Trial 11 finished with value: 0.027312367120546327 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.002554434662107642, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 07:23:16,593] Trial 12 finished with value: 0.026919155878322748 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0028581370873874876, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.4}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 07:29:58,919] Trial 13 finished with value: 0.02894575789760625 and parameters: {'learning_rate': 0.01, 'weight_decay': 6.607948522012599e-05, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.4}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 07:33:38,795] Trial 14 finished with value: 0.026971807086794718 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0029717934164458413, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 07:38:30,868] Trial 15 finished with value: 0.026921616430108687 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0014982220926948822, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 07:51:27,614] Trial 16 finished with value: 0.026943913253851915 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0038777568216684074, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 07:56:29,254] Trial 17 finished with value: 0.02701793792594808 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0017708999359619434, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 2 with value: 0.026884004884826524.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:00:05,658] Trial 18 finished with value: 0.026880113207065886 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0002073006749924273, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:02:43,122] Trial 19 finished with value: 0.026887654874419614 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.00020682870637427032, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:05:38,995] Trial 20 finished with value: 0.026988098669159577 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.007327552214289986, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:08:12,554] Trial 21 finished with value: 0.026920596621282572 and parameters: {'learning_rate': 0.0001, 'weight_decay': 3.6919435707952848e-06, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:10:37,384] Trial 22 finished with value: 0.02694007721994652 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0009543568619709013, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:13:43,740] Trial 23 finished with value: 0.02692838747850552 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0006264461946113155, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:16:26,780] Trial 24 finished with value: 0.02695295472256497 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0020682592795415776, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:19:12,556] Trial 25 finished with value: 0.026920081087975122 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0007703653888005247, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.5}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:22:56,218] Trial 26 finished with value: 0.02710533062091542 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.003876444198576039, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:26:43,691] Trial 27 finished with value: 0.026967435830282972 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.009772161835564095, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:26:55,423] Trial 28 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:27:20,916] Trial 29 finished with value: 0.0269645394622669 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0013876994804289905, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:29:30,402] Trial 30 finished with value: 0.027009817889713496 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0021595189685300747, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:31:05,078] Trial 31 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:41:35,910] Trial 32 finished with value: 0.0270450260771589 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.00377076851471814, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:44:25,128] Trial 33 finished with value: 0.026983730922378296 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0024567951354875174, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:45:09,675] Trial 34 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:45:34,510] Trial 35 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:46:36,175] Trial 36 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:46:43,525] Trial 37 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:51:22,016] Trial 38 finished with value: 0.02695843651391011 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0016697509857594326, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:52:03,867] Trial 39 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:52:27,932] Trial 40 finished with value: 0.02690709776067652 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004329290681731437, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:52:36,047] Trial 41 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:52:59,510] Trial 42 finished with value: 0.026929329725515316 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.006071620547761471, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:53:08,778] Trial 43 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:53:13,808] Trial 44 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:55:08,697] Trial 45 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 08:59:46,764] Trial 46 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:00:49,230] Trial 47 finished with value: 0.026905648960039504 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0025849837801565703, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:01:48,337] Trial 48 finished with value: 0.02693613956597183 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.001989665181342119, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.5}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:02:50,181] Trial 49 finished with value: 0.026930489987236954 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0025124636019387544, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 18 with value: 0.026880113207065886.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:03:34,947] Trial 50 finished with value: 0.02687407669725833 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0003201933246352698, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:03:50,460] Trial 51 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:04:55,646] Trial 52 finished with value: 0.02688564537107462 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0010148916216445982, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:05:17,872] Trial 53 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:05:34,913] Trial 54 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:06:37,958] Trial 55 finished with value: 0.026923880786381305 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0008674579414393147, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:07:09,446] Trial 56 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:08:08,240] Trial 57 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:10:21,103] Trial 58 finished with value: 0.026943072352168657 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0016787017505985827, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:11:34,428] Trial 59 finished with value: 0.026920894700647117 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0012495456925824195, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:12:33,155] Trial 60 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:12:59,723] Trial 61 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:14:22,852] Trial 62 finished with value: 0.026996196377785615 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.007174660702441004, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:15:06,824] Trial 63 finished with value: 0.0269393440557812 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.003425611673947849, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:15:32,557] Trial 64 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:15:37,657] Trial 65 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:17:42,935] Trial 66 finished with value: 0.026882085737641958 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.00020765159625861943, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:19:01,105] Trial 67 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:21:57,761] Trial 68 finished with value: 0.026904714845202023 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0013914243371702805, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:23:59,131] Trial 69 finished with value: 0.02694007852514097 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0014152635311296935, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:26:25,396] Trial 70 finished with value: 0.026917004653961525 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0010790190597535544, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:28:43,371] Trial 71 finished with value: 0.02689789185434075 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0002017206902487404, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:29:27,257] Trial 72 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:31:08,119] Trial 73 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:31:43,700] Trial 74 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:32:04,948] Trial 75 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:33:51,592] Trial 76 finished with value: 0.026911635583815402 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.000544940541241443, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:35:37,288] Trial 77 finished with value: 0.026985190506901145 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0015091036450996312, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:36:21,664] Trial 78 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:37:54,809] Trial 79 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:40:27,174] Trial 80 finished with value: 0.026926724551449798 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0007965758461422507, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:42:46,065] Trial 81 finished with value: 0.026951655475613078 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0023088650761001477, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 50 with value: 0.02687407669725833.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:44:58,592] Trial 82 finished with value: 0.026824730491197896 and parameters: {'learning_rate': 0.0001, 'weight_decay': 9.833205478141933e-06, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:47:08,573] Trial 83 finished with value: 0.02684882541917317 and parameters: {'learning_rate': 0.0001, 'weight_decay': 2.5782508116555658e-05, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:47:57,064] Trial 84 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:49:48,641] Trial 85 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:51:07,319] Trial 86 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:51:28,500] Trial 87 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:53:41,197] Trial 88 finished with value: 0.026921413492061636 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0010212773153648403, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:54:13,601] Trial 89 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 09:58:19,513] Trial 90 finished with value: 0.027035603727620556 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.008671099663771623, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:01:07,238] Trial 91 finished with value: 0.026875491778243988 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.00045567551435233146, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:03:20,450] Trial 92 finished with value: 0.02688749413915704 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0005012641075448434, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:04:12,960] Trial 93 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:06:33,324] Trial 94 finished with value: 0.02689337614363562 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0008065455942931601, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:09:04,127] Trial 95 finished with value: 0.026886028061862455 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.00046826361544812536, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:11:14,062] Trial 96 finished with value: 0.02690286473885372 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0010775193840073137, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 82 with value: 0.026824730491197896.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:11:37,838] Trial 97 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:14:44,569] Trial 98 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 10:15:48,865] Trial 99 pruned. 
GPU CUDA available, using GPU for training the models.
-----------------------------------------------
Input arguments: 
   + data_dir: /home/ra56kop/nnets_genomic_prediction/src
   + model: RNN
   + tuned: 1
   + minmax_scale: 1
   + standa_scale: 0
   + pca_fitting: 0
   + dataset: pheno_3
   + gpucuda: 2
   + data_variants: [1, 0, 0, 3]
-----------------------------------------------

---------------------------------------------------------
Tuning RNN with dataset pheno-3
---------------------------------------------------------


----------------------------------------------
Params for Trial 0
{'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.17219 | best_loss=0.17219
Epoch 1/80: current_loss=0.08464 | best_loss=0.08464
Epoch 2/80: current_loss=0.04121 | best_loss=0.04121
Epoch 3/80: current_loss=0.03559 | best_loss=0.03559
Epoch 4/80: current_loss=0.03495 | best_loss=0.03495
Epoch 5/80: current_loss=0.03472 | best_loss=0.03472
Epoch 6/80: current_loss=0.03408 | best_loss=0.03408
Epoch 7/80: current_loss=0.03366 | best_loss=0.03366
Epoch 8/80: current_loss=0.03350 | best_loss=0.03350
Epoch 9/80: current_loss=0.03316 | best_loss=0.03316
Epoch 10/80: current_loss=0.03286 | best_loss=0.03286
Epoch 11/80: current_loss=0.03252 | best_loss=0.03252
Epoch 12/80: current_loss=0.03239 | best_loss=0.03239
Epoch 13/80: current_loss=0.03204 | best_loss=0.03204
Epoch 14/80: current_loss=0.03197 | best_loss=0.03197
Epoch 15/80: current_loss=0.03160 | best_loss=0.03160
Epoch 16/80: current_loss=0.03143 | best_loss=0.03143
Epoch 17/80: current_loss=0.03125 | best_loss=0.03125
Epoch 18/80: current_loss=0.03102 | best_loss=0.03102
Epoch 19/80: current_loss=0.03094 | best_loss=0.03094
Epoch 20/80: current_loss=0.03089 | best_loss=0.03089
Epoch 21/80: current_loss=0.03060 | best_loss=0.03060
Epoch 22/80: current_loss=0.03054 | best_loss=0.03054
Epoch 23/80: current_loss=0.03040 | best_loss=0.03040
Epoch 24/80: current_loss=0.03030 | best_loss=0.03030
Epoch 25/80: current_loss=0.03021 | best_loss=0.03021
Epoch 26/80: current_loss=0.03034 | best_loss=0.03021
Epoch 27/80: current_loss=0.03013 | best_loss=0.03013
Epoch 28/80: current_loss=0.03005 | best_loss=0.03005
Epoch 29/80: current_loss=0.03009 | best_loss=0.03005
Epoch 30/80: current_loss=0.02997 | best_loss=0.02997
Epoch 31/80: current_loss=0.02990 | best_loss=0.02990
Epoch 32/80: current_loss=0.02988 | best_loss=0.02988
Epoch 33/80: current_loss=0.02980 | best_loss=0.02980
Epoch 34/80: current_loss=0.02969 | best_loss=0.02969
Epoch 35/80: current_loss=0.02969 | best_loss=0.02969
Epoch 36/80: current_loss=0.02957 | best_loss=0.02957
Epoch 37/80: current_loss=0.02978 | best_loss=0.02957
Epoch 38/80: current_loss=0.02954 | best_loss=0.02954
Epoch 39/80: current_loss=0.02968 | best_loss=0.02954
Epoch 40/80: current_loss=0.02949 | best_loss=0.02949
Epoch 41/80: current_loss=0.02952 | best_loss=0.02949
Epoch 42/80: current_loss=0.02943 | best_loss=0.02943
Epoch 43/80: current_loss=0.02939 | best_loss=0.02939
Epoch 44/80: current_loss=0.02939 | best_loss=0.02939
Epoch 45/80: current_loss=0.02940 | best_loss=0.02939
Epoch 46/80: current_loss=0.02932 | best_loss=0.02932
Epoch 47/80: current_loss=0.02943 | best_loss=0.02932
Epoch 48/80: current_loss=0.02929 | best_loss=0.02929
Epoch 49/80: current_loss=0.02934 | best_loss=0.02929
Epoch 50/80: current_loss=0.02930 | best_loss=0.02929
Epoch 51/80: current_loss=0.02935 | best_loss=0.02929
Epoch 52/80: current_loss=0.02921 | best_loss=0.02921
Epoch 53/80: current_loss=0.02925 | best_loss=0.02921
Epoch 54/80: current_loss=0.02921 | best_loss=0.02921
Epoch 55/80: current_loss=0.02929 | best_loss=0.02921
Epoch 56/80: current_loss=0.02930 | best_loss=0.02921
Epoch 57/80: current_loss=0.02922 | best_loss=0.02921
Epoch 58/80: current_loss=0.02923 | best_loss=0.02921
Epoch 59/80: current_loss=0.02920 | best_loss=0.02920
Epoch 60/80: current_loss=0.02926 | best_loss=0.02920
Epoch 61/80: current_loss=0.02919 | best_loss=0.02919
Epoch 62/80: current_loss=0.02918 | best_loss=0.02918
Epoch 63/80: current_loss=0.02918 | best_loss=0.02918
Epoch 64/80: current_loss=0.02915 | best_loss=0.02915
Epoch 65/80: current_loss=0.02920 | best_loss=0.02915
Epoch 66/80: current_loss=0.02914 | best_loss=0.02914
Epoch 67/80: current_loss=0.02917 | best_loss=0.02914
Epoch 68/80: current_loss=0.02918 | best_loss=0.02914
Epoch 69/80: current_loss=0.02919 | best_loss=0.02914
Epoch 70/80: current_loss=0.02919 | best_loss=0.02914
Epoch 71/80: current_loss=0.02915 | best_loss=0.02914
Epoch 72/80: current_loss=0.02920 | best_loss=0.02914
Epoch 73/80: current_loss=0.02911 | best_loss=0.02911
Epoch 74/80: current_loss=0.02912 | best_loss=0.02911
Epoch 75/80: current_loss=0.02916 | best_loss=0.02911
Epoch 76/80: current_loss=0.02917 | best_loss=0.02911
Epoch 77/80: current_loss=0.02917 | best_loss=0.02911
Epoch 78/80: current_loss=0.02918 | best_loss=0.02911
Epoch 79/80: current_loss=0.02916 | best_loss=0.02911
      explained_var=-0.00055 | mse_loss=0.02816
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02485 | best_loss=0.02485
Epoch 1/80: current_loss=0.02505 | best_loss=0.02485
Epoch 2/80: current_loss=0.02499 | best_loss=0.02485
Epoch 3/80: current_loss=0.02505 | best_loss=0.02485
Epoch 4/80: current_loss=0.02536 | best_loss=0.02485
Epoch 5/80: current_loss=0.02496 | best_loss=0.02485
Epoch 6/80: current_loss=0.02489 | best_loss=0.02485
Epoch 7/80: current_loss=0.02510 | best_loss=0.02485
Epoch 8/80: current_loss=0.02496 | best_loss=0.02485
Epoch 9/80: current_loss=0.02504 | best_loss=0.02485
Epoch 10/80: current_loss=0.02497 | best_loss=0.02485
Epoch 11/80: current_loss=0.02495 | best_loss=0.02485
Epoch 12/80: current_loss=0.02516 | best_loss=0.02485
Epoch 13/80: current_loss=0.02505 | best_loss=0.02485
Epoch 14/80: current_loss=0.02487 | best_loss=0.02485
Epoch 15/80: current_loss=0.02488 | best_loss=0.02485
Epoch 16/80: current_loss=0.02507 | best_loss=0.02485
Epoch 17/80: current_loss=0.02518 | best_loss=0.02485
Epoch 18/80: current_loss=0.02492 | best_loss=0.02485
Epoch 19/80: current_loss=0.02499 | best_loss=0.02485
Epoch 20/80: current_loss=0.02498 | best_loss=0.02485
Early Stopping at epoch 20
      explained_var=0.00101 | mse_loss=0.02522
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02833 | best_loss=0.02833
Epoch 1/80: current_loss=0.02820 | best_loss=0.02820
Epoch 2/80: current_loss=0.02838 | best_loss=0.02820
Epoch 3/80: current_loss=0.02852 | best_loss=0.02820
Epoch 4/80: current_loss=0.02802 | best_loss=0.02802
Epoch 5/80: current_loss=0.02808 | best_loss=0.02802
Epoch 6/80: current_loss=0.02830 | best_loss=0.02802
Epoch 7/80: current_loss=0.02814 | best_loss=0.02802
Epoch 8/80: current_loss=0.02820 | best_loss=0.02802
Epoch 9/80: current_loss=0.02854 | best_loss=0.02802
Epoch 10/80: current_loss=0.02811 | best_loss=0.02802
Epoch 11/80: current_loss=0.02819 | best_loss=0.02802
Epoch 12/80: current_loss=0.02826 | best_loss=0.02802
Epoch 13/80: current_loss=0.02807 | best_loss=0.02802
Epoch 14/80: current_loss=0.02829 | best_loss=0.02802
Epoch 15/80: current_loss=0.02788 | best_loss=0.02788
Epoch 16/80: current_loss=0.02826 | best_loss=0.02788
Epoch 17/80: current_loss=0.02809 | best_loss=0.02788
Epoch 18/80: current_loss=0.02820 | best_loss=0.02788
Epoch 19/80: current_loss=0.02812 | best_loss=0.02788
Epoch 20/80: current_loss=0.02799 | best_loss=0.02788
Epoch 21/80: current_loss=0.02803 | best_loss=0.02788
Epoch 22/80: current_loss=0.02805 | best_loss=0.02788
Epoch 23/80: current_loss=0.02845 | best_loss=0.02788
Epoch 24/80: current_loss=0.02818 | best_loss=0.02788
Epoch 25/80: current_loss=0.02783 | best_loss=0.02783
Epoch 26/80: current_loss=0.02806 | best_loss=0.02783
Epoch 27/80: current_loss=0.02791 | best_loss=0.02783
Epoch 28/80: current_loss=0.02832 | best_loss=0.02783
Epoch 29/80: current_loss=0.02834 | best_loss=0.02783
Epoch 30/80: current_loss=0.02825 | best_loss=0.02783
Epoch 31/80: current_loss=0.02801 | best_loss=0.02783
Epoch 32/80: current_loss=0.02798 | best_loss=0.02783
Epoch 33/80: current_loss=0.02823 | best_loss=0.02783
Epoch 34/80: current_loss=0.02806 | best_loss=0.02783
Epoch 35/80: current_loss=0.02844 | best_loss=0.02783
Epoch 36/80: current_loss=0.02790 | best_loss=0.02783
Epoch 37/80: current_loss=0.02799 | best_loss=0.02783
Epoch 38/80: current_loss=0.02788 | best_loss=0.02783
Epoch 39/80: current_loss=0.02788 | best_loss=0.02783
Epoch 40/80: current_loss=0.02784 | best_loss=0.02783
Epoch 41/80: current_loss=0.02785 | best_loss=0.02783
Epoch 42/80: current_loss=0.02788 | best_loss=0.02783
Epoch 43/80: current_loss=0.02807 | best_loss=0.02783
Epoch 44/80: current_loss=0.02827 | best_loss=0.02783
Epoch 45/80: current_loss=0.02826 | best_loss=0.02783
Early Stopping at epoch 45
      explained_var=-0.00695 | mse_loss=0.02829
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02677 | best_loss=0.02677
Epoch 1/80: current_loss=0.02683 | best_loss=0.02677
Epoch 2/80: current_loss=0.02682 | best_loss=0.02677
Epoch 3/80: current_loss=0.02678 | best_loss=0.02677
Epoch 4/80: current_loss=0.02679 | best_loss=0.02677
Epoch 5/80: current_loss=0.02679 | best_loss=0.02677
Epoch 6/80: current_loss=0.02679 | best_loss=0.02677
Epoch 7/80: current_loss=0.02679 | best_loss=0.02677
Epoch 8/80: current_loss=0.02679 | best_loss=0.02677
Epoch 9/80: current_loss=0.02680 | best_loss=0.02677
Epoch 10/80: current_loss=0.02683 | best_loss=0.02677
Epoch 11/80: current_loss=0.02680 | best_loss=0.02677
Epoch 12/80: current_loss=0.02679 | best_loss=0.02677
Epoch 13/80: current_loss=0.02679 | best_loss=0.02677
Epoch 14/80: current_loss=0.02680 | best_loss=0.02677
Epoch 15/80: current_loss=0.02680 | best_loss=0.02677
Epoch 16/80: current_loss=0.02680 | best_loss=0.02677
Epoch 17/80: current_loss=0.02682 | best_loss=0.02677
Epoch 18/80: current_loss=0.02680 | best_loss=0.02677
Epoch 19/80: current_loss=0.02683 | best_loss=0.02677
Epoch 20/80: current_loss=0.02681 | best_loss=0.02677
Early Stopping at epoch 20
      explained_var=0.00096 | mse_loss=0.02496
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02982 | best_loss=0.02982
Epoch 1/80: current_loss=0.02976 | best_loss=0.02976
Epoch 2/80: current_loss=0.02970 | best_loss=0.02970
Epoch 3/80: current_loss=0.02971 | best_loss=0.02970
Epoch 4/80: current_loss=0.02984 | best_loss=0.02970
Epoch 5/80: current_loss=0.02974 | best_loss=0.02970
Epoch 6/80: current_loss=0.02977 | best_loss=0.02970
Epoch 7/80: current_loss=0.02971 | best_loss=0.02970
Epoch 8/80: current_loss=0.02973 | best_loss=0.02970
Epoch 9/80: current_loss=0.02968 | best_loss=0.02968
Epoch 10/80: current_loss=0.02977 | best_loss=0.02968
Epoch 11/80: current_loss=0.02972 | best_loss=0.02968
Epoch 12/80: current_loss=0.02976 | best_loss=0.02968
Epoch 13/80: current_loss=0.02975 | best_loss=0.02968
Epoch 14/80: current_loss=0.02981 | best_loss=0.02968
Epoch 15/80: current_loss=0.02975 | best_loss=0.02968
Epoch 16/80: current_loss=0.02976 | best_loss=0.02968
Epoch 17/80: current_loss=0.02981 | best_loss=0.02968
Epoch 18/80: current_loss=0.02979 | best_loss=0.02968
Epoch 19/80: current_loss=0.02980 | best_loss=0.02968
Epoch 20/80: current_loss=0.02977 | best_loss=0.02968
Epoch 21/80: current_loss=0.02978 | best_loss=0.02968
Epoch 22/80: current_loss=0.02976 | best_loss=0.02968
Epoch 23/80: current_loss=0.02977 | best_loss=0.02968
Epoch 24/80: current_loss=0.02976 | best_loss=0.02968
Epoch 25/80: current_loss=0.02979 | best_loss=0.02968
Epoch 26/80: current_loss=0.02975 | best_loss=0.02968
Epoch 27/80: current_loss=0.02977 | best_loss=0.02968
Epoch 28/80: current_loss=0.02976 | best_loss=0.02968
Epoch 29/80: current_loss=0.02976 | best_loss=0.02968
Early Stopping at epoch 29
      explained_var=0.00247 | mse_loss=0.02869
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=-0.00061| avg_loss=0.02707
----------------------------------------------


----------------------------------------------
Params for Trial 1
{'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.15794 | best_loss=0.15794
Epoch 1/80: current_loss=0.12599 | best_loss=0.12599
Epoch 2/80: current_loss=0.09907 | best_loss=0.09907
Epoch 3/80: current_loss=0.07727 | best_loss=0.07727
Epoch 4/80: current_loss=0.06086 | best_loss=0.06086
Epoch 5/80: current_loss=0.04964 | best_loss=0.04964
Epoch 6/80: current_loss=0.04221 | best_loss=0.04221
Epoch 7/80: current_loss=0.03839 | best_loss=0.03839
Epoch 8/80: current_loss=0.03674 | best_loss=0.03674
Epoch 9/80: current_loss=0.03586 | best_loss=0.03586
Epoch 10/80: current_loss=0.03544 | best_loss=0.03544
Epoch 11/80: current_loss=0.03518 | best_loss=0.03518
Epoch 12/80: current_loss=0.03498 | best_loss=0.03498
Epoch 13/80: current_loss=0.03479 | best_loss=0.03479
Epoch 14/80: current_loss=0.03458 | best_loss=0.03458
Epoch 15/80: current_loss=0.03436 | best_loss=0.03436
Epoch 16/80: current_loss=0.03415 | best_loss=0.03415
Epoch 17/80: current_loss=0.03394 | best_loss=0.03394
Epoch 18/80: current_loss=0.03376 | best_loss=0.03376
Epoch 19/80: current_loss=0.03357 | best_loss=0.03357
Epoch 20/80: current_loss=0.03340 | best_loss=0.03340
Epoch 21/80: current_loss=0.03323 | best_loss=0.03323
Epoch 22/80: current_loss=0.03309 | best_loss=0.03309
Epoch 23/80: current_loss=0.03293 | best_loss=0.03293
Epoch 24/80: current_loss=0.03280 | best_loss=0.03280
Epoch 25/80: current_loss=0.03268 | best_loss=0.03268
Epoch 26/80: current_loss=0.03253 | best_loss=0.03253
Epoch 27/80: current_loss=0.03240 | best_loss=0.03240
Epoch 28/80: current_loss=0.03227 | best_loss=0.03227
Epoch 29/80: current_loss=0.03212 | best_loss=0.03212
Epoch 30/80: current_loss=0.03200 | best_loss=0.03200
Epoch 31/80: current_loss=0.03187 | best_loss=0.03187
Epoch 32/80: current_loss=0.03176 | best_loss=0.03176
Epoch 33/80: current_loss=0.03164 | best_loss=0.03164
Epoch 34/80: current_loss=0.03152 | best_loss=0.03152
Epoch 35/80: current_loss=0.03144 | best_loss=0.03144
Epoch 36/80: current_loss=0.03134 | best_loss=0.03134
Epoch 37/80: current_loss=0.03126 | best_loss=0.03126
Epoch 38/80: current_loss=0.03117 | best_loss=0.03117
Epoch 39/80: current_loss=0.03108 | best_loss=0.03108
Epoch 40/80: current_loss=0.03101 | best_loss=0.03101
Epoch 41/80: current_loss=0.03092 | best_loss=0.03092
Epoch 42/80: current_loss=0.03082 | best_loss=0.03082
Epoch 43/80: current_loss=0.03076 | best_loss=0.03076
Epoch 44/80: current_loss=0.03069 | best_loss=0.03069
Epoch 45/80: current_loss=0.03064 | best_loss=0.03064
Epoch 46/80: current_loss=0.03055 | best_loss=0.03055
Epoch 47/80: current_loss=0.03048 | best_loss=0.03048
Epoch 48/80: current_loss=0.03042 | best_loss=0.03042
Epoch 49/80: current_loss=0.03035 | best_loss=0.03035
Epoch 50/80: current_loss=0.03031 | best_loss=0.03031
Epoch 51/80: current_loss=0.03025 | best_loss=0.03025
Epoch 52/80: current_loss=0.03021 | best_loss=0.03021
Epoch 53/80: current_loss=0.03015 | best_loss=0.03015
Epoch 54/80: current_loss=0.03012 | best_loss=0.03012
Epoch 55/80: current_loss=0.03007 | best_loss=0.03007
Epoch 56/80: current_loss=0.03002 | best_loss=0.03002
Epoch 57/80: current_loss=0.02997 | best_loss=0.02997
Epoch 58/80: current_loss=0.02992 | best_loss=0.02992
Epoch 59/80: current_loss=0.02988 | best_loss=0.02988
Epoch 60/80: current_loss=0.02985 | best_loss=0.02985
Epoch 61/80: current_loss=0.02981 | best_loss=0.02981
Epoch 62/80: current_loss=0.02978 | best_loss=0.02978
Epoch 63/80: current_loss=0.02974 | best_loss=0.02974
Epoch 64/80: current_loss=0.02972 | best_loss=0.02972
Epoch 65/80: current_loss=0.02971 | best_loss=0.02971
Epoch 66/80: current_loss=0.02966 | best_loss=0.02966
Epoch 67/80: current_loss=0.02963 | best_loss=0.02963
Epoch 68/80: current_loss=0.02960 | best_loss=0.02960
Epoch 69/80: current_loss=0.02957 | best_loss=0.02957
Epoch 70/80: current_loss=0.02954 | best_loss=0.02954
Epoch 71/80: current_loss=0.02953 | best_loss=0.02953
Epoch 72/80: current_loss=0.02950 | best_loss=0.02950
Epoch 73/80: current_loss=0.02948 | best_loss=0.02948
Epoch 74/80: current_loss=0.02947 | best_loss=0.02947
Epoch 75/80: current_loss=0.02946 | best_loss=0.02946
Epoch 76/80: current_loss=0.02944 | best_loss=0.02944
Epoch 77/80: current_loss=0.02939 | best_loss=0.02939
Epoch 78/80: current_loss=0.02937 | best_loss=0.02937
Epoch 79/80: current_loss=0.02936 | best_loss=0.02936
      explained_var=-0.00348 | mse_loss=0.02825
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02560 | best_loss=0.02560
Epoch 1/80: current_loss=0.02552 | best_loss=0.02552
Epoch 2/80: current_loss=0.02555 | best_loss=0.02552
Epoch 3/80: current_loss=0.02546 | best_loss=0.02546
Epoch 4/80: current_loss=0.02548 | best_loss=0.02546
Epoch 5/80: current_loss=0.02543 | best_loss=0.02543
Epoch 6/80: current_loss=0.02542 | best_loss=0.02542
Epoch 7/80: current_loss=0.02551 | best_loss=0.02542
Epoch 8/80: current_loss=0.02541 | best_loss=0.02541
Epoch 9/80: current_loss=0.02536 | best_loss=0.02536
Epoch 10/80: current_loss=0.02534 | best_loss=0.02534
Epoch 11/80: current_loss=0.02533 | best_loss=0.02533
Epoch 12/80: current_loss=0.02530 | best_loss=0.02530
Epoch 13/80: current_loss=0.02534 | best_loss=0.02530
Epoch 14/80: current_loss=0.02532 | best_loss=0.02530
Epoch 15/80: current_loss=0.02525 | best_loss=0.02525
Epoch 16/80: current_loss=0.02525 | best_loss=0.02525
Epoch 17/80: current_loss=0.02527 | best_loss=0.02525
Epoch 18/80: current_loss=0.02525 | best_loss=0.02525
Epoch 19/80: current_loss=0.02527 | best_loss=0.02525
Epoch 20/80: current_loss=0.02523 | best_loss=0.02523
Epoch 21/80: current_loss=0.02525 | best_loss=0.02523
Epoch 22/80: current_loss=0.02525 | best_loss=0.02523
Epoch 23/80: current_loss=0.02523 | best_loss=0.02523
Epoch 24/80: current_loss=0.02521 | best_loss=0.02521
Epoch 25/80: current_loss=0.02522 | best_loss=0.02521
Epoch 26/80: current_loss=0.02519 | best_loss=0.02519
Epoch 27/80: current_loss=0.02521 | best_loss=0.02519
Epoch 28/80: current_loss=0.02519 | best_loss=0.02519
Epoch 29/80: current_loss=0.02516 | best_loss=0.02516
Epoch 30/80: current_loss=0.02521 | best_loss=0.02516
Epoch 31/80: current_loss=0.02518 | best_loss=0.02516
Epoch 32/80: current_loss=0.02519 | best_loss=0.02516
Epoch 33/80: current_loss=0.02514 | best_loss=0.02514
Epoch 34/80: current_loss=0.02514 | best_loss=0.02514
Epoch 35/80: current_loss=0.02518 | best_loss=0.02514
Epoch 36/80: current_loss=0.02512 | best_loss=0.02512
Epoch 37/80: current_loss=0.02508 | best_loss=0.02508
Epoch 38/80: current_loss=0.02512 | best_loss=0.02508
Epoch 39/80: current_loss=0.02512 | best_loss=0.02508
Epoch 40/80: current_loss=0.02514 | best_loss=0.02508
Epoch 41/80: current_loss=0.02515 | best_loss=0.02508
Epoch 42/80: current_loss=0.02511 | best_loss=0.02508
Epoch 43/80: current_loss=0.02511 | best_loss=0.02508
Epoch 44/80: current_loss=0.02511 | best_loss=0.02508
Epoch 45/80: current_loss=0.02510 | best_loss=0.02508
Epoch 46/80: current_loss=0.02513 | best_loss=0.02508
Epoch 47/80: current_loss=0.02509 | best_loss=0.02508
Epoch 48/80: current_loss=0.02508 | best_loss=0.02508
Epoch 49/80: current_loss=0.02512 | best_loss=0.02508
Epoch 50/80: current_loss=0.02508 | best_loss=0.02508
Epoch 51/80: current_loss=0.02504 | best_loss=0.02504
Epoch 52/80: current_loss=0.02506 | best_loss=0.02504
Epoch 53/80: current_loss=0.02507 | best_loss=0.02504
Epoch 54/80: current_loss=0.02505 | best_loss=0.02504
Epoch 55/80: current_loss=0.02505 | best_loss=0.02504
Epoch 56/80: current_loss=0.02506 | best_loss=0.02504
Epoch 57/80: current_loss=0.02504 | best_loss=0.02504
Epoch 58/80: current_loss=0.02504 | best_loss=0.02504
Epoch 59/80: current_loss=0.02506 | best_loss=0.02504
Epoch 60/80: current_loss=0.02504 | best_loss=0.02504
Epoch 61/80: current_loss=0.02505 | best_loss=0.02504
Epoch 62/80: current_loss=0.02501 | best_loss=0.02501
Epoch 63/80: current_loss=0.02501 | best_loss=0.02501
Epoch 64/80: current_loss=0.02500 | best_loss=0.02500
Epoch 65/80: current_loss=0.02500 | best_loss=0.02500
Epoch 66/80: current_loss=0.02498 | best_loss=0.02498
Epoch 67/80: current_loss=0.02500 | best_loss=0.02498
Epoch 68/80: current_loss=0.02502 | best_loss=0.02498
Epoch 69/80: current_loss=0.02499 | best_loss=0.02498
Epoch 70/80: current_loss=0.02500 | best_loss=0.02498
Epoch 71/80: current_loss=0.02497 | best_loss=0.02497
Epoch 72/80: current_loss=0.02498 | best_loss=0.02497
Epoch 73/80: current_loss=0.02499 | best_loss=0.02497
Epoch 74/80: current_loss=0.02498 | best_loss=0.02497
Epoch 75/80: current_loss=0.02500 | best_loss=0.02497
Epoch 76/80: current_loss=0.02498 | best_loss=0.02497
Epoch 77/80: current_loss=0.02499 | best_loss=0.02497
Epoch 78/80: current_loss=0.02499 | best_loss=0.02497
Epoch 79/80: current_loss=0.02498 | best_loss=0.02497
      explained_var=0.00115 | mse_loss=0.02520
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02832 | best_loss=0.02832
Epoch 1/80: current_loss=0.02839 | best_loss=0.02832
Epoch 2/80: current_loss=0.02836 | best_loss=0.02832
Epoch 3/80: current_loss=0.02833 | best_loss=0.02832
Epoch 4/80: current_loss=0.02838 | best_loss=0.02832
Epoch 5/80: current_loss=0.02844 | best_loss=0.02832
Epoch 6/80: current_loss=0.02830 | best_loss=0.02830
Epoch 7/80: current_loss=0.02835 | best_loss=0.02830
Epoch 8/80: current_loss=0.02837 | best_loss=0.02830
Epoch 9/80: current_loss=0.02835 | best_loss=0.02830
Epoch 10/80: current_loss=0.02838 | best_loss=0.02830
Epoch 11/80: current_loss=0.02833 | best_loss=0.02830
Epoch 12/80: current_loss=0.02827 | best_loss=0.02827
Epoch 13/80: current_loss=0.02835 | best_loss=0.02827
Epoch 14/80: current_loss=0.02837 | best_loss=0.02827
Epoch 15/80: current_loss=0.02842 | best_loss=0.02827
Epoch 16/80: current_loss=0.02838 | best_loss=0.02827
Epoch 17/80: current_loss=0.02847 | best_loss=0.02827
Epoch 18/80: current_loss=0.02835 | best_loss=0.02827
Epoch 19/80: current_loss=0.02832 | best_loss=0.02827
Epoch 20/80: current_loss=0.02837 | best_loss=0.02827
Epoch 21/80: current_loss=0.02833 | best_loss=0.02827
Epoch 22/80: current_loss=0.02829 | best_loss=0.02827
Epoch 23/80: current_loss=0.02830 | best_loss=0.02827
Epoch 24/80: current_loss=0.02826 | best_loss=0.02826
Epoch 25/80: current_loss=0.02821 | best_loss=0.02821
Epoch 26/80: current_loss=0.02821 | best_loss=0.02821
Epoch 27/80: current_loss=0.02823 | best_loss=0.02821
Epoch 28/80: current_loss=0.02828 | best_loss=0.02821
Epoch 29/80: current_loss=0.02835 | best_loss=0.02821
Epoch 30/80: current_loss=0.02829 | best_loss=0.02821
Epoch 31/80: current_loss=0.02834 | best_loss=0.02821
Epoch 32/80: current_loss=0.02831 | best_loss=0.02821
Epoch 33/80: current_loss=0.02828 | best_loss=0.02821
Epoch 34/80: current_loss=0.02823 | best_loss=0.02821
Epoch 35/80: current_loss=0.02836 | best_loss=0.02821
Epoch 36/80: current_loss=0.02830 | best_loss=0.02821
Epoch 37/80: current_loss=0.02830 | best_loss=0.02821
Epoch 38/80: current_loss=0.02824 | best_loss=0.02821
Epoch 39/80: current_loss=0.02831 | best_loss=0.02821
Epoch 40/80: current_loss=0.02829 | best_loss=0.02821
Epoch 41/80: current_loss=0.02817 | best_loss=0.02817
Epoch 42/80: current_loss=0.02820 | best_loss=0.02817
Epoch 43/80: current_loss=0.02827 | best_loss=0.02817
Epoch 44/80: current_loss=0.02824 | best_loss=0.02817
Epoch 45/80: current_loss=0.02818 | best_loss=0.02817
Epoch 46/80: current_loss=0.02827 | best_loss=0.02817
Epoch 47/80: current_loss=0.02829 | best_loss=0.02817
Epoch 48/80: current_loss=0.02817 | best_loss=0.02817
Epoch 49/80: current_loss=0.02822 | best_loss=0.02817
Epoch 50/80: current_loss=0.02825 | best_loss=0.02817
Epoch 51/80: current_loss=0.02829 | best_loss=0.02817
Epoch 52/80: current_loss=0.02824 | best_loss=0.02817
Epoch 53/80: current_loss=0.02825 | best_loss=0.02817
Epoch 54/80: current_loss=0.02831 | best_loss=0.02817
Epoch 55/80: current_loss=0.02827 | best_loss=0.02817
Epoch 56/80: current_loss=0.02816 | best_loss=0.02816
Epoch 57/80: current_loss=0.02816 | best_loss=0.02816
Epoch 58/80: current_loss=0.02820 | best_loss=0.02816
Epoch 59/80: current_loss=0.02819 | best_loss=0.02816
Epoch 60/80: current_loss=0.02820 | best_loss=0.02816
Epoch 61/80: current_loss=0.02823 | best_loss=0.02816
Epoch 62/80: current_loss=0.02823 | best_loss=0.02816
Epoch 63/80: current_loss=0.02824 | best_loss=0.02816
Epoch 64/80: current_loss=0.02821 | best_loss=0.02816
Epoch 65/80: current_loss=0.02829 | best_loss=0.02816
Epoch 66/80: current_loss=0.02823 | best_loss=0.02816
Epoch 67/80: current_loss=0.02819 | best_loss=0.02816
Epoch 68/80: current_loss=0.02824 | best_loss=0.02816
Epoch 69/80: current_loss=0.02819 | best_loss=0.02816
Epoch 70/80: current_loss=0.02809 | best_loss=0.02809
Epoch 71/80: current_loss=0.02816 | best_loss=0.02809
Epoch 72/80: current_loss=0.02817 | best_loss=0.02809
Epoch 73/80: current_loss=0.02821 | best_loss=0.02809
Epoch 74/80: current_loss=0.02820 | best_loss=0.02809
Epoch 75/80: current_loss=0.02818 | best_loss=0.02809
Epoch 76/80: current_loss=0.02814 | best_loss=0.02809
Epoch 77/80: current_loss=0.02816 | best_loss=0.02809
Epoch 78/80: current_loss=0.02815 | best_loss=0.02809
Epoch 79/80: current_loss=0.02826 | best_loss=0.02809
      explained_var=-0.02105 | mse_loss=0.02865
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02662 | best_loss=0.02662
Epoch 1/80: current_loss=0.02664 | best_loss=0.02662
Epoch 2/80: current_loss=0.02662 | best_loss=0.02662
Epoch 3/80: current_loss=0.02663 | best_loss=0.02662
Epoch 4/80: current_loss=0.02663 | best_loss=0.02662
Epoch 5/80: current_loss=0.02664 | best_loss=0.02662
Epoch 6/80: current_loss=0.02662 | best_loss=0.02662
Epoch 7/80: current_loss=0.02663 | best_loss=0.02662
Epoch 8/80: current_loss=0.02662 | best_loss=0.02662
Epoch 9/80: current_loss=0.02663 | best_loss=0.02662
Epoch 10/80: current_loss=0.02665 | best_loss=0.02662
Epoch 11/80: current_loss=0.02665 | best_loss=0.02662
Epoch 12/80: current_loss=0.02664 | best_loss=0.02662
Epoch 13/80: current_loss=0.02662 | best_loss=0.02662
Epoch 14/80: current_loss=0.02663 | best_loss=0.02662
Epoch 15/80: current_loss=0.02663 | best_loss=0.02662
Epoch 16/80: current_loss=0.02662 | best_loss=0.02662
Epoch 17/80: current_loss=0.02664 | best_loss=0.02662
Epoch 18/80: current_loss=0.02665 | best_loss=0.02662
Epoch 19/80: current_loss=0.02663 | best_loss=0.02662
Epoch 20/80: current_loss=0.02664 | best_loss=0.02662
Early Stopping at epoch 20
      explained_var=0.00245 | mse_loss=0.02492
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03043 | best_loss=0.03043
Epoch 1/80: current_loss=0.02998 | best_loss=0.02998
Epoch 2/80: current_loss=0.02986 | best_loss=0.02986
Epoch 3/80: current_loss=0.02985 | best_loss=0.02985
Epoch 4/80: current_loss=0.02984 | best_loss=0.02984
Epoch 5/80: current_loss=0.02983 | best_loss=0.02983
Epoch 6/80: current_loss=0.02982 | best_loss=0.02982
Epoch 7/80: current_loss=0.02982 | best_loss=0.02982
Epoch 8/80: current_loss=0.02982 | best_loss=0.02982
Epoch 9/80: current_loss=0.02983 | best_loss=0.02982
Epoch 10/80: current_loss=0.02983 | best_loss=0.02982
Epoch 11/80: current_loss=0.02982 | best_loss=0.02982
Epoch 12/80: current_loss=0.02982 | best_loss=0.02982
Epoch 13/80: current_loss=0.02983 | best_loss=0.02982
Epoch 14/80: current_loss=0.02983 | best_loss=0.02982
Epoch 15/80: current_loss=0.02983 | best_loss=0.02982
Epoch 16/80: current_loss=0.02983 | best_loss=0.02982
Epoch 17/80: current_loss=0.02981 | best_loss=0.02981
Epoch 18/80: current_loss=0.02983 | best_loss=0.02981
Epoch 19/80: current_loss=0.02983 | best_loss=0.02981
Epoch 20/80: current_loss=0.02981 | best_loss=0.02981
Epoch 21/80: current_loss=0.02982 | best_loss=0.02981
Epoch 22/80: current_loss=0.02981 | best_loss=0.02981
Epoch 23/80: current_loss=0.02980 | best_loss=0.02980
Epoch 24/80: current_loss=0.02981 | best_loss=0.02980
Epoch 25/80: current_loss=0.02980 | best_loss=0.02980
Epoch 26/80: current_loss=0.02982 | best_loss=0.02980
Epoch 27/80: current_loss=0.02981 | best_loss=0.02980
Epoch 28/80: current_loss=0.02979 | best_loss=0.02979
Epoch 29/80: current_loss=0.02979 | best_loss=0.02979
Epoch 30/80: current_loss=0.02981 | best_loss=0.02979
Epoch 31/80: current_loss=0.02980 | best_loss=0.02979
Epoch 32/80: current_loss=0.02980 | best_loss=0.02979
Epoch 33/80: current_loss=0.02979 | best_loss=0.02979
Epoch 34/80: current_loss=0.02979 | best_loss=0.02979
Epoch 35/80: current_loss=0.02978 | best_loss=0.02978
Epoch 36/80: current_loss=0.02981 | best_loss=0.02978
Epoch 37/80: current_loss=0.02979 | best_loss=0.02978
Epoch 38/80: current_loss=0.02979 | best_loss=0.02978
Epoch 39/80: current_loss=0.02980 | best_loss=0.02978
Epoch 40/80: current_loss=0.02979 | best_loss=0.02978
Epoch 41/80: current_loss=0.02978 | best_loss=0.02978
Epoch 42/80: current_loss=0.02977 | best_loss=0.02977
Epoch 43/80: current_loss=0.02977 | best_loss=0.02977
Epoch 44/80: current_loss=0.02979 | best_loss=0.02977
Epoch 45/80: current_loss=0.02977 | best_loss=0.02977
Epoch 46/80: current_loss=0.02977 | best_loss=0.02977
Epoch 47/80: current_loss=0.02978 | best_loss=0.02977
Epoch 48/80: current_loss=0.02978 | best_loss=0.02977
Epoch 49/80: current_loss=0.02977 | best_loss=0.02977
Epoch 50/80: current_loss=0.02978 | best_loss=0.02977
Epoch 51/80: current_loss=0.02977 | best_loss=0.02977
Epoch 52/80: current_loss=0.02978 | best_loss=0.02977
Epoch 53/80: current_loss=0.02979 | best_loss=0.02977
Epoch 54/80: current_loss=0.02978 | best_loss=0.02977
Epoch 55/80: current_loss=0.02978 | best_loss=0.02977
Epoch 56/80: current_loss=0.02978 | best_loss=0.02977
Epoch 57/80: current_loss=0.02978 | best_loss=0.02977
Epoch 58/80: current_loss=0.02978 | best_loss=0.02977
Epoch 59/80: current_loss=0.02977 | best_loss=0.02977
Epoch 60/80: current_loss=0.02977 | best_loss=0.02977
Epoch 61/80: current_loss=0.02978 | best_loss=0.02977
Epoch 62/80: current_loss=0.02978 | best_loss=0.02977
Epoch 63/80: current_loss=0.02978 | best_loss=0.02977
Epoch 64/80: current_loss=0.02976 | best_loss=0.02976
Epoch 65/80: current_loss=0.02978 | best_loss=0.02976
Epoch 66/80: current_loss=0.02978 | best_loss=0.02976
Epoch 67/80: current_loss=0.02978 | best_loss=0.02976
Epoch 68/80: current_loss=0.02978 | best_loss=0.02976
Epoch 69/80: current_loss=0.02979 | best_loss=0.02976
Epoch 70/80: current_loss=0.02978 | best_loss=0.02976
Epoch 71/80: current_loss=0.02976 | best_loss=0.02976
Epoch 72/80: current_loss=0.02976 | best_loss=0.02976
Epoch 73/80: current_loss=0.02976 | best_loss=0.02976
Epoch 74/80: current_loss=0.02977 | best_loss=0.02976
Epoch 75/80: current_loss=0.02976 | best_loss=0.02976
Epoch 76/80: current_loss=0.02977 | best_loss=0.02976
Epoch 77/80: current_loss=0.02977 | best_loss=0.02976
Epoch 78/80: current_loss=0.02977 | best_loss=0.02976
Epoch 79/80: current_loss=0.02978 | best_loss=0.02976
      explained_var=-0.00055 | mse_loss=0.02878
----------------------------------------------
Average early_stopping_point: 64| avg_exp_var=-0.00429| avg_loss=0.02716
----------------------------------------------


----------------------------------------------
Params for Trial 2
{'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02925 | best_loss=0.02925
Epoch 1/80: current_loss=0.02919 | best_loss=0.02919
Epoch 2/80: current_loss=0.02916 | best_loss=0.02916
Epoch 3/80: current_loss=0.02903 | best_loss=0.02903
Epoch 4/80: current_loss=0.02917 | best_loss=0.02903
Epoch 5/80: current_loss=0.02930 | best_loss=0.02903
Epoch 6/80: current_loss=0.02891 | best_loss=0.02891
Epoch 7/80: current_loss=0.02895 | best_loss=0.02891
Epoch 8/80: current_loss=0.02877 | best_loss=0.02877
Epoch 9/80: current_loss=0.02876 | best_loss=0.02876
Epoch 10/80: current_loss=0.02889 | best_loss=0.02876
Epoch 11/80: current_loss=0.02934 | best_loss=0.02876
Epoch 12/80: current_loss=0.02936 | best_loss=0.02876
Epoch 13/80: current_loss=0.02938 | best_loss=0.02876
Epoch 14/80: current_loss=0.02917 | best_loss=0.02876
Epoch 15/80: current_loss=0.02915 | best_loss=0.02876
Epoch 16/80: current_loss=0.02922 | best_loss=0.02876
Epoch 17/80: current_loss=0.03038 | best_loss=0.02876
Epoch 18/80: current_loss=0.02958 | best_loss=0.02876
Epoch 19/80: current_loss=0.02954 | best_loss=0.02876
Epoch 20/80: current_loss=0.02921 | best_loss=0.02876
Epoch 21/80: current_loss=0.02924 | best_loss=0.02876
Epoch 22/80: current_loss=0.02912 | best_loss=0.02876
Epoch 23/80: current_loss=0.02967 | best_loss=0.02876
Epoch 24/80: current_loss=0.02926 | best_loss=0.02876
Epoch 25/80: current_loss=0.03183 | best_loss=0.02876
Epoch 26/80: current_loss=0.02919 | best_loss=0.02876
Epoch 27/80: current_loss=0.02958 | best_loss=0.02876
Epoch 28/80: current_loss=0.02915 | best_loss=0.02876
Epoch 29/80: current_loss=0.02899 | best_loss=0.02876
Early Stopping at epoch 29
      explained_var=0.00931 | mse_loss=0.02794
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02488 | best_loss=0.02488
Epoch 1/80: current_loss=0.02491 | best_loss=0.02488
Epoch 2/80: current_loss=0.02525 | best_loss=0.02488
Epoch 3/80: current_loss=0.02512 | best_loss=0.02488
Epoch 4/80: current_loss=0.02523 | best_loss=0.02488
Epoch 5/80: current_loss=0.02724 | best_loss=0.02488
Epoch 6/80: current_loss=0.02517 | best_loss=0.02488
Epoch 7/80: current_loss=0.02525 | best_loss=0.02488
Epoch 8/80: current_loss=0.02610 | best_loss=0.02488
Epoch 9/80: current_loss=0.02494 | best_loss=0.02488
Epoch 10/80: current_loss=0.02623 | best_loss=0.02488
Epoch 11/80: current_loss=0.02485 | best_loss=0.02485
Epoch 12/80: current_loss=0.02537 | best_loss=0.02485
Epoch 13/80: current_loss=0.02524 | best_loss=0.02485
Epoch 14/80: current_loss=0.02502 | best_loss=0.02485
Epoch 15/80: current_loss=0.02487 | best_loss=0.02485
Epoch 16/80: current_loss=0.02510 | best_loss=0.02485
Epoch 17/80: current_loss=0.02590 | best_loss=0.02485
Epoch 18/80: current_loss=0.02547 | best_loss=0.02485
Epoch 19/80: current_loss=0.02491 | best_loss=0.02485
Epoch 20/80: current_loss=0.02500 | best_loss=0.02485
Epoch 21/80: current_loss=0.02539 | best_loss=0.02485
Epoch 22/80: current_loss=0.02508 | best_loss=0.02485
Epoch 23/80: current_loss=0.02527 | best_loss=0.02485
Epoch 24/80: current_loss=0.02583 | best_loss=0.02485
Epoch 25/80: current_loss=0.02500 | best_loss=0.02485
Epoch 26/80: current_loss=0.02522 | best_loss=0.02485
Epoch 27/80: current_loss=0.02553 | best_loss=0.02485
Epoch 28/80: current_loss=0.02507 | best_loss=0.02485
Epoch 29/80: current_loss=0.02496 | best_loss=0.02485
Epoch 30/80: current_loss=0.02494 | best_loss=0.02485
Epoch 31/80: current_loss=0.02495 | best_loss=0.02485
Early Stopping at epoch 31
      explained_var=0.00069 | mse_loss=0.02518
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02848 | best_loss=0.02848
Epoch 1/80: current_loss=0.02725 | best_loss=0.02725
Epoch 2/80: current_loss=0.02738 | best_loss=0.02725
Epoch 3/80: current_loss=0.02731 | best_loss=0.02725
Epoch 4/80: current_loss=0.02848 | best_loss=0.02725
Epoch 5/80: current_loss=0.02739 | best_loss=0.02725
Epoch 6/80: current_loss=0.02761 | best_loss=0.02725
Epoch 7/80: current_loss=0.02833 | best_loss=0.02725
Epoch 8/80: current_loss=0.02809 | best_loss=0.02725
Epoch 9/80: current_loss=0.02762 | best_loss=0.02725
Epoch 10/80: current_loss=0.02764 | best_loss=0.02725
Epoch 11/80: current_loss=0.02728 | best_loss=0.02725
Epoch 12/80: current_loss=0.02786 | best_loss=0.02725
Epoch 13/80: current_loss=0.02786 | best_loss=0.02725
Epoch 14/80: current_loss=0.02736 | best_loss=0.02725
Epoch 15/80: current_loss=0.02784 | best_loss=0.02725
Epoch 16/80: current_loss=0.02728 | best_loss=0.02725
Epoch 17/80: current_loss=0.02725 | best_loss=0.02725
Epoch 18/80: current_loss=0.02887 | best_loss=0.02725
Epoch 19/80: current_loss=0.02743 | best_loss=0.02725
Epoch 20/80: current_loss=0.02844 | best_loss=0.02725
Epoch 21/80: current_loss=0.02785 | best_loss=0.02725
Early Stopping at epoch 21
      explained_var=-0.00527 | mse_loss=0.02770
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02816 | best_loss=0.02816
Epoch 1/80: current_loss=0.02715 | best_loss=0.02715
Epoch 2/80: current_loss=0.02702 | best_loss=0.02702
Epoch 3/80: current_loss=0.02687 | best_loss=0.02687
Epoch 4/80: current_loss=0.02701 | best_loss=0.02687
Epoch 5/80: current_loss=0.02715 | best_loss=0.02687
Epoch 6/80: current_loss=0.02677 | best_loss=0.02677
Epoch 7/80: current_loss=0.02727 | best_loss=0.02677
Epoch 8/80: current_loss=0.02728 | best_loss=0.02677
Epoch 9/80: current_loss=0.02826 | best_loss=0.02677
Epoch 10/80: current_loss=0.02697 | best_loss=0.02677
Epoch 11/80: current_loss=0.02698 | best_loss=0.02677
Epoch 12/80: current_loss=0.02695 | best_loss=0.02677
Epoch 13/80: current_loss=0.02706 | best_loss=0.02677
Epoch 14/80: current_loss=0.02691 | best_loss=0.02677
Epoch 15/80: current_loss=0.02704 | best_loss=0.02677
Epoch 16/80: current_loss=0.02711 | best_loss=0.02677
Epoch 17/80: current_loss=0.02699 | best_loss=0.02677
Epoch 18/80: current_loss=0.02680 | best_loss=0.02677
Epoch 19/80: current_loss=0.02843 | best_loss=0.02677
Epoch 20/80: current_loss=0.02754 | best_loss=0.02677
Epoch 21/80: current_loss=0.02679 | best_loss=0.02677
Epoch 22/80: current_loss=0.02693 | best_loss=0.02677
Epoch 23/80: current_loss=0.02695 | best_loss=0.02677
Epoch 24/80: current_loss=0.02686 | best_loss=0.02677
Epoch 25/80: current_loss=0.02697 | best_loss=0.02677
Epoch 26/80: current_loss=0.02682 | best_loss=0.02677
Early Stopping at epoch 26
      explained_var=0.00160 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02966 | best_loss=0.02966
Epoch 1/80: current_loss=0.02972 | best_loss=0.02966
Epoch 2/80: current_loss=0.02980 | best_loss=0.02966
Epoch 3/80: current_loss=0.02995 | best_loss=0.02966
Epoch 4/80: current_loss=0.02980 | best_loss=0.02966
Epoch 5/80: current_loss=0.02975 | best_loss=0.02966
Epoch 6/80: current_loss=0.02961 | best_loss=0.02961
Epoch 7/80: current_loss=0.02992 | best_loss=0.02961
Epoch 8/80: current_loss=0.02993 | best_loss=0.02961
Epoch 9/80: current_loss=0.03014 | best_loss=0.02961
Epoch 10/80: current_loss=0.02973 | best_loss=0.02961
Epoch 11/80: current_loss=0.02995 | best_loss=0.02961
Epoch 12/80: current_loss=0.03039 | best_loss=0.02961
Epoch 13/80: current_loss=0.03058 | best_loss=0.02961
Epoch 14/80: current_loss=0.03001 | best_loss=0.02961
Epoch 15/80: current_loss=0.03008 | best_loss=0.02961
Epoch 16/80: current_loss=0.02976 | best_loss=0.02961
Epoch 17/80: current_loss=0.03103 | best_loss=0.02961
Epoch 18/80: current_loss=0.02973 | best_loss=0.02961
Epoch 19/80: current_loss=0.02975 | best_loss=0.02961
Epoch 20/80: current_loss=0.03008 | best_loss=0.02961
Epoch 21/80: current_loss=0.02974 | best_loss=0.02961
Epoch 22/80: current_loss=0.03022 | best_loss=0.02961
Epoch 23/80: current_loss=0.02996 | best_loss=0.02961
Epoch 24/80: current_loss=0.02973 | best_loss=0.02961
Epoch 25/80: current_loss=0.02979 | best_loss=0.02961
Epoch 26/80: current_loss=0.02967 | best_loss=0.02961
Early Stopping at epoch 26
      explained_var=0.00360 | mse_loss=0.02865
----------------------------------------------
Average early_stopping_point: 6| avg_exp_var=0.00199| avg_loss=0.02688
----------------------------------------------


----------------------------------------------
Params for Trial 3
{'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02917 | best_loss=0.02917
Epoch 1/80: current_loss=0.02912 | best_loss=0.02912
Epoch 2/80: current_loss=0.02906 | best_loss=0.02906
Epoch 3/80: current_loss=0.02924 | best_loss=0.02906
Epoch 4/80: current_loss=0.02896 | best_loss=0.02896
Epoch 5/80: current_loss=0.02905 | best_loss=0.02896
Epoch 6/80: current_loss=0.02898 | best_loss=0.02896
Epoch 7/80: current_loss=0.02897 | best_loss=0.02896
Epoch 8/80: current_loss=0.02898 | best_loss=0.02896
Epoch 9/80: current_loss=0.02898 | best_loss=0.02896
Epoch 10/80: current_loss=0.02908 | best_loss=0.02896
Epoch 11/80: current_loss=0.02899 | best_loss=0.02896
Epoch 12/80: current_loss=0.02895 | best_loss=0.02895
Epoch 13/80: current_loss=0.02920 | best_loss=0.02895
Epoch 14/80: current_loss=0.02912 | best_loss=0.02895
Epoch 15/80: current_loss=0.02915 | best_loss=0.02895
Epoch 16/80: current_loss=0.02907 | best_loss=0.02895
Epoch 17/80: current_loss=0.02901 | best_loss=0.02895
Epoch 18/80: current_loss=0.02936 | best_loss=0.02895
Epoch 19/80: current_loss=0.02925 | best_loss=0.02895
Epoch 20/80: current_loss=0.02897 | best_loss=0.02895
Epoch 21/80: current_loss=0.02899 | best_loss=0.02895
Epoch 22/80: current_loss=0.02911 | best_loss=0.02895
Epoch 23/80: current_loss=0.02922 | best_loss=0.02895
Epoch 24/80: current_loss=0.02903 | best_loss=0.02895
Epoch 25/80: current_loss=0.02919 | best_loss=0.02895
Epoch 26/80: current_loss=0.02911 | best_loss=0.02895
Epoch 27/80: current_loss=0.02909 | best_loss=0.02895
Epoch 28/80: current_loss=0.02910 | best_loss=0.02895
Epoch 29/80: current_loss=0.02911 | best_loss=0.02895
Epoch 30/80: current_loss=0.02927 | best_loss=0.02895
Epoch 31/80: current_loss=0.02900 | best_loss=0.02895
Epoch 32/80: current_loss=0.02899 | best_loss=0.02895
Early Stopping at epoch 32
      explained_var=0.00317 | mse_loss=0.02809
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02501 | best_loss=0.02501
Epoch 1/80: current_loss=0.02552 | best_loss=0.02501
Epoch 2/80: current_loss=0.02513 | best_loss=0.02501
Epoch 3/80: current_loss=0.02530 | best_loss=0.02501
Epoch 4/80: current_loss=0.02503 | best_loss=0.02501
Epoch 5/80: current_loss=0.02508 | best_loss=0.02501
Epoch 6/80: current_loss=0.02498 | best_loss=0.02498
Epoch 7/80: current_loss=0.02502 | best_loss=0.02498
Epoch 8/80: current_loss=0.02511 | best_loss=0.02498
Epoch 9/80: current_loss=0.02516 | best_loss=0.02498
Epoch 10/80: current_loss=0.02504 | best_loss=0.02498
Epoch 11/80: current_loss=0.02515 | best_loss=0.02498
Epoch 12/80: current_loss=0.02505 | best_loss=0.02498
Epoch 13/80: current_loss=0.02529 | best_loss=0.02498
Epoch 14/80: current_loss=0.02505 | best_loss=0.02498
Epoch 15/80: current_loss=0.02502 | best_loss=0.02498
Epoch 16/80: current_loss=0.02503 | best_loss=0.02498
Epoch 17/80: current_loss=0.02502 | best_loss=0.02498
Epoch 18/80: current_loss=0.02500 | best_loss=0.02498
Epoch 19/80: current_loss=0.02505 | best_loss=0.02498
Epoch 20/80: current_loss=0.02502 | best_loss=0.02498
Epoch 21/80: current_loss=0.02534 | best_loss=0.02498
Epoch 22/80: current_loss=0.02502 | best_loss=0.02498
Epoch 23/80: current_loss=0.02501 | best_loss=0.02498
Epoch 24/80: current_loss=0.02534 | best_loss=0.02498
Epoch 25/80: current_loss=0.02502 | best_loss=0.02498
Epoch 26/80: current_loss=0.02502 | best_loss=0.02498
Early Stopping at epoch 26
      explained_var=0.00057 | mse_loss=0.02519
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02814 | best_loss=0.02814
Epoch 1/80: current_loss=0.02738 | best_loss=0.02738
Epoch 2/80: current_loss=0.02741 | best_loss=0.02738
Epoch 3/80: current_loss=0.02762 | best_loss=0.02738
Epoch 4/80: current_loss=0.02737 | best_loss=0.02737
Epoch 5/80: current_loss=0.02769 | best_loss=0.02737
Epoch 6/80: current_loss=0.02808 | best_loss=0.02737
Epoch 7/80: current_loss=0.02759 | best_loss=0.02737
Epoch 8/80: current_loss=0.02805 | best_loss=0.02737
Epoch 9/80: current_loss=0.02781 | best_loss=0.02737
Epoch 10/80: current_loss=0.02754 | best_loss=0.02737
Epoch 11/80: current_loss=0.02760 | best_loss=0.02737
Epoch 12/80: current_loss=0.02764 | best_loss=0.02737
Epoch 13/80: current_loss=0.02771 | best_loss=0.02737
Epoch 14/80: current_loss=0.02768 | best_loss=0.02737
Epoch 15/80: current_loss=0.02754 | best_loss=0.02737
Epoch 16/80: current_loss=0.02764 | best_loss=0.02737
Epoch 17/80: current_loss=0.02766 | best_loss=0.02737
Epoch 18/80: current_loss=0.02764 | best_loss=0.02737
Epoch 19/80: current_loss=0.02771 | best_loss=0.02737
Epoch 20/80: current_loss=0.02765 | best_loss=0.02737
Epoch 21/80: current_loss=0.02758 | best_loss=0.02737
Epoch 22/80: current_loss=0.02760 | best_loss=0.02737
Epoch 23/80: current_loss=0.02766 | best_loss=0.02737
Epoch 24/80: current_loss=0.02774 | best_loss=0.02737
Early Stopping at epoch 24
      explained_var=-0.00096 | mse_loss=0.02776
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02681 | best_loss=0.02681
Epoch 1/80: current_loss=0.02705 | best_loss=0.02681
Epoch 2/80: current_loss=0.02686 | best_loss=0.02681
Epoch 3/80: current_loss=0.02682 | best_loss=0.02681
Epoch 4/80: current_loss=0.02687 | best_loss=0.02681
Epoch 5/80: current_loss=0.02682 | best_loss=0.02681
Epoch 6/80: current_loss=0.02688 | best_loss=0.02681
Epoch 7/80: current_loss=0.02686 | best_loss=0.02681
Epoch 8/80: current_loss=0.02685 | best_loss=0.02681
Epoch 9/80: current_loss=0.02688 | best_loss=0.02681
Epoch 10/80: current_loss=0.02682 | best_loss=0.02681
Epoch 11/80: current_loss=0.02692 | best_loss=0.02681
Epoch 12/80: current_loss=0.02684 | best_loss=0.02681
Epoch 13/80: current_loss=0.02689 | best_loss=0.02681
Epoch 14/80: current_loss=0.02685 | best_loss=0.02681
Epoch 15/80: current_loss=0.02685 | best_loss=0.02681
Epoch 16/80: current_loss=0.02686 | best_loss=0.02681
Epoch 17/80: current_loss=0.02695 | best_loss=0.02681
Epoch 18/80: current_loss=0.02684 | best_loss=0.02681
Epoch 19/80: current_loss=0.02687 | best_loss=0.02681
Epoch 20/80: current_loss=0.02686 | best_loss=0.02681
Early Stopping at epoch 20
      explained_var=0.00087 | mse_loss=0.02498
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02977 | best_loss=0.02977
Epoch 1/80: current_loss=0.02978 | best_loss=0.02977
Epoch 2/80: current_loss=0.02979 | best_loss=0.02977
Epoch 3/80: current_loss=0.02985 | best_loss=0.02977
Epoch 4/80: current_loss=0.02979 | best_loss=0.02977
Epoch 5/80: current_loss=0.02977 | best_loss=0.02977
Epoch 6/80: current_loss=0.02978 | best_loss=0.02977
Epoch 7/80: current_loss=0.02977 | best_loss=0.02977
Epoch 8/80: current_loss=0.02977 | best_loss=0.02977
Epoch 9/80: current_loss=0.02979 | best_loss=0.02977
Epoch 10/80: current_loss=0.02977 | best_loss=0.02977
Epoch 11/80: current_loss=0.02978 | best_loss=0.02977
Epoch 12/80: current_loss=0.02978 | best_loss=0.02977
Epoch 13/80: current_loss=0.02979 | best_loss=0.02977
Epoch 14/80: current_loss=0.02978 | best_loss=0.02977
Epoch 15/80: current_loss=0.02978 | best_loss=0.02977
Epoch 16/80: current_loss=0.02978 | best_loss=0.02977
Epoch 17/80: current_loss=0.02980 | best_loss=0.02977
Epoch 18/80: current_loss=0.02978 | best_loss=0.02977
Epoch 19/80: current_loss=0.02978 | best_loss=0.02977
Epoch 20/80: current_loss=0.02978 | best_loss=0.02977
Epoch 21/80: current_loss=0.02978 | best_loss=0.02977
Epoch 22/80: current_loss=0.02979 | best_loss=0.02977
Epoch 23/80: current_loss=0.02978 | best_loss=0.02977
Epoch 24/80: current_loss=0.02978 | best_loss=0.02977
Epoch 25/80: current_loss=0.02978 | best_loss=0.02977
Epoch 26/80: current_loss=0.02979 | best_loss=0.02977
Epoch 27/80: current_loss=0.02979 | best_loss=0.02977
Early Stopping at epoch 27
      explained_var=0.00073 | mse_loss=0.02874
----------------------------------------------
Average early_stopping_point: 5| avg_exp_var=0.00088| avg_loss=0.02695
----------------------------------------------


----------------------------------------------
Params for Trial 4
{'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03237 | best_loss=0.03237
Epoch 1/80: current_loss=0.02989 | best_loss=0.02989
Epoch 2/80: current_loss=0.02948 | best_loss=0.02948
Epoch 3/80: current_loss=0.02963 | best_loss=0.02948
Epoch 4/80: current_loss=0.02917 | best_loss=0.02917
Epoch 5/80: current_loss=0.02917 | best_loss=0.02917
Epoch 6/80: current_loss=0.02926 | best_loss=0.02917
Epoch 7/80: current_loss=0.02910 | best_loss=0.02910
Epoch 8/80: current_loss=0.02901 | best_loss=0.02901
Epoch 9/80: current_loss=0.02894 | best_loss=0.02894
Epoch 10/80: current_loss=0.02904 | best_loss=0.02894
Epoch 11/80: current_loss=0.02906 | best_loss=0.02894
Epoch 12/80: current_loss=0.02909 | best_loss=0.02894
Epoch 13/80: current_loss=0.02996 | best_loss=0.02894
Epoch 14/80: current_loss=0.02913 | best_loss=0.02894
Epoch 15/80: current_loss=0.02943 | best_loss=0.02894
Epoch 16/80: current_loss=0.02947 | best_loss=0.02894
Epoch 17/80: current_loss=0.02903 | best_loss=0.02894
Epoch 18/80: current_loss=0.02941 | best_loss=0.02894
Epoch 19/80: current_loss=0.02937 | best_loss=0.02894
Epoch 20/80: current_loss=0.02918 | best_loss=0.02894
Epoch 21/80: current_loss=0.02921 | best_loss=0.02894
Epoch 22/80: current_loss=0.02901 | best_loss=0.02894
Epoch 23/80: current_loss=0.02905 | best_loss=0.02894
Epoch 24/80: current_loss=0.02901 | best_loss=0.02894
Epoch 25/80: current_loss=0.02922 | best_loss=0.02894
Epoch 26/80: current_loss=0.02908 | best_loss=0.02894
Epoch 27/80: current_loss=0.02906 | best_loss=0.02894
Epoch 28/80: current_loss=0.02903 | best_loss=0.02894
Epoch 29/80: current_loss=0.02981 | best_loss=0.02894
Early Stopping at epoch 29
      explained_var=0.00501 | mse_loss=0.02801
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02492 | best_loss=0.02492
Epoch 1/80: current_loss=0.02494 | best_loss=0.02492
Epoch 2/80: current_loss=0.02515 | best_loss=0.02492
Epoch 3/80: current_loss=0.02521 | best_loss=0.02492
Epoch 4/80: current_loss=0.02492 | best_loss=0.02492
Epoch 5/80: current_loss=0.02499 | best_loss=0.02492
Epoch 6/80: current_loss=0.02497 | best_loss=0.02492
Epoch 7/80: current_loss=0.02602 | best_loss=0.02492
Epoch 8/80: current_loss=0.02500 | best_loss=0.02492
Epoch 9/80: current_loss=0.02501 | best_loss=0.02492
Epoch 10/80: current_loss=0.02514 | best_loss=0.02492
Epoch 11/80: current_loss=0.02523 | best_loss=0.02492
Epoch 12/80: current_loss=0.02508 | best_loss=0.02492
Epoch 13/80: current_loss=0.02644 | best_loss=0.02492
Epoch 14/80: current_loss=0.02504 | best_loss=0.02492
Epoch 15/80: current_loss=0.02509 | best_loss=0.02492
Epoch 16/80: current_loss=0.02512 | best_loss=0.02492
Epoch 17/80: current_loss=0.02602 | best_loss=0.02492
Epoch 18/80: current_loss=0.02507 | best_loss=0.02492
Epoch 19/80: current_loss=0.02558 | best_loss=0.02492
Epoch 20/80: current_loss=0.02496 | best_loss=0.02492
Early Stopping at epoch 20
      explained_var=0.00243 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02769 | best_loss=0.02769
Epoch 1/80: current_loss=0.02787 | best_loss=0.02769
Epoch 2/80: current_loss=0.02841 | best_loss=0.02769
Epoch 3/80: current_loss=0.02761 | best_loss=0.02761
Epoch 4/80: current_loss=0.02781 | best_loss=0.02761
Epoch 5/80: current_loss=0.02777 | best_loss=0.02761
Epoch 6/80: current_loss=0.02865 | best_loss=0.02761
Epoch 7/80: current_loss=0.02769 | best_loss=0.02761
Epoch 8/80: current_loss=0.02788 | best_loss=0.02761
Epoch 9/80: current_loss=0.02808 | best_loss=0.02761
Epoch 10/80: current_loss=0.02793 | best_loss=0.02761
Epoch 11/80: current_loss=0.02763 | best_loss=0.02761
Epoch 12/80: current_loss=0.02864 | best_loss=0.02761
Epoch 13/80: current_loss=0.02769 | best_loss=0.02761
Epoch 14/80: current_loss=0.02852 | best_loss=0.02761
Epoch 15/80: current_loss=0.02819 | best_loss=0.02761
Epoch 16/80: current_loss=0.02794 | best_loss=0.02761
Epoch 17/80: current_loss=0.02765 | best_loss=0.02761
Epoch 18/80: current_loss=0.02938 | best_loss=0.02761
Epoch 19/80: current_loss=0.02768 | best_loss=0.02761
Epoch 20/80: current_loss=0.02774 | best_loss=0.02761
Epoch 21/80: current_loss=0.02798 | best_loss=0.02761
Epoch 22/80: current_loss=0.02793 | best_loss=0.02761
Epoch 23/80: current_loss=0.02792 | best_loss=0.02761
Early Stopping at epoch 23
      explained_var=-0.00271 | mse_loss=0.02800
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02678 | best_loss=0.02678
Epoch 1/80: current_loss=0.02680 | best_loss=0.02678
Epoch 2/80: current_loss=0.02679 | best_loss=0.02678
Epoch 3/80: current_loss=0.02680 | best_loss=0.02678
Epoch 4/80: current_loss=0.02703 | best_loss=0.02678
Epoch 5/80: current_loss=0.02684 | best_loss=0.02678
Epoch 6/80: current_loss=0.02678 | best_loss=0.02678
Epoch 7/80: current_loss=0.02683 | best_loss=0.02678
Epoch 8/80: current_loss=0.02683 | best_loss=0.02678
Epoch 9/80: current_loss=0.02682 | best_loss=0.02678
Epoch 10/80: current_loss=0.02679 | best_loss=0.02678
Epoch 11/80: current_loss=0.02679 | best_loss=0.02678
Epoch 12/80: current_loss=0.02698 | best_loss=0.02678
Epoch 13/80: current_loss=0.02680 | best_loss=0.02678
Epoch 14/80: current_loss=0.02680 | best_loss=0.02678
Epoch 15/80: current_loss=0.02696 | best_loss=0.02678
Epoch 16/80: current_loss=0.02706 | best_loss=0.02678
Epoch 17/80: current_loss=0.02682 | best_loss=0.02678
Epoch 18/80: current_loss=0.02687 | best_loss=0.02678
Epoch 19/80: current_loss=0.02683 | best_loss=0.02678
Epoch 20/80: current_loss=0.02681 | best_loss=0.02678
Early Stopping at epoch 20
      explained_var=0.00018 | mse_loss=0.02498
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03006 | best_loss=0.03006
Epoch 1/80: current_loss=0.02980 | best_loss=0.02980
Epoch 2/80: current_loss=0.02979 | best_loss=0.02979
Epoch 3/80: current_loss=0.02979 | best_loss=0.02979
Epoch 4/80: current_loss=0.02980 | best_loss=0.02979
Epoch 5/80: current_loss=0.02980 | best_loss=0.02979
Epoch 6/80: current_loss=0.02983 | best_loss=0.02979
Epoch 7/80: current_loss=0.02997 | best_loss=0.02979
Epoch 8/80: current_loss=0.02979 | best_loss=0.02979
Epoch 9/80: current_loss=0.02979 | best_loss=0.02979
Epoch 10/80: current_loss=0.02980 | best_loss=0.02979
Epoch 11/80: current_loss=0.02988 | best_loss=0.02979
Epoch 12/80: current_loss=0.02988 | best_loss=0.02979
Epoch 13/80: current_loss=0.02979 | best_loss=0.02979
Epoch 14/80: current_loss=0.02981 | best_loss=0.02979
Epoch 15/80: current_loss=0.02980 | best_loss=0.02979
Epoch 16/80: current_loss=0.02983 | best_loss=0.02979
Epoch 17/80: current_loss=0.02979 | best_loss=0.02979
Epoch 18/80: current_loss=0.02979 | best_loss=0.02979
Epoch 19/80: current_loss=0.02981 | best_loss=0.02979
Epoch 20/80: current_loss=0.02980 | best_loss=0.02979
Epoch 21/80: current_loss=0.02982 | best_loss=0.02979
Epoch 22/80: current_loss=0.02980 | best_loss=0.02979
Epoch 23/80: current_loss=0.02980 | best_loss=0.02979
Early Stopping at epoch 23
      explained_var=0.00009 | mse_loss=0.02875
----------------------------------------------
Average early_stopping_point: 3| avg_exp_var=0.00100| avg_loss=0.02698
----------------------------------------------


----------------------------------------------
Params for Trial 5
{'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02906 | best_loss=0.02906
Epoch 1/80: current_loss=0.02896 | best_loss=0.02896
Epoch 2/80: current_loss=0.02905 | best_loss=0.02896
Epoch 3/80: current_loss=0.02917 | best_loss=0.02896
Epoch 4/80: current_loss=0.03054 | best_loss=0.02896
Epoch 5/80: current_loss=0.02924 | best_loss=0.02896
Epoch 6/80: current_loss=0.02926 | best_loss=0.02896
Epoch 7/80: current_loss=0.02899 | best_loss=0.02896
Epoch 8/80: current_loss=0.02899 | best_loss=0.02896
Epoch 9/80: current_loss=0.02918 | best_loss=0.02896
Epoch 10/80: current_loss=0.02900 | best_loss=0.02896
Epoch 11/80: current_loss=0.02898 | best_loss=0.02896
Epoch 12/80: current_loss=0.02943 | best_loss=0.02896
Epoch 13/80: current_loss=0.02914 | best_loss=0.02896
Epoch 14/80: current_loss=0.02921 | best_loss=0.02896
Epoch 15/80: current_loss=0.02916 | best_loss=0.02896
Epoch 16/80: current_loss=0.02917 | best_loss=0.02896
Epoch 17/80: current_loss=0.02956 | best_loss=0.02896
Epoch 18/80: current_loss=0.02895 | best_loss=0.02895
Epoch 19/80: current_loss=0.02953 | best_loss=0.02895
Epoch 20/80: current_loss=0.02988 | best_loss=0.02895
Epoch 21/80: current_loss=0.02929 | best_loss=0.02895
Epoch 22/80: current_loss=0.02901 | best_loss=0.02895
Epoch 23/80: current_loss=0.02920 | best_loss=0.02895
Epoch 24/80: current_loss=0.02938 | best_loss=0.02895
Epoch 25/80: current_loss=0.02966 | best_loss=0.02895
Epoch 26/80: current_loss=0.02941 | best_loss=0.02895
Epoch 27/80: current_loss=0.02983 | best_loss=0.02895
Epoch 28/80: current_loss=0.02903 | best_loss=0.02895
Epoch 29/80: current_loss=0.02922 | best_loss=0.02895
Epoch 30/80: current_loss=0.02932 | best_loss=0.02895
Epoch 31/80: current_loss=0.02927 | best_loss=0.02895
Epoch 32/80: current_loss=0.02924 | best_loss=0.02895
Epoch 33/80: current_loss=0.02909 | best_loss=0.02895
Epoch 34/80: current_loss=0.02901 | best_loss=0.02895
Epoch 35/80: current_loss=0.02901 | best_loss=0.02895
Epoch 36/80: current_loss=0.02899 | best_loss=0.02895
Epoch 37/80: current_loss=0.02958 | best_loss=0.02895
Epoch 38/80: current_loss=0.03044 | best_loss=0.02895
Early Stopping at epoch 38
      explained_var=0.00187 | mse_loss=0.02814
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02513 | best_loss=0.02513
Epoch 1/80: current_loss=0.02501 | best_loss=0.02501
Epoch 2/80: current_loss=0.02499 | best_loss=0.02499
Epoch 3/80: current_loss=0.02506 | best_loss=0.02499
Epoch 4/80: current_loss=0.02503 | best_loss=0.02499
Epoch 5/80: current_loss=0.02503 | best_loss=0.02499
Epoch 6/80: current_loss=0.02506 | best_loss=0.02499
Epoch 7/80: current_loss=0.02561 | best_loss=0.02499
Epoch 8/80: current_loss=0.02541 | best_loss=0.02499
Epoch 9/80: current_loss=0.02499 | best_loss=0.02499
Epoch 10/80: current_loss=0.02528 | best_loss=0.02499
Epoch 11/80: current_loss=0.02496 | best_loss=0.02496
Epoch 12/80: current_loss=0.02498 | best_loss=0.02496
Epoch 13/80: current_loss=0.02498 | best_loss=0.02496
Epoch 14/80: current_loss=0.02501 | best_loss=0.02496
Epoch 15/80: current_loss=0.02510 | best_loss=0.02496
Epoch 16/80: current_loss=0.02535 | best_loss=0.02496
Epoch 17/80: current_loss=0.02499 | best_loss=0.02496
Epoch 18/80: current_loss=0.02607 | best_loss=0.02496
Epoch 19/80: current_loss=0.02537 | best_loss=0.02496
Epoch 20/80: current_loss=0.02500 | best_loss=0.02496
Epoch 21/80: current_loss=0.02605 | best_loss=0.02496
Epoch 22/80: current_loss=0.02510 | best_loss=0.02496
Epoch 23/80: current_loss=0.02607 | best_loss=0.02496
Epoch 24/80: current_loss=0.02534 | best_loss=0.02496
Epoch 25/80: current_loss=0.02503 | best_loss=0.02496
Epoch 26/80: current_loss=0.02500 | best_loss=0.02496
Epoch 27/80: current_loss=0.02542 | best_loss=0.02496
Epoch 28/80: current_loss=0.02532 | best_loss=0.02496
Epoch 29/80: current_loss=0.02521 | best_loss=0.02496
Epoch 30/80: current_loss=0.02505 | best_loss=0.02496
Epoch 31/80: current_loss=0.02502 | best_loss=0.02496
Early Stopping at epoch 31
      explained_var=0.00114 | mse_loss=0.02518
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02734 | best_loss=0.02734
Epoch 1/80: current_loss=0.02775 | best_loss=0.02734
Epoch 2/80: current_loss=0.02788 | best_loss=0.02734
Epoch 3/80: current_loss=0.02886 | best_loss=0.02734
Epoch 4/80: current_loss=0.02851 | best_loss=0.02734
Epoch 5/80: current_loss=0.02796 | best_loss=0.02734
Epoch 6/80: current_loss=0.02780 | best_loss=0.02734
Epoch 7/80: current_loss=0.02828 | best_loss=0.02734
Epoch 8/80: current_loss=0.02804 | best_loss=0.02734
Epoch 9/80: current_loss=0.02740 | best_loss=0.02734
Epoch 10/80: current_loss=0.02876 | best_loss=0.02734
Epoch 11/80: current_loss=0.02801 | best_loss=0.02734
Epoch 12/80: current_loss=0.02726 | best_loss=0.02726
Epoch 13/80: current_loss=0.02787 | best_loss=0.02726
Epoch 14/80: current_loss=0.02740 | best_loss=0.02726
Epoch 15/80: current_loss=0.02750 | best_loss=0.02726
Epoch 16/80: current_loss=0.02741 | best_loss=0.02726
Epoch 17/80: current_loss=0.02811 | best_loss=0.02726
Epoch 18/80: current_loss=0.02854 | best_loss=0.02726
Epoch 19/80: current_loss=0.02725 | best_loss=0.02725
Epoch 20/80: current_loss=0.02759 | best_loss=0.02725
Epoch 21/80: current_loss=0.02727 | best_loss=0.02725
Epoch 22/80: current_loss=0.02745 | best_loss=0.02725
Epoch 23/80: current_loss=0.02781 | best_loss=0.02725
Epoch 24/80: current_loss=0.02753 | best_loss=0.02725
Epoch 25/80: current_loss=0.02803 | best_loss=0.02725
Epoch 26/80: current_loss=0.02762 | best_loss=0.02725
Epoch 27/80: current_loss=0.02812 | best_loss=0.02725
Epoch 28/80: current_loss=0.02791 | best_loss=0.02725
Epoch 29/80: current_loss=0.02775 | best_loss=0.02725
Epoch 30/80: current_loss=0.02763 | best_loss=0.02725
Epoch 31/80: current_loss=0.02799 | best_loss=0.02725
Epoch 32/80: current_loss=0.02782 | best_loss=0.02725
Epoch 33/80: current_loss=0.02744 | best_loss=0.02725
Epoch 34/80: current_loss=0.02785 | best_loss=0.02725
Epoch 35/80: current_loss=0.02739 | best_loss=0.02725
Epoch 36/80: current_loss=0.02748 | best_loss=0.02725
Epoch 37/80: current_loss=0.02750 | best_loss=0.02725
Epoch 38/80: current_loss=0.02786 | best_loss=0.02725
Epoch 39/80: current_loss=0.02788 | best_loss=0.02725
Early Stopping at epoch 39
      explained_var=-0.00371 | mse_loss=0.02767
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02689 | best_loss=0.02689
Epoch 1/80: current_loss=0.02698 | best_loss=0.02689
Epoch 2/80: current_loss=0.02702 | best_loss=0.02689
Epoch 3/80: current_loss=0.02693 | best_loss=0.02689
Epoch 4/80: current_loss=0.02678 | best_loss=0.02678
Epoch 5/80: current_loss=0.02684 | best_loss=0.02678
Epoch 6/80: current_loss=0.02691 | best_loss=0.02678
Epoch 7/80: current_loss=0.02718 | best_loss=0.02678
Epoch 8/80: current_loss=0.02685 | best_loss=0.02678
Epoch 9/80: current_loss=0.02686 | best_loss=0.02678
Epoch 10/80: current_loss=0.02680 | best_loss=0.02678
Epoch 11/80: current_loss=0.02694 | best_loss=0.02678
Epoch 12/80: current_loss=0.02687 | best_loss=0.02678
Epoch 13/80: current_loss=0.02705 | best_loss=0.02678
Epoch 14/80: current_loss=0.02683 | best_loss=0.02678
Epoch 15/80: current_loss=0.02693 | best_loss=0.02678
Epoch 16/80: current_loss=0.02690 | best_loss=0.02678
Epoch 17/80: current_loss=0.02684 | best_loss=0.02678
Epoch 18/80: current_loss=0.02683 | best_loss=0.02678
Epoch 19/80: current_loss=0.02680 | best_loss=0.02678
Epoch 20/80: current_loss=0.02692 | best_loss=0.02678
Epoch 21/80: current_loss=0.02681 | best_loss=0.02678
Epoch 22/80: current_loss=0.02689 | best_loss=0.02678
Epoch 23/80: current_loss=0.02693 | best_loss=0.02678
Epoch 24/80: current_loss=0.02685 | best_loss=0.02678
Early Stopping at epoch 24
      explained_var=0.00073 | mse_loss=0.02497
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02979 | best_loss=0.02979
Epoch 1/80: current_loss=0.02990 | best_loss=0.02979
Epoch 2/80: current_loss=0.02976 | best_loss=0.02976
Epoch 3/80: current_loss=0.02976 | best_loss=0.02976
Epoch 4/80: current_loss=0.02977 | best_loss=0.02976
Epoch 5/80: current_loss=0.02976 | best_loss=0.02976
Epoch 6/80: current_loss=0.02977 | best_loss=0.02976
Epoch 7/80: current_loss=0.02995 | best_loss=0.02976
Epoch 8/80: current_loss=0.02976 | best_loss=0.02976
Epoch 9/80: current_loss=0.02976 | best_loss=0.02976
Epoch 10/80: current_loss=0.02979 | best_loss=0.02976
Epoch 11/80: current_loss=0.02977 | best_loss=0.02976
Epoch 12/80: current_loss=0.02980 | best_loss=0.02976
Epoch 13/80: current_loss=0.02978 | best_loss=0.02976
Epoch 14/80: current_loss=0.02978 | best_loss=0.02976
Epoch 15/80: current_loss=0.02978 | best_loss=0.02976
Epoch 16/80: current_loss=0.02978 | best_loss=0.02976
Epoch 17/80: current_loss=0.02979 | best_loss=0.02976
Epoch 18/80: current_loss=0.02980 | best_loss=0.02976
Epoch 19/80: current_loss=0.02978 | best_loss=0.02976
Epoch 20/80: current_loss=0.02978 | best_loss=0.02976
Epoch 21/80: current_loss=0.02977 | best_loss=0.02976
Epoch 22/80: current_loss=0.02978 | best_loss=0.02976
Early Stopping at epoch 22
      explained_var=0.00089 | mse_loss=0.02873
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.00019| avg_loss=0.02694
----------------------------------------------


----------------------------------------------
Params for Trial 6
{'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03280 | best_loss=0.03280
Epoch 1/80: current_loss=0.03097 | best_loss=0.03097
Epoch 2/80: current_loss=0.03038 | best_loss=0.03038
Epoch 3/80: current_loss=0.02985 | best_loss=0.02985
Epoch 4/80: current_loss=0.02969 | best_loss=0.02969
Epoch 5/80: current_loss=0.02942 | best_loss=0.02942
Epoch 6/80: current_loss=0.02936 | best_loss=0.02936
Epoch 7/80: current_loss=0.02929 | best_loss=0.02929
Epoch 8/80: current_loss=0.02951 | best_loss=0.02929
Epoch 9/80: current_loss=0.02930 | best_loss=0.02929
Epoch 10/80: current_loss=0.02923 | best_loss=0.02923
Epoch 11/80: current_loss=0.02917 | best_loss=0.02917
Epoch 12/80: current_loss=0.02916 | best_loss=0.02916
Epoch 13/80: current_loss=0.02935 | best_loss=0.02916
Epoch 14/80: current_loss=0.02912 | best_loss=0.02912
Epoch 15/80: current_loss=0.02923 | best_loss=0.02912
Epoch 16/80: current_loss=0.02928 | best_loss=0.02912
Epoch 17/80: current_loss=0.02923 | best_loss=0.02912
Epoch 18/80: current_loss=0.02947 | best_loss=0.02912
Epoch 19/80: current_loss=0.02923 | best_loss=0.02912
Epoch 20/80: current_loss=0.02932 | best_loss=0.02912
Epoch 21/80: current_loss=0.02926 | best_loss=0.02912
Epoch 22/80: current_loss=0.02937 | best_loss=0.02912
Epoch 23/80: current_loss=0.02918 | best_loss=0.02912
Epoch 24/80: current_loss=0.02924 | best_loss=0.02912
Epoch 25/80: current_loss=0.02924 | best_loss=0.02912
Epoch 26/80: current_loss=0.02964 | best_loss=0.02912
Epoch 27/80: current_loss=0.02918 | best_loss=0.02912
Epoch 28/80: current_loss=0.02920 | best_loss=0.02912
Epoch 29/80: current_loss=0.02926 | best_loss=0.02912
Epoch 30/80: current_loss=0.02914 | best_loss=0.02912
Epoch 31/80: current_loss=0.02912 | best_loss=0.02912
Epoch 32/80: current_loss=0.02925 | best_loss=0.02912
Epoch 33/80: current_loss=0.02912 | best_loss=0.02912
Epoch 34/80: current_loss=0.02919 | best_loss=0.02912
Early Stopping at epoch 34
      explained_var=0.00056 | mse_loss=0.02820
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02494 | best_loss=0.02494
Epoch 1/80: current_loss=0.02498 | best_loss=0.02494
Epoch 2/80: current_loss=0.02503 | best_loss=0.02494
Epoch 3/80: current_loss=0.02503 | best_loss=0.02494
Epoch 4/80: current_loss=0.02499 | best_loss=0.02494
Epoch 5/80: current_loss=0.02506 | best_loss=0.02494
Epoch 6/80: current_loss=0.02492 | best_loss=0.02492
Epoch 7/80: current_loss=0.02492 | best_loss=0.02492
Epoch 8/80: current_loss=0.02497 | best_loss=0.02492
Epoch 9/80: current_loss=0.02491 | best_loss=0.02491
Epoch 10/80: current_loss=0.02492 | best_loss=0.02491
Epoch 11/80: current_loss=0.02493 | best_loss=0.02491
Epoch 12/80: current_loss=0.02493 | best_loss=0.02491
Epoch 13/80: current_loss=0.02498 | best_loss=0.02491
Epoch 14/80: current_loss=0.02490 | best_loss=0.02490
Epoch 15/80: current_loss=0.02525 | best_loss=0.02490
Epoch 16/80: current_loss=0.02519 | best_loss=0.02490
Epoch 17/80: current_loss=0.02558 | best_loss=0.02490
Epoch 18/80: current_loss=0.02493 | best_loss=0.02490
Epoch 19/80: current_loss=0.02492 | best_loss=0.02490
Epoch 20/80: current_loss=0.02491 | best_loss=0.02490
Epoch 21/80: current_loss=0.02508 | best_loss=0.02490
Epoch 22/80: current_loss=0.02499 | best_loss=0.02490
Epoch 23/80: current_loss=0.02496 | best_loss=0.02490
Epoch 24/80: current_loss=0.02500 | best_loss=0.02490
Epoch 25/80: current_loss=0.02511 | best_loss=0.02490
Epoch 26/80: current_loss=0.02494 | best_loss=0.02490
Epoch 27/80: current_loss=0.02493 | best_loss=0.02490
Epoch 28/80: current_loss=0.02538 | best_loss=0.02490
Epoch 29/80: current_loss=0.02494 | best_loss=0.02490
Epoch 30/80: current_loss=0.02492 | best_loss=0.02490
Epoch 31/80: current_loss=0.02498 | best_loss=0.02490
Epoch 32/80: current_loss=0.02490 | best_loss=0.02490
Epoch 33/80: current_loss=0.02505 | best_loss=0.02490
Epoch 34/80: current_loss=0.02497 | best_loss=0.02490
Early Stopping at epoch 34
      explained_var=0.00102 | mse_loss=0.02518
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02784 | best_loss=0.02784
Epoch 1/80: current_loss=0.02800 | best_loss=0.02784
Epoch 2/80: current_loss=0.02749 | best_loss=0.02749
Epoch 3/80: current_loss=0.02757 | best_loss=0.02749
Epoch 4/80: current_loss=0.02736 | best_loss=0.02736
Epoch 5/80: current_loss=0.02847 | best_loss=0.02736
Epoch 6/80: current_loss=0.02735 | best_loss=0.02735
Epoch 7/80: current_loss=0.02786 | best_loss=0.02735
Epoch 8/80: current_loss=0.02807 | best_loss=0.02735
Epoch 9/80: current_loss=0.02751 | best_loss=0.02735
Epoch 10/80: current_loss=0.02735 | best_loss=0.02735
Epoch 11/80: current_loss=0.02752 | best_loss=0.02735
Epoch 12/80: current_loss=0.02755 | best_loss=0.02735
Epoch 13/80: current_loss=0.02764 | best_loss=0.02735
Epoch 14/80: current_loss=0.02779 | best_loss=0.02735
Epoch 15/80: current_loss=0.02782 | best_loss=0.02735
Epoch 16/80: current_loss=0.02749 | best_loss=0.02735
Epoch 17/80: current_loss=0.02784 | best_loss=0.02735
Epoch 18/80: current_loss=0.02769 | best_loss=0.02735
Epoch 19/80: current_loss=0.02808 | best_loss=0.02735
Epoch 20/80: current_loss=0.02732 | best_loss=0.02732
Epoch 21/80: current_loss=0.02823 | best_loss=0.02732
Epoch 22/80: current_loss=0.02783 | best_loss=0.02732
Epoch 23/80: current_loss=0.02841 | best_loss=0.02732
Epoch 24/80: current_loss=0.02758 | best_loss=0.02732
Epoch 25/80: current_loss=0.02728 | best_loss=0.02728
Epoch 26/80: current_loss=0.02742 | best_loss=0.02728
Epoch 27/80: current_loss=0.02755 | best_loss=0.02728
Epoch 28/80: current_loss=0.02812 | best_loss=0.02728
Epoch 29/80: current_loss=0.02771 | best_loss=0.02728
Epoch 30/80: current_loss=0.02751 | best_loss=0.02728
Epoch 31/80: current_loss=0.02744 | best_loss=0.02728
Epoch 32/80: current_loss=0.02783 | best_loss=0.02728
Epoch 33/80: current_loss=0.02755 | best_loss=0.02728
Epoch 34/80: current_loss=0.02794 | best_loss=0.02728
Epoch 35/80: current_loss=0.02743 | best_loss=0.02728
Epoch 36/80: current_loss=0.02735 | best_loss=0.02728
Epoch 37/80: current_loss=0.02760 | best_loss=0.02728
Epoch 38/80: current_loss=0.02757 | best_loss=0.02728
Epoch 39/80: current_loss=0.02760 | best_loss=0.02728
Epoch 40/80: current_loss=0.02792 | best_loss=0.02728
Epoch 41/80: current_loss=0.02775 | best_loss=0.02728
Epoch 42/80: current_loss=0.02773 | best_loss=0.02728
Epoch 43/80: current_loss=0.02774 | best_loss=0.02728
Epoch 44/80: current_loss=0.02762 | best_loss=0.02728
Epoch 45/80: current_loss=0.02747 | best_loss=0.02728
Early Stopping at epoch 45
      explained_var=-0.00504 | mse_loss=0.02772
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02676 | best_loss=0.02676
Epoch 1/80: current_loss=0.02693 | best_loss=0.02676
Epoch 2/80: current_loss=0.02693 | best_loss=0.02676
Epoch 3/80: current_loss=0.02676 | best_loss=0.02676
Epoch 4/80: current_loss=0.02676 | best_loss=0.02676
Epoch 5/80: current_loss=0.02682 | best_loss=0.02676
Epoch 6/80: current_loss=0.02678 | best_loss=0.02676
Epoch 7/80: current_loss=0.02679 | best_loss=0.02676
Epoch 8/80: current_loss=0.02686 | best_loss=0.02676
Epoch 9/80: current_loss=0.02683 | best_loss=0.02676
Epoch 10/80: current_loss=0.02679 | best_loss=0.02676
Epoch 11/80: current_loss=0.02694 | best_loss=0.02676
Epoch 12/80: current_loss=0.02692 | best_loss=0.02676
Epoch 13/80: current_loss=0.02683 | best_loss=0.02676
Epoch 14/80: current_loss=0.02680 | best_loss=0.02676
Epoch 15/80: current_loss=0.02685 | best_loss=0.02676
Epoch 16/80: current_loss=0.02681 | best_loss=0.02676
Epoch 17/80: current_loss=0.02700 | best_loss=0.02676
Epoch 18/80: current_loss=0.02686 | best_loss=0.02676
Epoch 19/80: current_loss=0.02707 | best_loss=0.02676
Epoch 20/80: current_loss=0.02678 | best_loss=0.02676
Early Stopping at epoch 20
      explained_var=0.00182 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02972 | best_loss=0.02972
Epoch 1/80: current_loss=0.02971 | best_loss=0.02971
Epoch 2/80: current_loss=0.02991 | best_loss=0.02971
Epoch 3/80: current_loss=0.02976 | best_loss=0.02971
Epoch 4/80: current_loss=0.02974 | best_loss=0.02971
Epoch 5/80: current_loss=0.02978 | best_loss=0.02971
Epoch 6/80: current_loss=0.02975 | best_loss=0.02971
Epoch 7/80: current_loss=0.02978 | best_loss=0.02971
Epoch 8/80: current_loss=0.02977 | best_loss=0.02971
Epoch 9/80: current_loss=0.02973 | best_loss=0.02971
Epoch 10/80: current_loss=0.02976 | best_loss=0.02971
Epoch 11/80: current_loss=0.02971 | best_loss=0.02971
Epoch 12/80: current_loss=0.02976 | best_loss=0.02971
Epoch 13/80: current_loss=0.02991 | best_loss=0.02971
Epoch 14/80: current_loss=0.02968 | best_loss=0.02968
Epoch 15/80: current_loss=0.02976 | best_loss=0.02968
Epoch 16/80: current_loss=0.02972 | best_loss=0.02968
Epoch 17/80: current_loss=0.02972 | best_loss=0.02968
Epoch 18/80: current_loss=0.02970 | best_loss=0.02968
Epoch 19/80: current_loss=0.02969 | best_loss=0.02968
Epoch 20/80: current_loss=0.02978 | best_loss=0.02968
Epoch 21/80: current_loss=0.02970 | best_loss=0.02968
Epoch 22/80: current_loss=0.02986 | best_loss=0.02968
Epoch 23/80: current_loss=0.02970 | best_loss=0.02968
Epoch 24/80: current_loss=0.02978 | best_loss=0.02968
Epoch 25/80: current_loss=0.02970 | best_loss=0.02968
Epoch 26/80: current_loss=0.02969 | best_loss=0.02968
Epoch 27/80: current_loss=0.02975 | best_loss=0.02968
Epoch 28/80: current_loss=0.02975 | best_loss=0.02968
Epoch 29/80: current_loss=0.02971 | best_loss=0.02968
Epoch 30/80: current_loss=0.02973 | best_loss=0.02968
Epoch 31/80: current_loss=0.02972 | best_loss=0.02968
Epoch 32/80: current_loss=0.02972 | best_loss=0.02968
Epoch 33/80: current_loss=0.02972 | best_loss=0.02968
Epoch 34/80: current_loss=0.02975 | best_loss=0.02968
Early Stopping at epoch 34
      explained_var=0.00217 | mse_loss=0.02871
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.00011| avg_loss=0.02695
----------------------------------------------


----------------------------------------------
Params for Trial 7
{'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.58948 | best_loss=0.58948
Epoch 1/80: current_loss=0.15130 | best_loss=0.15130
Epoch 2/80: current_loss=0.60882 | best_loss=0.15130
Epoch 3/80: current_loss=0.03748 | best_loss=0.03748
Epoch 4/80: current_loss=0.45312 | best_loss=0.03748
Epoch 5/80: current_loss=0.04041 | best_loss=0.03748
Epoch 6/80: current_loss=0.02958 | best_loss=0.02958
Epoch 7/80: current_loss=0.03874 | best_loss=0.02958
Epoch 8/80: current_loss=0.03193 | best_loss=0.02958
Epoch 9/80: current_loss=0.04094 | best_loss=0.02958
Epoch 10/80: current_loss=0.05452 | best_loss=0.02958
Epoch 11/80: current_loss=0.02934 | best_loss=0.02934
Epoch 12/80: current_loss=0.04741 | best_loss=0.02934
Epoch 13/80: current_loss=0.03040 | best_loss=0.02934
Epoch 14/80: current_loss=0.03038 | best_loss=0.02934
Epoch 15/80: current_loss=0.02913 | best_loss=0.02913
Epoch 16/80: current_loss=0.03447 | best_loss=0.02913
Epoch 17/80: current_loss=0.02915 | best_loss=0.02913
Epoch 18/80: current_loss=0.03324 | best_loss=0.02913
Epoch 19/80: current_loss=0.03194 | best_loss=0.02913
Epoch 20/80: current_loss=0.03229 | best_loss=0.02913
Epoch 21/80: current_loss=0.03109 | best_loss=0.02913
Epoch 22/80: current_loss=0.03007 | best_loss=0.02913
Epoch 23/80: current_loss=0.03015 | best_loss=0.02913
Epoch 24/80: current_loss=0.02901 | best_loss=0.02901
Epoch 25/80: current_loss=0.02974 | best_loss=0.02901
Epoch 26/80: current_loss=0.02901 | best_loss=0.02901
Epoch 27/80: current_loss=0.02914 | best_loss=0.02901
Epoch 28/80: current_loss=0.02909 | best_loss=0.02901
Epoch 29/80: current_loss=0.02941 | best_loss=0.02901
Epoch 30/80: current_loss=0.02913 | best_loss=0.02901
Epoch 31/80: current_loss=0.02919 | best_loss=0.02901
Epoch 32/80: current_loss=0.02904 | best_loss=0.02901
Epoch 33/80: current_loss=0.02917 | best_loss=0.02901
Epoch 34/80: current_loss=0.02910 | best_loss=0.02901
Epoch 35/80: current_loss=0.02920 | best_loss=0.02901
Epoch 36/80: current_loss=0.02912 | best_loss=0.02901
Epoch 37/80: current_loss=0.02910 | best_loss=0.02901
Epoch 38/80: current_loss=0.02914 | best_loss=0.02901
Epoch 39/80: current_loss=0.02942 | best_loss=0.02901
Epoch 40/80: current_loss=0.02931 | best_loss=0.02901
Epoch 41/80: current_loss=0.02913 | best_loss=0.02901
Epoch 42/80: current_loss=0.02916 | best_loss=0.02901
Epoch 43/80: current_loss=0.02914 | best_loss=0.02901
Epoch 44/80: current_loss=0.02935 | best_loss=0.02901
Early Stopping at epoch 44
      explained_var=0.00167 | mse_loss=0.02820
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=1.99407 | best_loss=1.99407
Epoch 1/80: current_loss=0.52193 | best_loss=0.52193
Epoch 2/80: current_loss=0.28995 | best_loss=0.28995
Epoch 3/80: current_loss=0.06660 | best_loss=0.06660
Epoch 4/80: current_loss=0.03024 | best_loss=0.03024
Epoch 5/80: current_loss=0.02956 | best_loss=0.02956
Epoch 6/80: current_loss=0.10437 | best_loss=0.02956
Epoch 7/80: current_loss=0.10424 | best_loss=0.02956
Epoch 8/80: current_loss=0.26476 | best_loss=0.02956
Epoch 9/80: current_loss=6.87686 | best_loss=0.02956
Epoch 10/80: current_loss=1.00715 | best_loss=0.02956
Epoch 11/80: current_loss=0.20988 | best_loss=0.02956
Epoch 12/80: current_loss=0.71006 | best_loss=0.02956
Epoch 13/80: current_loss=1.97323 | best_loss=0.02956
Epoch 14/80: current_loss=0.23743 | best_loss=0.02956
Epoch 15/80: current_loss=0.10039 | best_loss=0.02956
Epoch 16/80: current_loss=0.11458 | best_loss=0.02956
Epoch 17/80: current_loss=0.06555 | best_loss=0.02956
Epoch 18/80: current_loss=0.06480 | best_loss=0.02956
Epoch 19/80: current_loss=0.06382 | best_loss=0.02956
Epoch 20/80: current_loss=0.07788 | best_loss=0.02956
Epoch 21/80: current_loss=0.05618 | best_loss=0.02956
Epoch 22/80: current_loss=0.06456 | best_loss=0.02956
Epoch 23/80: current_loss=0.03874 | best_loss=0.02956
Epoch 24/80: current_loss=0.93844 | best_loss=0.02956
Epoch 25/80: current_loss=11.01752 | best_loss=0.02956
Early Stopping at epoch 25
      explained_var=-0.18107 | mse_loss=0.02977
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=12.27152 | best_loss=12.27152
Epoch 1/80: current_loss=0.61416 | best_loss=0.61416
Epoch 2/80: current_loss=0.24731 | best_loss=0.24731
Epoch 3/80: current_loss=2.56055 | best_loss=0.24731
Epoch 4/80: current_loss=0.21349 | best_loss=0.21349
Epoch 5/80: current_loss=0.09494 | best_loss=0.09494
Epoch 6/80: current_loss=0.05566 | best_loss=0.05566
Epoch 7/80: current_loss=0.07491 | best_loss=0.05566
Epoch 8/80: current_loss=0.04563 | best_loss=0.04563
Epoch 9/80: current_loss=0.10023 | best_loss=0.04563
Epoch 10/80: current_loss=0.15356 | best_loss=0.04563
Epoch 11/80: current_loss=0.06500 | best_loss=0.04563
Epoch 12/80: current_loss=0.07831 | best_loss=0.04563
Epoch 13/80: current_loss=0.04572 | best_loss=0.04563
Epoch 14/80: current_loss=0.05643 | best_loss=0.04563
Epoch 15/80: current_loss=0.56320 | best_loss=0.04563
Epoch 16/80: current_loss=0.18974 | best_loss=0.04563
Epoch 17/80: current_loss=4.60125 | best_loss=0.04563
Epoch 18/80: current_loss=4.97899 | best_loss=0.04563
Epoch 19/80: current_loss=0.10099 | best_loss=0.04563
Epoch 20/80: current_loss=0.69129 | best_loss=0.04563
Epoch 21/80: current_loss=0.88076 | best_loss=0.04563
Epoch 22/80: current_loss=1.68974 | best_loss=0.04563
Epoch 23/80: current_loss=1.58394 | best_loss=0.04563
Epoch 24/80: current_loss=2.57635 | best_loss=0.04563
Epoch 25/80: current_loss=9.58820 | best_loss=0.04563
Epoch 26/80: current_loss=2.13013 | best_loss=0.04563
Epoch 27/80: current_loss=0.11393 | best_loss=0.04563
Epoch 28/80: current_loss=0.14907 | best_loss=0.04563
Early Stopping at epoch 28
      explained_var=-0.49280 | mse_loss=0.04593
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=6.12563 | best_loss=6.12563
Epoch 1/80: current_loss=0.49326 | best_loss=0.49326
Epoch 2/80: current_loss=0.20384 | best_loss=0.20384
Epoch 3/80: current_loss=0.93372 | best_loss=0.20384
Epoch 4/80: current_loss=0.02955 | best_loss=0.02955
Epoch 5/80: current_loss=0.04230 | best_loss=0.02955
Epoch 6/80: current_loss=0.05175 | best_loss=0.02955
Epoch 7/80: current_loss=0.22565 | best_loss=0.02955
Epoch 8/80: current_loss=0.11480 | best_loss=0.02955
Epoch 9/80: current_loss=0.10160 | best_loss=0.02955
Epoch 10/80: current_loss=0.04469 | best_loss=0.02955
Epoch 11/80: current_loss=0.05006 | best_loss=0.02955
Epoch 12/80: current_loss=0.08574 | best_loss=0.02955
Epoch 13/80: current_loss=0.08113 | best_loss=0.02955
Epoch 14/80: current_loss=0.08316 | best_loss=0.02955
Epoch 15/80: current_loss=0.03488 | best_loss=0.02955
Epoch 16/80: current_loss=0.05732 | best_loss=0.02955
Epoch 17/80: current_loss=0.19354 | best_loss=0.02955
Epoch 18/80: current_loss=0.32971 | best_loss=0.02955
Epoch 19/80: current_loss=60.26683 | best_loss=0.02955
Epoch 20/80: current_loss=0.98295 | best_loss=0.02955
Epoch 21/80: current_loss=12.40325 | best_loss=0.02955
Epoch 22/80: current_loss=0.04723 | best_loss=0.02955
Epoch 23/80: current_loss=0.08685 | best_loss=0.02955
Epoch 24/80: current_loss=0.04574 | best_loss=0.02955
Early Stopping at epoch 24
      explained_var=-0.01332 | mse_loss=0.02771
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=1.20533 | best_loss=1.20533
Epoch 1/80: current_loss=1.33176 | best_loss=1.20533
Epoch 2/80: current_loss=0.31122 | best_loss=0.31122
Epoch 3/80: current_loss=0.04832 | best_loss=0.04832
Epoch 4/80: current_loss=0.06383 | best_loss=0.04832
Epoch 5/80: current_loss=0.09571 | best_loss=0.04832
Epoch 6/80: current_loss=0.06664 | best_loss=0.04832
Epoch 7/80: current_loss=1.19086 | best_loss=0.04832
Epoch 8/80: current_loss=0.38149 | best_loss=0.04832
Epoch 9/80: current_loss=1.46817 | best_loss=0.04832
Epoch 10/80: current_loss=21.43110 | best_loss=0.04832
Epoch 11/80: current_loss=1.13189 | best_loss=0.04832
Epoch 12/80: current_loss=1.68936 | best_loss=0.04832
Epoch 13/80: current_loss=0.10141 | best_loss=0.04832
Epoch 14/80: current_loss=0.20036 | best_loss=0.04832
Epoch 15/80: current_loss=0.06346 | best_loss=0.04832
Epoch 16/80: current_loss=0.05110 | best_loss=0.04832
Epoch 17/80: current_loss=0.11040 | best_loss=0.04832
Epoch 18/80: current_loss=0.08906 | best_loss=0.04832
Epoch 19/80: current_loss=0.03134 | best_loss=0.03134
Epoch 20/80: current_loss=0.03050 | best_loss=0.03050
Epoch 21/80: current_loss=0.03094 | best_loss=0.03050
Epoch 22/80: current_loss=0.03102 | best_loss=0.03050
Epoch 23/80: current_loss=0.03075 | best_loss=0.03050
Epoch 24/80: current_loss=0.03064 | best_loss=0.03050
Epoch 25/80: current_loss=0.03000 | best_loss=0.03000
Epoch 26/80: current_loss=0.02981 | best_loss=0.02981
Epoch 27/80: current_loss=0.03061 | best_loss=0.02981
Epoch 28/80: current_loss=0.03065 | best_loss=0.02981
Epoch 29/80: current_loss=0.02983 | best_loss=0.02981
Epoch 30/80: current_loss=0.02978 | best_loss=0.02978
Epoch 31/80: current_loss=0.03026 | best_loss=0.02978
Epoch 32/80: current_loss=0.03087 | best_loss=0.02978
Epoch 33/80: current_loss=0.03081 | best_loss=0.02978
Epoch 34/80: current_loss=0.02973 | best_loss=0.02973
Epoch 35/80: current_loss=0.02980 | best_loss=0.02973
Epoch 36/80: current_loss=0.03109 | best_loss=0.02973
Epoch 37/80: current_loss=0.03045 | best_loss=0.02973
Epoch 38/80: current_loss=0.03006 | best_loss=0.02973
Epoch 39/80: current_loss=0.02975 | best_loss=0.02973
Epoch 40/80: current_loss=0.03015 | best_loss=0.02973
Epoch 41/80: current_loss=0.02988 | best_loss=0.02973
Epoch 42/80: current_loss=0.03001 | best_loss=0.02973
Epoch 43/80: current_loss=0.02974 | best_loss=0.02973
Epoch 44/80: current_loss=0.02976 | best_loss=0.02973
Epoch 45/80: current_loss=0.02981 | best_loss=0.02973
Epoch 46/80: current_loss=0.02981 | best_loss=0.02973
Epoch 47/80: current_loss=0.03004 | best_loss=0.02973
Epoch 48/80: current_loss=0.02977 | best_loss=0.02973
Epoch 49/80: current_loss=0.02982 | best_loss=0.02973
Epoch 50/80: current_loss=0.02975 | best_loss=0.02973
Epoch 51/80: current_loss=0.02996 | best_loss=0.02973
Epoch 52/80: current_loss=0.02978 | best_loss=0.02973
Epoch 53/80: current_loss=0.02980 | best_loss=0.02973
Epoch 54/80: current_loss=0.02981 | best_loss=0.02973
Early Stopping at epoch 54
      explained_var=0.00217 | mse_loss=0.02872
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=-0.13667| avg_loss=0.03207
----------------------------------------------


----------------------------------------------
Params for Trial 8
{'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.14008 | best_loss=0.14008
Epoch 1/80: current_loss=0.12408 | best_loss=0.12408
Epoch 2/80: current_loss=0.10969 | best_loss=0.10969
Epoch 3/80: current_loss=0.09695 | best_loss=0.09695
Epoch 4/80: current_loss=0.08558 | best_loss=0.08558
Epoch 5/80: current_loss=0.07547 | best_loss=0.07547
Epoch 6/80: current_loss=0.06674 | best_loss=0.06674
Epoch 7/80: current_loss=0.05913 | best_loss=0.05913
Epoch 8/80: current_loss=0.05279 | best_loss=0.05279
Epoch 9/80: current_loss=0.04756 | best_loss=0.04756
Epoch 10/80: current_loss=0.04346 | best_loss=0.04346
Epoch 11/80: current_loss=0.04030 | best_loss=0.04030
Epoch 12/80: current_loss=0.03797 | best_loss=0.03797
Epoch 13/80: current_loss=0.03632 | best_loss=0.03632
Epoch 14/80: current_loss=0.03512 | best_loss=0.03512
Epoch 15/80: current_loss=0.03441 | best_loss=0.03441
Epoch 16/80: current_loss=0.03394 | best_loss=0.03394
Epoch 17/80: current_loss=0.03354 | best_loss=0.03354
Epoch 18/80: current_loss=0.03329 | best_loss=0.03329
Epoch 19/80: current_loss=0.03313 | best_loss=0.03313
Epoch 20/80: current_loss=0.03297 | best_loss=0.03297
Epoch 21/80: current_loss=0.03285 | best_loss=0.03285
Epoch 22/80: current_loss=0.03275 | best_loss=0.03275
Epoch 23/80: current_loss=0.03266 | best_loss=0.03266
Epoch 24/80: current_loss=0.03256 | best_loss=0.03256
Epoch 25/80: current_loss=0.03248 | best_loss=0.03248
Epoch 26/80: current_loss=0.03241 | best_loss=0.03241
Epoch 27/80: current_loss=0.03233 | best_loss=0.03233
Epoch 28/80: current_loss=0.03225 | best_loss=0.03225
Epoch 29/80: current_loss=0.03219 | best_loss=0.03219
Epoch 30/80: current_loss=0.03211 | best_loss=0.03211
Epoch 31/80: current_loss=0.03205 | best_loss=0.03205
Epoch 32/80: current_loss=0.03198 | best_loss=0.03198
Epoch 33/80: current_loss=0.03191 | best_loss=0.03191
Epoch 34/80: current_loss=0.03184 | best_loss=0.03184
Epoch 35/80: current_loss=0.03178 | best_loss=0.03178
Epoch 36/80: current_loss=0.03172 | best_loss=0.03172
Epoch 37/80: current_loss=0.03166 | best_loss=0.03166
Epoch 38/80: current_loss=0.03159 | best_loss=0.03159
Epoch 39/80: current_loss=0.03154 | best_loss=0.03154
Epoch 40/80: current_loss=0.03147 | best_loss=0.03147
Epoch 41/80: current_loss=0.03141 | best_loss=0.03141
Epoch 42/80: current_loss=0.03135 | best_loss=0.03135
Epoch 43/80: current_loss=0.03130 | best_loss=0.03130
Epoch 44/80: current_loss=0.03124 | best_loss=0.03124
Epoch 45/80: current_loss=0.03119 | best_loss=0.03119
Epoch 46/80: current_loss=0.03113 | best_loss=0.03113
Epoch 47/80: current_loss=0.03107 | best_loss=0.03107
Epoch 48/80: current_loss=0.03102 | best_loss=0.03102
Epoch 49/80: current_loss=0.03096 | best_loss=0.03096
Epoch 50/80: current_loss=0.03091 | best_loss=0.03091
Epoch 51/80: current_loss=0.03087 | best_loss=0.03087
Epoch 52/80: current_loss=0.03082 | best_loss=0.03082
Epoch 53/80: current_loss=0.03078 | best_loss=0.03078
Epoch 54/80: current_loss=0.03073 | best_loss=0.03073
Epoch 55/80: current_loss=0.03068 | best_loss=0.03068
Epoch 56/80: current_loss=0.03064 | best_loss=0.03064
Epoch 57/80: current_loss=0.03059 | best_loss=0.03059
Epoch 58/80: current_loss=0.03055 | best_loss=0.03055
Epoch 59/80: current_loss=0.03051 | best_loss=0.03051
Epoch 60/80: current_loss=0.03047 | best_loss=0.03047
Epoch 61/80: current_loss=0.03044 | best_loss=0.03044
Epoch 62/80: current_loss=0.03040 | best_loss=0.03040
Epoch 63/80: current_loss=0.03036 | best_loss=0.03036
Epoch 64/80: current_loss=0.03033 | best_loss=0.03033
Epoch 65/80: current_loss=0.03029 | best_loss=0.03029
Epoch 66/80: current_loss=0.03025 | best_loss=0.03025
Epoch 67/80: current_loss=0.03022 | best_loss=0.03022
Epoch 68/80: current_loss=0.03019 | best_loss=0.03019
Epoch 69/80: current_loss=0.03015 | best_loss=0.03015
Epoch 70/80: current_loss=0.03011 | best_loss=0.03011
Epoch 71/80: current_loss=0.03008 | best_loss=0.03008
Epoch 72/80: current_loss=0.03006 | best_loss=0.03006
Epoch 73/80: current_loss=0.03002 | best_loss=0.03002
Epoch 74/80: current_loss=0.02999 | best_loss=0.02999
Epoch 75/80: current_loss=0.02996 | best_loss=0.02996
Epoch 76/80: current_loss=0.02994 | best_loss=0.02994
Epoch 77/80: current_loss=0.02991 | best_loss=0.02991
Epoch 78/80: current_loss=0.02988 | best_loss=0.02988
Epoch 79/80: current_loss=0.02985 | best_loss=0.02985
      explained_var=-0.01439 | mse_loss=0.02855
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02602 | best_loss=0.02602
Epoch 1/80: current_loss=0.02599 | best_loss=0.02599
Epoch 2/80: current_loss=0.02600 | best_loss=0.02599
Epoch 3/80: current_loss=0.02599 | best_loss=0.02599
Epoch 4/80: current_loss=0.02591 | best_loss=0.02591
Epoch 5/80: current_loss=0.02589 | best_loss=0.02589
Epoch 6/80: current_loss=0.02586 | best_loss=0.02586
Epoch 7/80: current_loss=0.02581 | best_loss=0.02581
Epoch 8/80: current_loss=0.02582 | best_loss=0.02581
Epoch 9/80: current_loss=0.02581 | best_loss=0.02581
Epoch 10/80: current_loss=0.02580 | best_loss=0.02580
Epoch 11/80: current_loss=0.02579 | best_loss=0.02579
Epoch 12/80: current_loss=0.02576 | best_loss=0.02576
Epoch 13/80: current_loss=0.02578 | best_loss=0.02576
Epoch 14/80: current_loss=0.02574 | best_loss=0.02574
Epoch 15/80: current_loss=0.02572 | best_loss=0.02572
Epoch 16/80: current_loss=0.02566 | best_loss=0.02566
Epoch 17/80: current_loss=0.02566 | best_loss=0.02566
Epoch 18/80: current_loss=0.02565 | best_loss=0.02565
Epoch 19/80: current_loss=0.02565 | best_loss=0.02565
Epoch 20/80: current_loss=0.02567 | best_loss=0.02565
Epoch 21/80: current_loss=0.02563 | best_loss=0.02563
Epoch 22/80: current_loss=0.02561 | best_loss=0.02561
Epoch 23/80: current_loss=0.02558 | best_loss=0.02558
Epoch 24/80: current_loss=0.02557 | best_loss=0.02557
Epoch 25/80: current_loss=0.02554 | best_loss=0.02554
Epoch 26/80: current_loss=0.02557 | best_loss=0.02554
Epoch 27/80: current_loss=0.02556 | best_loss=0.02554
Epoch 28/80: current_loss=0.02554 | best_loss=0.02554
Epoch 29/80: current_loss=0.02553 | best_loss=0.02553
Epoch 30/80: current_loss=0.02549 | best_loss=0.02549
Epoch 31/80: current_loss=0.02548 | best_loss=0.02548
Epoch 32/80: current_loss=0.02549 | best_loss=0.02548
Epoch 33/80: current_loss=0.02550 | best_loss=0.02548
Epoch 34/80: current_loss=0.02545 | best_loss=0.02545
Epoch 35/80: current_loss=0.02542 | best_loss=0.02542
Epoch 36/80: current_loss=0.02542 | best_loss=0.02542
Epoch 37/80: current_loss=0.02539 | best_loss=0.02539
Epoch 38/80: current_loss=0.02539 | best_loss=0.02539
Epoch 39/80: current_loss=0.02537 | best_loss=0.02537
Epoch 40/80: current_loss=0.02535 | best_loss=0.02535
Epoch 41/80: current_loss=0.02537 | best_loss=0.02535
Epoch 42/80: current_loss=0.02536 | best_loss=0.02535
Epoch 43/80: current_loss=0.02535 | best_loss=0.02535
Epoch 44/80: current_loss=0.02538 | best_loss=0.02535
Epoch 45/80: current_loss=0.02531 | best_loss=0.02531
Epoch 46/80: current_loss=0.02536 | best_loss=0.02531
Epoch 47/80: current_loss=0.02534 | best_loss=0.02531
Epoch 48/80: current_loss=0.02533 | best_loss=0.02531
Epoch 49/80: current_loss=0.02537 | best_loss=0.02531
Epoch 50/80: current_loss=0.02536 | best_loss=0.02531
Epoch 51/80: current_loss=0.02535 | best_loss=0.02531
Epoch 52/80: current_loss=0.02534 | best_loss=0.02531
Epoch 53/80: current_loss=0.02532 | best_loss=0.02531
Epoch 54/80: current_loss=0.02531 | best_loss=0.02531
Epoch 55/80: current_loss=0.02529 | best_loss=0.02529
Epoch 56/80: current_loss=0.02526 | best_loss=0.02526
Epoch 57/80: current_loss=0.02524 | best_loss=0.02524
Epoch 58/80: current_loss=0.02523 | best_loss=0.02523
Epoch 59/80: current_loss=0.02527 | best_loss=0.02523
Epoch 60/80: current_loss=0.02523 | best_loss=0.02523
Epoch 61/80: current_loss=0.02523 | best_loss=0.02523
Epoch 62/80: current_loss=0.02524 | best_loss=0.02523
Epoch 63/80: current_loss=0.02521 | best_loss=0.02521
Epoch 64/80: current_loss=0.02517 | best_loss=0.02517
Epoch 65/80: current_loss=0.02515 | best_loss=0.02515
Epoch 66/80: current_loss=0.02517 | best_loss=0.02515
Epoch 67/80: current_loss=0.02518 | best_loss=0.02515
Epoch 68/80: current_loss=0.02523 | best_loss=0.02515
Epoch 69/80: current_loss=0.02521 | best_loss=0.02515
Epoch 70/80: current_loss=0.02522 | best_loss=0.02515
Epoch 71/80: current_loss=0.02520 | best_loss=0.02515
Epoch 72/80: current_loss=0.02518 | best_loss=0.02515
Epoch 73/80: current_loss=0.02519 | best_loss=0.02515
Epoch 74/80: current_loss=0.02518 | best_loss=0.02515
Epoch 75/80: current_loss=0.02518 | best_loss=0.02515
Epoch 76/80: current_loss=0.02518 | best_loss=0.02515
Epoch 77/80: current_loss=0.02519 | best_loss=0.02515
Epoch 78/80: current_loss=0.02514 | best_loss=0.02514
Epoch 79/80: current_loss=0.02517 | best_loss=0.02514
      explained_var=-0.00380 | mse_loss=0.02538
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02892 | best_loss=0.02892
Epoch 1/80: current_loss=0.02900 | best_loss=0.02892
Epoch 2/80: current_loss=0.02898 | best_loss=0.02892
Epoch 3/80: current_loss=0.02898 | best_loss=0.02892
Epoch 4/80: current_loss=0.02896 | best_loss=0.02892
Epoch 5/80: current_loss=0.02901 | best_loss=0.02892
Epoch 6/80: current_loss=0.02902 | best_loss=0.02892
Epoch 7/80: current_loss=0.02898 | best_loss=0.02892
Epoch 8/80: current_loss=0.02891 | best_loss=0.02891
Epoch 9/80: current_loss=0.02895 | best_loss=0.02891
Epoch 10/80: current_loss=0.02892 | best_loss=0.02891
Epoch 11/80: current_loss=0.02898 | best_loss=0.02891
Epoch 12/80: current_loss=0.02895 | best_loss=0.02891
Epoch 13/80: current_loss=0.02891 | best_loss=0.02891
Epoch 14/80: current_loss=0.02891 | best_loss=0.02891
Epoch 15/80: current_loss=0.02889 | best_loss=0.02889
Epoch 16/80: current_loss=0.02888 | best_loss=0.02888
Epoch 17/80: current_loss=0.02888 | best_loss=0.02888
Epoch 18/80: current_loss=0.02884 | best_loss=0.02884
Epoch 19/80: current_loss=0.02885 | best_loss=0.02884
Epoch 20/80: current_loss=0.02894 | best_loss=0.02884
Epoch 21/80: current_loss=0.02891 | best_loss=0.02884
Epoch 22/80: current_loss=0.02879 | best_loss=0.02879
Epoch 23/80: current_loss=0.02885 | best_loss=0.02879
Epoch 24/80: current_loss=0.02890 | best_loss=0.02879
Epoch 25/80: current_loss=0.02884 | best_loss=0.02879
Epoch 26/80: current_loss=0.02891 | best_loss=0.02879
Epoch 27/80: current_loss=0.02883 | best_loss=0.02879
Epoch 28/80: current_loss=0.02878 | best_loss=0.02878
Epoch 29/80: current_loss=0.02883 | best_loss=0.02878
Epoch 30/80: current_loss=0.02882 | best_loss=0.02878
Epoch 31/80: current_loss=0.02871 | best_loss=0.02871
Epoch 32/80: current_loss=0.02876 | best_loss=0.02871
Epoch 33/80: current_loss=0.02881 | best_loss=0.02871
Epoch 34/80: current_loss=0.02881 | best_loss=0.02871
Epoch 35/80: current_loss=0.02874 | best_loss=0.02871
Epoch 36/80: current_loss=0.02873 | best_loss=0.02871
Epoch 37/80: current_loss=0.02870 | best_loss=0.02870
Epoch 38/80: current_loss=0.02869 | best_loss=0.02869
Epoch 39/80: current_loss=0.02871 | best_loss=0.02869
Epoch 40/80: current_loss=0.02871 | best_loss=0.02869
Epoch 41/80: current_loss=0.02868 | best_loss=0.02868
Epoch 42/80: current_loss=0.02869 | best_loss=0.02868
Epoch 43/80: current_loss=0.02871 | best_loss=0.02868
Epoch 44/80: current_loss=0.02873 | best_loss=0.02868
Epoch 45/80: current_loss=0.02875 | best_loss=0.02868
Epoch 46/80: current_loss=0.02867 | best_loss=0.02867
Epoch 47/80: current_loss=0.02866 | best_loss=0.02866
Epoch 48/80: current_loss=0.02871 | best_loss=0.02866
Epoch 49/80: current_loss=0.02866 | best_loss=0.02866
Epoch 50/80: current_loss=0.02859 | best_loss=0.02859
Epoch 51/80: current_loss=0.02867 | best_loss=0.02859
Epoch 52/80: current_loss=0.02868 | best_loss=0.02859
Epoch 53/80: current_loss=0.02872 | best_loss=0.02859
Epoch 54/80: current_loss=0.02862 | best_loss=0.02859
Epoch 55/80: current_loss=0.02866 | best_loss=0.02859
Epoch 56/80: current_loss=0.02858 | best_loss=0.02858
Epoch 57/80: current_loss=0.02860 | best_loss=0.02858
Epoch 58/80: current_loss=0.02864 | best_loss=0.02858
Epoch 59/80: current_loss=0.02867 | best_loss=0.02858
Epoch 60/80: current_loss=0.02861 | best_loss=0.02858
Epoch 61/80: current_loss=0.02864 | best_loss=0.02858
Epoch 62/80: current_loss=0.02866 | best_loss=0.02858
Epoch 63/80: current_loss=0.02864 | best_loss=0.02858
Epoch 64/80: current_loss=0.02864 | best_loss=0.02858
Epoch 65/80: current_loss=0.02863 | best_loss=0.02858
Epoch 66/80: current_loss=0.02862 | best_loss=0.02858
Epoch 67/80: current_loss=0.02860 | best_loss=0.02858
Epoch 68/80: current_loss=0.02861 | best_loss=0.02858
Epoch 69/80: current_loss=0.02850 | best_loss=0.02850
Epoch 70/80: current_loss=0.02854 | best_loss=0.02850
Epoch 71/80: current_loss=0.02849 | best_loss=0.02849
Epoch 72/80: current_loss=0.02854 | best_loss=0.02849
Epoch 73/80: current_loss=0.02855 | best_loss=0.02849
Epoch 74/80: current_loss=0.02851 | best_loss=0.02849
Epoch 75/80: current_loss=0.02849 | best_loss=0.02849
Epoch 76/80: current_loss=0.02845 | best_loss=0.02845
Epoch 77/80: current_loss=0.02846 | best_loss=0.02845
Epoch 78/80: current_loss=0.02848 | best_loss=0.02845
Epoch 79/80: current_loss=0.02854 | best_loss=0.02845
      explained_var=-0.03193 | mse_loss=0.02908
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02657 | best_loss=0.02657
Epoch 1/80: current_loss=0.02658 | best_loss=0.02657
Epoch 2/80: current_loss=0.02657 | best_loss=0.02657
Epoch 3/80: current_loss=0.02657 | best_loss=0.02657
Epoch 4/80: current_loss=0.02658 | best_loss=0.02657
Epoch 5/80: current_loss=0.02657 | best_loss=0.02657
Epoch 6/80: current_loss=0.02658 | best_loss=0.02657
Epoch 7/80: current_loss=0.02658 | best_loss=0.02657
Epoch 8/80: current_loss=0.02658 | best_loss=0.02657
Epoch 9/80: current_loss=0.02658 | best_loss=0.02657
Epoch 10/80: current_loss=0.02658 | best_loss=0.02657
Epoch 11/80: current_loss=0.02657 | best_loss=0.02657
Epoch 12/80: current_loss=0.02658 | best_loss=0.02657
Epoch 13/80: current_loss=0.02658 | best_loss=0.02657
Epoch 14/80: current_loss=0.02658 | best_loss=0.02657
Epoch 15/80: current_loss=0.02658 | best_loss=0.02657
Epoch 16/80: current_loss=0.02658 | best_loss=0.02657
Epoch 17/80: current_loss=0.02658 | best_loss=0.02657
Epoch 18/80: current_loss=0.02658 | best_loss=0.02657
Epoch 19/80: current_loss=0.02658 | best_loss=0.02657
Epoch 20/80: current_loss=0.02658 | best_loss=0.02657
Epoch 21/80: current_loss=0.02658 | best_loss=0.02657
Epoch 22/80: current_loss=0.02659 | best_loss=0.02657
Epoch 23/80: current_loss=0.02658 | best_loss=0.02657
Early Stopping at epoch 23
      explained_var=0.00164 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03114 | best_loss=0.03114
Epoch 1/80: current_loss=0.03051 | best_loss=0.03051
Epoch 2/80: current_loss=0.03029 | best_loss=0.03029
Epoch 3/80: current_loss=0.03013 | best_loss=0.03013
Epoch 4/80: current_loss=0.03007 | best_loss=0.03007
Epoch 5/80: current_loss=0.03003 | best_loss=0.03003
Epoch 6/80: current_loss=0.03003 | best_loss=0.03003
Epoch 7/80: current_loss=0.03002 | best_loss=0.03002
Epoch 8/80: current_loss=0.03002 | best_loss=0.03002
Epoch 9/80: current_loss=0.03000 | best_loss=0.03000
Epoch 10/80: current_loss=0.03000 | best_loss=0.03000
Epoch 11/80: current_loss=0.02999 | best_loss=0.02999
Epoch 12/80: current_loss=0.02998 | best_loss=0.02998
Epoch 13/80: current_loss=0.02997 | best_loss=0.02997
Epoch 14/80: current_loss=0.02999 | best_loss=0.02997
Epoch 15/80: current_loss=0.03000 | best_loss=0.02997
Epoch 16/80: current_loss=0.02999 | best_loss=0.02997
Epoch 17/80: current_loss=0.02996 | best_loss=0.02996
Epoch 18/80: current_loss=0.02997 | best_loss=0.02996
Epoch 19/80: current_loss=0.02995 | best_loss=0.02995
Epoch 20/80: current_loss=0.02998 | best_loss=0.02995
Epoch 21/80: current_loss=0.02997 | best_loss=0.02995
Epoch 22/80: current_loss=0.02995 | best_loss=0.02995
Epoch 23/80: current_loss=0.02996 | best_loss=0.02995
Epoch 24/80: current_loss=0.02995 | best_loss=0.02995
Epoch 25/80: current_loss=0.02995 | best_loss=0.02995
Epoch 26/80: current_loss=0.02996 | best_loss=0.02995
Epoch 27/80: current_loss=0.02996 | best_loss=0.02995
Epoch 28/80: current_loss=0.02997 | best_loss=0.02995
Epoch 29/80: current_loss=0.02995 | best_loss=0.02995
Epoch 30/80: current_loss=0.02994 | best_loss=0.02994
Epoch 31/80: current_loss=0.02993 | best_loss=0.02993
Epoch 32/80: current_loss=0.02994 | best_loss=0.02993
Epoch 33/80: current_loss=0.02996 | best_loss=0.02993
Epoch 34/80: current_loss=0.02994 | best_loss=0.02993
Epoch 35/80: current_loss=0.02992 | best_loss=0.02992
Epoch 36/80: current_loss=0.02993 | best_loss=0.02992
Epoch 37/80: current_loss=0.02993 | best_loss=0.02992
Epoch 38/80: current_loss=0.02992 | best_loss=0.02992
Epoch 39/80: current_loss=0.02992 | best_loss=0.02992
Epoch 40/80: current_loss=0.02990 | best_loss=0.02990
Epoch 41/80: current_loss=0.02991 | best_loss=0.02990
Epoch 42/80: current_loss=0.02990 | best_loss=0.02990
Epoch 43/80: current_loss=0.02989 | best_loss=0.02989
Epoch 44/80: current_loss=0.02989 | best_loss=0.02989
Epoch 45/80: current_loss=0.02990 | best_loss=0.02989
Epoch 46/80: current_loss=0.02989 | best_loss=0.02989
Epoch 47/80: current_loss=0.02990 | best_loss=0.02989
Epoch 48/80: current_loss=0.02990 | best_loss=0.02989
Epoch 49/80: current_loss=0.02989 | best_loss=0.02989
Epoch 50/80: current_loss=0.02989 | best_loss=0.02989
Epoch 51/80: current_loss=0.02990 | best_loss=0.02989
Epoch 52/80: current_loss=0.02989 | best_loss=0.02989
Epoch 53/80: current_loss=0.02988 | best_loss=0.02988
Epoch 54/80: current_loss=0.02989 | best_loss=0.02988
Epoch 55/80: current_loss=0.02989 | best_loss=0.02988
Epoch 56/80: current_loss=0.02988 | best_loss=0.02988
Epoch 57/80: current_loss=0.02989 | best_loss=0.02988
Epoch 58/80: current_loss=0.02989 | best_loss=0.02988
Epoch 59/80: current_loss=0.02989 | best_loss=0.02988
Epoch 60/80: current_loss=0.02988 | best_loss=0.02988
Epoch 61/80: current_loss=0.02988 | best_loss=0.02988
Epoch 62/80: current_loss=0.02988 | best_loss=0.02988
Epoch 63/80: current_loss=0.02987 | best_loss=0.02987
Epoch 64/80: current_loss=0.02988 | best_loss=0.02987
Epoch 65/80: current_loss=0.02987 | best_loss=0.02987
Epoch 66/80: current_loss=0.02988 | best_loss=0.02987
Epoch 67/80: current_loss=0.02986 | best_loss=0.02986
Epoch 68/80: current_loss=0.02985 | best_loss=0.02985
Epoch 69/80: current_loss=0.02986 | best_loss=0.02985
Epoch 70/80: current_loss=0.02985 | best_loss=0.02985
Epoch 71/80: current_loss=0.02985 | best_loss=0.02985
Epoch 72/80: current_loss=0.02986 | best_loss=0.02985
Epoch 73/80: current_loss=0.02985 | best_loss=0.02985
Epoch 74/80: current_loss=0.02985 | best_loss=0.02985
Epoch 75/80: current_loss=0.02985 | best_loss=0.02985
Epoch 76/80: current_loss=0.02986 | best_loss=0.02985
Epoch 77/80: current_loss=0.02985 | best_loss=0.02985
Epoch 78/80: current_loss=0.02984 | best_loss=0.02984
Epoch 79/80: current_loss=0.02984 | best_loss=0.02984
      explained_var=-0.00189 | mse_loss=0.02884
----------------------------------------------
Average early_stopping_point: 64| avg_exp_var=-0.01007| avg_loss=0.02736
----------------------------------------------


----------------------------------------------
Params for Trial 9
{'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=1.26281 | best_loss=1.26281
Epoch 1/80: current_loss=10.07283 | best_loss=1.26281
Epoch 2/80: current_loss=0.08870 | best_loss=0.08870
Epoch 3/80: current_loss=0.24244 | best_loss=0.08870
Epoch 4/80: current_loss=0.04924 | best_loss=0.04924
Epoch 5/80: current_loss=0.03393 | best_loss=0.03393
Epoch 6/80: current_loss=0.04991 | best_loss=0.03393
Epoch 7/80: current_loss=0.13586 | best_loss=0.03393
Epoch 8/80: current_loss=0.02985 | best_loss=0.02985
Epoch 9/80: current_loss=0.02964 | best_loss=0.02964
Epoch 10/80: current_loss=0.03512 | best_loss=0.02964
Epoch 11/80: current_loss=0.04086 | best_loss=0.02964
Epoch 12/80: current_loss=0.04234 | best_loss=0.02964
Epoch 13/80: current_loss=0.03139 | best_loss=0.02964
Epoch 14/80: current_loss=0.02909 | best_loss=0.02909
Epoch 15/80: current_loss=0.02903 | best_loss=0.02903
Epoch 16/80: current_loss=0.03339 | best_loss=0.02903
Epoch 17/80: current_loss=0.02907 | best_loss=0.02903
Epoch 18/80: current_loss=0.02896 | best_loss=0.02896
Epoch 19/80: current_loss=0.02893 | best_loss=0.02893
Epoch 20/80: current_loss=0.02889 | best_loss=0.02889
Epoch 21/80: current_loss=0.02945 | best_loss=0.02889
Epoch 22/80: current_loss=0.02981 | best_loss=0.02889
Epoch 23/80: current_loss=0.02986 | best_loss=0.02889
Epoch 24/80: current_loss=0.02921 | best_loss=0.02889
Epoch 25/80: current_loss=0.02908 | best_loss=0.02889
Epoch 26/80: current_loss=0.02915 | best_loss=0.02889
Epoch 27/80: current_loss=0.02924 | best_loss=0.02889
Epoch 28/80: current_loss=0.02911 | best_loss=0.02889
Epoch 29/80: current_loss=0.02911 | best_loss=0.02889
Epoch 30/80: current_loss=0.02918 | best_loss=0.02889
Epoch 31/80: current_loss=0.02916 | best_loss=0.02889
Epoch 32/80: current_loss=0.02920 | best_loss=0.02889
Epoch 33/80: current_loss=0.02912 | best_loss=0.02889
Epoch 34/80: current_loss=0.02923 | best_loss=0.02889
Epoch 35/80: current_loss=0.02911 | best_loss=0.02889
Epoch 36/80: current_loss=0.02912 | best_loss=0.02889
Epoch 37/80: current_loss=0.02914 | best_loss=0.02889
Epoch 38/80: current_loss=0.02928 | best_loss=0.02889
Epoch 39/80: current_loss=0.02927 | best_loss=0.02889
Epoch 40/80: current_loss=0.02957 | best_loss=0.02889
Early Stopping at epoch 40
      explained_var=0.00253 | mse_loss=0.02810
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.94375 | best_loss=0.94375
Epoch 1/80: current_loss=0.43516 | best_loss=0.43516
Epoch 2/80: current_loss=0.36586 | best_loss=0.36586
Epoch 3/80: current_loss=0.22982 | best_loss=0.22982
Epoch 4/80: current_loss=0.08873 | best_loss=0.08873
Epoch 5/80: current_loss=0.02575 | best_loss=0.02575
Epoch 6/80: current_loss=0.02716 | best_loss=0.02575
Epoch 7/80: current_loss=0.02741 | best_loss=0.02575
Epoch 8/80: current_loss=0.02582 | best_loss=0.02575
Epoch 9/80: current_loss=0.02523 | best_loss=0.02523
Epoch 10/80: current_loss=0.02511 | best_loss=0.02511
Epoch 11/80: current_loss=0.02501 | best_loss=0.02501
Epoch 12/80: current_loss=0.02504 | best_loss=0.02501
Epoch 13/80: current_loss=0.02502 | best_loss=0.02501
Epoch 14/80: current_loss=0.02512 | best_loss=0.02501
Epoch 15/80: current_loss=0.02503 | best_loss=0.02501
Epoch 16/80: current_loss=0.02512 | best_loss=0.02501
Epoch 17/80: current_loss=0.02505 | best_loss=0.02501
Epoch 18/80: current_loss=0.02504 | best_loss=0.02501
Epoch 19/80: current_loss=0.02503 | best_loss=0.02501
Epoch 20/80: current_loss=0.02521 | best_loss=0.02501
Epoch 21/80: current_loss=0.02503 | best_loss=0.02501
Epoch 22/80: current_loss=0.02526 | best_loss=0.02501
Epoch 23/80: current_loss=0.02502 | best_loss=0.02501
Epoch 24/80: current_loss=0.02504 | best_loss=0.02501
Epoch 25/80: current_loss=0.02507 | best_loss=0.02501
Epoch 26/80: current_loss=0.02509 | best_loss=0.02501
Epoch 27/80: current_loss=0.02502 | best_loss=0.02501
Epoch 28/80: current_loss=0.02502 | best_loss=0.02501
Epoch 29/80: current_loss=0.02507 | best_loss=0.02501
Epoch 30/80: current_loss=0.02528 | best_loss=0.02501
Epoch 31/80: current_loss=0.02504 | best_loss=0.02501
Early Stopping at epoch 31
      explained_var=0.00028 | mse_loss=0.02519
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.80191 | best_loss=0.80191
Epoch 1/80: current_loss=1.57206 | best_loss=0.80191
Epoch 2/80: current_loss=0.21297 | best_loss=0.21297
Epoch 3/80: current_loss=0.31899 | best_loss=0.21297
Epoch 4/80: current_loss=0.08285 | best_loss=0.08285
Epoch 5/80: current_loss=0.04313 | best_loss=0.04313
Epoch 6/80: current_loss=0.02869 | best_loss=0.02869
Epoch 7/80: current_loss=0.02863 | best_loss=0.02863
Epoch 8/80: current_loss=0.02896 | best_loss=0.02863
Epoch 9/80: current_loss=0.03917 | best_loss=0.02863
Epoch 10/80: current_loss=0.03026 | best_loss=0.02863
Epoch 11/80: current_loss=0.02772 | best_loss=0.02772
Epoch 12/80: current_loss=0.02800 | best_loss=0.02772
Epoch 13/80: current_loss=0.03321 | best_loss=0.02772
Epoch 14/80: current_loss=0.02811 | best_loss=0.02772
Epoch 15/80: current_loss=0.02864 | best_loss=0.02772
Epoch 16/80: current_loss=0.02799 | best_loss=0.02772
Epoch 17/80: current_loss=0.03454 | best_loss=0.02772
Epoch 18/80: current_loss=0.02985 | best_loss=0.02772
Epoch 19/80: current_loss=0.02924 | best_loss=0.02772
Epoch 20/80: current_loss=0.02831 | best_loss=0.02772
Epoch 21/80: current_loss=0.02852 | best_loss=0.02772
Epoch 22/80: current_loss=0.02788 | best_loss=0.02772
Epoch 23/80: current_loss=0.02971 | best_loss=0.02772
Epoch 24/80: current_loss=0.02825 | best_loss=0.02772
Epoch 25/80: current_loss=0.02744 | best_loss=0.02744
Epoch 26/80: current_loss=0.02799 | best_loss=0.02744
Epoch 27/80: current_loss=0.02737 | best_loss=0.02737
Epoch 28/80: current_loss=0.02728 | best_loss=0.02728
Epoch 29/80: current_loss=0.02810 | best_loss=0.02728
Epoch 30/80: current_loss=0.02781 | best_loss=0.02728
Epoch 31/80: current_loss=0.02761 | best_loss=0.02728
Epoch 32/80: current_loss=0.02796 | best_loss=0.02728
Epoch 33/80: current_loss=0.02761 | best_loss=0.02728
Epoch 34/80: current_loss=0.02724 | best_loss=0.02724
Epoch 35/80: current_loss=0.02725 | best_loss=0.02724
Epoch 36/80: current_loss=0.02727 | best_loss=0.02724
Epoch 37/80: current_loss=0.02743 | best_loss=0.02724
Epoch 38/80: current_loss=0.02723 | best_loss=0.02723
Epoch 39/80: current_loss=0.02724 | best_loss=0.02723
Epoch 40/80: current_loss=0.02775 | best_loss=0.02723
Epoch 41/80: current_loss=0.02803 | best_loss=0.02723
Epoch 42/80: current_loss=0.02751 | best_loss=0.02723
Epoch 43/80: current_loss=0.02766 | best_loss=0.02723
Epoch 44/80: current_loss=0.02747 | best_loss=0.02723
Epoch 45/80: current_loss=0.02747 | best_loss=0.02723
Epoch 46/80: current_loss=0.02750 | best_loss=0.02723
Epoch 47/80: current_loss=0.02794 | best_loss=0.02723
Epoch 48/80: current_loss=0.02728 | best_loss=0.02723
Epoch 49/80: current_loss=0.02815 | best_loss=0.02723
Epoch 50/80: current_loss=0.02749 | best_loss=0.02723
Epoch 51/80: current_loss=0.02724 | best_loss=0.02723
Epoch 52/80: current_loss=0.02744 | best_loss=0.02723
Epoch 53/80: current_loss=0.02784 | best_loss=0.02723
Epoch 54/80: current_loss=0.02761 | best_loss=0.02723
Epoch 55/80: current_loss=0.02741 | best_loss=0.02723
Epoch 56/80: current_loss=0.02800 | best_loss=0.02723
Epoch 57/80: current_loss=0.02802 | best_loss=0.02723
Epoch 58/80: current_loss=0.02777 | best_loss=0.02723
Early Stopping at epoch 58
      explained_var=-0.00143 | mse_loss=0.02761
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.29538 | best_loss=0.29538
Epoch 1/80: current_loss=0.45165 | best_loss=0.29538
Epoch 2/80: current_loss=0.10781 | best_loss=0.10781
Epoch 3/80: current_loss=0.78971 | best_loss=0.10781
Epoch 4/80: current_loss=0.07235 | best_loss=0.07235
Epoch 5/80: current_loss=2.04407 | best_loss=0.07235
Epoch 6/80: current_loss=1.11309 | best_loss=0.07235
Epoch 7/80: current_loss=0.57290 | best_loss=0.07235
Epoch 8/80: current_loss=4.49977 | best_loss=0.07235
Epoch 9/80: current_loss=0.10282 | best_loss=0.07235
Epoch 10/80: current_loss=0.80037 | best_loss=0.07235
Epoch 11/80: current_loss=0.04239 | best_loss=0.04239
Epoch 12/80: current_loss=0.12802 | best_loss=0.04239
Epoch 13/80: current_loss=0.03288 | best_loss=0.03288
Epoch 14/80: current_loss=4.30848 | best_loss=0.03288
Epoch 15/80: current_loss=0.26192 | best_loss=0.03288
Epoch 16/80: current_loss=1.58888 | best_loss=0.03288
Epoch 17/80: current_loss=0.08355 | best_loss=0.03288
Epoch 18/80: current_loss=0.27867 | best_loss=0.03288
Epoch 19/80: current_loss=2.89260 | best_loss=0.03288
Epoch 20/80: current_loss=0.68229 | best_loss=0.03288
Epoch 21/80: current_loss=12.84157 | best_loss=0.03288
Epoch 22/80: current_loss=12.75834 | best_loss=0.03288
Epoch 23/80: current_loss=1.31581 | best_loss=0.03288
Epoch 24/80: current_loss=0.62146 | best_loss=0.03288
Epoch 25/80: current_loss=0.07257 | best_loss=0.03288
Epoch 26/80: current_loss=0.05715 | best_loss=0.03288
Epoch 27/80: current_loss=0.05121 | best_loss=0.03288
Epoch 28/80: current_loss=0.04488 | best_loss=0.03288
Epoch 29/80: current_loss=0.04510 | best_loss=0.03288
Epoch 30/80: current_loss=0.02825 | best_loss=0.02825
Epoch 31/80: current_loss=0.06131 | best_loss=0.02825
Epoch 32/80: current_loss=0.03788 | best_loss=0.02825
Epoch 33/80: current_loss=0.03469 | best_loss=0.02825
Epoch 34/80: current_loss=0.04623 | best_loss=0.02825
Epoch 35/80: current_loss=0.03092 | best_loss=0.02825
Epoch 36/80: current_loss=0.03593 | best_loss=0.02825
Epoch 37/80: current_loss=0.02789 | best_loss=0.02789
Epoch 38/80: current_loss=0.03576 | best_loss=0.02789
Epoch 39/80: current_loss=0.03716 | best_loss=0.02789
Epoch 40/80: current_loss=0.03952 | best_loss=0.02789
Epoch 41/80: current_loss=0.02853 | best_loss=0.02789
Epoch 42/80: current_loss=0.03024 | best_loss=0.02789
Epoch 43/80: current_loss=0.03093 | best_loss=0.02789
Epoch 44/80: current_loss=0.04173 | best_loss=0.02789
Epoch 45/80: current_loss=0.04062 | best_loss=0.02789
Epoch 46/80: current_loss=0.05210 | best_loss=0.02789
Epoch 47/80: current_loss=0.03510 | best_loss=0.02789
Epoch 48/80: current_loss=0.04890 | best_loss=0.02789
Epoch 49/80: current_loss=0.03864 | best_loss=0.02789
Epoch 50/80: current_loss=0.02783 | best_loss=0.02783
Epoch 51/80: current_loss=0.04753 | best_loss=0.02783
Epoch 52/80: current_loss=0.05375 | best_loss=0.02783
Epoch 53/80: current_loss=0.03464 | best_loss=0.02783
Epoch 54/80: current_loss=0.03238 | best_loss=0.02783
Epoch 55/80: current_loss=0.06157 | best_loss=0.02783
Epoch 56/80: current_loss=0.03219 | best_loss=0.02783
Epoch 57/80: current_loss=0.06425 | best_loss=0.02783
Epoch 58/80: current_loss=0.03622 | best_loss=0.02783
Epoch 59/80: current_loss=0.08410 | best_loss=0.02783
Epoch 60/80: current_loss=0.03452 | best_loss=0.02783
Epoch 61/80: current_loss=0.09218 | best_loss=0.02783
Epoch 62/80: current_loss=0.05857 | best_loss=0.02783
Epoch 63/80: current_loss=0.09790 | best_loss=0.02783
Epoch 64/80: current_loss=0.13540 | best_loss=0.02783
Epoch 65/80: current_loss=1.62450 | best_loss=0.02783
Epoch 66/80: current_loss=51.55991 | best_loss=0.02783
Epoch 67/80: current_loss=3.22137 | best_loss=0.02783
Epoch 68/80: current_loss=11.40290 | best_loss=0.02783
Epoch 69/80: current_loss=0.66075 | best_loss=0.02783
Epoch 70/80: current_loss=0.06654 | best_loss=0.02783
Early Stopping at epoch 70
      explained_var=-0.04059 | mse_loss=0.02638
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=3.29479 | best_loss=3.29479
Epoch 1/80: current_loss=1.18641 | best_loss=1.18641
Epoch 2/80: current_loss=0.53579 | best_loss=0.53579
Epoch 3/80: current_loss=0.13477 | best_loss=0.13477
Epoch 4/80: current_loss=0.06116 | best_loss=0.06116
Epoch 5/80: current_loss=0.09482 | best_loss=0.06116
Epoch 6/80: current_loss=0.03435 | best_loss=0.03435
Epoch 7/80: current_loss=0.05943 | best_loss=0.03435
Epoch 8/80: current_loss=0.04629 | best_loss=0.03435
Epoch 9/80: current_loss=0.05421 | best_loss=0.03435
Epoch 10/80: current_loss=0.04675 | best_loss=0.03435
Epoch 11/80: current_loss=0.03506 | best_loss=0.03435
Epoch 12/80: current_loss=0.05406 | best_loss=0.03435
Epoch 13/80: current_loss=0.05207 | best_loss=0.03435
Epoch 14/80: current_loss=0.05842 | best_loss=0.03435
Epoch 15/80: current_loss=0.04367 | best_loss=0.03435
Epoch 16/80: current_loss=0.03453 | best_loss=0.03435
Epoch 17/80: current_loss=0.06398 | best_loss=0.03435
Epoch 18/80: current_loss=0.08053 | best_loss=0.03435
Epoch 19/80: current_loss=18.70640 | best_loss=0.03435
Epoch 20/80: current_loss=19.71752 | best_loss=0.03435
Epoch 21/80: current_loss=45.00685 | best_loss=0.03435
Epoch 22/80: current_loss=6.92165 | best_loss=0.03435
Epoch 23/80: current_loss=2.17353 | best_loss=0.03435
Epoch 24/80: current_loss=0.94337 | best_loss=0.03435
Epoch 25/80: current_loss=0.09717 | best_loss=0.03435
Epoch 26/80: current_loss=0.08826 | best_loss=0.03435
Early Stopping at epoch 26
      explained_var=-0.15048 | mse_loss=0.03318
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=-0.03794| avg_loss=0.02810
----------------------------------------------


----------------------------------------------
Params for Trial 10
{'learning_rate': 0.01, 'weight_decay': 0.0030741327184301173, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.13559 | best_loss=0.13559
Epoch 1/80: current_loss=0.04062 | best_loss=0.04062
Epoch 2/80: current_loss=0.04231 | best_loss=0.04062
Epoch 3/80: current_loss=0.04571 | best_loss=0.04062
Epoch 4/80: current_loss=0.03554 | best_loss=0.03554
Epoch 5/80: current_loss=0.04910 | best_loss=0.03554
Epoch 6/80: current_loss=0.04043 | best_loss=0.03554
Epoch 7/80: current_loss=0.06148 | best_loss=0.03554
Epoch 8/80: current_loss=0.03277 | best_loss=0.03277
Epoch 9/80: current_loss=0.03048 | best_loss=0.03048
Epoch 10/80: current_loss=0.03344 | best_loss=0.03048
Epoch 11/80: current_loss=0.03160 | best_loss=0.03048
Epoch 12/80: current_loss=0.05624 | best_loss=0.03048
Epoch 13/80: current_loss=0.03099 | best_loss=0.03048
Epoch 14/80: current_loss=0.05330 | best_loss=0.03048
Epoch 15/80: current_loss=0.03089 | best_loss=0.03048
Epoch 16/80: current_loss=0.03939 | best_loss=0.03048
Epoch 17/80: current_loss=0.03002 | best_loss=0.03002
Epoch 18/80: current_loss=0.02897 | best_loss=0.02897
Epoch 19/80: current_loss=0.02912 | best_loss=0.02897
Epoch 20/80: current_loss=0.02911 | best_loss=0.02897
Epoch 21/80: current_loss=0.02944 | best_loss=0.02897
Epoch 22/80: current_loss=0.02914 | best_loss=0.02897
Epoch 23/80: current_loss=0.02913 | best_loss=0.02897
Epoch 24/80: current_loss=0.02917 | best_loss=0.02897
Epoch 25/80: current_loss=0.02924 | best_loss=0.02897
Epoch 26/80: current_loss=0.02913 | best_loss=0.02897
Epoch 27/80: current_loss=0.02926 | best_loss=0.02897
Epoch 28/80: current_loss=0.02940 | best_loss=0.02897
Epoch 29/80: current_loss=0.02915 | best_loss=0.02897
Epoch 30/80: current_loss=0.02937 | best_loss=0.02897
Epoch 31/80: current_loss=0.02921 | best_loss=0.02897
Epoch 32/80: current_loss=0.02934 | best_loss=0.02897
Epoch 33/80: current_loss=0.02912 | best_loss=0.02897
Epoch 34/80: current_loss=0.02932 | best_loss=0.02897
Epoch 35/80: current_loss=0.02936 | best_loss=0.02897
Epoch 36/80: current_loss=0.02936 | best_loss=0.02897
Epoch 37/80: current_loss=0.02912 | best_loss=0.02897
Epoch 38/80: current_loss=0.02913 | best_loss=0.02897
Early Stopping at epoch 38
      explained_var=0.00418 | mse_loss=0.02811
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03352 | best_loss=0.03352
Epoch 1/80: current_loss=0.03806 | best_loss=0.03352
Epoch 2/80: current_loss=0.02790 | best_loss=0.02790
Epoch 3/80: current_loss=0.03083 | best_loss=0.02790
Epoch 4/80: current_loss=0.02549 | best_loss=0.02549
Epoch 5/80: current_loss=0.02507 | best_loss=0.02507
Epoch 6/80: current_loss=0.02503 | best_loss=0.02503
Epoch 7/80: current_loss=0.02508 | best_loss=0.02503
Epoch 8/80: current_loss=0.02502 | best_loss=0.02502
Epoch 9/80: current_loss=0.02502 | best_loss=0.02502
Epoch 10/80: current_loss=0.02520 | best_loss=0.02502
Epoch 11/80: current_loss=0.02502 | best_loss=0.02502
Epoch 12/80: current_loss=0.02503 | best_loss=0.02502
Epoch 13/80: current_loss=0.02503 | best_loss=0.02502
Epoch 14/80: current_loss=0.02504 | best_loss=0.02502
Epoch 15/80: current_loss=0.02502 | best_loss=0.02502
Epoch 16/80: current_loss=0.02503 | best_loss=0.02502
Epoch 17/80: current_loss=0.02515 | best_loss=0.02502
Epoch 18/80: current_loss=0.02502 | best_loss=0.02502
Epoch 19/80: current_loss=0.02508 | best_loss=0.02502
Epoch 20/80: current_loss=0.02502 | best_loss=0.02502
Epoch 21/80: current_loss=0.02510 | best_loss=0.02502
Epoch 22/80: current_loss=0.02508 | best_loss=0.02502
Epoch 23/80: current_loss=0.02513 | best_loss=0.02502
Epoch 24/80: current_loss=0.02504 | best_loss=0.02502
Epoch 25/80: current_loss=0.02503 | best_loss=0.02502
Epoch 26/80: current_loss=0.02504 | best_loss=0.02502
Epoch 27/80: current_loss=0.02502 | best_loss=0.02502
Epoch 28/80: current_loss=0.02513 | best_loss=0.02502
Epoch 29/80: current_loss=0.02519 | best_loss=0.02502
Epoch 30/80: current_loss=0.02502 | best_loss=0.02502
Epoch 31/80: current_loss=0.02502 | best_loss=0.02502
Epoch 32/80: current_loss=0.02504 | best_loss=0.02502
Epoch 33/80: current_loss=0.02517 | best_loss=0.02502
Epoch 34/80: current_loss=0.02511 | best_loss=0.02502
Epoch 35/80: current_loss=0.02532 | best_loss=0.02502
Epoch 36/80: current_loss=0.02505 | best_loss=0.02502
Epoch 37/80: current_loss=0.02504 | best_loss=0.02502
Epoch 38/80: current_loss=0.02502 | best_loss=0.02502
Epoch 39/80: current_loss=0.02518 | best_loss=0.02502
Epoch 40/80: current_loss=0.02502 | best_loss=0.02502
Epoch 41/80: current_loss=0.02508 | best_loss=0.02502
Epoch 42/80: current_loss=0.02506 | best_loss=0.02502
Epoch 43/80: current_loss=0.02516 | best_loss=0.02502
Epoch 44/80: current_loss=0.02503 | best_loss=0.02502
Epoch 45/80: current_loss=0.02505 | best_loss=0.02502
Epoch 46/80: current_loss=0.02503 | best_loss=0.02502
Epoch 47/80: current_loss=0.02524 | best_loss=0.02502
Epoch 48/80: current_loss=0.02530 | best_loss=0.02502
Epoch 49/80: current_loss=0.02537 | best_loss=0.02502
Epoch 50/80: current_loss=0.02510 | best_loss=0.02502
Early Stopping at epoch 50
      explained_var=0.00000 | mse_loss=0.02520
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04866 | best_loss=0.04866
Epoch 1/80: current_loss=0.03369 | best_loss=0.03369
Epoch 2/80: current_loss=0.07944 | best_loss=0.03369
Epoch 3/80: current_loss=0.03149 | best_loss=0.03149
Epoch 4/80: current_loss=0.03961 | best_loss=0.03149
Epoch 5/80: current_loss=0.03061 | best_loss=0.03061
Epoch 6/80: current_loss=0.02977 | best_loss=0.02977
Epoch 7/80: current_loss=0.02721 | best_loss=0.02721
Epoch 8/80: current_loss=0.02772 | best_loss=0.02721
Epoch 9/80: current_loss=0.02731 | best_loss=0.02721
Epoch 10/80: current_loss=0.02776 | best_loss=0.02721
Epoch 11/80: current_loss=0.02734 | best_loss=0.02721
Epoch 12/80: current_loss=0.02746 | best_loss=0.02721
Epoch 13/80: current_loss=0.02719 | best_loss=0.02719
Epoch 14/80: current_loss=0.02762 | best_loss=0.02719
Epoch 15/80: current_loss=0.02734 | best_loss=0.02719
Epoch 16/80: current_loss=0.02753 | best_loss=0.02719
Epoch 17/80: current_loss=0.02783 | best_loss=0.02719
Epoch 18/80: current_loss=0.02759 | best_loss=0.02719
Epoch 19/80: current_loss=0.02806 | best_loss=0.02719
Epoch 20/80: current_loss=0.02746 | best_loss=0.02719
Epoch 21/80: current_loss=0.02764 | best_loss=0.02719
Epoch 22/80: current_loss=0.02768 | best_loss=0.02719
Epoch 23/80: current_loss=0.02739 | best_loss=0.02719
Epoch 24/80: current_loss=0.02749 | best_loss=0.02719
Epoch 25/80: current_loss=0.02752 | best_loss=0.02719
Epoch 26/80: current_loss=0.02845 | best_loss=0.02719
Epoch 27/80: current_loss=0.02734 | best_loss=0.02719
Epoch 28/80: current_loss=0.02798 | best_loss=0.02719
Epoch 29/80: current_loss=0.02720 | best_loss=0.02719
Epoch 30/80: current_loss=0.02774 | best_loss=0.02719
Epoch 31/80: current_loss=0.02810 | best_loss=0.02719
Epoch 32/80: current_loss=0.02798 | best_loss=0.02719
Epoch 33/80: current_loss=0.02782 | best_loss=0.02719
Early Stopping at epoch 33
      explained_var=-0.00021 | mse_loss=0.02756
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04243 | best_loss=0.04243
Epoch 1/80: current_loss=0.05324 | best_loss=0.04243
Epoch 2/80: current_loss=0.02868 | best_loss=0.02868
Epoch 3/80: current_loss=0.02829 | best_loss=0.02829
Epoch 4/80: current_loss=0.03083 | best_loss=0.02829
Epoch 5/80: current_loss=0.02689 | best_loss=0.02689
Epoch 6/80: current_loss=0.02685 | best_loss=0.02685
Epoch 7/80: current_loss=0.02682 | best_loss=0.02682
Epoch 8/80: current_loss=0.02697 | best_loss=0.02682
Epoch 9/80: current_loss=0.02715 | best_loss=0.02682
Epoch 10/80: current_loss=0.02685 | best_loss=0.02682
Epoch 11/80: current_loss=0.02703 | best_loss=0.02682
Epoch 12/80: current_loss=0.02679 | best_loss=0.02679
Epoch 13/80: current_loss=0.02679 | best_loss=0.02679
Epoch 14/80: current_loss=0.02767 | best_loss=0.02679
Epoch 15/80: current_loss=0.02722 | best_loss=0.02679
Epoch 16/80: current_loss=0.02706 | best_loss=0.02679
Epoch 17/80: current_loss=0.02678 | best_loss=0.02678
Epoch 18/80: current_loss=0.02678 | best_loss=0.02678
Epoch 19/80: current_loss=0.02741 | best_loss=0.02678
Epoch 20/80: current_loss=0.02696 | best_loss=0.02678
Epoch 21/80: current_loss=0.02701 | best_loss=0.02678
Epoch 22/80: current_loss=0.02683 | best_loss=0.02678
Epoch 23/80: current_loss=0.02704 | best_loss=0.02678
Epoch 24/80: current_loss=0.02743 | best_loss=0.02678
Epoch 25/80: current_loss=0.02700 | best_loss=0.02678
Epoch 26/80: current_loss=0.02701 | best_loss=0.02678
Epoch 27/80: current_loss=0.02691 | best_loss=0.02678
Epoch 28/80: current_loss=0.02702 | best_loss=0.02678
Epoch 29/80: current_loss=0.02691 | best_loss=0.02678
Epoch 30/80: current_loss=0.02686 | best_loss=0.02678
Epoch 31/80: current_loss=0.02723 | best_loss=0.02678
Epoch 32/80: current_loss=0.02689 | best_loss=0.02678
Epoch 33/80: current_loss=0.02720 | best_loss=0.02678
Epoch 34/80: current_loss=0.02707 | best_loss=0.02678
Epoch 35/80: current_loss=0.02689 | best_loss=0.02678
Epoch 36/80: current_loss=0.02679 | best_loss=0.02678
Epoch 37/80: current_loss=0.02709 | best_loss=0.02678
Epoch 38/80: current_loss=0.02691 | best_loss=0.02678
Early Stopping at epoch 38
      explained_var=-0.00000 | mse_loss=0.02499
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.30146 | best_loss=0.30146
Epoch 1/80: current_loss=0.22665 | best_loss=0.22665
Epoch 2/80: current_loss=0.03088 | best_loss=0.03088
Epoch 3/80: current_loss=0.11069 | best_loss=0.03088
Epoch 4/80: current_loss=0.04063 | best_loss=0.03088
Epoch 5/80: current_loss=0.03087 | best_loss=0.03087
Epoch 6/80: current_loss=0.03262 | best_loss=0.03087
Epoch 7/80: current_loss=0.02980 | best_loss=0.02980
Epoch 8/80: current_loss=0.02980 | best_loss=0.02980
Epoch 9/80: current_loss=0.02982 | best_loss=0.02980
Epoch 10/80: current_loss=0.02980 | best_loss=0.02980
Epoch 11/80: current_loss=0.02980 | best_loss=0.02980
Epoch 12/80: current_loss=0.02980 | best_loss=0.02980
Epoch 13/80: current_loss=0.02980 | best_loss=0.02980
Epoch 14/80: current_loss=0.02998 | best_loss=0.02980
Epoch 15/80: current_loss=0.02982 | best_loss=0.02980
Epoch 16/80: current_loss=0.02981 | best_loss=0.02980
Epoch 17/80: current_loss=0.02989 | best_loss=0.02980
Epoch 18/80: current_loss=0.02980 | best_loss=0.02980
Epoch 19/80: current_loss=0.02999 | best_loss=0.02980
Epoch 20/80: current_loss=0.02985 | best_loss=0.02980
Epoch 21/80: current_loss=0.02987 | best_loss=0.02980
Epoch 22/80: current_loss=0.02990 | best_loss=0.02980
Epoch 23/80: current_loss=0.02981 | best_loss=0.02980
Epoch 24/80: current_loss=0.02986 | best_loss=0.02980
Epoch 25/80: current_loss=0.02980 | best_loss=0.02980
Epoch 26/80: current_loss=0.02981 | best_loss=0.02980
Epoch 27/80: current_loss=0.02980 | best_loss=0.02980
Epoch 28/80: current_loss=0.02985 | best_loss=0.02980
Epoch 29/80: current_loss=0.02981 | best_loss=0.02980
Epoch 30/80: current_loss=0.02982 | best_loss=0.02980
Epoch 31/80: current_loss=0.02990 | best_loss=0.02980
Epoch 32/80: current_loss=0.02981 | best_loss=0.02980
Epoch 33/80: current_loss=0.02986 | best_loss=0.02980
Epoch 34/80: current_loss=0.02998 | best_loss=0.02980
Epoch 35/80: current_loss=0.02998 | best_loss=0.02980
Epoch 36/80: current_loss=0.02980 | best_loss=0.02980
Epoch 37/80: current_loss=0.02980 | best_loss=0.02980
Epoch 38/80: current_loss=0.02982 | best_loss=0.02980
Epoch 39/80: current_loss=0.02981 | best_loss=0.02980
Epoch 40/80: current_loss=0.02981 | best_loss=0.02980
Epoch 41/80: current_loss=0.02980 | best_loss=0.02980
Epoch 42/80: current_loss=0.03011 | best_loss=0.02980
Epoch 43/80: current_loss=0.02981 | best_loss=0.02980
Epoch 44/80: current_loss=0.02984 | best_loss=0.02980
Epoch 45/80: current_loss=0.03003 | best_loss=0.02980
Epoch 46/80: current_loss=0.02984 | best_loss=0.02980
Epoch 47/80: current_loss=0.02980 | best_loss=0.02980
Early Stopping at epoch 47
      explained_var=0.00000 | mse_loss=0.02876
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.00079| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 11
{'learning_rate': 0.01, 'weight_decay': 0.002554434662107642, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.08424 | best_loss=0.08424
Epoch 1/80: current_loss=0.03619 | best_loss=0.03619
Epoch 2/80: current_loss=0.03157 | best_loss=0.03157
Epoch 3/80: current_loss=0.04457 | best_loss=0.03157
Epoch 4/80: current_loss=0.04679 | best_loss=0.03157
Epoch 5/80: current_loss=0.04028 | best_loss=0.03157
Epoch 6/80: current_loss=0.04049 | best_loss=0.03157
Epoch 7/80: current_loss=0.03466 | best_loss=0.03157
Epoch 8/80: current_loss=0.03069 | best_loss=0.03069
Epoch 9/80: current_loss=0.03377 | best_loss=0.03069
Epoch 10/80: current_loss=0.03309 | best_loss=0.03069
Epoch 11/80: current_loss=0.03216 | best_loss=0.03069
Epoch 12/80: current_loss=0.04180 | best_loss=0.03069
Epoch 13/80: current_loss=0.03011 | best_loss=0.03011
Epoch 14/80: current_loss=0.10404 | best_loss=0.03011
Epoch 15/80: current_loss=0.02880 | best_loss=0.02880
Epoch 16/80: current_loss=0.02913 | best_loss=0.02880
Epoch 17/80: current_loss=0.03405 | best_loss=0.02880
Epoch 18/80: current_loss=0.03163 | best_loss=0.02880
Epoch 19/80: current_loss=0.02922 | best_loss=0.02880
Epoch 20/80: current_loss=0.02971 | best_loss=0.02880
Epoch 21/80: current_loss=0.02916 | best_loss=0.02880
Epoch 22/80: current_loss=0.02921 | best_loss=0.02880
Epoch 23/80: current_loss=0.02916 | best_loss=0.02880
Epoch 24/80: current_loss=0.02939 | best_loss=0.02880
Epoch 25/80: current_loss=0.02937 | best_loss=0.02880
Epoch 26/80: current_loss=0.02940 | best_loss=0.02880
Epoch 27/80: current_loss=0.02912 | best_loss=0.02880
Epoch 28/80: current_loss=0.02922 | best_loss=0.02880
Epoch 29/80: current_loss=0.02950 | best_loss=0.02880
Epoch 30/80: current_loss=0.02934 | best_loss=0.02880
Epoch 31/80: current_loss=0.02917 | best_loss=0.02880
Epoch 32/80: current_loss=0.02913 | best_loss=0.02880
Epoch 33/80: current_loss=0.02911 | best_loss=0.02880
Epoch 34/80: current_loss=0.02912 | best_loss=0.02880
Epoch 35/80: current_loss=0.02915 | best_loss=0.02880
Early Stopping at epoch 35
      explained_var=0.00912 | mse_loss=0.02802
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.15920 | best_loss=0.15920
Epoch 1/80: current_loss=0.12924 | best_loss=0.12924
Epoch 2/80: current_loss=0.04066 | best_loss=0.04066
Epoch 3/80: current_loss=0.03513 | best_loss=0.03513
Epoch 4/80: current_loss=0.02664 | best_loss=0.02664
Epoch 5/80: current_loss=0.04047 | best_loss=0.02664
Epoch 6/80: current_loss=0.04146 | best_loss=0.02664
Epoch 7/80: current_loss=0.05857 | best_loss=0.02664
Epoch 8/80: current_loss=0.03950 | best_loss=0.02664
Epoch 9/80: current_loss=0.12013 | best_loss=0.02664
Epoch 10/80: current_loss=0.06169 | best_loss=0.02664
Epoch 11/80: current_loss=0.36906 | best_loss=0.02664
Epoch 12/80: current_loss=0.02645 | best_loss=0.02645
Epoch 13/80: current_loss=0.04524 | best_loss=0.02645
Epoch 14/80: current_loss=0.06313 | best_loss=0.02645
Epoch 15/80: current_loss=0.03798 | best_loss=0.02645
Epoch 16/80: current_loss=0.06223 | best_loss=0.02645
Epoch 17/80: current_loss=0.06766 | best_loss=0.02645
Epoch 18/80: current_loss=0.06630 | best_loss=0.02645
Epoch 19/80: current_loss=0.03437 | best_loss=0.02645
Epoch 20/80: current_loss=0.04508 | best_loss=0.02645
Epoch 21/80: current_loss=0.05553 | best_loss=0.02645
Epoch 22/80: current_loss=0.14669 | best_loss=0.02645
Epoch 23/80: current_loss=0.06313 | best_loss=0.02645
Epoch 24/80: current_loss=0.02659 | best_loss=0.02645
Epoch 25/80: current_loss=0.07146 | best_loss=0.02645
Epoch 26/80: current_loss=0.03849 | best_loss=0.02645
Epoch 27/80: current_loss=0.08089 | best_loss=0.02645
Epoch 28/80: current_loss=0.03025 | best_loss=0.02645
Epoch 29/80: current_loss=0.02893 | best_loss=0.02645
Epoch 30/80: current_loss=0.04387 | best_loss=0.02645
Epoch 31/80: current_loss=0.10105 | best_loss=0.02645
Epoch 32/80: current_loss=0.13861 | best_loss=0.02645
Early Stopping at epoch 32
      explained_var=-0.00104 | mse_loss=0.02651
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.07572 | best_loss=0.07572
Epoch 1/80: current_loss=0.03740 | best_loss=0.03740
Epoch 2/80: current_loss=0.04310 | best_loss=0.03740
Epoch 3/80: current_loss=0.02827 | best_loss=0.02827
Epoch 4/80: current_loss=0.02837 | best_loss=0.02827
Epoch 5/80: current_loss=0.03227 | best_loss=0.02827
Epoch 6/80: current_loss=0.02755 | best_loss=0.02755
Epoch 7/80: current_loss=0.02771 | best_loss=0.02755
Epoch 8/80: current_loss=0.02777 | best_loss=0.02755
Epoch 9/80: current_loss=0.02746 | best_loss=0.02746
Epoch 10/80: current_loss=0.02763 | best_loss=0.02746
Epoch 11/80: current_loss=0.02764 | best_loss=0.02746
Epoch 12/80: current_loss=0.02749 | best_loss=0.02746
Epoch 13/80: current_loss=0.02744 | best_loss=0.02744
Epoch 14/80: current_loss=0.02759 | best_loss=0.02744
Epoch 15/80: current_loss=0.02761 | best_loss=0.02744
Epoch 16/80: current_loss=0.02757 | best_loss=0.02744
Epoch 17/80: current_loss=0.02814 | best_loss=0.02744
Epoch 18/80: current_loss=0.02753 | best_loss=0.02744
Epoch 19/80: current_loss=0.02792 | best_loss=0.02744
Epoch 20/80: current_loss=0.02752 | best_loss=0.02744
Epoch 21/80: current_loss=0.02773 | best_loss=0.02744
Epoch 22/80: current_loss=0.02754 | best_loss=0.02744
Epoch 23/80: current_loss=0.02769 | best_loss=0.02744
Epoch 24/80: current_loss=0.02770 | best_loss=0.02744
Epoch 25/80: current_loss=0.02754 | best_loss=0.02744
Epoch 26/80: current_loss=0.02775 | best_loss=0.02744
Epoch 27/80: current_loss=0.02770 | best_loss=0.02744
Epoch 28/80: current_loss=0.02743 | best_loss=0.02743
Epoch 29/80: current_loss=0.02765 | best_loss=0.02743
Epoch 30/80: current_loss=0.02731 | best_loss=0.02731
Epoch 31/80: current_loss=0.02736 | best_loss=0.02731
Epoch 32/80: current_loss=0.02756 | best_loss=0.02731
Epoch 33/80: current_loss=0.02784 | best_loss=0.02731
Epoch 34/80: current_loss=0.02753 | best_loss=0.02731
Epoch 35/80: current_loss=0.02759 | best_loss=0.02731
Epoch 36/80: current_loss=0.02766 | best_loss=0.02731
Epoch 37/80: current_loss=0.02735 | best_loss=0.02731
Epoch 38/80: current_loss=0.02801 | best_loss=0.02731
Epoch 39/80: current_loss=0.02719 | best_loss=0.02719
Epoch 40/80: current_loss=0.02822 | best_loss=0.02719
Epoch 41/80: current_loss=0.02762 | best_loss=0.02719
Epoch 42/80: current_loss=0.02769 | best_loss=0.02719
Epoch 43/80: current_loss=0.02775 | best_loss=0.02719
Epoch 44/80: current_loss=0.02719 | best_loss=0.02719
Epoch 45/80: current_loss=0.02729 | best_loss=0.02719
Epoch 46/80: current_loss=0.02743 | best_loss=0.02719
Epoch 47/80: current_loss=0.02750 | best_loss=0.02719
Epoch 48/80: current_loss=0.02757 | best_loss=0.02719
Epoch 49/80: current_loss=0.02745 | best_loss=0.02719
Epoch 50/80: current_loss=0.02760 | best_loss=0.02719
Epoch 51/80: current_loss=0.02751 | best_loss=0.02719
Epoch 52/80: current_loss=0.02791 | best_loss=0.02719
Epoch 53/80: current_loss=0.02751 | best_loss=0.02719
Epoch 54/80: current_loss=0.02778 | best_loss=0.02719
Epoch 55/80: current_loss=0.02814 | best_loss=0.02719
Epoch 56/80: current_loss=0.02822 | best_loss=0.02719
Epoch 57/80: current_loss=0.02795 | best_loss=0.02719
Epoch 58/80: current_loss=0.02771 | best_loss=0.02719
Epoch 59/80: current_loss=0.02798 | best_loss=0.02719
Early Stopping at epoch 59
      explained_var=0.00000 | mse_loss=0.02756
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04251 | best_loss=0.04251
Epoch 1/80: current_loss=0.03214 | best_loss=0.03214
Epoch 2/80: current_loss=0.03068 | best_loss=0.03068
Epoch 3/80: current_loss=0.03368 | best_loss=0.03068
Epoch 4/80: current_loss=0.02994 | best_loss=0.02994
Epoch 5/80: current_loss=0.05800 | best_loss=0.02994
Epoch 6/80: current_loss=0.05201 | best_loss=0.02994
Epoch 7/80: current_loss=0.04805 | best_loss=0.02994
Epoch 8/80: current_loss=0.09874 | best_loss=0.02994
Epoch 9/80: current_loss=0.03008 | best_loss=0.02994
Epoch 10/80: current_loss=0.03184 | best_loss=0.02994
Epoch 11/80: current_loss=0.04025 | best_loss=0.02994
Epoch 12/80: current_loss=0.10903 | best_loss=0.02994
Epoch 13/80: current_loss=0.04761 | best_loss=0.02994
Epoch 14/80: current_loss=0.12030 | best_loss=0.02994
Epoch 15/80: current_loss=0.06154 | best_loss=0.02994
Epoch 16/80: current_loss=0.05288 | best_loss=0.02994
Epoch 17/80: current_loss=0.02820 | best_loss=0.02820
Epoch 18/80: current_loss=0.06059 | best_loss=0.02820
Epoch 19/80: current_loss=0.02754 | best_loss=0.02754
Epoch 20/80: current_loss=0.02792 | best_loss=0.02754
Epoch 21/80: current_loss=0.04376 | best_loss=0.02754
Epoch 22/80: current_loss=0.03005 | best_loss=0.02754
Epoch 23/80: current_loss=0.03031 | best_loss=0.02754
Epoch 24/80: current_loss=0.02687 | best_loss=0.02687
Epoch 25/80: current_loss=0.02684 | best_loss=0.02684
Epoch 26/80: current_loss=0.02683 | best_loss=0.02683
Epoch 27/80: current_loss=0.02680 | best_loss=0.02680
Epoch 28/80: current_loss=0.02683 | best_loss=0.02680
Epoch 29/80: current_loss=0.02686 | best_loss=0.02680
Epoch 30/80: current_loss=0.02679 | best_loss=0.02679
Epoch 31/80: current_loss=0.02683 | best_loss=0.02679
Epoch 32/80: current_loss=0.02679 | best_loss=0.02679
Epoch 33/80: current_loss=0.02684 | best_loss=0.02679
Epoch 34/80: current_loss=0.02691 | best_loss=0.02679
Epoch 35/80: current_loss=0.02694 | best_loss=0.02679
Epoch 36/80: current_loss=0.02694 | best_loss=0.02679
Epoch 37/80: current_loss=0.02688 | best_loss=0.02679
Epoch 38/80: current_loss=0.02682 | best_loss=0.02679
Epoch 39/80: current_loss=0.02688 | best_loss=0.02679
Epoch 40/80: current_loss=0.02702 | best_loss=0.02679
Epoch 41/80: current_loss=0.02681 | best_loss=0.02679
Epoch 42/80: current_loss=0.02692 | best_loss=0.02679
Epoch 43/80: current_loss=0.02699 | best_loss=0.02679
Epoch 44/80: current_loss=0.02678 | best_loss=0.02678
Epoch 45/80: current_loss=0.02686 | best_loss=0.02678
Epoch 46/80: current_loss=0.02688 | best_loss=0.02678
Epoch 47/80: current_loss=0.02685 | best_loss=0.02678
Epoch 48/80: current_loss=0.02698 | best_loss=0.02678
Epoch 49/80: current_loss=0.02684 | best_loss=0.02678
Epoch 50/80: current_loss=0.02698 | best_loss=0.02678
Epoch 51/80: current_loss=0.02678 | best_loss=0.02678
Epoch 52/80: current_loss=0.02678 | best_loss=0.02678
Epoch 53/80: current_loss=0.02703 | best_loss=0.02678
Epoch 54/80: current_loss=0.02683 | best_loss=0.02678
Epoch 55/80: current_loss=0.02706 | best_loss=0.02678
Epoch 56/80: current_loss=0.02689 | best_loss=0.02678
Epoch 57/80: current_loss=0.02696 | best_loss=0.02678
Epoch 58/80: current_loss=0.02685 | best_loss=0.02678
Epoch 59/80: current_loss=0.02680 | best_loss=0.02678
Epoch 60/80: current_loss=0.02693 | best_loss=0.02678
Epoch 61/80: current_loss=0.02680 | best_loss=0.02678
Epoch 62/80: current_loss=0.02697 | best_loss=0.02678
Epoch 63/80: current_loss=0.02680 | best_loss=0.02678
Epoch 64/80: current_loss=0.02702 | best_loss=0.02678
Epoch 65/80: current_loss=0.02703 | best_loss=0.02678
Epoch 66/80: current_loss=0.02709 | best_loss=0.02678
Epoch 67/80: current_loss=0.02687 | best_loss=0.02678
Epoch 68/80: current_loss=0.02680 | best_loss=0.02678
Epoch 69/80: current_loss=0.02680 | best_loss=0.02678
Epoch 70/80: current_loss=0.02695 | best_loss=0.02678
Epoch 71/80: current_loss=0.02681 | best_loss=0.02678
Early Stopping at epoch 71
      explained_var=-0.00000 | mse_loss=0.02499
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04436 | best_loss=0.04436
Epoch 1/80: current_loss=0.03287 | best_loss=0.03287
Epoch 2/80: current_loss=0.03500 | best_loss=0.03287
Epoch 3/80: current_loss=0.04960 | best_loss=0.03287
Epoch 4/80: current_loss=0.05614 | best_loss=0.03287
Epoch 5/80: current_loss=0.05937 | best_loss=0.03287
Epoch 6/80: current_loss=0.27973 | best_loss=0.03287
Epoch 7/80: current_loss=0.19304 | best_loss=0.03287
Epoch 8/80: current_loss=0.03225 | best_loss=0.03225
Epoch 9/80: current_loss=0.03099 | best_loss=0.03099
Epoch 10/80: current_loss=0.05776 | best_loss=0.03099
Epoch 11/80: current_loss=0.03352 | best_loss=0.03099
Epoch 12/80: current_loss=0.11392 | best_loss=0.03099
Epoch 13/80: current_loss=0.04956 | best_loss=0.03099
Epoch 14/80: current_loss=0.08539 | best_loss=0.03099
Epoch 15/80: current_loss=0.04215 | best_loss=0.03099
Epoch 16/80: current_loss=0.15687 | best_loss=0.03099
Epoch 17/80: current_loss=0.07295 | best_loss=0.03099
Epoch 18/80: current_loss=0.16716 | best_loss=0.03099
Epoch 19/80: current_loss=0.38426 | best_loss=0.03099
Epoch 20/80: current_loss=0.11316 | best_loss=0.03099
Epoch 21/80: current_loss=0.12257 | best_loss=0.03099
Epoch 22/80: current_loss=0.10746 | best_loss=0.03099
Epoch 23/80: current_loss=0.32281 | best_loss=0.03099
Epoch 24/80: current_loss=0.09182 | best_loss=0.03099
Epoch 25/80: current_loss=0.05451 | best_loss=0.03099
Epoch 26/80: current_loss=0.09775 | best_loss=0.03099
Epoch 27/80: current_loss=0.03697 | best_loss=0.03099
Epoch 28/80: current_loss=0.33437 | best_loss=0.03099
Epoch 29/80: current_loss=0.03425 | best_loss=0.03099
Early Stopping at epoch 29
      explained_var=-0.02497 | mse_loss=0.02949
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=-0.00338| avg_loss=0.02731
----------------------------------------------


----------------------------------------------
Params for Trial 12
{'learning_rate': 0.01, 'weight_decay': 0.0028581370873874876, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.20089 | best_loss=0.20089
Epoch 1/80: current_loss=0.03202 | best_loss=0.03202
Epoch 2/80: current_loss=0.04288 | best_loss=0.03202
Epoch 3/80: current_loss=0.04842 | best_loss=0.03202
Epoch 4/80: current_loss=0.05651 | best_loss=0.03202
Epoch 5/80: current_loss=0.03720 | best_loss=0.03202
Epoch 6/80: current_loss=0.03156 | best_loss=0.03156
Epoch 7/80: current_loss=0.03223 | best_loss=0.03156
Epoch 8/80: current_loss=0.03297 | best_loss=0.03156
Epoch 9/80: current_loss=0.02928 | best_loss=0.02928
Epoch 10/80: current_loss=0.04408 | best_loss=0.02928
Epoch 11/80: current_loss=0.03219 | best_loss=0.02928
Epoch 12/80: current_loss=0.03681 | best_loss=0.02928
Epoch 13/80: current_loss=0.02918 | best_loss=0.02918
Epoch 14/80: current_loss=0.02937 | best_loss=0.02918
Epoch 15/80: current_loss=0.03530 | best_loss=0.02918
Epoch 16/80: current_loss=0.03531 | best_loss=0.02918
Epoch 17/80: current_loss=0.02924 | best_loss=0.02918
Epoch 18/80: current_loss=0.03529 | best_loss=0.02918
Epoch 19/80: current_loss=0.04336 | best_loss=0.02918
Epoch 20/80: current_loss=0.02967 | best_loss=0.02918
Epoch 21/80: current_loss=0.02975 | best_loss=0.02918
Epoch 22/80: current_loss=0.03056 | best_loss=0.02918
Epoch 23/80: current_loss=0.02899 | best_loss=0.02899
Epoch 24/80: current_loss=0.02969 | best_loss=0.02899
Epoch 25/80: current_loss=0.02924 | best_loss=0.02899
Epoch 26/80: current_loss=0.02909 | best_loss=0.02899
Epoch 27/80: current_loss=0.02925 | best_loss=0.02899
Epoch 28/80: current_loss=0.02916 | best_loss=0.02899
Epoch 29/80: current_loss=0.02944 | best_loss=0.02899
Epoch 30/80: current_loss=0.02924 | best_loss=0.02899
Epoch 31/80: current_loss=0.02913 | best_loss=0.02899
Epoch 32/80: current_loss=0.02913 | best_loss=0.02899
Epoch 33/80: current_loss=0.02918 | best_loss=0.02899
Epoch 34/80: current_loss=0.02920 | best_loss=0.02899
Epoch 35/80: current_loss=0.02951 | best_loss=0.02899
Epoch 36/80: current_loss=0.02918 | best_loss=0.02899
Epoch 37/80: current_loss=0.02914 | best_loss=0.02899
Epoch 38/80: current_loss=0.02913 | best_loss=0.02899
Epoch 39/80: current_loss=0.02923 | best_loss=0.02899
Epoch 40/80: current_loss=0.02913 | best_loss=0.02899
Epoch 41/80: current_loss=0.02912 | best_loss=0.02899
Epoch 42/80: current_loss=0.02919 | best_loss=0.02899
Epoch 43/80: current_loss=0.02911 | best_loss=0.02899
Early Stopping at epoch 43
      explained_var=0.00297 | mse_loss=0.02806
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03010 | best_loss=0.03010
Epoch 1/80: current_loss=0.03182 | best_loss=0.03010
Epoch 2/80: current_loss=0.02473 | best_loss=0.02473
Epoch 3/80: current_loss=0.04203 | best_loss=0.02473
Epoch 4/80: current_loss=0.03375 | best_loss=0.02473
Epoch 5/80: current_loss=0.03549 | best_loss=0.02473
Epoch 6/80: current_loss=0.02702 | best_loss=0.02473
Epoch 7/80: current_loss=0.02792 | best_loss=0.02473
Epoch 8/80: current_loss=0.03531 | best_loss=0.02473
Epoch 9/80: current_loss=0.03435 | best_loss=0.02473
Epoch 10/80: current_loss=0.04322 | best_loss=0.02473
Epoch 11/80: current_loss=0.04796 | best_loss=0.02473
Epoch 12/80: current_loss=0.03177 | best_loss=0.02473
Epoch 13/80: current_loss=0.02974 | best_loss=0.02473
Epoch 14/80: current_loss=0.02527 | best_loss=0.02473
Epoch 15/80: current_loss=0.02506 | best_loss=0.02473
Epoch 16/80: current_loss=0.02557 | best_loss=0.02473
Epoch 17/80: current_loss=0.02507 | best_loss=0.02473
Epoch 18/80: current_loss=0.02508 | best_loss=0.02473
Epoch 19/80: current_loss=0.02519 | best_loss=0.02473
Epoch 20/80: current_loss=0.02526 | best_loss=0.02473
Epoch 21/80: current_loss=0.02502 | best_loss=0.02473
Epoch 22/80: current_loss=0.02503 | best_loss=0.02473
Early Stopping at epoch 22
      explained_var=-0.00127 | mse_loss=0.02523
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.10194 | best_loss=0.10194
Epoch 1/80: current_loss=0.02911 | best_loss=0.02911
Epoch 2/80: current_loss=0.08054 | best_loss=0.02911
Epoch 3/80: current_loss=0.02798 | best_loss=0.02798
Epoch 4/80: current_loss=0.03101 | best_loss=0.02798
Epoch 5/80: current_loss=0.03022 | best_loss=0.02798
Epoch 6/80: current_loss=0.04619 | best_loss=0.02798
Epoch 7/80: current_loss=0.07145 | best_loss=0.02798
Epoch 8/80: current_loss=0.02843 | best_loss=0.02798
Epoch 9/80: current_loss=0.03402 | best_loss=0.02798
Epoch 10/80: current_loss=0.02746 | best_loss=0.02746
Epoch 11/80: current_loss=0.02719 | best_loss=0.02719
Epoch 12/80: current_loss=0.02735 | best_loss=0.02719
Epoch 13/80: current_loss=0.02760 | best_loss=0.02719
Epoch 14/80: current_loss=0.02752 | best_loss=0.02719
Epoch 15/80: current_loss=0.02733 | best_loss=0.02719
Epoch 16/80: current_loss=0.02773 | best_loss=0.02719
Epoch 17/80: current_loss=0.02769 | best_loss=0.02719
Epoch 18/80: current_loss=0.02788 | best_loss=0.02719
Epoch 19/80: current_loss=0.02745 | best_loss=0.02719
Epoch 20/80: current_loss=0.02781 | best_loss=0.02719
Epoch 21/80: current_loss=0.02752 | best_loss=0.02719
Epoch 22/80: current_loss=0.02796 | best_loss=0.02719
Epoch 23/80: current_loss=0.02762 | best_loss=0.02719
Epoch 24/80: current_loss=0.02731 | best_loss=0.02719
Epoch 25/80: current_loss=0.02722 | best_loss=0.02719
Epoch 26/80: current_loss=0.02762 | best_loss=0.02719
Epoch 27/80: current_loss=0.02762 | best_loss=0.02719
Epoch 28/80: current_loss=0.02767 | best_loss=0.02719
Epoch 29/80: current_loss=0.02741 | best_loss=0.02719
Epoch 30/80: current_loss=0.02757 | best_loss=0.02719
Epoch 31/80: current_loss=0.02762 | best_loss=0.02719
Early Stopping at epoch 31
      explained_var=-0.00019 | mse_loss=0.02756
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.05161 | best_loss=0.05161
Epoch 1/80: current_loss=0.03045 | best_loss=0.03045
Epoch 2/80: current_loss=0.03262 | best_loss=0.03045
Epoch 3/80: current_loss=0.02706 | best_loss=0.02706
Epoch 4/80: current_loss=0.03105 | best_loss=0.02706
Epoch 5/80: current_loss=0.03511 | best_loss=0.02706
Epoch 6/80: current_loss=0.02756 | best_loss=0.02706
Epoch 7/80: current_loss=0.02717 | best_loss=0.02706
Epoch 8/80: current_loss=0.02680 | best_loss=0.02680
Epoch 9/80: current_loss=0.02682 | best_loss=0.02680
Epoch 10/80: current_loss=0.02679 | best_loss=0.02679
Epoch 11/80: current_loss=0.02696 | best_loss=0.02679
Epoch 12/80: current_loss=0.02683 | best_loss=0.02679
Epoch 13/80: current_loss=0.02698 | best_loss=0.02679
Epoch 14/80: current_loss=0.02679 | best_loss=0.02679
Epoch 15/80: current_loss=0.02678 | best_loss=0.02678
Epoch 16/80: current_loss=0.02679 | best_loss=0.02678
Epoch 17/80: current_loss=0.02685 | best_loss=0.02678
Epoch 18/80: current_loss=0.02700 | best_loss=0.02678
Epoch 19/80: current_loss=0.02699 | best_loss=0.02678
Epoch 20/80: current_loss=0.02690 | best_loss=0.02678
Epoch 21/80: current_loss=0.02685 | best_loss=0.02678
Epoch 22/80: current_loss=0.02679 | best_loss=0.02678
Epoch 23/80: current_loss=0.02679 | best_loss=0.02678
Epoch 24/80: current_loss=0.02687 | best_loss=0.02678
Epoch 25/80: current_loss=0.02684 | best_loss=0.02678
Epoch 26/80: current_loss=0.02680 | best_loss=0.02678
Epoch 27/80: current_loss=0.02681 | best_loss=0.02678
Epoch 28/80: current_loss=0.02694 | best_loss=0.02678
Epoch 29/80: current_loss=0.02720 | best_loss=0.02678
Epoch 30/80: current_loss=0.02680 | best_loss=0.02678
Epoch 31/80: current_loss=0.02699 | best_loss=0.02678
Epoch 32/80: current_loss=0.02679 | best_loss=0.02678
Epoch 33/80: current_loss=0.02717 | best_loss=0.02678
Epoch 34/80: current_loss=0.02680 | best_loss=0.02678
Epoch 35/80: current_loss=0.02697 | best_loss=0.02678
Early Stopping at epoch 35
      explained_var=0.00000 | mse_loss=0.02498
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04215 | best_loss=0.04215
Epoch 1/80: current_loss=0.09378 | best_loss=0.04215
Epoch 2/80: current_loss=0.03032 | best_loss=0.03032
Epoch 3/80: current_loss=0.03515 | best_loss=0.03032
Epoch 4/80: current_loss=0.04306 | best_loss=0.03032
Epoch 5/80: current_loss=0.03593 | best_loss=0.03032
Epoch 6/80: current_loss=0.03470 | best_loss=0.03032
Epoch 7/80: current_loss=0.17169 | best_loss=0.03032
Epoch 8/80: current_loss=0.11398 | best_loss=0.03032
Epoch 9/80: current_loss=0.07480 | best_loss=0.03032
Epoch 10/80: current_loss=0.11000 | best_loss=0.03032
Epoch 11/80: current_loss=0.04598 | best_loss=0.03032
Epoch 12/80: current_loss=0.08871 | best_loss=0.03032
Epoch 13/80: current_loss=0.15153 | best_loss=0.03032
Epoch 14/80: current_loss=0.03021 | best_loss=0.03021
Epoch 15/80: current_loss=0.03989 | best_loss=0.03021
Epoch 16/80: current_loss=0.02989 | best_loss=0.02989
Epoch 17/80: current_loss=0.02980 | best_loss=0.02980
Epoch 18/80: current_loss=0.02988 | best_loss=0.02980
Epoch 19/80: current_loss=0.03021 | best_loss=0.02980
Epoch 20/80: current_loss=0.02986 | best_loss=0.02980
Epoch 21/80: current_loss=0.02997 | best_loss=0.02980
Epoch 22/80: current_loss=0.02989 | best_loss=0.02980
Epoch 23/80: current_loss=0.02982 | best_loss=0.02980
Epoch 24/80: current_loss=0.02985 | best_loss=0.02980
Epoch 25/80: current_loss=0.02980 | best_loss=0.02980
Epoch 26/80: current_loss=0.02987 | best_loss=0.02980
Epoch 27/80: current_loss=0.02991 | best_loss=0.02980
Epoch 28/80: current_loss=0.02980 | best_loss=0.02980
Epoch 29/80: current_loss=0.02983 | best_loss=0.02980
Epoch 30/80: current_loss=0.02984 | best_loss=0.02980
Epoch 31/80: current_loss=0.02980 | best_loss=0.02980
Epoch 32/80: current_loss=0.02987 | best_loss=0.02980
Epoch 33/80: current_loss=0.03011 | best_loss=0.02980
Epoch 34/80: current_loss=0.02992 | best_loss=0.02980
Epoch 35/80: current_loss=0.02981 | best_loss=0.02980
Epoch 36/80: current_loss=0.02980 | best_loss=0.02980
Epoch 37/80: current_loss=0.02983 | best_loss=0.02980
Epoch 38/80: current_loss=0.02995 | best_loss=0.02980
Epoch 39/80: current_loss=0.02987 | best_loss=0.02980
Epoch 40/80: current_loss=0.02980 | best_loss=0.02980
Epoch 41/80: current_loss=0.02981 | best_loss=0.02980
Epoch 42/80: current_loss=0.02980 | best_loss=0.02980
Epoch 43/80: current_loss=0.02988 | best_loss=0.02980
Epoch 44/80: current_loss=0.02989 | best_loss=0.02980
Epoch 45/80: current_loss=0.02987 | best_loss=0.02980
Epoch 46/80: current_loss=0.02980 | best_loss=0.02980
Epoch 47/80: current_loss=0.02982 | best_loss=0.02980
Epoch 48/80: current_loss=0.02980 | best_loss=0.02980
Epoch 49/80: current_loss=0.02980 | best_loss=0.02980
Epoch 50/80: current_loss=0.02980 | best_loss=0.02980
Epoch 51/80: current_loss=0.02983 | best_loss=0.02980
Epoch 52/80: current_loss=0.02988 | best_loss=0.02980
Epoch 53/80: current_loss=0.02982 | best_loss=0.02980
Epoch 54/80: current_loss=0.02980 | best_loss=0.02980
Epoch 55/80: current_loss=0.02980 | best_loss=0.02980
Epoch 56/80: current_loss=0.02980 | best_loss=0.02980
Epoch 57/80: current_loss=0.02980 | best_loss=0.02980
Epoch 58/80: current_loss=0.02987 | best_loss=0.02980
Epoch 59/80: current_loss=0.02989 | best_loss=0.02980
Epoch 60/80: current_loss=0.03026 | best_loss=0.02980
Early Stopping at epoch 60
      explained_var=0.00000 | mse_loss=0.02876
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00030| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 13
{'learning_rate': 0.01, 'weight_decay': 6.607948522012599e-05, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.19279 | best_loss=0.19279
Epoch 1/80: current_loss=0.06790 | best_loss=0.06790
Epoch 2/80: current_loss=0.03612 | best_loss=0.03612
Epoch 3/80: current_loss=0.02950 | best_loss=0.02950
Epoch 4/80: current_loss=0.03035 | best_loss=0.02950
Epoch 5/80: current_loss=0.03439 | best_loss=0.02950
Epoch 6/80: current_loss=0.03400 | best_loss=0.02950
Epoch 7/80: current_loss=0.03551 | best_loss=0.02950
Epoch 8/80: current_loss=0.03386 | best_loss=0.02950
Epoch 9/80: current_loss=0.05706 | best_loss=0.02950
Epoch 10/80: current_loss=0.05104 | best_loss=0.02950
Epoch 11/80: current_loss=0.03639 | best_loss=0.02950
Epoch 12/80: current_loss=0.03665 | best_loss=0.02950
Epoch 13/80: current_loss=0.03764 | best_loss=0.02950
Epoch 14/80: current_loss=0.05769 | best_loss=0.02950
Epoch 15/80: current_loss=0.04691 | best_loss=0.02950
Epoch 16/80: current_loss=0.08812 | best_loss=0.02950
Epoch 17/80: current_loss=0.06275 | best_loss=0.02950
Epoch 18/80: current_loss=0.03857 | best_loss=0.02950
Epoch 19/80: current_loss=0.05145 | best_loss=0.02950
Epoch 20/80: current_loss=0.04893 | best_loss=0.02950
Epoch 21/80: current_loss=0.09834 | best_loss=0.02950
Epoch 22/80: current_loss=0.08492 | best_loss=0.02950
Epoch 23/80: current_loss=0.05146 | best_loss=0.02950
Early Stopping at epoch 23
      explained_var=-0.01415 | mse_loss=0.02855
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04682 | best_loss=0.04682
Epoch 1/80: current_loss=0.03178 | best_loss=0.03178
Epoch 2/80: current_loss=0.04710 | best_loss=0.03178
Epoch 3/80: current_loss=0.04358 | best_loss=0.03178
Epoch 4/80: current_loss=0.04269 | best_loss=0.03178
Epoch 5/80: current_loss=0.03563 | best_loss=0.03178
Epoch 6/80: current_loss=0.06046 | best_loss=0.03178
Epoch 7/80: current_loss=0.03120 | best_loss=0.03120
Epoch 8/80: current_loss=0.03120 | best_loss=0.03120
Epoch 9/80: current_loss=0.03194 | best_loss=0.03120
Epoch 10/80: current_loss=0.06998 | best_loss=0.03120
Epoch 11/80: current_loss=0.06566 | best_loss=0.03120
Epoch 12/80: current_loss=0.03509 | best_loss=0.03120
Epoch 13/80: current_loss=0.04633 | best_loss=0.03120
Epoch 14/80: current_loss=0.03672 | best_loss=0.03120
Epoch 15/80: current_loss=0.06174 | best_loss=0.03120
Epoch 16/80: current_loss=0.07231 | best_loss=0.03120
Epoch 17/80: current_loss=0.04352 | best_loss=0.03120
Epoch 18/80: current_loss=0.07962 | best_loss=0.03120
Epoch 19/80: current_loss=0.10014 | best_loss=0.03120
Epoch 20/80: current_loss=0.03491 | best_loss=0.03120
Epoch 21/80: current_loss=0.05034 | best_loss=0.03120
Epoch 22/80: current_loss=0.05767 | best_loss=0.03120
Epoch 23/80: current_loss=0.03280 | best_loss=0.03120
Epoch 24/80: current_loss=0.07299 | best_loss=0.03120
Epoch 25/80: current_loss=0.04678 | best_loss=0.03120
Epoch 26/80: current_loss=0.12461 | best_loss=0.03120
Epoch 27/80: current_loss=0.04091 | best_loss=0.03120
Epoch 28/80: current_loss=0.09115 | best_loss=0.03120
Early Stopping at epoch 28
      explained_var=-0.23343 | mse_loss=0.03125
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02976 | best_loss=0.02976
Epoch 1/80: current_loss=0.04376 | best_loss=0.02976
Epoch 2/80: current_loss=0.04106 | best_loss=0.02976
Epoch 3/80: current_loss=0.09908 | best_loss=0.02976
Epoch 4/80: current_loss=0.06972 | best_loss=0.02976
Epoch 5/80: current_loss=0.06246 | best_loss=0.02976
Epoch 6/80: current_loss=0.05954 | best_loss=0.02976
Epoch 7/80: current_loss=0.13576 | best_loss=0.02976
Epoch 8/80: current_loss=0.09795 | best_loss=0.02976
Epoch 9/80: current_loss=0.06109 | best_loss=0.02976
Epoch 10/80: current_loss=0.07054 | best_loss=0.02976
Epoch 11/80: current_loss=0.05687 | best_loss=0.02976
Epoch 12/80: current_loss=0.05372 | best_loss=0.02976
Epoch 13/80: current_loss=0.08134 | best_loss=0.02976
Epoch 14/80: current_loss=0.03413 | best_loss=0.02976
Epoch 15/80: current_loss=0.06016 | best_loss=0.02976
Epoch 16/80: current_loss=0.07300 | best_loss=0.02976
Epoch 17/80: current_loss=0.04203 | best_loss=0.02976
Epoch 18/80: current_loss=0.03237 | best_loss=0.02976
Epoch 19/80: current_loss=0.03482 | best_loss=0.02976
Epoch 20/80: current_loss=0.04191 | best_loss=0.02976
Early Stopping at epoch 20
      explained_var=-0.08632 | mse_loss=0.03027
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.08785 | best_loss=0.08785
Epoch 1/80: current_loss=0.04084 | best_loss=0.04084
Epoch 2/80: current_loss=0.07103 | best_loss=0.04084
Epoch 3/80: current_loss=0.03666 | best_loss=0.03666
Epoch 4/80: current_loss=0.04295 | best_loss=0.03666
Epoch 5/80: current_loss=0.09344 | best_loss=0.03666
Epoch 6/80: current_loss=0.06028 | best_loss=0.03666
Epoch 7/80: current_loss=0.06966 | best_loss=0.03666
Epoch 8/80: current_loss=0.04784 | best_loss=0.03666
Epoch 9/80: current_loss=0.11883 | best_loss=0.03666
Epoch 10/80: current_loss=0.05222 | best_loss=0.03666
Epoch 11/80: current_loss=0.04144 | best_loss=0.03666
Epoch 12/80: current_loss=0.03035 | best_loss=0.03035
Epoch 13/80: current_loss=0.04729 | best_loss=0.03035
Epoch 14/80: current_loss=0.06050 | best_loss=0.03035
Epoch 15/80: current_loss=0.08583 | best_loss=0.03035
Epoch 16/80: current_loss=0.06486 | best_loss=0.03035
Epoch 17/80: current_loss=0.10079 | best_loss=0.03035
Epoch 18/80: current_loss=0.05769 | best_loss=0.03035
Epoch 19/80: current_loss=0.05638 | best_loss=0.03035
Epoch 20/80: current_loss=0.23185 | best_loss=0.03035
Epoch 21/80: current_loss=0.07907 | best_loss=0.03035
Epoch 22/80: current_loss=0.02769 | best_loss=0.02769
Epoch 23/80: current_loss=0.04799 | best_loss=0.02769
Epoch 24/80: current_loss=0.02945 | best_loss=0.02769
Epoch 25/80: current_loss=0.13641 | best_loss=0.02769
Epoch 26/80: current_loss=0.07903 | best_loss=0.02769
Epoch 27/80: current_loss=0.03553 | best_loss=0.02769
Epoch 28/80: current_loss=0.02945 | best_loss=0.02769
Epoch 29/80: current_loss=0.02755 | best_loss=0.02755
Epoch 30/80: current_loss=0.02966 | best_loss=0.02755
Epoch 31/80: current_loss=0.10324 | best_loss=0.02755
Epoch 32/80: current_loss=0.07480 | best_loss=0.02755
Epoch 33/80: current_loss=0.03214 | best_loss=0.02755
Epoch 34/80: current_loss=0.13115 | best_loss=0.02755
Epoch 35/80: current_loss=0.05026 | best_loss=0.02755
Epoch 36/80: current_loss=0.03072 | best_loss=0.02755
Epoch 37/80: current_loss=0.07250 | best_loss=0.02755
Epoch 38/80: current_loss=0.05293 | best_loss=0.02755
Epoch 39/80: current_loss=0.08145 | best_loss=0.02755
Epoch 40/80: current_loss=0.05696 | best_loss=0.02755
Epoch 41/80: current_loss=0.12088 | best_loss=0.02755
Epoch 42/80: current_loss=0.06447 | best_loss=0.02755
Epoch 43/80: current_loss=0.02957 | best_loss=0.02755
Epoch 44/80: current_loss=0.04188 | best_loss=0.02755
Epoch 45/80: current_loss=0.03262 | best_loss=0.02755
Epoch 46/80: current_loss=0.03244 | best_loss=0.02755
Epoch 47/80: current_loss=0.05041 | best_loss=0.02755
Epoch 48/80: current_loss=0.04616 | best_loss=0.02755
Epoch 49/80: current_loss=0.03466 | best_loss=0.02755
Early Stopping at epoch 49
      explained_var=-0.01101 | mse_loss=0.02556
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.05316 | best_loss=0.05316
Epoch 1/80: current_loss=0.04266 | best_loss=0.04266
Epoch 2/80: current_loss=0.05418 | best_loss=0.04266
Epoch 3/80: current_loss=0.03177 | best_loss=0.03177
Epoch 4/80: current_loss=0.02971 | best_loss=0.02971
Epoch 5/80: current_loss=0.04680 | best_loss=0.02971
Epoch 6/80: current_loss=0.08521 | best_loss=0.02971
Epoch 7/80: current_loss=0.03041 | best_loss=0.02971
Epoch 8/80: current_loss=0.05623 | best_loss=0.02971
Epoch 9/80: current_loss=0.03010 | best_loss=0.02971
Epoch 10/80: current_loss=0.06204 | best_loss=0.02971
Epoch 11/80: current_loss=0.03375 | best_loss=0.02971
Epoch 12/80: current_loss=0.07067 | best_loss=0.02971
Epoch 13/80: current_loss=0.04162 | best_loss=0.02971
Epoch 14/80: current_loss=0.03203 | best_loss=0.02971
Epoch 15/80: current_loss=0.03796 | best_loss=0.02971
Epoch 16/80: current_loss=0.03751 | best_loss=0.02971
Epoch 17/80: current_loss=0.07331 | best_loss=0.02971
Epoch 18/80: current_loss=0.11540 | best_loss=0.02971
Epoch 19/80: current_loss=0.05336 | best_loss=0.02971
Epoch 20/80: current_loss=0.03265 | best_loss=0.02971
Epoch 21/80: current_loss=0.03362 | best_loss=0.02971
Epoch 22/80: current_loss=0.09132 | best_loss=0.02971
Epoch 23/80: current_loss=0.06566 | best_loss=0.02971
Epoch 24/80: current_loss=0.04740 | best_loss=0.02971
Early Stopping at epoch 24
      explained_var=-0.00962 | mse_loss=0.02909
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=-0.07091| avg_loss=0.02895
----------------------------------------------


----------------------------------------------
Params for Trial 14
{'learning_rate': 0.0001, 'weight_decay': 0.0029717934164458413, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03244 | best_loss=0.03244
Epoch 1/80: current_loss=0.03013 | best_loss=0.03013
Epoch 2/80: current_loss=0.02961 | best_loss=0.02961
Epoch 3/80: current_loss=0.02936 | best_loss=0.02936
Epoch 4/80: current_loss=0.02913 | best_loss=0.02913
Epoch 5/80: current_loss=0.02908 | best_loss=0.02908
Epoch 6/80: current_loss=0.02917 | best_loss=0.02908
Epoch 7/80: current_loss=0.02911 | best_loss=0.02908
Epoch 8/80: current_loss=0.02946 | best_loss=0.02908
Epoch 9/80: current_loss=0.02902 | best_loss=0.02902
Epoch 10/80: current_loss=0.02914 | best_loss=0.02902
Epoch 11/80: current_loss=0.02901 | best_loss=0.02901
Epoch 12/80: current_loss=0.02898 | best_loss=0.02898
Epoch 13/80: current_loss=0.02908 | best_loss=0.02898
Epoch 14/80: current_loss=0.02904 | best_loss=0.02898
Epoch 15/80: current_loss=0.02930 | best_loss=0.02898
Epoch 16/80: current_loss=0.02902 | best_loss=0.02898
Epoch 17/80: current_loss=0.02917 | best_loss=0.02898
Epoch 18/80: current_loss=0.02903 | best_loss=0.02898
Epoch 19/80: current_loss=0.02899 | best_loss=0.02898
Epoch 20/80: current_loss=0.02900 | best_loss=0.02898
Epoch 21/80: current_loss=0.02904 | best_loss=0.02898
Epoch 22/80: current_loss=0.02941 | best_loss=0.02898
Epoch 23/80: current_loss=0.02910 | best_loss=0.02898
Epoch 24/80: current_loss=0.02897 | best_loss=0.02897
Epoch 25/80: current_loss=0.02923 | best_loss=0.02897
Epoch 26/80: current_loss=0.02900 | best_loss=0.02897
Epoch 27/80: current_loss=0.02900 | best_loss=0.02897
Epoch 28/80: current_loss=0.02899 | best_loss=0.02897
Epoch 29/80: current_loss=0.02911 | best_loss=0.02897
Epoch 30/80: current_loss=0.02937 | best_loss=0.02897
Epoch 31/80: current_loss=0.02901 | best_loss=0.02897
Epoch 32/80: current_loss=0.02901 | best_loss=0.02897
Epoch 33/80: current_loss=0.02919 | best_loss=0.02897
Epoch 34/80: current_loss=0.02908 | best_loss=0.02897
Epoch 35/80: current_loss=0.02919 | best_loss=0.02897
Epoch 36/80: current_loss=0.02910 | best_loss=0.02897
Epoch 37/80: current_loss=0.02930 | best_loss=0.02897
Epoch 38/80: current_loss=0.02966 | best_loss=0.02897
Epoch 39/80: current_loss=0.02930 | best_loss=0.02897
Epoch 40/80: current_loss=0.02905 | best_loss=0.02897
Epoch 41/80: current_loss=0.02909 | best_loss=0.02897
Epoch 42/80: current_loss=0.02901 | best_loss=0.02897
Epoch 43/80: current_loss=0.02904 | best_loss=0.02897
Epoch 44/80: current_loss=0.02901 | best_loss=0.02897
Early Stopping at epoch 44
      explained_var=0.00420 | mse_loss=0.02803
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02486 | best_loss=0.02486
Epoch 1/80: current_loss=0.02486 | best_loss=0.02486
Epoch 2/80: current_loss=0.02492 | best_loss=0.02486
Epoch 3/80: current_loss=0.02486 | best_loss=0.02486
Epoch 4/80: current_loss=0.02519 | best_loss=0.02486
Epoch 5/80: current_loss=0.02487 | best_loss=0.02486
Epoch 6/80: current_loss=0.02501 | best_loss=0.02486
Epoch 7/80: current_loss=0.02487 | best_loss=0.02486
Epoch 8/80: current_loss=0.02493 | best_loss=0.02486
Epoch 9/80: current_loss=0.02513 | best_loss=0.02486
Epoch 10/80: current_loss=0.02510 | best_loss=0.02486
Epoch 11/80: current_loss=0.02510 | best_loss=0.02486
Epoch 12/80: current_loss=0.02533 | best_loss=0.02486
Epoch 13/80: current_loss=0.02487 | best_loss=0.02486
Epoch 14/80: current_loss=0.02485 | best_loss=0.02485
Epoch 15/80: current_loss=0.02491 | best_loss=0.02485
Epoch 16/80: current_loss=0.02491 | best_loss=0.02485
Epoch 17/80: current_loss=0.02502 | best_loss=0.02485
Epoch 18/80: current_loss=0.02523 | best_loss=0.02485
Epoch 19/80: current_loss=0.02487 | best_loss=0.02485
Epoch 20/80: current_loss=0.02519 | best_loss=0.02485
Epoch 21/80: current_loss=0.02509 | best_loss=0.02485
Epoch 22/80: current_loss=0.02493 | best_loss=0.02485
Epoch 23/80: current_loss=0.02487 | best_loss=0.02485
Epoch 24/80: current_loss=0.02506 | best_loss=0.02485
Epoch 25/80: current_loss=0.02506 | best_loss=0.02485
Epoch 26/80: current_loss=0.02491 | best_loss=0.02485
Epoch 27/80: current_loss=0.02532 | best_loss=0.02485
Epoch 28/80: current_loss=0.02507 | best_loss=0.02485
Epoch 29/80: current_loss=0.02496 | best_loss=0.02485
Epoch 30/80: current_loss=0.02489 | best_loss=0.02485
Epoch 31/80: current_loss=0.02504 | best_loss=0.02485
Epoch 32/80: current_loss=0.02497 | best_loss=0.02485
Epoch 33/80: current_loss=0.02492 | best_loss=0.02485
Epoch 34/80: current_loss=0.02491 | best_loss=0.02485
Early Stopping at epoch 34
      explained_var=0.00288 | mse_loss=0.02513
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02789 | best_loss=0.02789
Epoch 1/80: current_loss=0.02816 | best_loss=0.02789
Epoch 2/80: current_loss=0.02779 | best_loss=0.02779
Epoch 3/80: current_loss=0.02756 | best_loss=0.02756
Epoch 4/80: current_loss=0.02774 | best_loss=0.02756
Epoch 5/80: current_loss=0.02750 | best_loss=0.02750
Epoch 6/80: current_loss=0.02754 | best_loss=0.02750
Epoch 7/80: current_loss=0.02755 | best_loss=0.02750
Epoch 8/80: current_loss=0.02762 | best_loss=0.02750
Epoch 9/80: current_loss=0.02818 | best_loss=0.02750
Epoch 10/80: current_loss=0.02821 | best_loss=0.02750
Epoch 11/80: current_loss=0.02863 | best_loss=0.02750
Epoch 12/80: current_loss=0.02839 | best_loss=0.02750
Epoch 13/80: current_loss=0.02784 | best_loss=0.02750
Epoch 14/80: current_loss=0.02755 | best_loss=0.02750
Epoch 15/80: current_loss=0.02778 | best_loss=0.02750
Epoch 16/80: current_loss=0.02789 | best_loss=0.02750
Epoch 17/80: current_loss=0.02854 | best_loss=0.02750
Epoch 18/80: current_loss=0.02849 | best_loss=0.02750
Epoch 19/80: current_loss=0.02835 | best_loss=0.02750
Epoch 20/80: current_loss=0.02913 | best_loss=0.02750
Epoch 21/80: current_loss=0.02770 | best_loss=0.02750
Epoch 22/80: current_loss=0.02790 | best_loss=0.02750
Epoch 23/80: current_loss=0.02767 | best_loss=0.02750
Epoch 24/80: current_loss=0.02787 | best_loss=0.02750
Epoch 25/80: current_loss=0.02817 | best_loss=0.02750
Early Stopping at epoch 25
      explained_var=-0.01186 | mse_loss=0.02799
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02674 | best_loss=0.02674
Epoch 1/80: current_loss=0.02688 | best_loss=0.02674
Epoch 2/80: current_loss=0.02678 | best_loss=0.02674
Epoch 3/80: current_loss=0.02673 | best_loss=0.02673
Epoch 4/80: current_loss=0.02674 | best_loss=0.02673
Epoch 5/80: current_loss=0.02675 | best_loss=0.02673
Epoch 6/80: current_loss=0.02677 | best_loss=0.02673
Epoch 7/80: current_loss=0.02675 | best_loss=0.02673
Epoch 8/80: current_loss=0.02673 | best_loss=0.02673
Epoch 9/80: current_loss=0.02679 | best_loss=0.02673
Epoch 10/80: current_loss=0.02684 | best_loss=0.02673
Epoch 11/80: current_loss=0.02673 | best_loss=0.02673
Epoch 12/80: current_loss=0.02683 | best_loss=0.02673
Epoch 13/80: current_loss=0.02673 | best_loss=0.02673
Epoch 14/80: current_loss=0.02684 | best_loss=0.02673
Epoch 15/80: current_loss=0.02671 | best_loss=0.02671
Epoch 16/80: current_loss=0.02671 | best_loss=0.02671
Epoch 17/80: current_loss=0.02671 | best_loss=0.02671
Epoch 18/80: current_loss=0.02695 | best_loss=0.02671
Epoch 19/80: current_loss=0.02682 | best_loss=0.02671
Epoch 20/80: current_loss=0.02671 | best_loss=0.02671
Epoch 21/80: current_loss=0.02811 | best_loss=0.02671
Epoch 22/80: current_loss=0.02673 | best_loss=0.02671
Epoch 23/80: current_loss=0.02672 | best_loss=0.02671
Epoch 24/80: current_loss=0.02678 | best_loss=0.02671
Epoch 25/80: current_loss=0.02673 | best_loss=0.02671
Epoch 26/80: current_loss=0.02679 | best_loss=0.02671
Epoch 27/80: current_loss=0.02672 | best_loss=0.02671
Epoch 28/80: current_loss=0.02678 | best_loss=0.02671
Epoch 29/80: current_loss=0.02683 | best_loss=0.02671
Epoch 30/80: current_loss=0.02673 | best_loss=0.02671
Epoch 31/80: current_loss=0.02721 | best_loss=0.02671
Epoch 32/80: current_loss=0.02674 | best_loss=0.02671
Epoch 33/80: current_loss=0.02673 | best_loss=0.02671
Epoch 34/80: current_loss=0.02678 | best_loss=0.02671
Epoch 35/80: current_loss=0.02682 | best_loss=0.02671
Early Stopping at epoch 35
      explained_var=0.00095 | mse_loss=0.02496
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02978 | best_loss=0.02978
Epoch 1/80: current_loss=0.02978 | best_loss=0.02978
Epoch 2/80: current_loss=0.02984 | best_loss=0.02978
Epoch 3/80: current_loss=0.02981 | best_loss=0.02978
Epoch 4/80: current_loss=0.02980 | best_loss=0.02978
Epoch 5/80: current_loss=0.02985 | best_loss=0.02978
Epoch 6/80: current_loss=0.02979 | best_loss=0.02978
Epoch 7/80: current_loss=0.02987 | best_loss=0.02978
Epoch 8/80: current_loss=0.03000 | best_loss=0.02978
Epoch 9/80: current_loss=0.02977 | best_loss=0.02977
Epoch 10/80: current_loss=0.03000 | best_loss=0.02977
Epoch 11/80: current_loss=0.02989 | best_loss=0.02977
Epoch 12/80: current_loss=0.02976 | best_loss=0.02976
Epoch 13/80: current_loss=0.03025 | best_loss=0.02976
Epoch 14/80: current_loss=0.02977 | best_loss=0.02976
Epoch 15/80: current_loss=0.02980 | best_loss=0.02976
Epoch 16/80: current_loss=0.02975 | best_loss=0.02975
Epoch 17/80: current_loss=0.02991 | best_loss=0.02975
Epoch 18/80: current_loss=0.02976 | best_loss=0.02975
Epoch 19/80: current_loss=0.03000 | best_loss=0.02975
Epoch 20/80: current_loss=0.02990 | best_loss=0.02975
Epoch 21/80: current_loss=0.02975 | best_loss=0.02975
Epoch 22/80: current_loss=0.02978 | best_loss=0.02975
Epoch 23/80: current_loss=0.02980 | best_loss=0.02975
Epoch 24/80: current_loss=0.02977 | best_loss=0.02975
Epoch 25/80: current_loss=0.03056 | best_loss=0.02975
Epoch 26/80: current_loss=0.02996 | best_loss=0.02975
Epoch 27/80: current_loss=0.02976 | best_loss=0.02975
Epoch 28/80: current_loss=0.02989 | best_loss=0.02975
Epoch 29/80: current_loss=0.02983 | best_loss=0.02975
Epoch 30/80: current_loss=0.02979 | best_loss=0.02975
Epoch 31/80: current_loss=0.02976 | best_loss=0.02975
Epoch 32/80: current_loss=0.02978 | best_loss=0.02975
Epoch 33/80: current_loss=0.02980 | best_loss=0.02975
Epoch 34/80: current_loss=0.02977 | best_loss=0.02975
Epoch 35/80: current_loss=0.02986 | best_loss=0.02975
Epoch 36/80: current_loss=0.03003 | best_loss=0.02975
Epoch 37/80: current_loss=0.03004 | best_loss=0.02975
Epoch 38/80: current_loss=0.02985 | best_loss=0.02975
Epoch 39/80: current_loss=0.02981 | best_loss=0.02975
Epoch 40/80: current_loss=0.02977 | best_loss=0.02975
Epoch 41/80: current_loss=0.02979 | best_loss=0.02975
Early Stopping at epoch 41
      explained_var=0.00040 | mse_loss=0.02875
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=-0.00068| avg_loss=0.02697
----------------------------------------------


----------------------------------------------
Params for Trial 15
{'learning_rate': 0.0001, 'weight_decay': 0.0014982220926948822, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02921 | best_loss=0.02921
Epoch 1/80: current_loss=0.02938 | best_loss=0.02921
Epoch 2/80: current_loss=0.02892 | best_loss=0.02892
Epoch 3/80: current_loss=0.02924 | best_loss=0.02892
Epoch 4/80: current_loss=0.02967 | best_loss=0.02892
Epoch 5/80: current_loss=0.02889 | best_loss=0.02889
Epoch 6/80: current_loss=0.02885 | best_loss=0.02885
Epoch 7/80: current_loss=0.03062 | best_loss=0.02885
Epoch 8/80: current_loss=0.02917 | best_loss=0.02885
Epoch 9/80: current_loss=0.02898 | best_loss=0.02885
Epoch 10/80: current_loss=0.03084 | best_loss=0.02885
Epoch 11/80: current_loss=0.02901 | best_loss=0.02885
Epoch 12/80: current_loss=0.02988 | best_loss=0.02885
Epoch 13/80: current_loss=0.02915 | best_loss=0.02885
Epoch 14/80: current_loss=0.02909 | best_loss=0.02885
Epoch 15/80: current_loss=0.02944 | best_loss=0.02885
Epoch 16/80: current_loss=0.02910 | best_loss=0.02885
Epoch 17/80: current_loss=0.02896 | best_loss=0.02885
Epoch 18/80: current_loss=0.02903 | best_loss=0.02885
Epoch 19/80: current_loss=0.02904 | best_loss=0.02885
Epoch 20/80: current_loss=0.02892 | best_loss=0.02885
Epoch 21/80: current_loss=0.02908 | best_loss=0.02885
Epoch 22/80: current_loss=0.02901 | best_loss=0.02885
Epoch 23/80: current_loss=0.02901 | best_loss=0.02885
Epoch 24/80: current_loss=0.02896 | best_loss=0.02885
Epoch 25/80: current_loss=0.02892 | best_loss=0.02885
Epoch 26/80: current_loss=0.02921 | best_loss=0.02885
Early Stopping at epoch 26
      explained_var=0.00627 | mse_loss=0.02797
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02492 | best_loss=0.02492
Epoch 1/80: current_loss=0.02587 | best_loss=0.02492
Epoch 2/80: current_loss=0.02506 | best_loss=0.02492
Epoch 3/80: current_loss=0.02535 | best_loss=0.02492
Epoch 4/80: current_loss=0.02528 | best_loss=0.02492
Epoch 5/80: current_loss=0.02512 | best_loss=0.02492
Epoch 6/80: current_loss=0.02492 | best_loss=0.02492
Epoch 7/80: current_loss=0.02501 | best_loss=0.02492
Epoch 8/80: current_loss=0.02518 | best_loss=0.02492
Epoch 9/80: current_loss=0.02491 | best_loss=0.02491
Epoch 10/80: current_loss=0.02503 | best_loss=0.02491
Epoch 11/80: current_loss=0.02535 | best_loss=0.02491
Epoch 12/80: current_loss=0.02524 | best_loss=0.02491
Epoch 13/80: current_loss=0.02490 | best_loss=0.02490
Epoch 14/80: current_loss=0.02487 | best_loss=0.02487
Epoch 15/80: current_loss=0.02487 | best_loss=0.02487
Epoch 16/80: current_loss=0.02486 | best_loss=0.02486
Epoch 17/80: current_loss=0.02497 | best_loss=0.02486
Epoch 18/80: current_loss=0.02490 | best_loss=0.02486
Epoch 19/80: current_loss=0.02492 | best_loss=0.02486
Epoch 20/80: current_loss=0.02494 | best_loss=0.02486
Epoch 21/80: current_loss=0.02516 | best_loss=0.02486
Epoch 22/80: current_loss=0.02540 | best_loss=0.02486
Epoch 23/80: current_loss=0.02494 | best_loss=0.02486
Epoch 24/80: current_loss=0.02501 | best_loss=0.02486
Epoch 25/80: current_loss=0.02492 | best_loss=0.02486
Epoch 26/80: current_loss=0.02511 | best_loss=0.02486
Epoch 27/80: current_loss=0.02501 | best_loss=0.02486
Epoch 28/80: current_loss=0.02493 | best_loss=0.02486
Epoch 29/80: current_loss=0.02527 | best_loss=0.02486
Epoch 30/80: current_loss=0.02491 | best_loss=0.02486
Epoch 31/80: current_loss=0.02486 | best_loss=0.02486
Epoch 32/80: current_loss=0.02499 | best_loss=0.02486
Epoch 33/80: current_loss=0.02487 | best_loss=0.02486
Epoch 34/80: current_loss=0.02485 | best_loss=0.02485
Epoch 35/80: current_loss=0.02493 | best_loss=0.02485
Epoch 36/80: current_loss=0.02502 | best_loss=0.02485
Epoch 37/80: current_loss=0.02513 | best_loss=0.02485
Epoch 38/80: current_loss=0.02535 | best_loss=0.02485
Epoch 39/80: current_loss=0.02500 | best_loss=0.02485
Epoch 40/80: current_loss=0.02509 | best_loss=0.02485
Epoch 41/80: current_loss=0.02493 | best_loss=0.02485
Epoch 42/80: current_loss=0.02487 | best_loss=0.02485
Epoch 43/80: current_loss=0.02494 | best_loss=0.02485
Epoch 44/80: current_loss=0.02495 | best_loss=0.02485
Epoch 45/80: current_loss=0.02628 | best_loss=0.02485
Epoch 46/80: current_loss=0.02489 | best_loss=0.02485
Epoch 47/80: current_loss=0.02514 | best_loss=0.02485
Epoch 48/80: current_loss=0.02498 | best_loss=0.02485
Epoch 49/80: current_loss=0.02596 | best_loss=0.02485
Epoch 50/80: current_loss=0.02502 | best_loss=0.02485
Epoch 51/80: current_loss=0.02503 | best_loss=0.02485
Epoch 52/80: current_loss=0.02516 | best_loss=0.02485
Epoch 53/80: current_loss=0.02506 | best_loss=0.02485
Epoch 54/80: current_loss=0.02513 | best_loss=0.02485
Early Stopping at epoch 54
      explained_var=0.00188 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02761 | best_loss=0.02761
Epoch 1/80: current_loss=0.02800 | best_loss=0.02761
Epoch 2/80: current_loss=0.02752 | best_loss=0.02752
Epoch 3/80: current_loss=0.02780 | best_loss=0.02752
Epoch 4/80: current_loss=0.02847 | best_loss=0.02752
Epoch 5/80: current_loss=0.02736 | best_loss=0.02736
Epoch 6/80: current_loss=0.02744 | best_loss=0.02736
Epoch 7/80: current_loss=0.02798 | best_loss=0.02736
Epoch 8/80: current_loss=0.02758 | best_loss=0.02736
Epoch 9/80: current_loss=0.03085 | best_loss=0.02736
Epoch 10/80: current_loss=0.02767 | best_loss=0.02736
Epoch 11/80: current_loss=0.02766 | best_loss=0.02736
Epoch 12/80: current_loss=0.02782 | best_loss=0.02736
Epoch 13/80: current_loss=0.02734 | best_loss=0.02734
Epoch 14/80: current_loss=0.02928 | best_loss=0.02734
Epoch 15/80: current_loss=0.02775 | best_loss=0.02734
Epoch 16/80: current_loss=0.02735 | best_loss=0.02734
Epoch 17/80: current_loss=0.02731 | best_loss=0.02731
Epoch 18/80: current_loss=0.02834 | best_loss=0.02731
Epoch 19/80: current_loss=0.02781 | best_loss=0.02731
Epoch 20/80: current_loss=0.02755 | best_loss=0.02731
Epoch 21/80: current_loss=0.02784 | best_loss=0.02731
Epoch 22/80: current_loss=0.02746 | best_loss=0.02731
Epoch 23/80: current_loss=0.02871 | best_loss=0.02731
Epoch 24/80: current_loss=0.02937 | best_loss=0.02731
Epoch 25/80: current_loss=0.02745 | best_loss=0.02731
Epoch 26/80: current_loss=0.02735 | best_loss=0.02731
Epoch 27/80: current_loss=0.02730 | best_loss=0.02730
Epoch 28/80: current_loss=0.02732 | best_loss=0.02730
Epoch 29/80: current_loss=0.02808 | best_loss=0.02730
Epoch 30/80: current_loss=0.02733 | best_loss=0.02730
Epoch 31/80: current_loss=0.02805 | best_loss=0.02730
Epoch 32/80: current_loss=0.02733 | best_loss=0.02730
Epoch 33/80: current_loss=0.02772 | best_loss=0.02730
Epoch 34/80: current_loss=0.02757 | best_loss=0.02730
Epoch 35/80: current_loss=0.02796 | best_loss=0.02730
Epoch 36/80: current_loss=0.02787 | best_loss=0.02730
Epoch 37/80: current_loss=0.02762 | best_loss=0.02730
Epoch 38/80: current_loss=0.02749 | best_loss=0.02730
Epoch 39/80: current_loss=0.02731 | best_loss=0.02730
Epoch 40/80: current_loss=0.02781 | best_loss=0.02730
Epoch 41/80: current_loss=0.02737 | best_loss=0.02730
Epoch 42/80: current_loss=0.02745 | best_loss=0.02730
Epoch 43/80: current_loss=0.02935 | best_loss=0.02730
Epoch 44/80: current_loss=0.02731 | best_loss=0.02730
Epoch 45/80: current_loss=0.02773 | best_loss=0.02730
Epoch 46/80: current_loss=0.02772 | best_loss=0.02730
Epoch 47/80: current_loss=0.02738 | best_loss=0.02730
Early Stopping at epoch 47
      explained_var=-0.00754 | mse_loss=0.02777
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02709 | best_loss=0.02709
Epoch 1/80: current_loss=0.02683 | best_loss=0.02683
Epoch 2/80: current_loss=0.02682 | best_loss=0.02682
Epoch 3/80: current_loss=0.02686 | best_loss=0.02682
Epoch 4/80: current_loss=0.02711 | best_loss=0.02682
Epoch 5/80: current_loss=0.02683 | best_loss=0.02682
Epoch 6/80: current_loss=0.02686 | best_loss=0.02682
Epoch 7/80: current_loss=0.02720 | best_loss=0.02682
Epoch 8/80: current_loss=0.02691 | best_loss=0.02682
Epoch 9/80: current_loss=0.02686 | best_loss=0.02682
Epoch 10/80: current_loss=0.02686 | best_loss=0.02682
Epoch 11/80: current_loss=0.02690 | best_loss=0.02682
Epoch 12/80: current_loss=0.02688 | best_loss=0.02682
Epoch 13/80: current_loss=0.02686 | best_loss=0.02682
Epoch 14/80: current_loss=0.02685 | best_loss=0.02682
Epoch 15/80: current_loss=0.02689 | best_loss=0.02682
Epoch 16/80: current_loss=0.02779 | best_loss=0.02682
Epoch 17/80: current_loss=0.02696 | best_loss=0.02682
Epoch 18/80: current_loss=0.02685 | best_loss=0.02682
Epoch 19/80: current_loss=0.02734 | best_loss=0.02682
Epoch 20/80: current_loss=0.02778 | best_loss=0.02682
Epoch 21/80: current_loss=0.02685 | best_loss=0.02682
Epoch 22/80: current_loss=0.02695 | best_loss=0.02682
Early Stopping at epoch 22
      explained_var=0.00121 | mse_loss=0.02499
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03009 | best_loss=0.03009
Epoch 1/80: current_loss=0.02973 | best_loss=0.02973
Epoch 2/80: current_loss=0.02972 | best_loss=0.02972
Epoch 3/80: current_loss=0.02981 | best_loss=0.02972
Epoch 4/80: current_loss=0.03078 | best_loss=0.02972
Epoch 5/80: current_loss=0.02982 | best_loss=0.02972
Epoch 6/80: current_loss=0.02976 | best_loss=0.02972
Epoch 7/80: current_loss=0.02978 | best_loss=0.02972
Epoch 8/80: current_loss=0.02975 | best_loss=0.02972
Epoch 9/80: current_loss=0.03018 | best_loss=0.02972
Epoch 10/80: current_loss=0.02974 | best_loss=0.02972
Epoch 11/80: current_loss=0.02990 | best_loss=0.02972
Epoch 12/80: current_loss=0.03008 | best_loss=0.02972
Epoch 13/80: current_loss=0.02984 | best_loss=0.02972
Epoch 14/80: current_loss=0.02981 | best_loss=0.02972
Epoch 15/80: current_loss=0.02981 | best_loss=0.02972
Epoch 16/80: current_loss=0.02975 | best_loss=0.02972
Epoch 17/80: current_loss=0.03063 | best_loss=0.02972
Epoch 18/80: current_loss=0.03018 | best_loss=0.02972
Epoch 19/80: current_loss=0.02974 | best_loss=0.02972
Epoch 20/80: current_loss=0.02975 | best_loss=0.02972
Epoch 21/80: current_loss=0.02985 | best_loss=0.02972
Epoch 22/80: current_loss=0.02980 | best_loss=0.02972
Early Stopping at epoch 22
      explained_var=0.00113 | mse_loss=0.02872
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.00059| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 16
{'learning_rate': 0.01, 'weight_decay': 0.0038777568216684074, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.22403 | best_loss=0.22403
Epoch 1/80: current_loss=0.07557 | best_loss=0.07557
Epoch 2/80: current_loss=0.05892 | best_loss=0.05892
Epoch 3/80: current_loss=0.03146 | best_loss=0.03146
Epoch 4/80: current_loss=0.03764 | best_loss=0.03146
Epoch 5/80: current_loss=0.03980 | best_loss=0.03146
Epoch 6/80: current_loss=0.03103 | best_loss=0.03103
Epoch 7/80: current_loss=0.04264 | best_loss=0.03103
Epoch 8/80: current_loss=0.03108 | best_loss=0.03103
Epoch 9/80: current_loss=0.03867 | best_loss=0.03103
Epoch 10/80: current_loss=0.04294 | best_loss=0.03103
Epoch 11/80: current_loss=0.04014 | best_loss=0.03103
Epoch 12/80: current_loss=0.04578 | best_loss=0.03103
Epoch 13/80: current_loss=0.04155 | best_loss=0.03103
Epoch 14/80: current_loss=0.03990 | best_loss=0.03103
Epoch 15/80: current_loss=0.03987 | best_loss=0.03103
Epoch 16/80: current_loss=0.05427 | best_loss=0.03103
Epoch 17/80: current_loss=0.04773 | best_loss=0.03103
Epoch 18/80: current_loss=0.06316 | best_loss=0.03103
Epoch 19/80: current_loss=0.03846 | best_loss=0.03103
Epoch 20/80: current_loss=0.04269 | best_loss=0.03103
Epoch 21/80: current_loss=0.03778 | best_loss=0.03103
Epoch 22/80: current_loss=0.04617 | best_loss=0.03103
Epoch 23/80: current_loss=0.03056 | best_loss=0.03056
Epoch 24/80: current_loss=0.03692 | best_loss=0.03056
Epoch 25/80: current_loss=0.03083 | best_loss=0.03056
Epoch 26/80: current_loss=0.03687 | best_loss=0.03056
Epoch 27/80: current_loss=0.03108 | best_loss=0.03056
Epoch 28/80: current_loss=0.02969 | best_loss=0.02969
Epoch 29/80: current_loss=0.03574 | best_loss=0.02969
Epoch 30/80: current_loss=0.03121 | best_loss=0.02969
Epoch 31/80: current_loss=0.02958 | best_loss=0.02958
Epoch 32/80: current_loss=0.03047 | best_loss=0.02958
Epoch 33/80: current_loss=0.02989 | best_loss=0.02958
Epoch 34/80: current_loss=0.03045 | best_loss=0.02958
Epoch 35/80: current_loss=0.02979 | best_loss=0.02958
Epoch 36/80: current_loss=0.03123 | best_loss=0.02958
Epoch 37/80: current_loss=0.02999 | best_loss=0.02958
Epoch 38/80: current_loss=0.02952 | best_loss=0.02952
Epoch 39/80: current_loss=0.02993 | best_loss=0.02952
Epoch 40/80: current_loss=0.02959 | best_loss=0.02952
Epoch 41/80: current_loss=0.02986 | best_loss=0.02952
Epoch 42/80: current_loss=0.02910 | best_loss=0.02910
Epoch 43/80: current_loss=0.02915 | best_loss=0.02910
Epoch 44/80: current_loss=0.02908 | best_loss=0.02908
Epoch 45/80: current_loss=0.02962 | best_loss=0.02908
Epoch 46/80: current_loss=0.03004 | best_loss=0.02908
Epoch 47/80: current_loss=0.02932 | best_loss=0.02908
Epoch 48/80: current_loss=0.02916 | best_loss=0.02908
Epoch 49/80: current_loss=0.02919 | best_loss=0.02908
Epoch 50/80: current_loss=0.02910 | best_loss=0.02908
Epoch 51/80: current_loss=0.02907 | best_loss=0.02907
Epoch 52/80: current_loss=0.02926 | best_loss=0.02907
Epoch 53/80: current_loss=0.02958 | best_loss=0.02907
Epoch 54/80: current_loss=0.02915 | best_loss=0.02907
Epoch 55/80: current_loss=0.02924 | best_loss=0.02907
Epoch 56/80: current_loss=0.02931 | best_loss=0.02907
Epoch 57/80: current_loss=0.02916 | best_loss=0.02907
Epoch 58/80: current_loss=0.02935 | best_loss=0.02907
Epoch 59/80: current_loss=0.02912 | best_loss=0.02907
Epoch 60/80: current_loss=0.02917 | best_loss=0.02907
Epoch 61/80: current_loss=0.02911 | best_loss=0.02907
Epoch 62/80: current_loss=0.02912 | best_loss=0.02907
Epoch 63/80: current_loss=0.02912 | best_loss=0.02907
Epoch 64/80: current_loss=0.02947 | best_loss=0.02907
Epoch 65/80: current_loss=0.02926 | best_loss=0.02907
Epoch 66/80: current_loss=0.02918 | best_loss=0.02907
Epoch 67/80: current_loss=0.02967 | best_loss=0.02907
Epoch 68/80: current_loss=0.02942 | best_loss=0.02907
Epoch 69/80: current_loss=0.02930 | best_loss=0.02907
Epoch 70/80: current_loss=0.02912 | best_loss=0.02907
Epoch 71/80: current_loss=0.02913 | best_loss=0.02907
Early Stopping at epoch 71
      explained_var=0.00053 | mse_loss=0.02815
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.05590 | best_loss=0.05590
Epoch 1/80: current_loss=0.13393 | best_loss=0.05590
Epoch 2/80: current_loss=0.04888 | best_loss=0.04888
Epoch 3/80: current_loss=0.02735 | best_loss=0.02735
Epoch 4/80: current_loss=0.02583 | best_loss=0.02583
Epoch 5/80: current_loss=0.02528 | best_loss=0.02528
Epoch 6/80: current_loss=0.02502 | best_loss=0.02502
Epoch 7/80: current_loss=0.02509 | best_loss=0.02502
Epoch 8/80: current_loss=0.02504 | best_loss=0.02502
Epoch 9/80: current_loss=0.02508 | best_loss=0.02502
Epoch 10/80: current_loss=0.02504 | best_loss=0.02502
Epoch 11/80: current_loss=0.02509 | best_loss=0.02502
Epoch 12/80: current_loss=0.02506 | best_loss=0.02502
Epoch 13/80: current_loss=0.02502 | best_loss=0.02502
Epoch 14/80: current_loss=0.02530 | best_loss=0.02502
Epoch 15/80: current_loss=0.02502 | best_loss=0.02502
Epoch 16/80: current_loss=0.02520 | best_loss=0.02502
Epoch 17/80: current_loss=0.02502 | best_loss=0.02502
Epoch 18/80: current_loss=0.02503 | best_loss=0.02502
Epoch 19/80: current_loss=0.02523 | best_loss=0.02502
Epoch 20/80: current_loss=0.02513 | best_loss=0.02502
Epoch 21/80: current_loss=0.02536 | best_loss=0.02502
Epoch 22/80: current_loss=0.02503 | best_loss=0.02502
Epoch 23/80: current_loss=0.02503 | best_loss=0.02502
Epoch 24/80: current_loss=0.02509 | best_loss=0.02502
Epoch 25/80: current_loss=0.02503 | best_loss=0.02502
Epoch 26/80: current_loss=0.02502 | best_loss=0.02502
Early Stopping at epoch 26
      explained_var=-0.00048 | mse_loss=0.02522
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.05083 | best_loss=0.05083
Epoch 1/80: current_loss=0.02740 | best_loss=0.02740
Epoch 2/80: current_loss=0.02815 | best_loss=0.02740
Epoch 3/80: current_loss=0.03427 | best_loss=0.02740
Epoch 4/80: current_loss=0.02732 | best_loss=0.02732
Epoch 5/80: current_loss=0.02740 | best_loss=0.02732
Epoch 6/80: current_loss=0.02749 | best_loss=0.02732
Epoch 7/80: current_loss=0.02773 | best_loss=0.02732
Epoch 8/80: current_loss=0.02740 | best_loss=0.02732
Epoch 9/80: current_loss=0.02819 | best_loss=0.02732
Epoch 10/80: current_loss=0.02767 | best_loss=0.02732
Epoch 11/80: current_loss=0.02755 | best_loss=0.02732
Epoch 12/80: current_loss=0.02725 | best_loss=0.02725
Epoch 13/80: current_loss=0.02845 | best_loss=0.02725
Epoch 14/80: current_loss=0.02773 | best_loss=0.02725
Epoch 15/80: current_loss=0.02810 | best_loss=0.02725
Epoch 16/80: current_loss=0.02805 | best_loss=0.02725
Epoch 17/80: current_loss=0.02787 | best_loss=0.02725
Epoch 18/80: current_loss=0.02737 | best_loss=0.02725
Epoch 19/80: current_loss=0.02732 | best_loss=0.02725
Epoch 20/80: current_loss=0.02768 | best_loss=0.02725
Epoch 21/80: current_loss=0.02753 | best_loss=0.02725
Epoch 22/80: current_loss=0.02758 | best_loss=0.02725
Epoch 23/80: current_loss=0.02792 | best_loss=0.02725
Epoch 24/80: current_loss=0.02755 | best_loss=0.02725
Epoch 25/80: current_loss=0.02755 | best_loss=0.02725
Epoch 26/80: current_loss=0.02798 | best_loss=0.02725
Epoch 27/80: current_loss=0.02766 | best_loss=0.02725
Epoch 28/80: current_loss=0.02823 | best_loss=0.02725
Epoch 29/80: current_loss=0.02816 | best_loss=0.02725
Epoch 30/80: current_loss=0.02788 | best_loss=0.02725
Epoch 31/80: current_loss=0.02733 | best_loss=0.02725
Epoch 32/80: current_loss=0.02736 | best_loss=0.02725
Early Stopping at epoch 32
      explained_var=-0.00017 | mse_loss=0.02762
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03512 | best_loss=0.03512
Epoch 1/80: current_loss=0.03142 | best_loss=0.03142
Epoch 2/80: current_loss=0.06793 | best_loss=0.03142
Epoch 3/80: current_loss=0.03366 | best_loss=0.03142
Epoch 4/80: current_loss=0.03162 | best_loss=0.03142
Epoch 5/80: current_loss=0.02766 | best_loss=0.02766
Epoch 6/80: current_loss=0.02682 | best_loss=0.02682
Epoch 7/80: current_loss=0.02680 | best_loss=0.02680
Epoch 8/80: current_loss=0.02687 | best_loss=0.02680
Epoch 9/80: current_loss=0.02730 | best_loss=0.02680
Epoch 10/80: current_loss=0.02684 | best_loss=0.02680
Epoch 11/80: current_loss=0.02700 | best_loss=0.02680
Epoch 12/80: current_loss=0.02683 | best_loss=0.02680
Epoch 13/80: current_loss=0.02707 | best_loss=0.02680
Epoch 14/80: current_loss=0.02718 | best_loss=0.02680
Epoch 15/80: current_loss=0.02681 | best_loss=0.02680
Epoch 16/80: current_loss=0.02689 | best_loss=0.02680
Epoch 17/80: current_loss=0.02680 | best_loss=0.02680
Epoch 18/80: current_loss=0.02686 | best_loss=0.02680
Epoch 19/80: current_loss=0.02685 | best_loss=0.02680
Epoch 20/80: current_loss=0.02680 | best_loss=0.02680
Epoch 21/80: current_loss=0.02695 | best_loss=0.02680
Epoch 22/80: current_loss=0.02707 | best_loss=0.02680
Epoch 23/80: current_loss=0.02680 | best_loss=0.02680
Epoch 24/80: current_loss=0.02784 | best_loss=0.02680
Epoch 25/80: current_loss=0.02684 | best_loss=0.02680
Epoch 26/80: current_loss=0.02684 | best_loss=0.02680
Epoch 27/80: current_loss=0.02684 | best_loss=0.02680
Epoch 28/80: current_loss=0.02689 | best_loss=0.02680
Epoch 29/80: current_loss=0.02687 | best_loss=0.02680
Epoch 30/80: current_loss=0.02683 | best_loss=0.02680
Epoch 31/80: current_loss=0.02698 | best_loss=0.02680
Epoch 32/80: current_loss=0.02679 | best_loss=0.02679
Epoch 33/80: current_loss=0.02694 | best_loss=0.02679
Epoch 34/80: current_loss=0.02678 | best_loss=0.02678
Epoch 35/80: current_loss=0.02679 | best_loss=0.02678
Epoch 36/80: current_loss=0.02697 | best_loss=0.02678
Epoch 37/80: current_loss=0.02679 | best_loss=0.02678
Epoch 38/80: current_loss=0.02696 | best_loss=0.02678
Epoch 39/80: current_loss=0.02685 | best_loss=0.02678
Epoch 40/80: current_loss=0.02685 | best_loss=0.02678
Epoch 41/80: current_loss=0.02680 | best_loss=0.02678
Epoch 42/80: current_loss=0.02685 | best_loss=0.02678
Epoch 43/80: current_loss=0.02681 | best_loss=0.02678
Epoch 44/80: current_loss=0.02698 | best_loss=0.02678
Epoch 45/80: current_loss=0.02681 | best_loss=0.02678
Epoch 46/80: current_loss=0.02682 | best_loss=0.02678
Epoch 47/80: current_loss=0.02687 | best_loss=0.02678
Epoch 48/80: current_loss=0.02682 | best_loss=0.02678
Epoch 49/80: current_loss=0.02683 | best_loss=0.02678
Epoch 50/80: current_loss=0.02735 | best_loss=0.02678
Epoch 51/80: current_loss=0.02678 | best_loss=0.02678
Epoch 52/80: current_loss=0.02678 | best_loss=0.02678
Epoch 53/80: current_loss=0.02729 | best_loss=0.02678
Epoch 54/80: current_loss=0.02716 | best_loss=0.02678
Epoch 55/80: current_loss=0.02679 | best_loss=0.02678
Epoch 56/80: current_loss=0.02679 | best_loss=0.02678
Epoch 57/80: current_loss=0.02699 | best_loss=0.02678
Epoch 58/80: current_loss=0.02688 | best_loss=0.02678
Epoch 59/80: current_loss=0.02710 | best_loss=0.02678
Epoch 60/80: current_loss=0.02683 | best_loss=0.02678
Epoch 61/80: current_loss=0.02681 | best_loss=0.02678
Epoch 62/80: current_loss=0.02679 | best_loss=0.02678
Epoch 63/80: current_loss=0.02681 | best_loss=0.02678
Epoch 64/80: current_loss=0.02688 | best_loss=0.02678
Epoch 65/80: current_loss=0.02679 | best_loss=0.02678
Epoch 66/80: current_loss=0.02750 | best_loss=0.02678
Epoch 67/80: current_loss=0.02679 | best_loss=0.02678
Epoch 68/80: current_loss=0.02696 | best_loss=0.02678
Epoch 69/80: current_loss=0.02736 | best_loss=0.02678
Epoch 70/80: current_loss=0.02690 | best_loss=0.02678
Epoch 71/80: current_loss=0.02696 | best_loss=0.02678
Epoch 72/80: current_loss=0.02678 | best_loss=0.02678
Epoch 73/80: current_loss=0.02682 | best_loss=0.02678
Epoch 74/80: current_loss=0.02681 | best_loss=0.02678
Epoch 75/80: current_loss=0.02679 | best_loss=0.02678
Epoch 76/80: current_loss=0.02680 | best_loss=0.02678
Epoch 77/80: current_loss=0.02686 | best_loss=0.02678
Epoch 78/80: current_loss=0.02683 | best_loss=0.02678
Epoch 79/80: current_loss=0.02724 | best_loss=0.02678
      explained_var=-0.00000 | mse_loss=0.02499
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.08153 | best_loss=0.08153
Epoch 1/80: current_loss=0.04014 | best_loss=0.04014
Epoch 2/80: current_loss=0.03509 | best_loss=0.03509
Epoch 3/80: current_loss=0.05462 | best_loss=0.03509
Epoch 4/80: current_loss=0.14306 | best_loss=0.03509
Epoch 5/80: current_loss=0.05224 | best_loss=0.03509
Epoch 6/80: current_loss=0.03790 | best_loss=0.03509
Epoch 7/80: current_loss=0.05545 | best_loss=0.03509
Epoch 8/80: current_loss=0.03481 | best_loss=0.03481
Epoch 9/80: current_loss=0.06481 | best_loss=0.03481
Epoch 10/80: current_loss=0.09056 | best_loss=0.03481
Epoch 11/80: current_loss=0.11862 | best_loss=0.03481
Epoch 12/80: current_loss=0.05006 | best_loss=0.03481
Epoch 13/80: current_loss=0.03099 | best_loss=0.03099
Epoch 14/80: current_loss=0.03273 | best_loss=0.03099
Epoch 15/80: current_loss=0.03899 | best_loss=0.03099
Epoch 16/80: current_loss=0.05685 | best_loss=0.03099
Epoch 17/80: current_loss=0.28346 | best_loss=0.03099
Epoch 18/80: current_loss=0.04339 | best_loss=0.03099
Epoch 19/80: current_loss=0.07615 | best_loss=0.03099
Epoch 20/80: current_loss=0.04402 | best_loss=0.03099
Epoch 21/80: current_loss=0.09969 | best_loss=0.03099
Epoch 22/80: current_loss=0.03036 | best_loss=0.03036
Epoch 23/80: current_loss=0.05668 | best_loss=0.03036
Epoch 24/80: current_loss=0.11648 | best_loss=0.03036
Epoch 25/80: current_loss=0.10102 | best_loss=0.03036
Epoch 26/80: current_loss=0.16697 | best_loss=0.03036
Epoch 27/80: current_loss=0.03224 | best_loss=0.03036
Epoch 28/80: current_loss=0.24131 | best_loss=0.03036
Epoch 29/80: current_loss=0.10288 | best_loss=0.03036
Epoch 30/80: current_loss=0.46197 | best_loss=0.03036
Epoch 31/80: current_loss=0.05422 | best_loss=0.03036
Epoch 32/80: current_loss=0.08758 | best_loss=0.03036
Epoch 33/80: current_loss=0.04075 | best_loss=0.03036
Epoch 34/80: current_loss=0.03940 | best_loss=0.03036
Epoch 35/80: current_loss=0.05512 | best_loss=0.03036
Epoch 36/80: current_loss=0.04360 | best_loss=0.03036
Epoch 37/80: current_loss=0.02994 | best_loss=0.02994
Epoch 38/80: current_loss=0.03796 | best_loss=0.02994
Epoch 39/80: current_loss=0.05182 | best_loss=0.02994
Epoch 40/80: current_loss=0.03034 | best_loss=0.02994
Epoch 41/80: current_loss=0.02980 | best_loss=0.02980
Epoch 42/80: current_loss=0.02999 | best_loss=0.02980
Epoch 43/80: current_loss=0.02980 | best_loss=0.02980
Epoch 44/80: current_loss=0.02981 | best_loss=0.02980
Epoch 45/80: current_loss=0.02980 | best_loss=0.02980
Epoch 46/80: current_loss=0.02981 | best_loss=0.02980
Epoch 47/80: current_loss=0.02980 | best_loss=0.02980
Epoch 48/80: current_loss=0.02982 | best_loss=0.02980
Epoch 49/80: current_loss=0.02981 | best_loss=0.02980
Epoch 50/80: current_loss=0.02995 | best_loss=0.02980
Epoch 51/80: current_loss=0.02982 | best_loss=0.02980
Epoch 52/80: current_loss=0.02982 | best_loss=0.02980
Epoch 53/80: current_loss=0.02979 | best_loss=0.02979
Epoch 54/80: current_loss=0.02987 | best_loss=0.02979
Epoch 55/80: current_loss=0.02984 | best_loss=0.02979
Epoch 56/80: current_loss=0.02985 | best_loss=0.02979
Epoch 57/80: current_loss=0.02980 | best_loss=0.02979
Epoch 58/80: current_loss=0.02995 | best_loss=0.02979
Epoch 59/80: current_loss=0.02992 | best_loss=0.02979
Epoch 60/80: current_loss=0.02989 | best_loss=0.02979
Epoch 61/80: current_loss=0.02980 | best_loss=0.02979
Epoch 62/80: current_loss=0.02982 | best_loss=0.02979
Epoch 63/80: current_loss=0.02980 | best_loss=0.02979
Epoch 64/80: current_loss=0.02981 | best_loss=0.02979
Epoch 65/80: current_loss=0.03014 | best_loss=0.02979
Epoch 66/80: current_loss=0.02989 | best_loss=0.02979
Epoch 67/80: current_loss=0.02990 | best_loss=0.02979
Epoch 68/80: current_loss=0.02983 | best_loss=0.02979
Epoch 69/80: current_loss=0.02980 | best_loss=0.02979
Epoch 70/80: current_loss=0.02984 | best_loss=0.02979
Epoch 71/80: current_loss=0.02987 | best_loss=0.02979
Epoch 72/80: current_loss=0.02980 | best_loss=0.02979
Epoch 73/80: current_loss=0.02980 | best_loss=0.02979
Early Stopping at epoch 73
      explained_var=0.00005 | mse_loss=0.02876
----------------------------------------------
Average early_stopping_point: 40| avg_exp_var=-0.00001| avg_loss=0.02694
----------------------------------------------


----------------------------------------------
Params for Trial 17
{'learning_rate': 0.01, 'weight_decay': 0.0017708999359619434, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.18651 | best_loss=0.18651
Epoch 1/80: current_loss=0.06368 | best_loss=0.06368
Epoch 2/80: current_loss=0.03770 | best_loss=0.03770
Epoch 3/80: current_loss=0.03131 | best_loss=0.03131
Epoch 4/80: current_loss=0.03696 | best_loss=0.03131
Epoch 5/80: current_loss=0.04892 | best_loss=0.03131
Epoch 6/80: current_loss=0.04028 | best_loss=0.03131
Epoch 7/80: current_loss=0.05553 | best_loss=0.03131
Epoch 8/80: current_loss=0.03321 | best_loss=0.03131
Epoch 9/80: current_loss=0.03059 | best_loss=0.03059
Epoch 10/80: current_loss=0.02919 | best_loss=0.02919
Epoch 11/80: current_loss=0.06055 | best_loss=0.02919
Epoch 12/80: current_loss=0.05191 | best_loss=0.02919
Epoch 13/80: current_loss=0.03420 | best_loss=0.02919
Epoch 14/80: current_loss=0.05550 | best_loss=0.02919
Epoch 15/80: current_loss=0.05543 | best_loss=0.02919
Epoch 16/80: current_loss=0.03483 | best_loss=0.02919
Epoch 17/80: current_loss=0.06884 | best_loss=0.02919
Epoch 18/80: current_loss=0.03093 | best_loss=0.02919
Epoch 19/80: current_loss=0.06946 | best_loss=0.02919
Epoch 20/80: current_loss=0.03803 | best_loss=0.02919
Epoch 21/80: current_loss=0.04412 | best_loss=0.02919
Epoch 22/80: current_loss=0.03081 | best_loss=0.02919
Epoch 23/80: current_loss=0.03156 | best_loss=0.02919
Epoch 24/80: current_loss=0.05362 | best_loss=0.02919
Epoch 25/80: current_loss=0.03463 | best_loss=0.02919
Epoch 26/80: current_loss=0.05078 | best_loss=0.02919
Epoch 27/80: current_loss=0.03094 | best_loss=0.02919
Epoch 28/80: current_loss=0.04141 | best_loss=0.02919
Epoch 29/80: current_loss=0.03760 | best_loss=0.02919
Epoch 30/80: current_loss=0.04520 | best_loss=0.02919
Early Stopping at epoch 30
      explained_var=-0.01042 | mse_loss=0.02853
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02990 | best_loss=0.02990
Epoch 1/80: current_loss=0.03388 | best_loss=0.02990
Epoch 2/80: current_loss=0.04597 | best_loss=0.02990
Epoch 3/80: current_loss=0.05617 | best_loss=0.02990
Epoch 4/80: current_loss=0.07869 | best_loss=0.02990
Epoch 5/80: current_loss=0.13111 | best_loss=0.02990
Epoch 6/80: current_loss=0.04329 | best_loss=0.02990
Epoch 7/80: current_loss=0.02880 | best_loss=0.02880
Epoch 8/80: current_loss=0.02681 | best_loss=0.02681
Epoch 9/80: current_loss=0.02532 | best_loss=0.02532
Epoch 10/80: current_loss=0.02502 | best_loss=0.02502
Epoch 11/80: current_loss=0.02503 | best_loss=0.02502
Epoch 12/80: current_loss=0.02503 | best_loss=0.02502
Epoch 13/80: current_loss=0.02502 | best_loss=0.02502
Epoch 14/80: current_loss=0.02506 | best_loss=0.02502
Epoch 15/80: current_loss=0.02506 | best_loss=0.02502
Epoch 16/80: current_loss=0.02502 | best_loss=0.02502
Epoch 17/80: current_loss=0.02510 | best_loss=0.02502
Epoch 18/80: current_loss=0.02504 | best_loss=0.02502
Epoch 19/80: current_loss=0.02503 | best_loss=0.02502
Epoch 20/80: current_loss=0.02502 | best_loss=0.02502
Epoch 21/80: current_loss=0.02503 | best_loss=0.02502
Epoch 22/80: current_loss=0.02520 | best_loss=0.02502
Epoch 23/80: current_loss=0.02502 | best_loss=0.02502
Epoch 24/80: current_loss=0.02511 | best_loss=0.02502
Epoch 25/80: current_loss=0.02503 | best_loss=0.02502
Epoch 26/80: current_loss=0.02503 | best_loss=0.02502
Epoch 27/80: current_loss=0.02513 | best_loss=0.02502
Epoch 28/80: current_loss=0.02522 | best_loss=0.02502
Epoch 29/80: current_loss=0.02504 | best_loss=0.02502
Epoch 30/80: current_loss=0.02505 | best_loss=0.02502
Epoch 31/80: current_loss=0.02505 | best_loss=0.02502
Epoch 32/80: current_loss=0.02515 | best_loss=0.02502
Epoch 33/80: current_loss=0.02503 | best_loss=0.02502
Epoch 34/80: current_loss=0.02503 | best_loss=0.02502
Epoch 35/80: current_loss=0.02505 | best_loss=0.02502
Epoch 36/80: current_loss=0.02509 | best_loss=0.02502
Early Stopping at epoch 36
      explained_var=0.00003 | mse_loss=0.02521
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03893 | best_loss=0.03893
Epoch 1/80: current_loss=0.04584 | best_loss=0.03893
Epoch 2/80: current_loss=0.03380 | best_loss=0.03380
Epoch 3/80: current_loss=0.03165 | best_loss=0.03165
Epoch 4/80: current_loss=0.03138 | best_loss=0.03138
Epoch 5/80: current_loss=0.04291 | best_loss=0.03138
Epoch 6/80: current_loss=0.03751 | best_loss=0.03138
Epoch 7/80: current_loss=0.05539 | best_loss=0.03138
Epoch 8/80: current_loss=0.04948 | best_loss=0.03138
Epoch 9/80: current_loss=0.05439 | best_loss=0.03138
Epoch 10/80: current_loss=0.03786 | best_loss=0.03138
Epoch 11/80: current_loss=0.02804 | best_loss=0.02804
Epoch 12/80: current_loss=0.02777 | best_loss=0.02777
Epoch 13/80: current_loss=0.02747 | best_loss=0.02747
Epoch 14/80: current_loss=0.02794 | best_loss=0.02747
Epoch 15/80: current_loss=0.02789 | best_loss=0.02747
Epoch 16/80: current_loss=0.02739 | best_loss=0.02739
Epoch 17/80: current_loss=0.02816 | best_loss=0.02739
Epoch 18/80: current_loss=0.02753 | best_loss=0.02739
Epoch 19/80: current_loss=0.02818 | best_loss=0.02739
Epoch 20/80: current_loss=0.02765 | best_loss=0.02739
Epoch 21/80: current_loss=0.02830 | best_loss=0.02739
Epoch 22/80: current_loss=0.02725 | best_loss=0.02725
Epoch 23/80: current_loss=0.02769 | best_loss=0.02725
Epoch 24/80: current_loss=0.02727 | best_loss=0.02725
Epoch 25/80: current_loss=0.02803 | best_loss=0.02725
Epoch 26/80: current_loss=0.02845 | best_loss=0.02725
Epoch 27/80: current_loss=0.02734 | best_loss=0.02725
Epoch 28/80: current_loss=0.02748 | best_loss=0.02725
Epoch 29/80: current_loss=0.02734 | best_loss=0.02725
Epoch 30/80: current_loss=0.02753 | best_loss=0.02725
Epoch 31/80: current_loss=0.02741 | best_loss=0.02725
Epoch 32/80: current_loss=0.02811 | best_loss=0.02725
Epoch 33/80: current_loss=0.02761 | best_loss=0.02725
Epoch 34/80: current_loss=0.02821 | best_loss=0.02725
Epoch 35/80: current_loss=0.02768 | best_loss=0.02725
Epoch 36/80: current_loss=0.02745 | best_loss=0.02725
Epoch 37/80: current_loss=0.02751 | best_loss=0.02725
Epoch 38/80: current_loss=0.02757 | best_loss=0.02725
Epoch 39/80: current_loss=0.02758 | best_loss=0.02725
Epoch 40/80: current_loss=0.02762 | best_loss=0.02725
Epoch 41/80: current_loss=0.02800 | best_loss=0.02725
Epoch 42/80: current_loss=0.02798 | best_loss=0.02725
Early Stopping at epoch 42
      explained_var=-0.00001 | mse_loss=0.02762
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.13404 | best_loss=0.13404
Epoch 1/80: current_loss=0.05923 | best_loss=0.05923
Epoch 2/80: current_loss=0.02754 | best_loss=0.02754
Epoch 3/80: current_loss=0.04097 | best_loss=0.02754
Epoch 4/80: current_loss=0.04186 | best_loss=0.02754
Epoch 5/80: current_loss=0.03294 | best_loss=0.02754
Epoch 6/80: current_loss=0.03085 | best_loss=0.02754
Epoch 7/80: current_loss=0.07249 | best_loss=0.02754
Epoch 8/80: current_loss=0.06830 | best_loss=0.02754
Epoch 9/80: current_loss=0.09628 | best_loss=0.02754
Epoch 10/80: current_loss=0.07047 | best_loss=0.02754
Epoch 11/80: current_loss=0.03902 | best_loss=0.02754
Epoch 12/80: current_loss=0.04519 | best_loss=0.02754
Epoch 13/80: current_loss=0.21328 | best_loss=0.02754
Epoch 14/80: current_loss=0.06706 | best_loss=0.02754
Epoch 15/80: current_loss=0.03178 | best_loss=0.02754
Epoch 16/80: current_loss=0.06901 | best_loss=0.02754
Epoch 17/80: current_loss=0.02765 | best_loss=0.02754
Epoch 18/80: current_loss=0.02732 | best_loss=0.02732
Epoch 19/80: current_loss=0.02677 | best_loss=0.02677
Epoch 20/80: current_loss=0.02700 | best_loss=0.02677
Epoch 21/80: current_loss=0.02702 | best_loss=0.02677
Epoch 22/80: current_loss=0.02725 | best_loss=0.02677
Epoch 23/80: current_loss=0.02722 | best_loss=0.02677
Epoch 24/80: current_loss=0.02679 | best_loss=0.02677
Epoch 25/80: current_loss=0.02679 | best_loss=0.02677
Epoch 26/80: current_loss=0.02706 | best_loss=0.02677
Epoch 27/80: current_loss=0.02691 | best_loss=0.02677
Epoch 28/80: current_loss=0.02680 | best_loss=0.02677
Epoch 29/80: current_loss=0.02679 | best_loss=0.02677
Epoch 30/80: current_loss=0.02701 | best_loss=0.02677
Epoch 31/80: current_loss=0.02692 | best_loss=0.02677
Epoch 32/80: current_loss=0.02680 | best_loss=0.02677
Epoch 33/80: current_loss=0.02691 | best_loss=0.02677
Epoch 34/80: current_loss=0.02718 | best_loss=0.02677
Epoch 35/80: current_loss=0.02682 | best_loss=0.02677
Epoch 36/80: current_loss=0.02705 | best_loss=0.02677
Epoch 37/80: current_loss=0.02707 | best_loss=0.02677
Epoch 38/80: current_loss=0.02705 | best_loss=0.02677
Epoch 39/80: current_loss=0.02684 | best_loss=0.02677
Early Stopping at epoch 39
      explained_var=0.00041 | mse_loss=0.02499
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.05116 | best_loss=0.05116
Epoch 1/80: current_loss=0.03703 | best_loss=0.03703
Epoch 2/80: current_loss=0.03356 | best_loss=0.03356
Epoch 3/80: current_loss=0.03352 | best_loss=0.03352
Epoch 4/80: current_loss=0.03914 | best_loss=0.03352
Epoch 5/80: current_loss=0.03668 | best_loss=0.03352
Epoch 6/80: current_loss=0.03328 | best_loss=0.03328
Epoch 7/80: current_loss=0.03032 | best_loss=0.03032
Epoch 8/80: current_loss=0.02978 | best_loss=0.02978
Epoch 9/80: current_loss=0.02993 | best_loss=0.02978
Epoch 10/80: current_loss=0.02999 | best_loss=0.02978
Epoch 11/80: current_loss=0.02981 | best_loss=0.02978
Epoch 12/80: current_loss=0.02988 | best_loss=0.02978
Epoch 13/80: current_loss=0.02986 | best_loss=0.02978
Epoch 14/80: current_loss=0.02988 | best_loss=0.02978
Epoch 15/80: current_loss=0.02980 | best_loss=0.02978
Epoch 16/80: current_loss=0.02981 | best_loss=0.02978
Epoch 17/80: current_loss=0.02981 | best_loss=0.02978
Epoch 18/80: current_loss=0.02980 | best_loss=0.02978
Epoch 19/80: current_loss=0.02980 | best_loss=0.02978
Epoch 20/80: current_loss=0.02981 | best_loss=0.02978
Epoch 21/80: current_loss=0.02980 | best_loss=0.02978
Epoch 22/80: current_loss=0.02983 | best_loss=0.02978
Epoch 23/80: current_loss=0.02981 | best_loss=0.02978
Epoch 24/80: current_loss=0.02981 | best_loss=0.02978
Epoch 25/80: current_loss=0.02984 | best_loss=0.02978
Epoch 26/80: current_loss=0.02982 | best_loss=0.02978
Epoch 27/80: current_loss=0.02980 | best_loss=0.02978
Epoch 28/80: current_loss=0.02980 | best_loss=0.02978
Early Stopping at epoch 28
      explained_var=0.00066 | mse_loss=0.02874
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=-0.00186| avg_loss=0.02702
----------------------------------------------


----------------------------------------------
Params for Trial 18
{'learning_rate': 0.0001, 'weight_decay': 0.0002073006749924273, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03311 | best_loss=0.03311
Epoch 1/80: current_loss=0.03102 | best_loss=0.03102
Epoch 2/80: current_loss=0.03019 | best_loss=0.03019
Epoch 3/80: current_loss=0.03003 | best_loss=0.03003
Epoch 4/80: current_loss=0.02946 | best_loss=0.02946
Epoch 5/80: current_loss=0.02968 | best_loss=0.02946
Epoch 6/80: current_loss=0.02935 | best_loss=0.02935
Epoch 7/80: current_loss=0.02919 | best_loss=0.02919
Epoch 8/80: current_loss=0.02966 | best_loss=0.02919
Epoch 9/80: current_loss=0.02890 | best_loss=0.02890
Epoch 10/80: current_loss=0.02924 | best_loss=0.02890
Epoch 11/80: current_loss=0.02889 | best_loss=0.02889
Epoch 12/80: current_loss=0.02896 | best_loss=0.02889
Epoch 13/80: current_loss=0.02889 | best_loss=0.02889
Epoch 14/80: current_loss=0.02884 | best_loss=0.02884
Epoch 15/80: current_loss=0.02899 | best_loss=0.02884
Epoch 16/80: current_loss=0.02878 | best_loss=0.02878
Epoch 17/80: current_loss=0.02885 | best_loss=0.02878
Epoch 18/80: current_loss=0.02877 | best_loss=0.02877
Epoch 19/80: current_loss=0.02912 | best_loss=0.02877
Epoch 20/80: current_loss=0.02914 | best_loss=0.02877
Epoch 21/80: current_loss=0.02891 | best_loss=0.02877
Epoch 22/80: current_loss=0.02906 | best_loss=0.02877
Epoch 23/80: current_loss=0.02897 | best_loss=0.02877
Epoch 24/80: current_loss=0.02904 | best_loss=0.02877
Epoch 25/80: current_loss=0.02905 | best_loss=0.02877
Epoch 26/80: current_loss=0.02890 | best_loss=0.02877
Epoch 27/80: current_loss=0.02901 | best_loss=0.02877
Epoch 28/80: current_loss=0.02899 | best_loss=0.02877
Epoch 29/80: current_loss=0.02908 | best_loss=0.02877
Epoch 30/80: current_loss=0.02905 | best_loss=0.02877
Epoch 31/80: current_loss=0.02891 | best_loss=0.02877
Epoch 32/80: current_loss=0.02893 | best_loss=0.02877
Epoch 33/80: current_loss=0.02893 | best_loss=0.02877
Epoch 34/80: current_loss=0.02896 | best_loss=0.02877
Epoch 35/80: current_loss=0.02893 | best_loss=0.02877
Epoch 36/80: current_loss=0.02908 | best_loss=0.02877
Epoch 37/80: current_loss=0.02891 | best_loss=0.02877
Epoch 38/80: current_loss=0.02894 | best_loss=0.02877
Early Stopping at epoch 38
      explained_var=0.00910 | mse_loss=0.02790
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02493 | best_loss=0.02493
Epoch 1/80: current_loss=0.02514 | best_loss=0.02493
Epoch 2/80: current_loss=0.02498 | best_loss=0.02493
Epoch 3/80: current_loss=0.02495 | best_loss=0.02493
Epoch 4/80: current_loss=0.02488 | best_loss=0.02488
Epoch 5/80: current_loss=0.02499 | best_loss=0.02488
Epoch 6/80: current_loss=0.02506 | best_loss=0.02488
Epoch 7/80: current_loss=0.02555 | best_loss=0.02488
Epoch 8/80: current_loss=0.02493 | best_loss=0.02488
Epoch 9/80: current_loss=0.02502 | best_loss=0.02488
Epoch 10/80: current_loss=0.02540 | best_loss=0.02488
Epoch 11/80: current_loss=0.02499 | best_loss=0.02488
Epoch 12/80: current_loss=0.02512 | best_loss=0.02488
Epoch 13/80: current_loss=0.02487 | best_loss=0.02487
Epoch 14/80: current_loss=0.02546 | best_loss=0.02487
Epoch 15/80: current_loss=0.02487 | best_loss=0.02487
Epoch 16/80: current_loss=0.02492 | best_loss=0.02487
Epoch 17/80: current_loss=0.02539 | best_loss=0.02487
Epoch 18/80: current_loss=0.02520 | best_loss=0.02487
Epoch 19/80: current_loss=0.02484 | best_loss=0.02484
Epoch 20/80: current_loss=0.02509 | best_loss=0.02484
Epoch 21/80: current_loss=0.02486 | best_loss=0.02484
Epoch 22/80: current_loss=0.02505 | best_loss=0.02484
Epoch 23/80: current_loss=0.02488 | best_loss=0.02484
Epoch 24/80: current_loss=0.02494 | best_loss=0.02484
Epoch 25/80: current_loss=0.02493 | best_loss=0.02484
Epoch 26/80: current_loss=0.02489 | best_loss=0.02484
Epoch 27/80: current_loss=0.02529 | best_loss=0.02484
Epoch 28/80: current_loss=0.02494 | best_loss=0.02484
Epoch 29/80: current_loss=0.02547 | best_loss=0.02484
Epoch 30/80: current_loss=0.02496 | best_loss=0.02484
Epoch 31/80: current_loss=0.02505 | best_loss=0.02484
Epoch 32/80: current_loss=0.02498 | best_loss=0.02484
Epoch 33/80: current_loss=0.02507 | best_loss=0.02484
Epoch 34/80: current_loss=0.02494 | best_loss=0.02484
Epoch 35/80: current_loss=0.02499 | best_loss=0.02484
Epoch 36/80: current_loss=0.02497 | best_loss=0.02484
Epoch 37/80: current_loss=0.02509 | best_loss=0.02484
Epoch 38/80: current_loss=0.02518 | best_loss=0.02484
Epoch 39/80: current_loss=0.02489 | best_loss=0.02484
Early Stopping at epoch 39
      explained_var=0.00245 | mse_loss=0.02514
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02771 | best_loss=0.02771
Epoch 1/80: current_loss=0.02777 | best_loss=0.02771
Epoch 2/80: current_loss=0.02755 | best_loss=0.02755
Epoch 3/80: current_loss=0.02788 | best_loss=0.02755
Epoch 4/80: current_loss=0.02781 | best_loss=0.02755
Epoch 5/80: current_loss=0.02787 | best_loss=0.02755
Epoch 6/80: current_loss=0.02765 | best_loss=0.02755
Epoch 7/80: current_loss=0.02803 | best_loss=0.02755
Epoch 8/80: current_loss=0.02764 | best_loss=0.02755
Epoch 9/80: current_loss=0.02811 | best_loss=0.02755
Epoch 10/80: current_loss=0.02768 | best_loss=0.02755
Epoch 11/80: current_loss=0.02757 | best_loss=0.02755
Epoch 12/80: current_loss=0.02750 | best_loss=0.02750
Epoch 13/80: current_loss=0.02783 | best_loss=0.02750
Epoch 14/80: current_loss=0.02732 | best_loss=0.02732
Epoch 15/80: current_loss=0.02804 | best_loss=0.02732
Epoch 16/80: current_loss=0.02757 | best_loss=0.02732
Epoch 17/80: current_loss=0.02786 | best_loss=0.02732
Epoch 18/80: current_loss=0.02752 | best_loss=0.02732
Epoch 19/80: current_loss=0.02766 | best_loss=0.02732
Epoch 20/80: current_loss=0.02790 | best_loss=0.02732
Epoch 21/80: current_loss=0.02764 | best_loss=0.02732
Epoch 22/80: current_loss=0.02756 | best_loss=0.02732
Epoch 23/80: current_loss=0.02755 | best_loss=0.02732
Epoch 24/80: current_loss=0.02740 | best_loss=0.02732
Epoch 25/80: current_loss=0.02781 | best_loss=0.02732
Epoch 26/80: current_loss=0.02756 | best_loss=0.02732
Epoch 27/80: current_loss=0.02818 | best_loss=0.02732
Epoch 28/80: current_loss=0.02803 | best_loss=0.02732
Epoch 29/80: current_loss=0.02783 | best_loss=0.02732
Epoch 30/80: current_loss=0.02742 | best_loss=0.02732
Epoch 31/80: current_loss=0.02747 | best_loss=0.02732
Epoch 32/80: current_loss=0.02821 | best_loss=0.02732
Epoch 33/80: current_loss=0.02777 | best_loss=0.02732
Epoch 34/80: current_loss=0.02789 | best_loss=0.02732
Early Stopping at epoch 34
      explained_var=-0.00469 | mse_loss=0.02774
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02671 | best_loss=0.02671
Epoch 1/80: current_loss=0.02696 | best_loss=0.02671
Epoch 2/80: current_loss=0.02672 | best_loss=0.02671
Epoch 3/80: current_loss=0.02670 | best_loss=0.02670
Epoch 4/80: current_loss=0.02672 | best_loss=0.02670
Epoch 5/80: current_loss=0.02682 | best_loss=0.02670
Epoch 6/80: current_loss=0.02699 | best_loss=0.02670
Epoch 7/80: current_loss=0.02674 | best_loss=0.02670
Epoch 8/80: current_loss=0.02682 | best_loss=0.02670
Epoch 9/80: current_loss=0.02672 | best_loss=0.02670
Epoch 10/80: current_loss=0.02698 | best_loss=0.02670
Epoch 11/80: current_loss=0.02678 | best_loss=0.02670
Epoch 12/80: current_loss=0.02692 | best_loss=0.02670
Epoch 13/80: current_loss=0.02673 | best_loss=0.02670
Epoch 14/80: current_loss=0.02684 | best_loss=0.02670
Epoch 15/80: current_loss=0.02676 | best_loss=0.02670
Epoch 16/80: current_loss=0.02678 | best_loss=0.02670
Epoch 17/80: current_loss=0.02680 | best_loss=0.02670
Epoch 18/80: current_loss=0.02675 | best_loss=0.02670
Epoch 19/80: current_loss=0.02673 | best_loss=0.02670
Epoch 20/80: current_loss=0.02680 | best_loss=0.02670
Epoch 21/80: current_loss=0.02676 | best_loss=0.02670
Epoch 22/80: current_loss=0.02674 | best_loss=0.02670
Epoch 23/80: current_loss=0.02675 | best_loss=0.02670
Early Stopping at epoch 23
      explained_var=0.00210 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03007 | best_loss=0.03007
Epoch 1/80: current_loss=0.02980 | best_loss=0.02980
Epoch 2/80: current_loss=0.02975 | best_loss=0.02975
Epoch 3/80: current_loss=0.02977 | best_loss=0.02975
Epoch 4/80: current_loss=0.02988 | best_loss=0.02975
Epoch 5/80: current_loss=0.02972 | best_loss=0.02972
Epoch 6/80: current_loss=0.02981 | best_loss=0.02972
Epoch 7/80: current_loss=0.03083 | best_loss=0.02972
Epoch 8/80: current_loss=0.02972 | best_loss=0.02972
Epoch 9/80: current_loss=0.02977 | best_loss=0.02972
Epoch 10/80: current_loss=0.03002 | best_loss=0.02972
Epoch 11/80: current_loss=0.02971 | best_loss=0.02971
Epoch 12/80: current_loss=0.02974 | best_loss=0.02971
Epoch 13/80: current_loss=0.02970 | best_loss=0.02970
Epoch 14/80: current_loss=0.02975 | best_loss=0.02970
Epoch 15/80: current_loss=0.02970 | best_loss=0.02970
Epoch 16/80: current_loss=0.02973 | best_loss=0.02970
Epoch 17/80: current_loss=0.02970 | best_loss=0.02970
Epoch 18/80: current_loss=0.02977 | best_loss=0.02970
Epoch 19/80: current_loss=0.02996 | best_loss=0.02970
Epoch 20/80: current_loss=0.02981 | best_loss=0.02970
Epoch 21/80: current_loss=0.02974 | best_loss=0.02970
Epoch 22/80: current_loss=0.02970 | best_loss=0.02970
Epoch 23/80: current_loss=0.02982 | best_loss=0.02970
Epoch 24/80: current_loss=0.02973 | best_loss=0.02970
Epoch 25/80: current_loss=0.02970 | best_loss=0.02970
Epoch 26/80: current_loss=0.02973 | best_loss=0.02970
Epoch 27/80: current_loss=0.02980 | best_loss=0.02970
Epoch 28/80: current_loss=0.02975 | best_loss=0.02970
Epoch 29/80: current_loss=0.02974 | best_loss=0.02970
Epoch 30/80: current_loss=0.02973 | best_loss=0.02970
Epoch 31/80: current_loss=0.02977 | best_loss=0.02970
Epoch 32/80: current_loss=0.02975 | best_loss=0.02970
Epoch 33/80: current_loss=0.02985 | best_loss=0.02970
Epoch 34/80: current_loss=0.02980 | best_loss=0.02970
Epoch 35/80: current_loss=0.02982 | best_loss=0.02970
Epoch 36/80: current_loss=0.02985 | best_loss=0.02970
Epoch 37/80: current_loss=0.02973 | best_loss=0.02970
Epoch 38/80: current_loss=0.02974 | best_loss=0.02970
Epoch 39/80: current_loss=0.02972 | best_loss=0.02970
Epoch 40/80: current_loss=0.02989 | best_loss=0.02970
Epoch 41/80: current_loss=0.02974 | best_loss=0.02970
Epoch 42/80: current_loss=0.02971 | best_loss=0.02970
Early Stopping at epoch 42
      explained_var=0.00231 | mse_loss=0.02869
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.00225| avg_loss=0.02688
----------------------------------------------


----------------------------------------------
Params for Trial 19
{'learning_rate': 0.0001, 'weight_decay': 0.00020682870637427032, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03328 | best_loss=0.03328
Epoch 1/80: current_loss=0.03174 | best_loss=0.03174
Epoch 2/80: current_loss=0.03068 | best_loss=0.03068
Epoch 3/80: current_loss=0.03003 | best_loss=0.03003
Epoch 4/80: current_loss=0.02980 | best_loss=0.02980
Epoch 5/80: current_loss=0.02937 | best_loss=0.02937
Epoch 6/80: current_loss=0.02936 | best_loss=0.02936
Epoch 7/80: current_loss=0.02922 | best_loss=0.02922
Epoch 8/80: current_loss=0.02919 | best_loss=0.02919
Epoch 9/80: current_loss=0.02929 | best_loss=0.02919
Epoch 10/80: current_loss=0.02909 | best_loss=0.02909
Epoch 11/80: current_loss=0.02891 | best_loss=0.02891
Epoch 12/80: current_loss=0.02884 | best_loss=0.02884
Epoch 13/80: current_loss=0.02893 | best_loss=0.02884
Epoch 14/80: current_loss=0.02890 | best_loss=0.02884
Epoch 15/80: current_loss=0.02912 | best_loss=0.02884
Epoch 16/80: current_loss=0.02980 | best_loss=0.02884
Epoch 17/80: current_loss=0.02883 | best_loss=0.02883
Epoch 18/80: current_loss=0.02892 | best_loss=0.02883
Epoch 19/80: current_loss=0.02888 | best_loss=0.02883
Epoch 20/80: current_loss=0.02905 | best_loss=0.02883
Epoch 21/80: current_loss=0.02914 | best_loss=0.02883
Epoch 22/80: current_loss=0.02888 | best_loss=0.02883
Epoch 23/80: current_loss=0.02882 | best_loss=0.02882
Epoch 24/80: current_loss=0.02883 | best_loss=0.02882
Epoch 25/80: current_loss=0.02889 | best_loss=0.02882
Epoch 26/80: current_loss=0.02904 | best_loss=0.02882
Epoch 27/80: current_loss=0.02897 | best_loss=0.02882
Epoch 28/80: current_loss=0.02896 | best_loss=0.02882
Epoch 29/80: current_loss=0.02883 | best_loss=0.02882
Epoch 30/80: current_loss=0.02932 | best_loss=0.02882
Epoch 31/80: current_loss=0.02990 | best_loss=0.02882
Epoch 32/80: current_loss=0.02915 | best_loss=0.02882
Epoch 33/80: current_loss=0.02887 | best_loss=0.02882
Epoch 34/80: current_loss=0.02894 | best_loss=0.02882
Epoch 35/80: current_loss=0.02890 | best_loss=0.02882
Epoch 36/80: current_loss=0.02886 | best_loss=0.02882
Epoch 37/80: current_loss=0.02882 | best_loss=0.02882
Epoch 38/80: current_loss=0.02919 | best_loss=0.02882
Epoch 39/80: current_loss=0.02889 | best_loss=0.02882
Epoch 40/80: current_loss=0.02908 | best_loss=0.02882
Epoch 41/80: current_loss=0.02916 | best_loss=0.02882
Epoch 42/80: current_loss=0.02886 | best_loss=0.02882
Epoch 43/80: current_loss=0.02899 | best_loss=0.02882
Epoch 44/80: current_loss=0.02904 | best_loss=0.02882
Epoch 45/80: current_loss=0.02916 | best_loss=0.02882
Epoch 46/80: current_loss=0.02910 | best_loss=0.02882
Epoch 47/80: current_loss=0.02905 | best_loss=0.02882
Epoch 48/80: current_loss=0.02893 | best_loss=0.02882
Epoch 49/80: current_loss=0.02890 | best_loss=0.02882
Epoch 50/80: current_loss=0.02911 | best_loss=0.02882
Epoch 51/80: current_loss=0.02894 | best_loss=0.02882
Epoch 52/80: current_loss=0.02887 | best_loss=0.02882
Epoch 53/80: current_loss=0.02896 | best_loss=0.02882
Epoch 54/80: current_loss=0.02899 | best_loss=0.02882
Epoch 55/80: current_loss=0.02874 | best_loss=0.02874
Epoch 56/80: current_loss=0.02880 | best_loss=0.02874
Epoch 57/80: current_loss=0.02873 | best_loss=0.02873
Epoch 58/80: current_loss=0.02890 | best_loss=0.02873
Epoch 59/80: current_loss=0.02898 | best_loss=0.02873
Epoch 60/80: current_loss=0.02882 | best_loss=0.02873
Epoch 61/80: current_loss=0.02944 | best_loss=0.02873
Epoch 62/80: current_loss=0.02888 | best_loss=0.02873
Epoch 63/80: current_loss=0.02892 | best_loss=0.02873
Epoch 64/80: current_loss=0.02892 | best_loss=0.02873
Epoch 65/80: current_loss=0.02890 | best_loss=0.02873
Epoch 66/80: current_loss=0.02893 | best_loss=0.02873
Epoch 67/80: current_loss=0.02900 | best_loss=0.02873
Epoch 68/80: current_loss=0.02903 | best_loss=0.02873
Epoch 69/80: current_loss=0.02904 | best_loss=0.02873
Epoch 70/80: current_loss=0.02892 | best_loss=0.02873
Epoch 71/80: current_loss=0.02902 | best_loss=0.02873
Epoch 72/80: current_loss=0.02885 | best_loss=0.02873
Epoch 73/80: current_loss=0.02953 | best_loss=0.02873
Epoch 74/80: current_loss=0.02914 | best_loss=0.02873
Epoch 75/80: current_loss=0.02891 | best_loss=0.02873
Epoch 76/80: current_loss=0.02886 | best_loss=0.02873
Epoch 77/80: current_loss=0.02891 | best_loss=0.02873
Early Stopping at epoch 77
      explained_var=0.01088 | mse_loss=0.02789
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02496 | best_loss=0.02496
Epoch 1/80: current_loss=0.02498 | best_loss=0.02496
Epoch 2/80: current_loss=0.02535 | best_loss=0.02496
Epoch 3/80: current_loss=0.02503 | best_loss=0.02496
Epoch 4/80: current_loss=0.02504 | best_loss=0.02496
Epoch 5/80: current_loss=0.02613 | best_loss=0.02496
Epoch 6/80: current_loss=0.02530 | best_loss=0.02496
Epoch 7/80: current_loss=0.02508 | best_loss=0.02496
Epoch 8/80: current_loss=0.02526 | best_loss=0.02496
Epoch 9/80: current_loss=0.02501 | best_loss=0.02496
Epoch 10/80: current_loss=0.02493 | best_loss=0.02493
Epoch 11/80: current_loss=0.02503 | best_loss=0.02493
Epoch 12/80: current_loss=0.02502 | best_loss=0.02493
Epoch 13/80: current_loss=0.02497 | best_loss=0.02493
Epoch 14/80: current_loss=0.02482 | best_loss=0.02482
Epoch 15/80: current_loss=0.02490 | best_loss=0.02482
Epoch 16/80: current_loss=0.02496 | best_loss=0.02482
Epoch 17/80: current_loss=0.02489 | best_loss=0.02482
Epoch 18/80: current_loss=0.02489 | best_loss=0.02482
Epoch 19/80: current_loss=0.02486 | best_loss=0.02482
Epoch 20/80: current_loss=0.02489 | best_loss=0.02482
Epoch 21/80: current_loss=0.02533 | best_loss=0.02482
Epoch 22/80: current_loss=0.02491 | best_loss=0.02482
Epoch 23/80: current_loss=0.02502 | best_loss=0.02482
Epoch 24/80: current_loss=0.02497 | best_loss=0.02482
Epoch 25/80: current_loss=0.02530 | best_loss=0.02482
Epoch 26/80: current_loss=0.02495 | best_loss=0.02482
Epoch 27/80: current_loss=0.02518 | best_loss=0.02482
Epoch 28/80: current_loss=0.02546 | best_loss=0.02482
Epoch 29/80: current_loss=0.02569 | best_loss=0.02482
Epoch 30/80: current_loss=0.02510 | best_loss=0.02482
Epoch 31/80: current_loss=0.02517 | best_loss=0.02482
Epoch 32/80: current_loss=0.02500 | best_loss=0.02482
Epoch 33/80: current_loss=0.02527 | best_loss=0.02482
Epoch 34/80: current_loss=0.02530 | best_loss=0.02482
Early Stopping at epoch 34
      explained_var=0.00291 | mse_loss=0.02513
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02748 | best_loss=0.02748
Epoch 1/80: current_loss=0.02756 | best_loss=0.02748
Epoch 2/80: current_loss=0.02821 | best_loss=0.02748
Epoch 3/80: current_loss=0.02821 | best_loss=0.02748
Epoch 4/80: current_loss=0.02740 | best_loss=0.02740
Epoch 5/80: current_loss=0.02842 | best_loss=0.02740
Epoch 6/80: current_loss=0.02795 | best_loss=0.02740
Epoch 7/80: current_loss=0.02778 | best_loss=0.02740
Epoch 8/80: current_loss=0.02801 | best_loss=0.02740
Epoch 9/80: current_loss=0.02808 | best_loss=0.02740
Epoch 10/80: current_loss=0.02772 | best_loss=0.02740
Epoch 11/80: current_loss=0.02748 | best_loss=0.02740
Epoch 12/80: current_loss=0.02761 | best_loss=0.02740
Epoch 13/80: current_loss=0.02774 | best_loss=0.02740
Epoch 14/80: current_loss=0.02774 | best_loss=0.02740
Epoch 15/80: current_loss=0.02799 | best_loss=0.02740
Epoch 16/80: current_loss=0.02840 | best_loss=0.02740
Epoch 17/80: current_loss=0.02764 | best_loss=0.02740
Epoch 18/80: current_loss=0.02791 | best_loss=0.02740
Epoch 19/80: current_loss=0.02757 | best_loss=0.02740
Epoch 20/80: current_loss=0.02769 | best_loss=0.02740
Epoch 21/80: current_loss=0.02810 | best_loss=0.02740
Epoch 22/80: current_loss=0.02781 | best_loss=0.02740
Epoch 23/80: current_loss=0.02746 | best_loss=0.02740
Epoch 24/80: current_loss=0.02824 | best_loss=0.02740
Early Stopping at epoch 24
      explained_var=-0.01095 | mse_loss=0.02788
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02674 | best_loss=0.02674
Epoch 1/80: current_loss=0.02686 | best_loss=0.02674
Epoch 2/80: current_loss=0.02674 | best_loss=0.02674
Epoch 3/80: current_loss=0.02674 | best_loss=0.02674
Epoch 4/80: current_loss=0.02678 | best_loss=0.02674
Epoch 5/80: current_loss=0.02669 | best_loss=0.02669
Epoch 6/80: current_loss=0.02677 | best_loss=0.02669
Epoch 7/80: current_loss=0.02669 | best_loss=0.02669
Epoch 8/80: current_loss=0.02670 | best_loss=0.02669
Epoch 9/80: current_loss=0.02673 | best_loss=0.02669
Epoch 10/80: current_loss=0.02690 | best_loss=0.02669
Epoch 11/80: current_loss=0.02682 | best_loss=0.02669
Epoch 12/80: current_loss=0.02686 | best_loss=0.02669
Epoch 13/80: current_loss=0.02680 | best_loss=0.02669
Epoch 14/80: current_loss=0.02684 | best_loss=0.02669
Epoch 15/80: current_loss=0.02676 | best_loss=0.02669
Epoch 16/80: current_loss=0.02678 | best_loss=0.02669
Epoch 17/80: current_loss=0.02680 | best_loss=0.02669
Epoch 18/80: current_loss=0.02673 | best_loss=0.02669
Epoch 19/80: current_loss=0.02676 | best_loss=0.02669
Epoch 20/80: current_loss=0.02680 | best_loss=0.02669
Epoch 21/80: current_loss=0.02673 | best_loss=0.02669
Epoch 22/80: current_loss=0.02674 | best_loss=0.02669
Epoch 23/80: current_loss=0.02675 | best_loss=0.02669
Epoch 24/80: current_loss=0.02676 | best_loss=0.02669
Epoch 25/80: current_loss=0.02676 | best_loss=0.02669
Epoch 26/80: current_loss=0.02677 | best_loss=0.02669
Epoch 27/80: current_loss=0.02717 | best_loss=0.02669
Early Stopping at epoch 27
      explained_var=0.00260 | mse_loss=0.02492
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02981 | best_loss=0.02981
Epoch 1/80: current_loss=0.03001 | best_loss=0.02981
Epoch 2/80: current_loss=0.02977 | best_loss=0.02977
Epoch 3/80: current_loss=0.02990 | best_loss=0.02977
Epoch 4/80: current_loss=0.02972 | best_loss=0.02972
Epoch 5/80: current_loss=0.02973 | best_loss=0.02972
Epoch 6/80: current_loss=0.02991 | best_loss=0.02972
Epoch 7/80: current_loss=0.02962 | best_loss=0.02962
Epoch 8/80: current_loss=0.02967 | best_loss=0.02962
Epoch 9/80: current_loss=0.02955 | best_loss=0.02955
Epoch 10/80: current_loss=0.02960 | best_loss=0.02955
Epoch 11/80: current_loss=0.02963 | best_loss=0.02955
Epoch 12/80: current_loss=0.02967 | best_loss=0.02955
Epoch 13/80: current_loss=0.02968 | best_loss=0.02955
Epoch 14/80: current_loss=0.02967 | best_loss=0.02955
Epoch 15/80: current_loss=0.02973 | best_loss=0.02955
Epoch 16/80: current_loss=0.02994 | best_loss=0.02955
Epoch 17/80: current_loss=0.02990 | best_loss=0.02955
Epoch 18/80: current_loss=0.02979 | best_loss=0.02955
Epoch 19/80: current_loss=0.02966 | best_loss=0.02955
Epoch 20/80: current_loss=0.02969 | best_loss=0.02955
Epoch 21/80: current_loss=0.02972 | best_loss=0.02955
Epoch 22/80: current_loss=0.02970 | best_loss=0.02955
Epoch 23/80: current_loss=0.02994 | best_loss=0.02955
Epoch 24/80: current_loss=0.02985 | best_loss=0.02955
Epoch 25/80: current_loss=0.02979 | best_loss=0.02955
Epoch 26/80: current_loss=0.02971 | best_loss=0.02955
Epoch 27/80: current_loss=0.02968 | best_loss=0.02955
Epoch 28/80: current_loss=0.02973 | best_loss=0.02955
Epoch 29/80: current_loss=0.03015 | best_loss=0.02955
Early Stopping at epoch 29
      explained_var=0.00504 | mse_loss=0.02862
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00210| avg_loss=0.02689
----------------------------------------------


----------------------------------------------
Params for Trial 20
{'learning_rate': 0.0001, 'weight_decay': 0.007327552214289986, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03479 | best_loss=0.03479
Epoch 1/80: current_loss=0.03310 | best_loss=0.03310
Epoch 2/80: current_loss=0.03149 | best_loss=0.03149
Epoch 3/80: current_loss=0.03112 | best_loss=0.03112
Epoch 4/80: current_loss=0.03020 | best_loss=0.03020
Epoch 5/80: current_loss=0.02998 | best_loss=0.02998
Epoch 6/80: current_loss=0.02978 | best_loss=0.02978
Epoch 7/80: current_loss=0.02978 | best_loss=0.02978
Epoch 8/80: current_loss=0.02969 | best_loss=0.02969
Epoch 9/80: current_loss=0.02950 | best_loss=0.02950
Epoch 10/80: current_loss=0.02933 | best_loss=0.02933
Epoch 11/80: current_loss=0.02935 | best_loss=0.02933
Epoch 12/80: current_loss=0.02985 | best_loss=0.02933
Epoch 13/80: current_loss=0.02922 | best_loss=0.02922
Epoch 14/80: current_loss=0.02968 | best_loss=0.02922
Epoch 15/80: current_loss=0.02929 | best_loss=0.02922
Epoch 16/80: current_loss=0.02910 | best_loss=0.02910
Epoch 17/80: current_loss=0.02904 | best_loss=0.02904
Epoch 18/80: current_loss=0.02902 | best_loss=0.02902
Epoch 19/80: current_loss=0.02901 | best_loss=0.02901
Epoch 20/80: current_loss=0.02901 | best_loss=0.02901
Epoch 21/80: current_loss=0.02918 | best_loss=0.02901
Epoch 22/80: current_loss=0.02901 | best_loss=0.02901
Epoch 23/80: current_loss=0.02903 | best_loss=0.02901
Epoch 24/80: current_loss=0.02901 | best_loss=0.02901
Epoch 25/80: current_loss=0.02904 | best_loss=0.02901
Epoch 26/80: current_loss=0.02905 | best_loss=0.02901
Epoch 27/80: current_loss=0.02897 | best_loss=0.02897
Epoch 28/80: current_loss=0.02896 | best_loss=0.02896
Epoch 29/80: current_loss=0.02909 | best_loss=0.02896
Epoch 30/80: current_loss=0.02923 | best_loss=0.02896
Epoch 31/80: current_loss=0.02901 | best_loss=0.02896
Epoch 32/80: current_loss=0.02896 | best_loss=0.02896
Epoch 33/80: current_loss=0.02896 | best_loss=0.02896
Epoch 34/80: current_loss=0.02924 | best_loss=0.02896
Epoch 35/80: current_loss=0.02925 | best_loss=0.02896
Epoch 36/80: current_loss=0.02895 | best_loss=0.02895
Epoch 37/80: current_loss=0.02898 | best_loss=0.02895
Epoch 38/80: current_loss=0.02902 | best_loss=0.02895
Epoch 39/80: current_loss=0.02905 | best_loss=0.02895
Epoch 40/80: current_loss=0.02912 | best_loss=0.02895
Epoch 41/80: current_loss=0.02907 | best_loss=0.02895
Epoch 42/80: current_loss=0.02893 | best_loss=0.02893
Epoch 43/80: current_loss=0.02955 | best_loss=0.02893
Epoch 44/80: current_loss=0.02899 | best_loss=0.02893
Epoch 45/80: current_loss=0.02893 | best_loss=0.02893
Epoch 46/80: current_loss=0.02893 | best_loss=0.02893
Epoch 47/80: current_loss=0.02902 | best_loss=0.02893
Epoch 48/80: current_loss=0.02905 | best_loss=0.02893
Epoch 49/80: current_loss=0.02895 | best_loss=0.02893
Epoch 50/80: current_loss=0.02894 | best_loss=0.02893
Epoch 51/80: current_loss=0.02905 | best_loss=0.02893
Epoch 52/80: current_loss=0.02901 | best_loss=0.02893
Epoch 53/80: current_loss=0.02927 | best_loss=0.02893
Epoch 54/80: current_loss=0.02899 | best_loss=0.02893
Epoch 55/80: current_loss=0.02894 | best_loss=0.02893
Epoch 56/80: current_loss=0.02897 | best_loss=0.02893
Epoch 57/80: current_loss=0.02909 | best_loss=0.02893
Epoch 58/80: current_loss=0.02900 | best_loss=0.02893
Epoch 59/80: current_loss=0.02894 | best_loss=0.02893
Epoch 60/80: current_loss=0.02894 | best_loss=0.02893
Epoch 61/80: current_loss=0.02902 | best_loss=0.02893
Epoch 62/80: current_loss=0.02942 | best_loss=0.02893
Epoch 63/80: current_loss=0.02900 | best_loss=0.02893
Epoch 64/80: current_loss=0.02898 | best_loss=0.02893
Epoch 65/80: current_loss=0.02895 | best_loss=0.02893
Epoch 66/80: current_loss=0.02895 | best_loss=0.02893
Early Stopping at epoch 66
      explained_var=0.00482 | mse_loss=0.02801
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02495 | best_loss=0.02495
Epoch 1/80: current_loss=0.02493 | best_loss=0.02493
Epoch 2/80: current_loss=0.02488 | best_loss=0.02488
Epoch 3/80: current_loss=0.02520 | best_loss=0.02488
Epoch 4/80: current_loss=0.02511 | best_loss=0.02488
Epoch 5/80: current_loss=0.02588 | best_loss=0.02488
Epoch 6/80: current_loss=0.02489 | best_loss=0.02488
Epoch 7/80: current_loss=0.02513 | best_loss=0.02488
Epoch 8/80: current_loss=0.02506 | best_loss=0.02488
Epoch 9/80: current_loss=0.02503 | best_loss=0.02488
Epoch 10/80: current_loss=0.02492 | best_loss=0.02488
Epoch 11/80: current_loss=0.02492 | best_loss=0.02488
Epoch 12/80: current_loss=0.02492 | best_loss=0.02488
Epoch 13/80: current_loss=0.02493 | best_loss=0.02488
Epoch 14/80: current_loss=0.02510 | best_loss=0.02488
Epoch 15/80: current_loss=0.02492 | best_loss=0.02488
Epoch 16/80: current_loss=0.02494 | best_loss=0.02488
Epoch 17/80: current_loss=0.02493 | best_loss=0.02488
Epoch 18/80: current_loss=0.02489 | best_loss=0.02488
Epoch 19/80: current_loss=0.02495 | best_loss=0.02488
Epoch 20/80: current_loss=0.02497 | best_loss=0.02488
Epoch 21/80: current_loss=0.02497 | best_loss=0.02488
Epoch 22/80: current_loss=0.02495 | best_loss=0.02488
Early Stopping at epoch 22
      explained_var=0.00249 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02817 | best_loss=0.02817
Epoch 1/80: current_loss=0.02846 | best_loss=0.02817
Epoch 2/80: current_loss=0.02832 | best_loss=0.02817
Epoch 3/80: current_loss=0.02851 | best_loss=0.02817
Epoch 4/80: current_loss=0.02837 | best_loss=0.02817
Epoch 5/80: current_loss=0.02806 | best_loss=0.02806
Epoch 6/80: current_loss=0.02822 | best_loss=0.02806
Epoch 7/80: current_loss=0.02792 | best_loss=0.02792
Epoch 8/80: current_loss=0.02790 | best_loss=0.02790
Epoch 9/80: current_loss=0.02790 | best_loss=0.02790
Epoch 10/80: current_loss=0.02776 | best_loss=0.02776
Epoch 11/80: current_loss=0.02773 | best_loss=0.02773
Epoch 12/80: current_loss=0.02825 | best_loss=0.02773
Epoch 13/80: current_loss=0.02807 | best_loss=0.02773
Epoch 14/80: current_loss=0.02830 | best_loss=0.02773
Epoch 15/80: current_loss=0.02788 | best_loss=0.02773
Epoch 16/80: current_loss=0.02765 | best_loss=0.02765
Epoch 17/80: current_loss=0.02761 | best_loss=0.02761
Epoch 18/80: current_loss=0.02763 | best_loss=0.02761
Epoch 19/80: current_loss=0.02810 | best_loss=0.02761
Epoch 20/80: current_loss=0.02855 | best_loss=0.02761
Epoch 21/80: current_loss=0.02758 | best_loss=0.02758
Epoch 22/80: current_loss=0.02758 | best_loss=0.02758
Epoch 23/80: current_loss=0.02770 | best_loss=0.02758
Epoch 24/80: current_loss=0.02825 | best_loss=0.02758
Epoch 25/80: current_loss=0.02800 | best_loss=0.02758
Epoch 26/80: current_loss=0.02768 | best_loss=0.02758
Epoch 27/80: current_loss=0.02761 | best_loss=0.02758
Epoch 28/80: current_loss=0.02797 | best_loss=0.02758
Epoch 29/80: current_loss=0.02790 | best_loss=0.02758
Epoch 30/80: current_loss=0.02770 | best_loss=0.02758
Epoch 31/80: current_loss=0.02768 | best_loss=0.02758
Epoch 32/80: current_loss=0.02762 | best_loss=0.02758
Epoch 33/80: current_loss=0.02772 | best_loss=0.02758
Epoch 34/80: current_loss=0.02806 | best_loss=0.02758
Epoch 35/80: current_loss=0.02793 | best_loss=0.02758
Epoch 36/80: current_loss=0.02777 | best_loss=0.02758
Epoch 37/80: current_loss=0.02819 | best_loss=0.02758
Epoch 38/80: current_loss=0.02796 | best_loss=0.02758
Epoch 39/80: current_loss=0.02819 | best_loss=0.02758
Epoch 40/80: current_loss=0.02829 | best_loss=0.02758
Epoch 41/80: current_loss=0.02832 | best_loss=0.02758
Early Stopping at epoch 41
      explained_var=-0.01466 | mse_loss=0.02808
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02668 | best_loss=0.02668
Epoch 1/80: current_loss=0.02672 | best_loss=0.02668
Epoch 2/80: current_loss=0.02665 | best_loss=0.02665
Epoch 3/80: current_loss=0.02672 | best_loss=0.02665
Epoch 4/80: current_loss=0.02676 | best_loss=0.02665
Epoch 5/80: current_loss=0.02690 | best_loss=0.02665
Epoch 6/80: current_loss=0.02666 | best_loss=0.02665
Epoch 7/80: current_loss=0.02666 | best_loss=0.02665
Epoch 8/80: current_loss=0.02694 | best_loss=0.02665
Epoch 9/80: current_loss=0.02667 | best_loss=0.02665
Epoch 10/80: current_loss=0.02674 | best_loss=0.02665
Epoch 11/80: current_loss=0.02676 | best_loss=0.02665
Epoch 12/80: current_loss=0.02667 | best_loss=0.02665
Epoch 13/80: current_loss=0.02666 | best_loss=0.02665
Epoch 14/80: current_loss=0.02670 | best_loss=0.02665
Epoch 15/80: current_loss=0.02675 | best_loss=0.02665
Epoch 16/80: current_loss=0.02692 | best_loss=0.02665
Epoch 17/80: current_loss=0.02668 | best_loss=0.02665
Epoch 18/80: current_loss=0.02667 | best_loss=0.02665
Epoch 19/80: current_loss=0.02675 | best_loss=0.02665
Epoch 20/80: current_loss=0.02677 | best_loss=0.02665
Epoch 21/80: current_loss=0.02673 | best_loss=0.02665
Epoch 22/80: current_loss=0.02706 | best_loss=0.02665
Early Stopping at epoch 22
      explained_var=0.00135 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02997 | best_loss=0.02997
Epoch 1/80: current_loss=0.02983 | best_loss=0.02983
Epoch 2/80: current_loss=0.02982 | best_loss=0.02982
Epoch 3/80: current_loss=0.02983 | best_loss=0.02982
Epoch 4/80: current_loss=0.02988 | best_loss=0.02982
Epoch 5/80: current_loss=0.02984 | best_loss=0.02982
Epoch 6/80: current_loss=0.02985 | best_loss=0.02982
Epoch 7/80: current_loss=0.02990 | best_loss=0.02982
Epoch 8/80: current_loss=0.02984 | best_loss=0.02982
Epoch 9/80: current_loss=0.02979 | best_loss=0.02979
Epoch 10/80: current_loss=0.02981 | best_loss=0.02979
Epoch 11/80: current_loss=0.02979 | best_loss=0.02979
Epoch 12/80: current_loss=0.02995 | best_loss=0.02979
Epoch 13/80: current_loss=0.02979 | best_loss=0.02979
Epoch 14/80: current_loss=0.02981 | best_loss=0.02979
Epoch 15/80: current_loss=0.03010 | best_loss=0.02979
Epoch 16/80: current_loss=0.02998 | best_loss=0.02979
Epoch 17/80: current_loss=0.02980 | best_loss=0.02979
Epoch 18/80: current_loss=0.02980 | best_loss=0.02979
Epoch 19/80: current_loss=0.02984 | best_loss=0.02979
Epoch 20/80: current_loss=0.02979 | best_loss=0.02979
Epoch 21/80: current_loss=0.02977 | best_loss=0.02977
Epoch 22/80: current_loss=0.02977 | best_loss=0.02977
Epoch 23/80: current_loss=0.02980 | best_loss=0.02977
Epoch 24/80: current_loss=0.03025 | best_loss=0.02977
Epoch 25/80: current_loss=0.02979 | best_loss=0.02977
Epoch 26/80: current_loss=0.02980 | best_loss=0.02977
Epoch 27/80: current_loss=0.02977 | best_loss=0.02977
Epoch 28/80: current_loss=0.02977 | best_loss=0.02977
Epoch 29/80: current_loss=0.02977 | best_loss=0.02977
Epoch 30/80: current_loss=0.02978 | best_loss=0.02977
Epoch 31/80: current_loss=0.02991 | best_loss=0.02977
Epoch 32/80: current_loss=0.02977 | best_loss=0.02977
Epoch 33/80: current_loss=0.02978 | best_loss=0.02977
Epoch 34/80: current_loss=0.02988 | best_loss=0.02977
Epoch 35/80: current_loss=0.02976 | best_loss=0.02976
Epoch 36/80: current_loss=0.02979 | best_loss=0.02976
Epoch 37/80: current_loss=0.02982 | best_loss=0.02976
Epoch 38/80: current_loss=0.02983 | best_loss=0.02976
Epoch 39/80: current_loss=0.02993 | best_loss=0.02976
Epoch 40/80: current_loss=0.02987 | best_loss=0.02976
Epoch 41/80: current_loss=0.02976 | best_loss=0.02976
Epoch 42/80: current_loss=0.02981 | best_loss=0.02976
Epoch 43/80: current_loss=0.02976 | best_loss=0.02976
Epoch 44/80: current_loss=0.02976 | best_loss=0.02976
Epoch 45/80: current_loss=0.02978 | best_loss=0.02976
Epoch 46/80: current_loss=0.02977 | best_loss=0.02976
Epoch 47/80: current_loss=0.02977 | best_loss=0.02976
Epoch 48/80: current_loss=0.02990 | best_loss=0.02976
Epoch 49/80: current_loss=0.02976 | best_loss=0.02976
Epoch 50/80: current_loss=0.03029 | best_loss=0.02976
Epoch 51/80: current_loss=0.02985 | best_loss=0.02976
Epoch 52/80: current_loss=0.03007 | best_loss=0.02976
Epoch 53/80: current_loss=0.02978 | best_loss=0.02976
Epoch 54/80: current_loss=0.02976 | best_loss=0.02976
Epoch 55/80: current_loss=0.02979 | best_loss=0.02976
Epoch 56/80: current_loss=0.02976 | best_loss=0.02976
Epoch 57/80: current_loss=0.02981 | best_loss=0.02976
Epoch 58/80: current_loss=0.02978 | best_loss=0.02976
Epoch 59/80: current_loss=0.02976 | best_loss=0.02976
Epoch 60/80: current_loss=0.03002 | best_loss=0.02976
Epoch 61/80: current_loss=0.02977 | best_loss=0.02976
Epoch 62/80: current_loss=0.02978 | best_loss=0.02976
Epoch 63/80: current_loss=0.02980 | best_loss=0.02976
Early Stopping at epoch 63
      explained_var=0.00017 | mse_loss=0.02875
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=-0.00116| avg_loss=0.02699
----------------------------------------------


----------------------------------------------
Params for Trial 21
{'learning_rate': 0.0001, 'weight_decay': 3.6919435707952848e-06, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03301 | best_loss=0.03301
Epoch 1/80: current_loss=0.03096 | best_loss=0.03096
Epoch 2/80: current_loss=0.03022 | best_loss=0.03022
Epoch 3/80: current_loss=0.02983 | best_loss=0.02983
Epoch 4/80: current_loss=0.02976 | best_loss=0.02976
Epoch 5/80: current_loss=0.02953 | best_loss=0.02953
Epoch 6/80: current_loss=0.02935 | best_loss=0.02935
Epoch 7/80: current_loss=0.02946 | best_loss=0.02935
Epoch 8/80: current_loss=0.02929 | best_loss=0.02929
Epoch 9/80: current_loss=0.02932 | best_loss=0.02929
Epoch 10/80: current_loss=0.02922 | best_loss=0.02922
Epoch 11/80: current_loss=0.02917 | best_loss=0.02917
Epoch 12/80: current_loss=0.02923 | best_loss=0.02917
Epoch 13/80: current_loss=0.02936 | best_loss=0.02917
Epoch 14/80: current_loss=0.02894 | best_loss=0.02894
Epoch 15/80: current_loss=0.02902 | best_loss=0.02894
Epoch 16/80: current_loss=0.02897 | best_loss=0.02894
Epoch 17/80: current_loss=0.02924 | best_loss=0.02894
Epoch 18/80: current_loss=0.02913 | best_loss=0.02894
Epoch 19/80: current_loss=0.02902 | best_loss=0.02894
Epoch 20/80: current_loss=0.02905 | best_loss=0.02894
Epoch 21/80: current_loss=0.02900 | best_loss=0.02894
Epoch 22/80: current_loss=0.02902 | best_loss=0.02894
Epoch 23/80: current_loss=0.02979 | best_loss=0.02894
Epoch 24/80: current_loss=0.02904 | best_loss=0.02894
Epoch 25/80: current_loss=0.02969 | best_loss=0.02894
Epoch 26/80: current_loss=0.02902 | best_loss=0.02894
Epoch 27/80: current_loss=0.02907 | best_loss=0.02894
Epoch 28/80: current_loss=0.02914 | best_loss=0.02894
Epoch 29/80: current_loss=0.02918 | best_loss=0.02894
Epoch 30/80: current_loss=0.02910 | best_loss=0.02894
Epoch 31/80: current_loss=0.02909 | best_loss=0.02894
Epoch 32/80: current_loss=0.02923 | best_loss=0.02894
Epoch 33/80: current_loss=0.02917 | best_loss=0.02894
Epoch 34/80: current_loss=0.02910 | best_loss=0.02894
Early Stopping at epoch 34
      explained_var=0.00395 | mse_loss=0.02804
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02507 | best_loss=0.02507
Epoch 1/80: current_loss=0.02493 | best_loss=0.02493
Epoch 2/80: current_loss=0.02483 | best_loss=0.02483
Epoch 3/80: current_loss=0.02483 | best_loss=0.02483
Epoch 4/80: current_loss=0.02508 | best_loss=0.02483
Epoch 5/80: current_loss=0.02511 | best_loss=0.02483
Epoch 6/80: current_loss=0.02508 | best_loss=0.02483
Epoch 7/80: current_loss=0.02492 | best_loss=0.02483
Epoch 8/80: current_loss=0.02493 | best_loss=0.02483
Epoch 9/80: current_loss=0.02493 | best_loss=0.02483
Epoch 10/80: current_loss=0.02505 | best_loss=0.02483
Epoch 11/80: current_loss=0.02526 | best_loss=0.02483
Epoch 12/80: current_loss=0.02525 | best_loss=0.02483
Epoch 13/80: current_loss=0.02534 | best_loss=0.02483
Epoch 14/80: current_loss=0.02504 | best_loss=0.02483
Epoch 15/80: current_loss=0.02515 | best_loss=0.02483
Epoch 16/80: current_loss=0.02502 | best_loss=0.02483
Epoch 17/80: current_loss=0.02498 | best_loss=0.02483
Epoch 18/80: current_loss=0.02502 | best_loss=0.02483
Epoch 19/80: current_loss=0.02496 | best_loss=0.02483
Epoch 20/80: current_loss=0.02496 | best_loss=0.02483
Epoch 21/80: current_loss=0.02495 | best_loss=0.02483
Epoch 22/80: current_loss=0.02500 | best_loss=0.02483
Epoch 23/80: current_loss=0.02509 | best_loss=0.02483
Early Stopping at epoch 23
      explained_var=0.00126 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02739 | best_loss=0.02739
Epoch 1/80: current_loss=0.02762 | best_loss=0.02739
Epoch 2/80: current_loss=0.02869 | best_loss=0.02739
Epoch 3/80: current_loss=0.02755 | best_loss=0.02739
Epoch 4/80: current_loss=0.02873 | best_loss=0.02739
Epoch 5/80: current_loss=0.02749 | best_loss=0.02739
Epoch 6/80: current_loss=0.02745 | best_loss=0.02739
Epoch 7/80: current_loss=0.02746 | best_loss=0.02739
Epoch 8/80: current_loss=0.02758 | best_loss=0.02739
Epoch 9/80: current_loss=0.02805 | best_loss=0.02739
Epoch 10/80: current_loss=0.02857 | best_loss=0.02739
Epoch 11/80: current_loss=0.02846 | best_loss=0.02739
Epoch 12/80: current_loss=0.02820 | best_loss=0.02739
Epoch 13/80: current_loss=0.02806 | best_loss=0.02739
Epoch 14/80: current_loss=0.02779 | best_loss=0.02739
Epoch 15/80: current_loss=0.02771 | best_loss=0.02739
Epoch 16/80: current_loss=0.02828 | best_loss=0.02739
Epoch 17/80: current_loss=0.02752 | best_loss=0.02739
Epoch 18/80: current_loss=0.02787 | best_loss=0.02739
Epoch 19/80: current_loss=0.02743 | best_loss=0.02739
Epoch 20/80: current_loss=0.02739 | best_loss=0.02739
Epoch 21/80: current_loss=0.02744 | best_loss=0.02739
Epoch 22/80: current_loss=0.02870 | best_loss=0.02739
Epoch 23/80: current_loss=0.02833 | best_loss=0.02739
Epoch 24/80: current_loss=0.02790 | best_loss=0.02739
Epoch 25/80: current_loss=0.02747 | best_loss=0.02739
Epoch 26/80: current_loss=0.02743 | best_loss=0.02739
Epoch 27/80: current_loss=0.02767 | best_loss=0.02739
Epoch 28/80: current_loss=0.02811 | best_loss=0.02739
Epoch 29/80: current_loss=0.02735 | best_loss=0.02735
Epoch 30/80: current_loss=0.02768 | best_loss=0.02735
Epoch 31/80: current_loss=0.02782 | best_loss=0.02735
Epoch 32/80: current_loss=0.02773 | best_loss=0.02735
Epoch 33/80: current_loss=0.02744 | best_loss=0.02735
Epoch 34/80: current_loss=0.02735 | best_loss=0.02735
Epoch 35/80: current_loss=0.02778 | best_loss=0.02735
Epoch 36/80: current_loss=0.02773 | best_loss=0.02735
Epoch 37/80: current_loss=0.02802 | best_loss=0.02735
Epoch 38/80: current_loss=0.02779 | best_loss=0.02735
Epoch 39/80: current_loss=0.02800 | best_loss=0.02735
Epoch 40/80: current_loss=0.02778 | best_loss=0.02735
Epoch 41/80: current_loss=0.02776 | best_loss=0.02735
Epoch 42/80: current_loss=0.02789 | best_loss=0.02735
Epoch 43/80: current_loss=0.02752 | best_loss=0.02735
Epoch 44/80: current_loss=0.02744 | best_loss=0.02735
Epoch 45/80: current_loss=0.02810 | best_loss=0.02735
Epoch 46/80: current_loss=0.02758 | best_loss=0.02735
Epoch 47/80: current_loss=0.02769 | best_loss=0.02735
Epoch 48/80: current_loss=0.02758 | best_loss=0.02735
Epoch 49/80: current_loss=0.02756 | best_loss=0.02735
Early Stopping at epoch 49
      explained_var=-0.00273 | mse_loss=0.02776
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02723 | best_loss=0.02723
Epoch 1/80: current_loss=0.02677 | best_loss=0.02677
Epoch 2/80: current_loss=0.02782 | best_loss=0.02677
Epoch 3/80: current_loss=0.02682 | best_loss=0.02677
Epoch 4/80: current_loss=0.02732 | best_loss=0.02677
Epoch 5/80: current_loss=0.02675 | best_loss=0.02675
Epoch 6/80: current_loss=0.02707 | best_loss=0.02675
Epoch 7/80: current_loss=0.02685 | best_loss=0.02675
Epoch 8/80: current_loss=0.02679 | best_loss=0.02675
Epoch 9/80: current_loss=0.02695 | best_loss=0.02675
Epoch 10/80: current_loss=0.02677 | best_loss=0.02675
Epoch 11/80: current_loss=0.02680 | best_loss=0.02675
Epoch 12/80: current_loss=0.02683 | best_loss=0.02675
Epoch 13/80: current_loss=0.02679 | best_loss=0.02675
Epoch 14/80: current_loss=0.02694 | best_loss=0.02675
Epoch 15/80: current_loss=0.02704 | best_loss=0.02675
Epoch 16/80: current_loss=0.02680 | best_loss=0.02675
Epoch 17/80: current_loss=0.02700 | best_loss=0.02675
Epoch 18/80: current_loss=0.02680 | best_loss=0.02675
Epoch 19/80: current_loss=0.02681 | best_loss=0.02675
Epoch 20/80: current_loss=0.02675 | best_loss=0.02675
Epoch 21/80: current_loss=0.02693 | best_loss=0.02675
Epoch 22/80: current_loss=0.02698 | best_loss=0.02675
Epoch 23/80: current_loss=0.02674 | best_loss=0.02674
Epoch 24/80: current_loss=0.02670 | best_loss=0.02670
Epoch 25/80: current_loss=0.02670 | best_loss=0.02670
Epoch 26/80: current_loss=0.02679 | best_loss=0.02670
Epoch 27/80: current_loss=0.02694 | best_loss=0.02670
Epoch 28/80: current_loss=0.02669 | best_loss=0.02669
Epoch 29/80: current_loss=0.02667 | best_loss=0.02667
Epoch 30/80: current_loss=0.02671 | best_loss=0.02667
Epoch 31/80: current_loss=0.02714 | best_loss=0.02667
Epoch 32/80: current_loss=0.02717 | best_loss=0.02667
Epoch 33/80: current_loss=0.02673 | best_loss=0.02667
Epoch 34/80: current_loss=0.02680 | best_loss=0.02667
Epoch 35/80: current_loss=0.02682 | best_loss=0.02667
Epoch 36/80: current_loss=0.02671 | best_loss=0.02667
Epoch 37/80: current_loss=0.02672 | best_loss=0.02667
Epoch 38/80: current_loss=0.02683 | best_loss=0.02667
Epoch 39/80: current_loss=0.02680 | best_loss=0.02667
Epoch 40/80: current_loss=0.02682 | best_loss=0.02667
Epoch 41/80: current_loss=0.02688 | best_loss=0.02667
Epoch 42/80: current_loss=0.02675 | best_loss=0.02667
Epoch 43/80: current_loss=0.02684 | best_loss=0.02667
Epoch 44/80: current_loss=0.02680 | best_loss=0.02667
Epoch 45/80: current_loss=0.02679 | best_loss=0.02667
Epoch 46/80: current_loss=0.02677 | best_loss=0.02667
Epoch 47/80: current_loss=0.02679 | best_loss=0.02667
Epoch 48/80: current_loss=0.02681 | best_loss=0.02667
Epoch 49/80: current_loss=0.02687 | best_loss=0.02667
Early Stopping at epoch 49
      explained_var=0.00332 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02972 | best_loss=0.02972
Epoch 1/80: current_loss=0.02977 | best_loss=0.02972
Epoch 2/80: current_loss=0.02979 | best_loss=0.02972
Epoch 3/80: current_loss=0.02993 | best_loss=0.02972
Epoch 4/80: current_loss=0.03012 | best_loss=0.02972
Epoch 5/80: current_loss=0.03008 | best_loss=0.02972
Epoch 6/80: current_loss=0.02981 | best_loss=0.02972
Epoch 7/80: current_loss=0.02978 | best_loss=0.02972
Epoch 8/80: current_loss=0.02981 | best_loss=0.02972
Epoch 9/80: current_loss=0.02982 | best_loss=0.02972
Epoch 10/80: current_loss=0.03006 | best_loss=0.02972
Epoch 11/80: current_loss=0.02967 | best_loss=0.02967
Epoch 12/80: current_loss=0.02970 | best_loss=0.02967
Epoch 13/80: current_loss=0.02977 | best_loss=0.02967
Epoch 14/80: current_loss=0.02992 | best_loss=0.02967
Epoch 15/80: current_loss=0.02994 | best_loss=0.02967
Epoch 16/80: current_loss=0.02980 | best_loss=0.02967
Epoch 17/80: current_loss=0.02998 | best_loss=0.02967
Epoch 18/80: current_loss=0.02981 | best_loss=0.02967
Epoch 19/80: current_loss=0.02974 | best_loss=0.02967
Epoch 20/80: current_loss=0.02985 | best_loss=0.02967
Epoch 21/80: current_loss=0.02990 | best_loss=0.02967
Epoch 22/80: current_loss=0.02986 | best_loss=0.02967
Epoch 23/80: current_loss=0.03003 | best_loss=0.02967
Epoch 24/80: current_loss=0.02983 | best_loss=0.02967
Epoch 25/80: current_loss=0.02980 | best_loss=0.02967
Epoch 26/80: current_loss=0.02987 | best_loss=0.02967
Epoch 27/80: current_loss=0.02978 | best_loss=0.02967
Epoch 28/80: current_loss=0.02983 | best_loss=0.02967
Epoch 29/80: current_loss=0.02974 | best_loss=0.02967
Epoch 30/80: current_loss=0.02977 | best_loss=0.02967
Epoch 31/80: current_loss=0.02978 | best_loss=0.02967
Early Stopping at epoch 31
      explained_var=0.00229 | mse_loss=0.02869
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00162| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 22
{'learning_rate': 0.0001, 'weight_decay': 0.0009543568619709013, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03174 | best_loss=0.03174
Epoch 1/80: current_loss=0.03061 | best_loss=0.03061
Epoch 2/80: current_loss=0.03010 | best_loss=0.03010
Epoch 3/80: current_loss=0.02977 | best_loss=0.02977
Epoch 4/80: current_loss=0.02992 | best_loss=0.02977
Epoch 5/80: current_loss=0.02937 | best_loss=0.02937
Epoch 6/80: current_loss=0.03000 | best_loss=0.02937
Epoch 7/80: current_loss=0.02971 | best_loss=0.02937
Epoch 8/80: current_loss=0.02926 | best_loss=0.02926
Epoch 9/80: current_loss=0.02911 | best_loss=0.02911
Epoch 10/80: current_loss=0.02916 | best_loss=0.02911
Epoch 11/80: current_loss=0.02923 | best_loss=0.02911
Epoch 12/80: current_loss=0.02903 | best_loss=0.02903
Epoch 13/80: current_loss=0.02903 | best_loss=0.02903
Epoch 14/80: current_loss=0.02913 | best_loss=0.02903
Epoch 15/80: current_loss=0.02923 | best_loss=0.02903
Epoch 16/80: current_loss=0.02933 | best_loss=0.02903
Epoch 17/80: current_loss=0.02970 | best_loss=0.02903
Epoch 18/80: current_loss=0.02919 | best_loss=0.02903
Epoch 19/80: current_loss=0.02969 | best_loss=0.02903
Epoch 20/80: current_loss=0.02904 | best_loss=0.02903
Epoch 21/80: current_loss=0.02900 | best_loss=0.02900
Epoch 22/80: current_loss=0.02918 | best_loss=0.02900
Epoch 23/80: current_loss=0.02905 | best_loss=0.02900
Epoch 24/80: current_loss=0.02902 | best_loss=0.02900
Epoch 25/80: current_loss=0.02911 | best_loss=0.02900
Epoch 26/80: current_loss=0.02912 | best_loss=0.02900
Epoch 27/80: current_loss=0.02902 | best_loss=0.02900
Epoch 28/80: current_loss=0.02910 | best_loss=0.02900
Epoch 29/80: current_loss=0.02899 | best_loss=0.02899
Epoch 30/80: current_loss=0.02924 | best_loss=0.02899
Epoch 31/80: current_loss=0.02901 | best_loss=0.02899
Epoch 32/80: current_loss=0.02900 | best_loss=0.02899
Epoch 33/80: current_loss=0.02900 | best_loss=0.02899
Epoch 34/80: current_loss=0.02911 | best_loss=0.02899
Epoch 35/80: current_loss=0.02909 | best_loss=0.02899
Epoch 36/80: current_loss=0.02951 | best_loss=0.02899
Epoch 37/80: current_loss=0.02903 | best_loss=0.02899
Epoch 38/80: current_loss=0.02905 | best_loss=0.02899
Epoch 39/80: current_loss=0.02916 | best_loss=0.02899
Epoch 40/80: current_loss=0.02905 | best_loss=0.02899
Epoch 41/80: current_loss=0.02906 | best_loss=0.02899
Epoch 42/80: current_loss=0.02928 | best_loss=0.02899
Epoch 43/80: current_loss=0.02907 | best_loss=0.02899
Epoch 44/80: current_loss=0.02951 | best_loss=0.02899
Epoch 45/80: current_loss=0.02916 | best_loss=0.02899
Epoch 46/80: current_loss=0.02937 | best_loss=0.02899
Epoch 47/80: current_loss=0.02908 | best_loss=0.02899
Epoch 48/80: current_loss=0.02918 | best_loss=0.02899
Epoch 49/80: current_loss=0.02910 | best_loss=0.02899
Early Stopping at epoch 49
      explained_var=0.00189 | mse_loss=0.02811
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02504 | best_loss=0.02504
Epoch 1/80: current_loss=0.02482 | best_loss=0.02482
Epoch 2/80: current_loss=0.02486 | best_loss=0.02482
Epoch 3/80: current_loss=0.02552 | best_loss=0.02482
Epoch 4/80: current_loss=0.02529 | best_loss=0.02482
Epoch 5/80: current_loss=0.02503 | best_loss=0.02482
Epoch 6/80: current_loss=0.02495 | best_loss=0.02482
Epoch 7/80: current_loss=0.02559 | best_loss=0.02482
Epoch 8/80: current_loss=0.02489 | best_loss=0.02482
Epoch 9/80: current_loss=0.02485 | best_loss=0.02482
Epoch 10/80: current_loss=0.02487 | best_loss=0.02482
Epoch 11/80: current_loss=0.02519 | best_loss=0.02482
Epoch 12/80: current_loss=0.02486 | best_loss=0.02482
Epoch 13/80: current_loss=0.02488 | best_loss=0.02482
Epoch 14/80: current_loss=0.02488 | best_loss=0.02482
Epoch 15/80: current_loss=0.02517 | best_loss=0.02482
Epoch 16/80: current_loss=0.02490 | best_loss=0.02482
Epoch 17/80: current_loss=0.02494 | best_loss=0.02482
Epoch 18/80: current_loss=0.02518 | best_loss=0.02482
Epoch 19/80: current_loss=0.02519 | best_loss=0.02482
Epoch 20/80: current_loss=0.02496 | best_loss=0.02482
Epoch 21/80: current_loss=0.02493 | best_loss=0.02482
Early Stopping at epoch 21
      explained_var=0.00216 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02806 | best_loss=0.02806
Epoch 1/80: current_loss=0.02806 | best_loss=0.02806
Epoch 2/80: current_loss=0.02867 | best_loss=0.02806
Epoch 3/80: current_loss=0.02762 | best_loss=0.02762
Epoch 4/80: current_loss=0.02810 | best_loss=0.02762
Epoch 5/80: current_loss=0.02791 | best_loss=0.02762
Epoch 6/80: current_loss=0.02743 | best_loss=0.02743
Epoch 7/80: current_loss=0.02765 | best_loss=0.02743
Epoch 8/80: current_loss=0.02824 | best_loss=0.02743
Epoch 9/80: current_loss=0.02758 | best_loss=0.02743
Epoch 10/80: current_loss=0.02792 | best_loss=0.02743
Epoch 11/80: current_loss=0.02843 | best_loss=0.02743
Epoch 12/80: current_loss=0.02774 | best_loss=0.02743
Epoch 13/80: current_loss=0.02794 | best_loss=0.02743
Epoch 14/80: current_loss=0.02800 | best_loss=0.02743
Epoch 15/80: current_loss=0.02758 | best_loss=0.02743
Epoch 16/80: current_loss=0.02795 | best_loss=0.02743
Epoch 17/80: current_loss=0.02784 | best_loss=0.02743
Epoch 18/80: current_loss=0.02774 | best_loss=0.02743
Epoch 19/80: current_loss=0.02739 | best_loss=0.02739
Epoch 20/80: current_loss=0.02736 | best_loss=0.02736
Epoch 21/80: current_loss=0.02764 | best_loss=0.02736
Epoch 22/80: current_loss=0.02736 | best_loss=0.02736
Epoch 23/80: current_loss=0.02759 | best_loss=0.02736
Epoch 24/80: current_loss=0.02780 | best_loss=0.02736
Epoch 25/80: current_loss=0.02751 | best_loss=0.02736
Epoch 26/80: current_loss=0.02846 | best_loss=0.02736
Epoch 27/80: current_loss=0.02752 | best_loss=0.02736
Epoch 28/80: current_loss=0.02765 | best_loss=0.02736
Epoch 29/80: current_loss=0.02802 | best_loss=0.02736
Epoch 30/80: current_loss=0.02777 | best_loss=0.02736
Epoch 31/80: current_loss=0.02767 | best_loss=0.02736
Epoch 32/80: current_loss=0.02776 | best_loss=0.02736
Epoch 33/80: current_loss=0.02778 | best_loss=0.02736
Epoch 34/80: current_loss=0.02778 | best_loss=0.02736
Epoch 35/80: current_loss=0.02761 | best_loss=0.02736
Epoch 36/80: current_loss=0.02754 | best_loss=0.02736
Epoch 37/80: current_loss=0.02762 | best_loss=0.02736
Epoch 38/80: current_loss=0.02763 | best_loss=0.02736
Epoch 39/80: current_loss=0.02782 | best_loss=0.02736
Epoch 40/80: current_loss=0.02785 | best_loss=0.02736
Early Stopping at epoch 40
      explained_var=-0.00340 | mse_loss=0.02779
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02677 | best_loss=0.02677
Epoch 1/80: current_loss=0.02708 | best_loss=0.02677
Epoch 2/80: current_loss=0.02677 | best_loss=0.02677
Epoch 3/80: current_loss=0.02678 | best_loss=0.02677
Epoch 4/80: current_loss=0.02681 | best_loss=0.02677
Epoch 5/80: current_loss=0.02673 | best_loss=0.02673
Epoch 6/80: current_loss=0.02684 | best_loss=0.02673
Epoch 7/80: current_loss=0.02674 | best_loss=0.02673
Epoch 8/80: current_loss=0.02673 | best_loss=0.02673
Epoch 9/80: current_loss=0.02715 | best_loss=0.02673
Epoch 10/80: current_loss=0.02676 | best_loss=0.02673
Epoch 11/80: current_loss=0.02676 | best_loss=0.02673
Epoch 12/80: current_loss=0.02673 | best_loss=0.02673
Epoch 13/80: current_loss=0.02673 | best_loss=0.02673
Epoch 14/80: current_loss=0.02672 | best_loss=0.02672
Epoch 15/80: current_loss=0.02677 | best_loss=0.02672
Epoch 16/80: current_loss=0.02672 | best_loss=0.02672
Epoch 17/80: current_loss=0.02712 | best_loss=0.02672
Epoch 18/80: current_loss=0.02676 | best_loss=0.02672
Epoch 19/80: current_loss=0.02676 | best_loss=0.02672
Epoch 20/80: current_loss=0.02719 | best_loss=0.02672
Epoch 21/80: current_loss=0.02679 | best_loss=0.02672
Epoch 22/80: current_loss=0.02702 | best_loss=0.02672
Epoch 23/80: current_loss=0.02676 | best_loss=0.02672
Epoch 24/80: current_loss=0.02712 | best_loss=0.02672
Epoch 25/80: current_loss=0.02682 | best_loss=0.02672
Epoch 26/80: current_loss=0.02675 | best_loss=0.02672
Epoch 27/80: current_loss=0.02704 | best_loss=0.02672
Epoch 28/80: current_loss=0.02705 | best_loss=0.02672
Epoch 29/80: current_loss=0.02679 | best_loss=0.02672
Epoch 30/80: current_loss=0.02680 | best_loss=0.02672
Epoch 31/80: current_loss=0.02677 | best_loss=0.02672
Epoch 32/80: current_loss=0.02688 | best_loss=0.02672
Epoch 33/80: current_loss=0.02678 | best_loss=0.02672
Epoch 34/80: current_loss=0.02678 | best_loss=0.02672
Epoch 35/80: current_loss=0.02690 | best_loss=0.02672
Epoch 36/80: current_loss=0.02678 | best_loss=0.02672
Early Stopping at epoch 36
      explained_var=0.00179 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02995 | best_loss=0.02995
Epoch 1/80: current_loss=0.02971 | best_loss=0.02971
Epoch 2/80: current_loss=0.02970 | best_loss=0.02970
Epoch 3/80: current_loss=0.03010 | best_loss=0.02970
Epoch 4/80: current_loss=0.02989 | best_loss=0.02970
Epoch 5/80: current_loss=0.03019 | best_loss=0.02970
Epoch 6/80: current_loss=0.02986 | best_loss=0.02970
Epoch 7/80: current_loss=0.02976 | best_loss=0.02970
Epoch 8/80: current_loss=0.02971 | best_loss=0.02970
Epoch 9/80: current_loss=0.02969 | best_loss=0.02969
Epoch 10/80: current_loss=0.02970 | best_loss=0.02969
Epoch 11/80: current_loss=0.02971 | best_loss=0.02969
Epoch 12/80: current_loss=0.02985 | best_loss=0.02969
Epoch 13/80: current_loss=0.02972 | best_loss=0.02969
Epoch 14/80: current_loss=0.02974 | best_loss=0.02969
Epoch 15/80: current_loss=0.02971 | best_loss=0.02969
Epoch 16/80: current_loss=0.02981 | best_loss=0.02969
Epoch 17/80: current_loss=0.02972 | best_loss=0.02969
Epoch 18/80: current_loss=0.02970 | best_loss=0.02969
Epoch 19/80: current_loss=0.02971 | best_loss=0.02969
Epoch 20/80: current_loss=0.02975 | best_loss=0.02969
Epoch 21/80: current_loss=0.02971 | best_loss=0.02969
Epoch 22/80: current_loss=0.02971 | best_loss=0.02969
Epoch 23/80: current_loss=0.02971 | best_loss=0.02969
Epoch 24/80: current_loss=0.02971 | best_loss=0.02969
Epoch 25/80: current_loss=0.02987 | best_loss=0.02969
Epoch 26/80: current_loss=0.02988 | best_loss=0.02969
Epoch 27/80: current_loss=0.02972 | best_loss=0.02969
Epoch 28/80: current_loss=0.02992 | best_loss=0.02969
Epoch 29/80: current_loss=0.02978 | best_loss=0.02969
Early Stopping at epoch 29
      explained_var=0.00185 | mse_loss=0.02871
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.00086| avg_loss=0.02694
----------------------------------------------


----------------------------------------------
Params for Trial 23
{'learning_rate': 0.0001, 'weight_decay': 0.0006264461946113155, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03571 | best_loss=0.03571
Epoch 1/80: current_loss=0.03229 | best_loss=0.03229
Epoch 2/80: current_loss=0.03117 | best_loss=0.03117
Epoch 3/80: current_loss=0.03029 | best_loss=0.03029
Epoch 4/80: current_loss=0.02978 | best_loss=0.02978
Epoch 5/80: current_loss=0.02944 | best_loss=0.02944
Epoch 6/80: current_loss=0.02926 | best_loss=0.02926
Epoch 7/80: current_loss=0.02913 | best_loss=0.02913
Epoch 8/80: current_loss=0.02910 | best_loss=0.02910
Epoch 9/80: current_loss=0.02921 | best_loss=0.02910
Epoch 10/80: current_loss=0.02910 | best_loss=0.02910
Epoch 11/80: current_loss=0.02941 | best_loss=0.02910
Epoch 12/80: current_loss=0.02901 | best_loss=0.02901
Epoch 13/80: current_loss=0.02910 | best_loss=0.02901
Epoch 14/80: current_loss=0.02895 | best_loss=0.02895
Epoch 15/80: current_loss=0.02901 | best_loss=0.02895
Epoch 16/80: current_loss=0.02917 | best_loss=0.02895
Epoch 17/80: current_loss=0.02941 | best_loss=0.02895
Epoch 18/80: current_loss=0.02909 | best_loss=0.02895
Epoch 19/80: current_loss=0.02904 | best_loss=0.02895
Epoch 20/80: current_loss=0.02902 | best_loss=0.02895
Epoch 21/80: current_loss=0.02896 | best_loss=0.02895
Epoch 22/80: current_loss=0.02922 | best_loss=0.02895
Epoch 23/80: current_loss=0.02915 | best_loss=0.02895
Epoch 24/80: current_loss=0.02890 | best_loss=0.02890
Epoch 25/80: current_loss=0.02891 | best_loss=0.02890
Epoch 26/80: current_loss=0.02897 | best_loss=0.02890
Epoch 27/80: current_loss=0.02900 | best_loss=0.02890
Epoch 28/80: current_loss=0.02908 | best_loss=0.02890
Epoch 29/80: current_loss=0.02900 | best_loss=0.02890
Epoch 30/80: current_loss=0.02894 | best_loss=0.02890
Epoch 31/80: current_loss=0.02903 | best_loss=0.02890
Epoch 32/80: current_loss=0.02893 | best_loss=0.02890
Epoch 33/80: current_loss=0.02891 | best_loss=0.02890
Epoch 34/80: current_loss=0.02896 | best_loss=0.02890
Epoch 35/80: current_loss=0.02905 | best_loss=0.02890
Epoch 36/80: current_loss=0.02896 | best_loss=0.02890
Epoch 37/80: current_loss=0.02913 | best_loss=0.02890
Epoch 38/80: current_loss=0.02901 | best_loss=0.02890
Epoch 39/80: current_loss=0.02907 | best_loss=0.02890
Epoch 40/80: current_loss=0.02903 | best_loss=0.02890
Epoch 41/80: current_loss=0.02896 | best_loss=0.02890
Epoch 42/80: current_loss=0.02895 | best_loss=0.02890
Epoch 43/80: current_loss=0.02934 | best_loss=0.02890
Epoch 44/80: current_loss=0.02893 | best_loss=0.02890
Early Stopping at epoch 44
      explained_var=0.00577 | mse_loss=0.02799
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02490 | best_loss=0.02490
Epoch 1/80: current_loss=0.02488 | best_loss=0.02488
Epoch 2/80: current_loss=0.02486 | best_loss=0.02486
Epoch 3/80: current_loss=0.02494 | best_loss=0.02486
Epoch 4/80: current_loss=0.02490 | best_loss=0.02486
Epoch 5/80: current_loss=0.02502 | best_loss=0.02486
Epoch 6/80: current_loss=0.02496 | best_loss=0.02486
Epoch 7/80: current_loss=0.02510 | best_loss=0.02486
Epoch 8/80: current_loss=0.02484 | best_loss=0.02484
Epoch 9/80: current_loss=0.02503 | best_loss=0.02484
Epoch 10/80: current_loss=0.02489 | best_loss=0.02484
Epoch 11/80: current_loss=0.02501 | best_loss=0.02484
Epoch 12/80: current_loss=0.02497 | best_loss=0.02484
Epoch 13/80: current_loss=0.02496 | best_loss=0.02484
Epoch 14/80: current_loss=0.02502 | best_loss=0.02484
Epoch 15/80: current_loss=0.02498 | best_loss=0.02484
Epoch 16/80: current_loss=0.02490 | best_loss=0.02484
Epoch 17/80: current_loss=0.02500 | best_loss=0.02484
Epoch 18/80: current_loss=0.02488 | best_loss=0.02484
Epoch 19/80: current_loss=0.02509 | best_loss=0.02484
Epoch 20/80: current_loss=0.02493 | best_loss=0.02484
Epoch 21/80: current_loss=0.02522 | best_loss=0.02484
Epoch 22/80: current_loss=0.02506 | best_loss=0.02484
Epoch 23/80: current_loss=0.02515 | best_loss=0.02484
Epoch 24/80: current_loss=0.02493 | best_loss=0.02484
Epoch 25/80: current_loss=0.02491 | best_loss=0.02484
Epoch 26/80: current_loss=0.02497 | best_loss=0.02484
Epoch 27/80: current_loss=0.02504 | best_loss=0.02484
Epoch 28/80: current_loss=0.02499 | best_loss=0.02484
Early Stopping at epoch 28
      explained_var=0.00224 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02772 | best_loss=0.02772
Epoch 1/80: current_loss=0.02842 | best_loss=0.02772
Epoch 2/80: current_loss=0.02761 | best_loss=0.02761
Epoch 3/80: current_loss=0.02758 | best_loss=0.02758
Epoch 4/80: current_loss=0.02747 | best_loss=0.02747
Epoch 5/80: current_loss=0.02800 | best_loss=0.02747
Epoch 6/80: current_loss=0.02767 | best_loss=0.02747
Epoch 7/80: current_loss=0.02764 | best_loss=0.02747
Epoch 8/80: current_loss=0.02855 | best_loss=0.02747
Epoch 9/80: current_loss=0.02752 | best_loss=0.02747
Epoch 10/80: current_loss=0.02778 | best_loss=0.02747
Epoch 11/80: current_loss=0.02783 | best_loss=0.02747
Epoch 12/80: current_loss=0.02783 | best_loss=0.02747
Epoch 13/80: current_loss=0.02774 | best_loss=0.02747
Epoch 14/80: current_loss=0.02759 | best_loss=0.02747
Epoch 15/80: current_loss=0.02747 | best_loss=0.02747
Epoch 16/80: current_loss=0.02760 | best_loss=0.02747
Epoch 17/80: current_loss=0.02762 | best_loss=0.02747
Epoch 18/80: current_loss=0.02740 | best_loss=0.02740
Epoch 19/80: current_loss=0.02854 | best_loss=0.02740
Epoch 20/80: current_loss=0.02820 | best_loss=0.02740
Epoch 21/80: current_loss=0.02794 | best_loss=0.02740
Epoch 22/80: current_loss=0.02754 | best_loss=0.02740
Epoch 23/80: current_loss=0.02833 | best_loss=0.02740
Epoch 24/80: current_loss=0.02765 | best_loss=0.02740
Epoch 25/80: current_loss=0.02795 | best_loss=0.02740
Epoch 26/80: current_loss=0.02748 | best_loss=0.02740
Epoch 27/80: current_loss=0.02803 | best_loss=0.02740
Epoch 28/80: current_loss=0.02772 | best_loss=0.02740
Epoch 29/80: current_loss=0.02760 | best_loss=0.02740
Epoch 30/80: current_loss=0.02807 | best_loss=0.02740
Epoch 31/80: current_loss=0.02799 | best_loss=0.02740
Epoch 32/80: current_loss=0.02871 | best_loss=0.02740
Epoch 33/80: current_loss=0.02801 | best_loss=0.02740
Epoch 34/80: current_loss=0.02740 | best_loss=0.02740
Epoch 35/80: current_loss=0.02763 | best_loss=0.02740
Epoch 36/80: current_loss=0.02761 | best_loss=0.02740
Epoch 37/80: current_loss=0.02762 | best_loss=0.02740
Epoch 38/80: current_loss=0.02767 | best_loss=0.02740
Epoch 39/80: current_loss=0.02768 | best_loss=0.02740
Epoch 40/80: current_loss=0.02779 | best_loss=0.02740
Epoch 41/80: current_loss=0.02752 | best_loss=0.02740
Epoch 42/80: current_loss=0.02834 | best_loss=0.02740
Epoch 43/80: current_loss=0.02772 | best_loss=0.02740
Epoch 44/80: current_loss=0.02888 | best_loss=0.02740
Epoch 45/80: current_loss=0.02806 | best_loss=0.02740
Epoch 46/80: current_loss=0.02741 | best_loss=0.02740
Epoch 47/80: current_loss=0.02766 | best_loss=0.02740
Epoch 48/80: current_loss=0.02752 | best_loss=0.02740
Epoch 49/80: current_loss=0.02849 | best_loss=0.02740
Epoch 50/80: current_loss=0.02764 | best_loss=0.02740
Epoch 51/80: current_loss=0.02751 | best_loss=0.02740
Epoch 52/80: current_loss=0.02737 | best_loss=0.02737
Epoch 53/80: current_loss=0.02879 | best_loss=0.02737
Epoch 54/80: current_loss=0.02770 | best_loss=0.02737
Epoch 55/80: current_loss=0.02760 | best_loss=0.02737
Epoch 56/80: current_loss=0.02774 | best_loss=0.02737
Epoch 57/80: current_loss=0.02771 | best_loss=0.02737
Epoch 58/80: current_loss=0.02762 | best_loss=0.02737
Epoch 59/80: current_loss=0.02789 | best_loss=0.02737
Epoch 60/80: current_loss=0.02772 | best_loss=0.02737
Epoch 61/80: current_loss=0.02762 | best_loss=0.02737
Epoch 62/80: current_loss=0.02805 | best_loss=0.02737
Epoch 63/80: current_loss=0.02778 | best_loss=0.02737
Epoch 64/80: current_loss=0.02836 | best_loss=0.02737
Epoch 65/80: current_loss=0.02840 | best_loss=0.02737
Epoch 66/80: current_loss=0.02783 | best_loss=0.02737
Epoch 67/80: current_loss=0.02768 | best_loss=0.02737
Epoch 68/80: current_loss=0.02793 | best_loss=0.02737
Epoch 69/80: current_loss=0.02793 | best_loss=0.02737
Epoch 70/80: current_loss=0.02753 | best_loss=0.02737
Epoch 71/80: current_loss=0.02801 | best_loss=0.02737
Epoch 72/80: current_loss=0.02814 | best_loss=0.02737
Early Stopping at epoch 72
      explained_var=-0.01087 | mse_loss=0.02786
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02678 | best_loss=0.02678
Epoch 1/80: current_loss=0.02673 | best_loss=0.02673
Epoch 2/80: current_loss=0.02672 | best_loss=0.02672
Epoch 3/80: current_loss=0.02686 | best_loss=0.02672
Epoch 4/80: current_loss=0.02677 | best_loss=0.02672
Epoch 5/80: current_loss=0.02679 | best_loss=0.02672
Epoch 6/80: current_loss=0.02690 | best_loss=0.02672
Epoch 7/80: current_loss=0.02673 | best_loss=0.02672
Epoch 8/80: current_loss=0.02714 | best_loss=0.02672
Epoch 9/80: current_loss=0.02687 | best_loss=0.02672
Epoch 10/80: current_loss=0.02687 | best_loss=0.02672
Epoch 11/80: current_loss=0.02709 | best_loss=0.02672
Epoch 12/80: current_loss=0.02673 | best_loss=0.02672
Epoch 13/80: current_loss=0.02680 | best_loss=0.02672
Epoch 14/80: current_loss=0.02687 | best_loss=0.02672
Epoch 15/80: current_loss=0.02673 | best_loss=0.02672
Epoch 16/80: current_loss=0.02673 | best_loss=0.02672
Epoch 17/80: current_loss=0.02716 | best_loss=0.02672
Epoch 18/80: current_loss=0.02680 | best_loss=0.02672
Epoch 19/80: current_loss=0.02680 | best_loss=0.02672
Epoch 20/80: current_loss=0.02675 | best_loss=0.02672
Epoch 21/80: current_loss=0.02694 | best_loss=0.02672
Epoch 22/80: current_loss=0.02686 | best_loss=0.02672
Early Stopping at epoch 22
      explained_var=0.00136 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02996 | best_loss=0.02996
Epoch 1/80: current_loss=0.02978 | best_loss=0.02978
Epoch 2/80: current_loss=0.02981 | best_loss=0.02978
Epoch 3/80: current_loss=0.02972 | best_loss=0.02972
Epoch 4/80: current_loss=0.02977 | best_loss=0.02972
Epoch 5/80: current_loss=0.02970 | best_loss=0.02970
Epoch 6/80: current_loss=0.02990 | best_loss=0.02970
Epoch 7/80: current_loss=0.02996 | best_loss=0.02970
Epoch 8/80: current_loss=0.02972 | best_loss=0.02970
Epoch 9/80: current_loss=0.02972 | best_loss=0.02970
Epoch 10/80: current_loss=0.02981 | best_loss=0.02970
Epoch 11/80: current_loss=0.02997 | best_loss=0.02970
Epoch 12/80: current_loss=0.02977 | best_loss=0.02970
Epoch 13/80: current_loss=0.02987 | best_loss=0.02970
Epoch 14/80: current_loss=0.02972 | best_loss=0.02970
Epoch 15/80: current_loss=0.02976 | best_loss=0.02970
Epoch 16/80: current_loss=0.02968 | best_loss=0.02968
Epoch 17/80: current_loss=0.02973 | best_loss=0.02968
Epoch 18/80: current_loss=0.02988 | best_loss=0.02968
Epoch 19/80: current_loss=0.02991 | best_loss=0.02968
Epoch 20/80: current_loss=0.02970 | best_loss=0.02968
Epoch 21/80: current_loss=0.02971 | best_loss=0.02968
Epoch 22/80: current_loss=0.02972 | best_loss=0.02968
Epoch 23/80: current_loss=0.02969 | best_loss=0.02968
Epoch 24/80: current_loss=0.02972 | best_loss=0.02968
Epoch 25/80: current_loss=0.02972 | best_loss=0.02968
Epoch 26/80: current_loss=0.02967 | best_loss=0.02967
Epoch 27/80: current_loss=0.02968 | best_loss=0.02967
Epoch 28/80: current_loss=0.02969 | best_loss=0.02967
Epoch 29/80: current_loss=0.02978 | best_loss=0.02967
Epoch 30/80: current_loss=0.02975 | best_loss=0.02967
Epoch 31/80: current_loss=0.02969 | best_loss=0.02967
Epoch 32/80: current_loss=0.02975 | best_loss=0.02967
Epoch 33/80: current_loss=0.02980 | best_loss=0.02967
Epoch 34/80: current_loss=0.02988 | best_loss=0.02967
Epoch 35/80: current_loss=0.02968 | best_loss=0.02967
Epoch 36/80: current_loss=0.02979 | best_loss=0.02967
Epoch 37/80: current_loss=0.02971 | best_loss=0.02967
Epoch 38/80: current_loss=0.03010 | best_loss=0.02967
Epoch 39/80: current_loss=0.02976 | best_loss=0.02967
Epoch 40/80: current_loss=0.03011 | best_loss=0.02967
Epoch 41/80: current_loss=0.02966 | best_loss=0.02966
Epoch 42/80: current_loss=0.02973 | best_loss=0.02966
Epoch 43/80: current_loss=0.02981 | best_loss=0.02966
Epoch 44/80: current_loss=0.03002 | best_loss=0.02966
Epoch 45/80: current_loss=0.02968 | best_loss=0.02966
Epoch 46/80: current_loss=0.02969 | best_loss=0.02966
Epoch 47/80: current_loss=0.03001 | best_loss=0.02966
Epoch 48/80: current_loss=0.02971 | best_loss=0.02966
Epoch 49/80: current_loss=0.02980 | best_loss=0.02966
Epoch 50/80: current_loss=0.02987 | best_loss=0.02966
Epoch 51/80: current_loss=0.02993 | best_loss=0.02966
Epoch 52/80: current_loss=0.03008 | best_loss=0.02966
Epoch 53/80: current_loss=0.02977 | best_loss=0.02966
Epoch 54/80: current_loss=0.03003 | best_loss=0.02966
Epoch 55/80: current_loss=0.02971 | best_loss=0.02966
Epoch 56/80: current_loss=0.02975 | best_loss=0.02966
Epoch 57/80: current_loss=0.02968 | best_loss=0.02966
Epoch 58/80: current_loss=0.02972 | best_loss=0.02966
Epoch 59/80: current_loss=0.02988 | best_loss=0.02966
Epoch 60/80: current_loss=0.02986 | best_loss=0.02966
Epoch 61/80: current_loss=0.02970 | best_loss=0.02966
Early Stopping at epoch 61
      explained_var=0.00253 | mse_loss=0.02869
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=0.00021| avg_loss=0.02693
----------------------------------------------


----------------------------------------------
Params for Trial 24
{'learning_rate': 0.0001, 'weight_decay': 0.0020682592795415776, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03245 | best_loss=0.03245
Epoch 1/80: current_loss=0.03082 | best_loss=0.03082
Epoch 2/80: current_loss=0.02982 | best_loss=0.02982
Epoch 3/80: current_loss=0.02969 | best_loss=0.02969
Epoch 4/80: current_loss=0.02927 | best_loss=0.02927
Epoch 5/80: current_loss=0.02918 | best_loss=0.02918
Epoch 6/80: current_loss=0.02933 | best_loss=0.02918
Epoch 7/80: current_loss=0.02902 | best_loss=0.02902
Epoch 8/80: current_loss=0.02911 | best_loss=0.02902
Epoch 9/80: current_loss=0.02896 | best_loss=0.02896
Epoch 10/80: current_loss=0.02896 | best_loss=0.02896
Epoch 11/80: current_loss=0.02891 | best_loss=0.02891
Epoch 12/80: current_loss=0.02891 | best_loss=0.02891
Epoch 13/80: current_loss=0.02890 | best_loss=0.02890
Epoch 14/80: current_loss=0.02936 | best_loss=0.02890
Epoch 15/80: current_loss=0.02896 | best_loss=0.02890
Epoch 16/80: current_loss=0.02891 | best_loss=0.02890
Epoch 17/80: current_loss=0.02915 | best_loss=0.02890
Epoch 18/80: current_loss=0.02903 | best_loss=0.02890
Epoch 19/80: current_loss=0.02929 | best_loss=0.02890
Epoch 20/80: current_loss=0.02898 | best_loss=0.02890
Epoch 21/80: current_loss=0.02894 | best_loss=0.02890
Epoch 22/80: current_loss=0.02894 | best_loss=0.02890
Epoch 23/80: current_loss=0.02913 | best_loss=0.02890
Epoch 24/80: current_loss=0.02898 | best_loss=0.02890
Epoch 25/80: current_loss=0.02896 | best_loss=0.02890
Epoch 26/80: current_loss=0.02902 | best_loss=0.02890
Epoch 27/80: current_loss=0.02925 | best_loss=0.02890
Epoch 28/80: current_loss=0.02892 | best_loss=0.02890
Epoch 29/80: current_loss=0.02894 | best_loss=0.02890
Epoch 30/80: current_loss=0.02890 | best_loss=0.02890
Epoch 31/80: current_loss=0.02890 | best_loss=0.02890
Epoch 32/80: current_loss=0.02898 | best_loss=0.02890
Epoch 33/80: current_loss=0.02894 | best_loss=0.02890
Early Stopping at epoch 33
      explained_var=0.00562 | mse_loss=0.02798
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02524 | best_loss=0.02524
Epoch 1/80: current_loss=0.02494 | best_loss=0.02494
Epoch 2/80: current_loss=0.02621 | best_loss=0.02494
Epoch 3/80: current_loss=0.02494 | best_loss=0.02494
Epoch 4/80: current_loss=0.02487 | best_loss=0.02487
Epoch 5/80: current_loss=0.02487 | best_loss=0.02487
Epoch 6/80: current_loss=0.02486 | best_loss=0.02486
Epoch 7/80: current_loss=0.02488 | best_loss=0.02486
Epoch 8/80: current_loss=0.02487 | best_loss=0.02486
Epoch 9/80: current_loss=0.02494 | best_loss=0.02486
Epoch 10/80: current_loss=0.02496 | best_loss=0.02486
Epoch 11/80: current_loss=0.02507 | best_loss=0.02486
Epoch 12/80: current_loss=0.02484 | best_loss=0.02484
Epoch 13/80: current_loss=0.02488 | best_loss=0.02484
Epoch 14/80: current_loss=0.02502 | best_loss=0.02484
Epoch 15/80: current_loss=0.02502 | best_loss=0.02484
Epoch 16/80: current_loss=0.02499 | best_loss=0.02484
Epoch 17/80: current_loss=0.02485 | best_loss=0.02484
Epoch 18/80: current_loss=0.02513 | best_loss=0.02484
Epoch 19/80: current_loss=0.02486 | best_loss=0.02484
Epoch 20/80: current_loss=0.02497 | best_loss=0.02484
Epoch 21/80: current_loss=0.02497 | best_loss=0.02484
Epoch 22/80: current_loss=0.02487 | best_loss=0.02484
Epoch 23/80: current_loss=0.02488 | best_loss=0.02484
Epoch 24/80: current_loss=0.02499 | best_loss=0.02484
Epoch 25/80: current_loss=0.02512 | best_loss=0.02484
Epoch 26/80: current_loss=0.02489 | best_loss=0.02484
Epoch 27/80: current_loss=0.02487 | best_loss=0.02484
Epoch 28/80: current_loss=0.02513 | best_loss=0.02484
Epoch 29/80: current_loss=0.02489 | best_loss=0.02484
Epoch 30/80: current_loss=0.02496 | best_loss=0.02484
Epoch 31/80: current_loss=0.02493 | best_loss=0.02484
Epoch 32/80: current_loss=0.02519 | best_loss=0.02484
Early Stopping at epoch 32
      explained_var=0.00301 | mse_loss=0.02513
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02927 | best_loss=0.02927
Epoch 1/80: current_loss=0.02743 | best_loss=0.02743
Epoch 2/80: current_loss=0.02751 | best_loss=0.02743
Epoch 3/80: current_loss=0.02744 | best_loss=0.02743
Epoch 4/80: current_loss=0.02774 | best_loss=0.02743
Epoch 5/80: current_loss=0.02748 | best_loss=0.02743
Epoch 6/80: current_loss=0.02831 | best_loss=0.02743
Epoch 7/80: current_loss=0.02770 | best_loss=0.02743
Epoch 8/80: current_loss=0.02769 | best_loss=0.02743
Epoch 9/80: current_loss=0.02835 | best_loss=0.02743
Epoch 10/80: current_loss=0.02757 | best_loss=0.02743
Epoch 11/80: current_loss=0.02805 | best_loss=0.02743
Epoch 12/80: current_loss=0.02839 | best_loss=0.02743
Epoch 13/80: current_loss=0.02845 | best_loss=0.02743
Epoch 14/80: current_loss=0.02833 | best_loss=0.02743
Epoch 15/80: current_loss=0.02834 | best_loss=0.02743
Epoch 16/80: current_loss=0.02808 | best_loss=0.02743
Epoch 17/80: current_loss=0.02812 | best_loss=0.02743
Epoch 18/80: current_loss=0.02860 | best_loss=0.02743
Epoch 19/80: current_loss=0.02793 | best_loss=0.02743
Epoch 20/80: current_loss=0.02769 | best_loss=0.02743
Epoch 21/80: current_loss=0.02782 | best_loss=0.02743
Early Stopping at epoch 21
      explained_var=-0.01270 | mse_loss=0.02793
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02689 | best_loss=0.02689
Epoch 1/80: current_loss=0.02677 | best_loss=0.02677
Epoch 2/80: current_loss=0.02674 | best_loss=0.02674
Epoch 3/80: current_loss=0.02671 | best_loss=0.02671
Epoch 4/80: current_loss=0.02671 | best_loss=0.02671
Epoch 5/80: current_loss=0.02685 | best_loss=0.02671
Epoch 6/80: current_loss=0.02677 | best_loss=0.02671
Epoch 7/80: current_loss=0.02673 | best_loss=0.02671
Epoch 8/80: current_loss=0.02674 | best_loss=0.02671
Epoch 9/80: current_loss=0.02687 | best_loss=0.02671
Epoch 10/80: current_loss=0.02686 | best_loss=0.02671
Epoch 11/80: current_loss=0.02716 | best_loss=0.02671
Epoch 12/80: current_loss=0.02687 | best_loss=0.02671
Epoch 13/80: current_loss=0.02679 | best_loss=0.02671
Epoch 14/80: current_loss=0.02701 | best_loss=0.02671
Epoch 15/80: current_loss=0.02690 | best_loss=0.02671
Epoch 16/80: current_loss=0.02679 | best_loss=0.02671
Epoch 17/80: current_loss=0.02774 | best_loss=0.02671
Epoch 18/80: current_loss=0.02680 | best_loss=0.02671
Epoch 19/80: current_loss=0.02676 | best_loss=0.02671
Epoch 20/80: current_loss=0.02678 | best_loss=0.02671
Epoch 21/80: current_loss=0.02704 | best_loss=0.02671
Epoch 22/80: current_loss=0.02680 | best_loss=0.02671
Epoch 23/80: current_loss=0.02694 | best_loss=0.02671
Early Stopping at epoch 23
      explained_var=0.00077 | mse_loss=0.02497
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02983 | best_loss=0.02983
Epoch 1/80: current_loss=0.02983 | best_loss=0.02983
Epoch 2/80: current_loss=0.02973 | best_loss=0.02973
Epoch 3/80: current_loss=0.03028 | best_loss=0.02973
Epoch 4/80: current_loss=0.02975 | best_loss=0.02973
Epoch 5/80: current_loss=0.02974 | best_loss=0.02973
Epoch 6/80: current_loss=0.03011 | best_loss=0.02973
Epoch 7/80: current_loss=0.03003 | best_loss=0.02973
Epoch 8/80: current_loss=0.02978 | best_loss=0.02973
Epoch 9/80: current_loss=0.02976 | best_loss=0.02973
Epoch 10/80: current_loss=0.02981 | best_loss=0.02973
Epoch 11/80: current_loss=0.02974 | best_loss=0.02973
Epoch 12/80: current_loss=0.02974 | best_loss=0.02973
Epoch 13/80: current_loss=0.02973 | best_loss=0.02973
Epoch 14/80: current_loss=0.02974 | best_loss=0.02973
Epoch 15/80: current_loss=0.02997 | best_loss=0.02973
Epoch 16/80: current_loss=0.02991 | best_loss=0.02973
Epoch 17/80: current_loss=0.02978 | best_loss=0.02973
Epoch 18/80: current_loss=0.02979 | best_loss=0.02973
Epoch 19/80: current_loss=0.02995 | best_loss=0.02973
Epoch 20/80: current_loss=0.02979 | best_loss=0.02973
Epoch 21/80: current_loss=0.02974 | best_loss=0.02973
Epoch 22/80: current_loss=0.02985 | best_loss=0.02973
Early Stopping at epoch 22
      explained_var=0.00036 | mse_loss=0.02875
----------------------------------------------
Average early_stopping_point: 6| avg_exp_var=-0.00059| avg_loss=0.02695
----------------------------------------------


----------------------------------------------
Params for Trial 25
{'learning_rate': 0.0001, 'weight_decay': 0.0007703653888005247, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03215 | best_loss=0.03215
Epoch 1/80: current_loss=0.03106 | best_loss=0.03106
Epoch 2/80: current_loss=0.03030 | best_loss=0.03030
Epoch 3/80: current_loss=0.02990 | best_loss=0.02990
Epoch 4/80: current_loss=0.02963 | best_loss=0.02963
Epoch 5/80: current_loss=0.02959 | best_loss=0.02959
Epoch 6/80: current_loss=0.02950 | best_loss=0.02950
Epoch 7/80: current_loss=0.02943 | best_loss=0.02943
Epoch 8/80: current_loss=0.02935 | best_loss=0.02935
Epoch 9/80: current_loss=0.02926 | best_loss=0.02926
Epoch 10/80: current_loss=0.02932 | best_loss=0.02926
Epoch 11/80: current_loss=0.02914 | best_loss=0.02914
Epoch 12/80: current_loss=0.02915 | best_loss=0.02914
Epoch 13/80: current_loss=0.02913 | best_loss=0.02913
Epoch 14/80: current_loss=0.02928 | best_loss=0.02913
Epoch 15/80: current_loss=0.02910 | best_loss=0.02910
Epoch 16/80: current_loss=0.02930 | best_loss=0.02910
Epoch 17/80: current_loss=0.02912 | best_loss=0.02910
Epoch 18/80: current_loss=0.02906 | best_loss=0.02906
Epoch 19/80: current_loss=0.02907 | best_loss=0.02906
Epoch 20/80: current_loss=0.02901 | best_loss=0.02901
Epoch 21/80: current_loss=0.02905 | best_loss=0.02901
Epoch 22/80: current_loss=0.02905 | best_loss=0.02901
Epoch 23/80: current_loss=0.02899 | best_loss=0.02899
Epoch 24/80: current_loss=0.02897 | best_loss=0.02897
Epoch 25/80: current_loss=0.02898 | best_loss=0.02897
Epoch 26/80: current_loss=0.02897 | best_loss=0.02897
Epoch 27/80: current_loss=0.02930 | best_loss=0.02897
Epoch 28/80: current_loss=0.02902 | best_loss=0.02897
Epoch 29/80: current_loss=0.02910 | best_loss=0.02897
Epoch 30/80: current_loss=0.02896 | best_loss=0.02896
Epoch 31/80: current_loss=0.02914 | best_loss=0.02896
Epoch 32/80: current_loss=0.02907 | best_loss=0.02896
Epoch 33/80: current_loss=0.02897 | best_loss=0.02896
Epoch 34/80: current_loss=0.02944 | best_loss=0.02896
Epoch 35/80: current_loss=0.02906 | best_loss=0.02896
Epoch 36/80: current_loss=0.02903 | best_loss=0.02896
Epoch 37/80: current_loss=0.02894 | best_loss=0.02894
Epoch 38/80: current_loss=0.02905 | best_loss=0.02894
Epoch 39/80: current_loss=0.02899 | best_loss=0.02894
Epoch 40/80: current_loss=0.02893 | best_loss=0.02893
Epoch 41/80: current_loss=0.02889 | best_loss=0.02889
Epoch 42/80: current_loss=0.02887 | best_loss=0.02887
Epoch 43/80: current_loss=0.02889 | best_loss=0.02887
Epoch 44/80: current_loss=0.02890 | best_loss=0.02887
Epoch 45/80: current_loss=0.02890 | best_loss=0.02887
Epoch 46/80: current_loss=0.02894 | best_loss=0.02887
Epoch 47/80: current_loss=0.02895 | best_loss=0.02887
Epoch 48/80: current_loss=0.02894 | best_loss=0.02887
Epoch 49/80: current_loss=0.02906 | best_loss=0.02887
Epoch 50/80: current_loss=0.02897 | best_loss=0.02887
Epoch 51/80: current_loss=0.02899 | best_loss=0.02887
Epoch 52/80: current_loss=0.02901 | best_loss=0.02887
Epoch 53/80: current_loss=0.02898 | best_loss=0.02887
Epoch 54/80: current_loss=0.02897 | best_loss=0.02887
Epoch 55/80: current_loss=0.02896 | best_loss=0.02887
Epoch 56/80: current_loss=0.02898 | best_loss=0.02887
Epoch 57/80: current_loss=0.02928 | best_loss=0.02887
Epoch 58/80: current_loss=0.02906 | best_loss=0.02887
Epoch 59/80: current_loss=0.02899 | best_loss=0.02887
Epoch 60/80: current_loss=0.02898 | best_loss=0.02887
Epoch 61/80: current_loss=0.02897 | best_loss=0.02887
Epoch 62/80: current_loss=0.02898 | best_loss=0.02887
Early Stopping at epoch 62
      explained_var=0.00569 | mse_loss=0.02800
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02493 | best_loss=0.02493
Epoch 1/80: current_loss=0.02507 | best_loss=0.02493
Epoch 2/80: current_loss=0.02552 | best_loss=0.02493
Epoch 3/80: current_loss=0.02531 | best_loss=0.02493
Epoch 4/80: current_loss=0.02496 | best_loss=0.02493
Epoch 5/80: current_loss=0.02504 | best_loss=0.02493
Epoch 6/80: current_loss=0.02503 | best_loss=0.02493
Epoch 7/80: current_loss=0.02497 | best_loss=0.02493
Epoch 8/80: current_loss=0.02493 | best_loss=0.02493
Epoch 9/80: current_loss=0.02504 | best_loss=0.02493
Epoch 10/80: current_loss=0.02492 | best_loss=0.02492
Epoch 11/80: current_loss=0.02499 | best_loss=0.02492
Epoch 12/80: current_loss=0.02569 | best_loss=0.02492
Epoch 13/80: current_loss=0.02525 | best_loss=0.02492
Epoch 14/80: current_loss=0.02491 | best_loss=0.02491
Epoch 15/80: current_loss=0.02504 | best_loss=0.02491
Epoch 16/80: current_loss=0.02488 | best_loss=0.02488
Epoch 17/80: current_loss=0.02495 | best_loss=0.02488
Epoch 18/80: current_loss=0.02492 | best_loss=0.02488
Epoch 19/80: current_loss=0.02492 | best_loss=0.02488
Epoch 20/80: current_loss=0.02495 | best_loss=0.02488
Epoch 21/80: current_loss=0.02492 | best_loss=0.02488
Epoch 22/80: current_loss=0.02512 | best_loss=0.02488
Epoch 23/80: current_loss=0.02490 | best_loss=0.02488
Epoch 24/80: current_loss=0.02493 | best_loss=0.02488
Epoch 25/80: current_loss=0.02569 | best_loss=0.02488
Epoch 26/80: current_loss=0.02489 | best_loss=0.02488
Epoch 27/80: current_loss=0.02488 | best_loss=0.02488
Epoch 28/80: current_loss=0.02509 | best_loss=0.02488
Epoch 29/80: current_loss=0.02494 | best_loss=0.02488
Epoch 30/80: current_loss=0.02512 | best_loss=0.02488
Epoch 31/80: current_loss=0.02499 | best_loss=0.02488
Epoch 32/80: current_loss=0.02491 | best_loss=0.02488
Epoch 33/80: current_loss=0.02497 | best_loss=0.02488
Epoch 34/80: current_loss=0.02489 | best_loss=0.02488
Epoch 35/80: current_loss=0.02489 | best_loss=0.02488
Epoch 36/80: current_loss=0.02494 | best_loss=0.02488
Early Stopping at epoch 36
      explained_var=0.00247 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02753 | best_loss=0.02753
Epoch 1/80: current_loss=0.02764 | best_loss=0.02753
Epoch 2/80: current_loss=0.02815 | best_loss=0.02753
Epoch 3/80: current_loss=0.02753 | best_loss=0.02753
Epoch 4/80: current_loss=0.02793 | best_loss=0.02753
Epoch 5/80: current_loss=0.02799 | best_loss=0.02753
Epoch 6/80: current_loss=0.02772 | best_loss=0.02753
Epoch 7/80: current_loss=0.02776 | best_loss=0.02753
Epoch 8/80: current_loss=0.02799 | best_loss=0.02753
Epoch 9/80: current_loss=0.02787 | best_loss=0.02753
Epoch 10/80: current_loss=0.02752 | best_loss=0.02752
Epoch 11/80: current_loss=0.02751 | best_loss=0.02751
Epoch 12/80: current_loss=0.02806 | best_loss=0.02751
Epoch 13/80: current_loss=0.02833 | best_loss=0.02751
Epoch 14/80: current_loss=0.02766 | best_loss=0.02751
Epoch 15/80: current_loss=0.02766 | best_loss=0.02751
Epoch 16/80: current_loss=0.02843 | best_loss=0.02751
Epoch 17/80: current_loss=0.02736 | best_loss=0.02736
Epoch 18/80: current_loss=0.02783 | best_loss=0.02736
Epoch 19/80: current_loss=0.02755 | best_loss=0.02736
Epoch 20/80: current_loss=0.02767 | best_loss=0.02736
Epoch 21/80: current_loss=0.02810 | best_loss=0.02736
Epoch 22/80: current_loss=0.02775 | best_loss=0.02736
Epoch 23/80: current_loss=0.02789 | best_loss=0.02736
Epoch 24/80: current_loss=0.02763 | best_loss=0.02736
Epoch 25/80: current_loss=0.02781 | best_loss=0.02736
Epoch 26/80: current_loss=0.02740 | best_loss=0.02736
Epoch 27/80: current_loss=0.02813 | best_loss=0.02736
Epoch 28/80: current_loss=0.02734 | best_loss=0.02734
Epoch 29/80: current_loss=0.02828 | best_loss=0.02734
Epoch 30/80: current_loss=0.02888 | best_loss=0.02734
Epoch 31/80: current_loss=0.02774 | best_loss=0.02734
Epoch 32/80: current_loss=0.02756 | best_loss=0.02734
Epoch 33/80: current_loss=0.02786 | best_loss=0.02734
Epoch 34/80: current_loss=0.02791 | best_loss=0.02734
Epoch 35/80: current_loss=0.02799 | best_loss=0.02734
Epoch 36/80: current_loss=0.02783 | best_loss=0.02734
Epoch 37/80: current_loss=0.02809 | best_loss=0.02734
Epoch 38/80: current_loss=0.02771 | best_loss=0.02734
Epoch 39/80: current_loss=0.02738 | best_loss=0.02734
Epoch 40/80: current_loss=0.02755 | best_loss=0.02734
Epoch 41/80: current_loss=0.02747 | best_loss=0.02734
Epoch 42/80: current_loss=0.02788 | best_loss=0.02734
Epoch 43/80: current_loss=0.02781 | best_loss=0.02734
Epoch 44/80: current_loss=0.02789 | best_loss=0.02734
Epoch 45/80: current_loss=0.02750 | best_loss=0.02734
Epoch 46/80: current_loss=0.02817 | best_loss=0.02734
Epoch 47/80: current_loss=0.02739 | best_loss=0.02734
Epoch 48/80: current_loss=0.02787 | best_loss=0.02734
Early Stopping at epoch 48
      explained_var=-0.00470 | mse_loss=0.02778
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02684 | best_loss=0.02684
Epoch 1/80: current_loss=0.02678 | best_loss=0.02678
Epoch 2/80: current_loss=0.02676 | best_loss=0.02676
Epoch 3/80: current_loss=0.02676 | best_loss=0.02676
Epoch 4/80: current_loss=0.02700 | best_loss=0.02676
Epoch 5/80: current_loss=0.02675 | best_loss=0.02675
Epoch 6/80: current_loss=0.02682 | best_loss=0.02675
Epoch 7/80: current_loss=0.02676 | best_loss=0.02675
Epoch 8/80: current_loss=0.02693 | best_loss=0.02675
Epoch 9/80: current_loss=0.02705 | best_loss=0.02675
Epoch 10/80: current_loss=0.02678 | best_loss=0.02675
Epoch 11/80: current_loss=0.02679 | best_loss=0.02675
Epoch 12/80: current_loss=0.02679 | best_loss=0.02675
Epoch 13/80: current_loss=0.02677 | best_loss=0.02675
Epoch 14/80: current_loss=0.02684 | best_loss=0.02675
Epoch 15/80: current_loss=0.02680 | best_loss=0.02675
Epoch 16/80: current_loss=0.02677 | best_loss=0.02675
Epoch 17/80: current_loss=0.02678 | best_loss=0.02675
Epoch 18/80: current_loss=0.02691 | best_loss=0.02675
Epoch 19/80: current_loss=0.02696 | best_loss=0.02675
Epoch 20/80: current_loss=0.02681 | best_loss=0.02675
Epoch 21/80: current_loss=0.02680 | best_loss=0.02675
Epoch 22/80: current_loss=0.02705 | best_loss=0.02675
Epoch 23/80: current_loss=0.02677 | best_loss=0.02675
Epoch 24/80: current_loss=0.02684 | best_loss=0.02675
Epoch 25/80: current_loss=0.02679 | best_loss=0.02675
Early Stopping at epoch 25
      explained_var=0.00115 | mse_loss=0.02496
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02971 | best_loss=0.02971
Epoch 1/80: current_loss=0.02976 | best_loss=0.02971
Epoch 2/80: current_loss=0.02969 | best_loss=0.02969
Epoch 3/80: current_loss=0.02975 | best_loss=0.02969
Epoch 4/80: current_loss=0.02972 | best_loss=0.02969
Epoch 5/80: current_loss=0.02969 | best_loss=0.02969
Epoch 6/80: current_loss=0.02968 | best_loss=0.02968
Epoch 7/80: current_loss=0.02967 | best_loss=0.02967
Epoch 8/80: current_loss=0.02971 | best_loss=0.02967
Epoch 9/80: current_loss=0.02978 | best_loss=0.02967
Epoch 10/80: current_loss=0.02967 | best_loss=0.02967
Epoch 11/80: current_loss=0.02978 | best_loss=0.02967
Epoch 12/80: current_loss=0.02984 | best_loss=0.02967
Epoch 13/80: current_loss=0.02971 | best_loss=0.02967
Epoch 14/80: current_loss=0.02971 | best_loss=0.02967
Epoch 15/80: current_loss=0.02970 | best_loss=0.02967
Epoch 16/80: current_loss=0.02977 | best_loss=0.02967
Epoch 17/80: current_loss=0.02969 | best_loss=0.02967
Epoch 18/80: current_loss=0.02998 | best_loss=0.02967
Epoch 19/80: current_loss=0.02969 | best_loss=0.02967
Epoch 20/80: current_loss=0.02970 | best_loss=0.02967
Epoch 21/80: current_loss=0.02969 | best_loss=0.02967
Epoch 22/80: current_loss=0.02971 | best_loss=0.02967
Epoch 23/80: current_loss=0.02970 | best_loss=0.02967
Epoch 24/80: current_loss=0.02971 | best_loss=0.02967
Epoch 25/80: current_loss=0.02968 | best_loss=0.02967
Epoch 26/80: current_loss=0.02968 | best_loss=0.02967
Epoch 27/80: current_loss=0.02976 | best_loss=0.02967
Epoch 28/80: current_loss=0.02978 | best_loss=0.02967
Epoch 29/80: current_loss=0.02970 | best_loss=0.02967
Epoch 30/80: current_loss=0.02969 | best_loss=0.02967
Early Stopping at epoch 30
      explained_var=0.00209 | mse_loss=0.02870
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00134| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 26
{'learning_rate': 1e-05, 'weight_decay': 0.003876444198576039, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.10981 | best_loss=0.10981
Epoch 1/80: current_loss=0.07071 | best_loss=0.07071
Epoch 2/80: current_loss=0.04782 | best_loss=0.04782
Epoch 3/80: current_loss=0.03701 | best_loss=0.03701
Epoch 4/80: current_loss=0.03348 | best_loss=0.03348
Epoch 5/80: current_loss=0.03258 | best_loss=0.03258
Epoch 6/80: current_loss=0.03220 | best_loss=0.03220
Epoch 7/80: current_loss=0.03193 | best_loss=0.03193
Epoch 8/80: current_loss=0.03169 | best_loss=0.03169
Epoch 9/80: current_loss=0.03153 | best_loss=0.03153
Epoch 10/80: current_loss=0.03135 | best_loss=0.03135
Epoch 11/80: current_loss=0.03116 | best_loss=0.03116
Epoch 12/80: current_loss=0.03101 | best_loss=0.03101
Epoch 13/80: current_loss=0.03086 | best_loss=0.03086
Epoch 14/80: current_loss=0.03074 | best_loss=0.03074
Epoch 15/80: current_loss=0.03062 | best_loss=0.03062
Epoch 16/80: current_loss=0.03051 | best_loss=0.03051
Epoch 17/80: current_loss=0.03041 | best_loss=0.03041
Epoch 18/80: current_loss=0.03029 | best_loss=0.03029
Epoch 19/80: current_loss=0.03019 | best_loss=0.03019
Epoch 20/80: current_loss=0.03011 | best_loss=0.03011
Epoch 21/80: current_loss=0.03005 | best_loss=0.03005
Epoch 22/80: current_loss=0.02999 | best_loss=0.02999
Epoch 23/80: current_loss=0.02990 | best_loss=0.02990
Epoch 24/80: current_loss=0.02983 | best_loss=0.02983
Epoch 25/80: current_loss=0.02977 | best_loss=0.02977
Epoch 26/80: current_loss=0.02972 | best_loss=0.02972
Epoch 27/80: current_loss=0.02965 | best_loss=0.02965
Epoch 28/80: current_loss=0.02960 | best_loss=0.02960
Epoch 29/80: current_loss=0.02955 | best_loss=0.02955
Epoch 30/80: current_loss=0.02954 | best_loss=0.02954
Epoch 31/80: current_loss=0.02948 | best_loss=0.02948
Epoch 32/80: current_loss=0.02943 | best_loss=0.02943
Epoch 33/80: current_loss=0.02941 | best_loss=0.02941
Epoch 34/80: current_loss=0.02941 | best_loss=0.02941
Epoch 35/80: current_loss=0.02936 | best_loss=0.02936
Epoch 36/80: current_loss=0.02935 | best_loss=0.02935
Epoch 37/80: current_loss=0.02933 | best_loss=0.02933
Epoch 38/80: current_loss=0.02932 | best_loss=0.02932
Epoch 39/80: current_loss=0.02926 | best_loss=0.02926
Epoch 40/80: current_loss=0.02925 | best_loss=0.02925
Epoch 41/80: current_loss=0.02924 | best_loss=0.02924
Epoch 42/80: current_loss=0.02924 | best_loss=0.02924
Epoch 43/80: current_loss=0.02920 | best_loss=0.02920
Epoch 44/80: current_loss=0.02920 | best_loss=0.02920
Epoch 45/80: current_loss=0.02916 | best_loss=0.02916
Epoch 46/80: current_loss=0.02916 | best_loss=0.02916
Epoch 47/80: current_loss=0.02914 | best_loss=0.02914
Epoch 48/80: current_loss=0.02912 | best_loss=0.02912
Epoch 49/80: current_loss=0.02911 | best_loss=0.02911
Epoch 50/80: current_loss=0.02911 | best_loss=0.02911
Epoch 51/80: current_loss=0.02908 | best_loss=0.02908
Epoch 52/80: current_loss=0.02908 | best_loss=0.02908
Epoch 53/80: current_loss=0.02906 | best_loss=0.02906
Epoch 54/80: current_loss=0.02906 | best_loss=0.02906
Epoch 55/80: current_loss=0.02907 | best_loss=0.02906
Epoch 56/80: current_loss=0.02906 | best_loss=0.02906
Epoch 57/80: current_loss=0.02903 | best_loss=0.02903
Epoch 58/80: current_loss=0.02904 | best_loss=0.02903
Epoch 59/80: current_loss=0.02904 | best_loss=0.02903
Epoch 60/80: current_loss=0.02906 | best_loss=0.02903
Epoch 61/80: current_loss=0.02904 | best_loss=0.02903
Epoch 62/80: current_loss=0.02903 | best_loss=0.02903
Epoch 63/80: current_loss=0.02902 | best_loss=0.02902
Epoch 64/80: current_loss=0.02898 | best_loss=0.02898
Epoch 65/80: current_loss=0.02898 | best_loss=0.02898
Epoch 66/80: current_loss=0.02898 | best_loss=0.02898
Epoch 67/80: current_loss=0.02897 | best_loss=0.02897
Epoch 68/80: current_loss=0.02898 | best_loss=0.02897
Epoch 69/80: current_loss=0.02899 | best_loss=0.02897
Epoch 70/80: current_loss=0.02899 | best_loss=0.02897
Epoch 71/80: current_loss=0.02895 | best_loss=0.02895
Epoch 72/80: current_loss=0.02896 | best_loss=0.02895
Epoch 73/80: current_loss=0.02897 | best_loss=0.02895
Epoch 74/80: current_loss=0.02898 | best_loss=0.02895
Epoch 75/80: current_loss=0.02898 | best_loss=0.02895
Epoch 76/80: current_loss=0.02896 | best_loss=0.02895
Epoch 77/80: current_loss=0.02895 | best_loss=0.02895
Epoch 78/80: current_loss=0.02895 | best_loss=0.02895
Epoch 79/80: current_loss=0.02895 | best_loss=0.02895
      explained_var=0.00446 | mse_loss=0.02803
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02496 | best_loss=0.02496
Epoch 1/80: current_loss=0.02496 | best_loss=0.02496
Epoch 2/80: current_loss=0.02492 | best_loss=0.02492
Epoch 3/80: current_loss=0.02489 | best_loss=0.02489
Epoch 4/80: current_loss=0.02492 | best_loss=0.02489
Epoch 5/80: current_loss=0.02488 | best_loss=0.02488
Epoch 6/80: current_loss=0.02489 | best_loss=0.02488
Epoch 7/80: current_loss=0.02489 | best_loss=0.02488
Epoch 8/80: current_loss=0.02496 | best_loss=0.02488
Epoch 9/80: current_loss=0.02493 | best_loss=0.02488
Epoch 10/80: current_loss=0.02496 | best_loss=0.02488
Epoch 11/80: current_loss=0.02498 | best_loss=0.02488
Epoch 12/80: current_loss=0.02497 | best_loss=0.02488
Epoch 13/80: current_loss=0.02498 | best_loss=0.02488
Epoch 14/80: current_loss=0.02493 | best_loss=0.02488
Epoch 15/80: current_loss=0.02494 | best_loss=0.02488
Epoch 16/80: current_loss=0.02491 | best_loss=0.02488
Epoch 17/80: current_loss=0.02490 | best_loss=0.02488
Epoch 18/80: current_loss=0.02493 | best_loss=0.02488
Epoch 19/80: current_loss=0.02487 | best_loss=0.02487
Epoch 20/80: current_loss=0.02494 | best_loss=0.02487
Epoch 21/80: current_loss=0.02488 | best_loss=0.02487
Epoch 22/80: current_loss=0.02487 | best_loss=0.02487
Epoch 23/80: current_loss=0.02493 | best_loss=0.02487
Epoch 24/80: current_loss=0.02491 | best_loss=0.02487
Epoch 25/80: current_loss=0.02492 | best_loss=0.02487
Epoch 26/80: current_loss=0.02496 | best_loss=0.02487
Epoch 27/80: current_loss=0.02494 | best_loss=0.02487
Epoch 28/80: current_loss=0.02490 | best_loss=0.02487
Epoch 29/80: current_loss=0.02489 | best_loss=0.02487
Epoch 30/80: current_loss=0.02494 | best_loss=0.02487
Epoch 31/80: current_loss=0.02493 | best_loss=0.02487
Epoch 32/80: current_loss=0.02495 | best_loss=0.02487
Epoch 33/80: current_loss=0.02496 | best_loss=0.02487
Epoch 34/80: current_loss=0.02495 | best_loss=0.02487
Epoch 35/80: current_loss=0.02491 | best_loss=0.02487
Epoch 36/80: current_loss=0.02491 | best_loss=0.02487
Epoch 37/80: current_loss=0.02494 | best_loss=0.02487
Epoch 38/80: current_loss=0.02489 | best_loss=0.02487
Epoch 39/80: current_loss=0.02490 | best_loss=0.02487
Early Stopping at epoch 39
      explained_var=0.00183 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02820 | best_loss=0.02820
Epoch 1/80: current_loss=0.02826 | best_loss=0.02820
Epoch 2/80: current_loss=0.02826 | best_loss=0.02820
Epoch 3/80: current_loss=0.02817 | best_loss=0.02817
Epoch 4/80: current_loss=0.02809 | best_loss=0.02809
Epoch 5/80: current_loss=0.02823 | best_loss=0.02809
Epoch 6/80: current_loss=0.02813 | best_loss=0.02809
Epoch 7/80: current_loss=0.02825 | best_loss=0.02809
Epoch 8/80: current_loss=0.02828 | best_loss=0.02809
Epoch 9/80: current_loss=0.02807 | best_loss=0.02807
Epoch 10/80: current_loss=0.02819 | best_loss=0.02807
Epoch 11/80: current_loss=0.02810 | best_loss=0.02807
Epoch 12/80: current_loss=0.02813 | best_loss=0.02807
Epoch 13/80: current_loss=0.02826 | best_loss=0.02807
Epoch 14/80: current_loss=0.02830 | best_loss=0.02807
Epoch 15/80: current_loss=0.02822 | best_loss=0.02807
Epoch 16/80: current_loss=0.02823 | best_loss=0.02807
Epoch 17/80: current_loss=0.02818 | best_loss=0.02807
Epoch 18/80: current_loss=0.02815 | best_loss=0.02807
Epoch 19/80: current_loss=0.02819 | best_loss=0.02807
Epoch 20/80: current_loss=0.02820 | best_loss=0.02807
Epoch 21/80: current_loss=0.02823 | best_loss=0.02807
Epoch 22/80: current_loss=0.02816 | best_loss=0.02807
Epoch 23/80: current_loss=0.02808 | best_loss=0.02807
Epoch 24/80: current_loss=0.02811 | best_loss=0.02807
Epoch 25/80: current_loss=0.02821 | best_loss=0.02807
Epoch 26/80: current_loss=0.02813 | best_loss=0.02807
Epoch 27/80: current_loss=0.02809 | best_loss=0.02807
Epoch 28/80: current_loss=0.02824 | best_loss=0.02807
Epoch 29/80: current_loss=0.02824 | best_loss=0.02807
Early Stopping at epoch 29
      explained_var=-0.02274 | mse_loss=0.02865
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02665 | best_loss=0.02665
Epoch 1/80: current_loss=0.02666 | best_loss=0.02665
Epoch 2/80: current_loss=0.02667 | best_loss=0.02665
Epoch 3/80: current_loss=0.02666 | best_loss=0.02665
Epoch 4/80: current_loss=0.02665 | best_loss=0.02665
Epoch 5/80: current_loss=0.02665 | best_loss=0.02665
Epoch 6/80: current_loss=0.02667 | best_loss=0.02665
Epoch 7/80: current_loss=0.02665 | best_loss=0.02665
Epoch 8/80: current_loss=0.02666 | best_loss=0.02665
Epoch 9/80: current_loss=0.02666 | best_loss=0.02665
Epoch 10/80: current_loss=0.02665 | best_loss=0.02665
Epoch 11/80: current_loss=0.02666 | best_loss=0.02665
Epoch 12/80: current_loss=0.02665 | best_loss=0.02665
Epoch 13/80: current_loss=0.02665 | best_loss=0.02665
Epoch 14/80: current_loss=0.02666 | best_loss=0.02665
Epoch 15/80: current_loss=0.02668 | best_loss=0.02665
Epoch 16/80: current_loss=0.02668 | best_loss=0.02665
Epoch 17/80: current_loss=0.02665 | best_loss=0.02665
Epoch 18/80: current_loss=0.02665 | best_loss=0.02665
Epoch 19/80: current_loss=0.02665 | best_loss=0.02665
Epoch 20/80: current_loss=0.02664 | best_loss=0.02664
Epoch 21/80: current_loss=0.02664 | best_loss=0.02664
Epoch 22/80: current_loss=0.02664 | best_loss=0.02664
Epoch 23/80: current_loss=0.02666 | best_loss=0.02664
Epoch 24/80: current_loss=0.02665 | best_loss=0.02664
Epoch 25/80: current_loss=0.02665 | best_loss=0.02664
Epoch 26/80: current_loss=0.02668 | best_loss=0.02664
Epoch 27/80: current_loss=0.02664 | best_loss=0.02664
Epoch 28/80: current_loss=0.02666 | best_loss=0.02664
Epoch 29/80: current_loss=0.02668 | best_loss=0.02664
Epoch 30/80: current_loss=0.02667 | best_loss=0.02664
Epoch 31/80: current_loss=0.02666 | best_loss=0.02664
Epoch 32/80: current_loss=0.02667 | best_loss=0.02664
Epoch 33/80: current_loss=0.02667 | best_loss=0.02664
Epoch 34/80: current_loss=0.02667 | best_loss=0.02664
Epoch 35/80: current_loss=0.02670 | best_loss=0.02664
Epoch 36/80: current_loss=0.02668 | best_loss=0.02664
Epoch 37/80: current_loss=0.02667 | best_loss=0.02664
Epoch 38/80: current_loss=0.02666 | best_loss=0.02664
Epoch 39/80: current_loss=0.02667 | best_loss=0.02664
Epoch 40/80: current_loss=0.02665 | best_loss=0.02664
Epoch 41/80: current_loss=0.02667 | best_loss=0.02664
Epoch 42/80: current_loss=0.02668 | best_loss=0.02664
Epoch 43/80: current_loss=0.02665 | best_loss=0.02664
Epoch 44/80: current_loss=0.02667 | best_loss=0.02664
Epoch 45/80: current_loss=0.02668 | best_loss=0.02664
Epoch 46/80: current_loss=0.02667 | best_loss=0.02664
Epoch 47/80: current_loss=0.02666 | best_loss=0.02664
Early Stopping at epoch 47
      explained_var=0.00200 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03002 | best_loss=0.03002
Epoch 1/80: current_loss=0.02980 | best_loss=0.02980
Epoch 2/80: current_loss=0.02980 | best_loss=0.02980
Epoch 3/80: current_loss=0.02977 | best_loss=0.02977
Epoch 4/80: current_loss=0.02978 | best_loss=0.02977
Epoch 5/80: current_loss=0.02979 | best_loss=0.02977
Epoch 6/80: current_loss=0.02976 | best_loss=0.02976
Epoch 7/80: current_loss=0.02975 | best_loss=0.02975
Epoch 8/80: current_loss=0.02976 | best_loss=0.02975
Epoch 9/80: current_loss=0.02976 | best_loss=0.02975
Epoch 10/80: current_loss=0.02977 | best_loss=0.02975
Epoch 11/80: current_loss=0.02979 | best_loss=0.02975
Epoch 12/80: current_loss=0.02978 | best_loss=0.02975
Epoch 13/80: current_loss=0.02976 | best_loss=0.02975
Epoch 14/80: current_loss=0.02976 | best_loss=0.02975
Epoch 15/80: current_loss=0.02975 | best_loss=0.02975
Epoch 16/80: current_loss=0.02975 | best_loss=0.02975
Epoch 17/80: current_loss=0.02975 | best_loss=0.02975
Epoch 18/80: current_loss=0.02974 | best_loss=0.02974
Epoch 19/80: current_loss=0.02974 | best_loss=0.02974
Epoch 20/80: current_loss=0.02973 | best_loss=0.02973
Epoch 21/80: current_loss=0.02974 | best_loss=0.02973
Epoch 22/80: current_loss=0.02973 | best_loss=0.02973
Epoch 23/80: current_loss=0.02976 | best_loss=0.02973
Epoch 24/80: current_loss=0.02974 | best_loss=0.02973
Epoch 25/80: current_loss=0.02972 | best_loss=0.02972
Epoch 26/80: current_loss=0.02971 | best_loss=0.02971
Epoch 27/80: current_loss=0.02972 | best_loss=0.02971
Epoch 28/80: current_loss=0.02972 | best_loss=0.02971
Epoch 29/80: current_loss=0.02971 | best_loss=0.02971
Epoch 30/80: current_loss=0.02973 | best_loss=0.02971
Epoch 31/80: current_loss=0.02972 | best_loss=0.02971
Epoch 32/80: current_loss=0.02971 | best_loss=0.02971
Epoch 33/80: current_loss=0.02972 | best_loss=0.02971
Epoch 34/80: current_loss=0.02973 | best_loss=0.02971
Epoch 35/80: current_loss=0.02971 | best_loss=0.02971
Epoch 36/80: current_loss=0.02972 | best_loss=0.02971
Epoch 37/80: current_loss=0.02972 | best_loss=0.02971
Epoch 38/80: current_loss=0.02972 | best_loss=0.02971
Epoch 39/80: current_loss=0.02973 | best_loss=0.02971
Epoch 40/80: current_loss=0.02973 | best_loss=0.02971
Epoch 41/80: current_loss=0.02972 | best_loss=0.02971
Epoch 42/80: current_loss=0.02973 | best_loss=0.02971
Epoch 43/80: current_loss=0.02971 | best_loss=0.02971
Epoch 44/80: current_loss=0.02970 | best_loss=0.02970
Epoch 45/80: current_loss=0.02972 | best_loss=0.02970
Epoch 46/80: current_loss=0.02972 | best_loss=0.02970
Epoch 47/80: current_loss=0.02972 | best_loss=0.02970
Epoch 48/80: current_loss=0.02971 | best_loss=0.02970
Epoch 49/80: current_loss=0.02971 | best_loss=0.02970
Epoch 50/80: current_loss=0.02971 | best_loss=0.02970
Epoch 51/80: current_loss=0.02973 | best_loss=0.02970
Epoch 52/80: current_loss=0.02972 | best_loss=0.02970
Epoch 53/80: current_loss=0.02973 | best_loss=0.02970
Epoch 54/80: current_loss=0.02972 | best_loss=0.02970
Epoch 55/80: current_loss=0.02971 | best_loss=0.02970
Epoch 56/80: current_loss=0.02971 | best_loss=0.02970
Epoch 57/80: current_loss=0.02972 | best_loss=0.02970
Epoch 58/80: current_loss=0.02972 | best_loss=0.02970
Epoch 59/80: current_loss=0.02970 | best_loss=0.02970
Epoch 60/80: current_loss=0.02970 | best_loss=0.02970
Epoch 61/80: current_loss=0.02971 | best_loss=0.02970
Epoch 62/80: current_loss=0.02970 | best_loss=0.02970
Epoch 63/80: current_loss=0.02971 | best_loss=0.02970
Epoch 64/80: current_loss=0.02971 | best_loss=0.02970
Epoch 65/80: current_loss=0.02971 | best_loss=0.02970
Epoch 66/80: current_loss=0.02972 | best_loss=0.02970
Epoch 67/80: current_loss=0.02971 | best_loss=0.02970
Epoch 68/80: current_loss=0.02972 | best_loss=0.02970
Epoch 69/80: current_loss=0.02971 | best_loss=0.02970
Epoch 70/80: current_loss=0.02972 | best_loss=0.02970
Epoch 71/80: current_loss=0.02971 | best_loss=0.02970
Epoch 72/80: current_loss=0.02970 | best_loss=0.02970
Epoch 73/80: current_loss=0.02971 | best_loss=0.02970
Epoch 74/80: current_loss=0.02971 | best_loss=0.02970
Epoch 75/80: current_loss=0.02971 | best_loss=0.02970
Epoch 76/80: current_loss=0.02971 | best_loss=0.02970
Epoch 77/80: current_loss=0.02971 | best_loss=0.02970
Epoch 78/80: current_loss=0.02971 | best_loss=0.02970
Epoch 79/80: current_loss=0.02970 | best_loss=0.02970
      explained_var=0.00076 | mse_loss=0.02873
----------------------------------------------
Average early_stopping_point: 43| avg_exp_var=-0.00274| avg_loss=0.02711
----------------------------------------------


----------------------------------------------
Params for Trial 27
{'learning_rate': 0.0001, 'weight_decay': 0.009772161835564095, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03073 | best_loss=0.03073
Epoch 1/80: current_loss=0.02999 | best_loss=0.02999
Epoch 2/80: current_loss=0.02948 | best_loss=0.02948
Epoch 3/80: current_loss=0.02915 | best_loss=0.02915
Epoch 4/80: current_loss=0.02919 | best_loss=0.02915
Epoch 5/80: current_loss=0.02907 | best_loss=0.02907
Epoch 6/80: current_loss=0.02908 | best_loss=0.02907
Epoch 7/80: current_loss=0.02914 | best_loss=0.02907
Epoch 8/80: current_loss=0.02965 | best_loss=0.02907
Epoch 9/80: current_loss=0.02902 | best_loss=0.02902
Epoch 10/80: current_loss=0.02908 | best_loss=0.02902
Epoch 11/80: current_loss=0.02911 | best_loss=0.02902
Epoch 12/80: current_loss=0.02904 | best_loss=0.02902
Epoch 13/80: current_loss=0.02909 | best_loss=0.02902
Epoch 14/80: current_loss=0.02909 | best_loss=0.02902
Epoch 15/80: current_loss=0.02900 | best_loss=0.02900
Epoch 16/80: current_loss=0.02902 | best_loss=0.02900
Epoch 17/80: current_loss=0.02901 | best_loss=0.02900
Epoch 18/80: current_loss=0.02904 | best_loss=0.02900
Epoch 19/80: current_loss=0.02915 | best_loss=0.02900
Epoch 20/80: current_loss=0.02915 | best_loss=0.02900
Epoch 21/80: current_loss=0.02901 | best_loss=0.02900
Epoch 22/80: current_loss=0.02900 | best_loss=0.02900
Epoch 23/80: current_loss=0.02906 | best_loss=0.02900
Epoch 24/80: current_loss=0.02902 | best_loss=0.02900
Epoch 25/80: current_loss=0.02902 | best_loss=0.02900
Epoch 26/80: current_loss=0.02914 | best_loss=0.02900
Epoch 27/80: current_loss=0.02905 | best_loss=0.02900
Epoch 28/80: current_loss=0.02902 | best_loss=0.02900
Epoch 29/80: current_loss=0.02919 | best_loss=0.02900
Epoch 30/80: current_loss=0.02902 | best_loss=0.02900
Epoch 31/80: current_loss=0.02932 | best_loss=0.02900
Epoch 32/80: current_loss=0.02902 | best_loss=0.02900
Epoch 33/80: current_loss=0.02904 | best_loss=0.02900
Epoch 34/80: current_loss=0.02903 | best_loss=0.02900
Epoch 35/80: current_loss=0.02903 | best_loss=0.02900
Early Stopping at epoch 35
      explained_var=0.00453 | mse_loss=0.02801
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02498 | best_loss=0.02498
Epoch 1/80: current_loss=0.02525 | best_loss=0.02498
Epoch 2/80: current_loss=0.02504 | best_loss=0.02498
Epoch 3/80: current_loss=0.02519 | best_loss=0.02498
Epoch 4/80: current_loss=0.02489 | best_loss=0.02489
Epoch 5/80: current_loss=0.02488 | best_loss=0.02488
Epoch 6/80: current_loss=0.02540 | best_loss=0.02488
Epoch 7/80: current_loss=0.02506 | best_loss=0.02488
Epoch 8/80: current_loss=0.02520 | best_loss=0.02488
Epoch 9/80: current_loss=0.02493 | best_loss=0.02488
Epoch 10/80: current_loss=0.02488 | best_loss=0.02488
Epoch 11/80: current_loss=0.02497 | best_loss=0.02488
Epoch 12/80: current_loss=0.02489 | best_loss=0.02488
Epoch 13/80: current_loss=0.02511 | best_loss=0.02488
Epoch 14/80: current_loss=0.02515 | best_loss=0.02488
Epoch 15/80: current_loss=0.02496 | best_loss=0.02488
Epoch 16/80: current_loss=0.02504 | best_loss=0.02488
Epoch 17/80: current_loss=0.02489 | best_loss=0.02488
Epoch 18/80: current_loss=0.02497 | best_loss=0.02488
Epoch 19/80: current_loss=0.02492 | best_loss=0.02488
Epoch 20/80: current_loss=0.02489 | best_loss=0.02488
Epoch 21/80: current_loss=0.02492 | best_loss=0.02488
Epoch 22/80: current_loss=0.02491 | best_loss=0.02488
Epoch 23/80: current_loss=0.02494 | best_loss=0.02488
Epoch 24/80: current_loss=0.02492 | best_loss=0.02488
Epoch 25/80: current_loss=0.02490 | best_loss=0.02488
Epoch 26/80: current_loss=0.02511 | best_loss=0.02488
Epoch 27/80: current_loss=0.02512 | best_loss=0.02488
Epoch 28/80: current_loss=0.02493 | best_loss=0.02488
Epoch 29/80: current_loss=0.02504 | best_loss=0.02488
Epoch 30/80: current_loss=0.02514 | best_loss=0.02488
Early Stopping at epoch 30
      explained_var=0.00281 | mse_loss=0.02513
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02805 | best_loss=0.02805
Epoch 1/80: current_loss=0.02867 | best_loss=0.02805
Epoch 2/80: current_loss=0.02818 | best_loss=0.02805
Epoch 3/80: current_loss=0.02852 | best_loss=0.02805
Epoch 4/80: current_loss=0.02803 | best_loss=0.02803
Epoch 5/80: current_loss=0.02787 | best_loss=0.02787
Epoch 6/80: current_loss=0.02820 | best_loss=0.02787
Epoch 7/80: current_loss=0.02787 | best_loss=0.02787
Epoch 8/80: current_loss=0.02815 | best_loss=0.02787
Epoch 9/80: current_loss=0.02825 | best_loss=0.02787
Epoch 10/80: current_loss=0.02820 | best_loss=0.02787
Epoch 11/80: current_loss=0.02785 | best_loss=0.02785
Epoch 12/80: current_loss=0.02784 | best_loss=0.02784
Epoch 13/80: current_loss=0.02776 | best_loss=0.02776
Epoch 14/80: current_loss=0.02771 | best_loss=0.02771
Epoch 15/80: current_loss=0.02779 | best_loss=0.02771
Epoch 16/80: current_loss=0.02786 | best_loss=0.02771
Epoch 17/80: current_loss=0.02791 | best_loss=0.02771
Epoch 18/80: current_loss=0.02806 | best_loss=0.02771
Epoch 19/80: current_loss=0.02813 | best_loss=0.02771
Epoch 20/80: current_loss=0.02772 | best_loss=0.02771
Epoch 21/80: current_loss=0.02827 | best_loss=0.02771
Epoch 22/80: current_loss=0.02786 | best_loss=0.02771
Epoch 23/80: current_loss=0.02793 | best_loss=0.02771
Epoch 24/80: current_loss=0.02766 | best_loss=0.02766
Epoch 25/80: current_loss=0.02806 | best_loss=0.02766
Epoch 26/80: current_loss=0.02828 | best_loss=0.02766
Epoch 27/80: current_loss=0.02775 | best_loss=0.02766
Epoch 28/80: current_loss=0.02788 | best_loss=0.02766
Epoch 29/80: current_loss=0.02784 | best_loss=0.02766
Epoch 30/80: current_loss=0.02806 | best_loss=0.02766
Epoch 31/80: current_loss=0.02782 | best_loss=0.02766
Epoch 32/80: current_loss=0.02796 | best_loss=0.02766
Epoch 33/80: current_loss=0.02825 | best_loss=0.02766
Epoch 34/80: current_loss=0.02773 | best_loss=0.02766
Epoch 35/80: current_loss=0.02819 | best_loss=0.02766
Epoch 36/80: current_loss=0.02787 | best_loss=0.02766
Epoch 37/80: current_loss=0.02785 | best_loss=0.02766
Epoch 38/80: current_loss=0.02764 | best_loss=0.02764
Epoch 39/80: current_loss=0.02749 | best_loss=0.02749
Epoch 40/80: current_loss=0.02835 | best_loss=0.02749
Epoch 41/80: current_loss=0.02781 | best_loss=0.02749
Epoch 42/80: current_loss=0.02774 | best_loss=0.02749
Epoch 43/80: current_loss=0.02796 | best_loss=0.02749
Epoch 44/80: current_loss=0.02782 | best_loss=0.02749
Epoch 45/80: current_loss=0.02777 | best_loss=0.02749
Epoch 46/80: current_loss=0.02814 | best_loss=0.02749
Epoch 47/80: current_loss=0.02758 | best_loss=0.02749
Epoch 48/80: current_loss=0.02818 | best_loss=0.02749
Epoch 49/80: current_loss=0.02780 | best_loss=0.02749
Epoch 50/80: current_loss=0.02800 | best_loss=0.02749
Epoch 51/80: current_loss=0.02781 | best_loss=0.02749
Epoch 52/80: current_loss=0.02798 | best_loss=0.02749
Epoch 53/80: current_loss=0.02789 | best_loss=0.02749
Epoch 54/80: current_loss=0.02796 | best_loss=0.02749
Epoch 55/80: current_loss=0.02773 | best_loss=0.02749
Epoch 56/80: current_loss=0.02801 | best_loss=0.02749
Epoch 57/80: current_loss=0.02786 | best_loss=0.02749
Epoch 58/80: current_loss=0.02770 | best_loss=0.02749
Epoch 59/80: current_loss=0.02776 | best_loss=0.02749
Early Stopping at epoch 59
      explained_var=-0.01381 | mse_loss=0.02798
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02668 | best_loss=0.02668
Epoch 1/80: current_loss=0.02673 | best_loss=0.02668
Epoch 2/80: current_loss=0.02669 | best_loss=0.02668
Epoch 3/80: current_loss=0.02680 | best_loss=0.02668
Epoch 4/80: current_loss=0.02669 | best_loss=0.02668
Epoch 5/80: current_loss=0.02675 | best_loss=0.02668
Epoch 6/80: current_loss=0.02670 | best_loss=0.02668
Epoch 7/80: current_loss=0.02669 | best_loss=0.02668
Epoch 8/80: current_loss=0.02672 | best_loss=0.02668
Epoch 9/80: current_loss=0.02669 | best_loss=0.02668
Epoch 10/80: current_loss=0.02700 | best_loss=0.02668
Epoch 11/80: current_loss=0.02698 | best_loss=0.02668
Epoch 12/80: current_loss=0.02672 | best_loss=0.02668
Epoch 13/80: current_loss=0.02670 | best_loss=0.02668
Epoch 14/80: current_loss=0.02670 | best_loss=0.02668
Epoch 15/80: current_loss=0.02670 | best_loss=0.02668
Epoch 16/80: current_loss=0.02671 | best_loss=0.02668
Epoch 17/80: current_loss=0.02671 | best_loss=0.02668
Epoch 18/80: current_loss=0.02701 | best_loss=0.02668
Epoch 19/80: current_loss=0.02677 | best_loss=0.02668
Epoch 20/80: current_loss=0.02675 | best_loss=0.02668
Early Stopping at epoch 20
      explained_var=0.00124 | mse_loss=0.02496
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02989 | best_loss=0.02989
Epoch 1/80: current_loss=0.02983 | best_loss=0.02983
Epoch 2/80: current_loss=0.02981 | best_loss=0.02981
Epoch 3/80: current_loss=0.02987 | best_loss=0.02981
Epoch 4/80: current_loss=0.02984 | best_loss=0.02981
Epoch 5/80: current_loss=0.02991 | best_loss=0.02981
Epoch 6/80: current_loss=0.02991 | best_loss=0.02981
Epoch 7/80: current_loss=0.02985 | best_loss=0.02981
Epoch 8/80: current_loss=0.02979 | best_loss=0.02979
Epoch 9/80: current_loss=0.02978 | best_loss=0.02978
Epoch 10/80: current_loss=0.02979 | best_loss=0.02978
Epoch 11/80: current_loss=0.02981 | best_loss=0.02978
Epoch 12/80: current_loss=0.02979 | best_loss=0.02978
Epoch 13/80: current_loss=0.02977 | best_loss=0.02977
Epoch 14/80: current_loss=0.02986 | best_loss=0.02977
Epoch 15/80: current_loss=0.02978 | best_loss=0.02977
Epoch 16/80: current_loss=0.02977 | best_loss=0.02977
Epoch 17/80: current_loss=0.02990 | best_loss=0.02977
Epoch 18/80: current_loss=0.02977 | best_loss=0.02977
Epoch 19/80: current_loss=0.02983 | best_loss=0.02977
Epoch 20/80: current_loss=0.02977 | best_loss=0.02977
Epoch 21/80: current_loss=0.02976 | best_loss=0.02976
Epoch 22/80: current_loss=0.02977 | best_loss=0.02976
Epoch 23/80: current_loss=0.03005 | best_loss=0.02976
Epoch 24/80: current_loss=0.02979 | best_loss=0.02976
Epoch 25/80: current_loss=0.02977 | best_loss=0.02976
Epoch 26/80: current_loss=0.02977 | best_loss=0.02976
Epoch 27/80: current_loss=0.03006 | best_loss=0.02976
Epoch 28/80: current_loss=0.02978 | best_loss=0.02976
Epoch 29/80: current_loss=0.02979 | best_loss=0.02976
Epoch 30/80: current_loss=0.02979 | best_loss=0.02976
Epoch 31/80: current_loss=0.02979 | best_loss=0.02976
Epoch 32/80: current_loss=0.02984 | best_loss=0.02976
Epoch 33/80: current_loss=0.02977 | best_loss=0.02976
Epoch 34/80: current_loss=0.03010 | best_loss=0.02976
Epoch 35/80: current_loss=0.02977 | best_loss=0.02976
Epoch 36/80: current_loss=0.02980 | best_loss=0.02976
Epoch 37/80: current_loss=0.02980 | best_loss=0.02976
Epoch 38/80: current_loss=0.02979 | best_loss=0.02976
Epoch 39/80: current_loss=0.02979 | best_loss=0.02976
Epoch 40/80: current_loss=0.02988 | best_loss=0.02976
Epoch 41/80: current_loss=0.02986 | best_loss=0.02976
Early Stopping at epoch 41
      explained_var=0.00006 | mse_loss=0.02876
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=-0.00104| avg_loss=0.02697
----------------------------------------------


----------------------------------------------
Params for Trial 28
{'learning_rate': 0.1, 'weight_decay': 2.502346830446432e-05, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03115 | best_loss=0.03115
Epoch 1/80: current_loss=0.02986 | best_loss=0.02986
Epoch 2/80: current_loss=0.03513 | best_loss=0.02986
Epoch 3/80: current_loss=0.04496 | best_loss=0.02986
Epoch 4/80: current_loss=0.06996 | best_loss=0.02986
Epoch 5/80: current_loss=0.04584 | best_loss=0.02986
Epoch 6/80: current_loss=0.03634 | best_loss=0.02986
Epoch 7/80: current_loss=0.05403 | best_loss=0.02986
Epoch 8/80: current_loss=0.05607 | best_loss=0.02986
Epoch 9/80: current_loss=0.06384 | best_loss=0.02986
Epoch 10/80: current_loss=0.07045 | best_loss=0.02986
Epoch 11/80: current_loss=0.16764 | best_loss=0.02986
Epoch 12/80: current_loss=0.04963 | best_loss=0.02986
Epoch 13/80: current_loss=0.04353 | best_loss=0.02986
Epoch 14/80: current_loss=0.05224 | best_loss=0.02986
Epoch 15/80: current_loss=0.08706 | best_loss=0.02986
Epoch 16/80: current_loss=0.06848 | best_loss=0.02986
Epoch 17/80: current_loss=0.07413 | best_loss=0.02986
Epoch 18/80: current_loss=0.09955 | best_loss=0.02986
Epoch 19/80: current_loss=0.03200 | best_loss=0.02986
Epoch 20/80: current_loss=0.06494 | best_loss=0.02986
Epoch 21/80: current_loss=0.03277 | best_loss=0.02986
Early Stopping at epoch 21
      explained_var=-0.00843 | mse_loss=0.02906

----------------------------------------------
Params for Trial 29
{'learning_rate': 0.0001, 'weight_decay': 0.0013876994804289905, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03658 | best_loss=0.03658
Epoch 1/80: current_loss=0.03093 | best_loss=0.03093
Epoch 2/80: current_loss=0.03054 | best_loss=0.03054
Epoch 3/80: current_loss=0.03023 | best_loss=0.03023
Epoch 4/80: current_loss=0.03007 | best_loss=0.03007
Epoch 5/80: current_loss=0.02981 | best_loss=0.02981
Epoch 6/80: current_loss=0.02973 | best_loss=0.02973
Epoch 7/80: current_loss=0.02954 | best_loss=0.02954
Epoch 8/80: current_loss=0.02945 | best_loss=0.02945
Epoch 9/80: current_loss=0.02934 | best_loss=0.02934
Epoch 10/80: current_loss=0.02926 | best_loss=0.02926
Epoch 11/80: current_loss=0.02921 | best_loss=0.02921
Epoch 12/80: current_loss=0.02913 | best_loss=0.02913
Epoch 13/80: current_loss=0.02908 | best_loss=0.02908
Epoch 14/80: current_loss=0.02905 | best_loss=0.02905
Epoch 15/80: current_loss=0.02900 | best_loss=0.02900
Epoch 16/80: current_loss=0.02903 | best_loss=0.02900
Epoch 17/80: current_loss=0.02895 | best_loss=0.02895
Epoch 18/80: current_loss=0.02896 | best_loss=0.02895
Epoch 19/80: current_loss=0.02891 | best_loss=0.02891
Epoch 20/80: current_loss=0.02890 | best_loss=0.02890
Epoch 21/80: current_loss=0.02889 | best_loss=0.02889
Epoch 22/80: current_loss=0.02886 | best_loss=0.02886
Epoch 23/80: current_loss=0.02885 | best_loss=0.02885
Epoch 24/80: current_loss=0.02883 | best_loss=0.02883
Epoch 25/80: current_loss=0.02883 | best_loss=0.02883
Epoch 26/80: current_loss=0.02881 | best_loss=0.02881
Epoch 27/80: current_loss=0.02881 | best_loss=0.02881
Epoch 28/80: current_loss=0.02880 | best_loss=0.02880
Epoch 29/80: current_loss=0.02880 | best_loss=0.02880
Epoch 30/80: current_loss=0.02880 | best_loss=0.02880
Epoch 31/80: current_loss=0.02882 | best_loss=0.02880
Epoch 32/80: current_loss=0.02882 | best_loss=0.02880
Epoch 33/80: current_loss=0.02881 | best_loss=0.02880
Epoch 34/80: current_loss=0.02882 | best_loss=0.02880
Epoch 35/80: current_loss=0.02884 | best_loss=0.02880
Epoch 36/80: current_loss=0.02884 | best_loss=0.02880
Epoch 37/80: current_loss=0.02884 | best_loss=0.02880
Epoch 38/80: current_loss=0.02884 | best_loss=0.02880
Epoch 39/80: current_loss=0.02885 | best_loss=0.02880
Epoch 40/80: current_loss=0.02885 | best_loss=0.02880
Epoch 41/80: current_loss=0.02885 | best_loss=0.02880
Epoch 42/80: current_loss=0.02886 | best_loss=0.02880
Epoch 43/80: current_loss=0.02887 | best_loss=0.02880
Epoch 44/80: current_loss=0.02888 | best_loss=0.02880
Epoch 45/80: current_loss=0.02890 | best_loss=0.02880
Epoch 46/80: current_loss=0.02889 | best_loss=0.02880
Epoch 47/80: current_loss=0.02889 | best_loss=0.02880
Epoch 48/80: current_loss=0.02887 | best_loss=0.02880
Early Stopping at epoch 48
      explained_var=0.00818 | mse_loss=0.02792
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02496 | best_loss=0.02496
Epoch 1/80: current_loss=0.02491 | best_loss=0.02491
Epoch 2/80: current_loss=0.02496 | best_loss=0.02491
Epoch 3/80: current_loss=0.02496 | best_loss=0.02491
Epoch 4/80: current_loss=0.02495 | best_loss=0.02491
Epoch 5/80: current_loss=0.02496 | best_loss=0.02491
Epoch 6/80: current_loss=0.02496 | best_loss=0.02491
Epoch 7/80: current_loss=0.02503 | best_loss=0.02491
Epoch 8/80: current_loss=0.02504 | best_loss=0.02491
Epoch 9/80: current_loss=0.02502 | best_loss=0.02491
Epoch 10/80: current_loss=0.02497 | best_loss=0.02491
Epoch 11/80: current_loss=0.02503 | best_loss=0.02491
Epoch 12/80: current_loss=0.02498 | best_loss=0.02491
Epoch 13/80: current_loss=0.02498 | best_loss=0.02491
Epoch 14/80: current_loss=0.02501 | best_loss=0.02491
Epoch 15/80: current_loss=0.02502 | best_loss=0.02491
Epoch 16/80: current_loss=0.02506 | best_loss=0.02491
Epoch 17/80: current_loss=0.02514 | best_loss=0.02491
Epoch 18/80: current_loss=0.02505 | best_loss=0.02491
Epoch 19/80: current_loss=0.02513 | best_loss=0.02491
Epoch 20/80: current_loss=0.02506 | best_loss=0.02491
Epoch 21/80: current_loss=0.02504 | best_loss=0.02491
Early Stopping at epoch 21
      explained_var=0.00285 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02813 | best_loss=0.02813
Epoch 1/80: current_loss=0.02801 | best_loss=0.02801
Epoch 2/80: current_loss=0.02791 | best_loss=0.02791
Epoch 3/80: current_loss=0.02785 | best_loss=0.02785
Epoch 4/80: current_loss=0.02780 | best_loss=0.02780
Epoch 5/80: current_loss=0.02787 | best_loss=0.02780
Epoch 6/80: current_loss=0.02789 | best_loss=0.02780
Epoch 7/80: current_loss=0.02791 | best_loss=0.02780
Epoch 8/80: current_loss=0.02791 | best_loss=0.02780
Epoch 9/80: current_loss=0.02788 | best_loss=0.02780
Epoch 10/80: current_loss=0.02787 | best_loss=0.02780
Epoch 11/80: current_loss=0.02803 | best_loss=0.02780
Epoch 12/80: current_loss=0.02775 | best_loss=0.02775
Epoch 13/80: current_loss=0.02794 | best_loss=0.02775
Epoch 14/80: current_loss=0.02787 | best_loss=0.02775
Epoch 15/80: current_loss=0.02781 | best_loss=0.02775
Epoch 16/80: current_loss=0.02784 | best_loss=0.02775
Epoch 17/80: current_loss=0.02779 | best_loss=0.02775
Epoch 18/80: current_loss=0.02793 | best_loss=0.02775
Epoch 19/80: current_loss=0.02808 | best_loss=0.02775
Epoch 20/80: current_loss=0.02790 | best_loss=0.02775
Epoch 21/80: current_loss=0.02795 | best_loss=0.02775
Epoch 22/80: current_loss=0.02776 | best_loss=0.02775
Epoch 23/80: current_loss=0.02768 | best_loss=0.02768
Epoch 24/80: current_loss=0.02777 | best_loss=0.02768
Epoch 25/80: current_loss=0.02780 | best_loss=0.02768
Epoch 26/80: current_loss=0.02773 | best_loss=0.02768
Epoch 27/80: current_loss=0.02781 | best_loss=0.02768
Epoch 28/80: current_loss=0.02787 | best_loss=0.02768
Epoch 29/80: current_loss=0.02771 | best_loss=0.02768
Epoch 30/80: current_loss=0.02792 | best_loss=0.02768
Epoch 31/80: current_loss=0.02797 | best_loss=0.02768
Epoch 32/80: current_loss=0.02782 | best_loss=0.02768
Epoch 33/80: current_loss=0.02771 | best_loss=0.02768
Epoch 34/80: current_loss=0.02777 | best_loss=0.02768
Epoch 35/80: current_loss=0.02785 | best_loss=0.02768
Epoch 36/80: current_loss=0.02786 | best_loss=0.02768
Epoch 37/80: current_loss=0.02775 | best_loss=0.02768
Epoch 38/80: current_loss=0.02785 | best_loss=0.02768
Epoch 39/80: current_loss=0.02780 | best_loss=0.02768
Epoch 40/80: current_loss=0.02771 | best_loss=0.02768
Epoch 41/80: current_loss=0.02790 | best_loss=0.02768
Epoch 42/80: current_loss=0.02775 | best_loss=0.02768
Epoch 43/80: current_loss=0.02784 | best_loss=0.02768
Early Stopping at epoch 43
      explained_var=-0.00418 | mse_loss=0.02810
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02679 | best_loss=0.02679
Epoch 1/80: current_loss=0.02678 | best_loss=0.02678
Epoch 2/80: current_loss=0.02679 | best_loss=0.02678
Epoch 3/80: current_loss=0.02677 | best_loss=0.02677
Epoch 4/80: current_loss=0.02679 | best_loss=0.02677
Epoch 5/80: current_loss=0.02679 | best_loss=0.02677
Epoch 6/80: current_loss=0.02677 | best_loss=0.02677
Epoch 7/80: current_loss=0.02677 | best_loss=0.02677
Epoch 8/80: current_loss=0.02678 | best_loss=0.02677
Epoch 9/80: current_loss=0.02679 | best_loss=0.02677
Epoch 10/80: current_loss=0.02681 | best_loss=0.02677
Epoch 11/80: current_loss=0.02679 | best_loss=0.02677
Epoch 12/80: current_loss=0.02682 | best_loss=0.02677
Epoch 13/80: current_loss=0.02680 | best_loss=0.02677
Epoch 14/80: current_loss=0.02682 | best_loss=0.02677
Epoch 15/80: current_loss=0.02679 | best_loss=0.02677
Epoch 16/80: current_loss=0.02682 | best_loss=0.02677
Epoch 17/80: current_loss=0.02680 | best_loss=0.02677
Epoch 18/80: current_loss=0.02682 | best_loss=0.02677
Epoch 19/80: current_loss=0.02680 | best_loss=0.02677
Epoch 20/80: current_loss=0.02684 | best_loss=0.02677
Epoch 21/80: current_loss=0.02680 | best_loss=0.02677
Epoch 22/80: current_loss=0.02680 | best_loss=0.02677
Epoch 23/80: current_loss=0.02685 | best_loss=0.02677
Early Stopping at epoch 23
      explained_var=0.00173 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02985 | best_loss=0.02985
Epoch 1/80: current_loss=0.02972 | best_loss=0.02972
Epoch 2/80: current_loss=0.02972 | best_loss=0.02972
Epoch 3/80: current_loss=0.02974 | best_loss=0.02972
Epoch 4/80: current_loss=0.02970 | best_loss=0.02970
Epoch 5/80: current_loss=0.02972 | best_loss=0.02970
Epoch 6/80: current_loss=0.02974 | best_loss=0.02970
Epoch 7/80: current_loss=0.02973 | best_loss=0.02970
Epoch 8/80: current_loss=0.02973 | best_loss=0.02970
Epoch 9/80: current_loss=0.02973 | best_loss=0.02970
Epoch 10/80: current_loss=0.02975 | best_loss=0.02970
Epoch 11/80: current_loss=0.02976 | best_loss=0.02970
Epoch 12/80: current_loss=0.02977 | best_loss=0.02970
Epoch 13/80: current_loss=0.02977 | best_loss=0.02970
Epoch 14/80: current_loss=0.02975 | best_loss=0.02970
Epoch 15/80: current_loss=0.02975 | best_loss=0.02970
Epoch 16/80: current_loss=0.02976 | best_loss=0.02970
Epoch 17/80: current_loss=0.02977 | best_loss=0.02970
Epoch 18/80: current_loss=0.02976 | best_loss=0.02970
Epoch 19/80: current_loss=0.02975 | best_loss=0.02970
Epoch 20/80: current_loss=0.02974 | best_loss=0.02970
Epoch 21/80: current_loss=0.02974 | best_loss=0.02970
Epoch 22/80: current_loss=0.02976 | best_loss=0.02970
Epoch 23/80: current_loss=0.02976 | best_loss=0.02970
Epoch 24/80: current_loss=0.02977 | best_loss=0.02970
Early Stopping at epoch 24
      explained_var=0.00223 | mse_loss=0.02869
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00216| avg_loss=0.02696
----------------------------------------------


----------------------------------------------
Params for Trial 30
{'learning_rate': 0.0001, 'weight_decay': 0.0021595189685300747, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03325 | best_loss=0.03325
Epoch 1/80: current_loss=0.03103 | best_loss=0.03103
Epoch 2/80: current_loss=0.03060 | best_loss=0.03060
Epoch 3/80: current_loss=0.03034 | best_loss=0.03034
Epoch 4/80: current_loss=0.03016 | best_loss=0.03016
Epoch 5/80: current_loss=0.02991 | best_loss=0.02991
Epoch 6/80: current_loss=0.02985 | best_loss=0.02985
Epoch 7/80: current_loss=0.02969 | best_loss=0.02969
Epoch 8/80: current_loss=0.02954 | best_loss=0.02954
Epoch 9/80: current_loss=0.02945 | best_loss=0.02945
Epoch 10/80: current_loss=0.02941 | best_loss=0.02941
Epoch 11/80: current_loss=0.02935 | best_loss=0.02935
Epoch 12/80: current_loss=0.02936 | best_loss=0.02935
Epoch 13/80: current_loss=0.02927 | best_loss=0.02927
Epoch 14/80: current_loss=0.02921 | best_loss=0.02921
Epoch 15/80: current_loss=0.02919 | best_loss=0.02919
Epoch 16/80: current_loss=0.02911 | best_loss=0.02911
Epoch 17/80: current_loss=0.02912 | best_loss=0.02911
Epoch 18/80: current_loss=0.02913 | best_loss=0.02911
Epoch 19/80: current_loss=0.02907 | best_loss=0.02907
Epoch 20/80: current_loss=0.02909 | best_loss=0.02907
Epoch 21/80: current_loss=0.02910 | best_loss=0.02907
Epoch 22/80: current_loss=0.02906 | best_loss=0.02906
Epoch 23/80: current_loss=0.02908 | best_loss=0.02906
Epoch 24/80: current_loss=0.02904 | best_loss=0.02904
Epoch 25/80: current_loss=0.02908 | best_loss=0.02904
Epoch 26/80: current_loss=0.02905 | best_loss=0.02904
Epoch 27/80: current_loss=0.02909 | best_loss=0.02904
Epoch 28/80: current_loss=0.02901 | best_loss=0.02901
Epoch 29/80: current_loss=0.02899 | best_loss=0.02899
Epoch 30/80: current_loss=0.02899 | best_loss=0.02899
Epoch 31/80: current_loss=0.02900 | best_loss=0.02899
Epoch 32/80: current_loss=0.02901 | best_loss=0.02899
Epoch 33/80: current_loss=0.02900 | best_loss=0.02899
Epoch 34/80: current_loss=0.02898 | best_loss=0.02898
Epoch 35/80: current_loss=0.02898 | best_loss=0.02898
Epoch 36/80: current_loss=0.02898 | best_loss=0.02898
Epoch 37/80: current_loss=0.02898 | best_loss=0.02898
Epoch 38/80: current_loss=0.02900 | best_loss=0.02898
Epoch 39/80: current_loss=0.02898 | best_loss=0.02898
Epoch 40/80: current_loss=0.02897 | best_loss=0.02897
Epoch 41/80: current_loss=0.02898 | best_loss=0.02897
Epoch 42/80: current_loss=0.02899 | best_loss=0.02897
Epoch 43/80: current_loss=0.02899 | best_loss=0.02897
Epoch 44/80: current_loss=0.02898 | best_loss=0.02897
Epoch 45/80: current_loss=0.02899 | best_loss=0.02897
Epoch 46/80: current_loss=0.02898 | best_loss=0.02897
Epoch 47/80: current_loss=0.02898 | best_loss=0.02897
Epoch 48/80: current_loss=0.02899 | best_loss=0.02897
Epoch 49/80: current_loss=0.02898 | best_loss=0.02897
Epoch 50/80: current_loss=0.02898 | best_loss=0.02897
Epoch 51/80: current_loss=0.02899 | best_loss=0.02897
Epoch 52/80: current_loss=0.02899 | best_loss=0.02897
Epoch 53/80: current_loss=0.02898 | best_loss=0.02897
Epoch 54/80: current_loss=0.02898 | best_loss=0.02897
Epoch 55/80: current_loss=0.02899 | best_loss=0.02897
Epoch 56/80: current_loss=0.02898 | best_loss=0.02897
Epoch 57/80: current_loss=0.02898 | best_loss=0.02897
Epoch 58/80: current_loss=0.02898 | best_loss=0.02897
Epoch 59/80: current_loss=0.02899 | best_loss=0.02897
Epoch 60/80: current_loss=0.02898 | best_loss=0.02897
Early Stopping at epoch 60
      explained_var=0.00552 | mse_loss=0.02799
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02511 | best_loss=0.02511
Epoch 1/80: current_loss=0.02502 | best_loss=0.02502
Epoch 2/80: current_loss=0.02506 | best_loss=0.02502
Epoch 3/80: current_loss=0.02500 | best_loss=0.02500
Epoch 4/80: current_loss=0.02500 | best_loss=0.02500
Epoch 5/80: current_loss=0.02519 | best_loss=0.02500
Epoch 6/80: current_loss=0.02501 | best_loss=0.02500
Epoch 7/80: current_loss=0.02502 | best_loss=0.02500
Epoch 8/80: current_loss=0.02507 | best_loss=0.02500
Epoch 9/80: current_loss=0.02509 | best_loss=0.02500
Epoch 10/80: current_loss=0.02506 | best_loss=0.02500
Epoch 11/80: current_loss=0.02504 | best_loss=0.02500
Epoch 12/80: current_loss=0.02502 | best_loss=0.02500
Epoch 13/80: current_loss=0.02505 | best_loss=0.02500
Epoch 14/80: current_loss=0.02501 | best_loss=0.02500
Epoch 15/80: current_loss=0.02505 | best_loss=0.02500
Epoch 16/80: current_loss=0.02507 | best_loss=0.02500
Epoch 17/80: current_loss=0.02499 | best_loss=0.02499
Epoch 18/80: current_loss=0.02499 | best_loss=0.02499
Epoch 19/80: current_loss=0.02506 | best_loss=0.02499
Epoch 20/80: current_loss=0.02498 | best_loss=0.02498
Epoch 21/80: current_loss=0.02509 | best_loss=0.02498
Epoch 22/80: current_loss=0.02504 | best_loss=0.02498
Epoch 23/80: current_loss=0.02509 | best_loss=0.02498
Epoch 24/80: current_loss=0.02495 | best_loss=0.02495
Epoch 25/80: current_loss=0.02500 | best_loss=0.02495
Epoch 26/80: current_loss=0.02510 | best_loss=0.02495
Epoch 27/80: current_loss=0.02506 | best_loss=0.02495
Epoch 28/80: current_loss=0.02504 | best_loss=0.02495
Epoch 29/80: current_loss=0.02500 | best_loss=0.02495
Epoch 30/80: current_loss=0.02501 | best_loss=0.02495
Epoch 31/80: current_loss=0.02503 | best_loss=0.02495
Epoch 32/80: current_loss=0.02500 | best_loss=0.02495
Epoch 33/80: current_loss=0.02498 | best_loss=0.02495
Epoch 34/80: current_loss=0.02502 | best_loss=0.02495
Epoch 35/80: current_loss=0.02516 | best_loss=0.02495
Epoch 36/80: current_loss=0.02505 | best_loss=0.02495
Epoch 37/80: current_loss=0.02502 | best_loss=0.02495
Epoch 38/80: current_loss=0.02500 | best_loss=0.02495
Epoch 39/80: current_loss=0.02499 | best_loss=0.02495
Epoch 40/80: current_loss=0.02504 | best_loss=0.02495
Epoch 41/80: current_loss=0.02508 | best_loss=0.02495
Epoch 42/80: current_loss=0.02498 | best_loss=0.02495
Epoch 43/80: current_loss=0.02501 | best_loss=0.02495
Epoch 44/80: current_loss=0.02505 | best_loss=0.02495
Early Stopping at epoch 44
      explained_var=0.00246 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02801 | best_loss=0.02801
Epoch 1/80: current_loss=0.02796 | best_loss=0.02796
Epoch 2/80: current_loss=0.02801 | best_loss=0.02796
Epoch 3/80: current_loss=0.02806 | best_loss=0.02796
Epoch 4/80: current_loss=0.02790 | best_loss=0.02790
Epoch 5/80: current_loss=0.02791 | best_loss=0.02790
Epoch 6/80: current_loss=0.02791 | best_loss=0.02790
Epoch 7/80: current_loss=0.02786 | best_loss=0.02786
Epoch 8/80: current_loss=0.02788 | best_loss=0.02786
Epoch 9/80: current_loss=0.02785 | best_loss=0.02785
Epoch 10/80: current_loss=0.02787 | best_loss=0.02785
Epoch 11/80: current_loss=0.02779 | best_loss=0.02779
Epoch 12/80: current_loss=0.02796 | best_loss=0.02779
Epoch 13/80: current_loss=0.02787 | best_loss=0.02779
Epoch 14/80: current_loss=0.02787 | best_loss=0.02779
Epoch 15/80: current_loss=0.02779 | best_loss=0.02779
Epoch 16/80: current_loss=0.02782 | best_loss=0.02779
Epoch 17/80: current_loss=0.02804 | best_loss=0.02779
Epoch 18/80: current_loss=0.02781 | best_loss=0.02779
Epoch 19/80: current_loss=0.02793 | best_loss=0.02779
Epoch 20/80: current_loss=0.02776 | best_loss=0.02776
Epoch 21/80: current_loss=0.02791 | best_loss=0.02776
Epoch 22/80: current_loss=0.02794 | best_loss=0.02776
Epoch 23/80: current_loss=0.02789 | best_loss=0.02776
Epoch 24/80: current_loss=0.02781 | best_loss=0.02776
Epoch 25/80: current_loss=0.02789 | best_loss=0.02776
Epoch 26/80: current_loss=0.02796 | best_loss=0.02776
Epoch 27/80: current_loss=0.02786 | best_loss=0.02776
Epoch 28/80: current_loss=0.02781 | best_loss=0.02776
Epoch 29/80: current_loss=0.02780 | best_loss=0.02776
Epoch 30/80: current_loss=0.02783 | best_loss=0.02776
Epoch 31/80: current_loss=0.02779 | best_loss=0.02776
Epoch 32/80: current_loss=0.02784 | best_loss=0.02776
Epoch 33/80: current_loss=0.02785 | best_loss=0.02776
Epoch 34/80: current_loss=0.02790 | best_loss=0.02776
Epoch 35/80: current_loss=0.02789 | best_loss=0.02776
Epoch 36/80: current_loss=0.02789 | best_loss=0.02776
Epoch 37/80: current_loss=0.02791 | best_loss=0.02776
Epoch 38/80: current_loss=0.02789 | best_loss=0.02776
Epoch 39/80: current_loss=0.02789 | best_loss=0.02776
Epoch 40/80: current_loss=0.02786 | best_loss=0.02776
Early Stopping at epoch 40
      explained_var=-0.00592 | mse_loss=0.02819
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02677 | best_loss=0.02677
Epoch 1/80: current_loss=0.02677 | best_loss=0.02677
Epoch 2/80: current_loss=0.02677 | best_loss=0.02677
Epoch 3/80: current_loss=0.02676 | best_loss=0.02676
Epoch 4/80: current_loss=0.02674 | best_loss=0.02674
Epoch 5/80: current_loss=0.02676 | best_loss=0.02674
Epoch 6/80: current_loss=0.02674 | best_loss=0.02674
Epoch 7/80: current_loss=0.02676 | best_loss=0.02674
Epoch 8/80: current_loss=0.02675 | best_loss=0.02674
Epoch 9/80: current_loss=0.02678 | best_loss=0.02674
Epoch 10/80: current_loss=0.02679 | best_loss=0.02674
Epoch 11/80: current_loss=0.02675 | best_loss=0.02674
Epoch 12/80: current_loss=0.02676 | best_loss=0.02674
Epoch 13/80: current_loss=0.02677 | best_loss=0.02674
Epoch 14/80: current_loss=0.02677 | best_loss=0.02674
Epoch 15/80: current_loss=0.02675 | best_loss=0.02674
Epoch 16/80: current_loss=0.02677 | best_loss=0.02674
Epoch 17/80: current_loss=0.02676 | best_loss=0.02674
Epoch 18/80: current_loss=0.02678 | best_loss=0.02674
Epoch 19/80: current_loss=0.02676 | best_loss=0.02674
Epoch 20/80: current_loss=0.02680 | best_loss=0.02674
Epoch 21/80: current_loss=0.02676 | best_loss=0.02674
Epoch 22/80: current_loss=0.02678 | best_loss=0.02674
Epoch 23/80: current_loss=0.02676 | best_loss=0.02674
Epoch 24/80: current_loss=0.02675 | best_loss=0.02674
Epoch 25/80: current_loss=0.02679 | best_loss=0.02674
Epoch 26/80: current_loss=0.02678 | best_loss=0.02674
Early Stopping at epoch 26
      explained_var=0.00062 | mse_loss=0.02497
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02990 | best_loss=0.02990
Epoch 1/80: current_loss=0.02982 | best_loss=0.02982
Epoch 2/80: current_loss=0.02980 | best_loss=0.02980
Epoch 3/80: current_loss=0.02984 | best_loss=0.02980
Epoch 4/80: current_loss=0.02980 | best_loss=0.02980
Epoch 5/80: current_loss=0.02978 | best_loss=0.02978
Epoch 6/80: current_loss=0.02979 | best_loss=0.02978
Epoch 7/80: current_loss=0.02984 | best_loss=0.02978
Epoch 8/80: current_loss=0.02982 | best_loss=0.02978
Epoch 9/80: current_loss=0.02977 | best_loss=0.02977
Epoch 10/80: current_loss=0.02978 | best_loss=0.02977
Epoch 11/80: current_loss=0.02978 | best_loss=0.02977
Epoch 12/80: current_loss=0.02979 | best_loss=0.02977
Epoch 13/80: current_loss=0.02983 | best_loss=0.02977
Epoch 14/80: current_loss=0.02982 | best_loss=0.02977
Epoch 15/80: current_loss=0.02979 | best_loss=0.02977
Epoch 16/80: current_loss=0.02978 | best_loss=0.02977
Epoch 17/80: current_loss=0.02978 | best_loss=0.02977
Epoch 18/80: current_loss=0.02979 | best_loss=0.02977
Epoch 19/80: current_loss=0.02981 | best_loss=0.02977
Epoch 20/80: current_loss=0.02979 | best_loss=0.02977
Epoch 21/80: current_loss=0.02977 | best_loss=0.02977
Epoch 22/80: current_loss=0.02980 | best_loss=0.02977
Epoch 23/80: current_loss=0.02982 | best_loss=0.02977
Epoch 24/80: current_loss=0.02978 | best_loss=0.02977
Epoch 25/80: current_loss=0.02978 | best_loss=0.02977
Epoch 26/80: current_loss=0.02979 | best_loss=0.02977
Epoch 27/80: current_loss=0.02978 | best_loss=0.02977
Epoch 28/80: current_loss=0.02980 | best_loss=0.02977
Epoch 29/80: current_loss=0.02978 | best_loss=0.02977
Early Stopping at epoch 29
      explained_var=0.00022 | mse_loss=0.02875
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00058| avg_loss=0.02701
----------------------------------------------


----------------------------------------------
Params for Trial 31
{'learning_rate': 0.01, 'weight_decay': 0.0013354074101737164, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.12531 | best_loss=0.12531
Epoch 1/80: current_loss=0.03661 | best_loss=0.03661
Epoch 2/80: current_loss=0.04076 | best_loss=0.03661
Epoch 3/80: current_loss=0.06839 | best_loss=0.03661
Epoch 4/80: current_loss=0.04085 | best_loss=0.03661
Epoch 5/80: current_loss=0.04300 | best_loss=0.03661
Epoch 6/80: current_loss=0.04419 | best_loss=0.03661
Epoch 7/80: current_loss=0.03097 | best_loss=0.03097
Epoch 8/80: current_loss=0.03102 | best_loss=0.03097
Epoch 9/80: current_loss=0.03145 | best_loss=0.03097
Epoch 10/80: current_loss=0.05304 | best_loss=0.03097
Epoch 11/80: current_loss=0.04477 | best_loss=0.03097
Epoch 12/80: current_loss=0.05067 | best_loss=0.03097
Epoch 13/80: current_loss=0.07777 | best_loss=0.03097
Epoch 14/80: current_loss=0.04282 | best_loss=0.03097
Epoch 15/80: current_loss=0.02913 | best_loss=0.02913
Epoch 16/80: current_loss=0.04374 | best_loss=0.02913
Epoch 17/80: current_loss=0.04608 | best_loss=0.02913
Epoch 18/80: current_loss=0.03926 | best_loss=0.02913
Epoch 19/80: current_loss=0.03484 | best_loss=0.02913
Epoch 20/80: current_loss=0.04109 | best_loss=0.02913
Epoch 21/80: current_loss=0.04247 | best_loss=0.02913
Epoch 22/80: current_loss=0.03859 | best_loss=0.02913
Epoch 23/80: current_loss=0.04943 | best_loss=0.02913
Epoch 24/80: current_loss=0.06327 | best_loss=0.02913
Epoch 25/80: current_loss=0.03896 | best_loss=0.02913
Epoch 26/80: current_loss=0.03398 | best_loss=0.02913
Epoch 27/80: current_loss=0.03718 | best_loss=0.02913
Epoch 28/80: current_loss=0.03139 | best_loss=0.02913
Epoch 29/80: current_loss=0.04231 | best_loss=0.02913
Epoch 30/80: current_loss=0.04095 | best_loss=0.02913
Epoch 31/80: current_loss=0.03882 | best_loss=0.02913
Epoch 32/80: current_loss=0.03182 | best_loss=0.02913
Epoch 33/80: current_loss=0.04200 | best_loss=0.02913
Epoch 34/80: current_loss=0.03707 | best_loss=0.02913
Epoch 35/80: current_loss=0.03963 | best_loss=0.02913
Early Stopping at epoch 35
      explained_var=-0.00577 | mse_loss=0.02833

----------------------------------------------
Params for Trial 32
{'learning_rate': 1e-05, 'weight_decay': 0.00377076851471814, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03801 | best_loss=0.03801
Epoch 1/80: current_loss=0.03562 | best_loss=0.03562
Epoch 2/80: current_loss=0.03436 | best_loss=0.03436
Epoch 3/80: current_loss=0.03348 | best_loss=0.03348
Epoch 4/80: current_loss=0.03256 | best_loss=0.03256
Epoch 5/80: current_loss=0.03201 | best_loss=0.03201
Epoch 6/80: current_loss=0.03147 | best_loss=0.03147
Epoch 7/80: current_loss=0.03101 | best_loss=0.03101
Epoch 8/80: current_loss=0.03061 | best_loss=0.03061
Epoch 9/80: current_loss=0.03067 | best_loss=0.03061
Epoch 10/80: current_loss=0.03038 | best_loss=0.03038
Epoch 11/80: current_loss=0.03018 | best_loss=0.03018
Epoch 12/80: current_loss=0.02999 | best_loss=0.02999
Epoch 13/80: current_loss=0.02985 | best_loss=0.02985
Epoch 14/80: current_loss=0.02971 | best_loss=0.02971
Epoch 15/80: current_loss=0.02973 | best_loss=0.02971
Epoch 16/80: current_loss=0.02988 | best_loss=0.02971
Epoch 17/80: current_loss=0.02952 | best_loss=0.02952
Epoch 18/80: current_loss=0.02951 | best_loss=0.02951
Epoch 19/80: current_loss=0.02938 | best_loss=0.02938
Epoch 20/80: current_loss=0.02938 | best_loss=0.02938
Epoch 21/80: current_loss=0.02964 | best_loss=0.02938
Epoch 22/80: current_loss=0.02929 | best_loss=0.02929
Epoch 23/80: current_loss=0.02932 | best_loss=0.02929
Epoch 24/80: current_loss=0.02923 | best_loss=0.02923
Epoch 25/80: current_loss=0.02928 | best_loss=0.02923
Epoch 26/80: current_loss=0.02935 | best_loss=0.02923
Epoch 27/80: current_loss=0.02924 | best_loss=0.02923
Epoch 28/80: current_loss=0.02920 | best_loss=0.02920
Epoch 29/80: current_loss=0.02916 | best_loss=0.02916
Epoch 30/80: current_loss=0.02923 | best_loss=0.02916
Epoch 31/80: current_loss=0.02918 | best_loss=0.02916
Epoch 32/80: current_loss=0.02912 | best_loss=0.02912
Epoch 33/80: current_loss=0.02909 | best_loss=0.02909
Epoch 34/80: current_loss=0.02912 | best_loss=0.02909
Epoch 35/80: current_loss=0.02918 | best_loss=0.02909
Epoch 36/80: current_loss=0.02976 | best_loss=0.02909
Epoch 37/80: current_loss=0.02919 | best_loss=0.02909
Epoch 38/80: current_loss=0.02929 | best_loss=0.02909
Epoch 39/80: current_loss=0.02913 | best_loss=0.02909
Epoch 40/80: current_loss=0.02911 | best_loss=0.02909
Epoch 41/80: current_loss=0.02903 | best_loss=0.02903
Epoch 42/80: current_loss=0.02903 | best_loss=0.02903
Epoch 43/80: current_loss=0.02902 | best_loss=0.02902
Epoch 44/80: current_loss=0.02903 | best_loss=0.02902
Epoch 45/80: current_loss=0.02924 | best_loss=0.02902
Epoch 46/80: current_loss=0.02902 | best_loss=0.02902
Epoch 47/80: current_loss=0.02901 | best_loss=0.02901
Epoch 48/80: current_loss=0.02917 | best_loss=0.02901
Epoch 49/80: current_loss=0.02910 | best_loss=0.02901
Epoch 50/80: current_loss=0.02902 | best_loss=0.02901
Epoch 51/80: current_loss=0.02898 | best_loss=0.02898
Epoch 52/80: current_loss=0.02899 | best_loss=0.02898
Epoch 53/80: current_loss=0.02897 | best_loss=0.02897
Epoch 54/80: current_loss=0.02917 | best_loss=0.02897
Epoch 55/80: current_loss=0.02898 | best_loss=0.02897
Epoch 56/80: current_loss=0.02904 | best_loss=0.02897
Epoch 57/80: current_loss=0.02899 | best_loss=0.02897
Epoch 58/80: current_loss=0.02914 | best_loss=0.02897
Epoch 59/80: current_loss=0.02897 | best_loss=0.02897
Epoch 60/80: current_loss=0.02902 | best_loss=0.02897
Epoch 61/80: current_loss=0.02906 | best_loss=0.02897
Epoch 62/80: current_loss=0.02896 | best_loss=0.02896
Epoch 63/80: current_loss=0.02896 | best_loss=0.02896
Epoch 64/80: current_loss=0.02902 | best_loss=0.02896
Epoch 65/80: current_loss=0.02916 | best_loss=0.02896
Epoch 66/80: current_loss=0.02907 | best_loss=0.02896
Epoch 67/80: current_loss=0.02897 | best_loss=0.02896
Epoch 68/80: current_loss=0.02904 | best_loss=0.02896
Epoch 69/80: current_loss=0.02897 | best_loss=0.02896
Epoch 70/80: current_loss=0.02897 | best_loss=0.02896
Epoch 71/80: current_loss=0.02900 | best_loss=0.02896
Epoch 72/80: current_loss=0.02896 | best_loss=0.02896
Epoch 73/80: current_loss=0.02900 | best_loss=0.02896
Epoch 74/80: current_loss=0.02904 | best_loss=0.02896
Epoch 75/80: current_loss=0.02900 | best_loss=0.02896
Epoch 76/80: current_loss=0.02902 | best_loss=0.02896
Epoch 77/80: current_loss=0.02901 | best_loss=0.02896
Epoch 78/80: current_loss=0.02895 | best_loss=0.02895
Epoch 79/80: current_loss=0.02898 | best_loss=0.02895
      explained_var=0.00447 | mse_loss=0.02803
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02496 | best_loss=0.02496
Epoch 1/80: current_loss=0.02488 | best_loss=0.02488
Epoch 2/80: current_loss=0.02487 | best_loss=0.02487
Epoch 3/80: current_loss=0.02490 | best_loss=0.02487
Epoch 4/80: current_loss=0.02507 | best_loss=0.02487
Epoch 5/80: current_loss=0.02490 | best_loss=0.02487
Epoch 6/80: current_loss=0.02491 | best_loss=0.02487
Epoch 7/80: current_loss=0.02496 | best_loss=0.02487
Epoch 8/80: current_loss=0.02486 | best_loss=0.02486
Epoch 9/80: current_loss=0.02505 | best_loss=0.02486
Epoch 10/80: current_loss=0.02504 | best_loss=0.02486
Epoch 11/80: current_loss=0.02487 | best_loss=0.02486
Epoch 12/80: current_loss=0.02494 | best_loss=0.02486
Epoch 13/80: current_loss=0.02490 | best_loss=0.02486
Epoch 14/80: current_loss=0.02500 | best_loss=0.02486
Epoch 15/80: current_loss=0.02491 | best_loss=0.02486
Epoch 16/80: current_loss=0.02490 | best_loss=0.02486
Epoch 17/80: current_loss=0.02498 | best_loss=0.02486
Epoch 18/80: current_loss=0.02488 | best_loss=0.02486
Epoch 19/80: current_loss=0.02507 | best_loss=0.02486
Epoch 20/80: current_loss=0.02490 | best_loss=0.02486
Epoch 21/80: current_loss=0.02509 | best_loss=0.02486
Epoch 22/80: current_loss=0.02491 | best_loss=0.02486
Epoch 23/80: current_loss=0.02492 | best_loss=0.02486
Epoch 24/80: current_loss=0.02487 | best_loss=0.02486
Epoch 25/80: current_loss=0.02496 | best_loss=0.02486
Epoch 26/80: current_loss=0.02486 | best_loss=0.02486
Epoch 27/80: current_loss=0.02488 | best_loss=0.02486
Epoch 28/80: current_loss=0.02486 | best_loss=0.02486
Early Stopping at epoch 28
      explained_var=0.00228 | mse_loss=0.02514
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02804 | best_loss=0.02804
Epoch 1/80: current_loss=0.02799 | best_loss=0.02799
Epoch 2/80: current_loss=0.02805 | best_loss=0.02799
Epoch 3/80: current_loss=0.02802 | best_loss=0.02799
Epoch 4/80: current_loss=0.02792 | best_loss=0.02792
Epoch 5/80: current_loss=0.02816 | best_loss=0.02792
Epoch 6/80: current_loss=0.02834 | best_loss=0.02792
Epoch 7/80: current_loss=0.02774 | best_loss=0.02774
Epoch 8/80: current_loss=0.02830 | best_loss=0.02774
Epoch 9/80: current_loss=0.02836 | best_loss=0.02774
Epoch 10/80: current_loss=0.02797 | best_loss=0.02774
Epoch 11/80: current_loss=0.02788 | best_loss=0.02774
Epoch 12/80: current_loss=0.02842 | best_loss=0.02774
Epoch 13/80: current_loss=0.02797 | best_loss=0.02774
Epoch 14/80: current_loss=0.02840 | best_loss=0.02774
Epoch 15/80: current_loss=0.02793 | best_loss=0.02774
Epoch 16/80: current_loss=0.02800 | best_loss=0.02774
Epoch 17/80: current_loss=0.02810 | best_loss=0.02774
Epoch 18/80: current_loss=0.02802 | best_loss=0.02774
Epoch 19/80: current_loss=0.02820 | best_loss=0.02774
Epoch 20/80: current_loss=0.02837 | best_loss=0.02774
Epoch 21/80: current_loss=0.02807 | best_loss=0.02774
Epoch 22/80: current_loss=0.02798 | best_loss=0.02774
Epoch 23/80: current_loss=0.02796 | best_loss=0.02774
Epoch 24/80: current_loss=0.02821 | best_loss=0.02774
Epoch 25/80: current_loss=0.02792 | best_loss=0.02774
Epoch 26/80: current_loss=0.02843 | best_loss=0.02774
Epoch 27/80: current_loss=0.02812 | best_loss=0.02774
Early Stopping at epoch 27
      explained_var=-0.02524 | mse_loss=0.02834
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02660 | best_loss=0.02660
Epoch 1/80: current_loss=0.02678 | best_loss=0.02660
Epoch 2/80: current_loss=0.02662 | best_loss=0.02660
Epoch 3/80: current_loss=0.02663 | best_loss=0.02660
Epoch 4/80: current_loss=0.02661 | best_loss=0.02660
Epoch 5/80: current_loss=0.02661 | best_loss=0.02660
Epoch 6/80: current_loss=0.02662 | best_loss=0.02660
Epoch 7/80: current_loss=0.02680 | best_loss=0.02660
Epoch 8/80: current_loss=0.02667 | best_loss=0.02660
Epoch 9/80: current_loss=0.02664 | best_loss=0.02660
Epoch 10/80: current_loss=0.02661 | best_loss=0.02660
Epoch 11/80: current_loss=0.02668 | best_loss=0.02660
Epoch 12/80: current_loss=0.02667 | best_loss=0.02660
Epoch 13/80: current_loss=0.02662 | best_loss=0.02660
Epoch 14/80: current_loss=0.02678 | best_loss=0.02660
Epoch 15/80: current_loss=0.02664 | best_loss=0.02660
Epoch 16/80: current_loss=0.02665 | best_loss=0.02660
Epoch 17/80: current_loss=0.02666 | best_loss=0.02660
Epoch 18/80: current_loss=0.02670 | best_loss=0.02660
Epoch 19/80: current_loss=0.02667 | best_loss=0.02660
Epoch 20/80: current_loss=0.02673 | best_loss=0.02660
Early Stopping at epoch 20
      explained_var=0.00194 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02986 | best_loss=0.02986
Epoch 1/80: current_loss=0.02985 | best_loss=0.02985
Epoch 2/80: current_loss=0.02991 | best_loss=0.02985
Epoch 3/80: current_loss=0.02985 | best_loss=0.02985
Epoch 4/80: current_loss=0.02985 | best_loss=0.02985
Epoch 5/80: current_loss=0.02984 | best_loss=0.02984
Epoch 6/80: current_loss=0.03000 | best_loss=0.02984
Epoch 7/80: current_loss=0.02998 | best_loss=0.02984
Epoch 8/80: current_loss=0.02984 | best_loss=0.02984
Epoch 9/80: current_loss=0.02984 | best_loss=0.02984
Epoch 10/80: current_loss=0.02987 | best_loss=0.02984
Epoch 11/80: current_loss=0.02981 | best_loss=0.02981
Epoch 12/80: current_loss=0.03008 | best_loss=0.02981
Epoch 13/80: current_loss=0.02979 | best_loss=0.02979
Epoch 14/80: current_loss=0.02989 | best_loss=0.02979
Epoch 15/80: current_loss=0.02993 | best_loss=0.02979
Epoch 16/80: current_loss=0.03002 | best_loss=0.02979
Epoch 17/80: current_loss=0.02980 | best_loss=0.02979
Epoch 18/80: current_loss=0.02979 | best_loss=0.02979
Epoch 19/80: current_loss=0.02990 | best_loss=0.02979
Epoch 20/80: current_loss=0.02978 | best_loss=0.02978
Epoch 21/80: current_loss=0.02979 | best_loss=0.02978
Epoch 22/80: current_loss=0.02981 | best_loss=0.02978
Epoch 23/80: current_loss=0.02984 | best_loss=0.02978
Epoch 24/80: current_loss=0.02978 | best_loss=0.02978
Epoch 25/80: current_loss=0.02982 | best_loss=0.02978
Epoch 26/80: current_loss=0.02979 | best_loss=0.02978
Epoch 27/80: current_loss=0.02981 | best_loss=0.02978
Epoch 28/80: current_loss=0.02977 | best_loss=0.02977
Epoch 29/80: current_loss=0.02980 | best_loss=0.02977
Epoch 30/80: current_loss=0.02979 | best_loss=0.02977
Epoch 31/80: current_loss=0.02998 | best_loss=0.02977
Epoch 32/80: current_loss=0.02980 | best_loss=0.02977
Epoch 33/80: current_loss=0.02980 | best_loss=0.02977
Epoch 34/80: current_loss=0.02977 | best_loss=0.02977
Epoch 35/80: current_loss=0.02976 | best_loss=0.02976
Epoch 36/80: current_loss=0.02976 | best_loss=0.02976
Epoch 37/80: current_loss=0.02996 | best_loss=0.02976
Epoch 38/80: current_loss=0.02988 | best_loss=0.02976
Epoch 39/80: current_loss=0.02976 | best_loss=0.02976
Epoch 40/80: current_loss=0.02979 | best_loss=0.02976
Epoch 41/80: current_loss=0.02980 | best_loss=0.02976
Epoch 42/80: current_loss=0.02977 | best_loss=0.02976
Epoch 43/80: current_loss=0.02977 | best_loss=0.02976
Epoch 44/80: current_loss=0.02977 | best_loss=0.02976
Epoch 45/80: current_loss=0.02977 | best_loss=0.02976
Epoch 46/80: current_loss=0.02979 | best_loss=0.02976
Epoch 47/80: current_loss=0.02977 | best_loss=0.02976
Epoch 48/80: current_loss=0.02977 | best_loss=0.02976
Epoch 49/80: current_loss=0.02990 | best_loss=0.02976
Epoch 50/80: current_loss=0.02982 | best_loss=0.02976
Epoch 51/80: current_loss=0.02981 | best_loss=0.02976
Epoch 52/80: current_loss=0.02977 | best_loss=0.02976
Epoch 53/80: current_loss=0.02977 | best_loss=0.02976
Epoch 54/80: current_loss=0.02975 | best_loss=0.02975
Epoch 55/80: current_loss=0.02977 | best_loss=0.02975
Epoch 56/80: current_loss=0.02978 | best_loss=0.02975
Epoch 57/80: current_loss=0.02977 | best_loss=0.02975
Epoch 58/80: current_loss=0.02978 | best_loss=0.02975
Epoch 59/80: current_loss=0.02979 | best_loss=0.02975
Epoch 60/80: current_loss=0.02988 | best_loss=0.02975
Epoch 61/80: current_loss=0.02979 | best_loss=0.02975
Epoch 62/80: current_loss=0.02984 | best_loss=0.02975
Epoch 63/80: current_loss=0.02980 | best_loss=0.02975
Epoch 64/80: current_loss=0.02980 | best_loss=0.02975
Epoch 65/80: current_loss=0.02987 | best_loss=0.02975
Epoch 66/80: current_loss=0.02978 | best_loss=0.02975
Epoch 67/80: current_loss=0.02980 | best_loss=0.02975
Epoch 68/80: current_loss=0.02978 | best_loss=0.02975
Epoch 69/80: current_loss=0.02986 | best_loss=0.02975
Epoch 70/80: current_loss=0.02979 | best_loss=0.02975
Epoch 71/80: current_loss=0.02985 | best_loss=0.02975
Epoch 72/80: current_loss=0.02980 | best_loss=0.02975
Epoch 73/80: current_loss=0.02978 | best_loss=0.02975
Epoch 74/80: current_loss=0.02980 | best_loss=0.02975
Early Stopping at epoch 74
      explained_var=-0.00077 | mse_loss=0.02878
----------------------------------------------
Average early_stopping_point: 29| avg_exp_var=-0.00346| avg_loss=0.02705
----------------------------------------------


----------------------------------------------
Params for Trial 33
{'learning_rate': 0.0001, 'weight_decay': 0.0024567951354875174, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03208 | best_loss=0.03208
Epoch 1/80: current_loss=0.03039 | best_loss=0.03039
Epoch 2/80: current_loss=0.02996 | best_loss=0.02996
Epoch 3/80: current_loss=0.02951 | best_loss=0.02951
Epoch 4/80: current_loss=0.02954 | best_loss=0.02951
Epoch 5/80: current_loss=0.02910 | best_loss=0.02910
Epoch 6/80: current_loss=0.02902 | best_loss=0.02902
Epoch 7/80: current_loss=0.02920 | best_loss=0.02902
Epoch 8/80: current_loss=0.02899 | best_loss=0.02899
Epoch 9/80: current_loss=0.02915 | best_loss=0.02899
Epoch 10/80: current_loss=0.02896 | best_loss=0.02896
Epoch 11/80: current_loss=0.02900 | best_loss=0.02896
Epoch 12/80: current_loss=0.02898 | best_loss=0.02896
Epoch 13/80: current_loss=0.02892 | best_loss=0.02892
Epoch 14/80: current_loss=0.02891 | best_loss=0.02891
Epoch 15/80: current_loss=0.02890 | best_loss=0.02890
Epoch 16/80: current_loss=0.02890 | best_loss=0.02890
Epoch 17/80: current_loss=0.02903 | best_loss=0.02890
Epoch 18/80: current_loss=0.02915 | best_loss=0.02890
Epoch 19/80: current_loss=0.02896 | best_loss=0.02890
Epoch 20/80: current_loss=0.02918 | best_loss=0.02890
Epoch 21/80: current_loss=0.02896 | best_loss=0.02890
Epoch 22/80: current_loss=0.02897 | best_loss=0.02890
Epoch 23/80: current_loss=0.02904 | best_loss=0.02890
Epoch 24/80: current_loss=0.02908 | best_loss=0.02890
Epoch 25/80: current_loss=0.02939 | best_loss=0.02890
Epoch 26/80: current_loss=0.02897 | best_loss=0.02890
Epoch 27/80: current_loss=0.02904 | best_loss=0.02890
Epoch 28/80: current_loss=0.02898 | best_loss=0.02890
Epoch 29/80: current_loss=0.02898 | best_loss=0.02890
Epoch 30/80: current_loss=0.02899 | best_loss=0.02890
Epoch 31/80: current_loss=0.02897 | best_loss=0.02890
Epoch 32/80: current_loss=0.02905 | best_loss=0.02890
Epoch 33/80: current_loss=0.02907 | best_loss=0.02890
Epoch 34/80: current_loss=0.02898 | best_loss=0.02890
Epoch 35/80: current_loss=0.02899 | best_loss=0.02890
Early Stopping at epoch 35
      explained_var=0.00541 | mse_loss=0.02802
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02485 | best_loss=0.02485
Epoch 1/80: current_loss=0.02487 | best_loss=0.02485
Epoch 2/80: current_loss=0.02502 | best_loss=0.02485
Epoch 3/80: current_loss=0.02492 | best_loss=0.02485
Epoch 4/80: current_loss=0.02487 | best_loss=0.02485
Epoch 5/80: current_loss=0.02487 | best_loss=0.02485
Epoch 6/80: current_loss=0.02532 | best_loss=0.02485
Epoch 7/80: current_loss=0.02508 | best_loss=0.02485
Epoch 8/80: current_loss=0.02488 | best_loss=0.02485
Epoch 9/80: current_loss=0.02502 | best_loss=0.02485
Epoch 10/80: current_loss=0.02494 | best_loss=0.02485
Epoch 11/80: current_loss=0.02488 | best_loss=0.02485
Epoch 12/80: current_loss=0.02532 | best_loss=0.02485
Epoch 13/80: current_loss=0.02492 | best_loss=0.02485
Epoch 14/80: current_loss=0.02487 | best_loss=0.02485
Epoch 15/80: current_loss=0.02495 | best_loss=0.02485
Epoch 16/80: current_loss=0.02505 | best_loss=0.02485
Epoch 17/80: current_loss=0.02486 | best_loss=0.02485
Epoch 18/80: current_loss=0.02486 | best_loss=0.02485
Epoch 19/80: current_loss=0.02501 | best_loss=0.02485
Epoch 20/80: current_loss=0.02498 | best_loss=0.02485
Early Stopping at epoch 20
      explained_var=0.00274 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02751 | best_loss=0.02751
Epoch 1/80: current_loss=0.02761 | best_loss=0.02751
Epoch 2/80: current_loss=0.02785 | best_loss=0.02751
Epoch 3/80: current_loss=0.02811 | best_loss=0.02751
Epoch 4/80: current_loss=0.02771 | best_loss=0.02751
Epoch 5/80: current_loss=0.02824 | best_loss=0.02751
Epoch 6/80: current_loss=0.02878 | best_loss=0.02751
Epoch 7/80: current_loss=0.02769 | best_loss=0.02751
Epoch 8/80: current_loss=0.02881 | best_loss=0.02751
Epoch 9/80: current_loss=0.02753 | best_loss=0.02751
Epoch 10/80: current_loss=0.02785 | best_loss=0.02751
Epoch 11/80: current_loss=0.02872 | best_loss=0.02751
Epoch 12/80: current_loss=0.02972 | best_loss=0.02751
Epoch 13/80: current_loss=0.02875 | best_loss=0.02751
Epoch 14/80: current_loss=0.02807 | best_loss=0.02751
Epoch 15/80: current_loss=0.02857 | best_loss=0.02751
Epoch 16/80: current_loss=0.02801 | best_loss=0.02751
Epoch 17/80: current_loss=0.02778 | best_loss=0.02751
Epoch 18/80: current_loss=0.02840 | best_loss=0.02751
Epoch 19/80: current_loss=0.02793 | best_loss=0.02751
Epoch 20/80: current_loss=0.02814 | best_loss=0.02751
Early Stopping at epoch 20
      explained_var=-0.01417 | mse_loss=0.02803
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02670 | best_loss=0.02670
Epoch 1/80: current_loss=0.02668 | best_loss=0.02668
Epoch 2/80: current_loss=0.02668 | best_loss=0.02668
Epoch 3/80: current_loss=0.02671 | best_loss=0.02668
Epoch 4/80: current_loss=0.02668 | best_loss=0.02668
Epoch 5/80: current_loss=0.02699 | best_loss=0.02668
Epoch 6/80: current_loss=0.02679 | best_loss=0.02668
Epoch 7/80: current_loss=0.02671 | best_loss=0.02668
Epoch 8/80: current_loss=0.02701 | best_loss=0.02668
Epoch 9/80: current_loss=0.02682 | best_loss=0.02668
Epoch 10/80: current_loss=0.02671 | best_loss=0.02668
Epoch 11/80: current_loss=0.02712 | best_loss=0.02668
Epoch 12/80: current_loss=0.02695 | best_loss=0.02668
Epoch 13/80: current_loss=0.02683 | best_loss=0.02668
Epoch 14/80: current_loss=0.02687 | best_loss=0.02668
Epoch 15/80: current_loss=0.02683 | best_loss=0.02668
Epoch 16/80: current_loss=0.02669 | best_loss=0.02668
Epoch 17/80: current_loss=0.02674 | best_loss=0.02668
Epoch 18/80: current_loss=0.02686 | best_loss=0.02668
Epoch 19/80: current_loss=0.02673 | best_loss=0.02668
Epoch 20/80: current_loss=0.02683 | best_loss=0.02668
Epoch 21/80: current_loss=0.02674 | best_loss=0.02668
Epoch 22/80: current_loss=0.02676 | best_loss=0.02668
Early Stopping at epoch 22
      explained_var=0.00075 | mse_loss=0.02497
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03013 | best_loss=0.03013
Epoch 1/80: current_loss=0.02983 | best_loss=0.02983
Epoch 2/80: current_loss=0.03040 | best_loss=0.02983
Epoch 3/80: current_loss=0.02989 | best_loss=0.02983
Epoch 4/80: current_loss=0.02989 | best_loss=0.02983
Epoch 5/80: current_loss=0.02974 | best_loss=0.02974
Epoch 6/80: current_loss=0.02982 | best_loss=0.02974
Epoch 7/80: current_loss=0.02974 | best_loss=0.02974
Epoch 8/80: current_loss=0.02974 | best_loss=0.02974
Epoch 9/80: current_loss=0.02981 | best_loss=0.02974
Epoch 10/80: current_loss=0.02981 | best_loss=0.02974
Epoch 11/80: current_loss=0.02977 | best_loss=0.02974
Epoch 12/80: current_loss=0.03004 | best_loss=0.02974
Epoch 13/80: current_loss=0.03008 | best_loss=0.02974
Epoch 14/80: current_loss=0.02992 | best_loss=0.02974
Epoch 15/80: current_loss=0.02974 | best_loss=0.02974
Epoch 16/80: current_loss=0.02974 | best_loss=0.02974
Epoch 17/80: current_loss=0.02984 | best_loss=0.02974
Epoch 18/80: current_loss=0.03036 | best_loss=0.02974
Epoch 19/80: current_loss=0.02981 | best_loss=0.02974
Epoch 20/80: current_loss=0.02973 | best_loss=0.02973
Epoch 21/80: current_loss=0.02973 | best_loss=0.02973
Epoch 22/80: current_loss=0.02985 | best_loss=0.02973
Epoch 23/80: current_loss=0.02979 | best_loss=0.02973
Epoch 24/80: current_loss=0.02978 | best_loss=0.02973
Epoch 25/80: current_loss=0.02985 | best_loss=0.02973
Epoch 26/80: current_loss=0.02995 | best_loss=0.02973
Epoch 27/80: current_loss=0.02978 | best_loss=0.02973
Epoch 28/80: current_loss=0.02974 | best_loss=0.02973
Epoch 29/80: current_loss=0.02979 | best_loss=0.02973
Epoch 30/80: current_loss=0.02974 | best_loss=0.02973
Epoch 31/80: current_loss=0.02975 | best_loss=0.02973
Epoch 32/80: current_loss=0.02977 | best_loss=0.02973
Epoch 33/80: current_loss=0.02997 | best_loss=0.02973
Epoch 34/80: current_loss=0.02979 | best_loss=0.02973
Epoch 35/80: current_loss=0.03034 | best_loss=0.02973
Epoch 36/80: current_loss=0.02976 | best_loss=0.02973
Epoch 37/80: current_loss=0.02980 | best_loss=0.02973
Epoch 38/80: current_loss=0.02977 | best_loss=0.02973
Epoch 39/80: current_loss=0.02996 | best_loss=0.02973
Epoch 40/80: current_loss=0.02974 | best_loss=0.02973
Early Stopping at epoch 40
      explained_var=0.00057 | mse_loss=0.02874
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=-0.00094| avg_loss=0.02698
----------------------------------------------


----------------------------------------------
Params for Trial 34
{'learning_rate': 0.001, 'weight_decay': 0.0005457017100529565, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03381 | best_loss=0.03381
Epoch 1/80: current_loss=0.02931 | best_loss=0.02931
Epoch 2/80: current_loss=0.02915 | best_loss=0.02915
Epoch 3/80: current_loss=0.03075 | best_loss=0.02915
Epoch 4/80: current_loss=0.03229 | best_loss=0.02915
Epoch 5/80: current_loss=0.02963 | best_loss=0.02915
Epoch 6/80: current_loss=0.02905 | best_loss=0.02905
Epoch 7/80: current_loss=0.03069 | best_loss=0.02905
Epoch 8/80: current_loss=0.02969 | best_loss=0.02905
Epoch 9/80: current_loss=0.02981 | best_loss=0.02905
Epoch 10/80: current_loss=0.02995 | best_loss=0.02905
Epoch 11/80: current_loss=0.03124 | best_loss=0.02905
Epoch 12/80: current_loss=0.03345 | best_loss=0.02905
Epoch 13/80: current_loss=0.03037 | best_loss=0.02905
Epoch 14/80: current_loss=0.02908 | best_loss=0.02905
Epoch 15/80: current_loss=0.03103 | best_loss=0.02905
Epoch 16/80: current_loss=0.03021 | best_loss=0.02905
Epoch 17/80: current_loss=0.03065 | best_loss=0.02905
Epoch 18/80: current_loss=0.02993 | best_loss=0.02905
Epoch 19/80: current_loss=0.02937 | best_loss=0.02905
Epoch 20/80: current_loss=0.02915 | best_loss=0.02905
Epoch 21/80: current_loss=0.02930 | best_loss=0.02905
Epoch 22/80: current_loss=0.02969 | best_loss=0.02905
Epoch 23/80: current_loss=0.02937 | best_loss=0.02905
Epoch 24/80: current_loss=0.02999 | best_loss=0.02905
Epoch 25/80: current_loss=0.02947 | best_loss=0.02905
Epoch 26/80: current_loss=0.02928 | best_loss=0.02905
Early Stopping at epoch 26
      explained_var=0.00145 | mse_loss=0.02830

----------------------------------------------
Params for Trial 35
{'learning_rate': 0.01, 'weight_decay': 0.0011861665083553897, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02912 | best_loss=0.02912
Epoch 1/80: current_loss=0.02917 | best_loss=0.02912
Epoch 2/80: current_loss=0.02927 | best_loss=0.02912
Epoch 3/80: current_loss=0.02914 | best_loss=0.02912
Epoch 4/80: current_loss=0.02913 | best_loss=0.02912
Epoch 5/80: current_loss=0.02936 | best_loss=0.02912
Epoch 6/80: current_loss=0.02922 | best_loss=0.02912
Epoch 7/80: current_loss=0.02929 | best_loss=0.02912
Epoch 8/80: current_loss=0.02923 | best_loss=0.02912
Epoch 9/80: current_loss=0.02931 | best_loss=0.02912
Epoch 10/80: current_loss=0.02922 | best_loss=0.02912
Epoch 11/80: current_loss=0.02924 | best_loss=0.02912
Epoch 12/80: current_loss=0.02936 | best_loss=0.02912
Epoch 13/80: current_loss=0.02924 | best_loss=0.02912
Epoch 14/80: current_loss=0.02913 | best_loss=0.02912
Epoch 15/80: current_loss=0.02912 | best_loss=0.02912
Epoch 16/80: current_loss=0.02950 | best_loss=0.02912
Epoch 17/80: current_loss=0.02950 | best_loss=0.02912
Epoch 18/80: current_loss=0.02911 | best_loss=0.02911
Epoch 19/80: current_loss=0.02917 | best_loss=0.02911
Epoch 20/80: current_loss=0.02911 | best_loss=0.02911
Epoch 21/80: current_loss=0.02951 | best_loss=0.02911
Epoch 22/80: current_loss=0.02924 | best_loss=0.02911
Epoch 23/80: current_loss=0.02962 | best_loss=0.02911
Epoch 24/80: current_loss=0.02967 | best_loss=0.02911
Epoch 25/80: current_loss=0.02912 | best_loss=0.02911
Epoch 26/80: current_loss=0.02920 | best_loss=0.02911
Epoch 27/80: current_loss=0.02920 | best_loss=0.02911
Epoch 28/80: current_loss=0.02978 | best_loss=0.02911
Epoch 29/80: current_loss=0.02916 | best_loss=0.02911
Epoch 30/80: current_loss=0.02912 | best_loss=0.02911
Epoch 31/80: current_loss=0.02966 | best_loss=0.02911
Epoch 32/80: current_loss=0.02917 | best_loss=0.02911
Epoch 33/80: current_loss=0.02912 | best_loss=0.02911
Epoch 34/80: current_loss=0.02918 | best_loss=0.02911
Epoch 35/80: current_loss=0.02912 | best_loss=0.02911
Epoch 36/80: current_loss=0.02913 | best_loss=0.02911
Epoch 37/80: current_loss=0.02953 | best_loss=0.02911
Epoch 38/80: current_loss=0.02944 | best_loss=0.02911
Early Stopping at epoch 38
      explained_var=-0.00000 | mse_loss=0.02815

----------------------------------------------
Params for Trial 36
{'learning_rate': 0.001, 'weight_decay': 0.0030263023638658747, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02898 | best_loss=0.02898
Epoch 1/80: current_loss=0.02905 | best_loss=0.02898
Epoch 2/80: current_loss=0.02938 | best_loss=0.02898
Epoch 3/80: current_loss=0.02897 | best_loss=0.02897
Epoch 4/80: current_loss=0.02921 | best_loss=0.02897
Epoch 5/80: current_loss=0.02906 | best_loss=0.02897
Epoch 6/80: current_loss=0.02899 | best_loss=0.02897
Epoch 7/80: current_loss=0.02905 | best_loss=0.02897
Epoch 8/80: current_loss=0.03159 | best_loss=0.02897
Epoch 9/80: current_loss=0.02902 | best_loss=0.02897
Epoch 10/80: current_loss=0.02905 | best_loss=0.02897
Epoch 11/80: current_loss=0.02956 | best_loss=0.02897
Epoch 12/80: current_loss=0.02911 | best_loss=0.02897
Epoch 13/80: current_loss=0.02911 | best_loss=0.02897
Epoch 14/80: current_loss=0.02938 | best_loss=0.02897
Epoch 15/80: current_loss=0.02972 | best_loss=0.02897
Epoch 16/80: current_loss=0.02901 | best_loss=0.02897
Epoch 17/80: current_loss=0.02912 | best_loss=0.02897
Epoch 18/80: current_loss=0.02913 | best_loss=0.02897
Epoch 19/80: current_loss=0.02905 | best_loss=0.02897
Epoch 20/80: current_loss=0.02907 | best_loss=0.02897
Epoch 21/80: current_loss=0.02928 | best_loss=0.02897
Epoch 22/80: current_loss=0.03088 | best_loss=0.02897
Epoch 23/80: current_loss=0.02910 | best_loss=0.02897
Early Stopping at epoch 23
      explained_var=0.00306 | mse_loss=0.02809
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02591 | best_loss=0.02591
Epoch 1/80: current_loss=0.02524 | best_loss=0.02524
Epoch 2/80: current_loss=0.02503 | best_loss=0.02503
Epoch 3/80: current_loss=0.02501 | best_loss=0.02501
Epoch 4/80: current_loss=0.02496 | best_loss=0.02496
Epoch 5/80: current_loss=0.02540 | best_loss=0.02496
Epoch 6/80: current_loss=0.02496 | best_loss=0.02496
Epoch 7/80: current_loss=0.02574 | best_loss=0.02496
Epoch 8/80: current_loss=0.02543 | best_loss=0.02496
Epoch 9/80: current_loss=0.02535 | best_loss=0.02496
Epoch 10/80: current_loss=0.02506 | best_loss=0.02496
Epoch 11/80: current_loss=0.02509 | best_loss=0.02496
Epoch 12/80: current_loss=0.02501 | best_loss=0.02496
Epoch 13/80: current_loss=0.02523 | best_loss=0.02496
Epoch 14/80: current_loss=0.02506 | best_loss=0.02496
Epoch 15/80: current_loss=0.02552 | best_loss=0.02496
Epoch 16/80: current_loss=0.02520 | best_loss=0.02496
Epoch 17/80: current_loss=0.02507 | best_loss=0.02496
Epoch 18/80: current_loss=0.02512 | best_loss=0.02496
Epoch 19/80: current_loss=0.02502 | best_loss=0.02496
Epoch 20/80: current_loss=0.02505 | best_loss=0.02496
Epoch 21/80: current_loss=0.02500 | best_loss=0.02496
Epoch 22/80: current_loss=0.02501 | best_loss=0.02496
Epoch 23/80: current_loss=0.02502 | best_loss=0.02496
Epoch 24/80: current_loss=0.02532 | best_loss=0.02496
Early Stopping at epoch 24
      explained_var=0.00119 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02784 | best_loss=0.02784
Epoch 1/80: current_loss=0.02788 | best_loss=0.02784
Epoch 2/80: current_loss=0.02734 | best_loss=0.02734
Epoch 3/80: current_loss=0.02832 | best_loss=0.02734
Epoch 4/80: current_loss=0.02793 | best_loss=0.02734
Epoch 5/80: current_loss=0.02843 | best_loss=0.02734
Epoch 6/80: current_loss=0.02765 | best_loss=0.02734
Epoch 7/80: current_loss=0.02728 | best_loss=0.02728
Epoch 8/80: current_loss=0.02803 | best_loss=0.02728
Epoch 9/80: current_loss=0.02819 | best_loss=0.02728
Epoch 10/80: current_loss=0.02741 | best_loss=0.02728
Epoch 11/80: current_loss=0.02879 | best_loss=0.02728
Epoch 12/80: current_loss=0.02784 | best_loss=0.02728
Epoch 13/80: current_loss=0.02783 | best_loss=0.02728
Epoch 14/80: current_loss=0.02725 | best_loss=0.02725
Epoch 15/80: current_loss=0.02782 | best_loss=0.02725
Epoch 16/80: current_loss=0.02741 | best_loss=0.02725
Epoch 17/80: current_loss=0.02758 | best_loss=0.02725
Epoch 18/80: current_loss=0.02820 | best_loss=0.02725
Epoch 19/80: current_loss=0.02865 | best_loss=0.02725
Epoch 20/80: current_loss=0.02745 | best_loss=0.02725
Epoch 21/80: current_loss=0.02773 | best_loss=0.02725
Epoch 22/80: current_loss=0.02726 | best_loss=0.02725
Epoch 23/80: current_loss=0.02768 | best_loss=0.02725
Epoch 24/80: current_loss=0.02742 | best_loss=0.02725
Epoch 25/80: current_loss=0.02774 | best_loss=0.02725
Epoch 26/80: current_loss=0.02780 | best_loss=0.02725
Epoch 27/80: current_loss=0.02730 | best_loss=0.02725
Epoch 28/80: current_loss=0.02860 | best_loss=0.02725
Epoch 29/80: current_loss=0.02730 | best_loss=0.02725
Epoch 30/80: current_loss=0.02858 | best_loss=0.02725
Epoch 31/80: current_loss=0.02804 | best_loss=0.02725
Epoch 32/80: current_loss=0.02759 | best_loss=0.02725
Epoch 33/80: current_loss=0.02750 | best_loss=0.02725
Epoch 34/80: current_loss=0.02816 | best_loss=0.02725
Early Stopping at epoch 34
      explained_var=-0.00187 | mse_loss=0.02764
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02683 | best_loss=0.02683
Epoch 1/80: current_loss=0.02685 | best_loss=0.02683
Epoch 2/80: current_loss=0.02681 | best_loss=0.02681
Epoch 3/80: current_loss=0.02680 | best_loss=0.02680
Epoch 4/80: current_loss=0.02708 | best_loss=0.02680
Epoch 5/80: current_loss=0.02696 | best_loss=0.02680
Epoch 6/80: current_loss=0.02683 | best_loss=0.02680
Epoch 7/80: current_loss=0.02709 | best_loss=0.02680
Epoch 8/80: current_loss=0.02681 | best_loss=0.02680
Epoch 9/80: current_loss=0.02688 | best_loss=0.02680
Epoch 10/80: current_loss=0.02685 | best_loss=0.02680
Epoch 11/80: current_loss=0.02679 | best_loss=0.02679
Epoch 12/80: current_loss=0.02679 | best_loss=0.02679
Epoch 13/80: current_loss=0.02704 | best_loss=0.02679
Epoch 14/80: current_loss=0.02692 | best_loss=0.02679
Epoch 15/80: current_loss=0.02703 | best_loss=0.02679
Epoch 16/80: current_loss=0.02683 | best_loss=0.02679
Epoch 17/80: current_loss=0.02692 | best_loss=0.02679
Epoch 18/80: current_loss=0.02683 | best_loss=0.02679
Epoch 19/80: current_loss=0.02685 | best_loss=0.02679
Epoch 20/80: current_loss=0.02683 | best_loss=0.02679
Epoch 21/80: current_loss=0.02680 | best_loss=0.02679
Epoch 22/80: current_loss=0.02684 | best_loss=0.02679
Epoch 23/80: current_loss=0.02705 | best_loss=0.02679
Epoch 24/80: current_loss=0.02710 | best_loss=0.02679
Epoch 25/80: current_loss=0.02681 | best_loss=0.02679
Epoch 26/80: current_loss=0.02691 | best_loss=0.02679
Epoch 27/80: current_loss=0.02692 | best_loss=0.02679
Epoch 28/80: current_loss=0.02680 | best_loss=0.02679
Epoch 29/80: current_loss=0.02688 | best_loss=0.02679
Epoch 30/80: current_loss=0.02680 | best_loss=0.02679
Epoch 31/80: current_loss=0.02698 | best_loss=0.02679
Early Stopping at epoch 31
      explained_var=0.00016 | mse_loss=0.02499

----------------------------------------------
Params for Trial 37
{'learning_rate': 0.1, 'weight_decay': 0.0004231001464642373, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.10873 | best_loss=0.10873
Epoch 1/80: current_loss=0.03187 | best_loss=0.03187
Epoch 2/80: current_loss=0.23282 | best_loss=0.03187
Epoch 3/80: current_loss=0.05210 | best_loss=0.03187
Epoch 4/80: current_loss=0.06925 | best_loss=0.03187
Epoch 5/80: current_loss=0.20648 | best_loss=0.03187
Epoch 6/80: current_loss=0.20320 | best_loss=0.03187
Epoch 7/80: current_loss=0.07442 | best_loss=0.03187
Epoch 8/80: current_loss=0.14583 | best_loss=0.03187
Epoch 9/80: current_loss=0.03502 | best_loss=0.03187
Epoch 10/80: current_loss=0.14484 | best_loss=0.03187
Epoch 11/80: current_loss=0.18338 | best_loss=0.03187
Epoch 12/80: current_loss=0.13309 | best_loss=0.03187
Epoch 13/80: current_loss=0.03284 | best_loss=0.03187
Epoch 14/80: current_loss=0.68834 | best_loss=0.03187
Epoch 15/80: current_loss=0.27334 | best_loss=0.03187
Epoch 16/80: current_loss=0.31946 | best_loss=0.03187
Epoch 17/80: current_loss=0.03434 | best_loss=0.03187
Epoch 18/80: current_loss=0.05457 | best_loss=0.03187
Epoch 19/80: current_loss=0.15339 | best_loss=0.03187
Epoch 20/80: current_loss=0.04045 | best_loss=0.03187
Epoch 21/80: current_loss=0.23278 | best_loss=0.03187
Early Stopping at epoch 21
      explained_var=-0.00856 | mse_loss=0.03080

----------------------------------------------
Params for Trial 38
{'learning_rate': 0.0001, 'weight_decay': 0.0016697509857594326, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03005 | best_loss=0.03005
Epoch 1/80: current_loss=0.02942 | best_loss=0.02942
Epoch 2/80: current_loss=0.02945 | best_loss=0.02942
Epoch 3/80: current_loss=0.02921 | best_loss=0.02921
Epoch 4/80: current_loss=0.02917 | best_loss=0.02917
Epoch 5/80: current_loss=0.02915 | best_loss=0.02915
Epoch 6/80: current_loss=0.02909 | best_loss=0.02909
Epoch 7/80: current_loss=0.02960 | best_loss=0.02909
Epoch 8/80: current_loss=0.02908 | best_loss=0.02908
Epoch 9/80: current_loss=0.03018 | best_loss=0.02908
Epoch 10/80: current_loss=0.02970 | best_loss=0.02908
Epoch 11/80: current_loss=0.02970 | best_loss=0.02908
Epoch 12/80: current_loss=0.03040 | best_loss=0.02908
Epoch 13/80: current_loss=0.03117 | best_loss=0.02908
Epoch 14/80: current_loss=0.02925 | best_loss=0.02908
Epoch 15/80: current_loss=0.02900 | best_loss=0.02900
Epoch 16/80: current_loss=0.02999 | best_loss=0.02900
Epoch 17/80: current_loss=0.02933 | best_loss=0.02900
Epoch 18/80: current_loss=0.02899 | best_loss=0.02899
Epoch 19/80: current_loss=0.02897 | best_loss=0.02897
Epoch 20/80: current_loss=0.02944 | best_loss=0.02897
Epoch 21/80: current_loss=0.02905 | best_loss=0.02897
Epoch 22/80: current_loss=0.02892 | best_loss=0.02892
Epoch 23/80: current_loss=0.02897 | best_loss=0.02892
Epoch 24/80: current_loss=0.02903 | best_loss=0.02892
Epoch 25/80: current_loss=0.03004 | best_loss=0.02892
Epoch 26/80: current_loss=0.03024 | best_loss=0.02892
Epoch 27/80: current_loss=0.02982 | best_loss=0.02892
Epoch 28/80: current_loss=0.02929 | best_loss=0.02892
Epoch 29/80: current_loss=0.02911 | best_loss=0.02892
Epoch 30/80: current_loss=0.02907 | best_loss=0.02892
Epoch 31/80: current_loss=0.02951 | best_loss=0.02892
Epoch 32/80: current_loss=0.02899 | best_loss=0.02892
Epoch 33/80: current_loss=0.02905 | best_loss=0.02892
Epoch 34/80: current_loss=0.02933 | best_loss=0.02892
Epoch 35/80: current_loss=0.02907 | best_loss=0.02892
Epoch 36/80: current_loss=0.02951 | best_loss=0.02892
Epoch 37/80: current_loss=0.02909 | best_loss=0.02892
Epoch 38/80: current_loss=0.02913 | best_loss=0.02892
Epoch 39/80: current_loss=0.02961 | best_loss=0.02892
Epoch 40/80: current_loss=0.02943 | best_loss=0.02892
Epoch 41/80: current_loss=0.02906 | best_loss=0.02892
Epoch 42/80: current_loss=0.02916 | best_loss=0.02892
Early Stopping at epoch 42
      explained_var=0.00385 | mse_loss=0.02808
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02618 | best_loss=0.02618
Epoch 1/80: current_loss=0.02596 | best_loss=0.02596
Epoch 2/80: current_loss=0.02504 | best_loss=0.02504
Epoch 3/80: current_loss=0.02585 | best_loss=0.02504
Epoch 4/80: current_loss=0.02488 | best_loss=0.02488
Epoch 5/80: current_loss=0.02497 | best_loss=0.02488
Epoch 6/80: current_loss=0.02569 | best_loss=0.02488
Epoch 7/80: current_loss=0.02553 | best_loss=0.02488
Epoch 8/80: current_loss=0.02508 | best_loss=0.02488
Epoch 9/80: current_loss=0.02517 | best_loss=0.02488
Epoch 10/80: current_loss=0.02482 | best_loss=0.02482
Epoch 11/80: current_loss=0.02491 | best_loss=0.02482
Epoch 12/80: current_loss=0.02517 | best_loss=0.02482
Epoch 13/80: current_loss=0.02507 | best_loss=0.02482
Epoch 14/80: current_loss=0.02538 | best_loss=0.02482
Epoch 15/80: current_loss=0.02615 | best_loss=0.02482
Epoch 16/80: current_loss=0.02769 | best_loss=0.02482
Epoch 17/80: current_loss=0.02567 | best_loss=0.02482
Epoch 18/80: current_loss=0.02549 | best_loss=0.02482
Epoch 19/80: current_loss=0.02497 | best_loss=0.02482
Epoch 20/80: current_loss=0.02519 | best_loss=0.02482
Epoch 21/80: current_loss=0.02573 | best_loss=0.02482
Epoch 22/80: current_loss=0.02490 | best_loss=0.02482
Epoch 23/80: current_loss=0.02488 | best_loss=0.02482
Epoch 24/80: current_loss=0.02589 | best_loss=0.02482
Epoch 25/80: current_loss=0.02491 | best_loss=0.02482
Epoch 26/80: current_loss=0.02542 | best_loss=0.02482
Epoch 27/80: current_loss=0.02514 | best_loss=0.02482
Epoch 28/80: current_loss=0.02526 | best_loss=0.02482
Epoch 29/80: current_loss=0.02495 | best_loss=0.02482
Epoch 30/80: current_loss=0.02501 | best_loss=0.02482
Early Stopping at epoch 30
      explained_var=0.00206 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02819 | best_loss=0.02819
Epoch 1/80: current_loss=0.02774 | best_loss=0.02774
Epoch 2/80: current_loss=0.02750 | best_loss=0.02750
Epoch 3/80: current_loss=0.02824 | best_loss=0.02750
Epoch 4/80: current_loss=0.02769 | best_loss=0.02750
Epoch 5/80: current_loss=0.02799 | best_loss=0.02750
Epoch 6/80: current_loss=0.02785 | best_loss=0.02750
Epoch 7/80: current_loss=0.02760 | best_loss=0.02750
Epoch 8/80: current_loss=0.02881 | best_loss=0.02750
Epoch 9/80: current_loss=0.02775 | best_loss=0.02750
Epoch 10/80: current_loss=0.02740 | best_loss=0.02740
Epoch 11/80: current_loss=0.02758 | best_loss=0.02740
Epoch 12/80: current_loss=0.02797 | best_loss=0.02740
Epoch 13/80: current_loss=0.02773 | best_loss=0.02740
Epoch 14/80: current_loss=0.02966 | best_loss=0.02740
Epoch 15/80: current_loss=0.02906 | best_loss=0.02740
Epoch 16/80: current_loss=0.02801 | best_loss=0.02740
Epoch 17/80: current_loss=0.02744 | best_loss=0.02740
Epoch 18/80: current_loss=0.02747 | best_loss=0.02740
Epoch 19/80: current_loss=0.02806 | best_loss=0.02740
Epoch 20/80: current_loss=0.02736 | best_loss=0.02736
Epoch 21/80: current_loss=0.02772 | best_loss=0.02736
Epoch 22/80: current_loss=0.02765 | best_loss=0.02736
Epoch 23/80: current_loss=0.02777 | best_loss=0.02736
Epoch 24/80: current_loss=0.03008 | best_loss=0.02736
Epoch 25/80: current_loss=0.02857 | best_loss=0.02736
Epoch 26/80: current_loss=0.02792 | best_loss=0.02736
Epoch 27/80: current_loss=0.02879 | best_loss=0.02736
Epoch 28/80: current_loss=0.02737 | best_loss=0.02736
Epoch 29/80: current_loss=0.02787 | best_loss=0.02736
Epoch 30/80: current_loss=0.02881 | best_loss=0.02736
Epoch 31/80: current_loss=0.02799 | best_loss=0.02736
Epoch 32/80: current_loss=0.02752 | best_loss=0.02736
Epoch 33/80: current_loss=0.02790 | best_loss=0.02736
Epoch 34/80: current_loss=0.02742 | best_loss=0.02736
Epoch 35/80: current_loss=0.02772 | best_loss=0.02736
Epoch 36/80: current_loss=0.02761 | best_loss=0.02736
Epoch 37/80: current_loss=0.02740 | best_loss=0.02736
Epoch 38/80: current_loss=0.02748 | best_loss=0.02736
Epoch 39/80: current_loss=0.02918 | best_loss=0.02736
Epoch 40/80: current_loss=0.02740 | best_loss=0.02736
Early Stopping at epoch 40
      explained_var=-0.01061 | mse_loss=0.02786
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02796 | best_loss=0.02796
Epoch 1/80: current_loss=0.02676 | best_loss=0.02676
Epoch 2/80: current_loss=0.02775 | best_loss=0.02676
Epoch 3/80: current_loss=0.02798 | best_loss=0.02676
Epoch 4/80: current_loss=0.02712 | best_loss=0.02676
Epoch 5/80: current_loss=0.02677 | best_loss=0.02676
Epoch 6/80: current_loss=0.02700 | best_loss=0.02676
Epoch 7/80: current_loss=0.02707 | best_loss=0.02676
Epoch 8/80: current_loss=0.02685 | best_loss=0.02676
Epoch 9/80: current_loss=0.02892 | best_loss=0.02676
Epoch 10/80: current_loss=0.02678 | best_loss=0.02676
Epoch 11/80: current_loss=0.02685 | best_loss=0.02676
Epoch 12/80: current_loss=0.02776 | best_loss=0.02676
Epoch 13/80: current_loss=0.02689 | best_loss=0.02676
Epoch 14/80: current_loss=0.02690 | best_loss=0.02676
Epoch 15/80: current_loss=0.02676 | best_loss=0.02676
Epoch 16/80: current_loss=0.02682 | best_loss=0.02676
Epoch 17/80: current_loss=0.02785 | best_loss=0.02676
Epoch 18/80: current_loss=0.02678 | best_loss=0.02676
Epoch 19/80: current_loss=0.02678 | best_loss=0.02676
Epoch 20/80: current_loss=0.02711 | best_loss=0.02676
Epoch 21/80: current_loss=0.02700 | best_loss=0.02676
Early Stopping at epoch 21
      explained_var=0.00139 | mse_loss=0.02496
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02978 | best_loss=0.02978
Epoch 1/80: current_loss=0.02986 | best_loss=0.02978
Epoch 2/80: current_loss=0.03040 | best_loss=0.02978
Epoch 3/80: current_loss=0.02983 | best_loss=0.02978
Epoch 4/80: current_loss=0.03033 | best_loss=0.02978
Epoch 5/80: current_loss=0.02988 | best_loss=0.02978
Epoch 6/80: current_loss=0.02977 | best_loss=0.02977
Epoch 7/80: current_loss=0.02998 | best_loss=0.02977
Epoch 8/80: current_loss=0.02977 | best_loss=0.02977
Epoch 9/80: current_loss=0.03023 | best_loss=0.02977
Epoch 10/80: current_loss=0.02973 | best_loss=0.02973
Epoch 11/80: current_loss=0.02978 | best_loss=0.02973
Epoch 12/80: current_loss=0.03331 | best_loss=0.02973
Epoch 13/80: current_loss=0.03073 | best_loss=0.02973
Epoch 14/80: current_loss=0.02974 | best_loss=0.02973
Epoch 15/80: current_loss=0.02995 | best_loss=0.02973
Epoch 16/80: current_loss=0.02974 | best_loss=0.02973
Epoch 17/80: current_loss=0.02985 | best_loss=0.02973
Epoch 18/80: current_loss=0.02991 | best_loss=0.02973
Epoch 19/80: current_loss=0.02976 | best_loss=0.02973
Epoch 20/80: current_loss=0.02979 | best_loss=0.02973
Epoch 21/80: current_loss=0.02987 | best_loss=0.02973
Epoch 22/80: current_loss=0.02976 | best_loss=0.02973
Epoch 23/80: current_loss=0.03048 | best_loss=0.02973
Epoch 24/80: current_loss=0.02988 | best_loss=0.02973
Epoch 25/80: current_loss=0.02977 | best_loss=0.02973
Epoch 26/80: current_loss=0.02999 | best_loss=0.02973
Epoch 27/80: current_loss=0.02984 | best_loss=0.02973
Epoch 28/80: current_loss=0.03004 | best_loss=0.02973
Epoch 29/80: current_loss=0.03029 | best_loss=0.02973
Epoch 30/80: current_loss=0.02999 | best_loss=0.02973
Early Stopping at epoch 30
      explained_var=0.00092 | mse_loss=0.02875
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=-0.00048| avg_loss=0.02696
----------------------------------------------


----------------------------------------------
Params for Trial 39
{'learning_rate': 1e-05, 'weight_decay': 0.005261638571127878, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.15163 | best_loss=0.15163
Epoch 1/80: current_loss=0.12383 | best_loss=0.12383
Epoch 2/80: current_loss=0.10100 | best_loss=0.10100
Epoch 3/80: current_loss=0.08262 | best_loss=0.08262
Epoch 4/80: current_loss=0.06824 | best_loss=0.06824
Epoch 5/80: current_loss=0.05719 | best_loss=0.05719
Epoch 6/80: current_loss=0.04855 | best_loss=0.04855
Epoch 7/80: current_loss=0.04272 | best_loss=0.04272
Epoch 8/80: current_loss=0.03874 | best_loss=0.03874
Epoch 9/80: current_loss=0.03626 | best_loss=0.03626
Epoch 10/80: current_loss=0.03470 | best_loss=0.03470
Epoch 11/80: current_loss=0.03377 | best_loss=0.03377
Epoch 12/80: current_loss=0.03334 | best_loss=0.03334
Epoch 13/80: current_loss=0.03306 | best_loss=0.03306
Epoch 14/80: current_loss=0.03282 | best_loss=0.03282
Epoch 15/80: current_loss=0.03263 | best_loss=0.03263
Epoch 16/80: current_loss=0.03250 | best_loss=0.03250
Epoch 17/80: current_loss=0.03238 | best_loss=0.03238
Epoch 18/80: current_loss=0.03228 | best_loss=0.03228
Epoch 19/80: current_loss=0.03215 | best_loss=0.03215
Epoch 20/80: current_loss=0.03204 | best_loss=0.03204
Epoch 21/80: current_loss=0.03193 | best_loss=0.03193
Epoch 22/80: current_loss=0.03183 | best_loss=0.03183
Epoch 23/80: current_loss=0.03177 | best_loss=0.03177
Epoch 24/80: current_loss=0.03167 | best_loss=0.03167
Epoch 25/80: current_loss=0.03161 | best_loss=0.03161
Epoch 26/80: current_loss=0.03151 | best_loss=0.03151
Epoch 27/80: current_loss=0.03143 | best_loss=0.03143
Epoch 28/80: current_loss=0.03133 | best_loss=0.03133
Epoch 29/80: current_loss=0.03125 | best_loss=0.03125
Epoch 30/80: current_loss=0.03118 | best_loss=0.03118
Epoch 31/80: current_loss=0.03111 | best_loss=0.03111
Epoch 32/80: current_loss=0.03105 | best_loss=0.03105
Epoch 33/80: current_loss=0.03098 | best_loss=0.03098
Epoch 34/80: current_loss=0.03092 | best_loss=0.03092
Epoch 35/80: current_loss=0.03087 | best_loss=0.03087
Epoch 36/80: current_loss=0.03079 | best_loss=0.03079
Epoch 37/80: current_loss=0.03074 | best_loss=0.03074
Epoch 38/80: current_loss=0.03068 | best_loss=0.03068
Epoch 39/80: current_loss=0.03063 | best_loss=0.03063
Epoch 40/80: current_loss=0.03058 | best_loss=0.03058
Epoch 41/80: current_loss=0.03053 | best_loss=0.03053
Epoch 42/80: current_loss=0.03049 | best_loss=0.03049
Epoch 43/80: current_loss=0.03044 | best_loss=0.03044
Epoch 44/80: current_loss=0.03039 | best_loss=0.03039
Epoch 45/80: current_loss=0.03033 | best_loss=0.03033
Epoch 46/80: current_loss=0.03027 | best_loss=0.03027
Epoch 47/80: current_loss=0.03023 | best_loss=0.03023
Epoch 48/80: current_loss=0.03018 | best_loss=0.03018
Epoch 49/80: current_loss=0.03014 | best_loss=0.03014
Epoch 50/80: current_loss=0.03011 | best_loss=0.03011
Epoch 51/80: current_loss=0.03007 | best_loss=0.03007
Epoch 52/80: current_loss=0.03002 | best_loss=0.03002
Epoch 53/80: current_loss=0.02999 | best_loss=0.02999
Epoch 54/80: current_loss=0.02996 | best_loss=0.02996
Epoch 55/80: current_loss=0.02992 | best_loss=0.02992
Epoch 56/80: current_loss=0.02990 | best_loss=0.02990
Epoch 57/80: current_loss=0.02986 | best_loss=0.02986
Epoch 58/80: current_loss=0.02983 | best_loss=0.02983
Epoch 59/80: current_loss=0.02981 | best_loss=0.02981
Epoch 60/80: current_loss=0.02977 | best_loss=0.02977
Epoch 61/80: current_loss=0.02974 | best_loss=0.02974
Epoch 62/80: current_loss=0.02972 | best_loss=0.02972
Epoch 63/80: current_loss=0.02970 | best_loss=0.02970
Epoch 64/80: current_loss=0.02968 | best_loss=0.02968
Epoch 65/80: current_loss=0.02966 | best_loss=0.02966
Epoch 66/80: current_loss=0.02963 | best_loss=0.02963
Epoch 67/80: current_loss=0.02961 | best_loss=0.02961
Epoch 68/80: current_loss=0.02959 | best_loss=0.02959
Epoch 69/80: current_loss=0.02957 | best_loss=0.02957
Epoch 70/80: current_loss=0.02955 | best_loss=0.02955
Epoch 71/80: current_loss=0.02952 | best_loss=0.02952
Epoch 72/80: current_loss=0.02950 | best_loss=0.02950
Epoch 73/80: current_loss=0.02947 | best_loss=0.02947
Epoch 74/80: current_loss=0.02946 | best_loss=0.02946
Epoch 75/80: current_loss=0.02944 | best_loss=0.02944
Epoch 76/80: current_loss=0.02942 | best_loss=0.02942
Epoch 77/80: current_loss=0.02940 | best_loss=0.02940
Epoch 78/80: current_loss=0.02939 | best_loss=0.02939
Epoch 79/80: current_loss=0.02938 | best_loss=0.02938
      explained_var=-0.00499 | mse_loss=0.02828

----------------------------------------------
Params for Trial 40
{'learning_rate': 0.001, 'weight_decay': 0.004329290681731437, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03155 | best_loss=0.03155
Epoch 1/80: current_loss=0.02980 | best_loss=0.02980
Epoch 2/80: current_loss=0.02914 | best_loss=0.02914
Epoch 3/80: current_loss=0.02898 | best_loss=0.02898
Epoch 4/80: current_loss=0.02907 | best_loss=0.02898
Epoch 5/80: current_loss=0.02891 | best_loss=0.02891
Epoch 6/80: current_loss=0.02912 | best_loss=0.02891
Epoch 7/80: current_loss=0.02920 | best_loss=0.02891
Epoch 8/80: current_loss=0.02894 | best_loss=0.02891
Epoch 9/80: current_loss=0.02891 | best_loss=0.02891
Epoch 10/80: current_loss=0.02897 | best_loss=0.02891
Epoch 11/80: current_loss=0.02890 | best_loss=0.02890
Epoch 12/80: current_loss=0.02900 | best_loss=0.02890
Epoch 13/80: current_loss=0.02899 | best_loss=0.02890
Epoch 14/80: current_loss=0.02924 | best_loss=0.02890
Epoch 15/80: current_loss=0.02908 | best_loss=0.02890
Epoch 16/80: current_loss=0.02928 | best_loss=0.02890
Epoch 17/80: current_loss=0.02919 | best_loss=0.02890
Epoch 18/80: current_loss=0.02898 | best_loss=0.02890
Epoch 19/80: current_loss=0.02905 | best_loss=0.02890
Epoch 20/80: current_loss=0.02904 | best_loss=0.02890
Epoch 21/80: current_loss=0.02899 | best_loss=0.02890
Epoch 22/80: current_loss=0.02907 | best_loss=0.02890
Epoch 23/80: current_loss=0.02909 | best_loss=0.02890
Epoch 24/80: current_loss=0.02928 | best_loss=0.02890
Epoch 25/80: current_loss=0.02898 | best_loss=0.02890
Epoch 26/80: current_loss=0.02898 | best_loss=0.02890
Epoch 27/80: current_loss=0.02898 | best_loss=0.02890
Epoch 28/80: current_loss=0.02930 | best_loss=0.02890
Epoch 29/80: current_loss=0.02905 | best_loss=0.02890
Epoch 30/80: current_loss=0.02898 | best_loss=0.02890
Epoch 31/80: current_loss=0.02964 | best_loss=0.02890
Early Stopping at epoch 31
      explained_var=0.00362 | mse_loss=0.02804
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02569 | best_loss=0.02569
Epoch 1/80: current_loss=0.02605 | best_loss=0.02569
Epoch 2/80: current_loss=0.02497 | best_loss=0.02497
Epoch 3/80: current_loss=0.02532 | best_loss=0.02497
Epoch 4/80: current_loss=0.02510 | best_loss=0.02497
Epoch 5/80: current_loss=0.02531 | best_loss=0.02497
Epoch 6/80: current_loss=0.02494 | best_loss=0.02494
Epoch 7/80: current_loss=0.02508 | best_loss=0.02494
Epoch 8/80: current_loss=0.02534 | best_loss=0.02494
Epoch 9/80: current_loss=0.02523 | best_loss=0.02494
Epoch 10/80: current_loss=0.02500 | best_loss=0.02494
Epoch 11/80: current_loss=0.02495 | best_loss=0.02494
Epoch 12/80: current_loss=0.02551 | best_loss=0.02494
Epoch 13/80: current_loss=0.02499 | best_loss=0.02494
Epoch 14/80: current_loss=0.02499 | best_loss=0.02494
Epoch 15/80: current_loss=0.02611 | best_loss=0.02494
Epoch 16/80: current_loss=0.02584 | best_loss=0.02494
Epoch 17/80: current_loss=0.02516 | best_loss=0.02494
Epoch 18/80: current_loss=0.02505 | best_loss=0.02494
Epoch 19/80: current_loss=0.02518 | best_loss=0.02494
Epoch 20/80: current_loss=0.02621 | best_loss=0.02494
Epoch 21/80: current_loss=0.02525 | best_loss=0.02494
Epoch 22/80: current_loss=0.02523 | best_loss=0.02494
Epoch 23/80: current_loss=0.02509 | best_loss=0.02494
Epoch 24/80: current_loss=0.02501 | best_loss=0.02494
Epoch 25/80: current_loss=0.02512 | best_loss=0.02494
Epoch 26/80: current_loss=0.02501 | best_loss=0.02494
Early Stopping at epoch 26
      explained_var=0.00159 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02802 | best_loss=0.02802
Epoch 1/80: current_loss=0.02800 | best_loss=0.02800
Epoch 2/80: current_loss=0.02762 | best_loss=0.02762
Epoch 3/80: current_loss=0.02794 | best_loss=0.02762
Epoch 4/80: current_loss=0.02884 | best_loss=0.02762
Epoch 5/80: current_loss=0.02723 | best_loss=0.02723
Epoch 6/80: current_loss=0.02789 | best_loss=0.02723
Epoch 7/80: current_loss=0.02743 | best_loss=0.02723
Epoch 8/80: current_loss=0.02836 | best_loss=0.02723
Epoch 9/80: current_loss=0.02783 | best_loss=0.02723
Epoch 10/80: current_loss=0.02749 | best_loss=0.02723
Epoch 11/80: current_loss=0.02835 | best_loss=0.02723
Epoch 12/80: current_loss=0.02759 | best_loss=0.02723
Epoch 13/80: current_loss=0.02748 | best_loss=0.02723
Epoch 14/80: current_loss=0.02887 | best_loss=0.02723
Epoch 15/80: current_loss=0.02723 | best_loss=0.02723
Epoch 16/80: current_loss=0.02750 | best_loss=0.02723
Epoch 17/80: current_loss=0.02727 | best_loss=0.02723
Epoch 18/80: current_loss=0.02755 | best_loss=0.02723
Epoch 19/80: current_loss=0.02738 | best_loss=0.02723
Epoch 20/80: current_loss=0.02742 | best_loss=0.02723
Epoch 21/80: current_loss=0.02748 | best_loss=0.02723
Epoch 22/80: current_loss=0.02759 | best_loss=0.02723
Epoch 23/80: current_loss=0.02759 | best_loss=0.02723
Epoch 24/80: current_loss=0.02737 | best_loss=0.02723
Epoch 25/80: current_loss=0.02783 | best_loss=0.02723
Early Stopping at epoch 25
      explained_var=-0.00265 | mse_loss=0.02764
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02682 | best_loss=0.02682
Epoch 1/80: current_loss=0.02678 | best_loss=0.02678
Epoch 2/80: current_loss=0.02694 | best_loss=0.02678
Epoch 3/80: current_loss=0.02716 | best_loss=0.02678
Epoch 4/80: current_loss=0.02687 | best_loss=0.02678
Epoch 5/80: current_loss=0.02682 | best_loss=0.02678
Epoch 6/80: current_loss=0.02680 | best_loss=0.02678
Epoch 7/80: current_loss=0.02684 | best_loss=0.02678
Epoch 8/80: current_loss=0.02682 | best_loss=0.02678
Epoch 9/80: current_loss=0.02705 | best_loss=0.02678
Epoch 10/80: current_loss=0.02702 | best_loss=0.02678
Epoch 11/80: current_loss=0.02679 | best_loss=0.02678
Epoch 12/80: current_loss=0.02705 | best_loss=0.02678
Epoch 13/80: current_loss=0.02677 | best_loss=0.02677
Epoch 14/80: current_loss=0.02692 | best_loss=0.02677
Epoch 15/80: current_loss=0.02681 | best_loss=0.02677
Epoch 16/80: current_loss=0.02694 | best_loss=0.02677
Epoch 17/80: current_loss=0.02697 | best_loss=0.02677
Epoch 18/80: current_loss=0.02687 | best_loss=0.02677
Epoch 19/80: current_loss=0.02680 | best_loss=0.02677
Epoch 20/80: current_loss=0.02679 | best_loss=0.02677
Epoch 21/80: current_loss=0.02681 | best_loss=0.02677
Epoch 22/80: current_loss=0.02679 | best_loss=0.02677
Epoch 23/80: current_loss=0.02678 | best_loss=0.02677
Epoch 24/80: current_loss=0.02697 | best_loss=0.02677
Epoch 25/80: current_loss=0.02680 | best_loss=0.02677
Epoch 26/80: current_loss=0.02680 | best_loss=0.02677
Epoch 27/80: current_loss=0.02701 | best_loss=0.02677
Epoch 28/80: current_loss=0.02699 | best_loss=0.02677
Epoch 29/80: current_loss=0.02720 | best_loss=0.02677
Epoch 30/80: current_loss=0.02694 | best_loss=0.02677
Epoch 31/80: current_loss=0.02699 | best_loss=0.02677
Epoch 32/80: current_loss=0.02687 | best_loss=0.02677
Epoch 33/80: current_loss=0.02679 | best_loss=0.02677
Early Stopping at epoch 33
      explained_var=0.00120 | mse_loss=0.02496
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02978 | best_loss=0.02978
Epoch 1/80: current_loss=0.02991 | best_loss=0.02978
Epoch 2/80: current_loss=0.02983 | best_loss=0.02978
Epoch 3/80: current_loss=0.02982 | best_loss=0.02978
Epoch 4/80: current_loss=0.02988 | best_loss=0.02978
Epoch 5/80: current_loss=0.02978 | best_loss=0.02978
Epoch 6/80: current_loss=0.02979 | best_loss=0.02978
Epoch 7/80: current_loss=0.02975 | best_loss=0.02975
Epoch 8/80: current_loss=0.02979 | best_loss=0.02975
Epoch 9/80: current_loss=0.02978 | best_loss=0.02975
Epoch 10/80: current_loss=0.02975 | best_loss=0.02975
Epoch 11/80: current_loss=0.02977 | best_loss=0.02975
Epoch 12/80: current_loss=0.02976 | best_loss=0.02975
Epoch 13/80: current_loss=0.02975 | best_loss=0.02975
Epoch 14/80: current_loss=0.02979 | best_loss=0.02975
Epoch 15/80: current_loss=0.02977 | best_loss=0.02975
Epoch 16/80: current_loss=0.02985 | best_loss=0.02975
Epoch 17/80: current_loss=0.02976 | best_loss=0.02975
Epoch 18/80: current_loss=0.02975 | best_loss=0.02975
Epoch 19/80: current_loss=0.02981 | best_loss=0.02975
Epoch 20/80: current_loss=0.02975 | best_loss=0.02975
Epoch 21/80: current_loss=0.02975 | best_loss=0.02975
Epoch 22/80: current_loss=0.02979 | best_loss=0.02975
Epoch 23/80: current_loss=0.02975 | best_loss=0.02975
Epoch 24/80: current_loss=0.02980 | best_loss=0.02975
Epoch 25/80: current_loss=0.02976 | best_loss=0.02975
Epoch 26/80: current_loss=0.02976 | best_loss=0.02975
Epoch 27/80: current_loss=0.02976 | best_loss=0.02975
Epoch 28/80: current_loss=0.02976 | best_loss=0.02975
Epoch 29/80: current_loss=0.02976 | best_loss=0.02975
Epoch 30/80: current_loss=0.02988 | best_loss=0.02975
Epoch 31/80: current_loss=0.02977 | best_loss=0.02975
Epoch 32/80: current_loss=0.02978 | best_loss=0.02975
Epoch 33/80: current_loss=0.02976 | best_loss=0.02975
Early Stopping at epoch 33
      explained_var=0.00108 | mse_loss=0.02873
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.00097| avg_loss=0.02691
----------------------------------------------


----------------------------------------------
Params for Trial 41
{'learning_rate': 0.001, 'weight_decay': 0.004752798878469006, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02972 | best_loss=0.02972
Epoch 1/80: current_loss=0.02905 | best_loss=0.02905
Epoch 2/80: current_loss=0.02926 | best_loss=0.02905
Epoch 3/80: current_loss=0.02906 | best_loss=0.02905
Epoch 4/80: current_loss=0.02892 | best_loss=0.02892
Epoch 5/80: current_loss=0.02894 | best_loss=0.02892
Epoch 6/80: current_loss=0.02896 | best_loss=0.02892
Epoch 7/80: current_loss=0.02890 | best_loss=0.02890
Epoch 8/80: current_loss=0.02906 | best_loss=0.02890
Epoch 9/80: current_loss=0.02912 | best_loss=0.02890
Epoch 10/80: current_loss=0.02913 | best_loss=0.02890
Epoch 11/80: current_loss=0.02905 | best_loss=0.02890
Epoch 12/80: current_loss=0.02917 | best_loss=0.02890
Epoch 13/80: current_loss=0.02909 | best_loss=0.02890
Epoch 14/80: current_loss=0.02897 | best_loss=0.02890
Epoch 15/80: current_loss=0.02929 | best_loss=0.02890
Epoch 16/80: current_loss=0.02903 | best_loss=0.02890
Epoch 17/80: current_loss=0.02911 | best_loss=0.02890
Epoch 18/80: current_loss=0.02909 | best_loss=0.02890
Epoch 19/80: current_loss=0.02905 | best_loss=0.02890
Epoch 20/80: current_loss=0.02896 | best_loss=0.02890
Epoch 21/80: current_loss=0.02932 | best_loss=0.02890
Epoch 22/80: current_loss=0.02900 | best_loss=0.02890
Epoch 23/80: current_loss=0.02915 | best_loss=0.02890
Epoch 24/80: current_loss=0.02903 | best_loss=0.02890
Epoch 25/80: current_loss=0.02901 | best_loss=0.02890
Epoch 26/80: current_loss=0.02914 | best_loss=0.02890
Epoch 27/80: current_loss=0.02895 | best_loss=0.02890
Early Stopping at epoch 27
      explained_var=0.00407 | mse_loss=0.02805
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02497 | best_loss=0.02497
Epoch 1/80: current_loss=0.02495 | best_loss=0.02495
Epoch 2/80: current_loss=0.02574 | best_loss=0.02495
Epoch 3/80: current_loss=0.02500 | best_loss=0.02495
Epoch 4/80: current_loss=0.02607 | best_loss=0.02495
Epoch 5/80: current_loss=0.02544 | best_loss=0.02495
Epoch 6/80: current_loss=0.02498 | best_loss=0.02495
Epoch 7/80: current_loss=0.02502 | best_loss=0.02495
Epoch 8/80: current_loss=0.02534 | best_loss=0.02495
Epoch 9/80: current_loss=0.02504 | best_loss=0.02495
Epoch 10/80: current_loss=0.02594 | best_loss=0.02495
Epoch 11/80: current_loss=0.02524 | best_loss=0.02495
Epoch 12/80: current_loss=0.02549 | best_loss=0.02495
Epoch 13/80: current_loss=0.02511 | best_loss=0.02495
Epoch 14/80: current_loss=0.02573 | best_loss=0.02495
Epoch 15/80: current_loss=0.02511 | best_loss=0.02495
Epoch 16/80: current_loss=0.02504 | best_loss=0.02495
Epoch 17/80: current_loss=0.02535 | best_loss=0.02495
Epoch 18/80: current_loss=0.02512 | best_loss=0.02495
Epoch 19/80: current_loss=0.02517 | best_loss=0.02495
Epoch 20/80: current_loss=0.02511 | best_loss=0.02495
Epoch 21/80: current_loss=0.02501 | best_loss=0.02495
Early Stopping at epoch 21
      explained_var=0.00095 | mse_loss=0.02519

----------------------------------------------
Params for Trial 42
{'learning_rate': 0.001, 'weight_decay': 0.006071620547761471, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02985 | best_loss=0.02985
Epoch 1/80: current_loss=0.02927 | best_loss=0.02927
Epoch 2/80: current_loss=0.02904 | best_loss=0.02904
Epoch 3/80: current_loss=0.02908 | best_loss=0.02904
Epoch 4/80: current_loss=0.02889 | best_loss=0.02889
Epoch 5/80: current_loss=0.02998 | best_loss=0.02889
Epoch 6/80: current_loss=0.02902 | best_loss=0.02889
Epoch 7/80: current_loss=0.02894 | best_loss=0.02889
Epoch 8/80: current_loss=0.02900 | best_loss=0.02889
Epoch 9/80: current_loss=0.02907 | best_loss=0.02889
Epoch 10/80: current_loss=0.02943 | best_loss=0.02889
Epoch 11/80: current_loss=0.02894 | best_loss=0.02889
Epoch 12/80: current_loss=0.02936 | best_loss=0.02889
Epoch 13/80: current_loss=0.02893 | best_loss=0.02889
Epoch 14/80: current_loss=0.02924 | best_loss=0.02889
Epoch 15/80: current_loss=0.02919 | best_loss=0.02889
Epoch 16/80: current_loss=0.02920 | best_loss=0.02889
Epoch 17/80: current_loss=0.02907 | best_loss=0.02889
Epoch 18/80: current_loss=0.02900 | best_loss=0.02889
Epoch 19/80: current_loss=0.02920 | best_loss=0.02889
Epoch 20/80: current_loss=0.02896 | best_loss=0.02889
Epoch 21/80: current_loss=0.02929 | best_loss=0.02889
Epoch 22/80: current_loss=0.02897 | best_loss=0.02889
Epoch 23/80: current_loss=0.02936 | best_loss=0.02889
Epoch 24/80: current_loss=0.02916 | best_loss=0.02889
Early Stopping at epoch 24
      explained_var=0.00388 | mse_loss=0.02804
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02665 | best_loss=0.02665
Epoch 1/80: current_loss=0.02493 | best_loss=0.02493
Epoch 2/80: current_loss=0.02496 | best_loss=0.02493
Epoch 3/80: current_loss=0.02497 | best_loss=0.02493
Epoch 4/80: current_loss=0.02496 | best_loss=0.02493
Epoch 5/80: current_loss=0.02496 | best_loss=0.02493
Epoch 6/80: current_loss=0.02505 | best_loss=0.02493
Epoch 7/80: current_loss=0.02495 | best_loss=0.02493
Epoch 8/80: current_loss=0.02558 | best_loss=0.02493
Epoch 9/80: current_loss=0.02540 | best_loss=0.02493
Epoch 10/80: current_loss=0.02560 | best_loss=0.02493
Epoch 11/80: current_loss=0.02497 | best_loss=0.02493
Epoch 12/80: current_loss=0.02508 | best_loss=0.02493
Epoch 13/80: current_loss=0.02498 | best_loss=0.02493
Epoch 14/80: current_loss=0.02493 | best_loss=0.02493
Epoch 15/80: current_loss=0.02500 | best_loss=0.02493
Epoch 16/80: current_loss=0.02521 | best_loss=0.02493
Epoch 17/80: current_loss=0.02535 | best_loss=0.02493
Epoch 18/80: current_loss=0.02499 | best_loss=0.02493
Epoch 19/80: current_loss=0.02515 | best_loss=0.02493
Epoch 20/80: current_loss=0.02524 | best_loss=0.02493
Epoch 21/80: current_loss=0.02565 | best_loss=0.02493
Early Stopping at epoch 21
      explained_var=0.00125 | mse_loss=0.02518
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02863 | best_loss=0.02863
Epoch 1/80: current_loss=0.02936 | best_loss=0.02863
Epoch 2/80: current_loss=0.02745 | best_loss=0.02745
Epoch 3/80: current_loss=0.02769 | best_loss=0.02745
Epoch 4/80: current_loss=0.02795 | best_loss=0.02745
Epoch 5/80: current_loss=0.02890 | best_loss=0.02745
Epoch 6/80: current_loss=0.02768 | best_loss=0.02745
Epoch 7/80: current_loss=0.02745 | best_loss=0.02745
Epoch 8/80: current_loss=0.02775 | best_loss=0.02745
Epoch 9/80: current_loss=0.02816 | best_loss=0.02745
Epoch 10/80: current_loss=0.02857 | best_loss=0.02745
Epoch 11/80: current_loss=0.02897 | best_loss=0.02745
Epoch 12/80: current_loss=0.02745 | best_loss=0.02745
Epoch 13/80: current_loss=0.02737 | best_loss=0.02737
Epoch 14/80: current_loss=0.02789 | best_loss=0.02737
Epoch 15/80: current_loss=0.02753 | best_loss=0.02737
Epoch 16/80: current_loss=0.02873 | best_loss=0.02737
Epoch 17/80: current_loss=0.02812 | best_loss=0.02737
Epoch 18/80: current_loss=0.02853 | best_loss=0.02737
Epoch 19/80: current_loss=0.02813 | best_loss=0.02737
Epoch 20/80: current_loss=0.02730 | best_loss=0.02730
Epoch 21/80: current_loss=0.02747 | best_loss=0.02730
Epoch 22/80: current_loss=0.02731 | best_loss=0.02730
Epoch 23/80: current_loss=0.02822 | best_loss=0.02730
Epoch 24/80: current_loss=0.02742 | best_loss=0.02730
Epoch 25/80: current_loss=0.02780 | best_loss=0.02730
Epoch 26/80: current_loss=0.02744 | best_loss=0.02730
Epoch 27/80: current_loss=0.02787 | best_loss=0.02730
Epoch 28/80: current_loss=0.02775 | best_loss=0.02730
Epoch 29/80: current_loss=0.02744 | best_loss=0.02730
Epoch 30/80: current_loss=0.02744 | best_loss=0.02730
Epoch 31/80: current_loss=0.02781 | best_loss=0.02730
Epoch 32/80: current_loss=0.02742 | best_loss=0.02730
Epoch 33/80: current_loss=0.02818 | best_loss=0.02730
Epoch 34/80: current_loss=0.02788 | best_loss=0.02730
Epoch 35/80: current_loss=0.02765 | best_loss=0.02730
Epoch 36/80: current_loss=0.02784 | best_loss=0.02730
Epoch 37/80: current_loss=0.02823 | best_loss=0.02730
Epoch 38/80: current_loss=0.02739 | best_loss=0.02730
Epoch 39/80: current_loss=0.02773 | best_loss=0.02730
Epoch 40/80: current_loss=0.02793 | best_loss=0.02730
Early Stopping at epoch 40
      explained_var=-0.00496 | mse_loss=0.02774
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02737 | best_loss=0.02737
Epoch 1/80: current_loss=0.02678 | best_loss=0.02678
Epoch 2/80: current_loss=0.02722 | best_loss=0.02678
Epoch 3/80: current_loss=0.02679 | best_loss=0.02678
Epoch 4/80: current_loss=0.02704 | best_loss=0.02678
Epoch 5/80: current_loss=0.02680 | best_loss=0.02678
Epoch 6/80: current_loss=0.02691 | best_loss=0.02678
Epoch 7/80: current_loss=0.02697 | best_loss=0.02678
Epoch 8/80: current_loss=0.02685 | best_loss=0.02678
Epoch 9/80: current_loss=0.02692 | best_loss=0.02678
Epoch 10/80: current_loss=0.02687 | best_loss=0.02678
Epoch 11/80: current_loss=0.02682 | best_loss=0.02678
Epoch 12/80: current_loss=0.02697 | best_loss=0.02678
Epoch 13/80: current_loss=0.02679 | best_loss=0.02678
Epoch 14/80: current_loss=0.02682 | best_loss=0.02678
Epoch 15/80: current_loss=0.02709 | best_loss=0.02678
Epoch 16/80: current_loss=0.02683 | best_loss=0.02678
Epoch 17/80: current_loss=0.02723 | best_loss=0.02678
Epoch 18/80: current_loss=0.02680 | best_loss=0.02678
Epoch 19/80: current_loss=0.02686 | best_loss=0.02678
Epoch 20/80: current_loss=0.02678 | best_loss=0.02678
Epoch 21/80: current_loss=0.02679 | best_loss=0.02678
Epoch 22/80: current_loss=0.02680 | best_loss=0.02678
Epoch 23/80: current_loss=0.02685 | best_loss=0.02678
Epoch 24/80: current_loss=0.02679 | best_loss=0.02678
Epoch 25/80: current_loss=0.02694 | best_loss=0.02678
Epoch 26/80: current_loss=0.02679 | best_loss=0.02678
Epoch 27/80: current_loss=0.02678 | best_loss=0.02678
Epoch 28/80: current_loss=0.02680 | best_loss=0.02678
Epoch 29/80: current_loss=0.02679 | best_loss=0.02678
Epoch 30/80: current_loss=0.02681 | best_loss=0.02678
Epoch 31/80: current_loss=0.02698 | best_loss=0.02678
Epoch 32/80: current_loss=0.02691 | best_loss=0.02678
Epoch 33/80: current_loss=0.02698 | best_loss=0.02678
Epoch 34/80: current_loss=0.02683 | best_loss=0.02678
Epoch 35/80: current_loss=0.02705 | best_loss=0.02678
Epoch 36/80: current_loss=0.02686 | best_loss=0.02678
Epoch 37/80: current_loss=0.02679 | best_loss=0.02678
Epoch 38/80: current_loss=0.02683 | best_loss=0.02678
Epoch 39/80: current_loss=0.02678 | best_loss=0.02678
Epoch 40/80: current_loss=0.02695 | best_loss=0.02678
Early Stopping at epoch 40
      explained_var=0.00104 | mse_loss=0.02496
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02978 | best_loss=0.02978
Epoch 1/80: current_loss=0.02976 | best_loss=0.02976
Epoch 2/80: current_loss=0.02988 | best_loss=0.02976
Epoch 3/80: current_loss=0.02975 | best_loss=0.02975
Epoch 4/80: current_loss=0.02976 | best_loss=0.02975
Epoch 5/80: current_loss=0.02975 | best_loss=0.02975
Epoch 6/80: current_loss=0.02980 | best_loss=0.02975
Epoch 7/80: current_loss=0.02978 | best_loss=0.02975
Epoch 8/80: current_loss=0.02976 | best_loss=0.02975
Epoch 9/80: current_loss=0.02978 | best_loss=0.02975
Epoch 10/80: current_loss=0.02983 | best_loss=0.02975
Epoch 11/80: current_loss=0.02976 | best_loss=0.02975
Epoch 12/80: current_loss=0.02980 | best_loss=0.02975
Epoch 13/80: current_loss=0.02977 | best_loss=0.02975
Epoch 14/80: current_loss=0.02976 | best_loss=0.02975
Epoch 15/80: current_loss=0.02977 | best_loss=0.02975
Epoch 16/80: current_loss=0.02976 | best_loss=0.02975
Epoch 17/80: current_loss=0.02978 | best_loss=0.02975
Epoch 18/80: current_loss=0.02978 | best_loss=0.02975
Epoch 19/80: current_loss=0.02977 | best_loss=0.02975
Epoch 20/80: current_loss=0.02977 | best_loss=0.02975
Epoch 21/80: current_loss=0.02980 | best_loss=0.02975
Epoch 22/80: current_loss=0.02979 | best_loss=0.02975
Epoch 23/80: current_loss=0.02978 | best_loss=0.02975
Early Stopping at epoch 23
      explained_var=0.00097 | mse_loss=0.02873
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.00044| avg_loss=0.02693
----------------------------------------------


----------------------------------------------
Params for Trial 43
{'learning_rate': 0.001, 'weight_decay': 0.005789998554310914, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03065 | best_loss=0.03065
Epoch 1/80: current_loss=0.02962 | best_loss=0.02962
Epoch 2/80: current_loss=0.02923 | best_loss=0.02923
Epoch 3/80: current_loss=0.02907 | best_loss=0.02907
Epoch 4/80: current_loss=0.02901 | best_loss=0.02901
Epoch 5/80: current_loss=0.02919 | best_loss=0.02901
Epoch 6/80: current_loss=0.02909 | best_loss=0.02901
Epoch 7/80: current_loss=0.02910 | best_loss=0.02901
Epoch 8/80: current_loss=0.02904 | best_loss=0.02901
Epoch 9/80: current_loss=0.02897 | best_loss=0.02897
Epoch 10/80: current_loss=0.02898 | best_loss=0.02897
Epoch 11/80: current_loss=0.02904 | best_loss=0.02897
Epoch 12/80: current_loss=0.02893 | best_loss=0.02893
Epoch 13/80: current_loss=0.02894 | best_loss=0.02893
Epoch 14/80: current_loss=0.02926 | best_loss=0.02893
Epoch 15/80: current_loss=0.02962 | best_loss=0.02893
Epoch 16/80: current_loss=0.02973 | best_loss=0.02893
Epoch 17/80: current_loss=0.02945 | best_loss=0.02893
Epoch 18/80: current_loss=0.02934 | best_loss=0.02893
Epoch 19/80: current_loss=0.02898 | best_loss=0.02893
Epoch 20/80: current_loss=0.02894 | best_loss=0.02893
Epoch 21/80: current_loss=0.02916 | best_loss=0.02893
Epoch 22/80: current_loss=0.02914 | best_loss=0.02893
Epoch 23/80: current_loss=0.02920 | best_loss=0.02893
Epoch 24/80: current_loss=0.02897 | best_loss=0.02893
Epoch 25/80: current_loss=0.02900 | best_loss=0.02893
Epoch 26/80: current_loss=0.02924 | best_loss=0.02893
Epoch 27/80: current_loss=0.02967 | best_loss=0.02893
Epoch 28/80: current_loss=0.02896 | best_loss=0.02893
Epoch 29/80: current_loss=0.02901 | best_loss=0.02893
Epoch 30/80: current_loss=0.02894 | best_loss=0.02893
Epoch 31/80: current_loss=0.02946 | best_loss=0.02893
Epoch 32/80: current_loss=0.02900 | best_loss=0.02893
Early Stopping at epoch 32
      explained_var=0.00367 | mse_loss=0.02804
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02503 | best_loss=0.02503
Epoch 1/80: current_loss=0.02497 | best_loss=0.02497
Epoch 2/80: current_loss=0.02502 | best_loss=0.02497
Epoch 3/80: current_loss=0.02537 | best_loss=0.02497
Epoch 4/80: current_loss=0.02525 | best_loss=0.02497
Epoch 5/80: current_loss=0.02505 | best_loss=0.02497
Epoch 6/80: current_loss=0.02498 | best_loss=0.02497
Epoch 7/80: current_loss=0.02496 | best_loss=0.02496
Epoch 8/80: current_loss=0.02498 | best_loss=0.02496
Epoch 9/80: current_loss=0.02499 | best_loss=0.02496
Epoch 10/80: current_loss=0.02502 | best_loss=0.02496
Epoch 11/80: current_loss=0.02498 | best_loss=0.02496
Epoch 12/80: current_loss=0.02554 | best_loss=0.02496
Epoch 13/80: current_loss=0.02544 | best_loss=0.02496
Epoch 14/80: current_loss=0.02498 | best_loss=0.02496
Epoch 15/80: current_loss=0.02508 | best_loss=0.02496
Epoch 16/80: current_loss=0.02524 | best_loss=0.02496
Epoch 17/80: current_loss=0.02497 | best_loss=0.02496
Epoch 18/80: current_loss=0.02518 | best_loss=0.02496
Epoch 19/80: current_loss=0.02501 | best_loss=0.02496
Epoch 20/80: current_loss=0.02532 | best_loss=0.02496
Epoch 21/80: current_loss=0.02503 | best_loss=0.02496
Epoch 22/80: current_loss=0.02499 | best_loss=0.02496
Epoch 23/80: current_loss=0.02511 | best_loss=0.02496
Epoch 24/80: current_loss=0.02506 | best_loss=0.02496
Epoch 25/80: current_loss=0.02497 | best_loss=0.02496
Epoch 26/80: current_loss=0.02498 | best_loss=0.02496
Epoch 27/80: current_loss=0.02587 | best_loss=0.02496
Early Stopping at epoch 27
      explained_var=0.00097 | mse_loss=0.02520

----------------------------------------------
Params for Trial 44
{'learning_rate': 0.001, 'weight_decay': 0.004209948467837886, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03633 | best_loss=0.03633
Epoch 1/80: current_loss=0.03233 | best_loss=0.03233
Epoch 2/80: current_loss=0.03089 | best_loss=0.03089
Epoch 3/80: current_loss=0.02994 | best_loss=0.02994
Epoch 4/80: current_loss=0.02922 | best_loss=0.02922
Epoch 5/80: current_loss=0.02962 | best_loss=0.02922
Epoch 6/80: current_loss=0.02900 | best_loss=0.02900
Epoch 7/80: current_loss=0.02924 | best_loss=0.02900
Epoch 8/80: current_loss=0.02892 | best_loss=0.02892
Epoch 9/80: current_loss=0.02895 | best_loss=0.02892
Epoch 10/80: current_loss=0.02905 | best_loss=0.02892
Epoch 11/80: current_loss=0.02909 | best_loss=0.02892
Epoch 12/80: current_loss=0.02912 | best_loss=0.02892
Epoch 13/80: current_loss=0.02891 | best_loss=0.02891
Epoch 14/80: current_loss=0.02924 | best_loss=0.02891
Epoch 15/80: current_loss=0.02903 | best_loss=0.02891
Epoch 16/80: current_loss=0.02901 | best_loss=0.02891
Epoch 17/80: current_loss=0.02923 | best_loss=0.02891
Epoch 18/80: current_loss=0.02911 | best_loss=0.02891
Epoch 19/80: current_loss=0.02947 | best_loss=0.02891
Epoch 20/80: current_loss=0.02915 | best_loss=0.02891
Epoch 21/80: current_loss=0.02906 | best_loss=0.02891
Epoch 22/80: current_loss=0.02903 | best_loss=0.02891
Epoch 23/80: current_loss=0.02903 | best_loss=0.02891
Epoch 24/80: current_loss=0.02899 | best_loss=0.02891
Epoch 25/80: current_loss=0.02908 | best_loss=0.02891
Epoch 26/80: current_loss=0.02931 | best_loss=0.02891
Epoch 27/80: current_loss=0.02899 | best_loss=0.02891
Epoch 28/80: current_loss=0.02913 | best_loss=0.02891
Epoch 29/80: current_loss=0.02914 | best_loss=0.02891
Epoch 30/80: current_loss=0.02925 | best_loss=0.02891
Epoch 31/80: current_loss=0.02898 | best_loss=0.02891
Epoch 32/80: current_loss=0.02929 | best_loss=0.02891
Epoch 33/80: current_loss=0.02904 | best_loss=0.02891
Early Stopping at epoch 33
      explained_var=0.00275 | mse_loss=0.02811

----------------------------------------------
Params for Trial 45
{'learning_rate': 0.01, 'weight_decay': 0.004383277480823774, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.17420 | best_loss=0.17420
Epoch 1/80: current_loss=0.07052 | best_loss=0.07052
Epoch 2/80: current_loss=0.07472 | best_loss=0.07052
Epoch 3/80: current_loss=0.03194 | best_loss=0.03194
Epoch 4/80: current_loss=0.02993 | best_loss=0.02993
Epoch 5/80: current_loss=0.02949 | best_loss=0.02949
Epoch 6/80: current_loss=0.06860 | best_loss=0.02949
Epoch 7/80: current_loss=0.03395 | best_loss=0.02949
Epoch 8/80: current_loss=0.06128 | best_loss=0.02949
Epoch 9/80: current_loss=0.03090 | best_loss=0.02949
Epoch 10/80: current_loss=0.04417 | best_loss=0.02949
Epoch 11/80: current_loss=0.03071 | best_loss=0.02949
Epoch 12/80: current_loss=0.03306 | best_loss=0.02949
Epoch 13/80: current_loss=0.02978 | best_loss=0.02949
Epoch 14/80: current_loss=0.03092 | best_loss=0.02949
Epoch 15/80: current_loss=0.02962 | best_loss=0.02949
Epoch 16/80: current_loss=0.03023 | best_loss=0.02949
Epoch 17/80: current_loss=0.02915 | best_loss=0.02915
Epoch 18/80: current_loss=0.02915 | best_loss=0.02915
Epoch 19/80: current_loss=0.02915 | best_loss=0.02915
Epoch 20/80: current_loss=0.02913 | best_loss=0.02913
Epoch 21/80: current_loss=0.02921 | best_loss=0.02913
Epoch 22/80: current_loss=0.02911 | best_loss=0.02911
Epoch 23/80: current_loss=0.02918 | best_loss=0.02911
Epoch 24/80: current_loss=0.02913 | best_loss=0.02911
Epoch 25/80: current_loss=0.02919 | best_loss=0.02911
Epoch 26/80: current_loss=0.02947 | best_loss=0.02911
Epoch 27/80: current_loss=0.02914 | best_loss=0.02911
Epoch 28/80: current_loss=0.02913 | best_loss=0.02911
Epoch 29/80: current_loss=0.02955 | best_loss=0.02911
Epoch 30/80: current_loss=0.02925 | best_loss=0.02911
Epoch 31/80: current_loss=0.02912 | best_loss=0.02911
Epoch 32/80: current_loss=0.02924 | best_loss=0.02911
Epoch 33/80: current_loss=0.02918 | best_loss=0.02911
Epoch 34/80: current_loss=0.02914 | best_loss=0.02911
Epoch 35/80: current_loss=0.02917 | best_loss=0.02911
Epoch 36/80: current_loss=0.02921 | best_loss=0.02911
Epoch 37/80: current_loss=0.02959 | best_loss=0.02911
Epoch 38/80: current_loss=0.02928 | best_loss=0.02911
Epoch 39/80: current_loss=0.02923 | best_loss=0.02911
Epoch 40/80: current_loss=0.02913 | best_loss=0.02911
Epoch 41/80: current_loss=0.02914 | best_loss=0.02911
Epoch 42/80: current_loss=0.02949 | best_loss=0.02911
Early Stopping at epoch 42
      explained_var=-0.00001 | mse_loss=0.02815

----------------------------------------------
Params for Trial 46
{'learning_rate': 0.1, 'weight_decay': 0.0033404266432349094, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=2.98042 | best_loss=2.98042
Epoch 1/80: current_loss=2.90086 | best_loss=2.90086
Epoch 2/80: current_loss=2.88493 | best_loss=2.88493
Epoch 3/80: current_loss=0.29108 | best_loss=0.29108
Epoch 4/80: current_loss=0.37702 | best_loss=0.29108
Epoch 5/80: current_loss=0.10081 | best_loss=0.10081
Epoch 6/80: current_loss=0.15172 | best_loss=0.10081
Epoch 7/80: current_loss=0.04674 | best_loss=0.04674
Epoch 8/80: current_loss=0.08651 | best_loss=0.04674
Epoch 9/80: current_loss=0.05873 | best_loss=0.04674
Epoch 10/80: current_loss=0.03412 | best_loss=0.03412
Epoch 11/80: current_loss=0.03793 | best_loss=0.03412
Epoch 12/80: current_loss=0.03128 | best_loss=0.03128
Epoch 13/80: current_loss=0.03160 | best_loss=0.03128
Epoch 14/80: current_loss=0.03363 | best_loss=0.03128
Epoch 15/80: current_loss=0.03136 | best_loss=0.03128
Epoch 16/80: current_loss=0.02953 | best_loss=0.02953
Epoch 17/80: current_loss=0.02938 | best_loss=0.02938
Epoch 18/80: current_loss=0.03013 | best_loss=0.02938
Epoch 19/80: current_loss=0.03052 | best_loss=0.02938
Epoch 20/80: current_loss=0.02890 | best_loss=0.02890
Epoch 21/80: current_loss=0.02884 | best_loss=0.02884
Epoch 22/80: current_loss=0.02917 | best_loss=0.02884
Epoch 23/80: current_loss=0.02996 | best_loss=0.02884
Epoch 24/80: current_loss=0.02912 | best_loss=0.02884
Epoch 25/80: current_loss=0.02917 | best_loss=0.02884
Epoch 26/80: current_loss=0.02965 | best_loss=0.02884
Epoch 27/80: current_loss=0.02925 | best_loss=0.02884
Epoch 28/80: current_loss=0.02923 | best_loss=0.02884
Epoch 29/80: current_loss=0.02914 | best_loss=0.02884
Epoch 30/80: current_loss=0.02917 | best_loss=0.02884
Epoch 31/80: current_loss=0.02938 | best_loss=0.02884
Epoch 32/80: current_loss=0.02914 | best_loss=0.02884
Epoch 33/80: current_loss=0.02915 | best_loss=0.02884
Epoch 34/80: current_loss=0.02911 | best_loss=0.02884
Epoch 35/80: current_loss=0.02913 | best_loss=0.02884
Epoch 36/80: current_loss=0.02960 | best_loss=0.02884
Epoch 37/80: current_loss=0.02914 | best_loss=0.02884
Epoch 38/80: current_loss=0.02925 | best_loss=0.02884
Epoch 39/80: current_loss=0.02965 | best_loss=0.02884
Epoch 40/80: current_loss=0.02954 | best_loss=0.02884
Epoch 41/80: current_loss=0.02940 | best_loss=0.02884
Early Stopping at epoch 41
      explained_var=0.01419 | mse_loss=0.02800
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=1.09179 | best_loss=1.09179
Epoch 1/80: current_loss=0.94241 | best_loss=0.94241
Epoch 2/80: current_loss=0.20575 | best_loss=0.20575
Epoch 3/80: current_loss=0.67940 | best_loss=0.20575
Epoch 4/80: current_loss=0.55155 | best_loss=0.20575
Epoch 5/80: current_loss=0.56897 | best_loss=0.20575
Epoch 6/80: current_loss=0.07924 | best_loss=0.07924
Epoch 7/80: current_loss=0.02838 | best_loss=0.02838
Epoch 8/80: current_loss=0.05228 | best_loss=0.02838
Epoch 9/80: current_loss=0.08449 | best_loss=0.02838
Epoch 10/80: current_loss=0.03035 | best_loss=0.02838
Epoch 11/80: current_loss=0.03150 | best_loss=0.02838
Epoch 12/80: current_loss=0.02641 | best_loss=0.02641
Epoch 13/80: current_loss=0.02600 | best_loss=0.02600
Epoch 14/80: current_loss=0.02586 | best_loss=0.02586
Epoch 15/80: current_loss=0.02537 | best_loss=0.02537
Epoch 16/80: current_loss=0.02704 | best_loss=0.02537
Epoch 17/80: current_loss=0.02550 | best_loss=0.02537
Epoch 18/80: current_loss=0.02523 | best_loss=0.02523
Epoch 19/80: current_loss=0.02532 | best_loss=0.02523
Epoch 20/80: current_loss=0.02514 | best_loss=0.02514
Epoch 21/80: current_loss=0.02503 | best_loss=0.02503
Epoch 22/80: current_loss=0.02504 | best_loss=0.02503
Epoch 23/80: current_loss=0.02504 | best_loss=0.02503
Epoch 24/80: current_loss=0.02503 | best_loss=0.02503
Epoch 25/80: current_loss=0.02509 | best_loss=0.02503
Epoch 26/80: current_loss=0.02505 | best_loss=0.02503
Epoch 27/80: current_loss=0.02507 | best_loss=0.02503
Epoch 28/80: current_loss=0.02508 | best_loss=0.02503
Epoch 29/80: current_loss=0.02529 | best_loss=0.02503
Epoch 30/80: current_loss=0.02502 | best_loss=0.02502
Epoch 31/80: current_loss=0.02502 | best_loss=0.02502
Epoch 32/80: current_loss=0.02503 | best_loss=0.02502
Epoch 33/80: current_loss=0.02504 | best_loss=0.02502
Epoch 34/80: current_loss=0.02503 | best_loss=0.02502
Epoch 35/80: current_loss=0.02502 | best_loss=0.02502
Epoch 36/80: current_loss=0.02538 | best_loss=0.02502
Epoch 37/80: current_loss=0.02504 | best_loss=0.02502
Epoch 38/80: current_loss=0.02544 | best_loss=0.02502
Epoch 39/80: current_loss=0.02502 | best_loss=0.02502
Epoch 40/80: current_loss=0.02502 | best_loss=0.02502
Epoch 41/80: current_loss=0.02507 | best_loss=0.02502
Epoch 42/80: current_loss=0.02507 | best_loss=0.02502
Epoch 43/80: current_loss=0.02503 | best_loss=0.02502
Epoch 44/80: current_loss=0.02528 | best_loss=0.02502
Epoch 45/80: current_loss=0.02502 | best_loss=0.02502
Epoch 46/80: current_loss=0.02509 | best_loss=0.02502
Epoch 47/80: current_loss=0.02532 | best_loss=0.02502
Epoch 48/80: current_loss=0.02503 | best_loss=0.02502
Epoch 49/80: current_loss=0.02516 | best_loss=0.02502
Epoch 50/80: current_loss=0.02503 | best_loss=0.02502
Epoch 51/80: current_loss=0.02511 | best_loss=0.02502
Epoch 52/80: current_loss=0.02502 | best_loss=0.02502
Epoch 53/80: current_loss=0.02503 | best_loss=0.02502
Epoch 54/80: current_loss=0.02506 | best_loss=0.02502
Epoch 55/80: current_loss=0.02503 | best_loss=0.02502
Epoch 56/80: current_loss=0.02508 | best_loss=0.02502
Epoch 57/80: current_loss=0.02520 | best_loss=0.02502
Epoch 58/80: current_loss=0.02543 | best_loss=0.02502
Epoch 59/80: current_loss=0.02503 | best_loss=0.02502
Epoch 60/80: current_loss=0.02515 | best_loss=0.02502
Early Stopping at epoch 60
      explained_var=0.00000 | mse_loss=0.02520

----------------------------------------------
Params for Trial 47
{'learning_rate': 0.0001, 'weight_decay': 0.0025849837801565703, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03264 | best_loss=0.03264
Epoch 1/80: current_loss=0.03129 | best_loss=0.03129
Epoch 2/80: current_loss=0.03071 | best_loss=0.03071
Epoch 3/80: current_loss=0.03015 | best_loss=0.03015
Epoch 4/80: current_loss=0.02978 | best_loss=0.02978
Epoch 5/80: current_loss=0.02945 | best_loss=0.02945
Epoch 6/80: current_loss=0.02925 | best_loss=0.02925
Epoch 7/80: current_loss=0.02916 | best_loss=0.02916
Epoch 8/80: current_loss=0.02908 | best_loss=0.02908
Epoch 9/80: current_loss=0.02899 | best_loss=0.02899
Epoch 10/80: current_loss=0.02917 | best_loss=0.02899
Epoch 11/80: current_loss=0.02891 | best_loss=0.02891
Epoch 12/80: current_loss=0.02891 | best_loss=0.02891
Epoch 13/80: current_loss=0.02894 | best_loss=0.02891
Epoch 14/80: current_loss=0.02884 | best_loss=0.02884
Epoch 15/80: current_loss=0.02884 | best_loss=0.02884
Epoch 16/80: current_loss=0.02884 | best_loss=0.02884
Epoch 17/80: current_loss=0.02914 | best_loss=0.02884
Epoch 18/80: current_loss=0.02881 | best_loss=0.02881
Epoch 19/80: current_loss=0.02882 | best_loss=0.02881
Epoch 20/80: current_loss=0.02886 | best_loss=0.02881
Epoch 21/80: current_loss=0.02905 | best_loss=0.02881
Epoch 22/80: current_loss=0.02899 | best_loss=0.02881
Epoch 23/80: current_loss=0.02918 | best_loss=0.02881
Epoch 24/80: current_loss=0.02890 | best_loss=0.02881
Epoch 25/80: current_loss=0.02889 | best_loss=0.02881
Epoch 26/80: current_loss=0.02892 | best_loss=0.02881
Epoch 27/80: current_loss=0.02898 | best_loss=0.02881
Epoch 28/80: current_loss=0.02896 | best_loss=0.02881
Epoch 29/80: current_loss=0.02889 | best_loss=0.02881
Epoch 30/80: current_loss=0.02899 | best_loss=0.02881
Epoch 31/80: current_loss=0.02911 | best_loss=0.02881
Epoch 32/80: current_loss=0.02892 | best_loss=0.02881
Epoch 33/80: current_loss=0.02889 | best_loss=0.02881
Epoch 34/80: current_loss=0.02888 | best_loss=0.02881
Epoch 35/80: current_loss=0.02889 | best_loss=0.02881
Epoch 36/80: current_loss=0.02891 | best_loss=0.02881
Epoch 37/80: current_loss=0.02893 | best_loss=0.02881
Epoch 38/80: current_loss=0.02904 | best_loss=0.02881
Early Stopping at epoch 38
      explained_var=0.00712 | mse_loss=0.02794
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02495 | best_loss=0.02495
Epoch 1/80: current_loss=0.02487 | best_loss=0.02487
Epoch 2/80: current_loss=0.02493 | best_loss=0.02487
Epoch 3/80: current_loss=0.02501 | best_loss=0.02487
Epoch 4/80: current_loss=0.02490 | best_loss=0.02487
Epoch 5/80: current_loss=0.02493 | best_loss=0.02487
Epoch 6/80: current_loss=0.02494 | best_loss=0.02487
Epoch 7/80: current_loss=0.02492 | best_loss=0.02487
Epoch 8/80: current_loss=0.02508 | best_loss=0.02487
Epoch 9/80: current_loss=0.02508 | best_loss=0.02487
Epoch 10/80: current_loss=0.02497 | best_loss=0.02487
Epoch 11/80: current_loss=0.02500 | best_loss=0.02487
Epoch 12/80: current_loss=0.02505 | best_loss=0.02487
Epoch 13/80: current_loss=0.02506 | best_loss=0.02487
Epoch 14/80: current_loss=0.02505 | best_loss=0.02487
Epoch 15/80: current_loss=0.02502 | best_loss=0.02487
Epoch 16/80: current_loss=0.02493 | best_loss=0.02487
Epoch 17/80: current_loss=0.02491 | best_loss=0.02487
Epoch 18/80: current_loss=0.02496 | best_loss=0.02487
Epoch 19/80: current_loss=0.02505 | best_loss=0.02487
Epoch 20/80: current_loss=0.02530 | best_loss=0.02487
Epoch 21/80: current_loss=0.02495 | best_loss=0.02487
Early Stopping at epoch 21
      explained_var=0.00216 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02759 | best_loss=0.02759
Epoch 1/80: current_loss=0.02749 | best_loss=0.02749
Epoch 2/80: current_loss=0.02791 | best_loss=0.02749
Epoch 3/80: current_loss=0.02784 | best_loss=0.02749
Epoch 4/80: current_loss=0.02827 | best_loss=0.02749
Epoch 5/80: current_loss=0.02754 | best_loss=0.02749
Epoch 6/80: current_loss=0.02791 | best_loss=0.02749
Epoch 7/80: current_loss=0.02956 | best_loss=0.02749
Epoch 8/80: current_loss=0.02834 | best_loss=0.02749
Epoch 9/80: current_loss=0.02761 | best_loss=0.02749
Epoch 10/80: current_loss=0.02741 | best_loss=0.02741
Epoch 11/80: current_loss=0.02819 | best_loss=0.02741
Epoch 12/80: current_loss=0.02758 | best_loss=0.02741
Epoch 13/80: current_loss=0.02799 | best_loss=0.02741
Epoch 14/80: current_loss=0.02797 | best_loss=0.02741
Epoch 15/80: current_loss=0.02766 | best_loss=0.02741
Epoch 16/80: current_loss=0.02763 | best_loss=0.02741
Epoch 17/80: current_loss=0.02742 | best_loss=0.02741
Epoch 18/80: current_loss=0.02742 | best_loss=0.02741
Epoch 19/80: current_loss=0.02770 | best_loss=0.02741
Epoch 20/80: current_loss=0.02750 | best_loss=0.02741
Epoch 21/80: current_loss=0.02766 | best_loss=0.02741
Epoch 22/80: current_loss=0.02770 | best_loss=0.02741
Epoch 23/80: current_loss=0.02829 | best_loss=0.02741
Epoch 24/80: current_loss=0.02750 | best_loss=0.02741
Epoch 25/80: current_loss=0.02743 | best_loss=0.02741
Epoch 26/80: current_loss=0.02752 | best_loss=0.02741
Epoch 27/80: current_loss=0.02772 | best_loss=0.02741
Epoch 28/80: current_loss=0.02743 | best_loss=0.02741
Epoch 29/80: current_loss=0.02737 | best_loss=0.02737
Epoch 30/80: current_loss=0.02753 | best_loss=0.02737
Epoch 31/80: current_loss=0.02742 | best_loss=0.02737
Epoch 32/80: current_loss=0.02781 | best_loss=0.02737
Epoch 33/80: current_loss=0.02790 | best_loss=0.02737
Epoch 34/80: current_loss=0.02764 | best_loss=0.02737
Epoch 35/80: current_loss=0.02804 | best_loss=0.02737
Epoch 36/80: current_loss=0.02738 | best_loss=0.02737
Epoch 37/80: current_loss=0.02773 | best_loss=0.02737
Epoch 38/80: current_loss=0.02815 | best_loss=0.02737
Epoch 39/80: current_loss=0.02775 | best_loss=0.02737
Epoch 40/80: current_loss=0.02742 | best_loss=0.02737
Epoch 41/80: current_loss=0.02743 | best_loss=0.02737
Epoch 42/80: current_loss=0.02785 | best_loss=0.02737
Epoch 43/80: current_loss=0.02798 | best_loss=0.02737
Epoch 44/80: current_loss=0.02811 | best_loss=0.02737
Epoch 45/80: current_loss=0.02752 | best_loss=0.02737
Epoch 46/80: current_loss=0.02778 | best_loss=0.02737
Epoch 47/80: current_loss=0.02747 | best_loss=0.02737
Epoch 48/80: current_loss=0.02815 | best_loss=0.02737
Epoch 49/80: current_loss=0.02812 | best_loss=0.02737
Early Stopping at epoch 49
      explained_var=-0.00511 | mse_loss=0.02782
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02687 | best_loss=0.02687
Epoch 1/80: current_loss=0.02677 | best_loss=0.02677
Epoch 2/80: current_loss=0.02674 | best_loss=0.02674
Epoch 3/80: current_loss=0.02680 | best_loss=0.02674
Epoch 4/80: current_loss=0.02674 | best_loss=0.02674
Epoch 5/80: current_loss=0.02680 | best_loss=0.02674
Epoch 6/80: current_loss=0.02673 | best_loss=0.02673
Epoch 7/80: current_loss=0.02676 | best_loss=0.02673
Epoch 8/80: current_loss=0.02675 | best_loss=0.02673
Epoch 9/80: current_loss=0.02684 | best_loss=0.02673
Epoch 10/80: current_loss=0.02678 | best_loss=0.02673
Epoch 11/80: current_loss=0.02681 | best_loss=0.02673
Epoch 12/80: current_loss=0.02679 | best_loss=0.02673
Epoch 13/80: current_loss=0.02678 | best_loss=0.02673
Epoch 14/80: current_loss=0.02677 | best_loss=0.02673
Epoch 15/80: current_loss=0.02678 | best_loss=0.02673
Epoch 16/80: current_loss=0.02698 | best_loss=0.02673
Epoch 17/80: current_loss=0.02675 | best_loss=0.02673
Epoch 18/80: current_loss=0.02676 | best_loss=0.02673
Epoch 19/80: current_loss=0.02676 | best_loss=0.02673
Epoch 20/80: current_loss=0.02687 | best_loss=0.02673
Epoch 21/80: current_loss=0.02692 | best_loss=0.02673
Epoch 22/80: current_loss=0.02693 | best_loss=0.02673
Epoch 23/80: current_loss=0.02696 | best_loss=0.02673
Epoch 24/80: current_loss=0.02687 | best_loss=0.02673
Epoch 25/80: current_loss=0.02679 | best_loss=0.02673
Epoch 26/80: current_loss=0.02678 | best_loss=0.02673
Early Stopping at epoch 26
      explained_var=0.00204 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02977 | best_loss=0.02977
Epoch 1/80: current_loss=0.02973 | best_loss=0.02973
Epoch 2/80: current_loss=0.02974 | best_loss=0.02973
Epoch 3/80: current_loss=0.02972 | best_loss=0.02972
Epoch 4/80: current_loss=0.02977 | best_loss=0.02972
Epoch 5/80: current_loss=0.02974 | best_loss=0.02972
Epoch 6/80: current_loss=0.03011 | best_loss=0.02972
Epoch 7/80: current_loss=0.02976 | best_loss=0.02972
Epoch 8/80: current_loss=0.02974 | best_loss=0.02972
Epoch 9/80: current_loss=0.02985 | best_loss=0.02972
Epoch 10/80: current_loss=0.02980 | best_loss=0.02972
Epoch 11/80: current_loss=0.02996 | best_loss=0.02972
Epoch 12/80: current_loss=0.03003 | best_loss=0.02972
Epoch 13/80: current_loss=0.02972 | best_loss=0.02972
Epoch 14/80: current_loss=0.02970 | best_loss=0.02970
Epoch 15/80: current_loss=0.02991 | best_loss=0.02970
Epoch 16/80: current_loss=0.02969 | best_loss=0.02969
Epoch 17/80: current_loss=0.02975 | best_loss=0.02969
Epoch 18/80: current_loss=0.02971 | best_loss=0.02969
Epoch 19/80: current_loss=0.02973 | best_loss=0.02969
Epoch 20/80: current_loss=0.02987 | best_loss=0.02969
Epoch 21/80: current_loss=0.02971 | best_loss=0.02969
Epoch 22/80: current_loss=0.02982 | best_loss=0.02969
Epoch 23/80: current_loss=0.02983 | best_loss=0.02969
Epoch 24/80: current_loss=0.02970 | best_loss=0.02969
Epoch 25/80: current_loss=0.02973 | best_loss=0.02969
Epoch 26/80: current_loss=0.02976 | best_loss=0.02969
Epoch 27/80: current_loss=0.02975 | best_loss=0.02969
Epoch 28/80: current_loss=0.02973 | best_loss=0.02969
Epoch 29/80: current_loss=0.02970 | best_loss=0.02969
Epoch 30/80: current_loss=0.02975 | best_loss=0.02969
Epoch 31/80: current_loss=0.02972 | best_loss=0.02969
Epoch 32/80: current_loss=0.02979 | best_loss=0.02969
Epoch 33/80: current_loss=0.02975 | best_loss=0.02969
Epoch 34/80: current_loss=0.02968 | best_loss=0.02968
Epoch 35/80: current_loss=0.02971 | best_loss=0.02968
Epoch 36/80: current_loss=0.02967 | best_loss=0.02967
Epoch 37/80: current_loss=0.02983 | best_loss=0.02967
Epoch 38/80: current_loss=0.02988 | best_loss=0.02967
Epoch 39/80: current_loss=0.02990 | best_loss=0.02967
Epoch 40/80: current_loss=0.02975 | best_loss=0.02967
Epoch 41/80: current_loss=0.02972 | best_loss=0.02967
Epoch 42/80: current_loss=0.02986 | best_loss=0.02967
Epoch 43/80: current_loss=0.02973 | best_loss=0.02967
Epoch 44/80: current_loss=0.02973 | best_loss=0.02967
Epoch 45/80: current_loss=0.02979 | best_loss=0.02967
Epoch 46/80: current_loss=0.02973 | best_loss=0.02967
Epoch 47/80: current_loss=0.02976 | best_loss=0.02967
Epoch 48/80: current_loss=0.02981 | best_loss=0.02967
Epoch 49/80: current_loss=0.02975 | best_loss=0.02967
Epoch 50/80: current_loss=0.02972 | best_loss=0.02967
Epoch 51/80: current_loss=0.02971 | best_loss=0.02967
Epoch 52/80: current_loss=0.02982 | best_loss=0.02967
Epoch 53/80: current_loss=0.02975 | best_loss=0.02967
Epoch 54/80: current_loss=0.02979 | best_loss=0.02967
Epoch 55/80: current_loss=0.02986 | best_loss=0.02967
Epoch 56/80: current_loss=0.02974 | best_loss=0.02967
Early Stopping at epoch 56
      explained_var=0.00268 | mse_loss=0.02869
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00178| avg_loss=0.02691
----------------------------------------------


----------------------------------------------
Params for Trial 48
{'learning_rate': 0.0001, 'weight_decay': 0.001989665181342119, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03620 | best_loss=0.03620
Epoch 1/80: current_loss=0.03360 | best_loss=0.03360
Epoch 2/80: current_loss=0.03262 | best_loss=0.03262
Epoch 3/80: current_loss=0.03190 | best_loss=0.03190
Epoch 4/80: current_loss=0.03135 | best_loss=0.03135
Epoch 5/80: current_loss=0.03087 | best_loss=0.03087
Epoch 6/80: current_loss=0.03068 | best_loss=0.03068
Epoch 7/80: current_loss=0.03020 | best_loss=0.03020
Epoch 8/80: current_loss=0.03007 | best_loss=0.03007
Epoch 9/80: current_loss=0.02993 | best_loss=0.02993
Epoch 10/80: current_loss=0.02983 | best_loss=0.02983
Epoch 11/80: current_loss=0.02967 | best_loss=0.02967
Epoch 12/80: current_loss=0.02957 | best_loss=0.02957
Epoch 13/80: current_loss=0.02954 | best_loss=0.02954
Epoch 14/80: current_loss=0.02951 | best_loss=0.02951
Epoch 15/80: current_loss=0.02988 | best_loss=0.02951
Epoch 16/80: current_loss=0.02938 | best_loss=0.02938
Epoch 17/80: current_loss=0.02942 | best_loss=0.02938
Epoch 18/80: current_loss=0.02929 | best_loss=0.02929
Epoch 19/80: current_loss=0.02919 | best_loss=0.02919
Epoch 20/80: current_loss=0.02928 | best_loss=0.02919
Epoch 21/80: current_loss=0.02920 | best_loss=0.02919
Epoch 22/80: current_loss=0.02919 | best_loss=0.02919
Epoch 23/80: current_loss=0.02919 | best_loss=0.02919
Epoch 24/80: current_loss=0.02909 | best_loss=0.02909
Epoch 25/80: current_loss=0.02915 | best_loss=0.02909
Epoch 26/80: current_loss=0.02910 | best_loss=0.02909
Epoch 27/80: current_loss=0.02898 | best_loss=0.02898
Epoch 28/80: current_loss=0.02906 | best_loss=0.02898
Epoch 29/80: current_loss=0.02905 | best_loss=0.02898
Epoch 30/80: current_loss=0.02906 | best_loss=0.02898
Epoch 31/80: current_loss=0.02904 | best_loss=0.02898
Epoch 32/80: current_loss=0.02893 | best_loss=0.02893
Epoch 33/80: current_loss=0.02895 | best_loss=0.02893
Epoch 34/80: current_loss=0.02893 | best_loss=0.02893
Epoch 35/80: current_loss=0.02893 | best_loss=0.02893
Epoch 36/80: current_loss=0.02890 | best_loss=0.02890
Epoch 37/80: current_loss=0.02895 | best_loss=0.02890
Epoch 38/80: current_loss=0.02895 | best_loss=0.02890
Epoch 39/80: current_loss=0.02891 | best_loss=0.02890
Epoch 40/80: current_loss=0.02891 | best_loss=0.02890
Epoch 41/80: current_loss=0.02906 | best_loss=0.02890
Epoch 42/80: current_loss=0.02907 | best_loss=0.02890
Epoch 43/80: current_loss=0.02922 | best_loss=0.02890
Epoch 44/80: current_loss=0.02907 | best_loss=0.02890
Epoch 45/80: current_loss=0.02909 | best_loss=0.02890
Epoch 46/80: current_loss=0.02896 | best_loss=0.02890
Epoch 47/80: current_loss=0.02911 | best_loss=0.02890
Epoch 48/80: current_loss=0.02895 | best_loss=0.02890
Epoch 49/80: current_loss=0.02898 | best_loss=0.02890
Epoch 50/80: current_loss=0.02897 | best_loss=0.02890
Epoch 51/80: current_loss=0.02898 | best_loss=0.02890
Epoch 52/80: current_loss=0.02910 | best_loss=0.02890
Epoch 53/80: current_loss=0.02895 | best_loss=0.02890
Epoch 54/80: current_loss=0.02898 | best_loss=0.02890
Epoch 55/80: current_loss=0.02899 | best_loss=0.02890
Epoch 56/80: current_loss=0.02899 | best_loss=0.02890
Early Stopping at epoch 56
      explained_var=0.00464 | mse_loss=0.02801
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02493 | best_loss=0.02493
Epoch 1/80: current_loss=0.02545 | best_loss=0.02493
Epoch 2/80: current_loss=0.02518 | best_loss=0.02493
Epoch 3/80: current_loss=0.02490 | best_loss=0.02490
Epoch 4/80: current_loss=0.02494 | best_loss=0.02490
Epoch 5/80: current_loss=0.02487 | best_loss=0.02487
Epoch 6/80: current_loss=0.02509 | best_loss=0.02487
Epoch 7/80: current_loss=0.02490 | best_loss=0.02487
Epoch 8/80: current_loss=0.02505 | best_loss=0.02487
Epoch 9/80: current_loss=0.02497 | best_loss=0.02487
Epoch 10/80: current_loss=0.02517 | best_loss=0.02487
Epoch 11/80: current_loss=0.02494 | best_loss=0.02487
Epoch 12/80: current_loss=0.02491 | best_loss=0.02487
Epoch 13/80: current_loss=0.02519 | best_loss=0.02487
Epoch 14/80: current_loss=0.02524 | best_loss=0.02487
Epoch 15/80: current_loss=0.02530 | best_loss=0.02487
Epoch 16/80: current_loss=0.02492 | best_loss=0.02487
Epoch 17/80: current_loss=0.02498 | best_loss=0.02487
Epoch 18/80: current_loss=0.02523 | best_loss=0.02487
Epoch 19/80: current_loss=0.02547 | best_loss=0.02487
Epoch 20/80: current_loss=0.02500 | best_loss=0.02487
Epoch 21/80: current_loss=0.02504 | best_loss=0.02487
Epoch 22/80: current_loss=0.02499 | best_loss=0.02487
Epoch 23/80: current_loss=0.02500 | best_loss=0.02487
Epoch 24/80: current_loss=0.02497 | best_loss=0.02487
Epoch 25/80: current_loss=0.02496 | best_loss=0.02487
Early Stopping at epoch 25
      explained_var=0.00189 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02743 | best_loss=0.02743
Epoch 1/80: current_loss=0.02794 | best_loss=0.02743
Epoch 2/80: current_loss=0.02812 | best_loss=0.02743
Epoch 3/80: current_loss=0.02788 | best_loss=0.02743
Epoch 4/80: current_loss=0.02754 | best_loss=0.02743
Epoch 5/80: current_loss=0.02830 | best_loss=0.02743
Epoch 6/80: current_loss=0.02750 | best_loss=0.02743
Epoch 7/80: current_loss=0.02853 | best_loss=0.02743
Epoch 8/80: current_loss=0.02761 | best_loss=0.02743
Epoch 9/80: current_loss=0.02793 | best_loss=0.02743
Epoch 10/80: current_loss=0.02752 | best_loss=0.02743
Epoch 11/80: current_loss=0.02789 | best_loss=0.02743
Epoch 12/80: current_loss=0.02743 | best_loss=0.02743
Epoch 13/80: current_loss=0.02747 | best_loss=0.02743
Epoch 14/80: current_loss=0.02737 | best_loss=0.02737
Epoch 15/80: current_loss=0.02802 | best_loss=0.02737
Epoch 16/80: current_loss=0.02779 | best_loss=0.02737
Epoch 17/80: current_loss=0.02800 | best_loss=0.02737
Epoch 18/80: current_loss=0.02755 | best_loss=0.02737
Epoch 19/80: current_loss=0.02760 | best_loss=0.02737
Epoch 20/80: current_loss=0.02771 | best_loss=0.02737
Epoch 21/80: current_loss=0.02760 | best_loss=0.02737
Epoch 22/80: current_loss=0.02769 | best_loss=0.02737
Epoch 23/80: current_loss=0.02792 | best_loss=0.02737
Epoch 24/80: current_loss=0.02740 | best_loss=0.02737
Epoch 25/80: current_loss=0.02790 | best_loss=0.02737
Epoch 26/80: current_loss=0.02816 | best_loss=0.02737
Epoch 27/80: current_loss=0.02788 | best_loss=0.02737
Epoch 28/80: current_loss=0.02839 | best_loss=0.02737
Epoch 29/80: current_loss=0.02769 | best_loss=0.02737
Epoch 30/80: current_loss=0.02859 | best_loss=0.02737
Epoch 31/80: current_loss=0.02775 | best_loss=0.02737
Epoch 32/80: current_loss=0.02786 | best_loss=0.02737
Epoch 33/80: current_loss=0.02771 | best_loss=0.02737
Epoch 34/80: current_loss=0.02754 | best_loss=0.02737
Early Stopping at epoch 34
      explained_var=-0.00535 | mse_loss=0.02782
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02721 | best_loss=0.02721
Epoch 1/80: current_loss=0.02719 | best_loss=0.02719
Epoch 2/80: current_loss=0.02696 | best_loss=0.02696
Epoch 3/80: current_loss=0.02701 | best_loss=0.02696
Epoch 4/80: current_loss=0.02681 | best_loss=0.02681
Epoch 5/80: current_loss=0.02708 | best_loss=0.02681
Epoch 6/80: current_loss=0.02680 | best_loss=0.02680
Epoch 7/80: current_loss=0.02685 | best_loss=0.02680
Epoch 8/80: current_loss=0.02712 | best_loss=0.02680
Epoch 9/80: current_loss=0.02683 | best_loss=0.02680
Epoch 10/80: current_loss=0.02687 | best_loss=0.02680
Epoch 11/80: current_loss=0.02680 | best_loss=0.02680
Epoch 12/80: current_loss=0.02685 | best_loss=0.02680
Epoch 13/80: current_loss=0.02685 | best_loss=0.02680
Epoch 14/80: current_loss=0.02679 | best_loss=0.02679
Epoch 15/80: current_loss=0.02679 | best_loss=0.02679
Epoch 16/80: current_loss=0.02681 | best_loss=0.02679
Epoch 17/80: current_loss=0.02678 | best_loss=0.02678
Epoch 18/80: current_loss=0.02684 | best_loss=0.02678
Epoch 19/80: current_loss=0.02691 | best_loss=0.02678
Epoch 20/80: current_loss=0.02692 | best_loss=0.02678
Epoch 21/80: current_loss=0.02679 | best_loss=0.02678
Epoch 22/80: current_loss=0.02711 | best_loss=0.02678
Epoch 23/80: current_loss=0.02682 | best_loss=0.02678
Epoch 24/80: current_loss=0.02682 | best_loss=0.02678
Epoch 25/80: current_loss=0.02692 | best_loss=0.02678
Epoch 26/80: current_loss=0.02680 | best_loss=0.02678
Epoch 27/80: current_loss=0.02679 | best_loss=0.02678
Epoch 28/80: current_loss=0.02681 | best_loss=0.02678
Epoch 29/80: current_loss=0.02683 | best_loss=0.02678
Epoch 30/80: current_loss=0.02685 | best_loss=0.02678
Epoch 31/80: current_loss=0.02682 | best_loss=0.02678
Epoch 32/80: current_loss=0.02696 | best_loss=0.02678
Epoch 33/80: current_loss=0.02682 | best_loss=0.02678
Epoch 34/80: current_loss=0.02681 | best_loss=0.02678
Epoch 35/80: current_loss=0.02683 | best_loss=0.02678
Epoch 36/80: current_loss=0.02679 | best_loss=0.02678
Epoch 37/80: current_loss=0.02687 | best_loss=0.02678
Early Stopping at epoch 37
      explained_var=0.00127 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02980 | best_loss=0.02980
Epoch 1/80: current_loss=0.02980 | best_loss=0.02980
Epoch 2/80: current_loss=0.02978 | best_loss=0.02978
Epoch 3/80: current_loss=0.02987 | best_loss=0.02978
Epoch 4/80: current_loss=0.02997 | best_loss=0.02978
Epoch 5/80: current_loss=0.02983 | best_loss=0.02978
Epoch 6/80: current_loss=0.03024 | best_loss=0.02978
Epoch 7/80: current_loss=0.02975 | best_loss=0.02975
Epoch 8/80: current_loss=0.03050 | best_loss=0.02975
Epoch 9/80: current_loss=0.02974 | best_loss=0.02974
Epoch 10/80: current_loss=0.02977 | best_loss=0.02974
Epoch 11/80: current_loss=0.02977 | best_loss=0.02974
Epoch 12/80: current_loss=0.02995 | best_loss=0.02974
Epoch 13/80: current_loss=0.02991 | best_loss=0.02974
Epoch 14/80: current_loss=0.02987 | best_loss=0.02974
Epoch 15/80: current_loss=0.02985 | best_loss=0.02974
Epoch 16/80: current_loss=0.02995 | best_loss=0.02974
Epoch 17/80: current_loss=0.02993 | best_loss=0.02974
Epoch 18/80: current_loss=0.02981 | best_loss=0.02974
Epoch 19/80: current_loss=0.02996 | best_loss=0.02974
Epoch 20/80: current_loss=0.02979 | best_loss=0.02974
Epoch 21/80: current_loss=0.02983 | best_loss=0.02974
Epoch 22/80: current_loss=0.02979 | best_loss=0.02974
Epoch 23/80: current_loss=0.03034 | best_loss=0.02974
Epoch 24/80: current_loss=0.02994 | best_loss=0.02974
Epoch 25/80: current_loss=0.02984 | best_loss=0.02974
Epoch 26/80: current_loss=0.03005 | best_loss=0.02974
Epoch 27/80: current_loss=0.02979 | best_loss=0.02974
Epoch 28/80: current_loss=0.03002 | best_loss=0.02974
Epoch 29/80: current_loss=0.02980 | best_loss=0.02974
Early Stopping at epoch 29
      explained_var=0.00066 | mse_loss=0.02874
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.00062| avg_loss=0.02694
----------------------------------------------


----------------------------------------------
Params for Trial 49
{'learning_rate': 0.0001, 'weight_decay': 0.0025124636019387544, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03867 | best_loss=0.03867
Epoch 1/80: current_loss=0.03622 | best_loss=0.03622
Epoch 2/80: current_loss=0.03473 | best_loss=0.03473
Epoch 3/80: current_loss=0.03342 | best_loss=0.03342
Epoch 4/80: current_loss=0.03266 | best_loss=0.03266
Epoch 5/80: current_loss=0.03210 | best_loss=0.03210
Epoch 6/80: current_loss=0.03166 | best_loss=0.03166
Epoch 7/80: current_loss=0.03122 | best_loss=0.03122
Epoch 8/80: current_loss=0.03127 | best_loss=0.03122
Epoch 9/80: current_loss=0.03073 | best_loss=0.03073
Epoch 10/80: current_loss=0.03052 | best_loss=0.03052
Epoch 11/80: current_loss=0.03035 | best_loss=0.03035
Epoch 12/80: current_loss=0.03015 | best_loss=0.03015
Epoch 13/80: current_loss=0.02999 | best_loss=0.02999
Epoch 14/80: current_loss=0.02998 | best_loss=0.02998
Epoch 15/80: current_loss=0.02983 | best_loss=0.02983
Epoch 16/80: current_loss=0.03007 | best_loss=0.02983
Epoch 17/80: current_loss=0.02964 | best_loss=0.02964
Epoch 18/80: current_loss=0.02946 | best_loss=0.02946
Epoch 19/80: current_loss=0.02963 | best_loss=0.02946
Epoch 20/80: current_loss=0.02933 | best_loss=0.02933
Epoch 21/80: current_loss=0.02956 | best_loss=0.02933
Epoch 22/80: current_loss=0.02932 | best_loss=0.02932
Epoch 23/80: current_loss=0.02926 | best_loss=0.02926
Epoch 24/80: current_loss=0.02956 | best_loss=0.02926
Epoch 25/80: current_loss=0.02914 | best_loss=0.02914
Epoch 26/80: current_loss=0.02909 | best_loss=0.02909
Epoch 27/80: current_loss=0.02915 | best_loss=0.02909
Epoch 28/80: current_loss=0.02908 | best_loss=0.02908
Epoch 29/80: current_loss=0.02906 | best_loss=0.02906
Epoch 30/80: current_loss=0.02909 | best_loss=0.02906
Epoch 31/80: current_loss=0.02913 | best_loss=0.02906
Epoch 32/80: current_loss=0.02901 | best_loss=0.02901
Epoch 33/80: current_loss=0.02904 | best_loss=0.02901
Epoch 34/80: current_loss=0.02900 | best_loss=0.02900
Epoch 35/80: current_loss=0.02936 | best_loss=0.02900
Epoch 36/80: current_loss=0.02921 | best_loss=0.02900
Epoch 37/80: current_loss=0.02905 | best_loss=0.02900
Epoch 38/80: current_loss=0.02901 | best_loss=0.02900
Epoch 39/80: current_loss=0.02902 | best_loss=0.02900
Epoch 40/80: current_loss=0.02899 | best_loss=0.02899
Epoch 41/80: current_loss=0.02905 | best_loss=0.02899
Epoch 42/80: current_loss=0.02900 | best_loss=0.02899
Epoch 43/80: current_loss=0.02907 | best_loss=0.02899
Epoch 44/80: current_loss=0.02920 | best_loss=0.02899
Epoch 45/80: current_loss=0.02900 | best_loss=0.02899
Epoch 46/80: current_loss=0.02897 | best_loss=0.02897
Epoch 47/80: current_loss=0.02897 | best_loss=0.02897
Epoch 48/80: current_loss=0.02900 | best_loss=0.02897
Epoch 49/80: current_loss=0.02901 | best_loss=0.02897
Epoch 50/80: current_loss=0.02907 | best_loss=0.02897
Epoch 51/80: current_loss=0.02914 | best_loss=0.02897
Epoch 52/80: current_loss=0.02898 | best_loss=0.02897
Epoch 53/80: current_loss=0.02898 | best_loss=0.02897
Epoch 54/80: current_loss=0.02904 | best_loss=0.02897
Epoch 55/80: current_loss=0.02899 | best_loss=0.02897
Epoch 56/80: current_loss=0.02905 | best_loss=0.02897
Epoch 57/80: current_loss=0.02901 | best_loss=0.02897
Epoch 58/80: current_loss=0.02916 | best_loss=0.02897
Epoch 59/80: current_loss=0.02902 | best_loss=0.02897
Epoch 60/80: current_loss=0.02906 | best_loss=0.02897
Epoch 61/80: current_loss=0.02915 | best_loss=0.02897
Epoch 62/80: current_loss=0.02916 | best_loss=0.02897
Epoch 63/80: current_loss=0.02907 | best_loss=0.02897
Epoch 64/80: current_loss=0.02912 | best_loss=0.02897
Epoch 65/80: current_loss=0.02907 | best_loss=0.02897
Epoch 66/80: current_loss=0.02936 | best_loss=0.02897
Early Stopping at epoch 66
      explained_var=0.00299 | mse_loss=0.02806
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02490 | best_loss=0.02490
Epoch 1/80: current_loss=0.02495 | best_loss=0.02490
Epoch 2/80: current_loss=0.02497 | best_loss=0.02490
Epoch 3/80: current_loss=0.02495 | best_loss=0.02490
Epoch 4/80: current_loss=0.02514 | best_loss=0.02490
Epoch 5/80: current_loss=0.02496 | best_loss=0.02490
Epoch 6/80: current_loss=0.02498 | best_loss=0.02490
Epoch 7/80: current_loss=0.02513 | best_loss=0.02490
Epoch 8/80: current_loss=0.02493 | best_loss=0.02490
Epoch 9/80: current_loss=0.02510 | best_loss=0.02490
Epoch 10/80: current_loss=0.02502 | best_loss=0.02490
Epoch 11/80: current_loss=0.02493 | best_loss=0.02490
Epoch 12/80: current_loss=0.02499 | best_loss=0.02490
Epoch 13/80: current_loss=0.02490 | best_loss=0.02490
Epoch 14/80: current_loss=0.02497 | best_loss=0.02490
Epoch 15/80: current_loss=0.02496 | best_loss=0.02490
Epoch 16/80: current_loss=0.02491 | best_loss=0.02490
Epoch 17/80: current_loss=0.02492 | best_loss=0.02490
Epoch 18/80: current_loss=0.02494 | best_loss=0.02490
Epoch 19/80: current_loss=0.02508 | best_loss=0.02490
Epoch 20/80: current_loss=0.02507 | best_loss=0.02490
Early Stopping at epoch 20
      explained_var=0.00146 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02799 | best_loss=0.02799
Epoch 1/80: current_loss=0.02805 | best_loss=0.02799
Epoch 2/80: current_loss=0.02746 | best_loss=0.02746
Epoch 3/80: current_loss=0.02784 | best_loss=0.02746
Epoch 4/80: current_loss=0.02765 | best_loss=0.02746
Epoch 5/80: current_loss=0.02742 | best_loss=0.02742
Epoch 6/80: current_loss=0.02753 | best_loss=0.02742
Epoch 7/80: current_loss=0.02761 | best_loss=0.02742
Epoch 8/80: current_loss=0.02875 | best_loss=0.02742
Epoch 9/80: current_loss=0.02751 | best_loss=0.02742
Epoch 10/80: current_loss=0.02820 | best_loss=0.02742
Epoch 11/80: current_loss=0.02732 | best_loss=0.02732
Epoch 12/80: current_loss=0.02754 | best_loss=0.02732
Epoch 13/80: current_loss=0.02735 | best_loss=0.02732
Epoch 14/80: current_loss=0.02776 | best_loss=0.02732
Epoch 15/80: current_loss=0.02765 | best_loss=0.02732
Epoch 16/80: current_loss=0.02761 | best_loss=0.02732
Epoch 17/80: current_loss=0.02778 | best_loss=0.02732
Epoch 18/80: current_loss=0.02761 | best_loss=0.02732
Epoch 19/80: current_loss=0.02731 | best_loss=0.02731
Epoch 20/80: current_loss=0.02857 | best_loss=0.02731
Epoch 21/80: current_loss=0.02757 | best_loss=0.02731
Epoch 22/80: current_loss=0.02809 | best_loss=0.02731
Epoch 23/80: current_loss=0.02764 | best_loss=0.02731
Epoch 24/80: current_loss=0.02757 | best_loss=0.02731
Epoch 25/80: current_loss=0.02733 | best_loss=0.02731
Epoch 26/80: current_loss=0.02754 | best_loss=0.02731
Epoch 27/80: current_loss=0.02793 | best_loss=0.02731
Epoch 28/80: current_loss=0.02774 | best_loss=0.02731
Epoch 29/80: current_loss=0.02816 | best_loss=0.02731
Epoch 30/80: current_loss=0.02741 | best_loss=0.02731
Epoch 31/80: current_loss=0.02761 | best_loss=0.02731
Epoch 32/80: current_loss=0.02796 | best_loss=0.02731
Epoch 33/80: current_loss=0.02782 | best_loss=0.02731
Epoch 34/80: current_loss=0.02780 | best_loss=0.02731
Epoch 35/80: current_loss=0.02791 | best_loss=0.02731
Epoch 36/80: current_loss=0.02753 | best_loss=0.02731
Epoch 37/80: current_loss=0.02766 | best_loss=0.02731
Epoch 38/80: current_loss=0.02747 | best_loss=0.02731
Epoch 39/80: current_loss=0.02754 | best_loss=0.02731
Early Stopping at epoch 39
      explained_var=-0.00823 | mse_loss=0.02779
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02679 | best_loss=0.02679
Epoch 1/80: current_loss=0.02716 | best_loss=0.02679
Epoch 2/80: current_loss=0.02688 | best_loss=0.02679
Epoch 3/80: current_loss=0.02679 | best_loss=0.02679
Epoch 4/80: current_loss=0.02679 | best_loss=0.02679
Epoch 5/80: current_loss=0.02684 | best_loss=0.02679
Epoch 6/80: current_loss=0.02698 | best_loss=0.02679
Epoch 7/80: current_loss=0.02676 | best_loss=0.02676
Epoch 8/80: current_loss=0.02695 | best_loss=0.02676
Epoch 9/80: current_loss=0.02676 | best_loss=0.02676
Epoch 10/80: current_loss=0.02677 | best_loss=0.02676
Epoch 11/80: current_loss=0.02679 | best_loss=0.02676
Epoch 12/80: current_loss=0.02698 | best_loss=0.02676
Epoch 13/80: current_loss=0.02700 | best_loss=0.02676
Epoch 14/80: current_loss=0.02685 | best_loss=0.02676
Epoch 15/80: current_loss=0.02731 | best_loss=0.02676
Epoch 16/80: current_loss=0.02702 | best_loss=0.02676
Epoch 17/80: current_loss=0.02677 | best_loss=0.02676
Epoch 18/80: current_loss=0.02686 | best_loss=0.02676
Epoch 19/80: current_loss=0.02704 | best_loss=0.02676
Epoch 20/80: current_loss=0.02680 | best_loss=0.02676
Epoch 21/80: current_loss=0.02677 | best_loss=0.02676
Epoch 22/80: current_loss=0.02680 | best_loss=0.02676
Epoch 23/80: current_loss=0.02724 | best_loss=0.02676
Epoch 24/80: current_loss=0.02681 | best_loss=0.02676
Epoch 25/80: current_loss=0.02696 | best_loss=0.02676
Epoch 26/80: current_loss=0.02699 | best_loss=0.02676
Epoch 27/80: current_loss=0.02687 | best_loss=0.02676
Epoch 28/80: current_loss=0.02687 | best_loss=0.02676
Epoch 29/80: current_loss=0.02686 | best_loss=0.02676
Early Stopping at epoch 29
      explained_var=0.00197 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02987 | best_loss=0.02987
Epoch 1/80: current_loss=0.02979 | best_loss=0.02979
Epoch 2/80: current_loss=0.02990 | best_loss=0.02979
Epoch 3/80: current_loss=0.02976 | best_loss=0.02976
Epoch 4/80: current_loss=0.02996 | best_loss=0.02976
Epoch 5/80: current_loss=0.02976 | best_loss=0.02976
Epoch 6/80: current_loss=0.02987 | best_loss=0.02976
Epoch 7/80: current_loss=0.03058 | best_loss=0.02976
Epoch 8/80: current_loss=0.02975 | best_loss=0.02975
Epoch 9/80: current_loss=0.02972 | best_loss=0.02972
Epoch 10/80: current_loss=0.02971 | best_loss=0.02971
Epoch 11/80: current_loss=0.02972 | best_loss=0.02971
Epoch 12/80: current_loss=0.02977 | best_loss=0.02971
Epoch 13/80: current_loss=0.02972 | best_loss=0.02971
Epoch 14/80: current_loss=0.02977 | best_loss=0.02971
Epoch 15/80: current_loss=0.02987 | best_loss=0.02971
Epoch 16/80: current_loss=0.02970 | best_loss=0.02970
Epoch 17/80: current_loss=0.02980 | best_loss=0.02970
Epoch 18/80: current_loss=0.02975 | best_loss=0.02970
Epoch 19/80: current_loss=0.02977 | best_loss=0.02970
Epoch 20/80: current_loss=0.02972 | best_loss=0.02970
Epoch 21/80: current_loss=0.02978 | best_loss=0.02970
Epoch 22/80: current_loss=0.02980 | best_loss=0.02970
Epoch 23/80: current_loss=0.02973 | best_loss=0.02970
Epoch 24/80: current_loss=0.02979 | best_loss=0.02970
Epoch 25/80: current_loss=0.02992 | best_loss=0.02970
Epoch 26/80: current_loss=0.02989 | best_loss=0.02970
Epoch 27/80: current_loss=0.02984 | best_loss=0.02970
Epoch 28/80: current_loss=0.02974 | best_loss=0.02970
Epoch 29/80: current_loss=0.02975 | best_loss=0.02970
Epoch 30/80: current_loss=0.03003 | best_loss=0.02970
Epoch 31/80: current_loss=0.03093 | best_loss=0.02970
Epoch 32/80: current_loss=0.02982 | best_loss=0.02970
Epoch 33/80: current_loss=0.03020 | best_loss=0.02970
Epoch 34/80: current_loss=0.02978 | best_loss=0.02970
Epoch 35/80: current_loss=0.03003 | best_loss=0.02970
Epoch 36/80: current_loss=0.02987 | best_loss=0.02970
Early Stopping at epoch 36
      explained_var=0.00210 | mse_loss=0.02870
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00006| avg_loss=0.02693
----------------------------------------------


----------------------------------------------
Params for Trial 50
{'learning_rate': 0.0001, 'weight_decay': 0.0003201933246352698, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03356 | best_loss=0.03356
Epoch 1/80: current_loss=0.03224 | best_loss=0.03224
Epoch 2/80: current_loss=0.03113 | best_loss=0.03113
Epoch 3/80: current_loss=0.03067 | best_loss=0.03067
Epoch 4/80: current_loss=0.03013 | best_loss=0.03013
Epoch 5/80: current_loss=0.02986 | best_loss=0.02986
Epoch 6/80: current_loss=0.02958 | best_loss=0.02958
Epoch 7/80: current_loss=0.02921 | best_loss=0.02921
Epoch 8/80: current_loss=0.02901 | best_loss=0.02901
Epoch 9/80: current_loss=0.02906 | best_loss=0.02901
Epoch 10/80: current_loss=0.02886 | best_loss=0.02886
Epoch 11/80: current_loss=0.02888 | best_loss=0.02886
Epoch 12/80: current_loss=0.02879 | best_loss=0.02879
Epoch 13/80: current_loss=0.02881 | best_loss=0.02879
Epoch 14/80: current_loss=0.02885 | best_loss=0.02879
Epoch 15/80: current_loss=0.02884 | best_loss=0.02879
Epoch 16/80: current_loss=0.02897 | best_loss=0.02879
Epoch 17/80: current_loss=0.02891 | best_loss=0.02879
Epoch 18/80: current_loss=0.02901 | best_loss=0.02879
Epoch 19/80: current_loss=0.02896 | best_loss=0.02879
Epoch 20/80: current_loss=0.02902 | best_loss=0.02879
Epoch 21/80: current_loss=0.02892 | best_loss=0.02879
Epoch 22/80: current_loss=0.02897 | best_loss=0.02879
Epoch 23/80: current_loss=0.02899 | best_loss=0.02879
Epoch 24/80: current_loss=0.02908 | best_loss=0.02879
Epoch 25/80: current_loss=0.02899 | best_loss=0.02879
Epoch 26/80: current_loss=0.02897 | best_loss=0.02879
Epoch 27/80: current_loss=0.02895 | best_loss=0.02879
Epoch 28/80: current_loss=0.02916 | best_loss=0.02879
Epoch 29/80: current_loss=0.02906 | best_loss=0.02879
Epoch 30/80: current_loss=0.02896 | best_loss=0.02879
Epoch 31/80: current_loss=0.02903 | best_loss=0.02879
Epoch 32/80: current_loss=0.02910 | best_loss=0.02879
Early Stopping at epoch 32
      explained_var=0.00797 | mse_loss=0.02793
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02486 | best_loss=0.02486
Epoch 1/80: current_loss=0.02491 | best_loss=0.02486
Epoch 2/80: current_loss=0.02490 | best_loss=0.02486
Epoch 3/80: current_loss=0.02487 | best_loss=0.02486
Epoch 4/80: current_loss=0.02490 | best_loss=0.02486
Epoch 5/80: current_loss=0.02487 | best_loss=0.02486
Epoch 6/80: current_loss=0.02487 | best_loss=0.02486
Epoch 7/80: current_loss=0.02507 | best_loss=0.02486
Epoch 8/80: current_loss=0.02502 | best_loss=0.02486
Epoch 9/80: current_loss=0.02493 | best_loss=0.02486
Epoch 10/80: current_loss=0.02502 | best_loss=0.02486
Epoch 11/80: current_loss=0.02528 | best_loss=0.02486
Epoch 12/80: current_loss=0.02493 | best_loss=0.02486
Epoch 13/80: current_loss=0.02502 | best_loss=0.02486
Epoch 14/80: current_loss=0.02485 | best_loss=0.02485
Epoch 15/80: current_loss=0.02498 | best_loss=0.02485
Epoch 16/80: current_loss=0.02495 | best_loss=0.02485
Epoch 17/80: current_loss=0.02495 | best_loss=0.02485
Epoch 18/80: current_loss=0.02489 | best_loss=0.02485
Epoch 19/80: current_loss=0.02502 | best_loss=0.02485
Epoch 20/80: current_loss=0.02515 | best_loss=0.02485
Epoch 21/80: current_loss=0.02498 | best_loss=0.02485
Epoch 22/80: current_loss=0.02500 | best_loss=0.02485
Epoch 23/80: current_loss=0.02497 | best_loss=0.02485
Epoch 24/80: current_loss=0.02505 | best_loss=0.02485
Epoch 25/80: current_loss=0.02513 | best_loss=0.02485
Epoch 26/80: current_loss=0.02504 | best_loss=0.02485
Epoch 27/80: current_loss=0.02502 | best_loss=0.02485
Epoch 28/80: current_loss=0.02502 | best_loss=0.02485
Epoch 29/80: current_loss=0.02519 | best_loss=0.02485
Epoch 30/80: current_loss=0.02513 | best_loss=0.02485
Epoch 31/80: current_loss=0.02514 | best_loss=0.02485
Epoch 32/80: current_loss=0.02493 | best_loss=0.02485
Epoch 33/80: current_loss=0.02507 | best_loss=0.02485
Epoch 34/80: current_loss=0.02499 | best_loss=0.02485
Early Stopping at epoch 34
      explained_var=0.00150 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02729 | best_loss=0.02729
Epoch 1/80: current_loss=0.02766 | best_loss=0.02729
Epoch 2/80: current_loss=0.02757 | best_loss=0.02729
Epoch 3/80: current_loss=0.02773 | best_loss=0.02729
Epoch 4/80: current_loss=0.02766 | best_loss=0.02729
Epoch 5/80: current_loss=0.02765 | best_loss=0.02729
Epoch 6/80: current_loss=0.02787 | best_loss=0.02729
Epoch 7/80: current_loss=0.02813 | best_loss=0.02729
Epoch 8/80: current_loss=0.02767 | best_loss=0.02729
Epoch 9/80: current_loss=0.02767 | best_loss=0.02729
Epoch 10/80: current_loss=0.02745 | best_loss=0.02729
Epoch 11/80: current_loss=0.02738 | best_loss=0.02729
Epoch 12/80: current_loss=0.02769 | best_loss=0.02729
Epoch 13/80: current_loss=0.02766 | best_loss=0.02729
Epoch 14/80: current_loss=0.02739 | best_loss=0.02729
Epoch 15/80: current_loss=0.02781 | best_loss=0.02729
Epoch 16/80: current_loss=0.02867 | best_loss=0.02729
Epoch 17/80: current_loss=0.02735 | best_loss=0.02729
Epoch 18/80: current_loss=0.02752 | best_loss=0.02729
Epoch 19/80: current_loss=0.02741 | best_loss=0.02729
Epoch 20/80: current_loss=0.02752 | best_loss=0.02729
Early Stopping at epoch 20
      explained_var=0.00074 | mse_loss=0.02768
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02681 | best_loss=0.02681
Epoch 1/80: current_loss=0.02683 | best_loss=0.02681
Epoch 2/80: current_loss=0.02693 | best_loss=0.02681
Epoch 3/80: current_loss=0.02688 | best_loss=0.02681
Epoch 4/80: current_loss=0.02691 | best_loss=0.02681
Epoch 5/80: current_loss=0.02692 | best_loss=0.02681
Epoch 6/80: current_loss=0.02721 | best_loss=0.02681
Epoch 7/80: current_loss=0.02702 | best_loss=0.02681
Epoch 8/80: current_loss=0.02701 | best_loss=0.02681
Epoch 9/80: current_loss=0.02698 | best_loss=0.02681
Epoch 10/80: current_loss=0.02696 | best_loss=0.02681
Epoch 11/80: current_loss=0.02685 | best_loss=0.02681
Epoch 12/80: current_loss=0.02692 | best_loss=0.02681
Epoch 13/80: current_loss=0.02683 | best_loss=0.02681
Epoch 14/80: current_loss=0.02702 | best_loss=0.02681
Epoch 15/80: current_loss=0.02686 | best_loss=0.02681
Epoch 16/80: current_loss=0.02683 | best_loss=0.02681
Epoch 17/80: current_loss=0.02686 | best_loss=0.02681
Epoch 18/80: current_loss=0.02699 | best_loss=0.02681
Epoch 19/80: current_loss=0.02712 | best_loss=0.02681
Epoch 20/80: current_loss=0.02704 | best_loss=0.02681
Early Stopping at epoch 20
      explained_var=0.00202 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02972 | best_loss=0.02972
Epoch 1/80: current_loss=0.02988 | best_loss=0.02972
Epoch 2/80: current_loss=0.03001 | best_loss=0.02972
Epoch 3/80: current_loss=0.02984 | best_loss=0.02972
Epoch 4/80: current_loss=0.02982 | best_loss=0.02972
Epoch 5/80: current_loss=0.02990 | best_loss=0.02972
Epoch 6/80: current_loss=0.02980 | best_loss=0.02972
Epoch 7/80: current_loss=0.02972 | best_loss=0.02972
Epoch 8/80: current_loss=0.02982 | best_loss=0.02972
Epoch 9/80: current_loss=0.02961 | best_loss=0.02961
Epoch 10/80: current_loss=0.02967 | best_loss=0.02961
Epoch 11/80: current_loss=0.02973 | best_loss=0.02961
Epoch 12/80: current_loss=0.02969 | best_loss=0.02961
Epoch 13/80: current_loss=0.02970 | best_loss=0.02961
Epoch 14/80: current_loss=0.02969 | best_loss=0.02961
Epoch 15/80: current_loss=0.02978 | best_loss=0.02961
Epoch 16/80: current_loss=0.02993 | best_loss=0.02961
Epoch 17/80: current_loss=0.02974 | best_loss=0.02961
Epoch 18/80: current_loss=0.02962 | best_loss=0.02961
Epoch 19/80: current_loss=0.02965 | best_loss=0.02961
Epoch 20/80: current_loss=0.02962 | best_loss=0.02961
Epoch 21/80: current_loss=0.02976 | best_loss=0.02961
Epoch 22/80: current_loss=0.02975 | best_loss=0.02961
Epoch 23/80: current_loss=0.02974 | best_loss=0.02961
Epoch 24/80: current_loss=0.02972 | best_loss=0.02961
Epoch 25/80: current_loss=0.02975 | best_loss=0.02961
Epoch 26/80: current_loss=0.02977 | best_loss=0.02961
Epoch 27/80: current_loss=0.02982 | best_loss=0.02961
Epoch 28/80: current_loss=0.03017 | best_loss=0.02961
Epoch 29/80: current_loss=0.02993 | best_loss=0.02961
Early Stopping at epoch 29
      explained_var=0.00414 | mse_loss=0.02865
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=0.00328| avg_loss=0.02687
----------------------------------------------


----------------------------------------------
Params for Trial 51
{'learning_rate': 0.0001, 'weight_decay': 0.00040285427163279684, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03748 | best_loss=0.03748
Epoch 1/80: current_loss=0.03480 | best_loss=0.03480
Epoch 2/80: current_loss=0.03328 | best_loss=0.03328
Epoch 3/80: current_loss=0.03238 | best_loss=0.03238
Epoch 4/80: current_loss=0.03165 | best_loss=0.03165
Epoch 5/80: current_loss=0.03097 | best_loss=0.03097
Epoch 6/80: current_loss=0.03052 | best_loss=0.03052
Epoch 7/80: current_loss=0.03032 | best_loss=0.03032
Epoch 8/80: current_loss=0.03032 | best_loss=0.03032
Epoch 9/80: current_loss=0.02991 | best_loss=0.02991
Epoch 10/80: current_loss=0.02962 | best_loss=0.02962
Epoch 11/80: current_loss=0.02972 | best_loss=0.02962
Epoch 12/80: current_loss=0.02938 | best_loss=0.02938
Epoch 13/80: current_loss=0.02950 | best_loss=0.02938
Epoch 14/80: current_loss=0.02922 | best_loss=0.02922
Epoch 15/80: current_loss=0.02924 | best_loss=0.02922
Epoch 16/80: current_loss=0.02918 | best_loss=0.02918
Epoch 17/80: current_loss=0.02919 | best_loss=0.02918
Epoch 18/80: current_loss=0.02919 | best_loss=0.02918
Epoch 19/80: current_loss=0.02914 | best_loss=0.02914
Epoch 20/80: current_loss=0.02917 | best_loss=0.02914
Epoch 21/80: current_loss=0.02923 | best_loss=0.02914
Epoch 22/80: current_loss=0.02931 | best_loss=0.02914
Epoch 23/80: current_loss=0.02915 | best_loss=0.02914
Epoch 24/80: current_loss=0.02905 | best_loss=0.02905
Epoch 25/80: current_loss=0.02899 | best_loss=0.02899
Epoch 26/80: current_loss=0.02900 | best_loss=0.02899
Epoch 27/80: current_loss=0.02904 | best_loss=0.02899
Epoch 28/80: current_loss=0.02895 | best_loss=0.02895
Epoch 29/80: current_loss=0.02902 | best_loss=0.02895
Epoch 30/80: current_loss=0.02908 | best_loss=0.02895
Epoch 31/80: current_loss=0.02898 | best_loss=0.02895
Epoch 32/80: current_loss=0.02908 | best_loss=0.02895
Epoch 33/80: current_loss=0.02942 | best_loss=0.02895
Epoch 34/80: current_loss=0.02899 | best_loss=0.02895
Epoch 35/80: current_loss=0.02904 | best_loss=0.02895
Epoch 36/80: current_loss=0.02916 | best_loss=0.02895
Epoch 37/80: current_loss=0.02910 | best_loss=0.02895
Epoch 38/80: current_loss=0.02913 | best_loss=0.02895
Epoch 39/80: current_loss=0.02917 | best_loss=0.02895
Epoch 40/80: current_loss=0.02909 | best_loss=0.02895
Epoch 41/80: current_loss=0.02928 | best_loss=0.02895
Epoch 42/80: current_loss=0.02910 | best_loss=0.02895
Epoch 43/80: current_loss=0.02935 | best_loss=0.02895
Epoch 44/80: current_loss=0.02917 | best_loss=0.02895
Epoch 45/80: current_loss=0.02923 | best_loss=0.02895
Epoch 46/80: current_loss=0.02908 | best_loss=0.02895
Epoch 47/80: current_loss=0.02909 | best_loss=0.02895
Epoch 48/80: current_loss=0.02902 | best_loss=0.02895
Early Stopping at epoch 48
      explained_var=0.00384 | mse_loss=0.02812

----------------------------------------------
Params for Trial 52
{'learning_rate': 0.0001, 'weight_decay': 0.0010148916216445982, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03035 | best_loss=0.03035
Epoch 1/80: current_loss=0.02971 | best_loss=0.02971
Epoch 2/80: current_loss=0.02931 | best_loss=0.02931
Epoch 3/80: current_loss=0.02912 | best_loss=0.02912
Epoch 4/80: current_loss=0.02907 | best_loss=0.02907
Epoch 5/80: current_loss=0.02893 | best_loss=0.02893
Epoch 6/80: current_loss=0.02891 | best_loss=0.02891
Epoch 7/80: current_loss=0.02878 | best_loss=0.02878
Epoch 8/80: current_loss=0.02880 | best_loss=0.02878
Epoch 9/80: current_loss=0.02905 | best_loss=0.02878
Epoch 10/80: current_loss=0.02893 | best_loss=0.02878
Epoch 11/80: current_loss=0.02878 | best_loss=0.02878
Epoch 12/80: current_loss=0.02876 | best_loss=0.02876
Epoch 13/80: current_loss=0.02881 | best_loss=0.02876
Epoch 14/80: current_loss=0.02884 | best_loss=0.02876
Epoch 15/80: current_loss=0.02893 | best_loss=0.02876
Epoch 16/80: current_loss=0.02879 | best_loss=0.02876
Epoch 17/80: current_loss=0.02887 | best_loss=0.02876
Epoch 18/80: current_loss=0.02885 | best_loss=0.02876
Epoch 19/80: current_loss=0.02886 | best_loss=0.02876
Epoch 20/80: current_loss=0.02887 | best_loss=0.02876
Epoch 21/80: current_loss=0.02901 | best_loss=0.02876
Epoch 22/80: current_loss=0.02897 | best_loss=0.02876
Epoch 23/80: current_loss=0.02904 | best_loss=0.02876
Epoch 24/80: current_loss=0.02901 | best_loss=0.02876
Epoch 25/80: current_loss=0.02897 | best_loss=0.02876
Epoch 26/80: current_loss=0.02901 | best_loss=0.02876
Epoch 27/80: current_loss=0.02915 | best_loss=0.02876
Epoch 28/80: current_loss=0.02907 | best_loss=0.02876
Epoch 29/80: current_loss=0.02905 | best_loss=0.02876
Epoch 30/80: current_loss=0.02912 | best_loss=0.02876
Epoch 31/80: current_loss=0.02912 | best_loss=0.02876
Epoch 32/80: current_loss=0.02899 | best_loss=0.02876
Early Stopping at epoch 32
      explained_var=0.00809 | mse_loss=0.02792
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02521 | best_loss=0.02521
Epoch 1/80: current_loss=0.02494 | best_loss=0.02494
Epoch 2/80: current_loss=0.02517 | best_loss=0.02494
Epoch 3/80: current_loss=0.02508 | best_loss=0.02494
Epoch 4/80: current_loss=0.02495 | best_loss=0.02494
Epoch 5/80: current_loss=0.02524 | best_loss=0.02494
Epoch 6/80: current_loss=0.02491 | best_loss=0.02491
Epoch 7/80: current_loss=0.02495 | best_loss=0.02491
Epoch 8/80: current_loss=0.02494 | best_loss=0.02491
Epoch 9/80: current_loss=0.02535 | best_loss=0.02491
Epoch 10/80: current_loss=0.02496 | best_loss=0.02491
Epoch 11/80: current_loss=0.02505 | best_loss=0.02491
Epoch 12/80: current_loss=0.02511 | best_loss=0.02491
Epoch 13/80: current_loss=0.02508 | best_loss=0.02491
Epoch 14/80: current_loss=0.02496 | best_loss=0.02491
Epoch 15/80: current_loss=0.02496 | best_loss=0.02491
Epoch 16/80: current_loss=0.02511 | best_loss=0.02491
Epoch 17/80: current_loss=0.02502 | best_loss=0.02491
Epoch 18/80: current_loss=0.02508 | best_loss=0.02491
Epoch 19/80: current_loss=0.02495 | best_loss=0.02491
Epoch 20/80: current_loss=0.02500 | best_loss=0.02491
Epoch 21/80: current_loss=0.02492 | best_loss=0.02491
Epoch 22/80: current_loss=0.02498 | best_loss=0.02491
Epoch 23/80: current_loss=0.02498 | best_loss=0.02491
Epoch 24/80: current_loss=0.02493 | best_loss=0.02491
Epoch 25/80: current_loss=0.02515 | best_loss=0.02491
Epoch 26/80: current_loss=0.02516 | best_loss=0.02491
Early Stopping at epoch 26
      explained_var=0.00115 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02750 | best_loss=0.02750
Epoch 1/80: current_loss=0.02766 | best_loss=0.02750
Epoch 2/80: current_loss=0.02737 | best_loss=0.02737
Epoch 3/80: current_loss=0.02807 | best_loss=0.02737
Epoch 4/80: current_loss=0.02750 | best_loss=0.02737
Epoch 5/80: current_loss=0.02800 | best_loss=0.02737
Epoch 6/80: current_loss=0.02766 | best_loss=0.02737
Epoch 7/80: current_loss=0.02738 | best_loss=0.02737
Epoch 8/80: current_loss=0.02778 | best_loss=0.02737
Epoch 9/80: current_loss=0.02766 | best_loss=0.02737
Epoch 10/80: current_loss=0.02732 | best_loss=0.02732
Epoch 11/80: current_loss=0.02776 | best_loss=0.02732
Epoch 12/80: current_loss=0.02822 | best_loss=0.02732
Epoch 13/80: current_loss=0.02761 | best_loss=0.02732
Epoch 14/80: current_loss=0.02740 | best_loss=0.02732
Epoch 15/80: current_loss=0.02759 | best_loss=0.02732
Epoch 16/80: current_loss=0.02793 | best_loss=0.02732
Epoch 17/80: current_loss=0.02754 | best_loss=0.02732
Epoch 18/80: current_loss=0.02753 | best_loss=0.02732
Epoch 19/80: current_loss=0.02739 | best_loss=0.02732
Epoch 20/80: current_loss=0.02755 | best_loss=0.02732
Epoch 21/80: current_loss=0.02753 | best_loss=0.02732
Epoch 22/80: current_loss=0.02729 | best_loss=0.02729
Epoch 23/80: current_loss=0.02740 | best_loss=0.02729
Epoch 24/80: current_loss=0.02754 | best_loss=0.02729
Epoch 25/80: current_loss=0.02837 | best_loss=0.02729
Epoch 26/80: current_loss=0.02739 | best_loss=0.02729
Epoch 27/80: current_loss=0.02774 | best_loss=0.02729
Epoch 28/80: current_loss=0.02829 | best_loss=0.02729
Epoch 29/80: current_loss=0.02757 | best_loss=0.02729
Epoch 30/80: current_loss=0.02749 | best_loss=0.02729
Epoch 31/80: current_loss=0.02728 | best_loss=0.02728
Epoch 32/80: current_loss=0.02752 | best_loss=0.02728
Epoch 33/80: current_loss=0.02788 | best_loss=0.02728
Epoch 34/80: current_loss=0.02759 | best_loss=0.02728
Epoch 35/80: current_loss=0.02732 | best_loss=0.02728
Epoch 36/80: current_loss=0.02735 | best_loss=0.02728
Epoch 37/80: current_loss=0.02867 | best_loss=0.02728
Epoch 38/80: current_loss=0.02813 | best_loss=0.02728
Epoch 39/80: current_loss=0.02790 | best_loss=0.02728
Epoch 40/80: current_loss=0.02744 | best_loss=0.02728
Epoch 41/80: current_loss=0.02768 | best_loss=0.02728
Epoch 42/80: current_loss=0.02731 | best_loss=0.02728
Epoch 43/80: current_loss=0.02774 | best_loss=0.02728
Epoch 44/80: current_loss=0.02741 | best_loss=0.02728
Epoch 45/80: current_loss=0.02747 | best_loss=0.02728
Epoch 46/80: current_loss=0.02773 | best_loss=0.02728
Epoch 47/80: current_loss=0.02754 | best_loss=0.02728
Epoch 48/80: current_loss=0.02751 | best_loss=0.02728
Epoch 49/80: current_loss=0.02747 | best_loss=0.02728
Epoch 50/80: current_loss=0.02751 | best_loss=0.02728
Epoch 51/80: current_loss=0.02811 | best_loss=0.02728
Early Stopping at epoch 51
      explained_var=-0.00449 | mse_loss=0.02772
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02685 | best_loss=0.02685
Epoch 1/80: current_loss=0.02686 | best_loss=0.02685
Epoch 2/80: current_loss=0.02710 | best_loss=0.02685
Epoch 3/80: current_loss=0.02698 | best_loss=0.02685
Epoch 4/80: current_loss=0.02689 | best_loss=0.02685
Epoch 5/80: current_loss=0.02680 | best_loss=0.02680
Epoch 6/80: current_loss=0.02711 | best_loss=0.02680
Epoch 7/80: current_loss=0.02681 | best_loss=0.02680
Epoch 8/80: current_loss=0.02680 | best_loss=0.02680
Epoch 9/80: current_loss=0.02708 | best_loss=0.02680
Epoch 10/80: current_loss=0.02693 | best_loss=0.02680
Epoch 11/80: current_loss=0.02684 | best_loss=0.02680
Epoch 12/80: current_loss=0.02682 | best_loss=0.02680
Epoch 13/80: current_loss=0.02688 | best_loss=0.02680
Epoch 14/80: current_loss=0.02695 | best_loss=0.02680
Epoch 15/80: current_loss=0.02677 | best_loss=0.02677
Epoch 16/80: current_loss=0.02678 | best_loss=0.02677
Epoch 17/80: current_loss=0.02678 | best_loss=0.02677
Epoch 18/80: current_loss=0.02679 | best_loss=0.02677
Epoch 19/80: current_loss=0.02705 | best_loss=0.02677
Epoch 20/80: current_loss=0.02698 | best_loss=0.02677
Epoch 21/80: current_loss=0.02686 | best_loss=0.02677
Epoch 22/80: current_loss=0.02689 | best_loss=0.02677
Epoch 23/80: current_loss=0.02733 | best_loss=0.02677
Epoch 24/80: current_loss=0.02689 | best_loss=0.02677
Epoch 25/80: current_loss=0.02699 | best_loss=0.02677
Epoch 26/80: current_loss=0.02692 | best_loss=0.02677
Epoch 27/80: current_loss=0.02754 | best_loss=0.02677
Epoch 28/80: current_loss=0.02684 | best_loss=0.02677
Epoch 29/80: current_loss=0.02683 | best_loss=0.02677
Epoch 30/80: current_loss=0.02684 | best_loss=0.02677
Epoch 31/80: current_loss=0.02688 | best_loss=0.02677
Epoch 32/80: current_loss=0.02721 | best_loss=0.02677
Epoch 33/80: current_loss=0.02681 | best_loss=0.02677
Epoch 34/80: current_loss=0.02683 | best_loss=0.02677
Epoch 35/80: current_loss=0.02735 | best_loss=0.02677
Early Stopping at epoch 35
      explained_var=0.00192 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02993 | best_loss=0.02993
Epoch 1/80: current_loss=0.02973 | best_loss=0.02973
Epoch 2/80: current_loss=0.02973 | best_loss=0.02973
Epoch 3/80: current_loss=0.02974 | best_loss=0.02973
Epoch 4/80: current_loss=0.02973 | best_loss=0.02973
Epoch 5/80: current_loss=0.02987 | best_loss=0.02973
Epoch 6/80: current_loss=0.02972 | best_loss=0.02972
Epoch 7/80: current_loss=0.02970 | best_loss=0.02970
Epoch 8/80: current_loss=0.02971 | best_loss=0.02970
Epoch 9/80: current_loss=0.02972 | best_loss=0.02970
Epoch 10/80: current_loss=0.02979 | best_loss=0.02970
Epoch 11/80: current_loss=0.02976 | best_loss=0.02970
Epoch 12/80: current_loss=0.02974 | best_loss=0.02970
Epoch 13/80: current_loss=0.02970 | best_loss=0.02970
Epoch 14/80: current_loss=0.02993 | best_loss=0.02970
Epoch 15/80: current_loss=0.02973 | best_loss=0.02970
Epoch 16/80: current_loss=0.02973 | best_loss=0.02970
Epoch 17/80: current_loss=0.02979 | best_loss=0.02970
Epoch 18/80: current_loss=0.02978 | best_loss=0.02970
Epoch 19/80: current_loss=0.02973 | best_loss=0.02970
Epoch 20/80: current_loss=0.02984 | best_loss=0.02970
Epoch 21/80: current_loss=0.02971 | best_loss=0.02970
Epoch 22/80: current_loss=0.02974 | best_loss=0.02970
Epoch 23/80: current_loss=0.02987 | best_loss=0.02970
Epoch 24/80: current_loss=0.02975 | best_loss=0.02970
Epoch 25/80: current_loss=0.02972 | best_loss=0.02970
Epoch 26/80: current_loss=0.02986 | best_loss=0.02970
Epoch 27/80: current_loss=0.02984 | best_loss=0.02970
Epoch 28/80: current_loss=0.02974 | best_loss=0.02970
Epoch 29/80: current_loss=0.02975 | best_loss=0.02970
Epoch 30/80: current_loss=0.02971 | best_loss=0.02970
Epoch 31/80: current_loss=0.02976 | best_loss=0.02970
Epoch 32/80: current_loss=0.02972 | best_loss=0.02970
Epoch 33/80: current_loss=0.02968 | best_loss=0.02968
Epoch 34/80: current_loss=0.02981 | best_loss=0.02968
Epoch 35/80: current_loss=0.02972 | best_loss=0.02968
Epoch 36/80: current_loss=0.02969 | best_loss=0.02968
Epoch 37/80: current_loss=0.02966 | best_loss=0.02966
Epoch 38/80: current_loss=0.02970 | best_loss=0.02966
Epoch 39/80: current_loss=0.02971 | best_loss=0.02966
Epoch 40/80: current_loss=0.02976 | best_loss=0.02966
Epoch 41/80: current_loss=0.02970 | best_loss=0.02966
Epoch 42/80: current_loss=0.02969 | best_loss=0.02966
Epoch 43/80: current_loss=0.02971 | best_loss=0.02966
Epoch 44/80: current_loss=0.02975 | best_loss=0.02966
Epoch 45/80: current_loss=0.02971 | best_loss=0.02966
Epoch 46/80: current_loss=0.02974 | best_loss=0.02966
Epoch 47/80: current_loss=0.02977 | best_loss=0.02966
Epoch 48/80: current_loss=0.02974 | best_loss=0.02966
Epoch 49/80: current_loss=0.02982 | best_loss=0.02966
Epoch 50/80: current_loss=0.02978 | best_loss=0.02966
Epoch 51/80: current_loss=0.02975 | best_loss=0.02966
Epoch 52/80: current_loss=0.02975 | best_loss=0.02966
Epoch 53/80: current_loss=0.02979 | best_loss=0.02966
Epoch 54/80: current_loss=0.02973 | best_loss=0.02966
Epoch 55/80: current_loss=0.02972 | best_loss=0.02966
Epoch 56/80: current_loss=0.02973 | best_loss=0.02966
Epoch 57/80: current_loss=0.02979 | best_loss=0.02966
Early Stopping at epoch 57
      explained_var=0.00298 | mse_loss=0.02867
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00193| avg_loss=0.02689
----------------------------------------------


----------------------------------------------
Params for Trial 53
{'learning_rate': 0.0001, 'weight_decay': 0.0010738648151761752, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04013 | best_loss=0.04013
Epoch 1/80: current_loss=0.03489 | best_loss=0.03489
Epoch 2/80: current_loss=0.03354 | best_loss=0.03354
Epoch 3/80: current_loss=0.03238 | best_loss=0.03238
Epoch 4/80: current_loss=0.03141 | best_loss=0.03141
Epoch 5/80: current_loss=0.03070 | best_loss=0.03070
Epoch 6/80: current_loss=0.03047 | best_loss=0.03047
Epoch 7/80: current_loss=0.02990 | best_loss=0.02990
Epoch 8/80: current_loss=0.02980 | best_loss=0.02980
Epoch 9/80: current_loss=0.02947 | best_loss=0.02947
Epoch 10/80: current_loss=0.02930 | best_loss=0.02930
Epoch 11/80: current_loss=0.02926 | best_loss=0.02926
Epoch 12/80: current_loss=0.02930 | best_loss=0.02926
Epoch 13/80: current_loss=0.02921 | best_loss=0.02921
Epoch 14/80: current_loss=0.02906 | best_loss=0.02906
Epoch 15/80: current_loss=0.02898 | best_loss=0.02898
Epoch 16/80: current_loss=0.02893 | best_loss=0.02893
Epoch 17/80: current_loss=0.02892 | best_loss=0.02892
Epoch 18/80: current_loss=0.02897 | best_loss=0.02892
Epoch 19/80: current_loss=0.02916 | best_loss=0.02892
Epoch 20/80: current_loss=0.02903 | best_loss=0.02892
Epoch 21/80: current_loss=0.02897 | best_loss=0.02892
Epoch 22/80: current_loss=0.02904 | best_loss=0.02892
Epoch 23/80: current_loss=0.02892 | best_loss=0.02892
Epoch 24/80: current_loss=0.02897 | best_loss=0.02892
Epoch 25/80: current_loss=0.02901 | best_loss=0.02892
Epoch 26/80: current_loss=0.02903 | best_loss=0.02892
Epoch 27/80: current_loss=0.02897 | best_loss=0.02892
Epoch 28/80: current_loss=0.02894 | best_loss=0.02892
Epoch 29/80: current_loss=0.02897 | best_loss=0.02892
Epoch 30/80: current_loss=0.02894 | best_loss=0.02892
Epoch 31/80: current_loss=0.02913 | best_loss=0.02892
Epoch 32/80: current_loss=0.02910 | best_loss=0.02892
Epoch 33/80: current_loss=0.02902 | best_loss=0.02892
Epoch 34/80: current_loss=0.02906 | best_loss=0.02892
Epoch 35/80: current_loss=0.02902 | best_loss=0.02892
Epoch 36/80: current_loss=0.02900 | best_loss=0.02892
Epoch 37/80: current_loss=0.02900 | best_loss=0.02892
Early Stopping at epoch 37
      explained_var=0.00456 | mse_loss=0.02802
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02530 | best_loss=0.02530
Epoch 1/80: current_loss=0.02513 | best_loss=0.02513
Epoch 2/80: current_loss=0.02505 | best_loss=0.02505
Epoch 3/80: current_loss=0.02506 | best_loss=0.02505
Epoch 4/80: current_loss=0.02502 | best_loss=0.02502
Epoch 5/80: current_loss=0.02502 | best_loss=0.02502
Epoch 6/80: current_loss=0.02499 | best_loss=0.02499
Epoch 7/80: current_loss=0.02511 | best_loss=0.02499
Epoch 8/80: current_loss=0.02500 | best_loss=0.02499
Epoch 9/80: current_loss=0.02493 | best_loss=0.02493
Epoch 10/80: current_loss=0.02493 | best_loss=0.02493
Epoch 11/80: current_loss=0.02490 | best_loss=0.02490
Epoch 12/80: current_loss=0.02496 | best_loss=0.02490
Epoch 13/80: current_loss=0.02502 | best_loss=0.02490
Epoch 14/80: current_loss=0.02500 | best_loss=0.02490
Epoch 15/80: current_loss=0.02494 | best_loss=0.02490
Epoch 16/80: current_loss=0.02491 | best_loss=0.02490
Epoch 17/80: current_loss=0.02505 | best_loss=0.02490
Epoch 18/80: current_loss=0.02499 | best_loss=0.02490
Epoch 19/80: current_loss=0.02501 | best_loss=0.02490
Epoch 20/80: current_loss=0.02495 | best_loss=0.02490
Epoch 21/80: current_loss=0.02518 | best_loss=0.02490
Epoch 22/80: current_loss=0.02523 | best_loss=0.02490
Epoch 23/80: current_loss=0.02500 | best_loss=0.02490
Epoch 24/80: current_loss=0.02514 | best_loss=0.02490
Epoch 25/80: current_loss=0.02505 | best_loss=0.02490
Epoch 26/80: current_loss=0.02500 | best_loss=0.02490
Epoch 27/80: current_loss=0.02509 | best_loss=0.02490
Epoch 28/80: current_loss=0.02513 | best_loss=0.02490
Epoch 29/80: current_loss=0.02520 | best_loss=0.02490
Epoch 30/80: current_loss=0.02503 | best_loss=0.02490
Epoch 31/80: current_loss=0.02505 | best_loss=0.02490
Early Stopping at epoch 31
      explained_var=0.00080 | mse_loss=0.02518

----------------------------------------------
Params for Trial 54
{'learning_rate': 0.0001, 'weight_decay': 0.00038164210526184494, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03278 | best_loss=0.03278
Epoch 1/80: current_loss=0.03131 | best_loss=0.03131
Epoch 2/80: current_loss=0.03057 | best_loss=0.03057
Epoch 3/80: current_loss=0.03005 | best_loss=0.03005
Epoch 4/80: current_loss=0.02979 | best_loss=0.02979
Epoch 5/80: current_loss=0.02959 | best_loss=0.02959
Epoch 6/80: current_loss=0.02938 | best_loss=0.02938
Epoch 7/80: current_loss=0.02924 | best_loss=0.02924
Epoch 8/80: current_loss=0.02930 | best_loss=0.02924
Epoch 9/80: current_loss=0.02914 | best_loss=0.02914
Epoch 10/80: current_loss=0.02911 | best_loss=0.02911
Epoch 11/80: current_loss=0.02913 | best_loss=0.02911
Epoch 12/80: current_loss=0.02909 | best_loss=0.02909
Epoch 13/80: current_loss=0.02932 | best_loss=0.02909
Epoch 14/80: current_loss=0.02905 | best_loss=0.02905
Epoch 15/80: current_loss=0.02910 | best_loss=0.02905
Epoch 16/80: current_loss=0.02944 | best_loss=0.02905
Epoch 17/80: current_loss=0.02939 | best_loss=0.02905
Epoch 18/80: current_loss=0.02907 | best_loss=0.02905
Epoch 19/80: current_loss=0.02908 | best_loss=0.02905
Epoch 20/80: current_loss=0.02912 | best_loss=0.02905
Epoch 21/80: current_loss=0.02902 | best_loss=0.02902
Epoch 22/80: current_loss=0.02908 | best_loss=0.02902
Epoch 23/80: current_loss=0.02904 | best_loss=0.02902
Epoch 24/80: current_loss=0.02909 | best_loss=0.02902
Epoch 25/80: current_loss=0.02908 | best_loss=0.02902
Epoch 26/80: current_loss=0.02905 | best_loss=0.02902
Epoch 27/80: current_loss=0.02906 | best_loss=0.02902
Epoch 28/80: current_loss=0.02904 | best_loss=0.02902
Epoch 29/80: current_loss=0.02906 | best_loss=0.02902
Epoch 30/80: current_loss=0.02916 | best_loss=0.02902
Epoch 31/80: current_loss=0.02906 | best_loss=0.02902
Epoch 32/80: current_loss=0.02925 | best_loss=0.02902
Epoch 33/80: current_loss=0.02900 | best_loss=0.02900
Epoch 34/80: current_loss=0.02909 | best_loss=0.02900
Epoch 35/80: current_loss=0.02903 | best_loss=0.02900
Epoch 36/80: current_loss=0.02906 | best_loss=0.02900
Epoch 37/80: current_loss=0.02925 | best_loss=0.02900
Epoch 38/80: current_loss=0.02914 | best_loss=0.02900
Epoch 39/80: current_loss=0.02917 | best_loss=0.02900
Epoch 40/80: current_loss=0.02907 | best_loss=0.02900
Epoch 41/80: current_loss=0.02911 | best_loss=0.02900
Epoch 42/80: current_loss=0.02928 | best_loss=0.02900
Epoch 43/80: current_loss=0.02904 | best_loss=0.02900
Epoch 44/80: current_loss=0.02916 | best_loss=0.02900
Epoch 45/80: current_loss=0.02903 | best_loss=0.02900
Epoch 46/80: current_loss=0.02908 | best_loss=0.02900
Epoch 47/80: current_loss=0.02904 | best_loss=0.02900
Epoch 48/80: current_loss=0.02901 | best_loss=0.02900
Epoch 49/80: current_loss=0.02905 | best_loss=0.02900
Epoch 50/80: current_loss=0.02938 | best_loss=0.02900
Epoch 51/80: current_loss=0.02915 | best_loss=0.02900
Epoch 52/80: current_loss=0.02923 | best_loss=0.02900
Epoch 53/80: current_loss=0.02915 | best_loss=0.02900
Early Stopping at epoch 53
      explained_var=0.00195 | mse_loss=0.02811

----------------------------------------------
Params for Trial 55
{'learning_rate': 0.0001, 'weight_decay': 0.0008674579414393147, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03381 | best_loss=0.03381
Epoch 1/80: current_loss=0.03259 | best_loss=0.03259
Epoch 2/80: current_loss=0.03175 | best_loss=0.03175
Epoch 3/80: current_loss=0.03122 | best_loss=0.03122
Epoch 4/80: current_loss=0.03054 | best_loss=0.03054
Epoch 5/80: current_loss=0.03019 | best_loss=0.03019
Epoch 6/80: current_loss=0.02983 | best_loss=0.02983
Epoch 7/80: current_loss=0.02973 | best_loss=0.02973
Epoch 8/80: current_loss=0.02941 | best_loss=0.02941
Epoch 9/80: current_loss=0.02941 | best_loss=0.02941
Epoch 10/80: current_loss=0.02919 | best_loss=0.02919
Epoch 11/80: current_loss=0.02923 | best_loss=0.02919
Epoch 12/80: current_loss=0.02918 | best_loss=0.02918
Epoch 13/80: current_loss=0.02915 | best_loss=0.02915
Epoch 14/80: current_loss=0.02909 | best_loss=0.02909
Epoch 15/80: current_loss=0.02903 | best_loss=0.02903
Epoch 16/80: current_loss=0.02906 | best_loss=0.02903
Epoch 17/80: current_loss=0.02906 | best_loss=0.02903
Epoch 18/80: current_loss=0.02912 | best_loss=0.02903
Epoch 19/80: current_loss=0.02912 | best_loss=0.02903
Epoch 20/80: current_loss=0.02901 | best_loss=0.02901
Epoch 21/80: current_loss=0.02900 | best_loss=0.02900
Epoch 22/80: current_loss=0.02914 | best_loss=0.02900
Epoch 23/80: current_loss=0.02892 | best_loss=0.02892
Epoch 24/80: current_loss=0.02896 | best_loss=0.02892
Epoch 25/80: current_loss=0.02917 | best_loss=0.02892
Epoch 26/80: current_loss=0.02888 | best_loss=0.02888
Epoch 27/80: current_loss=0.02887 | best_loss=0.02887
Epoch 28/80: current_loss=0.02904 | best_loss=0.02887
Epoch 29/80: current_loss=0.02888 | best_loss=0.02887
Epoch 30/80: current_loss=0.02916 | best_loss=0.02887
Epoch 31/80: current_loss=0.02891 | best_loss=0.02887
Epoch 32/80: current_loss=0.02886 | best_loss=0.02886
Epoch 33/80: current_loss=0.02908 | best_loss=0.02886
Epoch 34/80: current_loss=0.02888 | best_loss=0.02886
Epoch 35/80: current_loss=0.02893 | best_loss=0.02886
Epoch 36/80: current_loss=0.02895 | best_loss=0.02886
Epoch 37/80: current_loss=0.02911 | best_loss=0.02886
Epoch 38/80: current_loss=0.02906 | best_loss=0.02886
Epoch 39/80: current_loss=0.02896 | best_loss=0.02886
Epoch 40/80: current_loss=0.02898 | best_loss=0.02886
Epoch 41/80: current_loss=0.02914 | best_loss=0.02886
Epoch 42/80: current_loss=0.02900 | best_loss=0.02886
Epoch 43/80: current_loss=0.02903 | best_loss=0.02886
Epoch 44/80: current_loss=0.02901 | best_loss=0.02886
Epoch 45/80: current_loss=0.02893 | best_loss=0.02886
Epoch 46/80: current_loss=0.02895 | best_loss=0.02886
Epoch 47/80: current_loss=0.02906 | best_loss=0.02886
Epoch 48/80: current_loss=0.02899 | best_loss=0.02886
Epoch 49/80: current_loss=0.02904 | best_loss=0.02886
Epoch 50/80: current_loss=0.02914 | best_loss=0.02886
Epoch 51/80: current_loss=0.02906 | best_loss=0.02886
Epoch 52/80: current_loss=0.02899 | best_loss=0.02886
Early Stopping at epoch 52
      explained_var=0.00583 | mse_loss=0.02798
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02512 | best_loss=0.02512
Epoch 1/80: current_loss=0.02479 | best_loss=0.02479
Epoch 2/80: current_loss=0.02489 | best_loss=0.02479
Epoch 3/80: current_loss=0.02484 | best_loss=0.02479
Epoch 4/80: current_loss=0.02499 | best_loss=0.02479
Epoch 5/80: current_loss=0.02487 | best_loss=0.02479
Epoch 6/80: current_loss=0.02485 | best_loss=0.02479
Epoch 7/80: current_loss=0.02495 | best_loss=0.02479
Epoch 8/80: current_loss=0.02509 | best_loss=0.02479
Epoch 9/80: current_loss=0.02510 | best_loss=0.02479
Epoch 10/80: current_loss=0.02494 | best_loss=0.02479
Epoch 11/80: current_loss=0.02504 | best_loss=0.02479
Epoch 12/80: current_loss=0.02518 | best_loss=0.02479
Epoch 13/80: current_loss=0.02511 | best_loss=0.02479
Epoch 14/80: current_loss=0.02519 | best_loss=0.02479
Epoch 15/80: current_loss=0.02500 | best_loss=0.02479
Epoch 16/80: current_loss=0.02518 | best_loss=0.02479
Epoch 17/80: current_loss=0.02493 | best_loss=0.02479
Epoch 18/80: current_loss=0.02491 | best_loss=0.02479
Epoch 19/80: current_loss=0.02511 | best_loss=0.02479
Epoch 20/80: current_loss=0.02523 | best_loss=0.02479
Epoch 21/80: current_loss=0.02509 | best_loss=0.02479
Early Stopping at epoch 21
      explained_var=0.00267 | mse_loss=0.02514
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02794 | best_loss=0.02794
Epoch 1/80: current_loss=0.02792 | best_loss=0.02792
Epoch 2/80: current_loss=0.02807 | best_loss=0.02792
Epoch 3/80: current_loss=0.02774 | best_loss=0.02774
Epoch 4/80: current_loss=0.02773 | best_loss=0.02773
Epoch 5/80: current_loss=0.02748 | best_loss=0.02748
Epoch 6/80: current_loss=0.02834 | best_loss=0.02748
Epoch 7/80: current_loss=0.02750 | best_loss=0.02748
Epoch 8/80: current_loss=0.02772 | best_loss=0.02748
Epoch 9/80: current_loss=0.02773 | best_loss=0.02748
Epoch 10/80: current_loss=0.02764 | best_loss=0.02748
Epoch 11/80: current_loss=0.02812 | best_loss=0.02748
Epoch 12/80: current_loss=0.02745 | best_loss=0.02745
Epoch 13/80: current_loss=0.02761 | best_loss=0.02745
Epoch 14/80: current_loss=0.02740 | best_loss=0.02740
Epoch 15/80: current_loss=0.02771 | best_loss=0.02740
Epoch 16/80: current_loss=0.02856 | best_loss=0.02740
Epoch 17/80: current_loss=0.02756 | best_loss=0.02740
Epoch 18/80: current_loss=0.02782 | best_loss=0.02740
Epoch 19/80: current_loss=0.02761 | best_loss=0.02740
Epoch 20/80: current_loss=0.02751 | best_loss=0.02740
Epoch 21/80: current_loss=0.02783 | best_loss=0.02740
Epoch 22/80: current_loss=0.02746 | best_loss=0.02740
Epoch 23/80: current_loss=0.02742 | best_loss=0.02740
Epoch 24/80: current_loss=0.02757 | best_loss=0.02740
Epoch 25/80: current_loss=0.02822 | best_loss=0.02740
Epoch 26/80: current_loss=0.02774 | best_loss=0.02740
Epoch 27/80: current_loss=0.02750 | best_loss=0.02740
Epoch 28/80: current_loss=0.02749 | best_loss=0.02740
Epoch 29/80: current_loss=0.02768 | best_loss=0.02740
Epoch 30/80: current_loss=0.02775 | best_loss=0.02740
Epoch 31/80: current_loss=0.02744 | best_loss=0.02740
Epoch 32/80: current_loss=0.02826 | best_loss=0.02740
Epoch 33/80: current_loss=0.02743 | best_loss=0.02740
Epoch 34/80: current_loss=0.02759 | best_loss=0.02740
Early Stopping at epoch 34
      explained_var=-0.00515 | mse_loss=0.02786
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02695 | best_loss=0.02695
Epoch 1/80: current_loss=0.02684 | best_loss=0.02684
Epoch 2/80: current_loss=0.02684 | best_loss=0.02684
Epoch 3/80: current_loss=0.02695 | best_loss=0.02684
Epoch 4/80: current_loss=0.02688 | best_loss=0.02684
Epoch 5/80: current_loss=0.02681 | best_loss=0.02681
Epoch 6/80: current_loss=0.02689 | best_loss=0.02681
Epoch 7/80: current_loss=0.02688 | best_loss=0.02681
Epoch 8/80: current_loss=0.02682 | best_loss=0.02681
Epoch 9/80: current_loss=0.02684 | best_loss=0.02681
Epoch 10/80: current_loss=0.02676 | best_loss=0.02676
Epoch 11/80: current_loss=0.02719 | best_loss=0.02676
Epoch 12/80: current_loss=0.02678 | best_loss=0.02676
Epoch 13/80: current_loss=0.02681 | best_loss=0.02676
Epoch 14/80: current_loss=0.02680 | best_loss=0.02676
Epoch 15/80: current_loss=0.02681 | best_loss=0.02676
Epoch 16/80: current_loss=0.02678 | best_loss=0.02676
Epoch 17/80: current_loss=0.02687 | best_loss=0.02676
Epoch 18/80: current_loss=0.02697 | best_loss=0.02676
Epoch 19/80: current_loss=0.02693 | best_loss=0.02676
Epoch 20/80: current_loss=0.02678 | best_loss=0.02676
Epoch 21/80: current_loss=0.02680 | best_loss=0.02676
Epoch 22/80: current_loss=0.02681 | best_loss=0.02676
Epoch 23/80: current_loss=0.02684 | best_loss=0.02676
Epoch 24/80: current_loss=0.02679 | best_loss=0.02676
Epoch 25/80: current_loss=0.02678 | best_loss=0.02676
Epoch 26/80: current_loss=0.02677 | best_loss=0.02676
Epoch 27/80: current_loss=0.02682 | best_loss=0.02676
Epoch 28/80: current_loss=0.02682 | best_loss=0.02676
Epoch 29/80: current_loss=0.02680 | best_loss=0.02676
Epoch 30/80: current_loss=0.02681 | best_loss=0.02676
Early Stopping at epoch 30
      explained_var=0.00204 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02977 | best_loss=0.02977
Epoch 1/80: current_loss=0.02982 | best_loss=0.02977
Epoch 2/80: current_loss=0.02980 | best_loss=0.02977
Epoch 3/80: current_loss=0.02983 | best_loss=0.02977
Epoch 4/80: current_loss=0.02983 | best_loss=0.02977
Epoch 5/80: current_loss=0.02976 | best_loss=0.02976
Epoch 6/80: current_loss=0.02978 | best_loss=0.02976
Epoch 7/80: current_loss=0.02977 | best_loss=0.02976
Epoch 8/80: current_loss=0.02991 | best_loss=0.02976
Epoch 9/80: current_loss=0.02984 | best_loss=0.02976
Epoch 10/80: current_loss=0.02998 | best_loss=0.02976
Epoch 11/80: current_loss=0.02974 | best_loss=0.02974
Epoch 12/80: current_loss=0.02995 | best_loss=0.02974
Epoch 13/80: current_loss=0.02971 | best_loss=0.02971
Epoch 14/80: current_loss=0.02991 | best_loss=0.02971
Epoch 15/80: current_loss=0.02971 | best_loss=0.02971
Epoch 16/80: current_loss=0.02972 | best_loss=0.02971
Epoch 17/80: current_loss=0.02984 | best_loss=0.02971
Epoch 18/80: current_loss=0.02972 | best_loss=0.02971
Epoch 19/80: current_loss=0.02974 | best_loss=0.02971
Epoch 20/80: current_loss=0.02974 | best_loss=0.02971
Epoch 21/80: current_loss=0.02980 | best_loss=0.02971
Epoch 22/80: current_loss=0.02974 | best_loss=0.02971
Epoch 23/80: current_loss=0.02974 | best_loss=0.02971
Epoch 24/80: current_loss=0.02987 | best_loss=0.02971
Epoch 25/80: current_loss=0.02984 | best_loss=0.02971
Epoch 26/80: current_loss=0.02975 | best_loss=0.02971
Epoch 27/80: current_loss=0.02996 | best_loss=0.02971
Epoch 28/80: current_loss=0.02982 | best_loss=0.02971
Epoch 29/80: current_loss=0.02971 | best_loss=0.02971
Epoch 30/80: current_loss=0.02971 | best_loss=0.02971
Epoch 31/80: current_loss=0.02974 | best_loss=0.02971
Epoch 32/80: current_loss=0.02969 | best_loss=0.02969
Epoch 33/80: current_loss=0.02994 | best_loss=0.02969
Epoch 34/80: current_loss=0.02976 | best_loss=0.02969
Epoch 35/80: current_loss=0.02977 | best_loss=0.02969
Epoch 36/80: current_loss=0.02985 | best_loss=0.02969
Epoch 37/80: current_loss=0.02972 | best_loss=0.02969
Epoch 38/80: current_loss=0.02976 | best_loss=0.02969
Epoch 39/80: current_loss=0.02986 | best_loss=0.02969
Epoch 40/80: current_loss=0.02976 | best_loss=0.02969
Epoch 41/80: current_loss=0.02978 | best_loss=0.02969
Epoch 42/80: current_loss=0.02983 | best_loss=0.02969
Epoch 43/80: current_loss=0.02976 | best_loss=0.02969
Epoch 44/80: current_loss=0.02981 | best_loss=0.02969
Epoch 45/80: current_loss=0.02978 | best_loss=0.02969
Epoch 46/80: current_loss=0.02972 | best_loss=0.02969
Epoch 47/80: current_loss=0.02971 | best_loss=0.02969
Epoch 48/80: current_loss=0.02983 | best_loss=0.02969
Epoch 49/80: current_loss=0.02980 | best_loss=0.02969
Epoch 50/80: current_loss=0.02971 | best_loss=0.02969
Epoch 51/80: current_loss=0.02975 | best_loss=0.02969
Epoch 52/80: current_loss=0.02977 | best_loss=0.02969
Early Stopping at epoch 52
      explained_var=0.00207 | mse_loss=0.02870
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00149| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 56
{'learning_rate': 0.0001, 'weight_decay': 0.0002640558338276515, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03908 | best_loss=0.03908
Epoch 1/80: current_loss=0.03667 | best_loss=0.03667
Epoch 2/80: current_loss=0.03551 | best_loss=0.03551
Epoch 3/80: current_loss=0.03417 | best_loss=0.03417
Epoch 4/80: current_loss=0.03301 | best_loss=0.03301
Epoch 5/80: current_loss=0.03219 | best_loss=0.03219
Epoch 6/80: current_loss=0.03233 | best_loss=0.03219
Epoch 7/80: current_loss=0.03114 | best_loss=0.03114
Epoch 8/80: current_loss=0.03106 | best_loss=0.03106
Epoch 9/80: current_loss=0.03049 | best_loss=0.03049
Epoch 10/80: current_loss=0.03046 | best_loss=0.03046
Epoch 11/80: current_loss=0.03021 | best_loss=0.03021
Epoch 12/80: current_loss=0.03031 | best_loss=0.03021
Epoch 13/80: current_loss=0.03013 | best_loss=0.03013
Epoch 14/80: current_loss=0.02996 | best_loss=0.02996
Epoch 15/80: current_loss=0.02973 | best_loss=0.02973
Epoch 16/80: current_loss=0.02962 | best_loss=0.02962
Epoch 17/80: current_loss=0.02963 | best_loss=0.02962
Epoch 18/80: current_loss=0.02946 | best_loss=0.02946
Epoch 19/80: current_loss=0.02978 | best_loss=0.02946
Epoch 20/80: current_loss=0.02967 | best_loss=0.02946
Epoch 21/80: current_loss=0.02941 | best_loss=0.02941
Epoch 22/80: current_loss=0.02959 | best_loss=0.02941
Epoch 23/80: current_loss=0.02924 | best_loss=0.02924
Epoch 24/80: current_loss=0.02971 | best_loss=0.02924
Epoch 25/80: current_loss=0.02926 | best_loss=0.02924
Epoch 26/80: current_loss=0.02922 | best_loss=0.02922
Epoch 27/80: current_loss=0.02908 | best_loss=0.02908
Epoch 28/80: current_loss=0.02921 | best_loss=0.02908
Epoch 29/80: current_loss=0.02909 | best_loss=0.02908
Epoch 30/80: current_loss=0.02912 | best_loss=0.02908
Epoch 31/80: current_loss=0.02912 | best_loss=0.02908
Epoch 32/80: current_loss=0.02911 | best_loss=0.02908
Epoch 33/80: current_loss=0.02921 | best_loss=0.02908
Epoch 34/80: current_loss=0.02911 | best_loss=0.02908
Epoch 35/80: current_loss=0.02929 | best_loss=0.02908
Epoch 36/80: current_loss=0.02902 | best_loss=0.02902
Epoch 37/80: current_loss=0.02906 | best_loss=0.02902
Epoch 38/80: current_loss=0.02905 | best_loss=0.02902
Epoch 39/80: current_loss=0.02919 | best_loss=0.02902
Epoch 40/80: current_loss=0.02897 | best_loss=0.02897
Epoch 41/80: current_loss=0.02892 | best_loss=0.02892
Epoch 42/80: current_loss=0.02896 | best_loss=0.02892
Epoch 43/80: current_loss=0.02897 | best_loss=0.02892
Epoch 44/80: current_loss=0.02900 | best_loss=0.02892
Epoch 45/80: current_loss=0.02933 | best_loss=0.02892
Epoch 46/80: current_loss=0.02903 | best_loss=0.02892
Epoch 47/80: current_loss=0.02899 | best_loss=0.02892
Epoch 48/80: current_loss=0.02903 | best_loss=0.02892
Epoch 49/80: current_loss=0.02895 | best_loss=0.02892
Epoch 50/80: current_loss=0.02891 | best_loss=0.02891
Epoch 51/80: current_loss=0.02889 | best_loss=0.02889
Epoch 52/80: current_loss=0.02883 | best_loss=0.02883
Epoch 53/80: current_loss=0.02897 | best_loss=0.02883
Epoch 54/80: current_loss=0.02902 | best_loss=0.02883
Epoch 55/80: current_loss=0.02899 | best_loss=0.02883
Epoch 56/80: current_loss=0.02907 | best_loss=0.02883
Epoch 57/80: current_loss=0.02905 | best_loss=0.02883
Epoch 58/80: current_loss=0.02908 | best_loss=0.02883
Epoch 59/80: current_loss=0.02920 | best_loss=0.02883
Epoch 60/80: current_loss=0.02893 | best_loss=0.02883
Epoch 61/80: current_loss=0.02910 | best_loss=0.02883
Epoch 62/80: current_loss=0.02889 | best_loss=0.02883
Epoch 63/80: current_loss=0.02897 | best_loss=0.02883
Epoch 64/80: current_loss=0.02897 | best_loss=0.02883
Epoch 65/80: current_loss=0.02897 | best_loss=0.02883
Epoch 66/80: current_loss=0.02921 | best_loss=0.02883
Epoch 67/80: current_loss=0.02904 | best_loss=0.02883
Epoch 68/80: current_loss=0.02905 | best_loss=0.02883
Epoch 69/80: current_loss=0.02916 | best_loss=0.02883
Epoch 70/80: current_loss=0.02896 | best_loss=0.02883
Epoch 71/80: current_loss=0.02896 | best_loss=0.02883
Epoch 72/80: current_loss=0.02896 | best_loss=0.02883
Early Stopping at epoch 72
      explained_var=0.00672 | mse_loss=0.02800
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02509 | best_loss=0.02509
Epoch 1/80: current_loss=0.02500 | best_loss=0.02500
Epoch 2/80: current_loss=0.02540 | best_loss=0.02500
Epoch 3/80: current_loss=0.02490 | best_loss=0.02490
Epoch 4/80: current_loss=0.02504 | best_loss=0.02490
Epoch 5/80: current_loss=0.02541 | best_loss=0.02490
Epoch 6/80: current_loss=0.02506 | best_loss=0.02490
Epoch 7/80: current_loss=0.02493 | best_loss=0.02490
Epoch 8/80: current_loss=0.02514 | best_loss=0.02490
Epoch 9/80: current_loss=0.02493 | best_loss=0.02490
Epoch 10/80: current_loss=0.02500 | best_loss=0.02490
Epoch 11/80: current_loss=0.02533 | best_loss=0.02490
Epoch 12/80: current_loss=0.02505 | best_loss=0.02490
Epoch 13/80: current_loss=0.02525 | best_loss=0.02490
Epoch 14/80: current_loss=0.02527 | best_loss=0.02490
Epoch 15/80: current_loss=0.02499 | best_loss=0.02490
Epoch 16/80: current_loss=0.02494 | best_loss=0.02490
Epoch 17/80: current_loss=0.02496 | best_loss=0.02490
Epoch 18/80: current_loss=0.02503 | best_loss=0.02490
Epoch 19/80: current_loss=0.02510 | best_loss=0.02490
Epoch 20/80: current_loss=0.02513 | best_loss=0.02490
Epoch 21/80: current_loss=0.02507 | best_loss=0.02490
Epoch 22/80: current_loss=0.02498 | best_loss=0.02490
Epoch 23/80: current_loss=0.02511 | best_loss=0.02490
Early Stopping at epoch 23
      explained_var=-0.00120 | mse_loss=0.02525

----------------------------------------------
Params for Trial 57
{'learning_rate': 0.0001, 'weight_decay': 0.0009306068596426204, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03202 | best_loss=0.03202
Epoch 1/80: current_loss=0.03045 | best_loss=0.03045
Epoch 2/80: current_loss=0.03008 | best_loss=0.03008
Epoch 3/80: current_loss=0.02915 | best_loss=0.02915
Epoch 4/80: current_loss=0.02900 | best_loss=0.02900
Epoch 5/80: current_loss=0.02888 | best_loss=0.02888
Epoch 6/80: current_loss=0.02896 | best_loss=0.02888
Epoch 7/80: current_loss=0.02935 | best_loss=0.02888
Epoch 8/80: current_loss=0.02889 | best_loss=0.02888
Epoch 9/80: current_loss=0.02889 | best_loss=0.02888
Epoch 10/80: current_loss=0.02923 | best_loss=0.02888
Epoch 11/80: current_loss=0.02893 | best_loss=0.02888
Epoch 12/80: current_loss=0.02919 | best_loss=0.02888
Epoch 13/80: current_loss=0.02887 | best_loss=0.02887
Epoch 14/80: current_loss=0.02916 | best_loss=0.02887
Epoch 15/80: current_loss=0.02916 | best_loss=0.02887
Epoch 16/80: current_loss=0.02897 | best_loss=0.02887
Epoch 17/80: current_loss=0.02897 | best_loss=0.02887
Epoch 18/80: current_loss=0.02912 | best_loss=0.02887
Epoch 19/80: current_loss=0.02910 | best_loss=0.02887
Epoch 20/80: current_loss=0.02914 | best_loss=0.02887
Epoch 21/80: current_loss=0.02940 | best_loss=0.02887
Epoch 22/80: current_loss=0.02931 | best_loss=0.02887
Epoch 23/80: current_loss=0.02917 | best_loss=0.02887
Epoch 24/80: current_loss=0.02913 | best_loss=0.02887
Epoch 25/80: current_loss=0.02952 | best_loss=0.02887
Epoch 26/80: current_loss=0.02904 | best_loss=0.02887
Epoch 27/80: current_loss=0.02905 | best_loss=0.02887
Epoch 28/80: current_loss=0.02900 | best_loss=0.02887
Epoch 29/80: current_loss=0.02899 | best_loss=0.02887
Epoch 30/80: current_loss=0.02920 | best_loss=0.02887
Epoch 31/80: current_loss=0.02893 | best_loss=0.02887
Epoch 32/80: current_loss=0.02890 | best_loss=0.02887
Epoch 33/80: current_loss=0.02909 | best_loss=0.02887
Early Stopping at epoch 33
      explained_var=0.00545 | mse_loss=0.02800
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02492 | best_loss=0.02492
Epoch 1/80: current_loss=0.02494 | best_loss=0.02492
Epoch 2/80: current_loss=0.02508 | best_loss=0.02492
Epoch 3/80: current_loss=0.02494 | best_loss=0.02492
Epoch 4/80: current_loss=0.02489 | best_loss=0.02489
Epoch 5/80: current_loss=0.02503 | best_loss=0.02489
Epoch 6/80: current_loss=0.02491 | best_loss=0.02489
Epoch 7/80: current_loss=0.02490 | best_loss=0.02489
Epoch 8/80: current_loss=0.02490 | best_loss=0.02489
Epoch 9/80: current_loss=0.02505 | best_loss=0.02489
Epoch 10/80: current_loss=0.02581 | best_loss=0.02489
Epoch 11/80: current_loss=0.02527 | best_loss=0.02489
Epoch 12/80: current_loss=0.02496 | best_loss=0.02489
Epoch 13/80: current_loss=0.02493 | best_loss=0.02489
Epoch 14/80: current_loss=0.02499 | best_loss=0.02489
Epoch 15/80: current_loss=0.02495 | best_loss=0.02489
Epoch 16/80: current_loss=0.02491 | best_loss=0.02489
Epoch 17/80: current_loss=0.02490 | best_loss=0.02489
Epoch 18/80: current_loss=0.02489 | best_loss=0.02489
Epoch 19/80: current_loss=0.02504 | best_loss=0.02489
Epoch 20/80: current_loss=0.02499 | best_loss=0.02489
Epoch 21/80: current_loss=0.02490 | best_loss=0.02489
Epoch 22/80: current_loss=0.02495 | best_loss=0.02489
Epoch 23/80: current_loss=0.02574 | best_loss=0.02489
Epoch 24/80: current_loss=0.02493 | best_loss=0.02489
Epoch 25/80: current_loss=0.02493 | best_loss=0.02489
Epoch 26/80: current_loss=0.02507 | best_loss=0.02489
Epoch 27/80: current_loss=0.02513 | best_loss=0.02489
Epoch 28/80: current_loss=0.02494 | best_loss=0.02489
Epoch 29/80: current_loss=0.02502 | best_loss=0.02489
Epoch 30/80: current_loss=0.02537 | best_loss=0.02489
Epoch 31/80: current_loss=0.02493 | best_loss=0.02489
Epoch 32/80: current_loss=0.02508 | best_loss=0.02489
Epoch 33/80: current_loss=0.02492 | best_loss=0.02489
Epoch 34/80: current_loss=0.02489 | best_loss=0.02489
Epoch 35/80: current_loss=0.02491 | best_loss=0.02489
Epoch 36/80: current_loss=0.02502 | best_loss=0.02489
Epoch 37/80: current_loss=0.02494 | best_loss=0.02489
Epoch 38/80: current_loss=0.02490 | best_loss=0.02489
Early Stopping at epoch 38
      explained_var=0.00116 | mse_loss=0.02519

----------------------------------------------
Params for Trial 58
{'learning_rate': 0.0001, 'weight_decay': 0.0016787017505985827, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03174 | best_loss=0.03174
Epoch 1/80: current_loss=0.03076 | best_loss=0.03076
Epoch 2/80: current_loss=0.03003 | best_loss=0.03003
Epoch 3/80: current_loss=0.02964 | best_loss=0.02964
Epoch 4/80: current_loss=0.02932 | best_loss=0.02932
Epoch 5/80: current_loss=0.02912 | best_loss=0.02912
Epoch 6/80: current_loss=0.02894 | best_loss=0.02894
Epoch 7/80: current_loss=0.02888 | best_loss=0.02888
Epoch 8/80: current_loss=0.02913 | best_loss=0.02888
Epoch 9/80: current_loss=0.02888 | best_loss=0.02888
Epoch 10/80: current_loss=0.02888 | best_loss=0.02888
Epoch 11/80: current_loss=0.02889 | best_loss=0.02888
Epoch 12/80: current_loss=0.02892 | best_loss=0.02888
Epoch 13/80: current_loss=0.02893 | best_loss=0.02888
Epoch 14/80: current_loss=0.02890 | best_loss=0.02888
Epoch 15/80: current_loss=0.02888 | best_loss=0.02888
Epoch 16/80: current_loss=0.02900 | best_loss=0.02888
Epoch 17/80: current_loss=0.02892 | best_loss=0.02888
Epoch 18/80: current_loss=0.02896 | best_loss=0.02888
Epoch 19/80: current_loss=0.02962 | best_loss=0.02888
Epoch 20/80: current_loss=0.02899 | best_loss=0.02888
Epoch 21/80: current_loss=0.02900 | best_loss=0.02888
Epoch 22/80: current_loss=0.02911 | best_loss=0.02888
Epoch 23/80: current_loss=0.02897 | best_loss=0.02888
Epoch 24/80: current_loss=0.02902 | best_loss=0.02888
Epoch 25/80: current_loss=0.02902 | best_loss=0.02888
Epoch 26/80: current_loss=0.02902 | best_loss=0.02888
Epoch 27/80: current_loss=0.02899 | best_loss=0.02888
Epoch 28/80: current_loss=0.02900 | best_loss=0.02888
Epoch 29/80: current_loss=0.02900 | best_loss=0.02888
Early Stopping at epoch 29
      explained_var=0.00580 | mse_loss=0.02798
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02509 | best_loss=0.02509
Epoch 1/80: current_loss=0.02487 | best_loss=0.02487
Epoch 2/80: current_loss=0.02508 | best_loss=0.02487
Epoch 3/80: current_loss=0.02514 | best_loss=0.02487
Epoch 4/80: current_loss=0.02491 | best_loss=0.02487
Epoch 5/80: current_loss=0.02490 | best_loss=0.02487
Epoch 6/80: current_loss=0.02484 | best_loss=0.02484
Epoch 7/80: current_loss=0.02502 | best_loss=0.02484
Epoch 8/80: current_loss=0.02500 | best_loss=0.02484
Epoch 9/80: current_loss=0.02501 | best_loss=0.02484
Epoch 10/80: current_loss=0.02485 | best_loss=0.02484
Epoch 11/80: current_loss=0.02513 | best_loss=0.02484
Epoch 12/80: current_loss=0.02487 | best_loss=0.02484
Epoch 13/80: current_loss=0.02488 | best_loss=0.02484
Epoch 14/80: current_loss=0.02487 | best_loss=0.02484
Epoch 15/80: current_loss=0.02515 | best_loss=0.02484
Epoch 16/80: current_loss=0.02489 | best_loss=0.02484
Epoch 17/80: current_loss=0.02488 | best_loss=0.02484
Epoch 18/80: current_loss=0.02494 | best_loss=0.02484
Epoch 19/80: current_loss=0.02547 | best_loss=0.02484
Epoch 20/80: current_loss=0.02552 | best_loss=0.02484
Epoch 21/80: current_loss=0.02491 | best_loss=0.02484
Epoch 22/80: current_loss=0.02581 | best_loss=0.02484
Epoch 23/80: current_loss=0.02503 | best_loss=0.02484
Epoch 24/80: current_loss=0.02494 | best_loss=0.02484
Epoch 25/80: current_loss=0.02493 | best_loss=0.02484
Epoch 26/80: current_loss=0.02488 | best_loss=0.02484
Early Stopping at epoch 26
      explained_var=0.00255 | mse_loss=0.02514
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02837 | best_loss=0.02837
Epoch 1/80: current_loss=0.02785 | best_loss=0.02785
Epoch 2/80: current_loss=0.02750 | best_loss=0.02750
Epoch 3/80: current_loss=0.02827 | best_loss=0.02750
Epoch 4/80: current_loss=0.02804 | best_loss=0.02750
Epoch 5/80: current_loss=0.02861 | best_loss=0.02750
Epoch 6/80: current_loss=0.02799 | best_loss=0.02750
Epoch 7/80: current_loss=0.02757 | best_loss=0.02750
Epoch 8/80: current_loss=0.02753 | best_loss=0.02750
Epoch 9/80: current_loss=0.02792 | best_loss=0.02750
Epoch 10/80: current_loss=0.02754 | best_loss=0.02750
Epoch 11/80: current_loss=0.02778 | best_loss=0.02750
Epoch 12/80: current_loss=0.02843 | best_loss=0.02750
Epoch 13/80: current_loss=0.02764 | best_loss=0.02750
Epoch 14/80: current_loss=0.02750 | best_loss=0.02750
Epoch 15/80: current_loss=0.02793 | best_loss=0.02750
Epoch 16/80: current_loss=0.02744 | best_loss=0.02744
Epoch 17/80: current_loss=0.02826 | best_loss=0.02744
Epoch 18/80: current_loss=0.02745 | best_loss=0.02744
Epoch 19/80: current_loss=0.02784 | best_loss=0.02744
Epoch 20/80: current_loss=0.02781 | best_loss=0.02744
Epoch 21/80: current_loss=0.02857 | best_loss=0.02744
Epoch 22/80: current_loss=0.02742 | best_loss=0.02742
Epoch 23/80: current_loss=0.02778 | best_loss=0.02742
Epoch 24/80: current_loss=0.02812 | best_loss=0.02742
Epoch 25/80: current_loss=0.02799 | best_loss=0.02742
Epoch 26/80: current_loss=0.02897 | best_loss=0.02742
Epoch 27/80: current_loss=0.02841 | best_loss=0.02742
Epoch 28/80: current_loss=0.02771 | best_loss=0.02742
Epoch 29/80: current_loss=0.02797 | best_loss=0.02742
Epoch 30/80: current_loss=0.02821 | best_loss=0.02742
Epoch 31/80: current_loss=0.02804 | best_loss=0.02742
Epoch 32/80: current_loss=0.02808 | best_loss=0.02742
Epoch 33/80: current_loss=0.02769 | best_loss=0.02742
Epoch 34/80: current_loss=0.02794 | best_loss=0.02742
Epoch 35/80: current_loss=0.02867 | best_loss=0.02742
Epoch 36/80: current_loss=0.02779 | best_loss=0.02742
Epoch 37/80: current_loss=0.02771 | best_loss=0.02742
Epoch 38/80: current_loss=0.02786 | best_loss=0.02742
Epoch 39/80: current_loss=0.02794 | best_loss=0.02742
Epoch 40/80: current_loss=0.02745 | best_loss=0.02742
Epoch 41/80: current_loss=0.02753 | best_loss=0.02742
Epoch 42/80: current_loss=0.02763 | best_loss=0.02742
Early Stopping at epoch 42
      explained_var=-0.01335 | mse_loss=0.02794
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02675 | best_loss=0.02675
Epoch 1/80: current_loss=0.02674 | best_loss=0.02674
Epoch 2/80: current_loss=0.02678 | best_loss=0.02674
Epoch 3/80: current_loss=0.02708 | best_loss=0.02674
Epoch 4/80: current_loss=0.02728 | best_loss=0.02674
Epoch 5/80: current_loss=0.02712 | best_loss=0.02674
Epoch 6/80: current_loss=0.02686 | best_loss=0.02674
Epoch 7/80: current_loss=0.02697 | best_loss=0.02674
Epoch 8/80: current_loss=0.02671 | best_loss=0.02671
Epoch 9/80: current_loss=0.02753 | best_loss=0.02671
Epoch 10/80: current_loss=0.02670 | best_loss=0.02670
Epoch 11/80: current_loss=0.02671 | best_loss=0.02670
Epoch 12/80: current_loss=0.02683 | best_loss=0.02670
Epoch 13/80: current_loss=0.02707 | best_loss=0.02670
Epoch 14/80: current_loss=0.02681 | best_loss=0.02670
Epoch 15/80: current_loss=0.02689 | best_loss=0.02670
Epoch 16/80: current_loss=0.02686 | best_loss=0.02670
Epoch 17/80: current_loss=0.02678 | best_loss=0.02670
Epoch 18/80: current_loss=0.02676 | best_loss=0.02670
Epoch 19/80: current_loss=0.02673 | best_loss=0.02670
Epoch 20/80: current_loss=0.02672 | best_loss=0.02670
Epoch 21/80: current_loss=0.02693 | best_loss=0.02670
Epoch 22/80: current_loss=0.02682 | best_loss=0.02670
Epoch 23/80: current_loss=0.02684 | best_loss=0.02670
Epoch 24/80: current_loss=0.02677 | best_loss=0.02670
Epoch 25/80: current_loss=0.02727 | best_loss=0.02670
Epoch 26/80: current_loss=0.02676 | best_loss=0.02670
Epoch 27/80: current_loss=0.02685 | best_loss=0.02670
Epoch 28/80: current_loss=0.02685 | best_loss=0.02670
Epoch 29/80: current_loss=0.02690 | best_loss=0.02670
Epoch 30/80: current_loss=0.02682 | best_loss=0.02670
Early Stopping at epoch 30
      explained_var=0.00211 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02980 | best_loss=0.02980
Epoch 1/80: current_loss=0.02973 | best_loss=0.02973
Epoch 2/80: current_loss=0.02995 | best_loss=0.02973
Epoch 3/80: current_loss=0.02975 | best_loss=0.02973
Epoch 4/80: current_loss=0.02971 | best_loss=0.02971
Epoch 5/80: current_loss=0.02976 | best_loss=0.02971
Epoch 6/80: current_loss=0.02974 | best_loss=0.02971
Epoch 7/80: current_loss=0.02971 | best_loss=0.02971
Epoch 8/80: current_loss=0.03010 | best_loss=0.02971
Epoch 9/80: current_loss=0.03015 | best_loss=0.02971
Epoch 10/80: current_loss=0.02975 | best_loss=0.02971
Epoch 11/80: current_loss=0.02989 | best_loss=0.02971
Epoch 12/80: current_loss=0.02978 | best_loss=0.02971
Epoch 13/80: current_loss=0.02971 | best_loss=0.02971
Epoch 14/80: current_loss=0.02972 | best_loss=0.02971
Epoch 15/80: current_loss=0.02982 | best_loss=0.02971
Epoch 16/80: current_loss=0.02972 | best_loss=0.02971
Epoch 17/80: current_loss=0.03000 | best_loss=0.02971
Epoch 18/80: current_loss=0.02985 | best_loss=0.02971
Epoch 19/80: current_loss=0.02973 | best_loss=0.02971
Epoch 20/80: current_loss=0.02972 | best_loss=0.02971
Epoch 21/80: current_loss=0.02972 | best_loss=0.02971
Epoch 22/80: current_loss=0.03006 | best_loss=0.02971
Epoch 23/80: current_loss=0.02997 | best_loss=0.02971
Epoch 24/80: current_loss=0.02973 | best_loss=0.02971
Epoch 25/80: current_loss=0.02973 | best_loss=0.02971
Epoch 26/80: current_loss=0.02989 | best_loss=0.02971
Epoch 27/80: current_loss=0.03002 | best_loss=0.02971
Epoch 28/80: current_loss=0.02977 | best_loss=0.02971
Epoch 29/80: current_loss=0.02975 | best_loss=0.02971
Epoch 30/80: current_loss=0.02976 | best_loss=0.02971
Epoch 31/80: current_loss=0.02974 | best_loss=0.02971
Epoch 32/80: current_loss=0.02972 | best_loss=0.02971
Epoch 33/80: current_loss=0.02974 | best_loss=0.02971
Early Stopping at epoch 33
      explained_var=0.00142 | mse_loss=0.02872
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=-0.00029| avg_loss=0.02694
----------------------------------------------


----------------------------------------------
Params for Trial 59
{'learning_rate': 0.0001, 'weight_decay': 0.0012495456925824195, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03643 | best_loss=0.03643
Epoch 1/80: current_loss=0.03460 | best_loss=0.03460
Epoch 2/80: current_loss=0.03317 | best_loss=0.03317
Epoch 3/80: current_loss=0.03225 | best_loss=0.03225
Epoch 4/80: current_loss=0.03174 | best_loss=0.03174
Epoch 5/80: current_loss=0.03105 | best_loss=0.03105
Epoch 6/80: current_loss=0.03060 | best_loss=0.03060
Epoch 7/80: current_loss=0.03038 | best_loss=0.03038
Epoch 8/80: current_loss=0.03015 | best_loss=0.03015
Epoch 9/80: current_loss=0.02987 | best_loss=0.02987
Epoch 10/80: current_loss=0.02966 | best_loss=0.02966
Epoch 11/80: current_loss=0.02957 | best_loss=0.02957
Epoch 12/80: current_loss=0.02952 | best_loss=0.02952
Epoch 13/80: current_loss=0.02950 | best_loss=0.02950
Epoch 14/80: current_loss=0.02943 | best_loss=0.02943
Epoch 15/80: current_loss=0.02945 | best_loss=0.02943
Epoch 16/80: current_loss=0.02933 | best_loss=0.02933
Epoch 17/80: current_loss=0.02927 | best_loss=0.02927
Epoch 18/80: current_loss=0.02924 | best_loss=0.02924
Epoch 19/80: current_loss=0.02926 | best_loss=0.02924
Epoch 20/80: current_loss=0.02914 | best_loss=0.02914
Epoch 21/80: current_loss=0.02914 | best_loss=0.02914
Epoch 22/80: current_loss=0.02912 | best_loss=0.02912
Epoch 23/80: current_loss=0.02915 | best_loss=0.02912
Epoch 24/80: current_loss=0.02906 | best_loss=0.02906
Epoch 25/80: current_loss=0.02898 | best_loss=0.02898
Epoch 26/80: current_loss=0.02902 | best_loss=0.02898
Epoch 27/80: current_loss=0.02899 | best_loss=0.02898
Epoch 28/80: current_loss=0.02928 | best_loss=0.02898
Epoch 29/80: current_loss=0.02904 | best_loss=0.02898
Epoch 30/80: current_loss=0.02923 | best_loss=0.02898
Epoch 31/80: current_loss=0.02919 | best_loss=0.02898
Epoch 32/80: current_loss=0.02902 | best_loss=0.02898
Epoch 33/80: current_loss=0.02916 | best_loss=0.02898
Epoch 34/80: current_loss=0.02897 | best_loss=0.02897
Epoch 35/80: current_loss=0.02903 | best_loss=0.02897
Epoch 36/80: current_loss=0.02909 | best_loss=0.02897
Epoch 37/80: current_loss=0.02894 | best_loss=0.02894
Epoch 38/80: current_loss=0.02894 | best_loss=0.02894
Epoch 39/80: current_loss=0.02893 | best_loss=0.02893
Epoch 40/80: current_loss=0.02890 | best_loss=0.02890
Epoch 41/80: current_loss=0.02889 | best_loss=0.02889
Epoch 42/80: current_loss=0.02888 | best_loss=0.02888
Epoch 43/80: current_loss=0.02888 | best_loss=0.02888
Epoch 44/80: current_loss=0.02898 | best_loss=0.02888
Epoch 45/80: current_loss=0.02908 | best_loss=0.02888
Epoch 46/80: current_loss=0.02896 | best_loss=0.02888
Epoch 47/80: current_loss=0.02893 | best_loss=0.02888
Epoch 48/80: current_loss=0.02891 | best_loss=0.02888
Epoch 49/80: current_loss=0.02892 | best_loss=0.02888
Epoch 50/80: current_loss=0.02896 | best_loss=0.02888
Epoch 51/80: current_loss=0.02895 | best_loss=0.02888
Epoch 52/80: current_loss=0.02901 | best_loss=0.02888
Epoch 53/80: current_loss=0.02895 | best_loss=0.02888
Epoch 54/80: current_loss=0.02894 | best_loss=0.02888
Epoch 55/80: current_loss=0.02894 | best_loss=0.02888
Epoch 56/80: current_loss=0.02908 | best_loss=0.02888
Epoch 57/80: current_loss=0.02892 | best_loss=0.02888
Epoch 58/80: current_loss=0.02892 | best_loss=0.02888
Epoch 59/80: current_loss=0.02910 | best_loss=0.02888
Epoch 60/80: current_loss=0.02891 | best_loss=0.02888
Epoch 61/80: current_loss=0.02892 | best_loss=0.02888
Epoch 62/80: current_loss=0.02904 | best_loss=0.02888
Early Stopping at epoch 62
      explained_var=0.00465 | mse_loss=0.02802
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02542 | best_loss=0.02542
Epoch 1/80: current_loss=0.02498 | best_loss=0.02498
Epoch 2/80: current_loss=0.02492 | best_loss=0.02492
Epoch 3/80: current_loss=0.02488 | best_loss=0.02488
Epoch 4/80: current_loss=0.02499 | best_loss=0.02488
Epoch 5/80: current_loss=0.02492 | best_loss=0.02488
Epoch 6/80: current_loss=0.02495 | best_loss=0.02488
Epoch 7/80: current_loss=0.02517 | best_loss=0.02488
Epoch 8/80: current_loss=0.02498 | best_loss=0.02488
Epoch 9/80: current_loss=0.02499 | best_loss=0.02488
Epoch 10/80: current_loss=0.02511 | best_loss=0.02488
Epoch 11/80: current_loss=0.02526 | best_loss=0.02488
Epoch 12/80: current_loss=0.02490 | best_loss=0.02488
Epoch 13/80: current_loss=0.02499 | best_loss=0.02488
Epoch 14/80: current_loss=0.02496 | best_loss=0.02488
Epoch 15/80: current_loss=0.02490 | best_loss=0.02488
Epoch 16/80: current_loss=0.02492 | best_loss=0.02488
Epoch 17/80: current_loss=0.02522 | best_loss=0.02488
Epoch 18/80: current_loss=0.02491 | best_loss=0.02488
Epoch 19/80: current_loss=0.02510 | best_loss=0.02488
Epoch 20/80: current_loss=0.02493 | best_loss=0.02488
Epoch 21/80: current_loss=0.02498 | best_loss=0.02488
Epoch 22/80: current_loss=0.02500 | best_loss=0.02488
Epoch 23/80: current_loss=0.02529 | best_loss=0.02488
Early Stopping at epoch 23
      explained_var=0.00114 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02743 | best_loss=0.02743
Epoch 1/80: current_loss=0.02769 | best_loss=0.02743
Epoch 2/80: current_loss=0.02774 | best_loss=0.02743
Epoch 3/80: current_loss=0.02774 | best_loss=0.02743
Epoch 4/80: current_loss=0.02807 | best_loss=0.02743
Epoch 5/80: current_loss=0.02824 | best_loss=0.02743
Epoch 6/80: current_loss=0.02808 | best_loss=0.02743
Epoch 7/80: current_loss=0.02755 | best_loss=0.02743
Epoch 8/80: current_loss=0.02742 | best_loss=0.02742
Epoch 9/80: current_loss=0.02798 | best_loss=0.02742
Epoch 10/80: current_loss=0.02769 | best_loss=0.02742
Epoch 11/80: current_loss=0.02771 | best_loss=0.02742
Epoch 12/80: current_loss=0.02764 | best_loss=0.02742
Epoch 13/80: current_loss=0.02771 | best_loss=0.02742
Epoch 14/80: current_loss=0.02755 | best_loss=0.02742
Epoch 15/80: current_loss=0.02788 | best_loss=0.02742
Epoch 16/80: current_loss=0.02751 | best_loss=0.02742
Epoch 17/80: current_loss=0.02768 | best_loss=0.02742
Epoch 18/80: current_loss=0.02800 | best_loss=0.02742
Epoch 19/80: current_loss=0.02768 | best_loss=0.02742
Epoch 20/80: current_loss=0.02758 | best_loss=0.02742
Epoch 21/80: current_loss=0.02813 | best_loss=0.02742
Epoch 22/80: current_loss=0.02794 | best_loss=0.02742
Epoch 23/80: current_loss=0.02762 | best_loss=0.02742
Epoch 24/80: current_loss=0.02780 | best_loss=0.02742
Epoch 25/80: current_loss=0.02782 | best_loss=0.02742
Epoch 26/80: current_loss=0.02731 | best_loss=0.02731
Epoch 27/80: current_loss=0.02787 | best_loss=0.02731
Epoch 28/80: current_loss=0.02801 | best_loss=0.02731
Epoch 29/80: current_loss=0.02872 | best_loss=0.02731
Epoch 30/80: current_loss=0.02767 | best_loss=0.02731
Epoch 31/80: current_loss=0.02813 | best_loss=0.02731
Epoch 32/80: current_loss=0.02751 | best_loss=0.02731
Epoch 33/80: current_loss=0.02753 | best_loss=0.02731
Epoch 34/80: current_loss=0.02772 | best_loss=0.02731
Epoch 35/80: current_loss=0.02751 | best_loss=0.02731
Epoch 36/80: current_loss=0.02823 | best_loss=0.02731
Epoch 37/80: current_loss=0.02734 | best_loss=0.02731
Epoch 38/80: current_loss=0.02768 | best_loss=0.02731
Epoch 39/80: current_loss=0.02780 | best_loss=0.02731
Epoch 40/80: current_loss=0.02766 | best_loss=0.02731
Epoch 41/80: current_loss=0.02887 | best_loss=0.02731
Epoch 42/80: current_loss=0.02795 | best_loss=0.02731
Epoch 43/80: current_loss=0.02731 | best_loss=0.02731
Epoch 44/80: current_loss=0.02746 | best_loss=0.02731
Epoch 45/80: current_loss=0.02805 | best_loss=0.02731
Epoch 46/80: current_loss=0.02735 | best_loss=0.02731
Epoch 47/80: current_loss=0.02772 | best_loss=0.02731
Epoch 48/80: current_loss=0.02756 | best_loss=0.02731
Epoch 49/80: current_loss=0.02765 | best_loss=0.02731
Epoch 50/80: current_loss=0.02758 | best_loss=0.02731
Epoch 51/80: current_loss=0.02748 | best_loss=0.02731
Epoch 52/80: current_loss=0.02781 | best_loss=0.02731
Epoch 53/80: current_loss=0.02770 | best_loss=0.02731
Epoch 54/80: current_loss=0.02769 | best_loss=0.02731
Epoch 55/80: current_loss=0.02753 | best_loss=0.02731
Epoch 56/80: current_loss=0.02747 | best_loss=0.02731
Epoch 57/80: current_loss=0.02808 | best_loss=0.02731
Epoch 58/80: current_loss=0.02784 | best_loss=0.02731
Epoch 59/80: current_loss=0.02747 | best_loss=0.02731
Epoch 60/80: current_loss=0.02794 | best_loss=0.02731
Epoch 61/80: current_loss=0.02774 | best_loss=0.02731
Epoch 62/80: current_loss=0.02751 | best_loss=0.02731
Epoch 63/80: current_loss=0.02754 | best_loss=0.02731
Early Stopping at epoch 63
      explained_var=-0.00317 | mse_loss=0.02773
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02685 | best_loss=0.02685
Epoch 1/80: current_loss=0.02680 | best_loss=0.02680
Epoch 2/80: current_loss=0.02680 | best_loss=0.02680
Epoch 3/80: current_loss=0.02681 | best_loss=0.02680
Epoch 4/80: current_loss=0.02683 | best_loss=0.02680
Epoch 5/80: current_loss=0.02685 | best_loss=0.02680
Epoch 6/80: current_loss=0.02704 | best_loss=0.02680
Epoch 7/80: current_loss=0.02693 | best_loss=0.02680
Epoch 8/80: current_loss=0.02681 | best_loss=0.02680
Epoch 9/80: current_loss=0.02699 | best_loss=0.02680
Epoch 10/80: current_loss=0.02696 | best_loss=0.02680
Epoch 11/80: current_loss=0.02687 | best_loss=0.02680
Epoch 12/80: current_loss=0.02689 | best_loss=0.02680
Epoch 13/80: current_loss=0.02682 | best_loss=0.02680
Epoch 14/80: current_loss=0.02682 | best_loss=0.02680
Epoch 15/80: current_loss=0.02679 | best_loss=0.02679
Epoch 16/80: current_loss=0.02693 | best_loss=0.02679
Epoch 17/80: current_loss=0.02695 | best_loss=0.02679
Epoch 18/80: current_loss=0.02688 | best_loss=0.02679
Epoch 19/80: current_loss=0.02679 | best_loss=0.02679
Epoch 20/80: current_loss=0.02678 | best_loss=0.02678
Epoch 21/80: current_loss=0.02681 | best_loss=0.02678
Epoch 22/80: current_loss=0.02713 | best_loss=0.02678
Epoch 23/80: current_loss=0.02683 | best_loss=0.02678
Epoch 24/80: current_loss=0.02697 | best_loss=0.02678
Epoch 25/80: current_loss=0.02688 | best_loss=0.02678
Epoch 26/80: current_loss=0.02709 | best_loss=0.02678
Epoch 27/80: current_loss=0.02691 | best_loss=0.02678
Epoch 28/80: current_loss=0.02682 | best_loss=0.02678
Epoch 29/80: current_loss=0.02685 | best_loss=0.02678
Epoch 30/80: current_loss=0.02688 | best_loss=0.02678
Epoch 31/80: current_loss=0.02679 | best_loss=0.02678
Epoch 32/80: current_loss=0.02692 | best_loss=0.02678
Epoch 33/80: current_loss=0.02678 | best_loss=0.02678
Epoch 34/80: current_loss=0.02682 | best_loss=0.02678
Epoch 35/80: current_loss=0.02680 | best_loss=0.02678
Epoch 36/80: current_loss=0.02727 | best_loss=0.02678
Epoch 37/80: current_loss=0.02694 | best_loss=0.02678
Epoch 38/80: current_loss=0.02682 | best_loss=0.02678
Epoch 39/80: current_loss=0.02681 | best_loss=0.02678
Epoch 40/80: current_loss=0.02688 | best_loss=0.02678
Epoch 41/80: current_loss=0.02680 | best_loss=0.02678
Epoch 42/80: current_loss=0.02683 | best_loss=0.02678
Epoch 43/80: current_loss=0.02685 | best_loss=0.02678
Epoch 44/80: current_loss=0.02681 | best_loss=0.02678
Epoch 45/80: current_loss=0.02713 | best_loss=0.02678
Epoch 46/80: current_loss=0.02683 | best_loss=0.02678
Epoch 47/80: current_loss=0.02682 | best_loss=0.02678
Epoch 48/80: current_loss=0.02701 | best_loss=0.02678
Epoch 49/80: current_loss=0.02688 | best_loss=0.02678
Epoch 50/80: current_loss=0.02681 | best_loss=0.02678
Epoch 51/80: current_loss=0.02685 | best_loss=0.02678
Epoch 52/80: current_loss=0.02684 | best_loss=0.02678
Epoch 53/80: current_loss=0.02684 | best_loss=0.02678
Early Stopping at epoch 53
      explained_var=0.00131 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03014 | best_loss=0.03014
Epoch 1/80: current_loss=0.02972 | best_loss=0.02972
Epoch 2/80: current_loss=0.02981 | best_loss=0.02972
Epoch 3/80: current_loss=0.02981 | best_loss=0.02972
Epoch 4/80: current_loss=0.02975 | best_loss=0.02972
Epoch 5/80: current_loss=0.02977 | best_loss=0.02972
Epoch 6/80: current_loss=0.02981 | best_loss=0.02972
Epoch 7/80: current_loss=0.02980 | best_loss=0.02972
Epoch 8/80: current_loss=0.02976 | best_loss=0.02972
Epoch 9/80: current_loss=0.02976 | best_loss=0.02972
Epoch 10/80: current_loss=0.02975 | best_loss=0.02972
Epoch 11/80: current_loss=0.02974 | best_loss=0.02972
Epoch 12/80: current_loss=0.02977 | best_loss=0.02972
Epoch 13/80: current_loss=0.02985 | best_loss=0.02972
Epoch 14/80: current_loss=0.02980 | best_loss=0.02972
Epoch 15/80: current_loss=0.02975 | best_loss=0.02972
Epoch 16/80: current_loss=0.02979 | best_loss=0.02972
Epoch 17/80: current_loss=0.02981 | best_loss=0.02972
Epoch 18/80: current_loss=0.02981 | best_loss=0.02972
Epoch 19/80: current_loss=0.02977 | best_loss=0.02972
Epoch 20/80: current_loss=0.02989 | best_loss=0.02972
Epoch 21/80: current_loss=0.02975 | best_loss=0.02972
Early Stopping at epoch 21
      explained_var=0.00114 | mse_loss=0.02873
----------------------------------------------
Average early_stopping_point: 24| avg_exp_var=0.00101| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 60
{'learning_rate': 0.0001, 'weight_decay': 0.0007328746433836118, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03022 | best_loss=0.03022
Epoch 1/80: current_loss=0.02939 | best_loss=0.02939
Epoch 2/80: current_loss=0.02908 | best_loss=0.02908
Epoch 3/80: current_loss=0.02898 | best_loss=0.02898
Epoch 4/80: current_loss=0.02894 | best_loss=0.02894
Epoch 5/80: current_loss=0.02898 | best_loss=0.02894
Epoch 6/80: current_loss=0.02893 | best_loss=0.02893
Epoch 7/80: current_loss=0.02886 | best_loss=0.02886
Epoch 8/80: current_loss=0.02887 | best_loss=0.02886
Epoch 9/80: current_loss=0.02881 | best_loss=0.02881
Epoch 10/80: current_loss=0.02884 | best_loss=0.02881
Epoch 11/80: current_loss=0.02896 | best_loss=0.02881
Epoch 12/80: current_loss=0.02885 | best_loss=0.02881
Epoch 13/80: current_loss=0.02888 | best_loss=0.02881
Epoch 14/80: current_loss=0.02904 | best_loss=0.02881
Epoch 15/80: current_loss=0.02889 | best_loss=0.02881
Epoch 16/80: current_loss=0.02893 | best_loss=0.02881
Epoch 17/80: current_loss=0.02891 | best_loss=0.02881
Epoch 18/80: current_loss=0.02892 | best_loss=0.02881
Epoch 19/80: current_loss=0.02898 | best_loss=0.02881
Epoch 20/80: current_loss=0.02912 | best_loss=0.02881
Epoch 21/80: current_loss=0.02916 | best_loss=0.02881
Epoch 22/80: current_loss=0.02898 | best_loss=0.02881
Epoch 23/80: current_loss=0.02905 | best_loss=0.02881
Epoch 24/80: current_loss=0.02913 | best_loss=0.02881
Epoch 25/80: current_loss=0.02900 | best_loss=0.02881
Epoch 26/80: current_loss=0.02903 | best_loss=0.02881
Epoch 27/80: current_loss=0.02907 | best_loss=0.02881
Epoch 28/80: current_loss=0.02900 | best_loss=0.02881
Epoch 29/80: current_loss=0.02899 | best_loss=0.02881
Early Stopping at epoch 29
      explained_var=0.00693 | mse_loss=0.02801
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02523 | best_loss=0.02523
Epoch 1/80: current_loss=0.02496 | best_loss=0.02496
Epoch 2/80: current_loss=0.02509 | best_loss=0.02496
Epoch 3/80: current_loss=0.02510 | best_loss=0.02496
Epoch 4/80: current_loss=0.02495 | best_loss=0.02495
Epoch 5/80: current_loss=0.02498 | best_loss=0.02495
Epoch 6/80: current_loss=0.02505 | best_loss=0.02495
Epoch 7/80: current_loss=0.02503 | best_loss=0.02495
Epoch 8/80: current_loss=0.02495 | best_loss=0.02495
Epoch 9/80: current_loss=0.02506 | best_loss=0.02495
Epoch 10/80: current_loss=0.02499 | best_loss=0.02495
Epoch 11/80: current_loss=0.02499 | best_loss=0.02495
Epoch 12/80: current_loss=0.02543 | best_loss=0.02495
Epoch 13/80: current_loss=0.02511 | best_loss=0.02495
Epoch 14/80: current_loss=0.02501 | best_loss=0.02495
Epoch 15/80: current_loss=0.02496 | best_loss=0.02495
Epoch 16/80: current_loss=0.02503 | best_loss=0.02495
Epoch 17/80: current_loss=0.02497 | best_loss=0.02495
Epoch 18/80: current_loss=0.02504 | best_loss=0.02495
Epoch 19/80: current_loss=0.02499 | best_loss=0.02495
Epoch 20/80: current_loss=0.02495 | best_loss=0.02495
Epoch 21/80: current_loss=0.02496 | best_loss=0.02495
Epoch 22/80: current_loss=0.02494 | best_loss=0.02494
Epoch 23/80: current_loss=0.02497 | best_loss=0.02494
Epoch 24/80: current_loss=0.02506 | best_loss=0.02494
Epoch 25/80: current_loss=0.02500 | best_loss=0.02494
Epoch 26/80: current_loss=0.02510 | best_loss=0.02494
Epoch 27/80: current_loss=0.02497 | best_loss=0.02494
Epoch 28/80: current_loss=0.02495 | best_loss=0.02494
Epoch 29/80: current_loss=0.02495 | best_loss=0.02494
Epoch 30/80: current_loss=0.02495 | best_loss=0.02494
Epoch 31/80: current_loss=0.02542 | best_loss=0.02494
Epoch 32/80: current_loss=0.02505 | best_loss=0.02494
Epoch 33/80: current_loss=0.02523 | best_loss=0.02494
Epoch 34/80: current_loss=0.02496 | best_loss=0.02494
Epoch 35/80: current_loss=0.02529 | best_loss=0.02494
Epoch 36/80: current_loss=0.02498 | best_loss=0.02494
Epoch 37/80: current_loss=0.02504 | best_loss=0.02494
Epoch 38/80: current_loss=0.02505 | best_loss=0.02494
Epoch 39/80: current_loss=0.02497 | best_loss=0.02494
Epoch 40/80: current_loss=0.02496 | best_loss=0.02494
Epoch 41/80: current_loss=0.02500 | best_loss=0.02494
Epoch 42/80: current_loss=0.02498 | best_loss=0.02494
Early Stopping at epoch 42
      explained_var=0.00114 | mse_loss=0.02520

----------------------------------------------
Params for Trial 61
{'learning_rate': 0.0001, 'weight_decay': 1.3490028359804883e-05, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03927 | best_loss=0.03927
Epoch 1/80: current_loss=0.03493 | best_loss=0.03493
Epoch 2/80: current_loss=0.03359 | best_loss=0.03359
Epoch 3/80: current_loss=0.03227 | best_loss=0.03227
Epoch 4/80: current_loss=0.03165 | best_loss=0.03165
Epoch 5/80: current_loss=0.03077 | best_loss=0.03077
Epoch 6/80: current_loss=0.03046 | best_loss=0.03046
Epoch 7/80: current_loss=0.03011 | best_loss=0.03011
Epoch 8/80: current_loss=0.02991 | best_loss=0.02991
Epoch 9/80: current_loss=0.03014 | best_loss=0.02991
Epoch 10/80: current_loss=0.02985 | best_loss=0.02985
Epoch 11/80: current_loss=0.02957 | best_loss=0.02957
Epoch 12/80: current_loss=0.02943 | best_loss=0.02943
Epoch 13/80: current_loss=0.02934 | best_loss=0.02934
Epoch 14/80: current_loss=0.02985 | best_loss=0.02934
Epoch 15/80: current_loss=0.02919 | best_loss=0.02919
Epoch 16/80: current_loss=0.02933 | best_loss=0.02919
Epoch 17/80: current_loss=0.02919 | best_loss=0.02919
Epoch 18/80: current_loss=0.02908 | best_loss=0.02908
Epoch 19/80: current_loss=0.02911 | best_loss=0.02908
Epoch 20/80: current_loss=0.02937 | best_loss=0.02908
Epoch 21/80: current_loss=0.02904 | best_loss=0.02904
Epoch 22/80: current_loss=0.02917 | best_loss=0.02904
Epoch 23/80: current_loss=0.02898 | best_loss=0.02898
Epoch 24/80: current_loss=0.02897 | best_loss=0.02897
Epoch 25/80: current_loss=0.02905 | best_loss=0.02897
Epoch 26/80: current_loss=0.02911 | best_loss=0.02897
Epoch 27/80: current_loss=0.02907 | best_loss=0.02897
Epoch 28/80: current_loss=0.02907 | best_loss=0.02897
Epoch 29/80: current_loss=0.02902 | best_loss=0.02897
Epoch 30/80: current_loss=0.02897 | best_loss=0.02897
Epoch 31/80: current_loss=0.02888 | best_loss=0.02888
Epoch 32/80: current_loss=0.02891 | best_loss=0.02888
Epoch 33/80: current_loss=0.02891 | best_loss=0.02888
Epoch 34/80: current_loss=0.02885 | best_loss=0.02885
Epoch 35/80: current_loss=0.02891 | best_loss=0.02885
Epoch 36/80: current_loss=0.02877 | best_loss=0.02877
Epoch 37/80: current_loss=0.02890 | best_loss=0.02877
Epoch 38/80: current_loss=0.02885 | best_loss=0.02877
Epoch 39/80: current_loss=0.02887 | best_loss=0.02877
Epoch 40/80: current_loss=0.02885 | best_loss=0.02877
Epoch 41/80: current_loss=0.02913 | best_loss=0.02877
Epoch 42/80: current_loss=0.02896 | best_loss=0.02877
Epoch 43/80: current_loss=0.02886 | best_loss=0.02877
Epoch 44/80: current_loss=0.02908 | best_loss=0.02877
Epoch 45/80: current_loss=0.02913 | best_loss=0.02877
Epoch 46/80: current_loss=0.02901 | best_loss=0.02877
Epoch 47/80: current_loss=0.02899 | best_loss=0.02877
Epoch 48/80: current_loss=0.02889 | best_loss=0.02877
Epoch 49/80: current_loss=0.02890 | best_loss=0.02877
Epoch 50/80: current_loss=0.02898 | best_loss=0.02877
Epoch 51/80: current_loss=0.02892 | best_loss=0.02877
Epoch 52/80: current_loss=0.02920 | best_loss=0.02877
Epoch 53/80: current_loss=0.02930 | best_loss=0.02877
Epoch 54/80: current_loss=0.02903 | best_loss=0.02877
Epoch 55/80: current_loss=0.02908 | best_loss=0.02877
Epoch 56/80: current_loss=0.02966 | best_loss=0.02877
Early Stopping at epoch 56
      explained_var=0.01005 | mse_loss=0.02795
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02510 | best_loss=0.02510
Epoch 1/80: current_loss=0.02539 | best_loss=0.02510
Epoch 2/80: current_loss=0.02503 | best_loss=0.02503
Epoch 3/80: current_loss=0.02491 | best_loss=0.02491
Epoch 4/80: current_loss=0.02491 | best_loss=0.02491
Epoch 5/80: current_loss=0.02501 | best_loss=0.02491
Epoch 6/80: current_loss=0.02504 | best_loss=0.02491
Epoch 7/80: current_loss=0.02508 | best_loss=0.02491
Epoch 8/80: current_loss=0.02518 | best_loss=0.02491
Epoch 9/80: current_loss=0.02505 | best_loss=0.02491
Epoch 10/80: current_loss=0.02507 | best_loss=0.02491
Epoch 11/80: current_loss=0.02498 | best_loss=0.02491
Epoch 12/80: current_loss=0.02550 | best_loss=0.02491
Epoch 13/80: current_loss=0.02528 | best_loss=0.02491
Epoch 14/80: current_loss=0.02532 | best_loss=0.02491
Epoch 15/80: current_loss=0.02517 | best_loss=0.02491
Epoch 16/80: current_loss=0.02518 | best_loss=0.02491
Epoch 17/80: current_loss=0.02508 | best_loss=0.02491
Epoch 18/80: current_loss=0.02514 | best_loss=0.02491
Epoch 19/80: current_loss=0.02521 | best_loss=0.02491
Epoch 20/80: current_loss=0.02508 | best_loss=0.02491
Epoch 21/80: current_loss=0.02502 | best_loss=0.02491
Epoch 22/80: current_loss=0.02505 | best_loss=0.02491
Epoch 23/80: current_loss=0.02506 | best_loss=0.02491
Epoch 24/80: current_loss=0.02509 | best_loss=0.02491
Early Stopping at epoch 24
      explained_var=-0.00027 | mse_loss=0.02521

----------------------------------------------
Params for Trial 62
{'learning_rate': 0.0001, 'weight_decay': 0.007174660702441004, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04159 | best_loss=0.04159
Epoch 1/80: current_loss=0.03724 | best_loss=0.03724
Epoch 2/80: current_loss=0.03562 | best_loss=0.03562
Epoch 3/80: current_loss=0.03447 | best_loss=0.03447
Epoch 4/80: current_loss=0.03351 | best_loss=0.03351
Epoch 5/80: current_loss=0.03311 | best_loss=0.03311
Epoch 6/80: current_loss=0.03227 | best_loss=0.03227
Epoch 7/80: current_loss=0.03166 | best_loss=0.03166
Epoch 8/80: current_loss=0.03120 | best_loss=0.03120
Epoch 9/80: current_loss=0.03083 | best_loss=0.03083
Epoch 10/80: current_loss=0.03075 | best_loss=0.03075
Epoch 11/80: current_loss=0.03037 | best_loss=0.03037
Epoch 12/80: current_loss=0.03020 | best_loss=0.03020
Epoch 13/80: current_loss=0.03003 | best_loss=0.03003
Epoch 14/80: current_loss=0.02984 | best_loss=0.02984
Epoch 15/80: current_loss=0.02970 | best_loss=0.02970
Epoch 16/80: current_loss=0.02973 | best_loss=0.02970
Epoch 17/80: current_loss=0.02959 | best_loss=0.02959
Epoch 18/80: current_loss=0.02942 | best_loss=0.02942
Epoch 19/80: current_loss=0.02952 | best_loss=0.02942
Epoch 20/80: current_loss=0.02929 | best_loss=0.02929
Epoch 21/80: current_loss=0.02955 | best_loss=0.02929
Epoch 22/80: current_loss=0.02919 | best_loss=0.02919
Epoch 23/80: current_loss=0.02916 | best_loss=0.02916
Epoch 24/80: current_loss=0.02916 | best_loss=0.02916
Epoch 25/80: current_loss=0.02909 | best_loss=0.02909
Epoch 26/80: current_loss=0.02907 | best_loss=0.02907
Epoch 27/80: current_loss=0.02915 | best_loss=0.02907
Epoch 28/80: current_loss=0.02904 | best_loss=0.02904
Epoch 29/80: current_loss=0.02900 | best_loss=0.02900
Epoch 30/80: current_loss=0.02899 | best_loss=0.02899
Epoch 31/80: current_loss=0.02897 | best_loss=0.02897
Epoch 32/80: current_loss=0.02897 | best_loss=0.02897
Epoch 33/80: current_loss=0.02895 | best_loss=0.02895
Epoch 34/80: current_loss=0.02894 | best_loss=0.02894
Epoch 35/80: current_loss=0.02900 | best_loss=0.02894
Epoch 36/80: current_loss=0.02893 | best_loss=0.02893
Epoch 37/80: current_loss=0.02893 | best_loss=0.02893
Epoch 38/80: current_loss=0.02896 | best_loss=0.02893
Epoch 39/80: current_loss=0.02910 | best_loss=0.02893
Epoch 40/80: current_loss=0.02892 | best_loss=0.02892
Epoch 41/80: current_loss=0.02901 | best_loss=0.02892
Epoch 42/80: current_loss=0.02895 | best_loss=0.02892
Epoch 43/80: current_loss=0.02956 | best_loss=0.02892
Epoch 44/80: current_loss=0.02923 | best_loss=0.02892
Epoch 45/80: current_loss=0.02898 | best_loss=0.02892
Epoch 46/80: current_loss=0.02924 | best_loss=0.02892
Epoch 47/80: current_loss=0.02890 | best_loss=0.02890
Epoch 48/80: current_loss=0.02894 | best_loss=0.02890
Epoch 49/80: current_loss=0.02891 | best_loss=0.02890
Epoch 50/80: current_loss=0.02909 | best_loss=0.02890
Epoch 51/80: current_loss=0.02900 | best_loss=0.02890
Epoch 52/80: current_loss=0.02919 | best_loss=0.02890
Epoch 53/80: current_loss=0.02895 | best_loss=0.02890
Epoch 54/80: current_loss=0.02891 | best_loss=0.02890
Epoch 55/80: current_loss=0.02900 | best_loss=0.02890
Epoch 56/80: current_loss=0.02898 | best_loss=0.02890
Epoch 57/80: current_loss=0.02893 | best_loss=0.02890
Epoch 58/80: current_loss=0.02892 | best_loss=0.02890
Epoch 59/80: current_loss=0.02891 | best_loss=0.02890
Epoch 60/80: current_loss=0.02895 | best_loss=0.02890
Epoch 61/80: current_loss=0.02890 | best_loss=0.02890
Epoch 62/80: current_loss=0.02915 | best_loss=0.02890
Epoch 63/80: current_loss=0.02896 | best_loss=0.02890
Epoch 64/80: current_loss=0.02890 | best_loss=0.02890
Epoch 65/80: current_loss=0.02891 | best_loss=0.02890
Epoch 66/80: current_loss=0.02890 | best_loss=0.02890
Epoch 67/80: current_loss=0.02892 | best_loss=0.02890
Epoch 68/80: current_loss=0.02893 | best_loss=0.02890
Epoch 69/80: current_loss=0.02902 | best_loss=0.02890
Epoch 70/80: current_loss=0.02895 | best_loss=0.02890
Epoch 71/80: current_loss=0.02898 | best_loss=0.02890
Epoch 72/80: current_loss=0.02896 | best_loss=0.02890
Epoch 73/80: current_loss=0.02908 | best_loss=0.02890
Epoch 74/80: current_loss=0.02903 | best_loss=0.02890
Epoch 75/80: current_loss=0.02893 | best_loss=0.02890
Epoch 76/80: current_loss=0.02894 | best_loss=0.02890
Epoch 77/80: current_loss=0.02901 | best_loss=0.02890
Epoch 78/80: current_loss=0.02915 | best_loss=0.02890
Epoch 79/80: current_loss=0.02896 | best_loss=0.02890
      explained_var=0.00456 | mse_loss=0.02804
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02492 | best_loss=0.02492
Epoch 1/80: current_loss=0.02498 | best_loss=0.02492
Epoch 2/80: current_loss=0.02586 | best_loss=0.02492
Epoch 3/80: current_loss=0.02493 | best_loss=0.02492
Epoch 4/80: current_loss=0.02496 | best_loss=0.02492
Epoch 5/80: current_loss=0.02516 | best_loss=0.02492
Epoch 6/80: current_loss=0.02500 | best_loss=0.02492
Epoch 7/80: current_loss=0.02497 | best_loss=0.02492
Epoch 8/80: current_loss=0.02496 | best_loss=0.02492
Epoch 9/80: current_loss=0.02517 | best_loss=0.02492
Epoch 10/80: current_loss=0.02499 | best_loss=0.02492
Epoch 11/80: current_loss=0.02495 | best_loss=0.02492
Epoch 12/80: current_loss=0.02494 | best_loss=0.02492
Epoch 13/80: current_loss=0.02503 | best_loss=0.02492
Epoch 14/80: current_loss=0.02493 | best_loss=0.02492
Epoch 15/80: current_loss=0.02492 | best_loss=0.02492
Epoch 16/80: current_loss=0.02525 | best_loss=0.02492
Epoch 17/80: current_loss=0.02495 | best_loss=0.02492
Epoch 18/80: current_loss=0.02492 | best_loss=0.02492
Epoch 19/80: current_loss=0.02495 | best_loss=0.02492
Epoch 20/80: current_loss=0.02504 | best_loss=0.02492
Epoch 21/80: current_loss=0.02510 | best_loss=0.02492
Epoch 22/80: current_loss=0.02492 | best_loss=0.02492
Epoch 23/80: current_loss=0.02498 | best_loss=0.02492
Epoch 24/80: current_loss=0.02497 | best_loss=0.02492
Epoch 25/80: current_loss=0.02496 | best_loss=0.02492
Epoch 26/80: current_loss=0.02495 | best_loss=0.02492
Epoch 27/80: current_loss=0.02493 | best_loss=0.02492
Epoch 28/80: current_loss=0.02517 | best_loss=0.02492
Epoch 29/80: current_loss=0.02507 | best_loss=0.02492
Epoch 30/80: current_loss=0.02498 | best_loss=0.02492
Epoch 31/80: current_loss=0.02492 | best_loss=0.02492
Epoch 32/80: current_loss=0.02493 | best_loss=0.02492
Epoch 33/80: current_loss=0.02508 | best_loss=0.02492
Epoch 34/80: current_loss=0.02521 | best_loss=0.02492
Epoch 35/80: current_loss=0.02498 | best_loss=0.02492
Epoch 36/80: current_loss=0.02528 | best_loss=0.02492
Epoch 37/80: current_loss=0.02493 | best_loss=0.02492
Epoch 38/80: current_loss=0.02515 | best_loss=0.02492
Early Stopping at epoch 38
      explained_var=0.00142 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02821 | best_loss=0.02821
Epoch 1/80: current_loss=0.02760 | best_loss=0.02760
Epoch 2/80: current_loss=0.02805 | best_loss=0.02760
Epoch 3/80: current_loss=0.02787 | best_loss=0.02760
Epoch 4/80: current_loss=0.02789 | best_loss=0.02760
Epoch 5/80: current_loss=0.02838 | best_loss=0.02760
Epoch 6/80: current_loss=0.02800 | best_loss=0.02760
Epoch 7/80: current_loss=0.02758 | best_loss=0.02758
Epoch 8/80: current_loss=0.02825 | best_loss=0.02758
Epoch 9/80: current_loss=0.02777 | best_loss=0.02758
Epoch 10/80: current_loss=0.02783 | best_loss=0.02758
Epoch 11/80: current_loss=0.02788 | best_loss=0.02758
Epoch 12/80: current_loss=0.02805 | best_loss=0.02758
Epoch 13/80: current_loss=0.02796 | best_loss=0.02758
Epoch 14/80: current_loss=0.02837 | best_loss=0.02758
Epoch 15/80: current_loss=0.02810 | best_loss=0.02758
Epoch 16/80: current_loss=0.02801 | best_loss=0.02758
Epoch 17/80: current_loss=0.02785 | best_loss=0.02758
Epoch 18/80: current_loss=0.02885 | best_loss=0.02758
Epoch 19/80: current_loss=0.02867 | best_loss=0.02758
Epoch 20/80: current_loss=0.02790 | best_loss=0.02758
Epoch 21/80: current_loss=0.02799 | best_loss=0.02758
Epoch 22/80: current_loss=0.02827 | best_loss=0.02758
Epoch 23/80: current_loss=0.02803 | best_loss=0.02758
Epoch 24/80: current_loss=0.02868 | best_loss=0.02758
Epoch 25/80: current_loss=0.02779 | best_loss=0.02758
Epoch 26/80: current_loss=0.02807 | best_loss=0.02758
Epoch 27/80: current_loss=0.02815 | best_loss=0.02758
Early Stopping at epoch 27
      explained_var=-0.01517 | mse_loss=0.02811
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02709 | best_loss=0.02709
Epoch 1/80: current_loss=0.02669 | best_loss=0.02669
Epoch 2/80: current_loss=0.02669 | best_loss=0.02669
Epoch 3/80: current_loss=0.02678 | best_loss=0.02669
Epoch 4/80: current_loss=0.02670 | best_loss=0.02669
Epoch 5/80: current_loss=0.02670 | best_loss=0.02669
Epoch 6/80: current_loss=0.02666 | best_loss=0.02666
Epoch 7/80: current_loss=0.02687 | best_loss=0.02666
Epoch 8/80: current_loss=0.02666 | best_loss=0.02666
Epoch 9/80: current_loss=0.02666 | best_loss=0.02666
Epoch 10/80: current_loss=0.02665 | best_loss=0.02665
Epoch 11/80: current_loss=0.02686 | best_loss=0.02665
Epoch 12/80: current_loss=0.02667 | best_loss=0.02665
Epoch 13/80: current_loss=0.02668 | best_loss=0.02665
Epoch 14/80: current_loss=0.02671 | best_loss=0.02665
Epoch 15/80: current_loss=0.02666 | best_loss=0.02665
Epoch 16/80: current_loss=0.02681 | best_loss=0.02665
Epoch 17/80: current_loss=0.02667 | best_loss=0.02665
Epoch 18/80: current_loss=0.02666 | best_loss=0.02665
Epoch 19/80: current_loss=0.02666 | best_loss=0.02665
Epoch 20/80: current_loss=0.02674 | best_loss=0.02665
Epoch 21/80: current_loss=0.02670 | best_loss=0.02665
Epoch 22/80: current_loss=0.02667 | best_loss=0.02665
Epoch 23/80: current_loss=0.02681 | best_loss=0.02665
Epoch 24/80: current_loss=0.02669 | best_loss=0.02665
Epoch 25/80: current_loss=0.02678 | best_loss=0.02665
Epoch 26/80: current_loss=0.02677 | best_loss=0.02665
Epoch 27/80: current_loss=0.02675 | best_loss=0.02665
Epoch 28/80: current_loss=0.02679 | best_loss=0.02665
Epoch 29/80: current_loss=0.02681 | best_loss=0.02665
Epoch 30/80: current_loss=0.02667 | best_loss=0.02665
Early Stopping at epoch 30
      explained_var=0.00317 | mse_loss=0.02490
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02989 | best_loss=0.02989
Epoch 1/80: current_loss=0.02989 | best_loss=0.02989
Epoch 2/80: current_loss=0.02991 | best_loss=0.02989
Epoch 3/80: current_loss=0.02980 | best_loss=0.02980
Epoch 4/80: current_loss=0.02984 | best_loss=0.02980
Epoch 5/80: current_loss=0.02979 | best_loss=0.02979
Epoch 6/80: current_loss=0.02977 | best_loss=0.02977
Epoch 7/80: current_loss=0.02977 | best_loss=0.02977
Epoch 8/80: current_loss=0.02984 | best_loss=0.02977
Epoch 9/80: current_loss=0.02993 | best_loss=0.02977
Epoch 10/80: current_loss=0.02981 | best_loss=0.02977
Epoch 11/80: current_loss=0.02987 | best_loss=0.02977
Epoch 12/80: current_loss=0.02976 | best_loss=0.02976
Epoch 13/80: current_loss=0.02977 | best_loss=0.02976
Epoch 14/80: current_loss=0.02984 | best_loss=0.02976
Epoch 15/80: current_loss=0.02981 | best_loss=0.02976
Epoch 16/80: current_loss=0.02976 | best_loss=0.02976
Epoch 17/80: current_loss=0.02983 | best_loss=0.02976
Epoch 18/80: current_loss=0.02984 | best_loss=0.02976
Epoch 19/80: current_loss=0.02984 | best_loss=0.02976
Epoch 20/80: current_loss=0.02982 | best_loss=0.02976
Epoch 21/80: current_loss=0.02994 | best_loss=0.02976
Epoch 22/80: current_loss=0.02978 | best_loss=0.02976
Epoch 23/80: current_loss=0.02975 | best_loss=0.02975
Epoch 24/80: current_loss=0.02982 | best_loss=0.02975
Epoch 25/80: current_loss=0.03000 | best_loss=0.02975
Epoch 26/80: current_loss=0.02975 | best_loss=0.02975
Epoch 27/80: current_loss=0.02977 | best_loss=0.02975
Epoch 28/80: current_loss=0.02988 | best_loss=0.02975
Epoch 29/80: current_loss=0.02990 | best_loss=0.02975
Epoch 30/80: current_loss=0.02980 | best_loss=0.02975
Epoch 31/80: current_loss=0.02979 | best_loss=0.02975
Epoch 32/80: current_loss=0.02976 | best_loss=0.02975
Epoch 33/80: current_loss=0.02985 | best_loss=0.02975
Epoch 34/80: current_loss=0.02976 | best_loss=0.02975
Epoch 35/80: current_loss=0.02975 | best_loss=0.02975
Epoch 36/80: current_loss=0.02975 | best_loss=0.02975
Epoch 37/80: current_loss=0.02984 | best_loss=0.02975
Epoch 38/80: current_loss=0.02978 | best_loss=0.02975
Epoch 39/80: current_loss=0.02989 | best_loss=0.02975
Epoch 40/80: current_loss=0.02978 | best_loss=0.02975
Epoch 41/80: current_loss=0.02979 | best_loss=0.02975
Epoch 42/80: current_loss=0.02980 | best_loss=0.02975
Epoch 43/80: current_loss=0.02982 | best_loss=0.02975
Epoch 44/80: current_loss=0.02995 | best_loss=0.02975
Epoch 45/80: current_loss=0.02979 | best_loss=0.02975
Epoch 46/80: current_loss=0.02975 | best_loss=0.02975
Epoch 47/80: current_loss=0.02989 | best_loss=0.02975
Epoch 48/80: current_loss=0.02975 | best_loss=0.02975
Epoch 49/80: current_loss=0.02975 | best_loss=0.02975
Epoch 50/80: current_loss=0.02975 | best_loss=0.02975
Epoch 51/80: current_loss=0.02989 | best_loss=0.02975
Epoch 52/80: current_loss=0.02980 | best_loss=0.02975
Epoch 53/80: current_loss=0.02975 | best_loss=0.02975
Epoch 54/80: current_loss=0.02974 | best_loss=0.02974
Epoch 55/80: current_loss=0.02981 | best_loss=0.02974
Epoch 56/80: current_loss=0.02983 | best_loss=0.02974
Epoch 57/80: current_loss=0.02975 | best_loss=0.02974
Epoch 58/80: current_loss=0.02980 | best_loss=0.02974
Epoch 59/80: current_loss=0.02979 | best_loss=0.02974
Epoch 60/80: current_loss=0.02976 | best_loss=0.02974
Epoch 61/80: current_loss=0.02974 | best_loss=0.02974
Epoch 62/80: current_loss=0.02977 | best_loss=0.02974
Epoch 63/80: current_loss=0.02976 | best_loss=0.02974
Epoch 64/80: current_loss=0.02979 | best_loss=0.02974
Epoch 65/80: current_loss=0.02975 | best_loss=0.02974
Epoch 66/80: current_loss=0.02974 | best_loss=0.02974
Epoch 67/80: current_loss=0.03014 | best_loss=0.02974
Epoch 68/80: current_loss=0.03003 | best_loss=0.02974
Epoch 69/80: current_loss=0.02980 | best_loss=0.02974
Epoch 70/80: current_loss=0.02980 | best_loss=0.02974
Epoch 71/80: current_loss=0.02974 | best_loss=0.02974
Epoch 72/80: current_loss=0.02974 | best_loss=0.02974
Epoch 73/80: current_loss=0.02992 | best_loss=0.02974
Epoch 74/80: current_loss=0.02988 | best_loss=0.02974
Epoch 75/80: current_loss=0.02977 | best_loss=0.02974
Epoch 76/80: current_loss=0.02974 | best_loss=0.02974
Epoch 77/80: current_loss=0.02975 | best_loss=0.02974
Epoch 78/80: current_loss=0.02976 | best_loss=0.02974
Epoch 79/80: current_loss=0.02984 | best_loss=0.02974
      explained_var=0.00028 | mse_loss=0.02875
----------------------------------------------
Average early_stopping_point: 39| avg_exp_var=-0.00115| avg_loss=0.02700
----------------------------------------------


----------------------------------------------
Params for Trial 63
{'learning_rate': 0.0001, 'weight_decay': 0.003425611673947849, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04978 | best_loss=0.04978
Epoch 1/80: current_loss=0.03969 | best_loss=0.03969
Epoch 2/80: current_loss=0.03794 | best_loss=0.03794
Epoch 3/80: current_loss=0.03633 | best_loss=0.03633
Epoch 4/80: current_loss=0.03508 | best_loss=0.03508
Epoch 5/80: current_loss=0.03420 | best_loss=0.03420
Epoch 6/80: current_loss=0.03311 | best_loss=0.03311
Epoch 7/80: current_loss=0.03227 | best_loss=0.03227
Epoch 8/80: current_loss=0.03207 | best_loss=0.03207
Epoch 9/80: current_loss=0.03122 | best_loss=0.03122
Epoch 10/80: current_loss=0.03089 | best_loss=0.03089
Epoch 11/80: current_loss=0.03063 | best_loss=0.03063
Epoch 12/80: current_loss=0.03063 | best_loss=0.03063
Epoch 13/80: current_loss=0.03020 | best_loss=0.03020
Epoch 14/80: current_loss=0.03000 | best_loss=0.03000
Epoch 15/80: current_loss=0.03034 | best_loss=0.03000
Epoch 16/80: current_loss=0.02992 | best_loss=0.02992
Epoch 17/80: current_loss=0.02984 | best_loss=0.02984
Epoch 18/80: current_loss=0.02962 | best_loss=0.02962
Epoch 19/80: current_loss=0.02949 | best_loss=0.02949
Epoch 20/80: current_loss=0.02958 | best_loss=0.02949
Epoch 21/80: current_loss=0.02935 | best_loss=0.02935
Epoch 22/80: current_loss=0.02935 | best_loss=0.02935
Epoch 23/80: current_loss=0.02922 | best_loss=0.02922
Epoch 24/80: current_loss=0.02918 | best_loss=0.02918
Epoch 25/80: current_loss=0.02919 | best_loss=0.02918
Epoch 26/80: current_loss=0.02918 | best_loss=0.02918
Epoch 27/80: current_loss=0.02919 | best_loss=0.02918
Epoch 28/80: current_loss=0.02914 | best_loss=0.02914
Epoch 29/80: current_loss=0.02909 | best_loss=0.02909
Epoch 30/80: current_loss=0.02909 | best_loss=0.02909
Epoch 31/80: current_loss=0.02907 | best_loss=0.02907
Epoch 32/80: current_loss=0.02908 | best_loss=0.02907
Epoch 33/80: current_loss=0.02902 | best_loss=0.02902
Epoch 34/80: current_loss=0.02901 | best_loss=0.02901
Epoch 35/80: current_loss=0.02900 | best_loss=0.02900
Epoch 36/80: current_loss=0.02903 | best_loss=0.02900
Epoch 37/80: current_loss=0.02903 | best_loss=0.02900
Epoch 38/80: current_loss=0.02899 | best_loss=0.02899
Epoch 39/80: current_loss=0.02903 | best_loss=0.02899
Epoch 40/80: current_loss=0.02902 | best_loss=0.02899
Epoch 41/80: current_loss=0.02900 | best_loss=0.02899
Epoch 42/80: current_loss=0.02908 | best_loss=0.02899
Epoch 43/80: current_loss=0.02898 | best_loss=0.02898
Epoch 44/80: current_loss=0.02903 | best_loss=0.02898
Epoch 45/80: current_loss=0.02896 | best_loss=0.02896
Epoch 46/80: current_loss=0.02898 | best_loss=0.02896
Epoch 47/80: current_loss=0.02895 | best_loss=0.02895
Epoch 48/80: current_loss=0.02896 | best_loss=0.02895
Epoch 49/80: current_loss=0.02904 | best_loss=0.02895
Epoch 50/80: current_loss=0.02895 | best_loss=0.02895
Epoch 51/80: current_loss=0.02896 | best_loss=0.02895
Epoch 52/80: current_loss=0.02894 | best_loss=0.02894
Epoch 53/80: current_loss=0.02893 | best_loss=0.02893
Epoch 54/80: current_loss=0.02895 | best_loss=0.02893
Epoch 55/80: current_loss=0.02894 | best_loss=0.02893
Epoch 56/80: current_loss=0.02893 | best_loss=0.02893
Epoch 57/80: current_loss=0.02906 | best_loss=0.02893
Epoch 58/80: current_loss=0.02899 | best_loss=0.02893
Epoch 59/80: current_loss=0.02898 | best_loss=0.02893
Epoch 60/80: current_loss=0.02894 | best_loss=0.02893
Epoch 61/80: current_loss=0.02896 | best_loss=0.02893
Epoch 62/80: current_loss=0.02895 | best_loss=0.02893
Epoch 63/80: current_loss=0.02895 | best_loss=0.02893
Epoch 64/80: current_loss=0.02895 | best_loss=0.02893
Epoch 65/80: current_loss=0.02895 | best_loss=0.02893
Epoch 66/80: current_loss=0.02897 | best_loss=0.02893
Epoch 67/80: current_loss=0.02901 | best_loss=0.02893
Epoch 68/80: current_loss=0.02897 | best_loss=0.02893
Epoch 69/80: current_loss=0.02917 | best_loss=0.02893
Epoch 70/80: current_loss=0.02899 | best_loss=0.02893
Epoch 71/80: current_loss=0.02911 | best_loss=0.02893
Epoch 72/80: current_loss=0.02899 | best_loss=0.02893
Epoch 73/80: current_loss=0.02904 | best_loss=0.02893
Early Stopping at epoch 73
      explained_var=0.00460 | mse_loss=0.02802
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02520 | best_loss=0.02520
Epoch 1/80: current_loss=0.02495 | best_loss=0.02495
Epoch 2/80: current_loss=0.02497 | best_loss=0.02495
Epoch 3/80: current_loss=0.02496 | best_loss=0.02495
Epoch 4/80: current_loss=0.02494 | best_loss=0.02494
Epoch 5/80: current_loss=0.02503 | best_loss=0.02494
Epoch 6/80: current_loss=0.02506 | best_loss=0.02494
Epoch 7/80: current_loss=0.02499 | best_loss=0.02494
Epoch 8/80: current_loss=0.02499 | best_loss=0.02494
Epoch 9/80: current_loss=0.02512 | best_loss=0.02494
Epoch 10/80: current_loss=0.02513 | best_loss=0.02494
Epoch 11/80: current_loss=0.02497 | best_loss=0.02494
Epoch 12/80: current_loss=0.02514 | best_loss=0.02494
Epoch 13/80: current_loss=0.02499 | best_loss=0.02494
Epoch 14/80: current_loss=0.02511 | best_loss=0.02494
Epoch 15/80: current_loss=0.02501 | best_loss=0.02494
Epoch 16/80: current_loss=0.02505 | best_loss=0.02494
Epoch 17/80: current_loss=0.02519 | best_loss=0.02494
Epoch 18/80: current_loss=0.02500 | best_loss=0.02494
Epoch 19/80: current_loss=0.02502 | best_loss=0.02494
Epoch 20/80: current_loss=0.02498 | best_loss=0.02494
Epoch 21/80: current_loss=0.02498 | best_loss=0.02494
Epoch 22/80: current_loss=0.02502 | best_loss=0.02494
Epoch 23/80: current_loss=0.02514 | best_loss=0.02494
Epoch 24/80: current_loss=0.02501 | best_loss=0.02494
Early Stopping at epoch 24
      explained_var=0.00117 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02808 | best_loss=0.02808
Epoch 1/80: current_loss=0.02757 | best_loss=0.02757
Epoch 2/80: current_loss=0.02772 | best_loss=0.02757
Epoch 3/80: current_loss=0.02745 | best_loss=0.02745
Epoch 4/80: current_loss=0.02777 | best_loss=0.02745
Epoch 5/80: current_loss=0.02760 | best_loss=0.02745
Epoch 6/80: current_loss=0.02742 | best_loss=0.02742
Epoch 7/80: current_loss=0.02808 | best_loss=0.02742
Epoch 8/80: current_loss=0.02817 | best_loss=0.02742
Epoch 9/80: current_loss=0.02753 | best_loss=0.02742
Epoch 10/80: current_loss=0.02781 | best_loss=0.02742
Epoch 11/80: current_loss=0.02768 | best_loss=0.02742
Epoch 12/80: current_loss=0.02765 | best_loss=0.02742
Epoch 13/80: current_loss=0.02786 | best_loss=0.02742
Epoch 14/80: current_loss=0.02788 | best_loss=0.02742
Epoch 15/80: current_loss=0.02742 | best_loss=0.02742
Epoch 16/80: current_loss=0.02760 | best_loss=0.02742
Epoch 17/80: current_loss=0.02798 | best_loss=0.02742
Epoch 18/80: current_loss=0.02777 | best_loss=0.02742
Epoch 19/80: current_loss=0.02764 | best_loss=0.02742
Epoch 20/80: current_loss=0.02788 | best_loss=0.02742
Epoch 21/80: current_loss=0.02788 | best_loss=0.02742
Epoch 22/80: current_loss=0.02758 | best_loss=0.02742
Epoch 23/80: current_loss=0.02752 | best_loss=0.02742
Epoch 24/80: current_loss=0.02793 | best_loss=0.02742
Epoch 25/80: current_loss=0.02745 | best_loss=0.02742
Epoch 26/80: current_loss=0.02784 | best_loss=0.02742
Early Stopping at epoch 26
      explained_var=-0.00313 | mse_loss=0.02784
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02680 | best_loss=0.02680
Epoch 1/80: current_loss=0.02686 | best_loss=0.02680
Epoch 2/80: current_loss=0.02689 | best_loss=0.02680
Epoch 3/80: current_loss=0.02684 | best_loss=0.02680
Epoch 4/80: current_loss=0.02682 | best_loss=0.02680
Epoch 5/80: current_loss=0.02677 | best_loss=0.02677
Epoch 6/80: current_loss=0.02679 | best_loss=0.02677
Epoch 7/80: current_loss=0.02676 | best_loss=0.02676
Epoch 8/80: current_loss=0.02690 | best_loss=0.02676
Epoch 9/80: current_loss=0.02683 | best_loss=0.02676
Epoch 10/80: current_loss=0.02681 | best_loss=0.02676
Epoch 11/80: current_loss=0.02683 | best_loss=0.02676
Epoch 12/80: current_loss=0.02675 | best_loss=0.02675
Epoch 13/80: current_loss=0.02676 | best_loss=0.02675
Epoch 14/80: current_loss=0.02689 | best_loss=0.02675
Epoch 15/80: current_loss=0.02684 | best_loss=0.02675
Epoch 16/80: current_loss=0.02686 | best_loss=0.02675
Epoch 17/80: current_loss=0.02677 | best_loss=0.02675
Epoch 18/80: current_loss=0.02677 | best_loss=0.02675
Epoch 19/80: current_loss=0.02679 | best_loss=0.02675
Epoch 20/80: current_loss=0.02675 | best_loss=0.02675
Epoch 21/80: current_loss=0.02680 | best_loss=0.02675
Epoch 22/80: current_loss=0.02683 | best_loss=0.02675
Epoch 23/80: current_loss=0.02680 | best_loss=0.02675
Epoch 24/80: current_loss=0.02681 | best_loss=0.02675
Epoch 25/80: current_loss=0.02677 | best_loss=0.02675
Epoch 26/80: current_loss=0.02682 | best_loss=0.02675
Epoch 27/80: current_loss=0.02675 | best_loss=0.02675
Epoch 28/80: current_loss=0.02679 | best_loss=0.02675
Epoch 29/80: current_loss=0.02675 | best_loss=0.02675
Epoch 30/80: current_loss=0.02679 | best_loss=0.02675
Epoch 31/80: current_loss=0.02685 | best_loss=0.02675
Epoch 32/80: current_loss=0.02675 | best_loss=0.02675
Epoch 33/80: current_loss=0.02676 | best_loss=0.02675
Epoch 34/80: current_loss=0.02693 | best_loss=0.02675
Epoch 35/80: current_loss=0.02676 | best_loss=0.02675
Epoch 36/80: current_loss=0.02676 | best_loss=0.02675
Epoch 37/80: current_loss=0.02683 | best_loss=0.02675
Epoch 38/80: current_loss=0.02675 | best_loss=0.02675
Epoch 39/80: current_loss=0.02690 | best_loss=0.02675
Epoch 40/80: current_loss=0.02674 | best_loss=0.02674
Epoch 41/80: current_loss=0.02687 | best_loss=0.02674
Epoch 42/80: current_loss=0.02674 | best_loss=0.02674
Epoch 43/80: current_loss=0.02674 | best_loss=0.02674
Epoch 44/80: current_loss=0.02675 | best_loss=0.02674
Epoch 45/80: current_loss=0.02677 | best_loss=0.02674
Epoch 46/80: current_loss=0.02677 | best_loss=0.02674
Epoch 47/80: current_loss=0.02675 | best_loss=0.02674
Epoch 48/80: current_loss=0.02694 | best_loss=0.02674
Epoch 49/80: current_loss=0.02685 | best_loss=0.02674
Epoch 50/80: current_loss=0.02674 | best_loss=0.02674
Epoch 51/80: current_loss=0.02677 | best_loss=0.02674
Epoch 52/80: current_loss=0.02673 | best_loss=0.02673
Epoch 53/80: current_loss=0.02677 | best_loss=0.02673
Epoch 54/80: current_loss=0.02675 | best_loss=0.02673
Epoch 55/80: current_loss=0.02692 | best_loss=0.02673
Epoch 56/80: current_loss=0.02681 | best_loss=0.02673
Epoch 57/80: current_loss=0.02674 | best_loss=0.02673
Epoch 58/80: current_loss=0.02689 | best_loss=0.02673
Epoch 59/80: current_loss=0.02688 | best_loss=0.02673
Epoch 60/80: current_loss=0.02677 | best_loss=0.02673
Epoch 61/80: current_loss=0.02682 | best_loss=0.02673
Epoch 62/80: current_loss=0.02676 | best_loss=0.02673
Epoch 63/80: current_loss=0.02679 | best_loss=0.02673
Epoch 64/80: current_loss=0.02676 | best_loss=0.02673
Epoch 65/80: current_loss=0.02683 | best_loss=0.02673
Epoch 66/80: current_loss=0.02683 | best_loss=0.02673
Epoch 67/80: current_loss=0.02681 | best_loss=0.02673
Epoch 68/80: current_loss=0.02676 | best_loss=0.02673
Epoch 69/80: current_loss=0.02680 | best_loss=0.02673
Epoch 70/80: current_loss=0.02683 | best_loss=0.02673
Epoch 71/80: current_loss=0.02686 | best_loss=0.02673
Epoch 72/80: current_loss=0.02677 | best_loss=0.02673
Early Stopping at epoch 72
      explained_var=0.00213 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02982 | best_loss=0.02982
Epoch 1/80: current_loss=0.02975 | best_loss=0.02975
Epoch 2/80: current_loss=0.02980 | best_loss=0.02975
Epoch 3/80: current_loss=0.02992 | best_loss=0.02975
Epoch 4/80: current_loss=0.02974 | best_loss=0.02974
Epoch 5/80: current_loss=0.02974 | best_loss=0.02974
Epoch 6/80: current_loss=0.02973 | best_loss=0.02973
Epoch 7/80: current_loss=0.02975 | best_loss=0.02973
Epoch 8/80: current_loss=0.02974 | best_loss=0.02973
Epoch 9/80: current_loss=0.02983 | best_loss=0.02973
Epoch 10/80: current_loss=0.02977 | best_loss=0.02973
Epoch 11/80: current_loss=0.02973 | best_loss=0.02973
Epoch 12/80: current_loss=0.02976 | best_loss=0.02973
Epoch 13/80: current_loss=0.02979 | best_loss=0.02973
Epoch 14/80: current_loss=0.02972 | best_loss=0.02972
Epoch 15/80: current_loss=0.02973 | best_loss=0.02972
Epoch 16/80: current_loss=0.02973 | best_loss=0.02972
Epoch 17/80: current_loss=0.02977 | best_loss=0.02972
Epoch 18/80: current_loss=0.02975 | best_loss=0.02972
Epoch 19/80: current_loss=0.02977 | best_loss=0.02972
Epoch 20/80: current_loss=0.02976 | best_loss=0.02972
Epoch 21/80: current_loss=0.02973 | best_loss=0.02972
Epoch 22/80: current_loss=0.02979 | best_loss=0.02972
Epoch 23/80: current_loss=0.02974 | best_loss=0.02972
Epoch 24/80: current_loss=0.02975 | best_loss=0.02972
Epoch 25/80: current_loss=0.02973 | best_loss=0.02972
Epoch 26/80: current_loss=0.02972 | best_loss=0.02972
Epoch 27/80: current_loss=0.02977 | best_loss=0.02972
Epoch 28/80: current_loss=0.02973 | best_loss=0.02972
Epoch 29/80: current_loss=0.02974 | best_loss=0.02972
Epoch 30/80: current_loss=0.02974 | best_loss=0.02972
Epoch 31/80: current_loss=0.02975 | best_loss=0.02972
Epoch 32/80: current_loss=0.02979 | best_loss=0.02972
Epoch 33/80: current_loss=0.02972 | best_loss=0.02972
Epoch 34/80: current_loss=0.02973 | best_loss=0.02972
Early Stopping at epoch 34
      explained_var=0.00112 | mse_loss=0.02873
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=0.00118| avg_loss=0.02694
----------------------------------------------


----------------------------------------------
Params for Trial 64
{'learning_rate': 1e-05, 'weight_decay': 0.0006650153273863279, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.15276 | best_loss=0.15276
Epoch 1/80: current_loss=0.12463 | best_loss=0.12463
Epoch 2/80: current_loss=0.09987 | best_loss=0.09987
Epoch 3/80: current_loss=0.07798 | best_loss=0.07798
Epoch 4/80: current_loss=0.06017 | best_loss=0.06017
Epoch 5/80: current_loss=0.04757 | best_loss=0.04757
Epoch 6/80: current_loss=0.04060 | best_loss=0.04060
Epoch 7/80: current_loss=0.03796 | best_loss=0.03796
Epoch 8/80: current_loss=0.03738 | best_loss=0.03738
Epoch 9/80: current_loss=0.03718 | best_loss=0.03718
Epoch 10/80: current_loss=0.03699 | best_loss=0.03699
Epoch 11/80: current_loss=0.03677 | best_loss=0.03677
Epoch 12/80: current_loss=0.03656 | best_loss=0.03656
Epoch 13/80: current_loss=0.03639 | best_loss=0.03639
Epoch 14/80: current_loss=0.03612 | best_loss=0.03612
Epoch 15/80: current_loss=0.03590 | best_loss=0.03590
Epoch 16/80: current_loss=0.03567 | best_loss=0.03567
Epoch 17/80: current_loss=0.03550 | best_loss=0.03550
Epoch 18/80: current_loss=0.03536 | best_loss=0.03536
Epoch 19/80: current_loss=0.03513 | best_loss=0.03513
Epoch 20/80: current_loss=0.03499 | best_loss=0.03499
Epoch 21/80: current_loss=0.03479 | best_loss=0.03479
Epoch 22/80: current_loss=0.03459 | best_loss=0.03459
Epoch 23/80: current_loss=0.03439 | best_loss=0.03439
Epoch 24/80: current_loss=0.03424 | best_loss=0.03424
Epoch 25/80: current_loss=0.03408 | best_loss=0.03408
Epoch 26/80: current_loss=0.03394 | best_loss=0.03394
Epoch 27/80: current_loss=0.03379 | best_loss=0.03379
Epoch 28/80: current_loss=0.03362 | best_loss=0.03362
Epoch 29/80: current_loss=0.03351 | best_loss=0.03351
Epoch 30/80: current_loss=0.03338 | best_loss=0.03338
Epoch 31/80: current_loss=0.03322 | best_loss=0.03322
Epoch 32/80: current_loss=0.03306 | best_loss=0.03306
Epoch 33/80: current_loss=0.03293 | best_loss=0.03293
Epoch 34/80: current_loss=0.03287 | best_loss=0.03287
Epoch 35/80: current_loss=0.03274 | best_loss=0.03274
Epoch 36/80: current_loss=0.03261 | best_loss=0.03261
Epoch 37/80: current_loss=0.03246 | best_loss=0.03246
Epoch 38/80: current_loss=0.03237 | best_loss=0.03237
Epoch 39/80: current_loss=0.03223 | best_loss=0.03223
Epoch 40/80: current_loss=0.03211 | best_loss=0.03211
Epoch 41/80: current_loss=0.03203 | best_loss=0.03203
Epoch 42/80: current_loss=0.03196 | best_loss=0.03196
Epoch 43/80: current_loss=0.03186 | best_loss=0.03186
Epoch 44/80: current_loss=0.03177 | best_loss=0.03177
Epoch 45/80: current_loss=0.03167 | best_loss=0.03167
Epoch 46/80: current_loss=0.03157 | best_loss=0.03157
Epoch 47/80: current_loss=0.03153 | best_loss=0.03153
Epoch 48/80: current_loss=0.03148 | best_loss=0.03148
Epoch 49/80: current_loss=0.03141 | best_loss=0.03141
Epoch 50/80: current_loss=0.03132 | best_loss=0.03132
Epoch 51/80: current_loss=0.03124 | best_loss=0.03124
Epoch 52/80: current_loss=0.03114 | best_loss=0.03114
Epoch 53/80: current_loss=0.03112 | best_loss=0.03112
Epoch 54/80: current_loss=0.03101 | best_loss=0.03101
Epoch 55/80: current_loss=0.03096 | best_loss=0.03096
Epoch 56/80: current_loss=0.03092 | best_loss=0.03092
Epoch 57/80: current_loss=0.03087 | best_loss=0.03087
Epoch 58/80: current_loss=0.03079 | best_loss=0.03079
Epoch 59/80: current_loss=0.03073 | best_loss=0.03073
Epoch 60/80: current_loss=0.03067 | best_loss=0.03067
Epoch 61/80: current_loss=0.03062 | best_loss=0.03062
Epoch 62/80: current_loss=0.03058 | best_loss=0.03058
Epoch 63/80: current_loss=0.03050 | best_loss=0.03050
Epoch 64/80: current_loss=0.03047 | best_loss=0.03047
Epoch 65/80: current_loss=0.03040 | best_loss=0.03040
Epoch 66/80: current_loss=0.03037 | best_loss=0.03037
Epoch 67/80: current_loss=0.03033 | best_loss=0.03033
Epoch 68/80: current_loss=0.03031 | best_loss=0.03031
Epoch 69/80: current_loss=0.03027 | best_loss=0.03027
Epoch 70/80: current_loss=0.03024 | best_loss=0.03024
Epoch 71/80: current_loss=0.03021 | best_loss=0.03021
Epoch 72/80: current_loss=0.03015 | best_loss=0.03015
Epoch 73/80: current_loss=0.03013 | best_loss=0.03013
Epoch 74/80: current_loss=0.03008 | best_loss=0.03008
Epoch 75/80: current_loss=0.03005 | best_loss=0.03005
Epoch 76/80: current_loss=0.03005 | best_loss=0.03005
Epoch 77/80: current_loss=0.03001 | best_loss=0.03001
Epoch 78/80: current_loss=0.02998 | best_loss=0.02998
Epoch 79/80: current_loss=0.02996 | best_loss=0.02996
      explained_var=-0.02526 | mse_loss=0.02886

----------------------------------------------
Params for Trial 65
{'learning_rate': 0.001, 'weight_decay': 0.001926116044501785, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03238 | best_loss=0.03238
Epoch 1/80: current_loss=0.03018 | best_loss=0.03018
Epoch 2/80: current_loss=0.02967 | best_loss=0.02967
Epoch 3/80: current_loss=0.02927 | best_loss=0.02927
Epoch 4/80: current_loss=0.02929 | best_loss=0.02927
Epoch 5/80: current_loss=0.02908 | best_loss=0.02908
Epoch 6/80: current_loss=0.02928 | best_loss=0.02908
Epoch 7/80: current_loss=0.03043 | best_loss=0.02908
Epoch 8/80: current_loss=0.02909 | best_loss=0.02908
Epoch 9/80: current_loss=0.02907 | best_loss=0.02907
Epoch 10/80: current_loss=0.02901 | best_loss=0.02901
Epoch 11/80: current_loss=0.02936 | best_loss=0.02901
Epoch 12/80: current_loss=0.02899 | best_loss=0.02899
Epoch 13/80: current_loss=0.02944 | best_loss=0.02899
Epoch 14/80: current_loss=0.02964 | best_loss=0.02899
Epoch 15/80: current_loss=0.02915 | best_loss=0.02899
Epoch 16/80: current_loss=0.02906 | best_loss=0.02899
Epoch 17/80: current_loss=0.02947 | best_loss=0.02899
Epoch 18/80: current_loss=0.02977 | best_loss=0.02899
Epoch 19/80: current_loss=0.02929 | best_loss=0.02899
Epoch 20/80: current_loss=0.02915 | best_loss=0.02899
Epoch 21/80: current_loss=0.02907 | best_loss=0.02899
Epoch 22/80: current_loss=0.02940 | best_loss=0.02899
Epoch 23/80: current_loss=0.02917 | best_loss=0.02899
Epoch 24/80: current_loss=0.02949 | best_loss=0.02899
Epoch 25/80: current_loss=0.02915 | best_loss=0.02899
Epoch 26/80: current_loss=0.02968 | best_loss=0.02899
Epoch 27/80: current_loss=0.02914 | best_loss=0.02899
Epoch 28/80: current_loss=0.02905 | best_loss=0.02899
Epoch 29/80: current_loss=0.02922 | best_loss=0.02899
Epoch 30/80: current_loss=0.02908 | best_loss=0.02899
Epoch 31/80: current_loss=0.02911 | best_loss=0.02899
Epoch 32/80: current_loss=0.02962 | best_loss=0.02899
Early Stopping at epoch 32
      explained_var=0.00203 | mse_loss=0.02809

----------------------------------------------
Params for Trial 66
{'learning_rate': 0.0001, 'weight_decay': 0.00020765159625861943, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03072 | best_loss=0.03072
Epoch 1/80: current_loss=0.02974 | best_loss=0.02974
Epoch 2/80: current_loss=0.02924 | best_loss=0.02924
Epoch 3/80: current_loss=0.02900 | best_loss=0.02900
Epoch 4/80: current_loss=0.02897 | best_loss=0.02897
Epoch 5/80: current_loss=0.02906 | best_loss=0.02897
Epoch 6/80: current_loss=0.02902 | best_loss=0.02897
Epoch 7/80: current_loss=0.02886 | best_loss=0.02886
Epoch 8/80: current_loss=0.02895 | best_loss=0.02886
Epoch 9/80: current_loss=0.02894 | best_loss=0.02886
Epoch 10/80: current_loss=0.02933 | best_loss=0.02886
Epoch 11/80: current_loss=0.02903 | best_loss=0.02886
Epoch 12/80: current_loss=0.02929 | best_loss=0.02886
Epoch 13/80: current_loss=0.02896 | best_loss=0.02886
Epoch 14/80: current_loss=0.02891 | best_loss=0.02886
Epoch 15/80: current_loss=0.02889 | best_loss=0.02886
Epoch 16/80: current_loss=0.02881 | best_loss=0.02881
Epoch 17/80: current_loss=0.02881 | best_loss=0.02881
Epoch 18/80: current_loss=0.02883 | best_loss=0.02881
Epoch 19/80: current_loss=0.02912 | best_loss=0.02881
Epoch 20/80: current_loss=0.02885 | best_loss=0.02881
Epoch 21/80: current_loss=0.02886 | best_loss=0.02881
Epoch 22/80: current_loss=0.02898 | best_loss=0.02881
Epoch 23/80: current_loss=0.02890 | best_loss=0.02881
Epoch 24/80: current_loss=0.02890 | best_loss=0.02881
Epoch 25/80: current_loss=0.02896 | best_loss=0.02881
Epoch 26/80: current_loss=0.02902 | best_loss=0.02881
Epoch 27/80: current_loss=0.02893 | best_loss=0.02881
Epoch 28/80: current_loss=0.02902 | best_loss=0.02881
Epoch 29/80: current_loss=0.02899 | best_loss=0.02881
Epoch 30/80: current_loss=0.02892 | best_loss=0.02881
Epoch 31/80: current_loss=0.02890 | best_loss=0.02881
Epoch 32/80: current_loss=0.02906 | best_loss=0.02881
Epoch 33/80: current_loss=0.02892 | best_loss=0.02881
Epoch 34/80: current_loss=0.02891 | best_loss=0.02881
Epoch 35/80: current_loss=0.02894 | best_loss=0.02881
Epoch 36/80: current_loss=0.02907 | best_loss=0.02881
Early Stopping at epoch 36
      explained_var=0.00771 | mse_loss=0.02796
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02506 | best_loss=0.02506
Epoch 1/80: current_loss=0.02506 | best_loss=0.02506
Epoch 2/80: current_loss=0.02521 | best_loss=0.02506
Epoch 3/80: current_loss=0.02502 | best_loss=0.02502
Epoch 4/80: current_loss=0.02498 | best_loss=0.02498
Epoch 5/80: current_loss=0.02495 | best_loss=0.02495
Epoch 6/80: current_loss=0.02495 | best_loss=0.02495
Epoch 7/80: current_loss=0.02499 | best_loss=0.02495
Epoch 8/80: current_loss=0.02497 | best_loss=0.02495
Epoch 9/80: current_loss=0.02492 | best_loss=0.02492
Epoch 10/80: current_loss=0.02492 | best_loss=0.02492
Epoch 11/80: current_loss=0.02496 | best_loss=0.02492
Epoch 12/80: current_loss=0.02512 | best_loss=0.02492
Epoch 13/80: current_loss=0.02495 | best_loss=0.02492
Epoch 14/80: current_loss=0.02513 | best_loss=0.02492
Epoch 15/80: current_loss=0.02495 | best_loss=0.02492
Epoch 16/80: current_loss=0.02492 | best_loss=0.02492
Epoch 17/80: current_loss=0.02490 | best_loss=0.02490
Epoch 18/80: current_loss=0.02492 | best_loss=0.02490
Epoch 19/80: current_loss=0.02494 | best_loss=0.02490
Epoch 20/80: current_loss=0.02491 | best_loss=0.02490
Epoch 21/80: current_loss=0.02494 | best_loss=0.02490
Epoch 22/80: current_loss=0.02499 | best_loss=0.02490
Epoch 23/80: current_loss=0.02494 | best_loss=0.02490
Epoch 24/80: current_loss=0.02498 | best_loss=0.02490
Epoch 25/80: current_loss=0.02501 | best_loss=0.02490
Epoch 26/80: current_loss=0.02502 | best_loss=0.02490
Epoch 27/80: current_loss=0.02505 | best_loss=0.02490
Epoch 28/80: current_loss=0.02510 | best_loss=0.02490
Epoch 29/80: current_loss=0.02521 | best_loss=0.02490
Epoch 30/80: current_loss=0.02504 | best_loss=0.02490
Epoch 31/80: current_loss=0.02519 | best_loss=0.02490
Epoch 32/80: current_loss=0.02505 | best_loss=0.02490
Epoch 33/80: current_loss=0.02510 | best_loss=0.02490
Epoch 34/80: current_loss=0.02504 | best_loss=0.02490
Epoch 35/80: current_loss=0.02506 | best_loss=0.02490
Epoch 36/80: current_loss=0.02506 | best_loss=0.02490
Epoch 37/80: current_loss=0.02537 | best_loss=0.02490
Early Stopping at epoch 37
      explained_var=0.00164 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02835 | best_loss=0.02835
Epoch 1/80: current_loss=0.02732 | best_loss=0.02732
Epoch 2/80: current_loss=0.02746 | best_loss=0.02732
Epoch 3/80: current_loss=0.02751 | best_loss=0.02732
Epoch 4/80: current_loss=0.02792 | best_loss=0.02732
Epoch 5/80: current_loss=0.02750 | best_loss=0.02732
Epoch 6/80: current_loss=0.02834 | best_loss=0.02732
Epoch 7/80: current_loss=0.02768 | best_loss=0.02732
Epoch 8/80: current_loss=0.02803 | best_loss=0.02732
Epoch 9/80: current_loss=0.02751 | best_loss=0.02732
Epoch 10/80: current_loss=0.02734 | best_loss=0.02732
Epoch 11/80: current_loss=0.02817 | best_loss=0.02732
Epoch 12/80: current_loss=0.02778 | best_loss=0.02732
Epoch 13/80: current_loss=0.02754 | best_loss=0.02732
Epoch 14/80: current_loss=0.02778 | best_loss=0.02732
Epoch 15/80: current_loss=0.02806 | best_loss=0.02732
Epoch 16/80: current_loss=0.02775 | best_loss=0.02732
Epoch 17/80: current_loss=0.02796 | best_loss=0.02732
Epoch 18/80: current_loss=0.02765 | best_loss=0.02732
Epoch 19/80: current_loss=0.02753 | best_loss=0.02732
Epoch 20/80: current_loss=0.02790 | best_loss=0.02732
Epoch 21/80: current_loss=0.02735 | best_loss=0.02732
Early Stopping at epoch 21
      explained_var=-0.00491 | mse_loss=0.02772
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02697 | best_loss=0.02697
Epoch 1/80: current_loss=0.02682 | best_loss=0.02682
Epoch 2/80: current_loss=0.02684 | best_loss=0.02682
Epoch 3/80: current_loss=0.02689 | best_loss=0.02682
Epoch 4/80: current_loss=0.02690 | best_loss=0.02682
Epoch 5/80: current_loss=0.02695 | best_loss=0.02682
Epoch 6/80: current_loss=0.02692 | best_loss=0.02682
Epoch 7/80: current_loss=0.02677 | best_loss=0.02677
Epoch 8/80: current_loss=0.02674 | best_loss=0.02674
Epoch 9/80: current_loss=0.02677 | best_loss=0.02674
Epoch 10/80: current_loss=0.02675 | best_loss=0.02674
Epoch 11/80: current_loss=0.02675 | best_loss=0.02674
Epoch 12/80: current_loss=0.02686 | best_loss=0.02674
Epoch 13/80: current_loss=0.02707 | best_loss=0.02674
Epoch 14/80: current_loss=0.02683 | best_loss=0.02674
Epoch 15/80: current_loss=0.02685 | best_loss=0.02674
Epoch 16/80: current_loss=0.02690 | best_loss=0.02674
Epoch 17/80: current_loss=0.02676 | best_loss=0.02674
Epoch 18/80: current_loss=0.02680 | best_loss=0.02674
Epoch 19/80: current_loss=0.02677 | best_loss=0.02674
Epoch 20/80: current_loss=0.02675 | best_loss=0.02674
Epoch 21/80: current_loss=0.02684 | best_loss=0.02674
Epoch 22/80: current_loss=0.02678 | best_loss=0.02674
Epoch 23/80: current_loss=0.02687 | best_loss=0.02674
Epoch 24/80: current_loss=0.02703 | best_loss=0.02674
Epoch 25/80: current_loss=0.02681 | best_loss=0.02674
Epoch 26/80: current_loss=0.02681 | best_loss=0.02674
Epoch 27/80: current_loss=0.02674 | best_loss=0.02674
Epoch 28/80: current_loss=0.02679 | best_loss=0.02674
Early Stopping at epoch 28
      explained_var=0.00136 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02989 | best_loss=0.02989
Epoch 1/80: current_loss=0.02993 | best_loss=0.02989
Epoch 2/80: current_loss=0.03015 | best_loss=0.02989
Epoch 3/80: current_loss=0.02981 | best_loss=0.02981
Epoch 4/80: current_loss=0.02974 | best_loss=0.02974
Epoch 5/80: current_loss=0.03008 | best_loss=0.02974
Epoch 6/80: current_loss=0.02983 | best_loss=0.02974
Epoch 7/80: current_loss=0.02977 | best_loss=0.02974
Epoch 8/80: current_loss=0.02959 | best_loss=0.02959
Epoch 9/80: current_loss=0.02961 | best_loss=0.02959
Epoch 10/80: current_loss=0.02968 | best_loss=0.02959
Epoch 11/80: current_loss=0.02969 | best_loss=0.02959
Epoch 12/80: current_loss=0.02975 | best_loss=0.02959
Epoch 13/80: current_loss=0.02979 | best_loss=0.02959
Epoch 14/80: current_loss=0.02981 | best_loss=0.02959
Epoch 15/80: current_loss=0.02978 | best_loss=0.02959
Epoch 16/80: current_loss=0.02979 | best_loss=0.02959
Epoch 17/80: current_loss=0.02974 | best_loss=0.02959
Epoch 18/80: current_loss=0.02982 | best_loss=0.02959
Epoch 19/80: current_loss=0.02978 | best_loss=0.02959
Epoch 20/80: current_loss=0.02983 | best_loss=0.02959
Epoch 21/80: current_loss=0.02983 | best_loss=0.02959
Epoch 22/80: current_loss=0.02998 | best_loss=0.02959
Epoch 23/80: current_loss=0.02978 | best_loss=0.02959
Epoch 24/80: current_loss=0.02977 | best_loss=0.02959
Epoch 25/80: current_loss=0.02969 | best_loss=0.02959
Epoch 26/80: current_loss=0.02973 | best_loss=0.02959
Epoch 27/80: current_loss=0.02965 | best_loss=0.02959
Epoch 28/80: current_loss=0.02975 | best_loss=0.02959
Early Stopping at epoch 28
      explained_var=0.00471 | mse_loss=0.02862
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.00210| avg_loss=0.02688
----------------------------------------------


----------------------------------------------
Params for Trial 67
{'learning_rate': 0.0001, 'weight_decay': 0.0002944662077417735, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03243 | best_loss=0.03243
Epoch 1/80: current_loss=0.03090 | best_loss=0.03090
Epoch 2/80: current_loss=0.02991 | best_loss=0.02991
Epoch 3/80: current_loss=0.02937 | best_loss=0.02937
Epoch 4/80: current_loss=0.02914 | best_loss=0.02914
Epoch 5/80: current_loss=0.02917 | best_loss=0.02914
Epoch 6/80: current_loss=0.02934 | best_loss=0.02914
Epoch 7/80: current_loss=0.02906 | best_loss=0.02906
Epoch 8/80: current_loss=0.02914 | best_loss=0.02906
Epoch 9/80: current_loss=0.02935 | best_loss=0.02906
Epoch 10/80: current_loss=0.02964 | best_loss=0.02906
Epoch 11/80: current_loss=0.02908 | best_loss=0.02906
Epoch 12/80: current_loss=0.02912 | best_loss=0.02906
Epoch 13/80: current_loss=0.02910 | best_loss=0.02906
Epoch 14/80: current_loss=0.02909 | best_loss=0.02906
Epoch 15/80: current_loss=0.02909 | best_loss=0.02906
Epoch 16/80: current_loss=0.02919 | best_loss=0.02906
Epoch 17/80: current_loss=0.02911 | best_loss=0.02906
Epoch 18/80: current_loss=0.02936 | best_loss=0.02906
Epoch 19/80: current_loss=0.02914 | best_loss=0.02906
Epoch 20/80: current_loss=0.02908 | best_loss=0.02906
Epoch 21/80: current_loss=0.02919 | best_loss=0.02906
Epoch 22/80: current_loss=0.02949 | best_loss=0.02906
Epoch 23/80: current_loss=0.02905 | best_loss=0.02905
Epoch 24/80: current_loss=0.02910 | best_loss=0.02905
Epoch 25/80: current_loss=0.02903 | best_loss=0.02903
Epoch 26/80: current_loss=0.02904 | best_loss=0.02903
Epoch 27/80: current_loss=0.02901 | best_loss=0.02901
Epoch 28/80: current_loss=0.02905 | best_loss=0.02901
Epoch 29/80: current_loss=0.02909 | best_loss=0.02901
Epoch 30/80: current_loss=0.02896 | best_loss=0.02896
Epoch 31/80: current_loss=0.02904 | best_loss=0.02896
Epoch 32/80: current_loss=0.02910 | best_loss=0.02896
Epoch 33/80: current_loss=0.02929 | best_loss=0.02896
Epoch 34/80: current_loss=0.02898 | best_loss=0.02896
Epoch 35/80: current_loss=0.02923 | best_loss=0.02896
Epoch 36/80: current_loss=0.02902 | best_loss=0.02896
Epoch 37/80: current_loss=0.02914 | best_loss=0.02896
Epoch 38/80: current_loss=0.02931 | best_loss=0.02896
Epoch 39/80: current_loss=0.02909 | best_loss=0.02896
Epoch 40/80: current_loss=0.02902 | best_loss=0.02896
Epoch 41/80: current_loss=0.02899 | best_loss=0.02896
Epoch 42/80: current_loss=0.02904 | best_loss=0.02896
Epoch 43/80: current_loss=0.02895 | best_loss=0.02895
Epoch 44/80: current_loss=0.02889 | best_loss=0.02889
Epoch 45/80: current_loss=0.02904 | best_loss=0.02889
Epoch 46/80: current_loss=0.02901 | best_loss=0.02889
Epoch 47/80: current_loss=0.02914 | best_loss=0.02889
Epoch 48/80: current_loss=0.02913 | best_loss=0.02889
Epoch 49/80: current_loss=0.02890 | best_loss=0.02889
Epoch 50/80: current_loss=0.02889 | best_loss=0.02889
Epoch 51/80: current_loss=0.02895 | best_loss=0.02889
Epoch 52/80: current_loss=0.02890 | best_loss=0.02889
Epoch 53/80: current_loss=0.02891 | best_loss=0.02889
Epoch 54/80: current_loss=0.02893 | best_loss=0.02889
Epoch 55/80: current_loss=0.02892 | best_loss=0.02889
Epoch 56/80: current_loss=0.02890 | best_loss=0.02889
Epoch 57/80: current_loss=0.02910 | best_loss=0.02889
Epoch 58/80: current_loss=0.02912 | best_loss=0.02889
Epoch 59/80: current_loss=0.02917 | best_loss=0.02889
Epoch 60/80: current_loss=0.02894 | best_loss=0.02889
Epoch 61/80: current_loss=0.02899 | best_loss=0.02889
Epoch 62/80: current_loss=0.02898 | best_loss=0.02889
Epoch 63/80: current_loss=0.02923 | best_loss=0.02889
Epoch 64/80: current_loss=0.02914 | best_loss=0.02889
Epoch 65/80: current_loss=0.02914 | best_loss=0.02889
Epoch 66/80: current_loss=0.02897 | best_loss=0.02889
Epoch 67/80: current_loss=0.02897 | best_loss=0.02889
Epoch 68/80: current_loss=0.02893 | best_loss=0.02889
Epoch 69/80: current_loss=0.02891 | best_loss=0.02889
Epoch 70/80: current_loss=0.02894 | best_loss=0.02889
Early Stopping at epoch 70
      explained_var=0.00447 | mse_loss=0.02803
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02535 | best_loss=0.02535
Epoch 1/80: current_loss=0.02499 | best_loss=0.02499
Epoch 2/80: current_loss=0.02557 | best_loss=0.02499
Epoch 3/80: current_loss=0.02492 | best_loss=0.02492
Epoch 4/80: current_loss=0.02492 | best_loss=0.02492
Epoch 5/80: current_loss=0.02489 | best_loss=0.02489
Epoch 6/80: current_loss=0.02519 | best_loss=0.02489
Epoch 7/80: current_loss=0.02528 | best_loss=0.02489
Epoch 8/80: current_loss=0.02496 | best_loss=0.02489
Epoch 9/80: current_loss=0.02497 | best_loss=0.02489
Epoch 10/80: current_loss=0.02504 | best_loss=0.02489
Epoch 11/80: current_loss=0.02511 | best_loss=0.02489
Epoch 12/80: current_loss=0.02506 | best_loss=0.02489
Epoch 13/80: current_loss=0.02503 | best_loss=0.02489
Epoch 14/80: current_loss=0.02495 | best_loss=0.02489
Epoch 15/80: current_loss=0.02498 | best_loss=0.02489
Epoch 16/80: current_loss=0.02497 | best_loss=0.02489
Epoch 17/80: current_loss=0.02502 | best_loss=0.02489
Epoch 18/80: current_loss=0.02512 | best_loss=0.02489
Epoch 19/80: current_loss=0.02519 | best_loss=0.02489
Epoch 20/80: current_loss=0.02510 | best_loss=0.02489
Epoch 21/80: current_loss=0.02515 | best_loss=0.02489
Epoch 22/80: current_loss=0.02501 | best_loss=0.02489
Epoch 23/80: current_loss=0.02518 | best_loss=0.02489
Epoch 24/80: current_loss=0.02501 | best_loss=0.02489
Epoch 25/80: current_loss=0.02529 | best_loss=0.02489
Early Stopping at epoch 25
      explained_var=0.00213 | mse_loss=0.02519

----------------------------------------------
Params for Trial 68
{'learning_rate': 0.0001, 'weight_decay': 0.0013914243371702805, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03276 | best_loss=0.03276
Epoch 1/80: current_loss=0.03137 | best_loss=0.03137
Epoch 2/80: current_loss=0.03067 | best_loss=0.03067
Epoch 3/80: current_loss=0.03101 | best_loss=0.03067
Epoch 4/80: current_loss=0.02957 | best_loss=0.02957
Epoch 5/80: current_loss=0.02934 | best_loss=0.02934
Epoch 6/80: current_loss=0.02922 | best_loss=0.02922
Epoch 7/80: current_loss=0.02908 | best_loss=0.02908
Epoch 8/80: current_loss=0.02908 | best_loss=0.02908
Epoch 9/80: current_loss=0.02901 | best_loss=0.02901
Epoch 10/80: current_loss=0.02920 | best_loss=0.02901
Epoch 11/80: current_loss=0.02897 | best_loss=0.02897
Epoch 12/80: current_loss=0.02893 | best_loss=0.02893
Epoch 13/80: current_loss=0.02915 | best_loss=0.02893
Epoch 14/80: current_loss=0.02910 | best_loss=0.02893
Epoch 15/80: current_loss=0.02898 | best_loss=0.02893
Epoch 16/80: current_loss=0.02899 | best_loss=0.02893
Epoch 17/80: current_loss=0.02892 | best_loss=0.02892
Epoch 18/80: current_loss=0.02898 | best_loss=0.02892
Epoch 19/80: current_loss=0.02891 | best_loss=0.02891
Epoch 20/80: current_loss=0.02922 | best_loss=0.02891
Epoch 21/80: current_loss=0.02903 | best_loss=0.02891
Epoch 22/80: current_loss=0.02896 | best_loss=0.02891
Epoch 23/80: current_loss=0.02927 | best_loss=0.02891
Epoch 24/80: current_loss=0.02893 | best_loss=0.02891
Epoch 25/80: current_loss=0.02901 | best_loss=0.02891
Epoch 26/80: current_loss=0.02889 | best_loss=0.02889
Epoch 27/80: current_loss=0.02888 | best_loss=0.02888
Epoch 28/80: current_loss=0.02887 | best_loss=0.02887
Epoch 29/80: current_loss=0.02894 | best_loss=0.02887
Epoch 30/80: current_loss=0.02893 | best_loss=0.02887
Epoch 31/80: current_loss=0.02888 | best_loss=0.02887
Epoch 32/80: current_loss=0.02893 | best_loss=0.02887
Epoch 33/80: current_loss=0.02904 | best_loss=0.02887
Epoch 34/80: current_loss=0.02890 | best_loss=0.02887
Epoch 35/80: current_loss=0.02886 | best_loss=0.02886
Epoch 36/80: current_loss=0.02895 | best_loss=0.02886
Epoch 37/80: current_loss=0.02888 | best_loss=0.02886
Epoch 38/80: current_loss=0.02889 | best_loss=0.02886
Epoch 39/80: current_loss=0.02891 | best_loss=0.02886
Epoch 40/80: current_loss=0.02890 | best_loss=0.02886
Epoch 41/80: current_loss=0.02896 | best_loss=0.02886
Epoch 42/80: current_loss=0.02894 | best_loss=0.02886
Epoch 43/80: current_loss=0.02895 | best_loss=0.02886
Epoch 44/80: current_loss=0.02893 | best_loss=0.02886
Epoch 45/80: current_loss=0.02895 | best_loss=0.02886
Epoch 46/80: current_loss=0.02894 | best_loss=0.02886
Epoch 47/80: current_loss=0.02895 | best_loss=0.02886
Epoch 48/80: current_loss=0.02912 | best_loss=0.02886
Epoch 49/80: current_loss=0.02897 | best_loss=0.02886
Epoch 50/80: current_loss=0.02898 | best_loss=0.02886
Epoch 51/80: current_loss=0.02915 | best_loss=0.02886
Epoch 52/80: current_loss=0.02909 | best_loss=0.02886
Epoch 53/80: current_loss=0.02907 | best_loss=0.02886
Epoch 54/80: current_loss=0.02908 | best_loss=0.02886
Epoch 55/80: current_loss=0.02923 | best_loss=0.02886
Early Stopping at epoch 55
      explained_var=0.00534 | mse_loss=0.02800
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02491 | best_loss=0.02491
Epoch 1/80: current_loss=0.02495 | best_loss=0.02491
Epoch 2/80: current_loss=0.02498 | best_loss=0.02491
Epoch 3/80: current_loss=0.02503 | best_loss=0.02491
Epoch 4/80: current_loss=0.02498 | best_loss=0.02491
Epoch 5/80: current_loss=0.02583 | best_loss=0.02491
Epoch 6/80: current_loss=0.02515 | best_loss=0.02491
Epoch 7/80: current_loss=0.02491 | best_loss=0.02491
Epoch 8/80: current_loss=0.02490 | best_loss=0.02490
Epoch 9/80: current_loss=0.02543 | best_loss=0.02490
Epoch 10/80: current_loss=0.02584 | best_loss=0.02490
Epoch 11/80: current_loss=0.02511 | best_loss=0.02490
Epoch 12/80: current_loss=0.02494 | best_loss=0.02490
Epoch 13/80: current_loss=0.02502 | best_loss=0.02490
Epoch 14/80: current_loss=0.02512 | best_loss=0.02490
Epoch 15/80: current_loss=0.02522 | best_loss=0.02490
Epoch 16/80: current_loss=0.02499 | best_loss=0.02490
Epoch 17/80: current_loss=0.02490 | best_loss=0.02490
Epoch 18/80: current_loss=0.02495 | best_loss=0.02490
Epoch 19/80: current_loss=0.02503 | best_loss=0.02490
Epoch 20/80: current_loss=0.02492 | best_loss=0.02490
Epoch 21/80: current_loss=0.02499 | best_loss=0.02490
Epoch 22/80: current_loss=0.02499 | best_loss=0.02490
Epoch 23/80: current_loss=0.02516 | best_loss=0.02490
Epoch 24/80: current_loss=0.02491 | best_loss=0.02490
Epoch 25/80: current_loss=0.02488 | best_loss=0.02488
Epoch 26/80: current_loss=0.02491 | best_loss=0.02488
Epoch 27/80: current_loss=0.02577 | best_loss=0.02488
Epoch 28/80: current_loss=0.02490 | best_loss=0.02488
Epoch 29/80: current_loss=0.02493 | best_loss=0.02488
Epoch 30/80: current_loss=0.02521 | best_loss=0.02488
Epoch 31/80: current_loss=0.02486 | best_loss=0.02486
Epoch 32/80: current_loss=0.02501 | best_loss=0.02486
Epoch 33/80: current_loss=0.02498 | best_loss=0.02486
Epoch 34/80: current_loss=0.02562 | best_loss=0.02486
Epoch 35/80: current_loss=0.02503 | best_loss=0.02486
Epoch 36/80: current_loss=0.02490 | best_loss=0.02486
Epoch 37/80: current_loss=0.02510 | best_loss=0.02486
Epoch 38/80: current_loss=0.02497 | best_loss=0.02486
Epoch 39/80: current_loss=0.02491 | best_loss=0.02486
Epoch 40/80: current_loss=0.02545 | best_loss=0.02486
Epoch 41/80: current_loss=0.02498 | best_loss=0.02486
Epoch 42/80: current_loss=0.02519 | best_loss=0.02486
Epoch 43/80: current_loss=0.02493 | best_loss=0.02486
Epoch 44/80: current_loss=0.02494 | best_loss=0.02486
Epoch 45/80: current_loss=0.02490 | best_loss=0.02486
Epoch 46/80: current_loss=0.02493 | best_loss=0.02486
Epoch 47/80: current_loss=0.02500 | best_loss=0.02486
Epoch 48/80: current_loss=0.02491 | best_loss=0.02486
Epoch 49/80: current_loss=0.02494 | best_loss=0.02486
Epoch 50/80: current_loss=0.02496 | best_loss=0.02486
Epoch 51/80: current_loss=0.02494 | best_loss=0.02486
Early Stopping at epoch 51
      explained_var=0.00228 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02866 | best_loss=0.02866
Epoch 1/80: current_loss=0.02754 | best_loss=0.02754
Epoch 2/80: current_loss=0.02772 | best_loss=0.02754
Epoch 3/80: current_loss=0.02832 | best_loss=0.02754
Epoch 4/80: current_loss=0.02756 | best_loss=0.02754
Epoch 5/80: current_loss=0.02839 | best_loss=0.02754
Epoch 6/80: current_loss=0.02749 | best_loss=0.02749
Epoch 7/80: current_loss=0.02860 | best_loss=0.02749
Epoch 8/80: current_loss=0.02799 | best_loss=0.02749
Epoch 9/80: current_loss=0.02814 | best_loss=0.02749
Epoch 10/80: current_loss=0.02739 | best_loss=0.02739
Epoch 11/80: current_loss=0.02807 | best_loss=0.02739
Epoch 12/80: current_loss=0.02749 | best_loss=0.02739
Epoch 13/80: current_loss=0.02800 | best_loss=0.02739
Epoch 14/80: current_loss=0.02898 | best_loss=0.02739
Epoch 15/80: current_loss=0.02823 | best_loss=0.02739
Epoch 16/80: current_loss=0.02894 | best_loss=0.02739
Epoch 17/80: current_loss=0.02837 | best_loss=0.02739
Epoch 18/80: current_loss=0.02773 | best_loss=0.02739
Epoch 19/80: current_loss=0.02815 | best_loss=0.02739
Epoch 20/80: current_loss=0.02770 | best_loss=0.02739
Epoch 21/80: current_loss=0.02827 | best_loss=0.02739
Epoch 22/80: current_loss=0.02772 | best_loss=0.02739
Epoch 23/80: current_loss=0.02801 | best_loss=0.02739
Epoch 24/80: current_loss=0.02775 | best_loss=0.02739
Epoch 25/80: current_loss=0.02727 | best_loss=0.02727
Epoch 26/80: current_loss=0.02734 | best_loss=0.02727
Epoch 27/80: current_loss=0.02901 | best_loss=0.02727
Epoch 28/80: current_loss=0.02751 | best_loss=0.02727
Epoch 29/80: current_loss=0.02779 | best_loss=0.02727
Epoch 30/80: current_loss=0.02749 | best_loss=0.02727
Epoch 31/80: current_loss=0.02866 | best_loss=0.02727
Epoch 32/80: current_loss=0.02767 | best_loss=0.02727
Epoch 33/80: current_loss=0.02811 | best_loss=0.02727
Epoch 34/80: current_loss=0.02768 | best_loss=0.02727
Epoch 35/80: current_loss=0.02744 | best_loss=0.02727
Epoch 36/80: current_loss=0.02784 | best_loss=0.02727
Epoch 37/80: current_loss=0.02799 | best_loss=0.02727
Epoch 38/80: current_loss=0.02745 | best_loss=0.02727
Epoch 39/80: current_loss=0.02793 | best_loss=0.02727
Epoch 40/80: current_loss=0.02839 | best_loss=0.02727
Epoch 41/80: current_loss=0.02864 | best_loss=0.02727
Epoch 42/80: current_loss=0.02819 | best_loss=0.02727
Epoch 43/80: current_loss=0.02755 | best_loss=0.02727
Epoch 44/80: current_loss=0.02830 | best_loss=0.02727
Epoch 45/80: current_loss=0.02775 | best_loss=0.02727
Early Stopping at epoch 45
      explained_var=-0.00602 | mse_loss=0.02772
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02717 | best_loss=0.02717
Epoch 1/80: current_loss=0.02672 | best_loss=0.02672
Epoch 2/80: current_loss=0.02679 | best_loss=0.02672
Epoch 3/80: current_loss=0.02684 | best_loss=0.02672
Epoch 4/80: current_loss=0.02677 | best_loss=0.02672
Epoch 5/80: current_loss=0.02675 | best_loss=0.02672
Epoch 6/80: current_loss=0.02673 | best_loss=0.02672
Epoch 7/80: current_loss=0.02675 | best_loss=0.02672
Epoch 8/80: current_loss=0.02671 | best_loss=0.02671
Epoch 9/80: current_loss=0.02678 | best_loss=0.02671
Epoch 10/80: current_loss=0.02681 | best_loss=0.02671
Epoch 11/80: current_loss=0.02686 | best_loss=0.02671
Epoch 12/80: current_loss=0.02688 | best_loss=0.02671
Epoch 13/80: current_loss=0.02737 | best_loss=0.02671
Epoch 14/80: current_loss=0.02668 | best_loss=0.02668
Epoch 15/80: current_loss=0.02672 | best_loss=0.02668
Epoch 16/80: current_loss=0.02696 | best_loss=0.02668
Epoch 17/80: current_loss=0.02692 | best_loss=0.02668
Epoch 18/80: current_loss=0.02675 | best_loss=0.02668
Epoch 19/80: current_loss=0.02684 | best_loss=0.02668
Epoch 20/80: current_loss=0.02672 | best_loss=0.02668
Epoch 21/80: current_loss=0.02677 | best_loss=0.02668
Epoch 22/80: current_loss=0.02691 | best_loss=0.02668
Epoch 23/80: current_loss=0.02705 | best_loss=0.02668
Epoch 24/80: current_loss=0.02718 | best_loss=0.02668
Epoch 25/80: current_loss=0.02686 | best_loss=0.02668
Epoch 26/80: current_loss=0.02681 | best_loss=0.02668
Epoch 27/80: current_loss=0.02698 | best_loss=0.02668
Epoch 28/80: current_loss=0.02686 | best_loss=0.02668
Epoch 29/80: current_loss=0.02675 | best_loss=0.02668
Epoch 30/80: current_loss=0.02675 | best_loss=0.02668
Epoch 31/80: current_loss=0.02685 | best_loss=0.02668
Epoch 32/80: current_loss=0.02675 | best_loss=0.02668
Epoch 33/80: current_loss=0.02681 | best_loss=0.02668
Epoch 34/80: current_loss=0.02693 | best_loss=0.02668
Early Stopping at epoch 34
      explained_var=0.00200 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02998 | best_loss=0.02998
Epoch 1/80: current_loss=0.02980 | best_loss=0.02980
Epoch 2/80: current_loss=0.02973 | best_loss=0.02973
Epoch 3/80: current_loss=0.02974 | best_loss=0.02973
Epoch 4/80: current_loss=0.02981 | best_loss=0.02973
Epoch 5/80: current_loss=0.02974 | best_loss=0.02973
Epoch 6/80: current_loss=0.02987 | best_loss=0.02973
Epoch 7/80: current_loss=0.02973 | best_loss=0.02973
Epoch 8/80: current_loss=0.02986 | best_loss=0.02973
Epoch 9/80: current_loss=0.02970 | best_loss=0.02970
Epoch 10/80: current_loss=0.02971 | best_loss=0.02970
Epoch 11/80: current_loss=0.02977 | best_loss=0.02970
Epoch 12/80: current_loss=0.02974 | best_loss=0.02970
Epoch 13/80: current_loss=0.02984 | best_loss=0.02970
Epoch 14/80: current_loss=0.02977 | best_loss=0.02970
Epoch 15/80: current_loss=0.02977 | best_loss=0.02970
Epoch 16/80: current_loss=0.02975 | best_loss=0.02970
Epoch 17/80: current_loss=0.02971 | best_loss=0.02970
Epoch 18/80: current_loss=0.02977 | best_loss=0.02970
Epoch 19/80: current_loss=0.02979 | best_loss=0.02970
Epoch 20/80: current_loss=0.02973 | best_loss=0.02970
Epoch 21/80: current_loss=0.02990 | best_loss=0.02970
Epoch 22/80: current_loss=0.02975 | best_loss=0.02970
Epoch 23/80: current_loss=0.02974 | best_loss=0.02970
Epoch 24/80: current_loss=0.02971 | best_loss=0.02970
Epoch 25/80: current_loss=0.02981 | best_loss=0.02970
Epoch 26/80: current_loss=0.02973 | best_loss=0.02970
Epoch 27/80: current_loss=0.02974 | best_loss=0.02970
Epoch 28/80: current_loss=0.02976 | best_loss=0.02970
Epoch 29/80: current_loss=0.03012 | best_loss=0.02970
Early Stopping at epoch 29
      explained_var=0.00134 | mse_loss=0.02872
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=0.00099| avg_loss=0.02690
----------------------------------------------


----------------------------------------------
Params for Trial 69
{'learning_rate': 0.0001, 'weight_decay': 0.0014152635311296935, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03224 | best_loss=0.03224
Epoch 1/80: current_loss=0.03085 | best_loss=0.03085
Epoch 2/80: current_loss=0.03000 | best_loss=0.03000
Epoch 3/80: current_loss=0.02955 | best_loss=0.02955
Epoch 4/80: current_loss=0.02926 | best_loss=0.02926
Epoch 5/80: current_loss=0.02910 | best_loss=0.02910
Epoch 6/80: current_loss=0.02903 | best_loss=0.02903
Epoch 7/80: current_loss=0.02904 | best_loss=0.02903
Epoch 8/80: current_loss=0.02891 | best_loss=0.02891
Epoch 9/80: current_loss=0.02937 | best_loss=0.02891
Epoch 10/80: current_loss=0.02907 | best_loss=0.02891
Epoch 11/80: current_loss=0.02887 | best_loss=0.02887
Epoch 12/80: current_loss=0.02884 | best_loss=0.02884
Epoch 13/80: current_loss=0.02892 | best_loss=0.02884
Epoch 14/80: current_loss=0.02889 | best_loss=0.02884
Epoch 15/80: current_loss=0.02892 | best_loss=0.02884
Epoch 16/80: current_loss=0.02894 | best_loss=0.02884
Epoch 17/80: current_loss=0.02890 | best_loss=0.02884
Epoch 18/80: current_loss=0.02907 | best_loss=0.02884
Epoch 19/80: current_loss=0.02906 | best_loss=0.02884
Epoch 20/80: current_loss=0.02927 | best_loss=0.02884
Epoch 21/80: current_loss=0.02899 | best_loss=0.02884
Epoch 22/80: current_loss=0.02905 | best_loss=0.02884
Epoch 23/80: current_loss=0.02921 | best_loss=0.02884
Epoch 24/80: current_loss=0.02892 | best_loss=0.02884
Epoch 25/80: current_loss=0.02915 | best_loss=0.02884
Epoch 26/80: current_loss=0.02892 | best_loss=0.02884
Epoch 27/80: current_loss=0.02895 | best_loss=0.02884
Epoch 28/80: current_loss=0.02893 | best_loss=0.02884
Epoch 29/80: current_loss=0.02911 | best_loss=0.02884
Epoch 30/80: current_loss=0.02912 | best_loss=0.02884
Epoch 31/80: current_loss=0.02916 | best_loss=0.02884
Epoch 32/80: current_loss=0.02919 | best_loss=0.02884
Early Stopping at epoch 32
      explained_var=0.00672 | mse_loss=0.02795
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02538 | best_loss=0.02538
Epoch 1/80: current_loss=0.02489 | best_loss=0.02489
Epoch 2/80: current_loss=0.02494 | best_loss=0.02489
Epoch 3/80: current_loss=0.02486 | best_loss=0.02486
Epoch 4/80: current_loss=0.02515 | best_loss=0.02486
Epoch 5/80: current_loss=0.02505 | best_loss=0.02486
Epoch 6/80: current_loss=0.02489 | best_loss=0.02486
Epoch 7/80: current_loss=0.02492 | best_loss=0.02486
Epoch 8/80: current_loss=0.02497 | best_loss=0.02486
Epoch 9/80: current_loss=0.02517 | best_loss=0.02486
Epoch 10/80: current_loss=0.02573 | best_loss=0.02486
Epoch 11/80: current_loss=0.02487 | best_loss=0.02486
Epoch 12/80: current_loss=0.02491 | best_loss=0.02486
Epoch 13/80: current_loss=0.02489 | best_loss=0.02486
Epoch 14/80: current_loss=0.02489 | best_loss=0.02486
Epoch 15/80: current_loss=0.02587 | best_loss=0.02486
Epoch 16/80: current_loss=0.02499 | best_loss=0.02486
Epoch 17/80: current_loss=0.02492 | best_loss=0.02486
Epoch 18/80: current_loss=0.02492 | best_loss=0.02486
Epoch 19/80: current_loss=0.02490 | best_loss=0.02486
Epoch 20/80: current_loss=0.02502 | best_loss=0.02486
Epoch 21/80: current_loss=0.02529 | best_loss=0.02486
Epoch 22/80: current_loss=0.02493 | best_loss=0.02486
Epoch 23/80: current_loss=0.02530 | best_loss=0.02486
Early Stopping at epoch 23
      explained_var=0.00217 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02747 | best_loss=0.02747
Epoch 1/80: current_loss=0.02751 | best_loss=0.02747
Epoch 2/80: current_loss=0.02745 | best_loss=0.02745
Epoch 3/80: current_loss=0.02793 | best_loss=0.02745
Epoch 4/80: current_loss=0.02777 | best_loss=0.02745
Epoch 5/80: current_loss=0.02841 | best_loss=0.02745
Epoch 6/80: current_loss=0.02780 | best_loss=0.02745
Epoch 7/80: current_loss=0.02870 | best_loss=0.02745
Epoch 8/80: current_loss=0.02754 | best_loss=0.02745
Epoch 9/80: current_loss=0.02755 | best_loss=0.02745
Epoch 10/80: current_loss=0.02843 | best_loss=0.02745
Epoch 11/80: current_loss=0.02771 | best_loss=0.02745
Epoch 12/80: current_loss=0.02789 | best_loss=0.02745
Epoch 13/80: current_loss=0.02794 | best_loss=0.02745
Epoch 14/80: current_loss=0.02916 | best_loss=0.02745
Epoch 15/80: current_loss=0.02755 | best_loss=0.02745
Epoch 16/80: current_loss=0.02784 | best_loss=0.02745
Epoch 17/80: current_loss=0.02780 | best_loss=0.02745
Epoch 18/80: current_loss=0.02854 | best_loss=0.02745
Epoch 19/80: current_loss=0.02774 | best_loss=0.02745
Epoch 20/80: current_loss=0.02798 | best_loss=0.02745
Epoch 21/80: current_loss=0.02830 | best_loss=0.02745
Epoch 22/80: current_loss=0.02786 | best_loss=0.02745
Early Stopping at epoch 22
      explained_var=-0.01410 | mse_loss=0.02796
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02670 | best_loss=0.02670
Epoch 1/80: current_loss=0.02673 | best_loss=0.02670
Epoch 2/80: current_loss=0.02669 | best_loss=0.02669
Epoch 3/80: current_loss=0.02667 | best_loss=0.02667
Epoch 4/80: current_loss=0.02672 | best_loss=0.02667
Epoch 5/80: current_loss=0.02684 | best_loss=0.02667
Epoch 6/80: current_loss=0.02669 | best_loss=0.02667
Epoch 7/80: current_loss=0.02715 | best_loss=0.02667
Epoch 8/80: current_loss=0.02669 | best_loss=0.02667
Epoch 9/80: current_loss=0.02670 | best_loss=0.02667
Epoch 10/80: current_loss=0.02670 | best_loss=0.02667
Epoch 11/80: current_loss=0.02681 | best_loss=0.02667
Epoch 12/80: current_loss=0.02678 | best_loss=0.02667
Epoch 13/80: current_loss=0.02675 | best_loss=0.02667
Epoch 14/80: current_loss=0.02674 | best_loss=0.02667
Epoch 15/80: current_loss=0.02702 | best_loss=0.02667
Epoch 16/80: current_loss=0.02670 | best_loss=0.02667
Epoch 17/80: current_loss=0.02668 | best_loss=0.02667
Epoch 18/80: current_loss=0.02697 | best_loss=0.02667
Epoch 19/80: current_loss=0.02666 | best_loss=0.02666
Epoch 20/80: current_loss=0.02666 | best_loss=0.02666
Epoch 21/80: current_loss=0.02669 | best_loss=0.02666
Epoch 22/80: current_loss=0.02668 | best_loss=0.02666
Epoch 23/80: current_loss=0.02673 | best_loss=0.02666
Epoch 24/80: current_loss=0.02695 | best_loss=0.02666
Epoch 25/80: current_loss=0.02672 | best_loss=0.02666
Epoch 26/80: current_loss=0.02671 | best_loss=0.02666
Epoch 27/80: current_loss=0.02676 | best_loss=0.02666
Epoch 28/80: current_loss=0.02672 | best_loss=0.02666
Epoch 29/80: current_loss=0.02675 | best_loss=0.02666
Epoch 30/80: current_loss=0.02691 | best_loss=0.02666
Epoch 31/80: current_loss=0.02699 | best_loss=0.02666
Epoch 32/80: current_loss=0.02703 | best_loss=0.02666
Epoch 33/80: current_loss=0.02677 | best_loss=0.02666
Epoch 34/80: current_loss=0.02681 | best_loss=0.02666
Epoch 35/80: current_loss=0.02710 | best_loss=0.02666
Epoch 36/80: current_loss=0.02674 | best_loss=0.02666
Epoch 37/80: current_loss=0.02726 | best_loss=0.02666
Epoch 38/80: current_loss=0.02673 | best_loss=0.02666
Epoch 39/80: current_loss=0.02684 | best_loss=0.02666
Early Stopping at epoch 39
      explained_var=0.00190 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02979 | best_loss=0.02979
Epoch 1/80: current_loss=0.02974 | best_loss=0.02974
Epoch 2/80: current_loss=0.02976 | best_loss=0.02974
Epoch 3/80: current_loss=0.02970 | best_loss=0.02970
Epoch 4/80: current_loss=0.02973 | best_loss=0.02970
Epoch 5/80: current_loss=0.02979 | best_loss=0.02970
Epoch 6/80: current_loss=0.02975 | best_loss=0.02970
Epoch 7/80: current_loss=0.02973 | best_loss=0.02970
Epoch 8/80: current_loss=0.02992 | best_loss=0.02970
Epoch 9/80: current_loss=0.02966 | best_loss=0.02966
Epoch 10/80: current_loss=0.02974 | best_loss=0.02966
Epoch 11/80: current_loss=0.02989 | best_loss=0.02966
Epoch 12/80: current_loss=0.02970 | best_loss=0.02966
Epoch 13/80: current_loss=0.02969 | best_loss=0.02966
Epoch 14/80: current_loss=0.02972 | best_loss=0.02966
Epoch 15/80: current_loss=0.02968 | best_loss=0.02966
Epoch 16/80: current_loss=0.03002 | best_loss=0.02966
Epoch 17/80: current_loss=0.02978 | best_loss=0.02966
Epoch 18/80: current_loss=0.02975 | best_loss=0.02966
Epoch 19/80: current_loss=0.02969 | best_loss=0.02966
Epoch 20/80: current_loss=0.02977 | best_loss=0.02966
Epoch 21/80: current_loss=0.02986 | best_loss=0.02966
Epoch 22/80: current_loss=0.02969 | best_loss=0.02966
Epoch 23/80: current_loss=0.02969 | best_loss=0.02966
Epoch 24/80: current_loss=0.02975 | best_loss=0.02966
Epoch 25/80: current_loss=0.02973 | best_loss=0.02966
Epoch 26/80: current_loss=0.02969 | best_loss=0.02966
Epoch 27/80: current_loss=0.02973 | best_loss=0.02966
Epoch 28/80: current_loss=0.02971 | best_loss=0.02966
Epoch 29/80: current_loss=0.02971 | best_loss=0.02966
Early Stopping at epoch 29
      explained_var=0.00197 | mse_loss=0.02870
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=-0.00027| avg_loss=0.02694
----------------------------------------------


----------------------------------------------
Params for Trial 70
{'learning_rate': 0.0001, 'weight_decay': 0.0010790190597535544, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03395 | best_loss=0.03395
Epoch 1/80: current_loss=0.03205 | best_loss=0.03205
Epoch 2/80: current_loss=0.03091 | best_loss=0.03091
Epoch 3/80: current_loss=0.03040 | best_loss=0.03040
Epoch 4/80: current_loss=0.02985 | best_loss=0.02985
Epoch 5/80: current_loss=0.02987 | best_loss=0.02985
Epoch 6/80: current_loss=0.02949 | best_loss=0.02949
Epoch 7/80: current_loss=0.02943 | best_loss=0.02943
Epoch 8/80: current_loss=0.02947 | best_loss=0.02943
Epoch 9/80: current_loss=0.02934 | best_loss=0.02934
Epoch 10/80: current_loss=0.02913 | best_loss=0.02913
Epoch 11/80: current_loss=0.02914 | best_loss=0.02913
Epoch 12/80: current_loss=0.02911 | best_loss=0.02911
Epoch 13/80: current_loss=0.02906 | best_loss=0.02906
Epoch 14/80: current_loss=0.02901 | best_loss=0.02901
Epoch 15/80: current_loss=0.02901 | best_loss=0.02901
Epoch 16/80: current_loss=0.02977 | best_loss=0.02901
Epoch 17/80: current_loss=0.02903 | best_loss=0.02901
Epoch 18/80: current_loss=0.02957 | best_loss=0.02901
Epoch 19/80: current_loss=0.02905 | best_loss=0.02901
Epoch 20/80: current_loss=0.02909 | best_loss=0.02901
Epoch 21/80: current_loss=0.02905 | best_loss=0.02901
Epoch 22/80: current_loss=0.02907 | best_loss=0.02901
Epoch 23/80: current_loss=0.02894 | best_loss=0.02894
Epoch 24/80: current_loss=0.02894 | best_loss=0.02894
Epoch 25/80: current_loss=0.02896 | best_loss=0.02894
Epoch 26/80: current_loss=0.02896 | best_loss=0.02894
Epoch 27/80: current_loss=0.02939 | best_loss=0.02894
Epoch 28/80: current_loss=0.02929 | best_loss=0.02894
Epoch 29/80: current_loss=0.02904 | best_loss=0.02894
Epoch 30/80: current_loss=0.02890 | best_loss=0.02890
Epoch 31/80: current_loss=0.02888 | best_loss=0.02888
Epoch 32/80: current_loss=0.02897 | best_loss=0.02888
Epoch 33/80: current_loss=0.02902 | best_loss=0.02888
Epoch 34/80: current_loss=0.02900 | best_loss=0.02888
Epoch 35/80: current_loss=0.02894 | best_loss=0.02888
Epoch 36/80: current_loss=0.02904 | best_loss=0.02888
Epoch 37/80: current_loss=0.02885 | best_loss=0.02885
Epoch 38/80: current_loss=0.02898 | best_loss=0.02885
Epoch 39/80: current_loss=0.02902 | best_loss=0.02885
Epoch 40/80: current_loss=0.02898 | best_loss=0.02885
Epoch 41/80: current_loss=0.02887 | best_loss=0.02885
Epoch 42/80: current_loss=0.02891 | best_loss=0.02885
Epoch 43/80: current_loss=0.02945 | best_loss=0.02885
Epoch 44/80: current_loss=0.02905 | best_loss=0.02885
Epoch 45/80: current_loss=0.02895 | best_loss=0.02885
Epoch 46/80: current_loss=0.02894 | best_loss=0.02885
Epoch 47/80: current_loss=0.02926 | best_loss=0.02885
Epoch 48/80: current_loss=0.02905 | best_loss=0.02885
Epoch 49/80: current_loss=0.02911 | best_loss=0.02885
Epoch 50/80: current_loss=0.02888 | best_loss=0.02885
Epoch 51/80: current_loss=0.02889 | best_loss=0.02885
Epoch 52/80: current_loss=0.02933 | best_loss=0.02885
Epoch 53/80: current_loss=0.02892 | best_loss=0.02885
Epoch 54/80: current_loss=0.02902 | best_loss=0.02885
Epoch 55/80: current_loss=0.02897 | best_loss=0.02885
Epoch 56/80: current_loss=0.02928 | best_loss=0.02885
Epoch 57/80: current_loss=0.02935 | best_loss=0.02885
Early Stopping at epoch 57
      explained_var=0.00572 | mse_loss=0.02801
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02508 | best_loss=0.02508
Epoch 1/80: current_loss=0.02551 | best_loss=0.02508
Epoch 2/80: current_loss=0.02534 | best_loss=0.02508
Epoch 3/80: current_loss=0.02524 | best_loss=0.02508
Epoch 4/80: current_loss=0.02508 | best_loss=0.02508
Epoch 5/80: current_loss=0.02563 | best_loss=0.02508
Epoch 6/80: current_loss=0.02489 | best_loss=0.02489
Epoch 7/80: current_loss=0.02559 | best_loss=0.02489
Epoch 8/80: current_loss=0.02497 | best_loss=0.02489
Epoch 9/80: current_loss=0.02490 | best_loss=0.02489
Epoch 10/80: current_loss=0.02493 | best_loss=0.02489
Epoch 11/80: current_loss=0.02498 | best_loss=0.02489
Epoch 12/80: current_loss=0.02499 | best_loss=0.02489
Epoch 13/80: current_loss=0.02518 | best_loss=0.02489
Epoch 14/80: current_loss=0.02516 | best_loss=0.02489
Epoch 15/80: current_loss=0.02493 | best_loss=0.02489
Epoch 16/80: current_loss=0.02508 | best_loss=0.02489
Epoch 17/80: current_loss=0.02504 | best_loss=0.02489
Epoch 18/80: current_loss=0.02487 | best_loss=0.02487
Epoch 19/80: current_loss=0.02488 | best_loss=0.02487
Epoch 20/80: current_loss=0.02513 | best_loss=0.02487
Epoch 21/80: current_loss=0.02489 | best_loss=0.02487
Epoch 22/80: current_loss=0.02509 | best_loss=0.02487
Epoch 23/80: current_loss=0.02492 | best_loss=0.02487
Epoch 24/80: current_loss=0.02491 | best_loss=0.02487
Epoch 25/80: current_loss=0.02493 | best_loss=0.02487
Epoch 26/80: current_loss=0.02494 | best_loss=0.02487
Epoch 27/80: current_loss=0.02502 | best_loss=0.02487
Epoch 28/80: current_loss=0.02532 | best_loss=0.02487
Epoch 29/80: current_loss=0.02518 | best_loss=0.02487
Epoch 30/80: current_loss=0.02493 | best_loss=0.02487
Epoch 31/80: current_loss=0.02510 | best_loss=0.02487
Epoch 32/80: current_loss=0.02502 | best_loss=0.02487
Epoch 33/80: current_loss=0.02493 | best_loss=0.02487
Epoch 34/80: current_loss=0.02496 | best_loss=0.02487
Epoch 35/80: current_loss=0.02493 | best_loss=0.02487
Epoch 36/80: current_loss=0.02500 | best_loss=0.02487
Epoch 37/80: current_loss=0.02492 | best_loss=0.02487
Epoch 38/80: current_loss=0.02512 | best_loss=0.02487
Early Stopping at epoch 38
      explained_var=0.00224 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02787 | best_loss=0.02787
Epoch 1/80: current_loss=0.02838 | best_loss=0.02787
Epoch 2/80: current_loss=0.02786 | best_loss=0.02786
Epoch 3/80: current_loss=0.02788 | best_loss=0.02786
Epoch 4/80: current_loss=0.02907 | best_loss=0.02786
Epoch 5/80: current_loss=0.02745 | best_loss=0.02745
Epoch 6/80: current_loss=0.02768 | best_loss=0.02745
Epoch 7/80: current_loss=0.02783 | best_loss=0.02745
Epoch 8/80: current_loss=0.02752 | best_loss=0.02745
Epoch 9/80: current_loss=0.02731 | best_loss=0.02731
Epoch 10/80: current_loss=0.02776 | best_loss=0.02731
Epoch 11/80: current_loss=0.02733 | best_loss=0.02731
Epoch 12/80: current_loss=0.02770 | best_loss=0.02731
Epoch 13/80: current_loss=0.02774 | best_loss=0.02731
Epoch 14/80: current_loss=0.02760 | best_loss=0.02731
Epoch 15/80: current_loss=0.02753 | best_loss=0.02731
Epoch 16/80: current_loss=0.02759 | best_loss=0.02731
Epoch 17/80: current_loss=0.02759 | best_loss=0.02731
Epoch 18/80: current_loss=0.02783 | best_loss=0.02731
Epoch 19/80: current_loss=0.02752 | best_loss=0.02731
Epoch 20/80: current_loss=0.02768 | best_loss=0.02731
Epoch 21/80: current_loss=0.02733 | best_loss=0.02731
Epoch 22/80: current_loss=0.02791 | best_loss=0.02731
Epoch 23/80: current_loss=0.02808 | best_loss=0.02731
Epoch 24/80: current_loss=0.02766 | best_loss=0.02731
Epoch 25/80: current_loss=0.02875 | best_loss=0.02731
Epoch 26/80: current_loss=0.02749 | best_loss=0.02731
Epoch 27/80: current_loss=0.02799 | best_loss=0.02731
Epoch 28/80: current_loss=0.02747 | best_loss=0.02731
Epoch 29/80: current_loss=0.02891 | best_loss=0.02731
Early Stopping at epoch 29
      explained_var=-0.00832 | mse_loss=0.02779
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02690 | best_loss=0.02690
Epoch 1/80: current_loss=0.02675 | best_loss=0.02675
Epoch 2/80: current_loss=0.02690 | best_loss=0.02675
Epoch 3/80: current_loss=0.02675 | best_loss=0.02675
Epoch 4/80: current_loss=0.02689 | best_loss=0.02675
Epoch 5/80: current_loss=0.02699 | best_loss=0.02675
Epoch 6/80: current_loss=0.02676 | best_loss=0.02675
Epoch 7/80: current_loss=0.02686 | best_loss=0.02675
Epoch 8/80: current_loss=0.02689 | best_loss=0.02675
Epoch 9/80: current_loss=0.02676 | best_loss=0.02675
Epoch 10/80: current_loss=0.02676 | best_loss=0.02675
Epoch 11/80: current_loss=0.02685 | best_loss=0.02675
Epoch 12/80: current_loss=0.02684 | best_loss=0.02675
Epoch 13/80: current_loss=0.02678 | best_loss=0.02675
Epoch 14/80: current_loss=0.02689 | best_loss=0.02675
Epoch 15/80: current_loss=0.02677 | best_loss=0.02675
Epoch 16/80: current_loss=0.02688 | best_loss=0.02675
Epoch 17/80: current_loss=0.02682 | best_loss=0.02675
Epoch 18/80: current_loss=0.02677 | best_loss=0.02675
Epoch 19/80: current_loss=0.02678 | best_loss=0.02675
Epoch 20/80: current_loss=0.02682 | best_loss=0.02675
Epoch 21/80: current_loss=0.02687 | best_loss=0.02675
Early Stopping at epoch 21
      explained_var=0.00156 | mse_loss=0.02496
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02976 | best_loss=0.02976
Epoch 1/80: current_loss=0.02997 | best_loss=0.02976
Epoch 2/80: current_loss=0.02977 | best_loss=0.02976
Epoch 3/80: current_loss=0.02969 | best_loss=0.02969
Epoch 4/80: current_loss=0.02977 | best_loss=0.02969
Epoch 5/80: current_loss=0.03009 | best_loss=0.02969
Epoch 6/80: current_loss=0.02978 | best_loss=0.02969
Epoch 7/80: current_loss=0.03004 | best_loss=0.02969
Epoch 8/80: current_loss=0.02983 | best_loss=0.02969
Epoch 9/80: current_loss=0.02979 | best_loss=0.02969
Epoch 10/80: current_loss=0.02975 | best_loss=0.02969
Epoch 11/80: current_loss=0.02967 | best_loss=0.02967
Epoch 12/80: current_loss=0.02970 | best_loss=0.02967
Epoch 13/80: current_loss=0.02967 | best_loss=0.02967
Epoch 14/80: current_loss=0.02969 | best_loss=0.02967
Epoch 15/80: current_loss=0.02973 | best_loss=0.02967
Epoch 16/80: current_loss=0.02988 | best_loss=0.02967
Epoch 17/80: current_loss=0.02969 | best_loss=0.02967
Epoch 18/80: current_loss=0.02968 | best_loss=0.02967
Epoch 19/80: current_loss=0.02968 | best_loss=0.02967
Epoch 20/80: current_loss=0.02967 | best_loss=0.02967
Epoch 21/80: current_loss=0.02973 | best_loss=0.02967
Epoch 22/80: current_loss=0.02980 | best_loss=0.02967
Epoch 23/80: current_loss=0.02971 | best_loss=0.02967
Epoch 24/80: current_loss=0.02968 | best_loss=0.02967
Epoch 25/80: current_loss=0.02971 | best_loss=0.02967
Epoch 26/80: current_loss=0.02986 | best_loss=0.02967
Epoch 27/80: current_loss=0.02977 | best_loss=0.02967
Epoch 28/80: current_loss=0.02979 | best_loss=0.02967
Epoch 29/80: current_loss=0.02979 | best_loss=0.02967
Epoch 30/80: current_loss=0.02970 | best_loss=0.02967
Epoch 31/80: current_loss=0.02983 | best_loss=0.02967
Early Stopping at epoch 31
      explained_var=0.00236 | mse_loss=0.02869
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.00071| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 71
{'learning_rate': 0.0001, 'weight_decay': 0.0002017206902487404, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03117 | best_loss=0.03117
Epoch 1/80: current_loss=0.03046 | best_loss=0.03046
Epoch 2/80: current_loss=0.03009 | best_loss=0.03009
Epoch 3/80: current_loss=0.02942 | best_loss=0.02942
Epoch 4/80: current_loss=0.02923 | best_loss=0.02923
Epoch 5/80: current_loss=0.02916 | best_loss=0.02916
Epoch 6/80: current_loss=0.02902 | best_loss=0.02902
Epoch 7/80: current_loss=0.02959 | best_loss=0.02902
Epoch 8/80: current_loss=0.02900 | best_loss=0.02900
Epoch 9/80: current_loss=0.02899 | best_loss=0.02899
Epoch 10/80: current_loss=0.02905 | best_loss=0.02899
Epoch 11/80: current_loss=0.02900 | best_loss=0.02899
Epoch 12/80: current_loss=0.02932 | best_loss=0.02899
Epoch 13/80: current_loss=0.02899 | best_loss=0.02899
Epoch 14/80: current_loss=0.02909 | best_loss=0.02899
Epoch 15/80: current_loss=0.02897 | best_loss=0.02897
Epoch 16/80: current_loss=0.02929 | best_loss=0.02897
Epoch 17/80: current_loss=0.02899 | best_loss=0.02897
Epoch 18/80: current_loss=0.02917 | best_loss=0.02897
Epoch 19/80: current_loss=0.02917 | best_loss=0.02897
Epoch 20/80: current_loss=0.02915 | best_loss=0.02897
Epoch 21/80: current_loss=0.02904 | best_loss=0.02897
Epoch 22/80: current_loss=0.02947 | best_loss=0.02897
Epoch 23/80: current_loss=0.02906 | best_loss=0.02897
Epoch 24/80: current_loss=0.02913 | best_loss=0.02897
Epoch 25/80: current_loss=0.02911 | best_loss=0.02897
Epoch 26/80: current_loss=0.02900 | best_loss=0.02897
Epoch 27/80: current_loss=0.02897 | best_loss=0.02897
Epoch 28/80: current_loss=0.02899 | best_loss=0.02897
Epoch 29/80: current_loss=0.02902 | best_loss=0.02897
Epoch 30/80: current_loss=0.02893 | best_loss=0.02893
Epoch 31/80: current_loss=0.02941 | best_loss=0.02893
Epoch 32/80: current_loss=0.02898 | best_loss=0.02893
Epoch 33/80: current_loss=0.02923 | best_loss=0.02893
Epoch 34/80: current_loss=0.02898 | best_loss=0.02893
Epoch 35/80: current_loss=0.02894 | best_loss=0.02893
Epoch 36/80: current_loss=0.02913 | best_loss=0.02893
Epoch 37/80: current_loss=0.02892 | best_loss=0.02892
Epoch 38/80: current_loss=0.02900 | best_loss=0.02892
Epoch 39/80: current_loss=0.02908 | best_loss=0.02892
Epoch 40/80: current_loss=0.02909 | best_loss=0.02892
Epoch 41/80: current_loss=0.02912 | best_loss=0.02892
Epoch 42/80: current_loss=0.02909 | best_loss=0.02892
Epoch 43/80: current_loss=0.02939 | best_loss=0.02892
Epoch 44/80: current_loss=0.02904 | best_loss=0.02892
Epoch 45/80: current_loss=0.02919 | best_loss=0.02892
Epoch 46/80: current_loss=0.02899 | best_loss=0.02892
Epoch 47/80: current_loss=0.02899 | best_loss=0.02892
Epoch 48/80: current_loss=0.02909 | best_loss=0.02892
Epoch 49/80: current_loss=0.02896 | best_loss=0.02892
Epoch 50/80: current_loss=0.02934 | best_loss=0.02892
Epoch 51/80: current_loss=0.02937 | best_loss=0.02892
Epoch 52/80: current_loss=0.02905 | best_loss=0.02892
Epoch 53/80: current_loss=0.02905 | best_loss=0.02892
Epoch 54/80: current_loss=0.02903 | best_loss=0.02892
Epoch 55/80: current_loss=0.02904 | best_loss=0.02892
Epoch 56/80: current_loss=0.02904 | best_loss=0.02892
Epoch 57/80: current_loss=0.02961 | best_loss=0.02892
Early Stopping at epoch 57
      explained_var=0.00477 | mse_loss=0.02802
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02500 | best_loss=0.02500
Epoch 1/80: current_loss=0.02490 | best_loss=0.02490
Epoch 2/80: current_loss=0.02495 | best_loss=0.02490
Epoch 3/80: current_loss=0.02498 | best_loss=0.02490
Epoch 4/80: current_loss=0.02492 | best_loss=0.02490
Epoch 5/80: current_loss=0.02492 | best_loss=0.02490
Epoch 6/80: current_loss=0.02491 | best_loss=0.02490
Epoch 7/80: current_loss=0.02498 | best_loss=0.02490
Epoch 8/80: current_loss=0.02492 | best_loss=0.02490
Epoch 9/80: current_loss=0.02495 | best_loss=0.02490
Epoch 10/80: current_loss=0.02499 | best_loss=0.02490
Epoch 11/80: current_loss=0.02517 | best_loss=0.02490
Epoch 12/80: current_loss=0.02504 | best_loss=0.02490
Epoch 13/80: current_loss=0.02524 | best_loss=0.02490
Epoch 14/80: current_loss=0.02499 | best_loss=0.02490
Epoch 15/80: current_loss=0.02517 | best_loss=0.02490
Epoch 16/80: current_loss=0.02508 | best_loss=0.02490
Epoch 17/80: current_loss=0.02498 | best_loss=0.02490
Epoch 18/80: current_loss=0.02499 | best_loss=0.02490
Epoch 19/80: current_loss=0.02539 | best_loss=0.02490
Epoch 20/80: current_loss=0.02494 | best_loss=0.02490
Epoch 21/80: current_loss=0.02515 | best_loss=0.02490
Early Stopping at epoch 21
      explained_var=0.00247 | mse_loss=0.02514
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02806 | best_loss=0.02806
Epoch 1/80: current_loss=0.02821 | best_loss=0.02806
Epoch 2/80: current_loss=0.02813 | best_loss=0.02806
Epoch 3/80: current_loss=0.02731 | best_loss=0.02731
Epoch 4/80: current_loss=0.02769 | best_loss=0.02731
Epoch 5/80: current_loss=0.02782 | best_loss=0.02731
Epoch 6/80: current_loss=0.02772 | best_loss=0.02731
Epoch 7/80: current_loss=0.02788 | best_loss=0.02731
Epoch 8/80: current_loss=0.02761 | best_loss=0.02731
Epoch 9/80: current_loss=0.02728 | best_loss=0.02728
Epoch 10/80: current_loss=0.02787 | best_loss=0.02728
Epoch 11/80: current_loss=0.02780 | best_loss=0.02728
Epoch 12/80: current_loss=0.02761 | best_loss=0.02728
Epoch 13/80: current_loss=0.02853 | best_loss=0.02728
Epoch 14/80: current_loss=0.02765 | best_loss=0.02728
Epoch 15/80: current_loss=0.02749 | best_loss=0.02728
Epoch 16/80: current_loss=0.02840 | best_loss=0.02728
Epoch 17/80: current_loss=0.02743 | best_loss=0.02728
Epoch 18/80: current_loss=0.02817 | best_loss=0.02728
Epoch 19/80: current_loss=0.02759 | best_loss=0.02728
Epoch 20/80: current_loss=0.02815 | best_loss=0.02728
Epoch 21/80: current_loss=0.02740 | best_loss=0.02728
Epoch 22/80: current_loss=0.02742 | best_loss=0.02728
Epoch 23/80: current_loss=0.02787 | best_loss=0.02728
Epoch 24/80: current_loss=0.02851 | best_loss=0.02728
Epoch 25/80: current_loss=0.02834 | best_loss=0.02728
Epoch 26/80: current_loss=0.02776 | best_loss=0.02728
Epoch 27/80: current_loss=0.02793 | best_loss=0.02728
Epoch 28/80: current_loss=0.02823 | best_loss=0.02728
Epoch 29/80: current_loss=0.02746 | best_loss=0.02728
Early Stopping at epoch 29
      explained_var=-0.00533 | mse_loss=0.02772
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02685 | best_loss=0.02685
Epoch 1/80: current_loss=0.02679 | best_loss=0.02679
Epoch 2/80: current_loss=0.02679 | best_loss=0.02679
Epoch 3/80: current_loss=0.02696 | best_loss=0.02679
Epoch 4/80: current_loss=0.02697 | best_loss=0.02679
Epoch 5/80: current_loss=0.02676 | best_loss=0.02676
Epoch 6/80: current_loss=0.02676 | best_loss=0.02676
Epoch 7/80: current_loss=0.02674 | best_loss=0.02674
Epoch 8/80: current_loss=0.02678 | best_loss=0.02674
Epoch 9/80: current_loss=0.02699 | best_loss=0.02674
Epoch 10/80: current_loss=0.02675 | best_loss=0.02674
Epoch 11/80: current_loss=0.02696 | best_loss=0.02674
Epoch 12/80: current_loss=0.02686 | best_loss=0.02674
Epoch 13/80: current_loss=0.02673 | best_loss=0.02673
Epoch 14/80: current_loss=0.02672 | best_loss=0.02672
Epoch 15/80: current_loss=0.02674 | best_loss=0.02672
Epoch 16/80: current_loss=0.02684 | best_loss=0.02672
Epoch 17/80: current_loss=0.02675 | best_loss=0.02672
Epoch 18/80: current_loss=0.02676 | best_loss=0.02672
Epoch 19/80: current_loss=0.02678 | best_loss=0.02672
Epoch 20/80: current_loss=0.02685 | best_loss=0.02672
Epoch 21/80: current_loss=0.02695 | best_loss=0.02672
Epoch 22/80: current_loss=0.02680 | best_loss=0.02672
Epoch 23/80: current_loss=0.02681 | best_loss=0.02672
Epoch 24/80: current_loss=0.02725 | best_loss=0.02672
Epoch 25/80: current_loss=0.02682 | best_loss=0.02672
Epoch 26/80: current_loss=0.02686 | best_loss=0.02672
Epoch 27/80: current_loss=0.02690 | best_loss=0.02672
Epoch 28/80: current_loss=0.02685 | best_loss=0.02672
Epoch 29/80: current_loss=0.02676 | best_loss=0.02672
Epoch 30/80: current_loss=0.02677 | best_loss=0.02672
Epoch 31/80: current_loss=0.02679 | best_loss=0.02672
Epoch 32/80: current_loss=0.02687 | best_loss=0.02672
Epoch 33/80: current_loss=0.02684 | best_loss=0.02672
Epoch 34/80: current_loss=0.02681 | best_loss=0.02672
Early Stopping at epoch 34
      explained_var=0.00173 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02966 | best_loss=0.02966
Epoch 1/80: current_loss=0.02962 | best_loss=0.02962
Epoch 2/80: current_loss=0.02975 | best_loss=0.02962
Epoch 3/80: current_loss=0.02963 | best_loss=0.02962
Epoch 4/80: current_loss=0.02960 | best_loss=0.02960
Epoch 5/80: current_loss=0.02964 | best_loss=0.02960
Epoch 6/80: current_loss=0.02972 | best_loss=0.02960
Epoch 7/80: current_loss=0.02964 | best_loss=0.02960
Epoch 8/80: current_loss=0.02998 | best_loss=0.02960
Epoch 9/80: current_loss=0.02973 | best_loss=0.02960
Epoch 10/80: current_loss=0.02971 | best_loss=0.02960
Epoch 11/80: current_loss=0.03000 | best_loss=0.02960
Epoch 12/80: current_loss=0.02978 | best_loss=0.02960
Epoch 13/80: current_loss=0.02987 | best_loss=0.02960
Epoch 14/80: current_loss=0.02969 | best_loss=0.02960
Epoch 15/80: current_loss=0.02979 | best_loss=0.02960
Epoch 16/80: current_loss=0.02976 | best_loss=0.02960
Epoch 17/80: current_loss=0.02972 | best_loss=0.02960
Epoch 18/80: current_loss=0.02971 | best_loss=0.02960
Epoch 19/80: current_loss=0.02970 | best_loss=0.02960
Epoch 20/80: current_loss=0.02973 | best_loss=0.02960
Epoch 21/80: current_loss=0.02974 | best_loss=0.02960
Epoch 22/80: current_loss=0.02977 | best_loss=0.02960
Epoch 23/80: current_loss=0.03000 | best_loss=0.02960
Epoch 24/80: current_loss=0.02973 | best_loss=0.02960
Early Stopping at epoch 24
      explained_var=0.00388 | mse_loss=0.02865
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.00150| avg_loss=0.02690
----------------------------------------------


----------------------------------------------
Params for Trial 72
{'learning_rate': 0.0001, 'weight_decay': 0.00023901150110526212, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03056 | best_loss=0.03056
Epoch 1/80: current_loss=0.02959 | best_loss=0.02959
Epoch 2/80: current_loss=0.02915 | best_loss=0.02915
Epoch 3/80: current_loss=0.02898 | best_loss=0.02898
Epoch 4/80: current_loss=0.02959 | best_loss=0.02898
Epoch 5/80: current_loss=0.02903 | best_loss=0.02898
Epoch 6/80: current_loss=0.02885 | best_loss=0.02885
Epoch 7/80: current_loss=0.02893 | best_loss=0.02885
Epoch 8/80: current_loss=0.02883 | best_loss=0.02883
Epoch 9/80: current_loss=0.02907 | best_loss=0.02883
Epoch 10/80: current_loss=0.02896 | best_loss=0.02883
Epoch 11/80: current_loss=0.02906 | best_loss=0.02883
Epoch 12/80: current_loss=0.02900 | best_loss=0.02883
Epoch 13/80: current_loss=0.02910 | best_loss=0.02883
Epoch 14/80: current_loss=0.02908 | best_loss=0.02883
Epoch 15/80: current_loss=0.02931 | best_loss=0.02883
Epoch 16/80: current_loss=0.02911 | best_loss=0.02883
Epoch 17/80: current_loss=0.02927 | best_loss=0.02883
Epoch 18/80: current_loss=0.02911 | best_loss=0.02883
Epoch 19/80: current_loss=0.02918 | best_loss=0.02883
Epoch 20/80: current_loss=0.02928 | best_loss=0.02883
Epoch 21/80: current_loss=0.02913 | best_loss=0.02883
Epoch 22/80: current_loss=0.02907 | best_loss=0.02883
Epoch 23/80: current_loss=0.02911 | best_loss=0.02883
Epoch 24/80: current_loss=0.02922 | best_loss=0.02883
Epoch 25/80: current_loss=0.02931 | best_loss=0.02883
Epoch 26/80: current_loss=0.02902 | best_loss=0.02883
Epoch 27/80: current_loss=0.02898 | best_loss=0.02883
Epoch 28/80: current_loss=0.02897 | best_loss=0.02883
Early Stopping at epoch 28
      explained_var=0.00850 | mse_loss=0.02795
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02546 | best_loss=0.02546
Epoch 1/80: current_loss=0.02499 | best_loss=0.02499
Epoch 2/80: current_loss=0.02492 | best_loss=0.02492
Epoch 3/80: current_loss=0.02490 | best_loss=0.02490
Epoch 4/80: current_loss=0.02485 | best_loss=0.02485
Epoch 5/80: current_loss=0.02511 | best_loss=0.02485
Epoch 6/80: current_loss=0.02488 | best_loss=0.02485
Epoch 7/80: current_loss=0.02507 | best_loss=0.02485
Epoch 8/80: current_loss=0.02490 | best_loss=0.02485
Epoch 9/80: current_loss=0.02502 | best_loss=0.02485
Epoch 10/80: current_loss=0.02518 | best_loss=0.02485
Epoch 11/80: current_loss=0.02561 | best_loss=0.02485
Epoch 12/80: current_loss=0.02492 | best_loss=0.02485
Epoch 13/80: current_loss=0.02498 | best_loss=0.02485
Epoch 14/80: current_loss=0.02507 | best_loss=0.02485
Epoch 15/80: current_loss=0.02494 | best_loss=0.02485
Epoch 16/80: current_loss=0.02545 | best_loss=0.02485
Epoch 17/80: current_loss=0.02507 | best_loss=0.02485
Epoch 18/80: current_loss=0.02502 | best_loss=0.02485
Epoch 19/80: current_loss=0.02514 | best_loss=0.02485
Epoch 20/80: current_loss=0.02499 | best_loss=0.02485
Epoch 21/80: current_loss=0.02490 | best_loss=0.02485
Epoch 22/80: current_loss=0.02492 | best_loss=0.02485
Epoch 23/80: current_loss=0.02498 | best_loss=0.02485
Epoch 24/80: current_loss=0.02502 | best_loss=0.02485
Early Stopping at epoch 24
      explained_var=0.00096 | mse_loss=0.02518

----------------------------------------------
Params for Trial 73
{'learning_rate': 0.0001, 'weight_decay': 0.0005965109595382262, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03196 | best_loss=0.03196
Epoch 1/80: current_loss=0.03023 | best_loss=0.03023
Epoch 2/80: current_loss=0.02957 | best_loss=0.02957
Epoch 3/80: current_loss=0.02907 | best_loss=0.02907
Epoch 4/80: current_loss=0.02889 | best_loss=0.02889
Epoch 5/80: current_loss=0.02892 | best_loss=0.02889
Epoch 6/80: current_loss=0.02888 | best_loss=0.02888
Epoch 7/80: current_loss=0.02927 | best_loss=0.02888
Epoch 8/80: current_loss=0.02903 | best_loss=0.02888
Epoch 9/80: current_loss=0.02885 | best_loss=0.02885
Epoch 10/80: current_loss=0.02897 | best_loss=0.02885
Epoch 11/80: current_loss=0.02870 | best_loss=0.02870
Epoch 12/80: current_loss=0.02877 | best_loss=0.02870
Epoch 13/80: current_loss=0.02877 | best_loss=0.02870
Epoch 14/80: current_loss=0.02907 | best_loss=0.02870
Epoch 15/80: current_loss=0.02878 | best_loss=0.02870
Epoch 16/80: current_loss=0.02880 | best_loss=0.02870
Epoch 17/80: current_loss=0.02902 | best_loss=0.02870
Epoch 18/80: current_loss=0.02884 | best_loss=0.02870
Epoch 19/80: current_loss=0.02885 | best_loss=0.02870
Epoch 20/80: current_loss=0.02892 | best_loss=0.02870
Epoch 21/80: current_loss=0.02933 | best_loss=0.02870
Epoch 22/80: current_loss=0.02921 | best_loss=0.02870
Epoch 23/80: current_loss=0.02949 | best_loss=0.02870
Epoch 24/80: current_loss=0.02910 | best_loss=0.02870
Epoch 25/80: current_loss=0.02925 | best_loss=0.02870
Epoch 26/80: current_loss=0.02893 | best_loss=0.02870
Epoch 27/80: current_loss=0.02891 | best_loss=0.02870
Epoch 28/80: current_loss=0.02922 | best_loss=0.02870
Epoch 29/80: current_loss=0.02892 | best_loss=0.02870
Epoch 30/80: current_loss=0.02931 | best_loss=0.02870
Epoch 31/80: current_loss=0.02898 | best_loss=0.02870
Early Stopping at epoch 31
      explained_var=0.00904 | mse_loss=0.02790
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02517 | best_loss=0.02517
Epoch 1/80: current_loss=0.02524 | best_loss=0.02517
Epoch 2/80: current_loss=0.02501 | best_loss=0.02501
Epoch 3/80: current_loss=0.02502 | best_loss=0.02501
Epoch 4/80: current_loss=0.02492 | best_loss=0.02492
Epoch 5/80: current_loss=0.02485 | best_loss=0.02485
Epoch 6/80: current_loss=0.02505 | best_loss=0.02485
Epoch 7/80: current_loss=0.02503 | best_loss=0.02485
Epoch 8/80: current_loss=0.02533 | best_loss=0.02485
Epoch 9/80: current_loss=0.02505 | best_loss=0.02485
Epoch 10/80: current_loss=0.02511 | best_loss=0.02485
Epoch 11/80: current_loss=0.02570 | best_loss=0.02485
Epoch 12/80: current_loss=0.02532 | best_loss=0.02485
Epoch 13/80: current_loss=0.02495 | best_loss=0.02485
Epoch 14/80: current_loss=0.02496 | best_loss=0.02485
Epoch 15/80: current_loss=0.02509 | best_loss=0.02485
Epoch 16/80: current_loss=0.02498 | best_loss=0.02485
Epoch 17/80: current_loss=0.02504 | best_loss=0.02485
Epoch 18/80: current_loss=0.02494 | best_loss=0.02485
Epoch 19/80: current_loss=0.02491 | best_loss=0.02485
Epoch 20/80: current_loss=0.02487 | best_loss=0.02485
Epoch 21/80: current_loss=0.02488 | best_loss=0.02485
Epoch 22/80: current_loss=0.02490 | best_loss=0.02485
Epoch 23/80: current_loss=0.02488 | best_loss=0.02485
Epoch 24/80: current_loss=0.02491 | best_loss=0.02485
Epoch 25/80: current_loss=0.02503 | best_loss=0.02485
Early Stopping at epoch 25
      explained_var=0.00150 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02769 | best_loss=0.02769
Epoch 1/80: current_loss=0.02769 | best_loss=0.02769
Epoch 2/80: current_loss=0.02813 | best_loss=0.02769
Epoch 3/80: current_loss=0.02755 | best_loss=0.02755
Epoch 4/80: current_loss=0.02733 | best_loss=0.02733
Epoch 5/80: current_loss=0.02763 | best_loss=0.02733
Epoch 6/80: current_loss=0.02740 | best_loss=0.02733
Epoch 7/80: current_loss=0.02747 | best_loss=0.02733
Epoch 8/80: current_loss=0.02737 | best_loss=0.02733
Epoch 9/80: current_loss=0.02774 | best_loss=0.02733
Epoch 10/80: current_loss=0.02771 | best_loss=0.02733
Epoch 11/80: current_loss=0.02741 | best_loss=0.02733
Epoch 12/80: current_loss=0.02776 | best_loss=0.02733
Epoch 13/80: current_loss=0.02786 | best_loss=0.02733
Epoch 14/80: current_loss=0.02793 | best_loss=0.02733
Epoch 15/80: current_loss=0.02826 | best_loss=0.02733
Epoch 16/80: current_loss=0.02810 | best_loss=0.02733
Epoch 17/80: current_loss=0.02799 | best_loss=0.02733
Epoch 18/80: current_loss=0.02891 | best_loss=0.02733
Epoch 19/80: current_loss=0.02740 | best_loss=0.02733
Epoch 20/80: current_loss=0.02753 | best_loss=0.02733
Epoch 21/80: current_loss=0.02766 | best_loss=0.02733
Epoch 22/80: current_loss=0.02747 | best_loss=0.02733
Epoch 23/80: current_loss=0.02744 | best_loss=0.02733
Epoch 24/80: current_loss=0.02766 | best_loss=0.02733
Early Stopping at epoch 24
      explained_var=-0.00751 | mse_loss=0.02780
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02709 | best_loss=0.02709
Epoch 1/80: current_loss=0.02760 | best_loss=0.02709
Epoch 2/80: current_loss=0.02682 | best_loss=0.02682
Epoch 3/80: current_loss=0.02679 | best_loss=0.02679
Epoch 4/80: current_loss=0.02682 | best_loss=0.02679
Epoch 5/80: current_loss=0.02678 | best_loss=0.02678
Epoch 6/80: current_loss=0.02683 | best_loss=0.02678
Epoch 7/80: current_loss=0.02696 | best_loss=0.02678
Epoch 8/80: current_loss=0.02681 | best_loss=0.02678
Epoch 9/80: current_loss=0.02684 | best_loss=0.02678
Epoch 10/80: current_loss=0.02696 | best_loss=0.02678
Epoch 11/80: current_loss=0.02709 | best_loss=0.02678
Epoch 12/80: current_loss=0.02678 | best_loss=0.02678
Epoch 13/80: current_loss=0.02715 | best_loss=0.02678
Epoch 14/80: current_loss=0.02728 | best_loss=0.02678
Epoch 15/80: current_loss=0.02704 | best_loss=0.02678
Epoch 16/80: current_loss=0.02679 | best_loss=0.02678
Epoch 17/80: current_loss=0.02686 | best_loss=0.02678
Epoch 18/80: current_loss=0.02691 | best_loss=0.02678
Epoch 19/80: current_loss=0.02688 | best_loss=0.02678
Epoch 20/80: current_loss=0.02676 | best_loss=0.02676
Epoch 21/80: current_loss=0.02701 | best_loss=0.02676
Epoch 22/80: current_loss=0.02686 | best_loss=0.02676
Epoch 23/80: current_loss=0.02686 | best_loss=0.02676
Epoch 24/80: current_loss=0.02676 | best_loss=0.02676
Epoch 25/80: current_loss=0.02683 | best_loss=0.02676
Epoch 26/80: current_loss=0.02683 | best_loss=0.02676
Epoch 27/80: current_loss=0.02689 | best_loss=0.02676
Epoch 28/80: current_loss=0.02696 | best_loss=0.02676
Epoch 29/80: current_loss=0.02707 | best_loss=0.02676
Epoch 30/80: current_loss=0.02714 | best_loss=0.02676
Epoch 31/80: current_loss=0.02685 | best_loss=0.02676
Epoch 32/80: current_loss=0.02678 | best_loss=0.02676
Epoch 33/80: current_loss=0.02697 | best_loss=0.02676
Epoch 34/80: current_loss=0.02689 | best_loss=0.02676
Epoch 35/80: current_loss=0.02680 | best_loss=0.02676
Epoch 36/80: current_loss=0.02681 | best_loss=0.02676
Epoch 37/80: current_loss=0.02738 | best_loss=0.02676
Epoch 38/80: current_loss=0.02698 | best_loss=0.02676
Epoch 39/80: current_loss=0.02687 | best_loss=0.02676
Epoch 40/80: current_loss=0.02690 | best_loss=0.02676
Early Stopping at epoch 40
      explained_var=0.00170 | mse_loss=0.02496

----------------------------------------------
Params for Trial 74
{'learning_rate': 0.0001, 'weight_decay': 5.1899762924774516e-05, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03415 | best_loss=0.03415
Epoch 1/80: current_loss=0.03210 | best_loss=0.03210
Epoch 2/80: current_loss=0.03130 | best_loss=0.03130
Epoch 3/80: current_loss=0.03025 | best_loss=0.03025
Epoch 4/80: current_loss=0.02960 | best_loss=0.02960
Epoch 5/80: current_loss=0.02962 | best_loss=0.02960
Epoch 6/80: current_loss=0.02952 | best_loss=0.02952
Epoch 7/80: current_loss=0.02943 | best_loss=0.02943
Epoch 8/80: current_loss=0.02913 | best_loss=0.02913
Epoch 9/80: current_loss=0.02905 | best_loss=0.02905
Epoch 10/80: current_loss=0.02899 | best_loss=0.02899
Epoch 11/80: current_loss=0.02899 | best_loss=0.02899
Epoch 12/80: current_loss=0.02899 | best_loss=0.02899
Epoch 13/80: current_loss=0.02931 | best_loss=0.02899
Epoch 14/80: current_loss=0.02925 | best_loss=0.02899
Epoch 15/80: current_loss=0.02931 | best_loss=0.02899
Epoch 16/80: current_loss=0.02910 | best_loss=0.02899
Epoch 17/80: current_loss=0.02924 | best_loss=0.02899
Epoch 18/80: current_loss=0.02910 | best_loss=0.02899
Epoch 19/80: current_loss=0.02971 | best_loss=0.02899
Epoch 20/80: current_loss=0.02917 | best_loss=0.02899
Epoch 21/80: current_loss=0.02912 | best_loss=0.02899
Epoch 22/80: current_loss=0.02900 | best_loss=0.02899
Epoch 23/80: current_loss=0.02894 | best_loss=0.02894
Epoch 24/80: current_loss=0.02909 | best_loss=0.02894
Epoch 25/80: current_loss=0.02908 | best_loss=0.02894
Epoch 26/80: current_loss=0.03052 | best_loss=0.02894
Epoch 27/80: current_loss=0.02921 | best_loss=0.02894
Epoch 28/80: current_loss=0.02961 | best_loss=0.02894
Epoch 29/80: current_loss=0.02913 | best_loss=0.02894
Epoch 30/80: current_loss=0.02912 | best_loss=0.02894
Epoch 31/80: current_loss=0.02898 | best_loss=0.02894
Epoch 32/80: current_loss=0.02924 | best_loss=0.02894
Epoch 33/80: current_loss=0.02944 | best_loss=0.02894
Epoch 34/80: current_loss=0.02911 | best_loss=0.02894
Epoch 35/80: current_loss=0.02929 | best_loss=0.02894
Epoch 36/80: current_loss=0.02905 | best_loss=0.02894
Epoch 37/80: current_loss=0.02908 | best_loss=0.02894
Epoch 38/80: current_loss=0.02906 | best_loss=0.02894
Epoch 39/80: current_loss=0.02918 | best_loss=0.02894
Epoch 40/80: current_loss=0.03003 | best_loss=0.02894
Epoch 41/80: current_loss=0.02923 | best_loss=0.02894
Epoch 42/80: current_loss=0.02909 | best_loss=0.02894
Epoch 43/80: current_loss=0.02917 | best_loss=0.02894
Early Stopping at epoch 43
      explained_var=0.00463 | mse_loss=0.02805

----------------------------------------------
Params for Trial 75
{'learning_rate': 0.1, 'weight_decay': 0.0009113822252789696, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.12432 | best_loss=0.12432
Epoch 1/80: current_loss=0.03463 | best_loss=0.03463
Epoch 2/80: current_loss=0.03083 | best_loss=0.03083
Epoch 3/80: current_loss=0.03201 | best_loss=0.03083
Epoch 4/80: current_loss=0.03318 | best_loss=0.03083
Epoch 5/80: current_loss=0.02983 | best_loss=0.02983
Epoch 6/80: current_loss=0.03487 | best_loss=0.02983
Epoch 7/80: current_loss=0.03163 | best_loss=0.02983
Epoch 8/80: current_loss=0.04117 | best_loss=0.02983
Epoch 9/80: current_loss=0.03575 | best_loss=0.02983
Epoch 10/80: current_loss=0.04973 | best_loss=0.02983
Epoch 11/80: current_loss=0.04674 | best_loss=0.02983
Epoch 12/80: current_loss=0.03946 | best_loss=0.02983
Epoch 13/80: current_loss=0.03613 | best_loss=0.02983
Epoch 14/80: current_loss=0.07506 | best_loss=0.02983
Epoch 15/80: current_loss=0.04410 | best_loss=0.02983
Epoch 16/80: current_loss=0.03808 | best_loss=0.02983
Epoch 17/80: current_loss=0.04587 | best_loss=0.02983
Epoch 18/80: current_loss=0.05526 | best_loss=0.02983
Epoch 19/80: current_loss=0.04128 | best_loss=0.02983
Epoch 20/80: current_loss=0.21327 | best_loss=0.02983
Epoch 21/80: current_loss=0.10329 | best_loss=0.02983
Epoch 22/80: current_loss=1.37245 | best_loss=0.02983
Epoch 23/80: current_loss=0.08917 | best_loss=0.02983
Epoch 24/80: current_loss=0.59160 | best_loss=0.02983
Epoch 25/80: current_loss=0.05211 | best_loss=0.02983
Early Stopping at epoch 25
      explained_var=-0.05062 | mse_loss=0.02964

----------------------------------------------
Params for Trial 76
{'learning_rate': 0.0001, 'weight_decay': 0.000544940541241443, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03164 | best_loss=0.03164
Epoch 1/80: current_loss=0.03029 | best_loss=0.03029
Epoch 2/80: current_loss=0.02980 | best_loss=0.02980
Epoch 3/80: current_loss=0.02966 | best_loss=0.02966
Epoch 4/80: current_loss=0.02902 | best_loss=0.02902
Epoch 5/80: current_loss=0.02899 | best_loss=0.02899
Epoch 6/80: current_loss=0.02901 | best_loss=0.02899
Epoch 7/80: current_loss=0.02890 | best_loss=0.02890
Epoch 8/80: current_loss=0.02888 | best_loss=0.02888
Epoch 9/80: current_loss=0.02884 | best_loss=0.02884
Epoch 10/80: current_loss=0.02921 | best_loss=0.02884
Epoch 11/80: current_loss=0.02886 | best_loss=0.02884
Epoch 12/80: current_loss=0.02917 | best_loss=0.02884
Epoch 13/80: current_loss=0.02888 | best_loss=0.02884
Epoch 14/80: current_loss=0.02895 | best_loss=0.02884
Epoch 15/80: current_loss=0.02896 | best_loss=0.02884
Epoch 16/80: current_loss=0.02916 | best_loss=0.02884
Epoch 17/80: current_loss=0.02915 | best_loss=0.02884
Epoch 18/80: current_loss=0.02974 | best_loss=0.02884
Epoch 19/80: current_loss=0.02910 | best_loss=0.02884
Epoch 20/80: current_loss=0.02909 | best_loss=0.02884
Epoch 21/80: current_loss=0.02898 | best_loss=0.02884
Epoch 22/80: current_loss=0.02895 | best_loss=0.02884
Epoch 23/80: current_loss=0.02902 | best_loss=0.02884
Epoch 24/80: current_loss=0.02908 | best_loss=0.02884
Epoch 25/80: current_loss=0.02906 | best_loss=0.02884
Epoch 26/80: current_loss=0.02904 | best_loss=0.02884
Epoch 27/80: current_loss=0.02901 | best_loss=0.02884
Epoch 28/80: current_loss=0.02902 | best_loss=0.02884
Epoch 29/80: current_loss=0.02928 | best_loss=0.02884
Early Stopping at epoch 29
      explained_var=0.00696 | mse_loss=0.02797
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02475 | best_loss=0.02475
Epoch 1/80: current_loss=0.02482 | best_loss=0.02475
Epoch 2/80: current_loss=0.02481 | best_loss=0.02475
Epoch 3/80: current_loss=0.02481 | best_loss=0.02475
Epoch 4/80: current_loss=0.02483 | best_loss=0.02475
Epoch 5/80: current_loss=0.02499 | best_loss=0.02475
Epoch 6/80: current_loss=0.02489 | best_loss=0.02475
Epoch 7/80: current_loss=0.02494 | best_loss=0.02475
Epoch 8/80: current_loss=0.02486 | best_loss=0.02475
Epoch 9/80: current_loss=0.02483 | best_loss=0.02475
Epoch 10/80: current_loss=0.02500 | best_loss=0.02475
Epoch 11/80: current_loss=0.02495 | best_loss=0.02475
Epoch 12/80: current_loss=0.02514 | best_loss=0.02475
Epoch 13/80: current_loss=0.02485 | best_loss=0.02475
Epoch 14/80: current_loss=0.02522 | best_loss=0.02475
Epoch 15/80: current_loss=0.02490 | best_loss=0.02475
Epoch 16/80: current_loss=0.02503 | best_loss=0.02475
Epoch 17/80: current_loss=0.02538 | best_loss=0.02475
Epoch 18/80: current_loss=0.02498 | best_loss=0.02475
Epoch 19/80: current_loss=0.02516 | best_loss=0.02475
Epoch 20/80: current_loss=0.02510 | best_loss=0.02475
Early Stopping at epoch 20
      explained_var=0.00296 | mse_loss=0.02513
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02790 | best_loss=0.02790
Epoch 1/80: current_loss=0.02775 | best_loss=0.02775
Epoch 2/80: current_loss=0.02755 | best_loss=0.02755
Epoch 3/80: current_loss=0.02736 | best_loss=0.02736
Epoch 4/80: current_loss=0.02796 | best_loss=0.02736
Epoch 5/80: current_loss=0.02739 | best_loss=0.02736
Epoch 6/80: current_loss=0.02787 | best_loss=0.02736
Epoch 7/80: current_loss=0.02814 | best_loss=0.02736
Epoch 8/80: current_loss=0.02791 | best_loss=0.02736
Epoch 9/80: current_loss=0.02798 | best_loss=0.02736
Epoch 10/80: current_loss=0.02767 | best_loss=0.02736
Epoch 11/80: current_loss=0.02881 | best_loss=0.02736
Epoch 12/80: current_loss=0.02763 | best_loss=0.02736
Epoch 13/80: current_loss=0.02853 | best_loss=0.02736
Epoch 14/80: current_loss=0.02829 | best_loss=0.02736
Epoch 15/80: current_loss=0.02800 | best_loss=0.02736
Epoch 16/80: current_loss=0.02809 | best_loss=0.02736
Epoch 17/80: current_loss=0.02793 | best_loss=0.02736
Epoch 18/80: current_loss=0.02781 | best_loss=0.02736
Epoch 19/80: current_loss=0.02751 | best_loss=0.02736
Epoch 20/80: current_loss=0.02763 | best_loss=0.02736
Epoch 21/80: current_loss=0.02761 | best_loss=0.02736
Epoch 22/80: current_loss=0.02772 | best_loss=0.02736
Epoch 23/80: current_loss=0.02797 | best_loss=0.02736
Early Stopping at epoch 23
      explained_var=-0.01047 | mse_loss=0.02785
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02670 | best_loss=0.02670
Epoch 1/80: current_loss=0.02670 | best_loss=0.02670
Epoch 2/80: current_loss=0.02668 | best_loss=0.02668
Epoch 3/80: current_loss=0.02669 | best_loss=0.02668
Epoch 4/80: current_loss=0.02682 | best_loss=0.02668
Epoch 5/80: current_loss=0.02674 | best_loss=0.02668
Epoch 6/80: current_loss=0.02698 | best_loss=0.02668
Epoch 7/80: current_loss=0.02689 | best_loss=0.02668
Epoch 8/80: current_loss=0.02672 | best_loss=0.02668
Epoch 9/80: current_loss=0.02703 | best_loss=0.02668
Epoch 10/80: current_loss=0.02680 | best_loss=0.02668
Epoch 11/80: current_loss=0.02673 | best_loss=0.02668
Epoch 12/80: current_loss=0.02701 | best_loss=0.02668
Epoch 13/80: current_loss=0.02687 | best_loss=0.02668
Epoch 14/80: current_loss=0.02678 | best_loss=0.02668
Epoch 15/80: current_loss=0.02720 | best_loss=0.02668
Epoch 16/80: current_loss=0.02793 | best_loss=0.02668
Epoch 17/80: current_loss=0.02672 | best_loss=0.02668
Epoch 18/80: current_loss=0.02699 | best_loss=0.02668
Epoch 19/80: current_loss=0.02677 | best_loss=0.02668
Epoch 20/80: current_loss=0.02696 | best_loss=0.02668
Epoch 21/80: current_loss=0.02718 | best_loss=0.02668
Epoch 22/80: current_loss=0.02676 | best_loss=0.02668
Early Stopping at epoch 22
      explained_var=0.00230 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02971 | best_loss=0.02971
Epoch 1/80: current_loss=0.02991 | best_loss=0.02971
Epoch 2/80: current_loss=0.02970 | best_loss=0.02970
Epoch 3/80: current_loss=0.02970 | best_loss=0.02970
Epoch 4/80: current_loss=0.02972 | best_loss=0.02970
Epoch 5/80: current_loss=0.02998 | best_loss=0.02970
Epoch 6/80: current_loss=0.02973 | best_loss=0.02970
Epoch 7/80: current_loss=0.03000 | best_loss=0.02970
Epoch 8/80: current_loss=0.03003 | best_loss=0.02970
Epoch 9/80: current_loss=0.02965 | best_loss=0.02965
Epoch 10/80: current_loss=0.02970 | best_loss=0.02965
Epoch 11/80: current_loss=0.02980 | best_loss=0.02965
Epoch 12/80: current_loss=0.02965 | best_loss=0.02965
Epoch 13/80: current_loss=0.02982 | best_loss=0.02965
Epoch 14/80: current_loss=0.02976 | best_loss=0.02965
Epoch 15/80: current_loss=0.02978 | best_loss=0.02965
Epoch 16/80: current_loss=0.02983 | best_loss=0.02965
Epoch 17/80: current_loss=0.02968 | best_loss=0.02965
Epoch 18/80: current_loss=0.02984 | best_loss=0.02965
Epoch 19/80: current_loss=0.02967 | best_loss=0.02965
Epoch 20/80: current_loss=0.02970 | best_loss=0.02965
Epoch 21/80: current_loss=0.02968 | best_loss=0.02965
Epoch 22/80: current_loss=0.02975 | best_loss=0.02965
Epoch 23/80: current_loss=0.02982 | best_loss=0.02965
Epoch 24/80: current_loss=0.02971 | best_loss=0.02965
Epoch 25/80: current_loss=0.02974 | best_loss=0.02965
Epoch 26/80: current_loss=0.02998 | best_loss=0.02965
Epoch 27/80: current_loss=0.02972 | best_loss=0.02965
Epoch 28/80: current_loss=0.02972 | best_loss=0.02965
Epoch 29/80: current_loss=0.02978 | best_loss=0.02965
Epoch 30/80: current_loss=0.02979 | best_loss=0.02965
Epoch 31/80: current_loss=0.02968 | best_loss=0.02965
Epoch 32/80: current_loss=0.02997 | best_loss=0.02965
Early Stopping at epoch 32
      explained_var=0.00285 | mse_loss=0.02868
----------------------------------------------
Average early_stopping_point: 5| avg_exp_var=0.00092| avg_loss=0.02691
----------------------------------------------


----------------------------------------------
Params for Trial 77
{'learning_rate': 0.0001, 'weight_decay': 0.0015091036450996312, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03331 | best_loss=0.03331
Epoch 1/80: current_loss=0.03208 | best_loss=0.03208
Epoch 2/80: current_loss=0.03119 | best_loss=0.03119
Epoch 3/80: current_loss=0.03060 | best_loss=0.03060
Epoch 4/80: current_loss=0.03015 | best_loss=0.03015
Epoch 5/80: current_loss=0.02988 | best_loss=0.02988
Epoch 6/80: current_loss=0.02962 | best_loss=0.02962
Epoch 7/80: current_loss=0.02946 | best_loss=0.02946
Epoch 8/80: current_loss=0.02936 | best_loss=0.02936
Epoch 9/80: current_loss=0.02920 | best_loss=0.02920
Epoch 10/80: current_loss=0.02898 | best_loss=0.02898
Epoch 11/80: current_loss=0.02899 | best_loss=0.02898
Epoch 12/80: current_loss=0.02894 | best_loss=0.02894
Epoch 13/80: current_loss=0.02892 | best_loss=0.02892
Epoch 14/80: current_loss=0.02901 | best_loss=0.02892
Epoch 15/80: current_loss=0.02897 | best_loss=0.02892
Epoch 16/80: current_loss=0.02896 | best_loss=0.02892
Epoch 17/80: current_loss=0.02902 | best_loss=0.02892
Epoch 18/80: current_loss=0.02890 | best_loss=0.02890
Epoch 19/80: current_loss=0.02894 | best_loss=0.02890
Epoch 20/80: current_loss=0.02892 | best_loss=0.02890
Epoch 21/80: current_loss=0.02889 | best_loss=0.02889
Epoch 22/80: current_loss=0.02894 | best_loss=0.02889
Epoch 23/80: current_loss=0.02894 | best_loss=0.02889
Epoch 24/80: current_loss=0.02903 | best_loss=0.02889
Epoch 25/80: current_loss=0.02897 | best_loss=0.02889
Epoch 26/80: current_loss=0.02888 | best_loss=0.02888
Epoch 27/80: current_loss=0.02887 | best_loss=0.02887
Epoch 28/80: current_loss=0.02901 | best_loss=0.02887
Epoch 29/80: current_loss=0.02890 | best_loss=0.02887
Epoch 30/80: current_loss=0.02892 | best_loss=0.02887
Epoch 31/80: current_loss=0.02900 | best_loss=0.02887
Epoch 32/80: current_loss=0.02896 | best_loss=0.02887
Epoch 33/80: current_loss=0.02890 | best_loss=0.02887
Epoch 34/80: current_loss=0.02895 | best_loss=0.02887
Epoch 35/80: current_loss=0.02889 | best_loss=0.02887
Epoch 36/80: current_loss=0.02903 | best_loss=0.02887
Epoch 37/80: current_loss=0.02887 | best_loss=0.02887
Epoch 38/80: current_loss=0.02885 | best_loss=0.02885
Epoch 39/80: current_loss=0.02889 | best_loss=0.02885
Epoch 40/80: current_loss=0.02890 | best_loss=0.02885
Epoch 41/80: current_loss=0.02899 | best_loss=0.02885
Epoch 42/80: current_loss=0.02898 | best_loss=0.02885
Epoch 43/80: current_loss=0.02890 | best_loss=0.02885
Epoch 44/80: current_loss=0.02887 | best_loss=0.02885
Epoch 45/80: current_loss=0.02891 | best_loss=0.02885
Epoch 46/80: current_loss=0.02900 | best_loss=0.02885
Epoch 47/80: current_loss=0.02892 | best_loss=0.02885
Epoch 48/80: current_loss=0.02889 | best_loss=0.02885
Epoch 49/80: current_loss=0.02889 | best_loss=0.02885
Epoch 50/80: current_loss=0.02914 | best_loss=0.02885
Epoch 51/80: current_loss=0.02891 | best_loss=0.02885
Epoch 52/80: current_loss=0.02890 | best_loss=0.02885
Epoch 53/80: current_loss=0.02889 | best_loss=0.02885
Epoch 54/80: current_loss=0.02891 | best_loss=0.02885
Epoch 55/80: current_loss=0.02897 | best_loss=0.02885
Epoch 56/80: current_loss=0.02889 | best_loss=0.02885
Epoch 57/80: current_loss=0.02895 | best_loss=0.02885
Epoch 58/80: current_loss=0.02889 | best_loss=0.02885
Early Stopping at epoch 58
      explained_var=0.00636 | mse_loss=0.02799
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02488 | best_loss=0.02488
Epoch 1/80: current_loss=0.02493 | best_loss=0.02488
Epoch 2/80: current_loss=0.02488 | best_loss=0.02488
Epoch 3/80: current_loss=0.02501 | best_loss=0.02488
Epoch 4/80: current_loss=0.02505 | best_loss=0.02488
Epoch 5/80: current_loss=0.02492 | best_loss=0.02488
Epoch 6/80: current_loss=0.02493 | best_loss=0.02488
Epoch 7/80: current_loss=0.02555 | best_loss=0.02488
Epoch 8/80: current_loss=0.02537 | best_loss=0.02488
Epoch 9/80: current_loss=0.02504 | best_loss=0.02488
Epoch 10/80: current_loss=0.02491 | best_loss=0.02488
Epoch 11/80: current_loss=0.02517 | best_loss=0.02488
Epoch 12/80: current_loss=0.02514 | best_loss=0.02488
Epoch 13/80: current_loss=0.02509 | best_loss=0.02488
Epoch 14/80: current_loss=0.02488 | best_loss=0.02488
Epoch 15/80: current_loss=0.02491 | best_loss=0.02488
Epoch 16/80: current_loss=0.02522 | best_loss=0.02488
Epoch 17/80: current_loss=0.02518 | best_loss=0.02488
Epoch 18/80: current_loss=0.02484 | best_loss=0.02484
Epoch 19/80: current_loss=0.02488 | best_loss=0.02484
Epoch 20/80: current_loss=0.02521 | best_loss=0.02484
Epoch 21/80: current_loss=0.02498 | best_loss=0.02484
Epoch 22/80: current_loss=0.02490 | best_loss=0.02484
Epoch 23/80: current_loss=0.02492 | best_loss=0.02484
Epoch 24/80: current_loss=0.02507 | best_loss=0.02484
Epoch 25/80: current_loss=0.02502 | best_loss=0.02484
Epoch 26/80: current_loss=0.02492 | best_loss=0.02484
Epoch 27/80: current_loss=0.02525 | best_loss=0.02484
Epoch 28/80: current_loss=0.02496 | best_loss=0.02484
Epoch 29/80: current_loss=0.02488 | best_loss=0.02484
Epoch 30/80: current_loss=0.02501 | best_loss=0.02484
Epoch 31/80: current_loss=0.02494 | best_loss=0.02484
Epoch 32/80: current_loss=0.02524 | best_loss=0.02484
Epoch 33/80: current_loss=0.02564 | best_loss=0.02484
Epoch 34/80: current_loss=0.02488 | best_loss=0.02484
Epoch 35/80: current_loss=0.02498 | best_loss=0.02484
Epoch 36/80: current_loss=0.02500 | best_loss=0.02484
Epoch 37/80: current_loss=0.02498 | best_loss=0.02484
Epoch 38/80: current_loss=0.02488 | best_loss=0.02484
Early Stopping at epoch 38
      explained_var=0.00309 | mse_loss=0.02513
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02813 | best_loss=0.02813
Epoch 1/80: current_loss=0.02864 | best_loss=0.02813
Epoch 2/80: current_loss=0.02822 | best_loss=0.02813
Epoch 3/80: current_loss=0.02832 | best_loss=0.02813
Epoch 4/80: current_loss=0.02782 | best_loss=0.02782
Epoch 5/80: current_loss=0.02821 | best_loss=0.02782
Epoch 6/80: current_loss=0.02795 | best_loss=0.02782
Epoch 7/80: current_loss=0.02800 | best_loss=0.02782
Epoch 8/80: current_loss=0.02802 | best_loss=0.02782
Epoch 9/80: current_loss=0.02818 | best_loss=0.02782
Epoch 10/80: current_loss=0.02773 | best_loss=0.02773
Epoch 11/80: current_loss=0.02764 | best_loss=0.02764
Epoch 12/80: current_loss=0.02811 | best_loss=0.02764
Epoch 13/80: current_loss=0.02781 | best_loss=0.02764
Epoch 14/80: current_loss=0.02818 | best_loss=0.02764
Epoch 15/80: current_loss=0.02860 | best_loss=0.02764
Epoch 16/80: current_loss=0.02786 | best_loss=0.02764
Epoch 17/80: current_loss=0.02846 | best_loss=0.02764
Epoch 18/80: current_loss=0.02781 | best_loss=0.02764
Epoch 19/80: current_loss=0.02809 | best_loss=0.02764
Epoch 20/80: current_loss=0.02791 | best_loss=0.02764
Epoch 21/80: current_loss=0.02791 | best_loss=0.02764
Epoch 22/80: current_loss=0.02822 | best_loss=0.02764
Epoch 23/80: current_loss=0.02846 | best_loss=0.02764
Epoch 24/80: current_loss=0.02798 | best_loss=0.02764
Epoch 25/80: current_loss=0.02776 | best_loss=0.02764
Epoch 26/80: current_loss=0.02789 | best_loss=0.02764
Epoch 27/80: current_loss=0.02808 | best_loss=0.02764
Epoch 28/80: current_loss=0.02806 | best_loss=0.02764
Epoch 29/80: current_loss=0.02837 | best_loss=0.02764
Epoch 30/80: current_loss=0.02768 | best_loss=0.02764
Epoch 31/80: current_loss=0.02787 | best_loss=0.02764
Early Stopping at epoch 31
      explained_var=-0.01414 | mse_loss=0.02816
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02685 | best_loss=0.02685
Epoch 1/80: current_loss=0.02671 | best_loss=0.02671
Epoch 2/80: current_loss=0.02667 | best_loss=0.02667
Epoch 3/80: current_loss=0.02671 | best_loss=0.02667
Epoch 4/80: current_loss=0.02668 | best_loss=0.02667
Epoch 5/80: current_loss=0.02670 | best_loss=0.02667
Epoch 6/80: current_loss=0.02668 | best_loss=0.02667
Epoch 7/80: current_loss=0.02670 | best_loss=0.02667
Epoch 8/80: current_loss=0.02672 | best_loss=0.02667
Epoch 9/80: current_loss=0.02668 | best_loss=0.02667
Epoch 10/80: current_loss=0.02669 | best_loss=0.02667
Epoch 11/80: current_loss=0.02670 | best_loss=0.02667
Epoch 12/80: current_loss=0.02669 | best_loss=0.02667
Epoch 13/80: current_loss=0.02669 | best_loss=0.02667
Epoch 14/80: current_loss=0.02675 | best_loss=0.02667
Epoch 15/80: current_loss=0.02670 | best_loss=0.02667
Epoch 16/80: current_loss=0.02673 | best_loss=0.02667
Epoch 17/80: current_loss=0.02671 | best_loss=0.02667
Epoch 18/80: current_loss=0.02679 | best_loss=0.02667
Epoch 19/80: current_loss=0.02671 | best_loss=0.02667
Epoch 20/80: current_loss=0.02675 | best_loss=0.02667
Epoch 21/80: current_loss=0.02681 | best_loss=0.02667
Epoch 22/80: current_loss=0.02672 | best_loss=0.02667
Early Stopping at epoch 22
      explained_var=0.00196 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02986 | best_loss=0.02986
Epoch 1/80: current_loss=0.02972 | best_loss=0.02972
Epoch 2/80: current_loss=0.02974 | best_loss=0.02972
Epoch 3/80: current_loss=0.02970 | best_loss=0.02970
Epoch 4/80: current_loss=0.02970 | best_loss=0.02970
Epoch 5/80: current_loss=0.02971 | best_loss=0.02970
Epoch 6/80: current_loss=0.02985 | best_loss=0.02970
Epoch 7/80: current_loss=0.02970 | best_loss=0.02970
Epoch 8/80: current_loss=0.02974 | best_loss=0.02970
Epoch 9/80: current_loss=0.02976 | best_loss=0.02970
Epoch 10/80: current_loss=0.02970 | best_loss=0.02970
Epoch 11/80: current_loss=0.02973 | best_loss=0.02970
Epoch 12/80: current_loss=0.02973 | best_loss=0.02970
Epoch 13/80: current_loss=0.02973 | best_loss=0.02970
Epoch 14/80: current_loss=0.02982 | best_loss=0.02970
Epoch 15/80: current_loss=0.02970 | best_loss=0.02970
Epoch 16/80: current_loss=0.02974 | best_loss=0.02970
Epoch 17/80: current_loss=0.02974 | best_loss=0.02970
Epoch 18/80: current_loss=0.03010 | best_loss=0.02970
Epoch 19/80: current_loss=0.02974 | best_loss=0.02970
Epoch 20/80: current_loss=0.02971 | best_loss=0.02970
Epoch 21/80: current_loss=0.02976 | best_loss=0.02970
Epoch 22/80: current_loss=0.02973 | best_loss=0.02970
Epoch 23/80: current_loss=0.02974 | best_loss=0.02970
Epoch 24/80: current_loss=0.02974 | best_loss=0.02970
Epoch 25/80: current_loss=0.02971 | best_loss=0.02970
Epoch 26/80: current_loss=0.02970 | best_loss=0.02970
Epoch 27/80: current_loss=0.02976 | best_loss=0.02970
Epoch 28/80: current_loss=0.02979 | best_loss=0.02970
Epoch 29/80: current_loss=0.02970 | best_loss=0.02970
Epoch 30/80: current_loss=0.02977 | best_loss=0.02970
Epoch 31/80: current_loss=0.02972 | best_loss=0.02970
Epoch 32/80: current_loss=0.02977 | best_loss=0.02970
Epoch 33/80: current_loss=0.02970 | best_loss=0.02970
Epoch 34/80: current_loss=0.02972 | best_loss=0.02970
Epoch 35/80: current_loss=0.02999 | best_loss=0.02970
Epoch 36/80: current_loss=0.02970 | best_loss=0.02970
Epoch 37/80: current_loss=0.02982 | best_loss=0.02970
Epoch 38/80: current_loss=0.02971 | best_loss=0.02970
Epoch 39/80: current_loss=0.02975 | best_loss=0.02970
Epoch 40/80: current_loss=0.02970 | best_loss=0.02970
Epoch 41/80: current_loss=0.02972 | best_loss=0.02970
Epoch 42/80: current_loss=0.02977 | best_loss=0.02970
Epoch 43/80: current_loss=0.02974 | best_loss=0.02970
Epoch 44/80: current_loss=0.02980 | best_loss=0.02970
Epoch 45/80: current_loss=0.02971 | best_loss=0.02970
Epoch 46/80: current_loss=0.02971 | best_loss=0.02970
Early Stopping at epoch 46
      explained_var=0.00162 | mse_loss=0.02871
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=-0.00022| avg_loss=0.02699
----------------------------------------------


----------------------------------------------
Params for Trial 78
{'learning_rate': 0.0001, 'weight_decay': 0.0012257808116744372, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03068 | best_loss=0.03068
Epoch 1/80: current_loss=0.02957 | best_loss=0.02957
Epoch 2/80: current_loss=0.02928 | best_loss=0.02928
Epoch 3/80: current_loss=0.02928 | best_loss=0.02928
Epoch 4/80: current_loss=0.02950 | best_loss=0.02928
Epoch 5/80: current_loss=0.02930 | best_loss=0.02928
Epoch 6/80: current_loss=0.02905 | best_loss=0.02905
Epoch 7/80: current_loss=0.02911 | best_loss=0.02905
Epoch 8/80: current_loss=0.02908 | best_loss=0.02905
Epoch 9/80: current_loss=0.02905 | best_loss=0.02905
Epoch 10/80: current_loss=0.02906 | best_loss=0.02905
Epoch 11/80: current_loss=0.02903 | best_loss=0.02903
Epoch 12/80: current_loss=0.02916 | best_loss=0.02903
Epoch 13/80: current_loss=0.02902 | best_loss=0.02902
Epoch 14/80: current_loss=0.02925 | best_loss=0.02902
Epoch 15/80: current_loss=0.02894 | best_loss=0.02894
Epoch 16/80: current_loss=0.02921 | best_loss=0.02894
Epoch 17/80: current_loss=0.02897 | best_loss=0.02894
Epoch 18/80: current_loss=0.02896 | best_loss=0.02894
Epoch 19/80: current_loss=0.02909 | best_loss=0.02894
Epoch 20/80: current_loss=0.02914 | best_loss=0.02894
Epoch 21/80: current_loss=0.02900 | best_loss=0.02894
Epoch 22/80: current_loss=0.02922 | best_loss=0.02894
Epoch 23/80: current_loss=0.02900 | best_loss=0.02894
Epoch 24/80: current_loss=0.02895 | best_loss=0.02894
Epoch 25/80: current_loss=0.02914 | best_loss=0.02894
Epoch 26/80: current_loss=0.02973 | best_loss=0.02894
Epoch 27/80: current_loss=0.02898 | best_loss=0.02894
Epoch 28/80: current_loss=0.02895 | best_loss=0.02894
Epoch 29/80: current_loss=0.02900 | best_loss=0.02894
Epoch 30/80: current_loss=0.02908 | best_loss=0.02894
Epoch 31/80: current_loss=0.02898 | best_loss=0.02894
Epoch 32/80: current_loss=0.02899 | best_loss=0.02894
Epoch 33/80: current_loss=0.02897 | best_loss=0.02894
Epoch 34/80: current_loss=0.02892 | best_loss=0.02892
Epoch 35/80: current_loss=0.02896 | best_loss=0.02892
Epoch 36/80: current_loss=0.02897 | best_loss=0.02892
Epoch 37/80: current_loss=0.02952 | best_loss=0.02892
Epoch 38/80: current_loss=0.02899 | best_loss=0.02892
Epoch 39/80: current_loss=0.02898 | best_loss=0.02892
Epoch 40/80: current_loss=0.02900 | best_loss=0.02892
Epoch 41/80: current_loss=0.02897 | best_loss=0.02892
Epoch 42/80: current_loss=0.02925 | best_loss=0.02892
Epoch 43/80: current_loss=0.02899 | best_loss=0.02892
Epoch 44/80: current_loss=0.02903 | best_loss=0.02892
Epoch 45/80: current_loss=0.02900 | best_loss=0.02892
Epoch 46/80: current_loss=0.02902 | best_loss=0.02892
Epoch 47/80: current_loss=0.02907 | best_loss=0.02892
Epoch 48/80: current_loss=0.02906 | best_loss=0.02892
Epoch 49/80: current_loss=0.02931 | best_loss=0.02892
Epoch 50/80: current_loss=0.02912 | best_loss=0.02892
Epoch 51/80: current_loss=0.02901 | best_loss=0.02892
Epoch 52/80: current_loss=0.02942 | best_loss=0.02892
Epoch 53/80: current_loss=0.02908 | best_loss=0.02892
Epoch 54/80: current_loss=0.02904 | best_loss=0.02892
Early Stopping at epoch 54
      explained_var=0.00349 | mse_loss=0.02807

----------------------------------------------
Params for Trial 79
{'learning_rate': 1e-05, 'weight_decay': 0.0002962168766393867, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03446 | best_loss=0.03446
Epoch 1/80: current_loss=0.03269 | best_loss=0.03269
Epoch 2/80: current_loss=0.03182 | best_loss=0.03182
Epoch 3/80: current_loss=0.03151 | best_loss=0.03151
Epoch 4/80: current_loss=0.03094 | best_loss=0.03094
Epoch 5/80: current_loss=0.03052 | best_loss=0.03052
Epoch 6/80: current_loss=0.03021 | best_loss=0.03021
Epoch 7/80: current_loss=0.02994 | best_loss=0.02994
Epoch 8/80: current_loss=0.02970 | best_loss=0.02970
Epoch 9/80: current_loss=0.02972 | best_loss=0.02970
Epoch 10/80: current_loss=0.02962 | best_loss=0.02962
Epoch 11/80: current_loss=0.02932 | best_loss=0.02932
Epoch 12/80: current_loss=0.02927 | best_loss=0.02927
Epoch 13/80: current_loss=0.02911 | best_loss=0.02911
Epoch 14/80: current_loss=0.02907 | best_loss=0.02907
Epoch 15/80: current_loss=0.02912 | best_loss=0.02907
Epoch 16/80: current_loss=0.02903 | best_loss=0.02903
Epoch 17/80: current_loss=0.02902 | best_loss=0.02902
Epoch 18/80: current_loss=0.02921 | best_loss=0.02902
Epoch 19/80: current_loss=0.02902 | best_loss=0.02902
Epoch 20/80: current_loss=0.02898 | best_loss=0.02898
Epoch 21/80: current_loss=0.02897 | best_loss=0.02897
Epoch 22/80: current_loss=0.02906 | best_loss=0.02897
Epoch 23/80: current_loss=0.02903 | best_loss=0.02897
Epoch 24/80: current_loss=0.02911 | best_loss=0.02897
Epoch 25/80: current_loss=0.02898 | best_loss=0.02897
Epoch 26/80: current_loss=0.02900 | best_loss=0.02897
Epoch 27/80: current_loss=0.02911 | best_loss=0.02897
Epoch 28/80: current_loss=0.02897 | best_loss=0.02897
Epoch 29/80: current_loss=0.02907 | best_loss=0.02897
Epoch 30/80: current_loss=0.02898 | best_loss=0.02897
Epoch 31/80: current_loss=0.02904 | best_loss=0.02897
Epoch 32/80: current_loss=0.02898 | best_loss=0.02897
Epoch 33/80: current_loss=0.02904 | best_loss=0.02897
Epoch 34/80: current_loss=0.02895 | best_loss=0.02895
Epoch 35/80: current_loss=0.02892 | best_loss=0.02892
Epoch 36/80: current_loss=0.02890 | best_loss=0.02890
Epoch 37/80: current_loss=0.02893 | best_loss=0.02890
Epoch 38/80: current_loss=0.02898 | best_loss=0.02890
Epoch 39/80: current_loss=0.02891 | best_loss=0.02890
Epoch 40/80: current_loss=0.02913 | best_loss=0.02890
Epoch 41/80: current_loss=0.02896 | best_loss=0.02890
Epoch 42/80: current_loss=0.02898 | best_loss=0.02890
Epoch 43/80: current_loss=0.02899 | best_loss=0.02890
Epoch 44/80: current_loss=0.02898 | best_loss=0.02890
Epoch 45/80: current_loss=0.02901 | best_loss=0.02890
Epoch 46/80: current_loss=0.02905 | best_loss=0.02890
Epoch 47/80: current_loss=0.02900 | best_loss=0.02890
Epoch 48/80: current_loss=0.02906 | best_loss=0.02890
Epoch 49/80: current_loss=0.02902 | best_loss=0.02890
Epoch 50/80: current_loss=0.02922 | best_loss=0.02890
Epoch 51/80: current_loss=0.02904 | best_loss=0.02890
Epoch 52/80: current_loss=0.02913 | best_loss=0.02890
Epoch 53/80: current_loss=0.02906 | best_loss=0.02890
Epoch 54/80: current_loss=0.02911 | best_loss=0.02890
Epoch 55/80: current_loss=0.02920 | best_loss=0.02890
Epoch 56/80: current_loss=0.02914 | best_loss=0.02890
Early Stopping at epoch 56
      explained_var=0.00415 | mse_loss=0.02805

----------------------------------------------
Params for Trial 80
{'learning_rate': 0.0001, 'weight_decay': 0.0007965758461422507, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03370 | best_loss=0.03370
Epoch 1/80: current_loss=0.03174 | best_loss=0.03174
Epoch 2/80: current_loss=0.03058 | best_loss=0.03058
Epoch 3/80: current_loss=0.02993 | best_loss=0.02993
Epoch 4/80: current_loss=0.02963 | best_loss=0.02963
Epoch 5/80: current_loss=0.02944 | best_loss=0.02944
Epoch 6/80: current_loss=0.02961 | best_loss=0.02944
Epoch 7/80: current_loss=0.02932 | best_loss=0.02932
Epoch 8/80: current_loss=0.02924 | best_loss=0.02924
Epoch 9/80: current_loss=0.02924 | best_loss=0.02924
Epoch 10/80: current_loss=0.02917 | best_loss=0.02917
Epoch 11/80: current_loss=0.02918 | best_loss=0.02917
Epoch 12/80: current_loss=0.02910 | best_loss=0.02910
Epoch 13/80: current_loss=0.02921 | best_loss=0.02910
Epoch 14/80: current_loss=0.02905 | best_loss=0.02905
Epoch 15/80: current_loss=0.02907 | best_loss=0.02905
Epoch 16/80: current_loss=0.02906 | best_loss=0.02905
Epoch 17/80: current_loss=0.02925 | best_loss=0.02905
Epoch 18/80: current_loss=0.02905 | best_loss=0.02905
Epoch 19/80: current_loss=0.02908 | best_loss=0.02905
Epoch 20/80: current_loss=0.02905 | best_loss=0.02905
Epoch 21/80: current_loss=0.02912 | best_loss=0.02905
Epoch 22/80: current_loss=0.02903 | best_loss=0.02903
Epoch 23/80: current_loss=0.02900 | best_loss=0.02900
Epoch 24/80: current_loss=0.02896 | best_loss=0.02896
Epoch 25/80: current_loss=0.02903 | best_loss=0.02896
Epoch 26/80: current_loss=0.02898 | best_loss=0.02896
Epoch 27/80: current_loss=0.02903 | best_loss=0.02896
Epoch 28/80: current_loss=0.02944 | best_loss=0.02896
Epoch 29/80: current_loss=0.02896 | best_loss=0.02896
Epoch 30/80: current_loss=0.02890 | best_loss=0.02890
Epoch 31/80: current_loss=0.02919 | best_loss=0.02890
Epoch 32/80: current_loss=0.02903 | best_loss=0.02890
Epoch 33/80: current_loss=0.02913 | best_loss=0.02890
Epoch 34/80: current_loss=0.02894 | best_loss=0.02890
Epoch 35/80: current_loss=0.02896 | best_loss=0.02890
Epoch 36/80: current_loss=0.02899 | best_loss=0.02890
Epoch 37/80: current_loss=0.02947 | best_loss=0.02890
Epoch 38/80: current_loss=0.02896 | best_loss=0.02890
Epoch 39/80: current_loss=0.02927 | best_loss=0.02890
Epoch 40/80: current_loss=0.02897 | best_loss=0.02890
Epoch 41/80: current_loss=0.02960 | best_loss=0.02890
Epoch 42/80: current_loss=0.02913 | best_loss=0.02890
Epoch 43/80: current_loss=0.02903 | best_loss=0.02890
Epoch 44/80: current_loss=0.02901 | best_loss=0.02890
Epoch 45/80: current_loss=0.02909 | best_loss=0.02890
Epoch 46/80: current_loss=0.02904 | best_loss=0.02890
Epoch 47/80: current_loss=0.02910 | best_loss=0.02890
Epoch 48/80: current_loss=0.02931 | best_loss=0.02890
Epoch 49/80: current_loss=0.02930 | best_loss=0.02890
Epoch 50/80: current_loss=0.02915 | best_loss=0.02890
Early Stopping at epoch 50
      explained_var=0.00554 | mse_loss=0.02801
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02522 | best_loss=0.02522
Epoch 1/80: current_loss=0.02675 | best_loss=0.02522
Epoch 2/80: current_loss=0.02518 | best_loss=0.02518
Epoch 3/80: current_loss=0.02497 | best_loss=0.02497
Epoch 4/80: current_loss=0.02620 | best_loss=0.02497
Epoch 5/80: current_loss=0.02498 | best_loss=0.02497
Epoch 6/80: current_loss=0.02502 | best_loss=0.02497
Epoch 7/80: current_loss=0.02568 | best_loss=0.02497
Epoch 8/80: current_loss=0.02501 | best_loss=0.02497
Epoch 9/80: current_loss=0.02555 | best_loss=0.02497
Epoch 10/80: current_loss=0.02498 | best_loss=0.02497
Epoch 11/80: current_loss=0.02491 | best_loss=0.02491
Epoch 12/80: current_loss=0.02498 | best_loss=0.02491
Epoch 13/80: current_loss=0.02486 | best_loss=0.02486
Epoch 14/80: current_loss=0.02483 | best_loss=0.02483
Epoch 15/80: current_loss=0.02487 | best_loss=0.02483
Epoch 16/80: current_loss=0.02486 | best_loss=0.02483
Epoch 17/80: current_loss=0.02486 | best_loss=0.02483
Epoch 18/80: current_loss=0.02487 | best_loss=0.02483
Epoch 19/80: current_loss=0.02493 | best_loss=0.02483
Epoch 20/80: current_loss=0.02537 | best_loss=0.02483
Epoch 21/80: current_loss=0.02487 | best_loss=0.02483
Epoch 22/80: current_loss=0.02493 | best_loss=0.02483
Epoch 23/80: current_loss=0.02532 | best_loss=0.02483
Epoch 24/80: current_loss=0.02491 | best_loss=0.02483
Epoch 25/80: current_loss=0.02498 | best_loss=0.02483
Epoch 26/80: current_loss=0.02553 | best_loss=0.02483
Epoch 27/80: current_loss=0.02525 | best_loss=0.02483
Epoch 28/80: current_loss=0.02491 | best_loss=0.02483
Epoch 29/80: current_loss=0.02504 | best_loss=0.02483
Epoch 30/80: current_loss=0.02507 | best_loss=0.02483
Epoch 31/80: current_loss=0.02555 | best_loss=0.02483
Epoch 32/80: current_loss=0.02571 | best_loss=0.02483
Epoch 33/80: current_loss=0.02538 | best_loss=0.02483
Epoch 34/80: current_loss=0.02544 | best_loss=0.02483
Early Stopping at epoch 34
      explained_var=0.00282 | mse_loss=0.02513
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02756 | best_loss=0.02756
Epoch 1/80: current_loss=0.02883 | best_loss=0.02756
Epoch 2/80: current_loss=0.02772 | best_loss=0.02756
Epoch 3/80: current_loss=0.02755 | best_loss=0.02755
Epoch 4/80: current_loss=0.02754 | best_loss=0.02754
Epoch 5/80: current_loss=0.02737 | best_loss=0.02737
Epoch 6/80: current_loss=0.02744 | best_loss=0.02737
Epoch 7/80: current_loss=0.02854 | best_loss=0.02737
Epoch 8/80: current_loss=0.02762 | best_loss=0.02737
Epoch 9/80: current_loss=0.02735 | best_loss=0.02735
Epoch 10/80: current_loss=0.02754 | best_loss=0.02735
Epoch 11/80: current_loss=0.02736 | best_loss=0.02735
Epoch 12/80: current_loss=0.02801 | best_loss=0.02735
Epoch 13/80: current_loss=0.02848 | best_loss=0.02735
Epoch 14/80: current_loss=0.02849 | best_loss=0.02735
Epoch 15/80: current_loss=0.02801 | best_loss=0.02735
Epoch 16/80: current_loss=0.02756 | best_loss=0.02735
Epoch 17/80: current_loss=0.02828 | best_loss=0.02735
Epoch 18/80: current_loss=0.02794 | best_loss=0.02735
Epoch 19/80: current_loss=0.02756 | best_loss=0.02735
Epoch 20/80: current_loss=0.02764 | best_loss=0.02735
Epoch 21/80: current_loss=0.02816 | best_loss=0.02735
Epoch 22/80: current_loss=0.02779 | best_loss=0.02735
Epoch 23/80: current_loss=0.02765 | best_loss=0.02735
Epoch 24/80: current_loss=0.02767 | best_loss=0.02735
Epoch 25/80: current_loss=0.02837 | best_loss=0.02735
Epoch 26/80: current_loss=0.02778 | best_loss=0.02735
Epoch 27/80: current_loss=0.02757 | best_loss=0.02735
Epoch 28/80: current_loss=0.02745 | best_loss=0.02735
Epoch 29/80: current_loss=0.02777 | best_loss=0.02735
Early Stopping at epoch 29
      explained_var=-0.00995 | mse_loss=0.02783
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02690 | best_loss=0.02690
Epoch 1/80: current_loss=0.02684 | best_loss=0.02684
Epoch 2/80: current_loss=0.02760 | best_loss=0.02684
Epoch 3/80: current_loss=0.02701 | best_loss=0.02684
Epoch 4/80: current_loss=0.02686 | best_loss=0.02684
Epoch 5/80: current_loss=0.02781 | best_loss=0.02684
Epoch 6/80: current_loss=0.02684 | best_loss=0.02684
Epoch 7/80: current_loss=0.02679 | best_loss=0.02679
Epoch 8/80: current_loss=0.02679 | best_loss=0.02679
Epoch 9/80: current_loss=0.02680 | best_loss=0.02679
Epoch 10/80: current_loss=0.02674 | best_loss=0.02674
Epoch 11/80: current_loss=0.02680 | best_loss=0.02674
Epoch 12/80: current_loss=0.02671 | best_loss=0.02671
Epoch 13/80: current_loss=0.02671 | best_loss=0.02671
Epoch 14/80: current_loss=0.02738 | best_loss=0.02671
Epoch 15/80: current_loss=0.02690 | best_loss=0.02671
Epoch 16/80: current_loss=0.02692 | best_loss=0.02671
Epoch 17/80: current_loss=0.02680 | best_loss=0.02671
Epoch 18/80: current_loss=0.02673 | best_loss=0.02671
Epoch 19/80: current_loss=0.02672 | best_loss=0.02671
Epoch 20/80: current_loss=0.02689 | best_loss=0.02671
Epoch 21/80: current_loss=0.02687 | best_loss=0.02671
Epoch 22/80: current_loss=0.02674 | best_loss=0.02671
Epoch 23/80: current_loss=0.02680 | best_loss=0.02671
Epoch 24/80: current_loss=0.02674 | best_loss=0.02671
Epoch 25/80: current_loss=0.02674 | best_loss=0.02671
Epoch 26/80: current_loss=0.02676 | best_loss=0.02671
Epoch 27/80: current_loss=0.02681 | best_loss=0.02671
Epoch 28/80: current_loss=0.02717 | best_loss=0.02671
Epoch 29/80: current_loss=0.02716 | best_loss=0.02671
Epoch 30/80: current_loss=0.02695 | best_loss=0.02671
Epoch 31/80: current_loss=0.02676 | best_loss=0.02671
Epoch 32/80: current_loss=0.02674 | best_loss=0.02671
Early Stopping at epoch 32
      explained_var=0.00162 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02971 | best_loss=0.02971
Epoch 1/80: current_loss=0.02976 | best_loss=0.02971
Epoch 2/80: current_loss=0.02995 | best_loss=0.02971
Epoch 3/80: current_loss=0.02974 | best_loss=0.02971
Epoch 4/80: current_loss=0.02972 | best_loss=0.02971
Epoch 5/80: current_loss=0.02979 | best_loss=0.02971
Epoch 6/80: current_loss=0.03003 | best_loss=0.02971
Epoch 7/80: current_loss=0.02980 | best_loss=0.02971
Epoch 8/80: current_loss=0.03010 | best_loss=0.02971
Epoch 9/80: current_loss=0.03000 | best_loss=0.02971
Epoch 10/80: current_loss=0.02972 | best_loss=0.02971
Epoch 11/80: current_loss=0.02978 | best_loss=0.02971
Epoch 12/80: current_loss=0.02971 | best_loss=0.02971
Epoch 13/80: current_loss=0.02976 | best_loss=0.02971
Epoch 14/80: current_loss=0.02972 | best_loss=0.02971
Epoch 15/80: current_loss=0.02970 | best_loss=0.02970
Epoch 16/80: current_loss=0.02978 | best_loss=0.02970
Epoch 17/80: current_loss=0.02970 | best_loss=0.02970
Epoch 18/80: current_loss=0.02969 | best_loss=0.02969
Epoch 19/80: current_loss=0.02967 | best_loss=0.02967
Epoch 20/80: current_loss=0.02990 | best_loss=0.02967
Epoch 21/80: current_loss=0.02969 | best_loss=0.02967
Epoch 22/80: current_loss=0.02970 | best_loss=0.02967
Epoch 23/80: current_loss=0.02969 | best_loss=0.02967
Epoch 24/80: current_loss=0.02988 | best_loss=0.02967
Epoch 25/80: current_loss=0.02981 | best_loss=0.02967
Epoch 26/80: current_loss=0.02983 | best_loss=0.02967
Epoch 27/80: current_loss=0.02978 | best_loss=0.02967
Epoch 28/80: current_loss=0.02972 | best_loss=0.02967
Epoch 29/80: current_loss=0.02973 | best_loss=0.02967
Epoch 30/80: current_loss=0.02989 | best_loss=0.02967
Epoch 31/80: current_loss=0.02971 | best_loss=0.02967
Epoch 32/80: current_loss=0.02971 | best_loss=0.02967
Epoch 33/80: current_loss=0.02972 | best_loss=0.02967
Epoch 34/80: current_loss=0.03044 | best_loss=0.02967
Epoch 35/80: current_loss=0.02985 | best_loss=0.02967
Epoch 36/80: current_loss=0.02975 | best_loss=0.02967
Epoch 37/80: current_loss=0.02971 | best_loss=0.02967
Epoch 38/80: current_loss=0.02969 | best_loss=0.02967
Epoch 39/80: current_loss=0.02977 | best_loss=0.02967
Early Stopping at epoch 39
      explained_var=0.00177 | mse_loss=0.02871
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.00036| avg_loss=0.02693
----------------------------------------------


----------------------------------------------
Params for Trial 81
{'learning_rate': 0.0001, 'weight_decay': 0.0023088650761001477, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03153 | best_loss=0.03153
Epoch 1/80: current_loss=0.03022 | best_loss=0.03022
Epoch 2/80: current_loss=0.02958 | best_loss=0.02958
Epoch 3/80: current_loss=0.02931 | best_loss=0.02931
Epoch 4/80: current_loss=0.02927 | best_loss=0.02927
Epoch 5/80: current_loss=0.02888 | best_loss=0.02888
Epoch 6/80: current_loss=0.02905 | best_loss=0.02888
Epoch 7/80: current_loss=0.02881 | best_loss=0.02881
Epoch 8/80: current_loss=0.02880 | best_loss=0.02880
Epoch 9/80: current_loss=0.02875 | best_loss=0.02875
Epoch 10/80: current_loss=0.02875 | best_loss=0.02875
Epoch 11/80: current_loss=0.02875 | best_loss=0.02875
Epoch 12/80: current_loss=0.02875 | best_loss=0.02875
Epoch 13/80: current_loss=0.02888 | best_loss=0.02875
Epoch 14/80: current_loss=0.02892 | best_loss=0.02875
Epoch 15/80: current_loss=0.02885 | best_loss=0.02875
Epoch 16/80: current_loss=0.02912 | best_loss=0.02875
Epoch 17/80: current_loss=0.02917 | best_loss=0.02875
Epoch 18/80: current_loss=0.02898 | best_loss=0.02875
Epoch 19/80: current_loss=0.02884 | best_loss=0.02875
Epoch 20/80: current_loss=0.02905 | best_loss=0.02875
Epoch 21/80: current_loss=0.02886 | best_loss=0.02875
Epoch 22/80: current_loss=0.02888 | best_loss=0.02875
Epoch 23/80: current_loss=0.02943 | best_loss=0.02875
Epoch 24/80: current_loss=0.02889 | best_loss=0.02875
Epoch 25/80: current_loss=0.02890 | best_loss=0.02875
Epoch 26/80: current_loss=0.02891 | best_loss=0.02875
Epoch 27/80: current_loss=0.02889 | best_loss=0.02875
Epoch 28/80: current_loss=0.02892 | best_loss=0.02875
Epoch 29/80: current_loss=0.02888 | best_loss=0.02875
Epoch 30/80: current_loss=0.02888 | best_loss=0.02875
Epoch 31/80: current_loss=0.02889 | best_loss=0.02875
Early Stopping at epoch 31
      explained_var=0.00872 | mse_loss=0.02791
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02492 | best_loss=0.02492
Epoch 1/80: current_loss=0.02500 | best_loss=0.02492
Epoch 2/80: current_loss=0.02504 | best_loss=0.02492
Epoch 3/80: current_loss=0.02538 | best_loss=0.02492
Epoch 4/80: current_loss=0.02501 | best_loss=0.02492
Epoch 5/80: current_loss=0.02507 | best_loss=0.02492
Epoch 6/80: current_loss=0.02497 | best_loss=0.02492
Epoch 7/80: current_loss=0.02492 | best_loss=0.02492
Epoch 8/80: current_loss=0.02490 | best_loss=0.02490
Epoch 9/80: current_loss=0.02510 | best_loss=0.02490
Epoch 10/80: current_loss=0.02504 | best_loss=0.02490
Epoch 11/80: current_loss=0.02506 | best_loss=0.02490
Epoch 12/80: current_loss=0.02495 | best_loss=0.02490
Epoch 13/80: current_loss=0.02490 | best_loss=0.02490
Epoch 14/80: current_loss=0.02496 | best_loss=0.02490
Epoch 15/80: current_loss=0.02500 | best_loss=0.02490
Epoch 16/80: current_loss=0.02495 | best_loss=0.02490
Epoch 17/80: current_loss=0.02490 | best_loss=0.02490
Epoch 18/80: current_loss=0.02499 | best_loss=0.02490
Epoch 19/80: current_loss=0.02527 | best_loss=0.02490
Epoch 20/80: current_loss=0.02532 | best_loss=0.02490
Epoch 21/80: current_loss=0.02521 | best_loss=0.02490
Epoch 22/80: current_loss=0.02496 | best_loss=0.02490
Epoch 23/80: current_loss=0.02510 | best_loss=0.02490
Epoch 24/80: current_loss=0.02490 | best_loss=0.02490
Epoch 25/80: current_loss=0.02495 | best_loss=0.02490
Epoch 26/80: current_loss=0.02517 | best_loss=0.02490
Epoch 27/80: current_loss=0.02503 | best_loss=0.02490
Epoch 28/80: current_loss=0.02496 | best_loss=0.02490
Epoch 29/80: current_loss=0.02490 | best_loss=0.02490
Epoch 30/80: current_loss=0.02504 | best_loss=0.02490
Epoch 31/80: current_loss=0.02500 | best_loss=0.02490
Epoch 32/80: current_loss=0.02495 | best_loss=0.02490
Epoch 33/80: current_loss=0.02488 | best_loss=0.02488
Epoch 34/80: current_loss=0.02503 | best_loss=0.02488
Epoch 35/80: current_loss=0.02507 | best_loss=0.02488
Epoch 36/80: current_loss=0.02507 | best_loss=0.02488
Epoch 37/80: current_loss=0.02513 | best_loss=0.02488
Epoch 38/80: current_loss=0.02580 | best_loss=0.02488
Epoch 39/80: current_loss=0.02488 | best_loss=0.02488
Epoch 40/80: current_loss=0.02487 | best_loss=0.02487
Epoch 41/80: current_loss=0.02539 | best_loss=0.02487
Epoch 42/80: current_loss=0.02490 | best_loss=0.02487
Epoch 43/80: current_loss=0.02493 | best_loss=0.02487
Epoch 44/80: current_loss=0.02489 | best_loss=0.02487
Epoch 45/80: current_loss=0.02494 | best_loss=0.02487
Epoch 46/80: current_loss=0.02527 | best_loss=0.02487
Epoch 47/80: current_loss=0.02491 | best_loss=0.02487
Epoch 48/80: current_loss=0.02534 | best_loss=0.02487
Epoch 49/80: current_loss=0.02510 | best_loss=0.02487
Epoch 50/80: current_loss=0.02491 | best_loss=0.02487
Epoch 51/80: current_loss=0.02506 | best_loss=0.02487
Epoch 52/80: current_loss=0.02498 | best_loss=0.02487
Epoch 53/80: current_loss=0.02515 | best_loss=0.02487
Epoch 54/80: current_loss=0.02502 | best_loss=0.02487
Epoch 55/80: current_loss=0.02508 | best_loss=0.02487
Epoch 56/80: current_loss=0.02518 | best_loss=0.02487
Epoch 57/80: current_loss=0.02493 | best_loss=0.02487
Epoch 58/80: current_loss=0.02491 | best_loss=0.02487
Epoch 59/80: current_loss=0.02508 | best_loss=0.02487
Epoch 60/80: current_loss=0.02504 | best_loss=0.02487
Early Stopping at epoch 60
      explained_var=0.00224 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02754 | best_loss=0.02754
Epoch 1/80: current_loss=0.02772 | best_loss=0.02754
Epoch 2/80: current_loss=0.02762 | best_loss=0.02754
Epoch 3/80: current_loss=0.02883 | best_loss=0.02754
Epoch 4/80: current_loss=0.02857 | best_loss=0.02754
Epoch 5/80: current_loss=0.02790 | best_loss=0.02754
Epoch 6/80: current_loss=0.03008 | best_loss=0.02754
Epoch 7/80: current_loss=0.02897 | best_loss=0.02754
Epoch 8/80: current_loss=0.02751 | best_loss=0.02751
Epoch 9/80: current_loss=0.02827 | best_loss=0.02751
Epoch 10/80: current_loss=0.02775 | best_loss=0.02751
Epoch 11/80: current_loss=0.02780 | best_loss=0.02751
Epoch 12/80: current_loss=0.02811 | best_loss=0.02751
Epoch 13/80: current_loss=0.02784 | best_loss=0.02751
Epoch 14/80: current_loss=0.02809 | best_loss=0.02751
Epoch 15/80: current_loss=0.02760 | best_loss=0.02751
Epoch 16/80: current_loss=0.02760 | best_loss=0.02751
Epoch 17/80: current_loss=0.02809 | best_loss=0.02751
Epoch 18/80: current_loss=0.02820 | best_loss=0.02751
Epoch 19/80: current_loss=0.02758 | best_loss=0.02751
Epoch 20/80: current_loss=0.02820 | best_loss=0.02751
Epoch 21/80: current_loss=0.02768 | best_loss=0.02751
Epoch 22/80: current_loss=0.02778 | best_loss=0.02751
Epoch 23/80: current_loss=0.02783 | best_loss=0.02751
Epoch 24/80: current_loss=0.02854 | best_loss=0.02751
Epoch 25/80: current_loss=0.02797 | best_loss=0.02751
Epoch 26/80: current_loss=0.02782 | best_loss=0.02751
Epoch 27/80: current_loss=0.02785 | best_loss=0.02751
Epoch 28/80: current_loss=0.02803 | best_loss=0.02751
Early Stopping at epoch 28
      explained_var=-0.01353 | mse_loss=0.02802
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02666 | best_loss=0.02666
Epoch 1/80: current_loss=0.02684 | best_loss=0.02666
Epoch 2/80: current_loss=0.02695 | best_loss=0.02666
Epoch 3/80: current_loss=0.02683 | best_loss=0.02666
Epoch 4/80: current_loss=0.02671 | best_loss=0.02666
Epoch 5/80: current_loss=0.02673 | best_loss=0.02666
Epoch 6/80: current_loss=0.02674 | best_loss=0.02666
Epoch 7/80: current_loss=0.02683 | best_loss=0.02666
Epoch 8/80: current_loss=0.02673 | best_loss=0.02666
Epoch 9/80: current_loss=0.02680 | best_loss=0.02666
Epoch 10/80: current_loss=0.02730 | best_loss=0.02666
Epoch 11/80: current_loss=0.02736 | best_loss=0.02666
Epoch 12/80: current_loss=0.02670 | best_loss=0.02666
Epoch 13/80: current_loss=0.02679 | best_loss=0.02666
Epoch 14/80: current_loss=0.02677 | best_loss=0.02666
Epoch 15/80: current_loss=0.02671 | best_loss=0.02666
Epoch 16/80: current_loss=0.02670 | best_loss=0.02666
Epoch 17/80: current_loss=0.02674 | best_loss=0.02666
Epoch 18/80: current_loss=0.02672 | best_loss=0.02666
Epoch 19/80: current_loss=0.02688 | best_loss=0.02666
Epoch 20/80: current_loss=0.02694 | best_loss=0.02666
Early Stopping at epoch 20
      explained_var=0.00235 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02979 | best_loss=0.02979
Epoch 1/80: current_loss=0.02976 | best_loss=0.02976
Epoch 2/80: current_loss=0.02978 | best_loss=0.02976
Epoch 3/80: current_loss=0.02980 | best_loss=0.02976
Epoch 4/80: current_loss=0.02976 | best_loss=0.02976
Epoch 5/80: current_loss=0.02973 | best_loss=0.02973
Epoch 6/80: current_loss=0.02973 | best_loss=0.02973
Epoch 7/80: current_loss=0.02974 | best_loss=0.02973
Epoch 8/80: current_loss=0.02972 | best_loss=0.02972
Epoch 9/80: current_loss=0.02975 | best_loss=0.02972
Epoch 10/80: current_loss=0.02980 | best_loss=0.02972
Epoch 11/80: current_loss=0.02974 | best_loss=0.02972
Epoch 12/80: current_loss=0.02974 | best_loss=0.02972
Epoch 13/80: current_loss=0.02975 | best_loss=0.02972
Epoch 14/80: current_loss=0.03018 | best_loss=0.02972
Epoch 15/80: current_loss=0.02974 | best_loss=0.02972
Epoch 16/80: current_loss=0.03009 | best_loss=0.02972
Epoch 17/80: current_loss=0.02979 | best_loss=0.02972
Epoch 18/80: current_loss=0.03040 | best_loss=0.02972
Epoch 19/80: current_loss=0.02981 | best_loss=0.02972
Epoch 20/80: current_loss=0.03009 | best_loss=0.02972
Epoch 21/80: current_loss=0.02973 | best_loss=0.02972
Epoch 22/80: current_loss=0.02976 | best_loss=0.02972
Epoch 23/80: current_loss=0.03015 | best_loss=0.02972
Epoch 24/80: current_loss=0.02973 | best_loss=0.02972
Epoch 25/80: current_loss=0.02988 | best_loss=0.02972
Epoch 26/80: current_loss=0.02990 | best_loss=0.02972
Epoch 27/80: current_loss=0.02995 | best_loss=0.02972
Epoch 28/80: current_loss=0.02977 | best_loss=0.02972
Early Stopping at epoch 28
      explained_var=0.00103 | mse_loss=0.02874
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.00016| avg_loss=0.02695
----------------------------------------------


----------------------------------------------
Params for Trial 82
{'learning_rate': 0.0001, 'weight_decay': 9.833205478141933e-06, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03037 | best_loss=0.03037
Epoch 1/80: current_loss=0.02927 | best_loss=0.02927
Epoch 2/80: current_loss=0.02892 | best_loss=0.02892
Epoch 3/80: current_loss=0.02872 | best_loss=0.02872
Epoch 4/80: current_loss=0.02860 | best_loss=0.02860
Epoch 5/80: current_loss=0.02858 | best_loss=0.02858
Epoch 6/80: current_loss=0.02866 | best_loss=0.02858
Epoch 7/80: current_loss=0.02856 | best_loss=0.02856
Epoch 8/80: current_loss=0.02884 | best_loss=0.02856
Epoch 9/80: current_loss=0.02860 | best_loss=0.02856
Epoch 10/80: current_loss=0.02888 | best_loss=0.02856
Epoch 11/80: current_loss=0.02871 | best_loss=0.02856
Epoch 12/80: current_loss=0.02875 | best_loss=0.02856
Epoch 13/80: current_loss=0.02884 | best_loss=0.02856
Epoch 14/80: current_loss=0.02891 | best_loss=0.02856
Epoch 15/80: current_loss=0.02891 | best_loss=0.02856
Epoch 16/80: current_loss=0.02911 | best_loss=0.02856
Epoch 17/80: current_loss=0.02886 | best_loss=0.02856
Epoch 18/80: current_loss=0.02912 | best_loss=0.02856
Epoch 19/80: current_loss=0.02908 | best_loss=0.02856
Epoch 20/80: current_loss=0.02909 | best_loss=0.02856
Epoch 21/80: current_loss=0.02891 | best_loss=0.02856
Epoch 22/80: current_loss=0.02905 | best_loss=0.02856
Epoch 23/80: current_loss=0.02890 | best_loss=0.02856
Epoch 24/80: current_loss=0.02951 | best_loss=0.02856
Epoch 25/80: current_loss=0.02903 | best_loss=0.02856
Epoch 26/80: current_loss=0.02900 | best_loss=0.02856
Epoch 27/80: current_loss=0.02893 | best_loss=0.02856
Early Stopping at epoch 27
      explained_var=0.01253 | mse_loss=0.02779
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02486 | best_loss=0.02486
Epoch 1/80: current_loss=0.02497 | best_loss=0.02486
Epoch 2/80: current_loss=0.02488 | best_loss=0.02486
Epoch 3/80: current_loss=0.02489 | best_loss=0.02486
Epoch 4/80: current_loss=0.02510 | best_loss=0.02486
Epoch 5/80: current_loss=0.02524 | best_loss=0.02486
Epoch 6/80: current_loss=0.02494 | best_loss=0.02486
Epoch 7/80: current_loss=0.02496 | best_loss=0.02486
Epoch 8/80: current_loss=0.02514 | best_loss=0.02486
Epoch 9/80: current_loss=0.02543 | best_loss=0.02486
Epoch 10/80: current_loss=0.02495 | best_loss=0.02486
Epoch 11/80: current_loss=0.02491 | best_loss=0.02486
Epoch 12/80: current_loss=0.02489 | best_loss=0.02486
Epoch 13/80: current_loss=0.02485 | best_loss=0.02485
Epoch 14/80: current_loss=0.02497 | best_loss=0.02485
Epoch 15/80: current_loss=0.02499 | best_loss=0.02485
Epoch 16/80: current_loss=0.02502 | best_loss=0.02485
Epoch 17/80: current_loss=0.02514 | best_loss=0.02485
Epoch 18/80: current_loss=0.02553 | best_loss=0.02485
Epoch 19/80: current_loss=0.02547 | best_loss=0.02485
Epoch 20/80: current_loss=0.02547 | best_loss=0.02485
Epoch 21/80: current_loss=0.02499 | best_loss=0.02485
Epoch 22/80: current_loss=0.02510 | best_loss=0.02485
Epoch 23/80: current_loss=0.02506 | best_loss=0.02485
Epoch 24/80: current_loss=0.02531 | best_loss=0.02485
Epoch 25/80: current_loss=0.02510 | best_loss=0.02485
Epoch 26/80: current_loss=0.02503 | best_loss=0.02485
Epoch 27/80: current_loss=0.02506 | best_loss=0.02485
Epoch 28/80: current_loss=0.02502 | best_loss=0.02485
Epoch 29/80: current_loss=0.02501 | best_loss=0.02485
Epoch 30/80: current_loss=0.02504 | best_loss=0.02485
Epoch 31/80: current_loss=0.02510 | best_loss=0.02485
Epoch 32/80: current_loss=0.02506 | best_loss=0.02485
Epoch 33/80: current_loss=0.02503 | best_loss=0.02485
Early Stopping at epoch 33
      explained_var=0.00168 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02757 | best_loss=0.02757
Epoch 1/80: current_loss=0.02872 | best_loss=0.02757
Epoch 2/80: current_loss=0.02776 | best_loss=0.02757
Epoch 3/80: current_loss=0.02767 | best_loss=0.02757
Epoch 4/80: current_loss=0.02737 | best_loss=0.02737
Epoch 5/80: current_loss=0.02723 | best_loss=0.02723
Epoch 6/80: current_loss=0.02741 | best_loss=0.02723
Epoch 7/80: current_loss=0.02818 | best_loss=0.02723
Epoch 8/80: current_loss=0.02786 | best_loss=0.02723
Epoch 9/80: current_loss=0.02756 | best_loss=0.02723
Epoch 10/80: current_loss=0.02724 | best_loss=0.02723
Epoch 11/80: current_loss=0.02792 | best_loss=0.02723
Epoch 12/80: current_loss=0.02751 | best_loss=0.02723
Epoch 13/80: current_loss=0.02798 | best_loss=0.02723
Epoch 14/80: current_loss=0.02744 | best_loss=0.02723
Epoch 15/80: current_loss=0.02774 | best_loss=0.02723
Epoch 16/80: current_loss=0.02763 | best_loss=0.02723
Epoch 17/80: current_loss=0.02757 | best_loss=0.02723
Epoch 18/80: current_loss=0.02766 | best_loss=0.02723
Epoch 19/80: current_loss=0.02808 | best_loss=0.02723
Epoch 20/80: current_loss=0.02781 | best_loss=0.02723
Epoch 21/80: current_loss=0.02825 | best_loss=0.02723
Epoch 22/80: current_loss=0.02753 | best_loss=0.02723
Epoch 23/80: current_loss=0.02759 | best_loss=0.02723
Epoch 24/80: current_loss=0.02835 | best_loss=0.02723
Epoch 25/80: current_loss=0.02785 | best_loss=0.02723
Early Stopping at epoch 25
      explained_var=0.00137 | mse_loss=0.02760
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02687 | best_loss=0.02687
Epoch 1/80: current_loss=0.02691 | best_loss=0.02687
Epoch 2/80: current_loss=0.02680 | best_loss=0.02680
Epoch 3/80: current_loss=0.02696 | best_loss=0.02680
Epoch 4/80: current_loss=0.02696 | best_loss=0.02680
Epoch 5/80: current_loss=0.02679 | best_loss=0.02679
Epoch 6/80: current_loss=0.02679 | best_loss=0.02679
Epoch 7/80: current_loss=0.02693 | best_loss=0.02679
Epoch 8/80: current_loss=0.02687 | best_loss=0.02679
Epoch 9/80: current_loss=0.02703 | best_loss=0.02679
Epoch 10/80: current_loss=0.02677 | best_loss=0.02677
Epoch 11/80: current_loss=0.02673 | best_loss=0.02673
Epoch 12/80: current_loss=0.02677 | best_loss=0.02673
Epoch 13/80: current_loss=0.02708 | best_loss=0.02673
Epoch 14/80: current_loss=0.02683 | best_loss=0.02673
Epoch 15/80: current_loss=0.02679 | best_loss=0.02673
Epoch 16/80: current_loss=0.02679 | best_loss=0.02673
Epoch 17/80: current_loss=0.02681 | best_loss=0.02673
Epoch 18/80: current_loss=0.02733 | best_loss=0.02673
Epoch 19/80: current_loss=0.02726 | best_loss=0.02673
Epoch 20/80: current_loss=0.02708 | best_loss=0.02673
Epoch 21/80: current_loss=0.02709 | best_loss=0.02673
Epoch 22/80: current_loss=0.02683 | best_loss=0.02673
Epoch 23/80: current_loss=0.02683 | best_loss=0.02673
Epoch 24/80: current_loss=0.02678 | best_loss=0.02673
Epoch 25/80: current_loss=0.02686 | best_loss=0.02673
Epoch 26/80: current_loss=0.02688 | best_loss=0.02673
Epoch 27/80: current_loss=0.02686 | best_loss=0.02673
Epoch 28/80: current_loss=0.02688 | best_loss=0.02673
Epoch 29/80: current_loss=0.02679 | best_loss=0.02673
Epoch 30/80: current_loss=0.02717 | best_loss=0.02673
Epoch 31/80: current_loss=0.02704 | best_loss=0.02673
Early Stopping at epoch 31
      explained_var=0.00149 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02963 | best_loss=0.02963
Epoch 1/80: current_loss=0.02982 | best_loss=0.02963
Epoch 2/80: current_loss=0.02986 | best_loss=0.02963
Epoch 3/80: current_loss=0.02965 | best_loss=0.02963
Epoch 4/80: current_loss=0.02968 | best_loss=0.02963
Epoch 5/80: current_loss=0.02960 | best_loss=0.02960
Epoch 6/80: current_loss=0.02966 | best_loss=0.02960
Epoch 7/80: current_loss=0.02975 | best_loss=0.02960
Epoch 8/80: current_loss=0.02970 | best_loss=0.02960
Epoch 9/80: current_loss=0.02978 | best_loss=0.02960
Epoch 10/80: current_loss=0.02981 | best_loss=0.02960
Epoch 11/80: current_loss=0.02982 | best_loss=0.02960
Epoch 12/80: current_loss=0.02984 | best_loss=0.02960
Epoch 13/80: current_loss=0.02982 | best_loss=0.02960
Epoch 14/80: current_loss=0.02984 | best_loss=0.02960
Epoch 15/80: current_loss=0.02990 | best_loss=0.02960
Epoch 16/80: current_loss=0.02971 | best_loss=0.02960
Epoch 17/80: current_loss=0.02990 | best_loss=0.02960
Epoch 18/80: current_loss=0.02973 | best_loss=0.02960
Epoch 19/80: current_loss=0.02989 | best_loss=0.02960
Epoch 20/80: current_loss=0.02970 | best_loss=0.02960
Epoch 21/80: current_loss=0.02968 | best_loss=0.02960
Epoch 22/80: current_loss=0.02975 | best_loss=0.02960
Epoch 23/80: current_loss=0.02958 | best_loss=0.02958
Epoch 24/80: current_loss=0.02965 | best_loss=0.02958
Epoch 25/80: current_loss=0.02971 | best_loss=0.02958
Epoch 26/80: current_loss=0.02970 | best_loss=0.02958
Epoch 27/80: current_loss=0.02971 | best_loss=0.02958
Epoch 28/80: current_loss=0.02976 | best_loss=0.02958
Epoch 29/80: current_loss=0.02964 | best_loss=0.02958
Epoch 30/80: current_loss=0.02980 | best_loss=0.02958
Epoch 31/80: current_loss=0.02975 | best_loss=0.02958
Epoch 32/80: current_loss=0.02979 | best_loss=0.02958
Epoch 33/80: current_loss=0.02988 | best_loss=0.02958
Epoch 34/80: current_loss=0.02980 | best_loss=0.02958
Epoch 35/80: current_loss=0.02980 | best_loss=0.02958
Epoch 36/80: current_loss=0.02977 | best_loss=0.02958
Epoch 37/80: current_loss=0.02979 | best_loss=0.02958
Epoch 38/80: current_loss=0.02984 | best_loss=0.02958
Epoch 39/80: current_loss=0.02981 | best_loss=0.02958
Epoch 40/80: current_loss=0.02985 | best_loss=0.02958
Epoch 41/80: current_loss=0.02984 | best_loss=0.02958
Epoch 42/80: current_loss=0.02988 | best_loss=0.02958
Epoch 43/80: current_loss=0.02991 | best_loss=0.02958
Early Stopping at epoch 43
      explained_var=0.00541 | mse_loss=0.02860
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00450| avg_loss=0.02682
----------------------------------------------


----------------------------------------------
Params for Trial 83
{'learning_rate': 0.0001, 'weight_decay': 2.5782508116555658e-05, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03102 | best_loss=0.03102
Epoch 1/80: current_loss=0.02962 | best_loss=0.02962
Epoch 2/80: current_loss=0.02917 | best_loss=0.02917
Epoch 3/80: current_loss=0.02893 | best_loss=0.02893
Epoch 4/80: current_loss=0.02874 | best_loss=0.02874
Epoch 5/80: current_loss=0.02870 | best_loss=0.02870
Epoch 6/80: current_loss=0.02933 | best_loss=0.02870
Epoch 7/80: current_loss=0.02895 | best_loss=0.02870
Epoch 8/80: current_loss=0.02895 | best_loss=0.02870
Epoch 9/80: current_loss=0.02880 | best_loss=0.02870
Epoch 10/80: current_loss=0.02931 | best_loss=0.02870
Epoch 11/80: current_loss=0.02895 | best_loss=0.02870
Epoch 12/80: current_loss=0.02904 | best_loss=0.02870
Epoch 13/80: current_loss=0.02917 | best_loss=0.02870
Epoch 14/80: current_loss=0.02920 | best_loss=0.02870
Epoch 15/80: current_loss=0.02892 | best_loss=0.02870
Epoch 16/80: current_loss=0.02901 | best_loss=0.02870
Epoch 17/80: current_loss=0.02903 | best_loss=0.02870
Epoch 18/80: current_loss=0.02889 | best_loss=0.02870
Epoch 19/80: current_loss=0.02881 | best_loss=0.02870
Epoch 20/80: current_loss=0.02882 | best_loss=0.02870
Epoch 21/80: current_loss=0.02884 | best_loss=0.02870
Epoch 22/80: current_loss=0.02904 | best_loss=0.02870
Epoch 23/80: current_loss=0.02901 | best_loss=0.02870
Epoch 24/80: current_loss=0.02957 | best_loss=0.02870
Epoch 25/80: current_loss=0.02906 | best_loss=0.02870
Early Stopping at epoch 25
      explained_var=0.00853 | mse_loss=0.02791
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02493 | best_loss=0.02493
Epoch 1/80: current_loss=0.02498 | best_loss=0.02493
Epoch 2/80: current_loss=0.02498 | best_loss=0.02493
Epoch 3/80: current_loss=0.02516 | best_loss=0.02493
Epoch 4/80: current_loss=0.02554 | best_loss=0.02493
Epoch 5/80: current_loss=0.02505 | best_loss=0.02493
Epoch 6/80: current_loss=0.02527 | best_loss=0.02493
Epoch 7/80: current_loss=0.02514 | best_loss=0.02493
Epoch 8/80: current_loss=0.02512 | best_loss=0.02493
Epoch 9/80: current_loss=0.02535 | best_loss=0.02493
Epoch 10/80: current_loss=0.02500 | best_loss=0.02493
Epoch 11/80: current_loss=0.02499 | best_loss=0.02493
Epoch 12/80: current_loss=0.02495 | best_loss=0.02493
Epoch 13/80: current_loss=0.02495 | best_loss=0.02493
Epoch 14/80: current_loss=0.02503 | best_loss=0.02493
Epoch 15/80: current_loss=0.02513 | best_loss=0.02493
Epoch 16/80: current_loss=0.02490 | best_loss=0.02490
Epoch 17/80: current_loss=0.02492 | best_loss=0.02490
Epoch 18/80: current_loss=0.02497 | best_loss=0.02490
Epoch 19/80: current_loss=0.02509 | best_loss=0.02490
Epoch 20/80: current_loss=0.02498 | best_loss=0.02490
Epoch 21/80: current_loss=0.02519 | best_loss=0.02490
Epoch 22/80: current_loss=0.02554 | best_loss=0.02490
Epoch 23/80: current_loss=0.02498 | best_loss=0.02490
Epoch 24/80: current_loss=0.02494 | best_loss=0.02490
Epoch 25/80: current_loss=0.02493 | best_loss=0.02490
Epoch 26/80: current_loss=0.02498 | best_loss=0.02490
Epoch 27/80: current_loss=0.02507 | best_loss=0.02490
Epoch 28/80: current_loss=0.02502 | best_loss=0.02490
Epoch 29/80: current_loss=0.02520 | best_loss=0.02490
Epoch 30/80: current_loss=0.02507 | best_loss=0.02490
Epoch 31/80: current_loss=0.02503 | best_loss=0.02490
Epoch 32/80: current_loss=0.02492 | best_loss=0.02490
Epoch 33/80: current_loss=0.02498 | best_loss=0.02490
Epoch 34/80: current_loss=0.02498 | best_loss=0.02490
Epoch 35/80: current_loss=0.02505 | best_loss=0.02490
Epoch 36/80: current_loss=0.02512 | best_loss=0.02490
Early Stopping at epoch 36
      explained_var=0.00172 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02893 | best_loss=0.02893
Epoch 1/80: current_loss=0.02775 | best_loss=0.02775
Epoch 2/80: current_loss=0.02822 | best_loss=0.02775
Epoch 3/80: current_loss=0.02725 | best_loss=0.02725
Epoch 4/80: current_loss=0.02773 | best_loss=0.02725
Epoch 5/80: current_loss=0.02877 | best_loss=0.02725
Epoch 6/80: current_loss=0.02797 | best_loss=0.02725
Epoch 7/80: current_loss=0.02722 | best_loss=0.02722
Epoch 8/80: current_loss=0.02752 | best_loss=0.02722
Epoch 9/80: current_loss=0.02738 | best_loss=0.02722
Epoch 10/80: current_loss=0.02747 | best_loss=0.02722
Epoch 11/80: current_loss=0.02733 | best_loss=0.02722
Epoch 12/80: current_loss=0.02739 | best_loss=0.02722
Epoch 13/80: current_loss=0.02729 | best_loss=0.02722
Epoch 14/80: current_loss=0.02754 | best_loss=0.02722
Epoch 15/80: current_loss=0.02763 | best_loss=0.02722
Epoch 16/80: current_loss=0.02741 | best_loss=0.02722
Epoch 17/80: current_loss=0.02789 | best_loss=0.02722
Epoch 18/80: current_loss=0.02805 | best_loss=0.02722
Epoch 19/80: current_loss=0.02728 | best_loss=0.02722
Epoch 20/80: current_loss=0.02795 | best_loss=0.02722
Epoch 21/80: current_loss=0.02752 | best_loss=0.02722
Epoch 22/80: current_loss=0.02741 | best_loss=0.02722
Epoch 23/80: current_loss=0.02767 | best_loss=0.02722
Epoch 24/80: current_loss=0.02765 | best_loss=0.02722
Epoch 25/80: current_loss=0.02832 | best_loss=0.02722
Epoch 26/80: current_loss=0.02784 | best_loss=0.02722
Epoch 27/80: current_loss=0.02769 | best_loss=0.02722
Early Stopping at epoch 27
      explained_var=-0.00104 | mse_loss=0.02763
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02673 | best_loss=0.02673
Epoch 1/80: current_loss=0.02704 | best_loss=0.02673
Epoch 2/80: current_loss=0.02681 | best_loss=0.02673
Epoch 3/80: current_loss=0.02693 | best_loss=0.02673
Epoch 4/80: current_loss=0.02691 | best_loss=0.02673
Epoch 5/80: current_loss=0.02708 | best_loss=0.02673
Epoch 6/80: current_loss=0.02688 | best_loss=0.02673
Epoch 7/80: current_loss=0.02673 | best_loss=0.02673
Epoch 8/80: current_loss=0.02686 | best_loss=0.02673
Epoch 9/80: current_loss=0.02691 | best_loss=0.02673
Epoch 10/80: current_loss=0.02687 | best_loss=0.02673
Epoch 11/80: current_loss=0.02693 | best_loss=0.02673
Epoch 12/80: current_loss=0.02686 | best_loss=0.02673
Epoch 13/80: current_loss=0.02701 | best_loss=0.02673
Epoch 14/80: current_loss=0.02702 | best_loss=0.02673
Epoch 15/80: current_loss=0.02685 | best_loss=0.02673
Epoch 16/80: current_loss=0.02677 | best_loss=0.02673
Epoch 17/80: current_loss=0.02669 | best_loss=0.02669
Epoch 18/80: current_loss=0.02675 | best_loss=0.02669
Epoch 19/80: current_loss=0.02680 | best_loss=0.02669
Epoch 20/80: current_loss=0.02685 | best_loss=0.02669
Epoch 21/80: current_loss=0.02704 | best_loss=0.02669
Epoch 22/80: current_loss=0.02682 | best_loss=0.02669
Epoch 23/80: current_loss=0.02704 | best_loss=0.02669
Epoch 24/80: current_loss=0.02695 | best_loss=0.02669
Epoch 25/80: current_loss=0.02718 | best_loss=0.02669
Epoch 26/80: current_loss=0.02698 | best_loss=0.02669
Epoch 27/80: current_loss=0.02699 | best_loss=0.02669
Epoch 28/80: current_loss=0.02705 | best_loss=0.02669
Epoch 29/80: current_loss=0.02715 | best_loss=0.02669
Epoch 30/80: current_loss=0.02706 | best_loss=0.02669
Epoch 31/80: current_loss=0.02708 | best_loss=0.02669
Epoch 32/80: current_loss=0.02703 | best_loss=0.02669
Epoch 33/80: current_loss=0.02690 | best_loss=0.02669
Epoch 34/80: current_loss=0.02691 | best_loss=0.02669
Epoch 35/80: current_loss=0.02691 | best_loss=0.02669
Epoch 36/80: current_loss=0.02685 | best_loss=0.02669
Epoch 37/80: current_loss=0.02690 | best_loss=0.02669
Early Stopping at epoch 37
      explained_var=0.00399 | mse_loss=0.02490
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03022 | best_loss=0.03022
Epoch 1/80: current_loss=0.02983 | best_loss=0.02983
Epoch 2/80: current_loss=0.02997 | best_loss=0.02983
Epoch 3/80: current_loss=0.02984 | best_loss=0.02983
Epoch 4/80: current_loss=0.02992 | best_loss=0.02983
Epoch 5/80: current_loss=0.02989 | best_loss=0.02983
Epoch 6/80: current_loss=0.02967 | best_loss=0.02967
Epoch 7/80: current_loss=0.02967 | best_loss=0.02967
Epoch 8/80: current_loss=0.02982 | best_loss=0.02967
Epoch 9/80: current_loss=0.02977 | best_loss=0.02967
Epoch 10/80: current_loss=0.02968 | best_loss=0.02967
Epoch 11/80: current_loss=0.02956 | best_loss=0.02956
Epoch 12/80: current_loss=0.02963 | best_loss=0.02956
Epoch 13/80: current_loss=0.02956 | best_loss=0.02956
Epoch 14/80: current_loss=0.02958 | best_loss=0.02956
Epoch 15/80: current_loss=0.02966 | best_loss=0.02956
Epoch 16/80: current_loss=0.02973 | best_loss=0.02956
Epoch 17/80: current_loss=0.02982 | best_loss=0.02956
Epoch 18/80: current_loss=0.02993 | best_loss=0.02956
Epoch 19/80: current_loss=0.02978 | best_loss=0.02956
Epoch 20/80: current_loss=0.03005 | best_loss=0.02956
Epoch 21/80: current_loss=0.03019 | best_loss=0.02956
Epoch 22/80: current_loss=0.03018 | best_loss=0.02956
Epoch 23/80: current_loss=0.02989 | best_loss=0.02956
Epoch 24/80: current_loss=0.02987 | best_loss=0.02956
Epoch 25/80: current_loss=0.02980 | best_loss=0.02956
Epoch 26/80: current_loss=0.02979 | best_loss=0.02956
Epoch 27/80: current_loss=0.03009 | best_loss=0.02956
Epoch 28/80: current_loss=0.02983 | best_loss=0.02956
Epoch 29/80: current_loss=0.02971 | best_loss=0.02956
Epoch 30/80: current_loss=0.02961 | best_loss=0.02956
Epoch 31/80: current_loss=0.02978 | best_loss=0.02956
Early Stopping at epoch 31
      explained_var=0.00379 | mse_loss=0.02865
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00340| avg_loss=0.02685
----------------------------------------------


----------------------------------------------
Params for Trial 84
{'learning_rate': 0.0001, 'weight_decay': 1.3134958744341255e-05, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03009 | best_loss=0.03009
Epoch 1/80: current_loss=0.02921 | best_loss=0.02921
Epoch 2/80: current_loss=0.02891 | best_loss=0.02891
Epoch 3/80: current_loss=0.02858 | best_loss=0.02858
Epoch 4/80: current_loss=0.02884 | best_loss=0.02858
Epoch 5/80: current_loss=0.02862 | best_loss=0.02858
Epoch 6/80: current_loss=0.02863 | best_loss=0.02858
Epoch 7/80: current_loss=0.02879 | best_loss=0.02858
Epoch 8/80: current_loss=0.02876 | best_loss=0.02858
Epoch 9/80: current_loss=0.02871 | best_loss=0.02858
Epoch 10/80: current_loss=0.02895 | best_loss=0.02858
Epoch 11/80: current_loss=0.02881 | best_loss=0.02858
Epoch 12/80: current_loss=0.02910 | best_loss=0.02858
Epoch 13/80: current_loss=0.02926 | best_loss=0.02858
Epoch 14/80: current_loss=0.02897 | best_loss=0.02858
Epoch 15/80: current_loss=0.02919 | best_loss=0.02858
Epoch 16/80: current_loss=0.02892 | best_loss=0.02858
Epoch 17/80: current_loss=0.02908 | best_loss=0.02858
Epoch 18/80: current_loss=0.02898 | best_loss=0.02858
Epoch 19/80: current_loss=0.02899 | best_loss=0.02858
Epoch 20/80: current_loss=0.02897 | best_loss=0.02858
Epoch 21/80: current_loss=0.02915 | best_loss=0.02858
Epoch 22/80: current_loss=0.02916 | best_loss=0.02858
Epoch 23/80: current_loss=0.02897 | best_loss=0.02858
Early Stopping at epoch 23
      explained_var=0.01361 | mse_loss=0.02776
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02531 | best_loss=0.02531
Epoch 1/80: current_loss=0.02497 | best_loss=0.02497
Epoch 2/80: current_loss=0.02497 | best_loss=0.02497
Epoch 3/80: current_loss=0.02511 | best_loss=0.02497
Epoch 4/80: current_loss=0.02551 | best_loss=0.02497
Epoch 5/80: current_loss=0.02509 | best_loss=0.02497
Epoch 6/80: current_loss=0.02534 | best_loss=0.02497
Epoch 7/80: current_loss=0.02502 | best_loss=0.02497
Epoch 8/80: current_loss=0.02551 | best_loss=0.02497
Epoch 9/80: current_loss=0.02517 | best_loss=0.02497
Epoch 10/80: current_loss=0.02496 | best_loss=0.02496
Epoch 11/80: current_loss=0.02496 | best_loss=0.02496
Epoch 12/80: current_loss=0.02512 | best_loss=0.02496
Epoch 13/80: current_loss=0.02504 | best_loss=0.02496
Epoch 14/80: current_loss=0.02494 | best_loss=0.02494
Epoch 15/80: current_loss=0.02494 | best_loss=0.02494
Epoch 16/80: current_loss=0.02539 | best_loss=0.02494
Epoch 17/80: current_loss=0.02513 | best_loss=0.02494
Epoch 18/80: current_loss=0.02529 | best_loss=0.02494
Epoch 19/80: current_loss=0.02532 | best_loss=0.02494
Epoch 20/80: current_loss=0.02509 | best_loss=0.02494
Epoch 21/80: current_loss=0.02510 | best_loss=0.02494
Epoch 22/80: current_loss=0.02501 | best_loss=0.02494
Epoch 23/80: current_loss=0.02500 | best_loss=0.02494
Epoch 24/80: current_loss=0.02518 | best_loss=0.02494
Epoch 25/80: current_loss=0.02528 | best_loss=0.02494
Epoch 26/80: current_loss=0.02496 | best_loss=0.02494
Epoch 27/80: current_loss=0.02503 | best_loss=0.02494
Epoch 28/80: current_loss=0.02516 | best_loss=0.02494
Epoch 29/80: current_loss=0.02505 | best_loss=0.02494
Epoch 30/80: current_loss=0.02528 | best_loss=0.02494
Epoch 31/80: current_loss=0.02519 | best_loss=0.02494
Epoch 32/80: current_loss=0.02517 | best_loss=0.02494
Epoch 33/80: current_loss=0.02514 | best_loss=0.02494
Epoch 34/80: current_loss=0.02498 | best_loss=0.02494
Epoch 35/80: current_loss=0.02501 | best_loss=0.02494
Early Stopping at epoch 35
      explained_var=0.00209 | mse_loss=0.02518

----------------------------------------------
Params for Trial 85
{'learning_rate': 0.0001, 'weight_decay': 0.0005081789207489343, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03351 | best_loss=0.03351
Epoch 1/80: current_loss=0.03167 | best_loss=0.03167
Epoch 2/80: current_loss=0.03065 | best_loss=0.03065
Epoch 3/80: current_loss=0.03014 | best_loss=0.03014
Epoch 4/80: current_loss=0.03004 | best_loss=0.03004
Epoch 5/80: current_loss=0.02968 | best_loss=0.02968
Epoch 6/80: current_loss=0.02935 | best_loss=0.02935
Epoch 7/80: current_loss=0.02945 | best_loss=0.02935
Epoch 8/80: current_loss=0.02926 | best_loss=0.02926
Epoch 9/80: current_loss=0.02922 | best_loss=0.02922
Epoch 10/80: current_loss=0.02913 | best_loss=0.02913
Epoch 11/80: current_loss=0.02912 | best_loss=0.02912
Epoch 12/80: current_loss=0.02914 | best_loss=0.02912
Epoch 13/80: current_loss=0.02916 | best_loss=0.02912
Epoch 14/80: current_loss=0.02911 | best_loss=0.02911
Epoch 15/80: current_loss=0.02911 | best_loss=0.02911
Epoch 16/80: current_loss=0.02941 | best_loss=0.02911
Epoch 17/80: current_loss=0.02935 | best_loss=0.02911
Epoch 18/80: current_loss=0.02927 | best_loss=0.02911
Epoch 19/80: current_loss=0.02910 | best_loss=0.02910
Epoch 20/80: current_loss=0.02915 | best_loss=0.02910
Epoch 21/80: current_loss=0.02907 | best_loss=0.02907
Epoch 22/80: current_loss=0.02901 | best_loss=0.02901
Epoch 23/80: current_loss=0.02917 | best_loss=0.02901
Epoch 24/80: current_loss=0.02915 | best_loss=0.02901
Epoch 25/80: current_loss=0.02894 | best_loss=0.02894
Epoch 26/80: current_loss=0.02891 | best_loss=0.02891
Epoch 27/80: current_loss=0.02921 | best_loss=0.02891
Epoch 28/80: current_loss=0.02896 | best_loss=0.02891
Epoch 29/80: current_loss=0.02896 | best_loss=0.02891
Epoch 30/80: current_loss=0.02900 | best_loss=0.02891
Epoch 31/80: current_loss=0.02931 | best_loss=0.02891
Epoch 32/80: current_loss=0.02900 | best_loss=0.02891
Epoch 33/80: current_loss=0.02949 | best_loss=0.02891
Epoch 34/80: current_loss=0.02906 | best_loss=0.02891
Epoch 35/80: current_loss=0.02904 | best_loss=0.02891
Epoch 36/80: current_loss=0.02908 | best_loss=0.02891
Epoch 37/80: current_loss=0.02898 | best_loss=0.02891
Epoch 38/80: current_loss=0.02898 | best_loss=0.02891
Epoch 39/80: current_loss=0.02903 | best_loss=0.02891
Epoch 40/80: current_loss=0.02910 | best_loss=0.02891
Epoch 41/80: current_loss=0.02901 | best_loss=0.02891
Epoch 42/80: current_loss=0.02901 | best_loss=0.02891
Epoch 43/80: current_loss=0.02942 | best_loss=0.02891
Epoch 44/80: current_loss=0.02932 | best_loss=0.02891
Epoch 45/80: current_loss=0.02905 | best_loss=0.02891
Epoch 46/80: current_loss=0.02948 | best_loss=0.02891
Early Stopping at epoch 46
      explained_var=0.00486 | mse_loss=0.02801
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02698 | best_loss=0.02698
Epoch 1/80: current_loss=0.02511 | best_loss=0.02511
Epoch 2/80: current_loss=0.02498 | best_loss=0.02498
Epoch 3/80: current_loss=0.02539 | best_loss=0.02498
Epoch 4/80: current_loss=0.02499 | best_loss=0.02498
Epoch 5/80: current_loss=0.02513 | best_loss=0.02498
Epoch 6/80: current_loss=0.02554 | best_loss=0.02498
Epoch 7/80: current_loss=0.02524 | best_loss=0.02498
Epoch 8/80: current_loss=0.02521 | best_loss=0.02498
Epoch 9/80: current_loss=0.02499 | best_loss=0.02498
Epoch 10/80: current_loss=0.02551 | best_loss=0.02498
Epoch 11/80: current_loss=0.02506 | best_loss=0.02498
Epoch 12/80: current_loss=0.02527 | best_loss=0.02498
Epoch 13/80: current_loss=0.02500 | best_loss=0.02498
Epoch 14/80: current_loss=0.02512 | best_loss=0.02498
Epoch 15/80: current_loss=0.02495 | best_loss=0.02495
Epoch 16/80: current_loss=0.02496 | best_loss=0.02495
Epoch 17/80: current_loss=0.02496 | best_loss=0.02495
Epoch 18/80: current_loss=0.02511 | best_loss=0.02495
Epoch 19/80: current_loss=0.02497 | best_loss=0.02495
Epoch 20/80: current_loss=0.02502 | best_loss=0.02495
Epoch 21/80: current_loss=0.02516 | best_loss=0.02495
Epoch 22/80: current_loss=0.02501 | best_loss=0.02495
Epoch 23/80: current_loss=0.02500 | best_loss=0.02495
Epoch 24/80: current_loss=0.02506 | best_loss=0.02495
Epoch 25/80: current_loss=0.02527 | best_loss=0.02495
Epoch 26/80: current_loss=0.02512 | best_loss=0.02495
Epoch 27/80: current_loss=0.02497 | best_loss=0.02495
Epoch 28/80: current_loss=0.02507 | best_loss=0.02495
Epoch 29/80: current_loss=0.02539 | best_loss=0.02495
Epoch 30/80: current_loss=0.02503 | best_loss=0.02495
Epoch 31/80: current_loss=0.02499 | best_loss=0.02495
Epoch 32/80: current_loss=0.02502 | best_loss=0.02495
Epoch 33/80: current_loss=0.02497 | best_loss=0.02495
Epoch 34/80: current_loss=0.02499 | best_loss=0.02495
Epoch 35/80: current_loss=0.02496 | best_loss=0.02495
Early Stopping at epoch 35
      explained_var=0.00154 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02749 | best_loss=0.02749
Epoch 1/80: current_loss=0.02785 | best_loss=0.02749
Epoch 2/80: current_loss=0.02761 | best_loss=0.02749
Epoch 3/80: current_loss=0.02747 | best_loss=0.02747
Epoch 4/80: current_loss=0.02796 | best_loss=0.02747
Epoch 5/80: current_loss=0.02845 | best_loss=0.02747
Epoch 6/80: current_loss=0.02812 | best_loss=0.02747
Epoch 7/80: current_loss=0.02809 | best_loss=0.02747
Epoch 8/80: current_loss=0.02830 | best_loss=0.02747
Epoch 9/80: current_loss=0.02740 | best_loss=0.02740
Epoch 10/80: current_loss=0.02758 | best_loss=0.02740
Epoch 11/80: current_loss=0.02748 | best_loss=0.02740
Epoch 12/80: current_loss=0.02737 | best_loss=0.02737
Epoch 13/80: current_loss=0.02797 | best_loss=0.02737
Epoch 14/80: current_loss=0.02738 | best_loss=0.02737
Epoch 15/80: current_loss=0.02788 | best_loss=0.02737
Epoch 16/80: current_loss=0.02806 | best_loss=0.02737
Epoch 17/80: current_loss=0.02757 | best_loss=0.02737
Epoch 18/80: current_loss=0.02779 | best_loss=0.02737
Epoch 19/80: current_loss=0.02796 | best_loss=0.02737
Epoch 20/80: current_loss=0.02748 | best_loss=0.02737
Epoch 21/80: current_loss=0.02740 | best_loss=0.02737
Epoch 22/80: current_loss=0.02794 | best_loss=0.02737
Epoch 23/80: current_loss=0.02777 | best_loss=0.02737
Epoch 24/80: current_loss=0.02822 | best_loss=0.02737
Epoch 25/80: current_loss=0.02852 | best_loss=0.02737
Epoch 26/80: current_loss=0.02860 | best_loss=0.02737
Epoch 27/80: current_loss=0.02817 | best_loss=0.02737
Epoch 28/80: current_loss=0.02740 | best_loss=0.02737
Epoch 29/80: current_loss=0.02764 | best_loss=0.02737
Epoch 30/80: current_loss=0.02801 | best_loss=0.02737
Epoch 31/80: current_loss=0.02764 | best_loss=0.02737
Epoch 32/80: current_loss=0.02769 | best_loss=0.02737
Early Stopping at epoch 32
      explained_var=-0.01000 | mse_loss=0.02785
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02683 | best_loss=0.02683
Epoch 1/80: current_loss=0.02673 | best_loss=0.02673
Epoch 2/80: current_loss=0.02673 | best_loss=0.02673
Epoch 3/80: current_loss=0.02679 | best_loss=0.02673
Epoch 4/80: current_loss=0.02677 | best_loss=0.02673
Epoch 5/80: current_loss=0.02677 | best_loss=0.02673
Epoch 6/80: current_loss=0.02676 | best_loss=0.02673
Epoch 7/80: current_loss=0.02685 | best_loss=0.02673
Epoch 8/80: current_loss=0.02700 | best_loss=0.02673
Epoch 9/80: current_loss=0.02687 | best_loss=0.02673
Epoch 10/80: current_loss=0.02678 | best_loss=0.02673
Epoch 11/80: current_loss=0.02716 | best_loss=0.02673
Epoch 12/80: current_loss=0.02692 | best_loss=0.02673
Epoch 13/80: current_loss=0.02683 | best_loss=0.02673
Epoch 14/80: current_loss=0.02677 | best_loss=0.02673
Epoch 15/80: current_loss=0.02677 | best_loss=0.02673
Epoch 16/80: current_loss=0.02679 | best_loss=0.02673
Epoch 17/80: current_loss=0.02694 | best_loss=0.02673
Epoch 18/80: current_loss=0.02678 | best_loss=0.02673
Epoch 19/80: current_loss=0.02679 | best_loss=0.02673
Epoch 20/80: current_loss=0.02690 | best_loss=0.02673
Epoch 21/80: current_loss=0.02681 | best_loss=0.02673
Early Stopping at epoch 21
      explained_var=0.00158 | mse_loss=0.02496

----------------------------------------------
Params for Trial 86
{'learning_rate': 0.0001, 'weight_decay': 0.0002163136231179549, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02931 | best_loss=0.02931
Epoch 1/80: current_loss=0.02913 | best_loss=0.02913
Epoch 2/80: current_loss=0.02997 | best_loss=0.02913
Epoch 3/80: current_loss=0.02926 | best_loss=0.02913
Epoch 4/80: current_loss=0.02944 | best_loss=0.02913
Epoch 5/80: current_loss=0.02918 | best_loss=0.02913
Epoch 6/80: current_loss=0.02908 | best_loss=0.02908
Epoch 7/80: current_loss=0.02911 | best_loss=0.02908
Epoch 8/80: current_loss=0.02962 | best_loss=0.02908
Epoch 9/80: current_loss=0.02888 | best_loss=0.02888
Epoch 10/80: current_loss=0.02910 | best_loss=0.02888
Epoch 11/80: current_loss=0.02951 | best_loss=0.02888
Epoch 12/80: current_loss=0.02933 | best_loss=0.02888
Epoch 13/80: current_loss=0.02953 | best_loss=0.02888
Epoch 14/80: current_loss=0.02900 | best_loss=0.02888
Epoch 15/80: current_loss=0.02905 | best_loss=0.02888
Epoch 16/80: current_loss=0.02920 | best_loss=0.02888
Epoch 17/80: current_loss=0.02929 | best_loss=0.02888
Epoch 18/80: current_loss=0.02911 | best_loss=0.02888
Epoch 19/80: current_loss=0.02920 | best_loss=0.02888
Epoch 20/80: current_loss=0.02908 | best_loss=0.02888
Epoch 21/80: current_loss=0.02933 | best_loss=0.02888
Epoch 22/80: current_loss=0.02923 | best_loss=0.02888
Epoch 23/80: current_loss=0.02913 | best_loss=0.02888
Epoch 24/80: current_loss=0.02917 | best_loss=0.02888
Epoch 25/80: current_loss=0.02904 | best_loss=0.02888
Epoch 26/80: current_loss=0.02891 | best_loss=0.02888
Epoch 27/80: current_loss=0.02885 | best_loss=0.02885
Epoch 28/80: current_loss=0.02899 | best_loss=0.02885
Epoch 29/80: current_loss=0.02926 | best_loss=0.02885
Epoch 30/80: current_loss=0.02899 | best_loss=0.02885
Epoch 31/80: current_loss=0.02911 | best_loss=0.02885
Epoch 32/80: current_loss=0.02944 | best_loss=0.02885
Epoch 33/80: current_loss=0.02920 | best_loss=0.02885
Epoch 34/80: current_loss=0.02897 | best_loss=0.02885
Epoch 35/80: current_loss=0.02901 | best_loss=0.02885
Epoch 36/80: current_loss=0.02927 | best_loss=0.02885
Epoch 37/80: current_loss=0.02922 | best_loss=0.02885
Epoch 38/80: current_loss=0.02912 | best_loss=0.02885
Epoch 39/80: current_loss=0.02898 | best_loss=0.02885
Epoch 40/80: current_loss=0.02925 | best_loss=0.02885
Epoch 41/80: current_loss=0.02900 | best_loss=0.02885
Epoch 42/80: current_loss=0.02894 | best_loss=0.02885
Epoch 43/80: current_loss=0.02896 | best_loss=0.02885
Epoch 44/80: current_loss=0.02889 | best_loss=0.02885
Epoch 45/80: current_loss=0.02902 | best_loss=0.02885
Epoch 46/80: current_loss=0.02932 | best_loss=0.02885
Epoch 47/80: current_loss=0.02986 | best_loss=0.02885
Early Stopping at epoch 47
      explained_var=0.00955 | mse_loss=0.02810

----------------------------------------------
Params for Trial 87
{'learning_rate': 0.1, 'weight_decay': 0.0006638624512491894, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.09374 | best_loss=0.09374
Epoch 1/80: current_loss=0.04305 | best_loss=0.04305
Epoch 2/80: current_loss=0.03608 | best_loss=0.03608
Epoch 3/80: current_loss=0.04073 | best_loss=0.03608
Epoch 4/80: current_loss=0.03810 | best_loss=0.03608
Epoch 5/80: current_loss=0.03477 | best_loss=0.03477
Epoch 6/80: current_loss=0.04135 | best_loss=0.03477
Epoch 7/80: current_loss=0.05663 | best_loss=0.03477
Epoch 8/80: current_loss=0.03487 | best_loss=0.03477
Epoch 9/80: current_loss=0.05485 | best_loss=0.03477
Epoch 10/80: current_loss=0.73411 | best_loss=0.03477
Epoch 11/80: current_loss=0.08177 | best_loss=0.03477
Epoch 12/80: current_loss=0.11544 | best_loss=0.03477
Epoch 13/80: current_loss=0.08789 | best_loss=0.03477
Epoch 14/80: current_loss=0.04539 | best_loss=0.03477
Epoch 15/80: current_loss=0.04592 | best_loss=0.03477
Epoch 16/80: current_loss=0.04200 | best_loss=0.03477
Epoch 17/80: current_loss=0.05728 | best_loss=0.03477
Epoch 18/80: current_loss=0.05198 | best_loss=0.03477
Epoch 19/80: current_loss=0.03994 | best_loss=0.03477
Epoch 20/80: current_loss=0.03973 | best_loss=0.03477
Epoch 21/80: current_loss=0.08211 | best_loss=0.03477
Epoch 22/80: current_loss=0.09092 | best_loss=0.03477
Epoch 23/80: current_loss=0.07569 | best_loss=0.03477
Epoch 24/80: current_loss=0.06045 | best_loss=0.03477
Epoch 25/80: current_loss=0.10719 | best_loss=0.03477
Early Stopping at epoch 25
      explained_var=-0.20166 | mse_loss=0.03384

----------------------------------------------
Params for Trial 88
{'learning_rate': 0.0001, 'weight_decay': 0.0010212773153648403, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03483 | best_loss=0.03483
Epoch 1/80: current_loss=0.03164 | best_loss=0.03164
Epoch 2/80: current_loss=0.03032 | best_loss=0.03032
Epoch 3/80: current_loss=0.02978 | best_loss=0.02978
Epoch 4/80: current_loss=0.02962 | best_loss=0.02962
Epoch 5/80: current_loss=0.02899 | best_loss=0.02899
Epoch 6/80: current_loss=0.02897 | best_loss=0.02897
Epoch 7/80: current_loss=0.02892 | best_loss=0.02892
Epoch 8/80: current_loss=0.02955 | best_loss=0.02892
Epoch 9/80: current_loss=0.02894 | best_loss=0.02892
Epoch 10/80: current_loss=0.02887 | best_loss=0.02887
Epoch 11/80: current_loss=0.02882 | best_loss=0.02882
Epoch 12/80: current_loss=0.02885 | best_loss=0.02882
Epoch 13/80: current_loss=0.02898 | best_loss=0.02882
Epoch 14/80: current_loss=0.02920 | best_loss=0.02882
Epoch 15/80: current_loss=0.02911 | best_loss=0.02882
Epoch 16/80: current_loss=0.02987 | best_loss=0.02882
Epoch 17/80: current_loss=0.02920 | best_loss=0.02882
Epoch 18/80: current_loss=0.02900 | best_loss=0.02882
Epoch 19/80: current_loss=0.02898 | best_loss=0.02882
Epoch 20/80: current_loss=0.02941 | best_loss=0.02882
Epoch 21/80: current_loss=0.02906 | best_loss=0.02882
Epoch 22/80: current_loss=0.02905 | best_loss=0.02882
Epoch 23/80: current_loss=0.02906 | best_loss=0.02882
Epoch 24/80: current_loss=0.02892 | best_loss=0.02882
Epoch 25/80: current_loss=0.02939 | best_loss=0.02882
Epoch 26/80: current_loss=0.02943 | best_loss=0.02882
Epoch 27/80: current_loss=0.02898 | best_loss=0.02882
Epoch 28/80: current_loss=0.02898 | best_loss=0.02882
Epoch 29/80: current_loss=0.02903 | best_loss=0.02882
Epoch 30/80: current_loss=0.02916 | best_loss=0.02882
Epoch 31/80: current_loss=0.02894 | best_loss=0.02882
Early Stopping at epoch 31
      explained_var=0.00631 | mse_loss=0.02797
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02502 | best_loss=0.02502
Epoch 1/80: current_loss=0.02489 | best_loss=0.02489
Epoch 2/80: current_loss=0.02515 | best_loss=0.02489
Epoch 3/80: current_loss=0.02495 | best_loss=0.02489
Epoch 4/80: current_loss=0.02487 | best_loss=0.02487
Epoch 5/80: current_loss=0.02512 | best_loss=0.02487
Epoch 6/80: current_loss=0.02546 | best_loss=0.02487
Epoch 7/80: current_loss=0.02500 | best_loss=0.02487
Epoch 8/80: current_loss=0.02537 | best_loss=0.02487
Epoch 9/80: current_loss=0.02492 | best_loss=0.02487
Epoch 10/80: current_loss=0.02495 | best_loss=0.02487
Epoch 11/80: current_loss=0.02490 | best_loss=0.02487
Epoch 12/80: current_loss=0.02514 | best_loss=0.02487
Epoch 13/80: current_loss=0.02497 | best_loss=0.02487
Epoch 14/80: current_loss=0.02508 | best_loss=0.02487
Epoch 15/80: current_loss=0.02498 | best_loss=0.02487
Epoch 16/80: current_loss=0.02521 | best_loss=0.02487
Epoch 17/80: current_loss=0.02501 | best_loss=0.02487
Epoch 18/80: current_loss=0.02486 | best_loss=0.02486
Epoch 19/80: current_loss=0.02515 | best_loss=0.02486
Epoch 20/80: current_loss=0.02490 | best_loss=0.02486
Epoch 21/80: current_loss=0.02511 | best_loss=0.02486
Epoch 22/80: current_loss=0.02486 | best_loss=0.02486
Epoch 23/80: current_loss=0.02504 | best_loss=0.02486
Epoch 24/80: current_loss=0.02494 | best_loss=0.02486
Epoch 25/80: current_loss=0.02494 | best_loss=0.02486
Epoch 26/80: current_loss=0.02487 | best_loss=0.02486
Epoch 27/80: current_loss=0.02501 | best_loss=0.02486
Epoch 28/80: current_loss=0.02488 | best_loss=0.02486
Epoch 29/80: current_loss=0.02490 | best_loss=0.02486
Epoch 30/80: current_loss=0.02521 | best_loss=0.02486
Epoch 31/80: current_loss=0.02507 | best_loss=0.02486
Epoch 32/80: current_loss=0.02522 | best_loss=0.02486
Epoch 33/80: current_loss=0.02490 | best_loss=0.02486
Epoch 34/80: current_loss=0.02524 | best_loss=0.02486
Epoch 35/80: current_loss=0.02504 | best_loss=0.02486
Epoch 36/80: current_loss=0.02489 | best_loss=0.02486
Epoch 37/80: current_loss=0.02501 | best_loss=0.02486
Epoch 38/80: current_loss=0.02487 | best_loss=0.02486
Early Stopping at epoch 38
      explained_var=0.00274 | mse_loss=0.02513
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02768 | best_loss=0.02768
Epoch 1/80: current_loss=0.02882 | best_loss=0.02768
Epoch 2/80: current_loss=0.02793 | best_loss=0.02768
Epoch 3/80: current_loss=0.02750 | best_loss=0.02750
Epoch 4/80: current_loss=0.02744 | best_loss=0.02744
Epoch 5/80: current_loss=0.02763 | best_loss=0.02744
Epoch 6/80: current_loss=0.02744 | best_loss=0.02744
Epoch 7/80: current_loss=0.02808 | best_loss=0.02744
Epoch 8/80: current_loss=0.02774 | best_loss=0.02744
Epoch 9/80: current_loss=0.02859 | best_loss=0.02744
Epoch 10/80: current_loss=0.02755 | best_loss=0.02744
Epoch 11/80: current_loss=0.02793 | best_loss=0.02744
Epoch 12/80: current_loss=0.02779 | best_loss=0.02744
Epoch 13/80: current_loss=0.02744 | best_loss=0.02744
Epoch 14/80: current_loss=0.02801 | best_loss=0.02744
Epoch 15/80: current_loss=0.02828 | best_loss=0.02744
Epoch 16/80: current_loss=0.02807 | best_loss=0.02744
Epoch 17/80: current_loss=0.02739 | best_loss=0.02739
Epoch 18/80: current_loss=0.02787 | best_loss=0.02739
Epoch 19/80: current_loss=0.02768 | best_loss=0.02739
Epoch 20/80: current_loss=0.02765 | best_loss=0.02739
Epoch 21/80: current_loss=0.02792 | best_loss=0.02739
Epoch 22/80: current_loss=0.02752 | best_loss=0.02739
Epoch 23/80: current_loss=0.02778 | best_loss=0.02739
Epoch 24/80: current_loss=0.02767 | best_loss=0.02739
Epoch 25/80: current_loss=0.02746 | best_loss=0.02739
Epoch 26/80: current_loss=0.02748 | best_loss=0.02739
Epoch 27/80: current_loss=0.02800 | best_loss=0.02739
Epoch 28/80: current_loss=0.02776 | best_loss=0.02739
Epoch 29/80: current_loss=0.02779 | best_loss=0.02739
Epoch 30/80: current_loss=0.02775 | best_loss=0.02739
Epoch 31/80: current_loss=0.02834 | best_loss=0.02739
Epoch 32/80: current_loss=0.02758 | best_loss=0.02739
Epoch 33/80: current_loss=0.02798 | best_loss=0.02739
Epoch 34/80: current_loss=0.02773 | best_loss=0.02739
Epoch 35/80: current_loss=0.02786 | best_loss=0.02739
Epoch 36/80: current_loss=0.02748 | best_loss=0.02739
Epoch 37/80: current_loss=0.02771 | best_loss=0.02739
Early Stopping at epoch 37
      explained_var=-0.01060 | mse_loss=0.02788
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02687 | best_loss=0.02687
Epoch 1/80: current_loss=0.02669 | best_loss=0.02669
Epoch 2/80: current_loss=0.02672 | best_loss=0.02669
Epoch 3/80: current_loss=0.02671 | best_loss=0.02669
Epoch 4/80: current_loss=0.02680 | best_loss=0.02669
Epoch 5/80: current_loss=0.02685 | best_loss=0.02669
Epoch 6/80: current_loss=0.02671 | best_loss=0.02669
Epoch 7/80: current_loss=0.02674 | best_loss=0.02669
Epoch 8/80: current_loss=0.02682 | best_loss=0.02669
Epoch 9/80: current_loss=0.02675 | best_loss=0.02669
Epoch 10/80: current_loss=0.02678 | best_loss=0.02669
Epoch 11/80: current_loss=0.02672 | best_loss=0.02669
Epoch 12/80: current_loss=0.02678 | best_loss=0.02669
Epoch 13/80: current_loss=0.02700 | best_loss=0.02669
Epoch 14/80: current_loss=0.02711 | best_loss=0.02669
Epoch 15/80: current_loss=0.02690 | best_loss=0.02669
Epoch 16/80: current_loss=0.02679 | best_loss=0.02669
Epoch 17/80: current_loss=0.02681 | best_loss=0.02669
Epoch 18/80: current_loss=0.02672 | best_loss=0.02669
Epoch 19/80: current_loss=0.02689 | best_loss=0.02669
Epoch 20/80: current_loss=0.02672 | best_loss=0.02669
Epoch 21/80: current_loss=0.02743 | best_loss=0.02669
Early Stopping at epoch 21
      explained_var=0.00191 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02969 | best_loss=0.02969
Epoch 1/80: current_loss=0.02971 | best_loss=0.02969
Epoch 2/80: current_loss=0.03010 | best_loss=0.02969
Epoch 3/80: current_loss=0.02971 | best_loss=0.02969
Epoch 4/80: current_loss=0.02976 | best_loss=0.02969
Epoch 5/80: current_loss=0.02974 | best_loss=0.02969
Epoch 6/80: current_loss=0.02970 | best_loss=0.02969
Epoch 7/80: current_loss=0.02970 | best_loss=0.02969
Epoch 8/80: current_loss=0.02965 | best_loss=0.02965
Epoch 9/80: current_loss=0.02965 | best_loss=0.02965
Epoch 10/80: current_loss=0.02969 | best_loss=0.02965
Epoch 11/80: current_loss=0.02966 | best_loss=0.02965
Epoch 12/80: current_loss=0.02965 | best_loss=0.02965
Epoch 13/80: current_loss=0.02965 | best_loss=0.02965
Epoch 14/80: current_loss=0.02966 | best_loss=0.02965
Epoch 15/80: current_loss=0.02967 | best_loss=0.02965
Epoch 16/80: current_loss=0.02974 | best_loss=0.02965
Epoch 17/80: current_loss=0.02989 | best_loss=0.02965
Epoch 18/80: current_loss=0.02968 | best_loss=0.02965
Epoch 19/80: current_loss=0.02968 | best_loss=0.02965
Epoch 20/80: current_loss=0.02981 | best_loss=0.02965
Epoch 21/80: current_loss=0.02970 | best_loss=0.02965
Epoch 22/80: current_loss=0.02994 | best_loss=0.02965
Epoch 23/80: current_loss=0.02970 | best_loss=0.02965
Epoch 24/80: current_loss=0.02983 | best_loss=0.02965
Epoch 25/80: current_loss=0.02970 | best_loss=0.02965
Epoch 26/80: current_loss=0.02987 | best_loss=0.02965
Epoch 27/80: current_loss=0.02992 | best_loss=0.02965
Epoch 28/80: current_loss=0.02968 | best_loss=0.02965
Epoch 29/80: current_loss=0.02972 | best_loss=0.02965
Epoch 30/80: current_loss=0.02970 | best_loss=0.02965
Epoch 31/80: current_loss=0.02975 | best_loss=0.02965
Epoch 32/80: current_loss=0.02974 | best_loss=0.02965
Early Stopping at epoch 32
      explained_var=0.00258 | mse_loss=0.02868
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00059| avg_loss=0.02692
----------------------------------------------


----------------------------------------------
Params for Trial 89
{'learning_rate': 0.01, 'weight_decay': 0.0002014798279336856, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03367 | best_loss=0.03367
Epoch 1/80: current_loss=0.03523 | best_loss=0.03367
Epoch 2/80: current_loss=0.03164 | best_loss=0.03164
Epoch 3/80: current_loss=0.03356 | best_loss=0.03164
Epoch 4/80: current_loss=0.04914 | best_loss=0.03164
Epoch 5/80: current_loss=0.04513 | best_loss=0.03164
Epoch 6/80: current_loss=0.03372 | best_loss=0.03164
Epoch 7/80: current_loss=0.03000 | best_loss=0.03000
Epoch 8/80: current_loss=0.03222 | best_loss=0.03000
Epoch 9/80: current_loss=0.03414 | best_loss=0.03000
Epoch 10/80: current_loss=0.04020 | best_loss=0.03000
Epoch 11/80: current_loss=0.03100 | best_loss=0.03000
Epoch 12/80: current_loss=0.03609 | best_loss=0.03000
Epoch 13/80: current_loss=0.04877 | best_loss=0.03000
Epoch 14/80: current_loss=0.03497 | best_loss=0.03000
Epoch 15/80: current_loss=0.04524 | best_loss=0.03000
Epoch 16/80: current_loss=0.04249 | best_loss=0.03000
Epoch 17/80: current_loss=0.03008 | best_loss=0.03000
Epoch 18/80: current_loss=0.03252 | best_loss=0.03000
Epoch 19/80: current_loss=0.02871 | best_loss=0.02871
Epoch 20/80: current_loss=0.04717 | best_loss=0.02871
Epoch 21/80: current_loss=0.03332 | best_loss=0.02871
Epoch 22/80: current_loss=0.03096 | best_loss=0.02871
Epoch 23/80: current_loss=0.03820 | best_loss=0.02871
Epoch 24/80: current_loss=0.02992 | best_loss=0.02871
Epoch 25/80: current_loss=0.03048 | best_loss=0.02871
Epoch 26/80: current_loss=0.02881 | best_loss=0.02871
Epoch 27/80: current_loss=0.03056 | best_loss=0.02871
Epoch 28/80: current_loss=0.02942 | best_loss=0.02871
Epoch 29/80: current_loss=0.02916 | best_loss=0.02871
Epoch 30/80: current_loss=0.02904 | best_loss=0.02871
Epoch 31/80: current_loss=0.02969 | best_loss=0.02871
Epoch 32/80: current_loss=0.02918 | best_loss=0.02871
Epoch 33/80: current_loss=0.02918 | best_loss=0.02871
Epoch 34/80: current_loss=0.02976 | best_loss=0.02871
Epoch 35/80: current_loss=0.02916 | best_loss=0.02871
Epoch 36/80: current_loss=0.02928 | best_loss=0.02871
Epoch 37/80: current_loss=0.02914 | best_loss=0.02871
Epoch 38/80: current_loss=0.02929 | best_loss=0.02871
Epoch 39/80: current_loss=0.02945 | best_loss=0.02871
Early Stopping at epoch 39
      explained_var=0.00633 | mse_loss=0.02810

----------------------------------------------
Params for Trial 90
{'learning_rate': 0.0001, 'weight_decay': 0.008671099663771623, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03060 | best_loss=0.03060
Epoch 1/80: current_loss=0.03038 | best_loss=0.03038
Epoch 2/80: current_loss=0.03008 | best_loss=0.03008
Epoch 3/80: current_loss=0.02997 | best_loss=0.02997
Epoch 4/80: current_loss=0.02913 | best_loss=0.02913
Epoch 5/80: current_loss=0.02919 | best_loss=0.02913
Epoch 6/80: current_loss=0.02935 | best_loss=0.02913
Epoch 7/80: current_loss=0.02932 | best_loss=0.02913
Epoch 8/80: current_loss=0.02923 | best_loss=0.02913
Epoch 9/80: current_loss=0.02991 | best_loss=0.02913
Epoch 10/80: current_loss=0.02944 | best_loss=0.02913
Epoch 11/80: current_loss=0.02894 | best_loss=0.02894
Epoch 12/80: current_loss=0.02890 | best_loss=0.02890
Epoch 13/80: current_loss=0.02899 | best_loss=0.02890
Epoch 14/80: current_loss=0.02890 | best_loss=0.02890
Epoch 15/80: current_loss=0.02915 | best_loss=0.02890
Epoch 16/80: current_loss=0.02902 | best_loss=0.02890
Epoch 17/80: current_loss=0.02895 | best_loss=0.02890
Epoch 18/80: current_loss=0.02896 | best_loss=0.02890
Epoch 19/80: current_loss=0.02900 | best_loss=0.02890
Epoch 20/80: current_loss=0.02922 | best_loss=0.02890
Epoch 21/80: current_loss=0.02978 | best_loss=0.02890
Epoch 22/80: current_loss=0.02903 | best_loss=0.02890
Epoch 23/80: current_loss=0.02893 | best_loss=0.02890
Epoch 24/80: current_loss=0.02917 | best_loss=0.02890
Epoch 25/80: current_loss=0.02960 | best_loss=0.02890
Epoch 26/80: current_loss=0.02893 | best_loss=0.02890
Epoch 27/80: current_loss=0.02900 | best_loss=0.02890
Epoch 28/80: current_loss=0.02900 | best_loss=0.02890
Epoch 29/80: current_loss=0.02892 | best_loss=0.02890
Epoch 30/80: current_loss=0.02912 | best_loss=0.02890
Epoch 31/80: current_loss=0.02912 | best_loss=0.02890
Epoch 32/80: current_loss=0.02894 | best_loss=0.02890
Early Stopping at epoch 32
      explained_var=0.00445 | mse_loss=0.02802
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02489 | best_loss=0.02489
Epoch 1/80: current_loss=0.02491 | best_loss=0.02489
Epoch 2/80: current_loss=0.02496 | best_loss=0.02489
Epoch 3/80: current_loss=0.02568 | best_loss=0.02489
Epoch 4/80: current_loss=0.02517 | best_loss=0.02489
Epoch 5/80: current_loss=0.02511 | best_loss=0.02489
Epoch 6/80: current_loss=0.02494 | best_loss=0.02489
Epoch 7/80: current_loss=0.02490 | best_loss=0.02489
Epoch 8/80: current_loss=0.02491 | best_loss=0.02489
Epoch 9/80: current_loss=0.02537 | best_loss=0.02489
Epoch 10/80: current_loss=0.02504 | best_loss=0.02489
Epoch 11/80: current_loss=0.02526 | best_loss=0.02489
Epoch 12/80: current_loss=0.02494 | best_loss=0.02489
Epoch 13/80: current_loss=0.02516 | best_loss=0.02489
Epoch 14/80: current_loss=0.02524 | best_loss=0.02489
Epoch 15/80: current_loss=0.02519 | best_loss=0.02489
Epoch 16/80: current_loss=0.02491 | best_loss=0.02489
Epoch 17/80: current_loss=0.02547 | best_loss=0.02489
Epoch 18/80: current_loss=0.02592 | best_loss=0.02489
Epoch 19/80: current_loss=0.02524 | best_loss=0.02489
Epoch 20/80: current_loss=0.02496 | best_loss=0.02489
Early Stopping at epoch 20
      explained_var=0.00226 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02770 | best_loss=0.02770
Epoch 1/80: current_loss=0.02848 | best_loss=0.02770
Epoch 2/80: current_loss=0.02780 | best_loss=0.02770
Epoch 3/80: current_loss=0.02890 | best_loss=0.02770
Epoch 4/80: current_loss=0.02773 | best_loss=0.02770
Epoch 5/80: current_loss=0.02822 | best_loss=0.02770
Epoch 6/80: current_loss=0.02824 | best_loss=0.02770
Epoch 7/80: current_loss=0.02813 | best_loss=0.02770
Epoch 8/80: current_loss=0.02812 | best_loss=0.02770
Epoch 9/80: current_loss=0.02865 | best_loss=0.02770
Epoch 10/80: current_loss=0.02827 | best_loss=0.02770
Epoch 11/80: current_loss=0.02794 | best_loss=0.02770
Epoch 12/80: current_loss=0.02793 | best_loss=0.02770
Epoch 13/80: current_loss=0.02807 | best_loss=0.02770
Epoch 14/80: current_loss=0.02787 | best_loss=0.02770
Epoch 15/80: current_loss=0.02952 | best_loss=0.02770
Epoch 16/80: current_loss=0.02907 | best_loss=0.02770
Epoch 17/80: current_loss=0.02812 | best_loss=0.02770
Epoch 18/80: current_loss=0.02871 | best_loss=0.02770
Epoch 19/80: current_loss=0.02814 | best_loss=0.02770
Epoch 20/80: current_loss=0.02799 | best_loss=0.02770
Early Stopping at epoch 20
      explained_var=-0.02671 | mse_loss=0.02829
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02664 | best_loss=0.02664
Epoch 1/80: current_loss=0.02677 | best_loss=0.02664
Epoch 2/80: current_loss=0.02666 | best_loss=0.02664
Epoch 3/80: current_loss=0.02670 | best_loss=0.02664
Epoch 4/80: current_loss=0.02689 | best_loss=0.02664
Epoch 5/80: current_loss=0.02675 | best_loss=0.02664
Epoch 6/80: current_loss=0.02693 | best_loss=0.02664
Epoch 7/80: current_loss=0.02665 | best_loss=0.02664
Epoch 8/80: current_loss=0.02669 | best_loss=0.02664
Epoch 9/80: current_loss=0.02679 | best_loss=0.02664
Epoch 10/80: current_loss=0.02718 | best_loss=0.02664
Epoch 11/80: current_loss=0.02663 | best_loss=0.02663
Epoch 12/80: current_loss=0.02664 | best_loss=0.02663
Epoch 13/80: current_loss=0.02668 | best_loss=0.02663
Epoch 14/80: current_loss=0.02695 | best_loss=0.02663
Epoch 15/80: current_loss=0.02667 | best_loss=0.02663
Epoch 16/80: current_loss=0.02695 | best_loss=0.02663
Epoch 17/80: current_loss=0.02665 | best_loss=0.02663
Epoch 18/80: current_loss=0.02669 | best_loss=0.02663
Epoch 19/80: current_loss=0.02835 | best_loss=0.02663
Epoch 20/80: current_loss=0.02666 | best_loss=0.02663
Epoch 21/80: current_loss=0.02666 | best_loss=0.02663
Epoch 22/80: current_loss=0.02666 | best_loss=0.02663
Epoch 23/80: current_loss=0.02665 | best_loss=0.02663
Epoch 24/80: current_loss=0.02723 | best_loss=0.02663
Epoch 25/80: current_loss=0.02678 | best_loss=0.02663
Epoch 26/80: current_loss=0.02677 | best_loss=0.02663
Epoch 27/80: current_loss=0.02671 | best_loss=0.02663
Epoch 28/80: current_loss=0.02678 | best_loss=0.02663
Epoch 29/80: current_loss=0.02668 | best_loss=0.02663
Epoch 30/80: current_loss=0.02676 | best_loss=0.02663
Epoch 31/80: current_loss=0.02674 | best_loss=0.02663
Early Stopping at epoch 31
      explained_var=0.00124 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02988 | best_loss=0.02988
Epoch 1/80: current_loss=0.02990 | best_loss=0.02988
Epoch 2/80: current_loss=0.03035 | best_loss=0.02988
Epoch 3/80: current_loss=0.02985 | best_loss=0.02985
Epoch 4/80: current_loss=0.03057 | best_loss=0.02985
Epoch 5/80: current_loss=0.02979 | best_loss=0.02979
Epoch 6/80: current_loss=0.03000 | best_loss=0.02979
Epoch 7/80: current_loss=0.02981 | best_loss=0.02979
Epoch 8/80: current_loss=0.02980 | best_loss=0.02979
Epoch 9/80: current_loss=0.02989 | best_loss=0.02979
Epoch 10/80: current_loss=0.02978 | best_loss=0.02978
Epoch 11/80: current_loss=0.02980 | best_loss=0.02978
Epoch 12/80: current_loss=0.02997 | best_loss=0.02978
Epoch 13/80: current_loss=0.02986 | best_loss=0.02978
Epoch 14/80: current_loss=0.03011 | best_loss=0.02978
Epoch 15/80: current_loss=0.02985 | best_loss=0.02978
Epoch 16/80: current_loss=0.03052 | best_loss=0.02978
Epoch 17/80: current_loss=0.02992 | best_loss=0.02978
Epoch 18/80: current_loss=0.02978 | best_loss=0.02978
Epoch 19/80: current_loss=0.02979 | best_loss=0.02978
Epoch 20/80: current_loss=0.02978 | best_loss=0.02978
Epoch 21/80: current_loss=0.02995 | best_loss=0.02978
Epoch 22/80: current_loss=0.02985 | best_loss=0.02978
Epoch 23/80: current_loss=0.02989 | best_loss=0.02978
Epoch 24/80: current_loss=0.02978 | best_loss=0.02978
Epoch 25/80: current_loss=0.02983 | best_loss=0.02978
Epoch 26/80: current_loss=0.03029 | best_loss=0.02978
Epoch 27/80: current_loss=0.02982 | best_loss=0.02978
Epoch 28/80: current_loss=0.02989 | best_loss=0.02978
Epoch 29/80: current_loss=0.02983 | best_loss=0.02978
Epoch 30/80: current_loss=0.02987 | best_loss=0.02978
Epoch 31/80: current_loss=0.02992 | best_loss=0.02978
Epoch 32/80: current_loss=0.02981 | best_loss=0.02978
Epoch 33/80: current_loss=0.02987 | best_loss=0.02978
Epoch 34/80: current_loss=0.03019 | best_loss=0.02978
Epoch 35/80: current_loss=0.02999 | best_loss=0.02978
Epoch 36/80: current_loss=0.02991 | best_loss=0.02978
Epoch 37/80: current_loss=0.02978 | best_loss=0.02978
Epoch 38/80: current_loss=0.02981 | best_loss=0.02978
Epoch 39/80: current_loss=0.03055 | best_loss=0.02978
Epoch 40/80: current_loss=0.02981 | best_loss=0.02978
Early Stopping at epoch 40
      explained_var=-0.00038 | mse_loss=0.02877
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=-0.00383| avg_loss=0.02704
----------------------------------------------


----------------------------------------------
Params for Trial 91
{'learning_rate': 0.0001, 'weight_decay': 0.00045567551435233146, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03128 | best_loss=0.03128
Epoch 1/80: current_loss=0.03089 | best_loss=0.03089
Epoch 2/80: current_loss=0.02975 | best_loss=0.02975
Epoch 3/80: current_loss=0.02920 | best_loss=0.02920
Epoch 4/80: current_loss=0.02932 | best_loss=0.02920
Epoch 5/80: current_loss=0.02881 | best_loss=0.02881
Epoch 6/80: current_loss=0.02930 | best_loss=0.02881
Epoch 7/80: current_loss=0.02883 | best_loss=0.02881
Epoch 8/80: current_loss=0.02879 | best_loss=0.02879
Epoch 9/80: current_loss=0.02878 | best_loss=0.02878
Epoch 10/80: current_loss=0.02896 | best_loss=0.02878
Epoch 11/80: current_loss=0.02879 | best_loss=0.02878
Epoch 12/80: current_loss=0.02880 | best_loss=0.02878
Epoch 13/80: current_loss=0.02882 | best_loss=0.02878
Epoch 14/80: current_loss=0.02886 | best_loss=0.02878
Epoch 15/80: current_loss=0.02889 | best_loss=0.02878
Epoch 16/80: current_loss=0.02883 | best_loss=0.02878
Epoch 17/80: current_loss=0.02935 | best_loss=0.02878
Epoch 18/80: current_loss=0.03045 | best_loss=0.02878
Epoch 19/80: current_loss=0.02907 | best_loss=0.02878
Epoch 20/80: current_loss=0.02918 | best_loss=0.02878
Epoch 21/80: current_loss=0.02901 | best_loss=0.02878
Epoch 22/80: current_loss=0.02904 | best_loss=0.02878
Epoch 23/80: current_loss=0.02898 | best_loss=0.02878
Epoch 24/80: current_loss=0.02902 | best_loss=0.02878
Epoch 25/80: current_loss=0.02908 | best_loss=0.02878
Epoch 26/80: current_loss=0.02905 | best_loss=0.02878
Epoch 27/80: current_loss=0.02915 | best_loss=0.02878
Epoch 28/80: current_loss=0.02915 | best_loss=0.02878
Epoch 29/80: current_loss=0.02899 | best_loss=0.02878
Early Stopping at epoch 29
      explained_var=0.00917 | mse_loss=0.02793
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02493 | best_loss=0.02493
Epoch 1/80: current_loss=0.02490 | best_loss=0.02490
Epoch 2/80: current_loss=0.02511 | best_loss=0.02490
Epoch 3/80: current_loss=0.02495 | best_loss=0.02490
Epoch 4/80: current_loss=0.02488 | best_loss=0.02488
Epoch 5/80: current_loss=0.02486 | best_loss=0.02486
Epoch 6/80: current_loss=0.02506 | best_loss=0.02486
Epoch 7/80: current_loss=0.02508 | best_loss=0.02486
Epoch 8/80: current_loss=0.02486 | best_loss=0.02486
Epoch 9/80: current_loss=0.02499 | best_loss=0.02486
Epoch 10/80: current_loss=0.02530 | best_loss=0.02486
Epoch 11/80: current_loss=0.02502 | best_loss=0.02486
Epoch 12/80: current_loss=0.02494 | best_loss=0.02486
Epoch 13/80: current_loss=0.02496 | best_loss=0.02486
Epoch 14/80: current_loss=0.02493 | best_loss=0.02486
Epoch 15/80: current_loss=0.02529 | best_loss=0.02486
Epoch 16/80: current_loss=0.02615 | best_loss=0.02486
Epoch 17/80: current_loss=0.02508 | best_loss=0.02486
Epoch 18/80: current_loss=0.02536 | best_loss=0.02486
Epoch 19/80: current_loss=0.02497 | best_loss=0.02486
Epoch 20/80: current_loss=0.02499 | best_loss=0.02486
Epoch 21/80: current_loss=0.02519 | best_loss=0.02486
Epoch 22/80: current_loss=0.02498 | best_loss=0.02486
Epoch 23/80: current_loss=0.02491 | best_loss=0.02486
Epoch 24/80: current_loss=0.02566 | best_loss=0.02486
Epoch 25/80: current_loss=0.02497 | best_loss=0.02486
Early Stopping at epoch 25
      explained_var=0.00205 | mse_loss=0.02517
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02822 | best_loss=0.02822
Epoch 1/80: current_loss=0.02797 | best_loss=0.02797
Epoch 2/80: current_loss=0.02777 | best_loss=0.02777
Epoch 3/80: current_loss=0.02820 | best_loss=0.02777
Epoch 4/80: current_loss=0.02750 | best_loss=0.02750
Epoch 5/80: current_loss=0.02753 | best_loss=0.02750
Epoch 6/80: current_loss=0.02994 | best_loss=0.02750
Epoch 7/80: current_loss=0.02869 | best_loss=0.02750
Epoch 8/80: current_loss=0.02835 | best_loss=0.02750
Epoch 9/80: current_loss=0.02731 | best_loss=0.02731
Epoch 10/80: current_loss=0.02727 | best_loss=0.02727
Epoch 11/80: current_loss=0.02841 | best_loss=0.02727
Epoch 12/80: current_loss=0.02784 | best_loss=0.02727
Epoch 13/80: current_loss=0.02744 | best_loss=0.02727
Epoch 14/80: current_loss=0.02753 | best_loss=0.02727
Epoch 15/80: current_loss=0.02824 | best_loss=0.02727
Epoch 16/80: current_loss=0.02749 | best_loss=0.02727
Epoch 17/80: current_loss=0.02770 | best_loss=0.02727
Epoch 18/80: current_loss=0.02821 | best_loss=0.02727
Epoch 19/80: current_loss=0.02803 | best_loss=0.02727
Epoch 20/80: current_loss=0.02789 | best_loss=0.02727
Epoch 21/80: current_loss=0.02777 | best_loss=0.02727
Epoch 22/80: current_loss=0.02766 | best_loss=0.02727
Epoch 23/80: current_loss=0.02733 | best_loss=0.02727
Epoch 24/80: current_loss=0.02819 | best_loss=0.02727
Epoch 25/80: current_loss=0.02779 | best_loss=0.02727
Epoch 26/80: current_loss=0.02794 | best_loss=0.02727
Epoch 27/80: current_loss=0.02727 | best_loss=0.02727
Epoch 28/80: current_loss=0.02837 | best_loss=0.02727
Epoch 29/80: current_loss=0.02723 | best_loss=0.02723
Epoch 30/80: current_loss=0.02826 | best_loss=0.02723
Epoch 31/80: current_loss=0.02726 | best_loss=0.02723
Epoch 32/80: current_loss=0.02739 | best_loss=0.02723
Epoch 33/80: current_loss=0.02788 | best_loss=0.02723
Epoch 34/80: current_loss=0.02794 | best_loss=0.02723
Epoch 35/80: current_loss=0.02794 | best_loss=0.02723
Epoch 36/80: current_loss=0.02766 | best_loss=0.02723
Epoch 37/80: current_loss=0.02744 | best_loss=0.02723
Epoch 38/80: current_loss=0.02851 | best_loss=0.02723
Epoch 39/80: current_loss=0.02777 | best_loss=0.02723
Epoch 40/80: current_loss=0.02750 | best_loss=0.02723
Epoch 41/80: current_loss=0.02793 | best_loss=0.02723
Epoch 42/80: current_loss=0.02810 | best_loss=0.02723
Epoch 43/80: current_loss=0.02794 | best_loss=0.02723
Epoch 44/80: current_loss=0.02817 | best_loss=0.02723
Epoch 45/80: current_loss=0.02741 | best_loss=0.02723
Epoch 46/80: current_loss=0.02722 | best_loss=0.02722
Epoch 47/80: current_loss=0.02726 | best_loss=0.02722
Epoch 48/80: current_loss=0.02739 | best_loss=0.02722
Epoch 49/80: current_loss=0.02778 | best_loss=0.02722
Epoch 50/80: current_loss=0.02799 | best_loss=0.02722
Epoch 51/80: current_loss=0.02794 | best_loss=0.02722
Epoch 52/80: current_loss=0.02760 | best_loss=0.02722
Epoch 53/80: current_loss=0.02762 | best_loss=0.02722
Epoch 54/80: current_loss=0.02767 | best_loss=0.02722
Epoch 55/80: current_loss=0.02754 | best_loss=0.02722
Epoch 56/80: current_loss=0.02763 | best_loss=0.02722
Epoch 57/80: current_loss=0.02765 | best_loss=0.02722
Epoch 58/80: current_loss=0.02803 | best_loss=0.02722
Epoch 59/80: current_loss=0.02780 | best_loss=0.02722
Epoch 60/80: current_loss=0.02809 | best_loss=0.02722
Epoch 61/80: current_loss=0.02760 | best_loss=0.02722
Epoch 62/80: current_loss=0.02755 | best_loss=0.02722
Epoch 63/80: current_loss=0.02759 | best_loss=0.02722
Epoch 64/80: current_loss=0.02737 | best_loss=0.02722
Epoch 65/80: current_loss=0.02755 | best_loss=0.02722
Epoch 66/80: current_loss=0.02787 | best_loss=0.02722
Early Stopping at epoch 66
      explained_var=-0.00232 | mse_loss=0.02763
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02700 | best_loss=0.02700
Epoch 1/80: current_loss=0.02685 | best_loss=0.02685
Epoch 2/80: current_loss=0.02687 | best_loss=0.02685
Epoch 3/80: current_loss=0.02691 | best_loss=0.02685
Epoch 4/80: current_loss=0.02684 | best_loss=0.02684
Epoch 5/80: current_loss=0.02700 | best_loss=0.02684
Epoch 6/80: current_loss=0.02678 | best_loss=0.02678
Epoch 7/80: current_loss=0.02677 | best_loss=0.02677
Epoch 8/80: current_loss=0.02701 | best_loss=0.02677
Epoch 9/80: current_loss=0.02677 | best_loss=0.02677
Epoch 10/80: current_loss=0.02678 | best_loss=0.02677
Epoch 11/80: current_loss=0.02686 | best_loss=0.02677
Epoch 12/80: current_loss=0.02696 | best_loss=0.02677
Epoch 13/80: current_loss=0.02682 | best_loss=0.02677
Epoch 14/80: current_loss=0.02675 | best_loss=0.02675
Epoch 15/80: current_loss=0.02682 | best_loss=0.02675
Epoch 16/80: current_loss=0.02678 | best_loss=0.02675
Epoch 17/80: current_loss=0.02692 | best_loss=0.02675
Epoch 18/80: current_loss=0.02686 | best_loss=0.02675
Epoch 19/80: current_loss=0.02703 | best_loss=0.02675
Epoch 20/80: current_loss=0.02674 | best_loss=0.02674
Epoch 21/80: current_loss=0.02713 | best_loss=0.02674
Epoch 22/80: current_loss=0.02672 | best_loss=0.02672
Epoch 23/80: current_loss=0.02671 | best_loss=0.02671
Epoch 24/80: current_loss=0.02677 | best_loss=0.02671
Epoch 25/80: current_loss=0.02681 | best_loss=0.02671
Epoch 26/80: current_loss=0.02689 | best_loss=0.02671
Epoch 27/80: current_loss=0.02709 | best_loss=0.02671
Epoch 28/80: current_loss=0.02697 | best_loss=0.02671
Epoch 29/80: current_loss=0.02679 | best_loss=0.02671
Epoch 30/80: current_loss=0.02678 | best_loss=0.02671
Epoch 31/80: current_loss=0.02707 | best_loss=0.02671
Epoch 32/80: current_loss=0.02679 | best_loss=0.02671
Epoch 33/80: current_loss=0.02681 | best_loss=0.02671
Epoch 34/80: current_loss=0.02684 | best_loss=0.02671
Epoch 35/80: current_loss=0.02679 | best_loss=0.02671
Epoch 36/80: current_loss=0.02691 | best_loss=0.02671
Epoch 37/80: current_loss=0.02694 | best_loss=0.02671
Epoch 38/80: current_loss=0.02696 | best_loss=0.02671
Epoch 39/80: current_loss=0.02689 | best_loss=0.02671
Epoch 40/80: current_loss=0.02690 | best_loss=0.02671
Epoch 41/80: current_loss=0.02693 | best_loss=0.02671
Epoch 42/80: current_loss=0.02677 | best_loss=0.02671
Epoch 43/80: current_loss=0.02689 | best_loss=0.02671
Early Stopping at epoch 43
      explained_var=0.00168 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02985 | best_loss=0.02985
Epoch 1/80: current_loss=0.02971 | best_loss=0.02971
Epoch 2/80: current_loss=0.02981 | best_loss=0.02971
Epoch 3/80: current_loss=0.02980 | best_loss=0.02971
Epoch 4/80: current_loss=0.02968 | best_loss=0.02968
Epoch 5/80: current_loss=0.02976 | best_loss=0.02968
Epoch 6/80: current_loss=0.02969 | best_loss=0.02968
Epoch 7/80: current_loss=0.02970 | best_loss=0.02968
Epoch 8/80: current_loss=0.02973 | best_loss=0.02968
Epoch 9/80: current_loss=0.02983 | best_loss=0.02968
Epoch 10/80: current_loss=0.02979 | best_loss=0.02968
Epoch 11/80: current_loss=0.02983 | best_loss=0.02968
Epoch 12/80: current_loss=0.02972 | best_loss=0.02968
Epoch 13/80: current_loss=0.02974 | best_loss=0.02968
Epoch 14/80: current_loss=0.02971 | best_loss=0.02968
Epoch 15/80: current_loss=0.02975 | best_loss=0.02968
Epoch 16/80: current_loss=0.02970 | best_loss=0.02968
Epoch 17/80: current_loss=0.02992 | best_loss=0.02968
Epoch 18/80: current_loss=0.02969 | best_loss=0.02968
Epoch 19/80: current_loss=0.02969 | best_loss=0.02968
Epoch 20/80: current_loss=0.02967 | best_loss=0.02967
Epoch 21/80: current_loss=0.02974 | best_loss=0.02967
Epoch 22/80: current_loss=0.02975 | best_loss=0.02967
Epoch 23/80: current_loss=0.02970 | best_loss=0.02967
Epoch 24/80: current_loss=0.02979 | best_loss=0.02967
Epoch 25/80: current_loss=0.03024 | best_loss=0.02967
Epoch 26/80: current_loss=0.02970 | best_loss=0.02967
Epoch 27/80: current_loss=0.02976 | best_loss=0.02967
Epoch 28/80: current_loss=0.02968 | best_loss=0.02967
Epoch 29/80: current_loss=0.02976 | best_loss=0.02967
Epoch 30/80: current_loss=0.03012 | best_loss=0.02967
Epoch 31/80: current_loss=0.02975 | best_loss=0.02967
Epoch 32/80: current_loss=0.02971 | best_loss=0.02967
Epoch 33/80: current_loss=0.02988 | best_loss=0.02967
Epoch 34/80: current_loss=0.02970 | best_loss=0.02967
Epoch 35/80: current_loss=0.02974 | best_loss=0.02967
Epoch 36/80: current_loss=0.02975 | best_loss=0.02967
Epoch 37/80: current_loss=0.02971 | best_loss=0.02967
Epoch 38/80: current_loss=0.02974 | best_loss=0.02967
Epoch 39/80: current_loss=0.03013 | best_loss=0.02967
Epoch 40/80: current_loss=0.02974 | best_loss=0.02967
Early Stopping at epoch 40
      explained_var=0.00303 | mse_loss=0.02871
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00272| avg_loss=0.02688
----------------------------------------------


----------------------------------------------
Params for Trial 92
{'learning_rate': 0.0001, 'weight_decay': 0.0005012641075448434, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03307 | best_loss=0.03307
Epoch 1/80: current_loss=0.03121 | best_loss=0.03121
Epoch 2/80: current_loss=0.03015 | best_loss=0.03015
Epoch 3/80: current_loss=0.02989 | best_loss=0.02989
Epoch 4/80: current_loss=0.02939 | best_loss=0.02939
Epoch 5/80: current_loss=0.02898 | best_loss=0.02898
Epoch 6/80: current_loss=0.02889 | best_loss=0.02889
Epoch 7/80: current_loss=0.02902 | best_loss=0.02889
Epoch 8/80: current_loss=0.02890 | best_loss=0.02889
Epoch 9/80: current_loss=0.02896 | best_loss=0.02889
Epoch 10/80: current_loss=0.02877 | best_loss=0.02877
Epoch 11/80: current_loss=0.02877 | best_loss=0.02877
Epoch 12/80: current_loss=0.02884 | best_loss=0.02877
Epoch 13/80: current_loss=0.02886 | best_loss=0.02877
Epoch 14/80: current_loss=0.02876 | best_loss=0.02876
Epoch 15/80: current_loss=0.02881 | best_loss=0.02876
Epoch 16/80: current_loss=0.02922 | best_loss=0.02876
Epoch 17/80: current_loss=0.02913 | best_loss=0.02876
Epoch 18/80: current_loss=0.02895 | best_loss=0.02876
Epoch 19/80: current_loss=0.02897 | best_loss=0.02876
Epoch 20/80: current_loss=0.02892 | best_loss=0.02876
Epoch 21/80: current_loss=0.02897 | best_loss=0.02876
Epoch 22/80: current_loss=0.02899 | best_loss=0.02876
Epoch 23/80: current_loss=0.02918 | best_loss=0.02876
Epoch 24/80: current_loss=0.02897 | best_loss=0.02876
Epoch 25/80: current_loss=0.02902 | best_loss=0.02876
Epoch 26/80: current_loss=0.02906 | best_loss=0.02876
Epoch 27/80: current_loss=0.02892 | best_loss=0.02876
Epoch 28/80: current_loss=0.02892 | best_loss=0.02876
Epoch 29/80: current_loss=0.02894 | best_loss=0.02876
Epoch 30/80: current_loss=0.02900 | best_loss=0.02876
Epoch 31/80: current_loss=0.02900 | best_loss=0.02876
Epoch 32/80: current_loss=0.02929 | best_loss=0.02876
Epoch 33/80: current_loss=0.02960 | best_loss=0.02876
Epoch 34/80: current_loss=0.02894 | best_loss=0.02876
Early Stopping at epoch 34
      explained_var=0.00961 | mse_loss=0.02793
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02565 | best_loss=0.02565
Epoch 1/80: current_loss=0.02495 | best_loss=0.02495
Epoch 2/80: current_loss=0.02555 | best_loss=0.02495
Epoch 3/80: current_loss=0.02545 | best_loss=0.02495
Epoch 4/80: current_loss=0.02509 | best_loss=0.02495
Epoch 5/80: current_loss=0.02501 | best_loss=0.02495
Epoch 6/80: current_loss=0.02571 | best_loss=0.02495
Epoch 7/80: current_loss=0.02622 | best_loss=0.02495
Epoch 8/80: current_loss=0.02510 | best_loss=0.02495
Epoch 9/80: current_loss=0.02499 | best_loss=0.02495
Epoch 10/80: current_loss=0.02502 | best_loss=0.02495
Epoch 11/80: current_loss=0.02495 | best_loss=0.02495
Epoch 12/80: current_loss=0.02512 | best_loss=0.02495
Epoch 13/80: current_loss=0.02491 | best_loss=0.02491
Epoch 14/80: current_loss=0.02491 | best_loss=0.02491
Epoch 15/80: current_loss=0.02491 | best_loss=0.02491
Epoch 16/80: current_loss=0.02539 | best_loss=0.02491
Epoch 17/80: current_loss=0.02498 | best_loss=0.02491
Epoch 18/80: current_loss=0.02498 | best_loss=0.02491
Epoch 19/80: current_loss=0.02488 | best_loss=0.02488
Epoch 20/80: current_loss=0.02505 | best_loss=0.02488
Epoch 21/80: current_loss=0.02491 | best_loss=0.02488
Epoch 22/80: current_loss=0.02487 | best_loss=0.02487
Epoch 23/80: current_loss=0.02555 | best_loss=0.02487
Epoch 24/80: current_loss=0.02509 | best_loss=0.02487
Epoch 25/80: current_loss=0.02491 | best_loss=0.02487
Epoch 26/80: current_loss=0.02491 | best_loss=0.02487
Epoch 27/80: current_loss=0.02507 | best_loss=0.02487
Epoch 28/80: current_loss=0.02499 | best_loss=0.02487
Epoch 29/80: current_loss=0.02493 | best_loss=0.02487
Epoch 30/80: current_loss=0.02507 | best_loss=0.02487
Epoch 31/80: current_loss=0.02509 | best_loss=0.02487
Epoch 32/80: current_loss=0.02493 | best_loss=0.02487
Epoch 33/80: current_loss=0.02492 | best_loss=0.02487
Epoch 34/80: current_loss=0.02500 | best_loss=0.02487
Epoch 35/80: current_loss=0.02492 | best_loss=0.02487
Epoch 36/80: current_loss=0.02495 | best_loss=0.02487
Epoch 37/80: current_loss=0.02496 | best_loss=0.02487
Epoch 38/80: current_loss=0.02496 | best_loss=0.02487
Epoch 39/80: current_loss=0.02532 | best_loss=0.02487
Epoch 40/80: current_loss=0.02490 | best_loss=0.02487
Epoch 41/80: current_loss=0.02489 | best_loss=0.02487
Epoch 42/80: current_loss=0.02494 | best_loss=0.02487
Early Stopping at epoch 42
      explained_var=0.00193 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02754 | best_loss=0.02754
Epoch 1/80: current_loss=0.02758 | best_loss=0.02754
Epoch 2/80: current_loss=0.02780 | best_loss=0.02754
Epoch 3/80: current_loss=0.02890 | best_loss=0.02754
Epoch 4/80: current_loss=0.02767 | best_loss=0.02754
Epoch 5/80: current_loss=0.02864 | best_loss=0.02754
Epoch 6/80: current_loss=0.02797 | best_loss=0.02754
Epoch 7/80: current_loss=0.02767 | best_loss=0.02754
Epoch 8/80: current_loss=0.02758 | best_loss=0.02754
Epoch 9/80: current_loss=0.02763 | best_loss=0.02754
Epoch 10/80: current_loss=0.02817 | best_loss=0.02754
Epoch 11/80: current_loss=0.02761 | best_loss=0.02754
Epoch 12/80: current_loss=0.02751 | best_loss=0.02751
Epoch 13/80: current_loss=0.02784 | best_loss=0.02751
Epoch 14/80: current_loss=0.02783 | best_loss=0.02751
Epoch 15/80: current_loss=0.02789 | best_loss=0.02751
Epoch 16/80: current_loss=0.02728 | best_loss=0.02728
Epoch 17/80: current_loss=0.02771 | best_loss=0.02728
Epoch 18/80: current_loss=0.02744 | best_loss=0.02728
Epoch 19/80: current_loss=0.02781 | best_loss=0.02728
Epoch 20/80: current_loss=0.02774 | best_loss=0.02728
Epoch 21/80: current_loss=0.02757 | best_loss=0.02728
Epoch 22/80: current_loss=0.02780 | best_loss=0.02728
Epoch 23/80: current_loss=0.02770 | best_loss=0.02728
Epoch 24/80: current_loss=0.02772 | best_loss=0.02728
Epoch 25/80: current_loss=0.02746 | best_loss=0.02728
Epoch 26/80: current_loss=0.02756 | best_loss=0.02728
Epoch 27/80: current_loss=0.02789 | best_loss=0.02728
Epoch 28/80: current_loss=0.02829 | best_loss=0.02728
Epoch 29/80: current_loss=0.02793 | best_loss=0.02728
Epoch 30/80: current_loss=0.02757 | best_loss=0.02728
Epoch 31/80: current_loss=0.02734 | best_loss=0.02728
Epoch 32/80: current_loss=0.02772 | best_loss=0.02728
Epoch 33/80: current_loss=0.02749 | best_loss=0.02728
Epoch 34/80: current_loss=0.02775 | best_loss=0.02728
Epoch 35/80: current_loss=0.02752 | best_loss=0.02728
Epoch 36/80: current_loss=0.02782 | best_loss=0.02728
Early Stopping at epoch 36
      explained_var=-0.00391 | mse_loss=0.02770
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02689 | best_loss=0.02689
Epoch 1/80: current_loss=0.02694 | best_loss=0.02689
Epoch 2/80: current_loss=0.02703 | best_loss=0.02689
Epoch 3/80: current_loss=0.02700 | best_loss=0.02689
Epoch 4/80: current_loss=0.02678 | best_loss=0.02678
Epoch 5/80: current_loss=0.02672 | best_loss=0.02672
Epoch 6/80: current_loss=0.02692 | best_loss=0.02672
Epoch 7/80: current_loss=0.02703 | best_loss=0.02672
Epoch 8/80: current_loss=0.02703 | best_loss=0.02672
Epoch 9/80: current_loss=0.02673 | best_loss=0.02672
Epoch 10/80: current_loss=0.02672 | best_loss=0.02672
Epoch 11/80: current_loss=0.02675 | best_loss=0.02672
Epoch 12/80: current_loss=0.02683 | best_loss=0.02672
Epoch 13/80: current_loss=0.02698 | best_loss=0.02672
Epoch 14/80: current_loss=0.02717 | best_loss=0.02672
Epoch 15/80: current_loss=0.02681 | best_loss=0.02672
Epoch 16/80: current_loss=0.02680 | best_loss=0.02672
Epoch 17/80: current_loss=0.02676 | best_loss=0.02672
Epoch 18/80: current_loss=0.02678 | best_loss=0.02672
Epoch 19/80: current_loss=0.02678 | best_loss=0.02672
Epoch 20/80: current_loss=0.02678 | best_loss=0.02672
Epoch 21/80: current_loss=0.02703 | best_loss=0.02672
Epoch 22/80: current_loss=0.02680 | best_loss=0.02672
Epoch 23/80: current_loss=0.02684 | best_loss=0.02672
Epoch 24/80: current_loss=0.02678 | best_loss=0.02672
Epoch 25/80: current_loss=0.02679 | best_loss=0.02672
Early Stopping at epoch 25
      explained_var=0.00237 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02994 | best_loss=0.02994
Epoch 1/80: current_loss=0.02992 | best_loss=0.02992
Epoch 2/80: current_loss=0.02966 | best_loss=0.02966
Epoch 3/80: current_loss=0.02965 | best_loss=0.02965
Epoch 4/80: current_loss=0.02969 | best_loss=0.02965
Epoch 5/80: current_loss=0.02976 | best_loss=0.02965
Epoch 6/80: current_loss=0.02969 | best_loss=0.02965
Epoch 7/80: current_loss=0.02968 | best_loss=0.02965
Epoch 8/80: current_loss=0.02971 | best_loss=0.02965
Epoch 9/80: current_loss=0.02966 | best_loss=0.02965
Epoch 10/80: current_loss=0.02979 | best_loss=0.02965
Epoch 11/80: current_loss=0.02967 | best_loss=0.02965
Epoch 12/80: current_loss=0.02990 | best_loss=0.02965
Epoch 13/80: current_loss=0.02973 | best_loss=0.02965
Epoch 14/80: current_loss=0.02984 | best_loss=0.02965
Epoch 15/80: current_loss=0.02969 | best_loss=0.02965
Epoch 16/80: current_loss=0.02997 | best_loss=0.02965
Epoch 17/80: current_loss=0.02973 | best_loss=0.02965
Epoch 18/80: current_loss=0.02969 | best_loss=0.02965
Epoch 19/80: current_loss=0.02976 | best_loss=0.02965
Epoch 20/80: current_loss=0.02970 | best_loss=0.02965
Epoch 21/80: current_loss=0.02974 | best_loss=0.02965
Epoch 22/80: current_loss=0.02981 | best_loss=0.02965
Epoch 23/80: current_loss=0.03032 | best_loss=0.02965
Early Stopping at epoch 23
      explained_var=0.00388 | mse_loss=0.02870
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=0.00278| avg_loss=0.02689
----------------------------------------------


----------------------------------------------
Params for Trial 93
{'learning_rate': 0.0001, 'weight_decay': 0.000489377182183247, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03251 | best_loss=0.03251
Epoch 1/80: current_loss=0.03063 | best_loss=0.03063
Epoch 2/80: current_loss=0.02973 | best_loss=0.02973
Epoch 3/80: current_loss=0.02916 | best_loss=0.02916
Epoch 4/80: current_loss=0.02905 | best_loss=0.02905
Epoch 5/80: current_loss=0.02899 | best_loss=0.02899
Epoch 6/80: current_loss=0.02894 | best_loss=0.02894
Epoch 7/80: current_loss=0.02887 | best_loss=0.02887
Epoch 8/80: current_loss=0.02903 | best_loss=0.02887
Epoch 9/80: current_loss=0.02922 | best_loss=0.02887
Epoch 10/80: current_loss=0.02885 | best_loss=0.02885
Epoch 11/80: current_loss=0.02930 | best_loss=0.02885
Epoch 12/80: current_loss=0.02883 | best_loss=0.02883
Epoch 13/80: current_loss=0.02888 | best_loss=0.02883
Epoch 14/80: current_loss=0.02931 | best_loss=0.02883
Epoch 15/80: current_loss=0.02891 | best_loss=0.02883
Epoch 16/80: current_loss=0.02889 | best_loss=0.02883
Epoch 17/80: current_loss=0.03001 | best_loss=0.02883
Epoch 18/80: current_loss=0.02916 | best_loss=0.02883
Epoch 19/80: current_loss=0.02892 | best_loss=0.02883
Epoch 20/80: current_loss=0.02891 | best_loss=0.02883
Epoch 21/80: current_loss=0.02892 | best_loss=0.02883
Epoch 22/80: current_loss=0.02940 | best_loss=0.02883
Epoch 23/80: current_loss=0.02901 | best_loss=0.02883
Epoch 24/80: current_loss=0.02896 | best_loss=0.02883
Epoch 25/80: current_loss=0.02895 | best_loss=0.02883
Epoch 26/80: current_loss=0.02894 | best_loss=0.02883
Epoch 27/80: current_loss=0.02889 | best_loss=0.02883
Epoch 28/80: current_loss=0.02888 | best_loss=0.02883
Epoch 29/80: current_loss=0.02891 | best_loss=0.02883
Epoch 30/80: current_loss=0.02920 | best_loss=0.02883
Epoch 31/80: current_loss=0.02898 | best_loss=0.02883
Epoch 32/80: current_loss=0.02898 | best_loss=0.02883
Early Stopping at epoch 32
      explained_var=0.00531 | mse_loss=0.02799
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02509 | best_loss=0.02509
Epoch 1/80: current_loss=0.02505 | best_loss=0.02505
Epoch 2/80: current_loss=0.02526 | best_loss=0.02505
Epoch 3/80: current_loss=0.02528 | best_loss=0.02505
Epoch 4/80: current_loss=0.02530 | best_loss=0.02505
Epoch 5/80: current_loss=0.02493 | best_loss=0.02493
Epoch 6/80: current_loss=0.02504 | best_loss=0.02493
Epoch 7/80: current_loss=0.02515 | best_loss=0.02493
Epoch 8/80: current_loss=0.02510 | best_loss=0.02493
Epoch 9/80: current_loss=0.02524 | best_loss=0.02493
Epoch 10/80: current_loss=0.02497 | best_loss=0.02493
Epoch 11/80: current_loss=0.02493 | best_loss=0.02493
Epoch 12/80: current_loss=0.02512 | best_loss=0.02493
Epoch 13/80: current_loss=0.02494 | best_loss=0.02493
Epoch 14/80: current_loss=0.02500 | best_loss=0.02493
Epoch 15/80: current_loss=0.02505 | best_loss=0.02493
Epoch 16/80: current_loss=0.02559 | best_loss=0.02493
Epoch 17/80: current_loss=0.02539 | best_loss=0.02493
Epoch 18/80: current_loss=0.02495 | best_loss=0.02493
Epoch 19/80: current_loss=0.02499 | best_loss=0.02493
Epoch 20/80: current_loss=0.02496 | best_loss=0.02493
Epoch 21/80: current_loss=0.02509 | best_loss=0.02493
Epoch 22/80: current_loss=0.02499 | best_loss=0.02493
Epoch 23/80: current_loss=0.02498 | best_loss=0.02493
Epoch 24/80: current_loss=0.02503 | best_loss=0.02493
Epoch 25/80: current_loss=0.02514 | best_loss=0.02493
Epoch 26/80: current_loss=0.02549 | best_loss=0.02493
Epoch 27/80: current_loss=0.02495 | best_loss=0.02493
Epoch 28/80: current_loss=0.02507 | best_loss=0.02493
Epoch 29/80: current_loss=0.02561 | best_loss=0.02493
Epoch 30/80: current_loss=0.02541 | best_loss=0.02493
Epoch 31/80: current_loss=0.02498 | best_loss=0.02493
Early Stopping at epoch 31
      explained_var=0.00099 | mse_loss=0.02518

----------------------------------------------
Params for Trial 94
{'learning_rate': 0.0001, 'weight_decay': 0.0008065455942931601, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03173 | best_loss=0.03173
Epoch 1/80: current_loss=0.03026 | best_loss=0.03026
Epoch 2/80: current_loss=0.02947 | best_loss=0.02947
Epoch 3/80: current_loss=0.02944 | best_loss=0.02944
Epoch 4/80: current_loss=0.02933 | best_loss=0.02933
Epoch 5/80: current_loss=0.02920 | best_loss=0.02920
Epoch 6/80: current_loss=0.02879 | best_loss=0.02879
Epoch 7/80: current_loss=0.02879 | best_loss=0.02879
Epoch 8/80: current_loss=0.02882 | best_loss=0.02879
Epoch 9/80: current_loss=0.02897 | best_loss=0.02879
Epoch 10/80: current_loss=0.02890 | best_loss=0.02879
Epoch 11/80: current_loss=0.02899 | best_loss=0.02879
Epoch 12/80: current_loss=0.02891 | best_loss=0.02879
Epoch 13/80: current_loss=0.02919 | best_loss=0.02879
Epoch 14/80: current_loss=0.02893 | best_loss=0.02879
Epoch 15/80: current_loss=0.02888 | best_loss=0.02879
Epoch 16/80: current_loss=0.02885 | best_loss=0.02879
Epoch 17/80: current_loss=0.02890 | best_loss=0.02879
Epoch 18/80: current_loss=0.02891 | best_loss=0.02879
Epoch 19/80: current_loss=0.02890 | best_loss=0.02879
Epoch 20/80: current_loss=0.02907 | best_loss=0.02879
Epoch 21/80: current_loss=0.02891 | best_loss=0.02879
Epoch 22/80: current_loss=0.02884 | best_loss=0.02879
Epoch 23/80: current_loss=0.02893 | best_loss=0.02879
Epoch 24/80: current_loss=0.02891 | best_loss=0.02879
Epoch 25/80: current_loss=0.02919 | best_loss=0.02879
Epoch 26/80: current_loss=0.02885 | best_loss=0.02879
Early Stopping at epoch 26
      explained_var=0.00740 | mse_loss=0.02793
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02504 | best_loss=0.02504
Epoch 1/80: current_loss=0.02496 | best_loss=0.02496
Epoch 2/80: current_loss=0.02519 | best_loss=0.02496
Epoch 3/80: current_loss=0.02490 | best_loss=0.02490
Epoch 4/80: current_loss=0.02491 | best_loss=0.02490
Epoch 5/80: current_loss=0.02496 | best_loss=0.02490
Epoch 6/80: current_loss=0.02513 | best_loss=0.02490
Epoch 7/80: current_loss=0.02490 | best_loss=0.02490
Epoch 8/80: current_loss=0.02501 | best_loss=0.02490
Epoch 9/80: current_loss=0.02492 | best_loss=0.02490
Epoch 10/80: current_loss=0.02488 | best_loss=0.02488
Epoch 11/80: current_loss=0.02502 | best_loss=0.02488
Epoch 12/80: current_loss=0.02489 | best_loss=0.02488
Epoch 13/80: current_loss=0.02495 | best_loss=0.02488
Epoch 14/80: current_loss=0.02503 | best_loss=0.02488
Epoch 15/80: current_loss=0.02492 | best_loss=0.02488
Epoch 16/80: current_loss=0.02513 | best_loss=0.02488
Epoch 17/80: current_loss=0.02497 | best_loss=0.02488
Epoch 18/80: current_loss=0.02506 | best_loss=0.02488
Epoch 19/80: current_loss=0.02492 | best_loss=0.02488
Epoch 20/80: current_loss=0.02505 | best_loss=0.02488
Epoch 21/80: current_loss=0.02503 | best_loss=0.02488
Epoch 22/80: current_loss=0.02504 | best_loss=0.02488
Epoch 23/80: current_loss=0.02499 | best_loss=0.02488
Epoch 24/80: current_loss=0.02494 | best_loss=0.02488
Epoch 25/80: current_loss=0.02490 | best_loss=0.02488
Epoch 26/80: current_loss=0.02489 | best_loss=0.02488
Epoch 27/80: current_loss=0.02489 | best_loss=0.02488
Epoch 28/80: current_loss=0.02486 | best_loss=0.02486
Epoch 29/80: current_loss=0.02483 | best_loss=0.02483
Epoch 30/80: current_loss=0.02490 | best_loss=0.02483
Epoch 31/80: current_loss=0.02486 | best_loss=0.02483
Epoch 32/80: current_loss=0.02493 | best_loss=0.02483
Epoch 33/80: current_loss=0.02506 | best_loss=0.02483
Epoch 34/80: current_loss=0.02494 | best_loss=0.02483
Epoch 35/80: current_loss=0.02489 | best_loss=0.02483
Epoch 36/80: current_loss=0.02489 | best_loss=0.02483
Epoch 37/80: current_loss=0.02496 | best_loss=0.02483
Epoch 38/80: current_loss=0.02496 | best_loss=0.02483
Epoch 39/80: current_loss=0.02495 | best_loss=0.02483
Epoch 40/80: current_loss=0.02496 | best_loss=0.02483
Epoch 41/80: current_loss=0.02498 | best_loss=0.02483
Epoch 42/80: current_loss=0.02500 | best_loss=0.02483
Epoch 43/80: current_loss=0.02503 | best_loss=0.02483
Epoch 44/80: current_loss=0.02529 | best_loss=0.02483
Epoch 45/80: current_loss=0.02492 | best_loss=0.02483
Epoch 46/80: current_loss=0.02492 | best_loss=0.02483
Epoch 47/80: current_loss=0.02525 | best_loss=0.02483
Epoch 48/80: current_loss=0.02496 | best_loss=0.02483
Epoch 49/80: current_loss=0.02495 | best_loss=0.02483
Early Stopping at epoch 49
      explained_var=0.00226 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02817 | best_loss=0.02817
Epoch 1/80: current_loss=0.02818 | best_loss=0.02817
Epoch 2/80: current_loss=0.02745 | best_loss=0.02745
Epoch 3/80: current_loss=0.02759 | best_loss=0.02745
Epoch 4/80: current_loss=0.02802 | best_loss=0.02745
Epoch 5/80: current_loss=0.02784 | best_loss=0.02745
Epoch 6/80: current_loss=0.02744 | best_loss=0.02744
Epoch 7/80: current_loss=0.02789 | best_loss=0.02744
Epoch 8/80: current_loss=0.02785 | best_loss=0.02744
Epoch 9/80: current_loss=0.02728 | best_loss=0.02728
Epoch 10/80: current_loss=0.02762 | best_loss=0.02728
Epoch 11/80: current_loss=0.02764 | best_loss=0.02728
Epoch 12/80: current_loss=0.02734 | best_loss=0.02728
Epoch 13/80: current_loss=0.02729 | best_loss=0.02728
Epoch 14/80: current_loss=0.02751 | best_loss=0.02728
Epoch 15/80: current_loss=0.02785 | best_loss=0.02728
Epoch 16/80: current_loss=0.02750 | best_loss=0.02728
Epoch 17/80: current_loss=0.02775 | best_loss=0.02728
Epoch 18/80: current_loss=0.02811 | best_loss=0.02728
Epoch 19/80: current_loss=0.02800 | best_loss=0.02728
Epoch 20/80: current_loss=0.02772 | best_loss=0.02728
Epoch 21/80: current_loss=0.02741 | best_loss=0.02728
Epoch 22/80: current_loss=0.02792 | best_loss=0.02728
Epoch 23/80: current_loss=0.02759 | best_loss=0.02728
Epoch 24/80: current_loss=0.02806 | best_loss=0.02728
Epoch 25/80: current_loss=0.02770 | best_loss=0.02728
Epoch 26/80: current_loss=0.02775 | best_loss=0.02728
Epoch 27/80: current_loss=0.02746 | best_loss=0.02728
Epoch 28/80: current_loss=0.02762 | best_loss=0.02728
Epoch 29/80: current_loss=0.02786 | best_loss=0.02728
Early Stopping at epoch 29
      explained_var=-0.00454 | mse_loss=0.02773
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02676 | best_loss=0.02676
Epoch 1/80: current_loss=0.02675 | best_loss=0.02675
Epoch 2/80: current_loss=0.02732 | best_loss=0.02675
Epoch 3/80: current_loss=0.02702 | best_loss=0.02675
Epoch 4/80: current_loss=0.02677 | best_loss=0.02675
Epoch 5/80: current_loss=0.02690 | best_loss=0.02675
Epoch 6/80: current_loss=0.02682 | best_loss=0.02675
Epoch 7/80: current_loss=0.02681 | best_loss=0.02675
Epoch 8/80: current_loss=0.02671 | best_loss=0.02671
Epoch 9/80: current_loss=0.02673 | best_loss=0.02671
Epoch 10/80: current_loss=0.02685 | best_loss=0.02671
Epoch 11/80: current_loss=0.02717 | best_loss=0.02671
Epoch 12/80: current_loss=0.02678 | best_loss=0.02671
Epoch 13/80: current_loss=0.02685 | best_loss=0.02671
Epoch 14/80: current_loss=0.02676 | best_loss=0.02671
Epoch 15/80: current_loss=0.02695 | best_loss=0.02671
Epoch 16/80: current_loss=0.02675 | best_loss=0.02671
Epoch 17/80: current_loss=0.02693 | best_loss=0.02671
Epoch 18/80: current_loss=0.02672 | best_loss=0.02671
Epoch 19/80: current_loss=0.02678 | best_loss=0.02671
Epoch 20/80: current_loss=0.02675 | best_loss=0.02671
Epoch 21/80: current_loss=0.02674 | best_loss=0.02671
Epoch 22/80: current_loss=0.02681 | best_loss=0.02671
Epoch 23/80: current_loss=0.02703 | best_loss=0.02671
Epoch 24/80: current_loss=0.02678 | best_loss=0.02671
Epoch 25/80: current_loss=0.02680 | best_loss=0.02671
Epoch 26/80: current_loss=0.02729 | best_loss=0.02671
Epoch 27/80: current_loss=0.02677 | best_loss=0.02671
Epoch 28/80: current_loss=0.02709 | best_loss=0.02671
Early Stopping at epoch 28
      explained_var=0.00211 | mse_loss=0.02493
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02984 | best_loss=0.02984
Epoch 1/80: current_loss=0.02978 | best_loss=0.02978
Epoch 2/80: current_loss=0.02974 | best_loss=0.02974
Epoch 3/80: current_loss=0.02972 | best_loss=0.02972
Epoch 4/80: current_loss=0.02976 | best_loss=0.02972
Epoch 5/80: current_loss=0.02971 | best_loss=0.02971
Epoch 6/80: current_loss=0.02984 | best_loss=0.02971
Epoch 7/80: current_loss=0.02977 | best_loss=0.02971
Epoch 8/80: current_loss=0.02985 | best_loss=0.02971
Epoch 9/80: current_loss=0.02982 | best_loss=0.02971
Epoch 10/80: current_loss=0.02997 | best_loss=0.02971
Epoch 11/80: current_loss=0.02972 | best_loss=0.02971
Epoch 12/80: current_loss=0.02970 | best_loss=0.02970
Epoch 13/80: current_loss=0.02983 | best_loss=0.02970
Epoch 14/80: current_loss=0.02973 | best_loss=0.02970
Epoch 15/80: current_loss=0.02995 | best_loss=0.02970
Epoch 16/80: current_loss=0.02982 | best_loss=0.02970
Epoch 17/80: current_loss=0.02970 | best_loss=0.02970
Epoch 18/80: current_loss=0.02973 | best_loss=0.02970
Epoch 19/80: current_loss=0.02976 | best_loss=0.02970
Epoch 20/80: current_loss=0.02973 | best_loss=0.02970
Epoch 21/80: current_loss=0.02971 | best_loss=0.02970
Epoch 22/80: current_loss=0.02971 | best_loss=0.02970
Epoch 23/80: current_loss=0.02984 | best_loss=0.02970
Epoch 24/80: current_loss=0.02979 | best_loss=0.02970
Epoch 25/80: current_loss=0.03022 | best_loss=0.02970
Epoch 26/80: current_loss=0.02981 | best_loss=0.02970
Epoch 27/80: current_loss=0.03005 | best_loss=0.02970
Epoch 28/80: current_loss=0.02985 | best_loss=0.02970
Epoch 29/80: current_loss=0.02975 | best_loss=0.02970
Epoch 30/80: current_loss=0.02972 | best_loss=0.02970
Epoch 31/80: current_loss=0.02972 | best_loss=0.02970
Epoch 32/80: current_loss=0.02975 | best_loss=0.02970
Epoch 33/80: current_loss=0.02972 | best_loss=0.02970
Epoch 34/80: current_loss=0.02986 | best_loss=0.02970
Epoch 35/80: current_loss=0.02986 | best_loss=0.02970
Epoch 36/80: current_loss=0.02972 | best_loss=0.02970
Epoch 37/80: current_loss=0.02978 | best_loss=0.02970
Early Stopping at epoch 37
      explained_var=0.00175 | mse_loss=0.02871
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.00180| avg_loss=0.02689
----------------------------------------------


----------------------------------------------
Params for Trial 95
{'learning_rate': 0.0001, 'weight_decay': 0.00046826361544812536, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03405 | best_loss=0.03405
Epoch 1/80: current_loss=0.03233 | best_loss=0.03233
Epoch 2/80: current_loss=0.03125 | best_loss=0.03125
Epoch 3/80: current_loss=0.03053 | best_loss=0.03053
Epoch 4/80: current_loss=0.03026 | best_loss=0.03026
Epoch 5/80: current_loss=0.02995 | best_loss=0.02995
Epoch 6/80: current_loss=0.02952 | best_loss=0.02952
Epoch 7/80: current_loss=0.02972 | best_loss=0.02952
Epoch 8/80: current_loss=0.02942 | best_loss=0.02942
Epoch 9/80: current_loss=0.02897 | best_loss=0.02897
Epoch 10/80: current_loss=0.02892 | best_loss=0.02892
Epoch 11/80: current_loss=0.02897 | best_loss=0.02892
Epoch 12/80: current_loss=0.02890 | best_loss=0.02890
Epoch 13/80: current_loss=0.02893 | best_loss=0.02890
Epoch 14/80: current_loss=0.02890 | best_loss=0.02890
Epoch 15/80: current_loss=0.02905 | best_loss=0.02890
Epoch 16/80: current_loss=0.02894 | best_loss=0.02890
Epoch 17/80: current_loss=0.02895 | best_loss=0.02890
Epoch 18/80: current_loss=0.02908 | best_loss=0.02890
Epoch 19/80: current_loss=0.02888 | best_loss=0.02888
Epoch 20/80: current_loss=0.02906 | best_loss=0.02888
Epoch 21/80: current_loss=0.02897 | best_loss=0.02888
Epoch 22/80: current_loss=0.02880 | best_loss=0.02880
Epoch 23/80: current_loss=0.02884 | best_loss=0.02880
Epoch 24/80: current_loss=0.02890 | best_loss=0.02880
Epoch 25/80: current_loss=0.02906 | best_loss=0.02880
Epoch 26/80: current_loss=0.02895 | best_loss=0.02880
Epoch 27/80: current_loss=0.02893 | best_loss=0.02880
Epoch 28/80: current_loss=0.02883 | best_loss=0.02880
Epoch 29/80: current_loss=0.02884 | best_loss=0.02880
Epoch 30/80: current_loss=0.02920 | best_loss=0.02880
Epoch 31/80: current_loss=0.02896 | best_loss=0.02880
Epoch 32/80: current_loss=0.02891 | best_loss=0.02880
Epoch 33/80: current_loss=0.02902 | best_loss=0.02880
Epoch 34/80: current_loss=0.02895 | best_loss=0.02880
Epoch 35/80: current_loss=0.02900 | best_loss=0.02880
Epoch 36/80: current_loss=0.02906 | best_loss=0.02880
Epoch 37/80: current_loss=0.02919 | best_loss=0.02880
Epoch 38/80: current_loss=0.02916 | best_loss=0.02880
Epoch 39/80: current_loss=0.02908 | best_loss=0.02880
Epoch 40/80: current_loss=0.02909 | best_loss=0.02880
Epoch 41/80: current_loss=0.02905 | best_loss=0.02880
Epoch 42/80: current_loss=0.02964 | best_loss=0.02880
Early Stopping at epoch 42
      explained_var=0.00733 | mse_loss=0.02794
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02493 | best_loss=0.02493
Epoch 1/80: current_loss=0.02532 | best_loss=0.02493
Epoch 2/80: current_loss=0.02489 | best_loss=0.02489
Epoch 3/80: current_loss=0.02497 | best_loss=0.02489
Epoch 4/80: current_loss=0.02522 | best_loss=0.02489
Epoch 5/80: current_loss=0.02511 | best_loss=0.02489
Epoch 6/80: current_loss=0.02521 | best_loss=0.02489
Epoch 7/80: current_loss=0.02504 | best_loss=0.02489
Epoch 8/80: current_loss=0.02494 | best_loss=0.02489
Epoch 9/80: current_loss=0.02489 | best_loss=0.02489
Epoch 10/80: current_loss=0.02487 | best_loss=0.02487
Epoch 11/80: current_loss=0.02520 | best_loss=0.02487
Epoch 12/80: current_loss=0.02501 | best_loss=0.02487
Epoch 13/80: current_loss=0.02494 | best_loss=0.02487
Epoch 14/80: current_loss=0.02514 | best_loss=0.02487
Epoch 15/80: current_loss=0.02486 | best_loss=0.02486
Epoch 16/80: current_loss=0.02488 | best_loss=0.02486
Epoch 17/80: current_loss=0.02502 | best_loss=0.02486
Epoch 18/80: current_loss=0.02525 | best_loss=0.02486
Epoch 19/80: current_loss=0.02493 | best_loss=0.02486
Epoch 20/80: current_loss=0.02487 | best_loss=0.02486
Epoch 21/80: current_loss=0.02484 | best_loss=0.02484
Epoch 22/80: current_loss=0.02499 | best_loss=0.02484
Epoch 23/80: current_loss=0.02490 | best_loss=0.02484
Epoch 24/80: current_loss=0.02496 | best_loss=0.02484
Epoch 25/80: current_loss=0.02492 | best_loss=0.02484
Epoch 26/80: current_loss=0.02491 | best_loss=0.02484
Epoch 27/80: current_loss=0.02492 | best_loss=0.02484
Epoch 28/80: current_loss=0.02587 | best_loss=0.02484
Epoch 29/80: current_loss=0.02507 | best_loss=0.02484
Epoch 30/80: current_loss=0.02489 | best_loss=0.02484
Epoch 31/80: current_loss=0.02486 | best_loss=0.02484
Epoch 32/80: current_loss=0.02496 | best_loss=0.02484
Epoch 33/80: current_loss=0.02492 | best_loss=0.02484
Epoch 34/80: current_loss=0.02518 | best_loss=0.02484
Epoch 35/80: current_loss=0.02511 | best_loss=0.02484
Epoch 36/80: current_loss=0.02508 | best_loss=0.02484
Epoch 37/80: current_loss=0.02521 | best_loss=0.02484
Epoch 38/80: current_loss=0.02498 | best_loss=0.02484
Epoch 39/80: current_loss=0.02503 | best_loss=0.02484
Epoch 40/80: current_loss=0.02536 | best_loss=0.02484
Epoch 41/80: current_loss=0.02528 | best_loss=0.02484
Early Stopping at epoch 41
      explained_var=0.00223 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02736 | best_loss=0.02736
Epoch 1/80: current_loss=0.02750 | best_loss=0.02736
Epoch 2/80: current_loss=0.02795 | best_loss=0.02736
Epoch 3/80: current_loss=0.02768 | best_loss=0.02736
Epoch 4/80: current_loss=0.02832 | best_loss=0.02736
Epoch 5/80: current_loss=0.02748 | best_loss=0.02736
Epoch 6/80: current_loss=0.02798 | best_loss=0.02736
Epoch 7/80: current_loss=0.02748 | best_loss=0.02736
Epoch 8/80: current_loss=0.02787 | best_loss=0.02736
Epoch 9/80: current_loss=0.02744 | best_loss=0.02736
Epoch 10/80: current_loss=0.02796 | best_loss=0.02736
Epoch 11/80: current_loss=0.02736 | best_loss=0.02736
Epoch 12/80: current_loss=0.02793 | best_loss=0.02736
Epoch 13/80: current_loss=0.02774 | best_loss=0.02736
Epoch 14/80: current_loss=0.02736 | best_loss=0.02736
Epoch 15/80: current_loss=0.02849 | best_loss=0.02736
Epoch 16/80: current_loss=0.02884 | best_loss=0.02736
Epoch 17/80: current_loss=0.02836 | best_loss=0.02736
Epoch 18/80: current_loss=0.02730 | best_loss=0.02730
Epoch 19/80: current_loss=0.02738 | best_loss=0.02730
Epoch 20/80: current_loss=0.02838 | best_loss=0.02730
Epoch 21/80: current_loss=0.02741 | best_loss=0.02730
Epoch 22/80: current_loss=0.02812 | best_loss=0.02730
Epoch 23/80: current_loss=0.02786 | best_loss=0.02730
Epoch 24/80: current_loss=0.02870 | best_loss=0.02730
Epoch 25/80: current_loss=0.02770 | best_loss=0.02730
Epoch 26/80: current_loss=0.02780 | best_loss=0.02730
Epoch 27/80: current_loss=0.02728 | best_loss=0.02728
Epoch 28/80: current_loss=0.02735 | best_loss=0.02728
Epoch 29/80: current_loss=0.02778 | best_loss=0.02728
Epoch 30/80: current_loss=0.02733 | best_loss=0.02728
Epoch 31/80: current_loss=0.02764 | best_loss=0.02728
Epoch 32/80: current_loss=0.02742 | best_loss=0.02728
Epoch 33/80: current_loss=0.02734 | best_loss=0.02728
Epoch 34/80: current_loss=0.02751 | best_loss=0.02728
Epoch 35/80: current_loss=0.02729 | best_loss=0.02728
Epoch 36/80: current_loss=0.02768 | best_loss=0.02728
Epoch 37/80: current_loss=0.02743 | best_loss=0.02728
Epoch 38/80: current_loss=0.02788 | best_loss=0.02728
Epoch 39/80: current_loss=0.02744 | best_loss=0.02728
Epoch 40/80: current_loss=0.02822 | best_loss=0.02728
Epoch 41/80: current_loss=0.02776 | best_loss=0.02728
Epoch 42/80: current_loss=0.02749 | best_loss=0.02728
Epoch 43/80: current_loss=0.02751 | best_loss=0.02728
Epoch 44/80: current_loss=0.02826 | best_loss=0.02728
Epoch 45/80: current_loss=0.02760 | best_loss=0.02728
Epoch 46/80: current_loss=0.02734 | best_loss=0.02728
Epoch 47/80: current_loss=0.02734 | best_loss=0.02728
Early Stopping at epoch 47
      explained_var=-0.00502 | mse_loss=0.02772
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02676 | best_loss=0.02676
Epoch 1/80: current_loss=0.02674 | best_loss=0.02674
Epoch 2/80: current_loss=0.02677 | best_loss=0.02674
Epoch 3/80: current_loss=0.02678 | best_loss=0.02674
Epoch 4/80: current_loss=0.02694 | best_loss=0.02674
Epoch 5/80: current_loss=0.02732 | best_loss=0.02674
Epoch 6/80: current_loss=0.02678 | best_loss=0.02674
Epoch 7/80: current_loss=0.02676 | best_loss=0.02674
Epoch 8/80: current_loss=0.02675 | best_loss=0.02674
Epoch 9/80: current_loss=0.02683 | best_loss=0.02674
Epoch 10/80: current_loss=0.02677 | best_loss=0.02674
Epoch 11/80: current_loss=0.02682 | best_loss=0.02674
Epoch 12/80: current_loss=0.02697 | best_loss=0.02674
Epoch 13/80: current_loss=0.02705 | best_loss=0.02674
Epoch 14/80: current_loss=0.02817 | best_loss=0.02674
Epoch 15/80: current_loss=0.02681 | best_loss=0.02674
Epoch 16/80: current_loss=0.02686 | best_loss=0.02674
Epoch 17/80: current_loss=0.02680 | best_loss=0.02674
Epoch 18/80: current_loss=0.02679 | best_loss=0.02674
Epoch 19/80: current_loss=0.02678 | best_loss=0.02674
Epoch 20/80: current_loss=0.02707 | best_loss=0.02674
Epoch 21/80: current_loss=0.02689 | best_loss=0.02674
Early Stopping at epoch 21
      explained_var=0.00152 | mse_loss=0.02495
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02995 | best_loss=0.02995
Epoch 1/80: current_loss=0.02972 | best_loss=0.02972
Epoch 2/80: current_loss=0.02996 | best_loss=0.02972
Epoch 3/80: current_loss=0.02976 | best_loss=0.02972
Epoch 4/80: current_loss=0.02974 | best_loss=0.02972
Epoch 5/80: current_loss=0.02966 | best_loss=0.02966
Epoch 6/80: current_loss=0.02971 | best_loss=0.02966
Epoch 7/80: current_loss=0.02989 | best_loss=0.02966
Epoch 8/80: current_loss=0.02974 | best_loss=0.02966
Epoch 9/80: current_loss=0.02974 | best_loss=0.02966
Epoch 10/80: current_loss=0.02979 | best_loss=0.02966
Epoch 11/80: current_loss=0.02964 | best_loss=0.02964
Epoch 12/80: current_loss=0.02967 | best_loss=0.02964
Epoch 13/80: current_loss=0.02989 | best_loss=0.02964
Epoch 14/80: current_loss=0.02971 | best_loss=0.02964
Epoch 15/80: current_loss=0.02982 | best_loss=0.02964
Epoch 16/80: current_loss=0.03018 | best_loss=0.02964
Epoch 17/80: current_loss=0.03005 | best_loss=0.02964
Epoch 18/80: current_loss=0.02973 | best_loss=0.02964
Epoch 19/80: current_loss=0.02980 | best_loss=0.02964
Epoch 20/80: current_loss=0.02976 | best_loss=0.02964
Epoch 21/80: current_loss=0.03000 | best_loss=0.02964
Epoch 22/80: current_loss=0.02984 | best_loss=0.02964
Epoch 23/80: current_loss=0.02981 | best_loss=0.02964
Epoch 24/80: current_loss=0.03012 | best_loss=0.02964
Epoch 25/80: current_loss=0.02987 | best_loss=0.02964
Epoch 26/80: current_loss=0.02997 | best_loss=0.02964
Epoch 27/80: current_loss=0.02981 | best_loss=0.02964
Epoch 28/80: current_loss=0.02983 | best_loss=0.02964
Epoch 29/80: current_loss=0.02979 | best_loss=0.02964
Epoch 30/80: current_loss=0.02978 | best_loss=0.02964
Epoch 31/80: current_loss=0.02993 | best_loss=0.02964
Early Stopping at epoch 31
      explained_var=0.00345 | mse_loss=0.02867
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.00190| avg_loss=0.02689
----------------------------------------------


----------------------------------------------
Params for Trial 96
{'learning_rate': 0.0001, 'weight_decay': 0.0010775193840073137, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03441 | best_loss=0.03441
Epoch 1/80: current_loss=0.03073 | best_loss=0.03073
Epoch 2/80: current_loss=0.02964 | best_loss=0.02964
Epoch 3/80: current_loss=0.02983 | best_loss=0.02964
Epoch 4/80: current_loss=0.02901 | best_loss=0.02901
Epoch 5/80: current_loss=0.02911 | best_loss=0.02901
Epoch 6/80: current_loss=0.02876 | best_loss=0.02876
Epoch 7/80: current_loss=0.02873 | best_loss=0.02873
Epoch 8/80: current_loss=0.02871 | best_loss=0.02871
Epoch 9/80: current_loss=0.02901 | best_loss=0.02871
Epoch 10/80: current_loss=0.02888 | best_loss=0.02871
Epoch 11/80: current_loss=0.02905 | best_loss=0.02871
Epoch 12/80: current_loss=0.02900 | best_loss=0.02871
Epoch 13/80: current_loss=0.02917 | best_loss=0.02871
Epoch 14/80: current_loss=0.02898 | best_loss=0.02871
Epoch 15/80: current_loss=0.02892 | best_loss=0.02871
Epoch 16/80: current_loss=0.02894 | best_loss=0.02871
Epoch 17/80: current_loss=0.02894 | best_loss=0.02871
Epoch 18/80: current_loss=0.02891 | best_loss=0.02871
Epoch 19/80: current_loss=0.02902 | best_loss=0.02871
Epoch 20/80: current_loss=0.02922 | best_loss=0.02871
Epoch 21/80: current_loss=0.02950 | best_loss=0.02871
Epoch 22/80: current_loss=0.02898 | best_loss=0.02871
Epoch 23/80: current_loss=0.02922 | best_loss=0.02871
Epoch 24/80: current_loss=0.02898 | best_loss=0.02871
Epoch 25/80: current_loss=0.02894 | best_loss=0.02871
Epoch 26/80: current_loss=0.02941 | best_loss=0.02871
Epoch 27/80: current_loss=0.02911 | best_loss=0.02871
Epoch 28/80: current_loss=0.02903 | best_loss=0.02871
Early Stopping at epoch 28
      explained_var=0.00848 | mse_loss=0.02792
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02493 | best_loss=0.02493
Epoch 1/80: current_loss=0.02515 | best_loss=0.02493
Epoch 2/80: current_loss=0.02545 | best_loss=0.02493
Epoch 3/80: current_loss=0.02492 | best_loss=0.02492
Epoch 4/80: current_loss=0.02489 | best_loss=0.02489
Epoch 5/80: current_loss=0.02491 | best_loss=0.02489
Epoch 6/80: current_loss=0.02538 | best_loss=0.02489
Epoch 7/80: current_loss=0.02490 | best_loss=0.02489
Epoch 8/80: current_loss=0.02495 | best_loss=0.02489
Epoch 9/80: current_loss=0.02488 | best_loss=0.02488
Epoch 10/80: current_loss=0.02489 | best_loss=0.02488
Epoch 11/80: current_loss=0.02510 | best_loss=0.02488
Epoch 12/80: current_loss=0.02498 | best_loss=0.02488
Epoch 13/80: current_loss=0.02551 | best_loss=0.02488
Epoch 14/80: current_loss=0.02515 | best_loss=0.02488
Epoch 15/80: current_loss=0.02513 | best_loss=0.02488
Epoch 16/80: current_loss=0.02494 | best_loss=0.02488
Epoch 17/80: current_loss=0.02495 | best_loss=0.02488
Epoch 18/80: current_loss=0.02497 | best_loss=0.02488
Epoch 19/80: current_loss=0.02499 | best_loss=0.02488
Epoch 20/80: current_loss=0.02505 | best_loss=0.02488
Epoch 21/80: current_loss=0.02548 | best_loss=0.02488
Epoch 22/80: current_loss=0.02497 | best_loss=0.02488
Epoch 23/80: current_loss=0.02526 | best_loss=0.02488
Epoch 24/80: current_loss=0.02537 | best_loss=0.02488
Epoch 25/80: current_loss=0.02501 | best_loss=0.02488
Epoch 26/80: current_loss=0.02515 | best_loss=0.02488
Epoch 27/80: current_loss=0.02490 | best_loss=0.02488
Epoch 28/80: current_loss=0.02502 | best_loss=0.02488
Epoch 29/80: current_loss=0.02495 | best_loss=0.02488
Early Stopping at epoch 29
      explained_var=0.00265 | mse_loss=0.02515
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02826 | best_loss=0.02826
Epoch 1/80: current_loss=0.02840 | best_loss=0.02826
Epoch 2/80: current_loss=0.02737 | best_loss=0.02737
Epoch 3/80: current_loss=0.02833 | best_loss=0.02737
Epoch 4/80: current_loss=0.02781 | best_loss=0.02737
Epoch 5/80: current_loss=0.02816 | best_loss=0.02737
Epoch 6/80: current_loss=0.02907 | best_loss=0.02737
Epoch 7/80: current_loss=0.02732 | best_loss=0.02732
Epoch 8/80: current_loss=0.02747 | best_loss=0.02732
Epoch 9/80: current_loss=0.02800 | best_loss=0.02732
Epoch 10/80: current_loss=0.02849 | best_loss=0.02732
Epoch 11/80: current_loss=0.02892 | best_loss=0.02732
Epoch 12/80: current_loss=0.02885 | best_loss=0.02732
Epoch 13/80: current_loss=0.02849 | best_loss=0.02732
Epoch 14/80: current_loss=0.02785 | best_loss=0.02732
Epoch 15/80: current_loss=0.02822 | best_loss=0.02732
Epoch 16/80: current_loss=0.02777 | best_loss=0.02732
Epoch 17/80: current_loss=0.02767 | best_loss=0.02732
Epoch 18/80: current_loss=0.02745 | best_loss=0.02732
Epoch 19/80: current_loss=0.02816 | best_loss=0.02732
Epoch 20/80: current_loss=0.02806 | best_loss=0.02732
Epoch 21/80: current_loss=0.02826 | best_loss=0.02732
Epoch 22/80: current_loss=0.02739 | best_loss=0.02732
Epoch 23/80: current_loss=0.02937 | best_loss=0.02732
Epoch 24/80: current_loss=0.02746 | best_loss=0.02732
Epoch 25/80: current_loss=0.02735 | best_loss=0.02732
Epoch 26/80: current_loss=0.02780 | best_loss=0.02732
Epoch 27/80: current_loss=0.02834 | best_loss=0.02732
Early Stopping at epoch 27
      explained_var=-0.00949 | mse_loss=0.02782
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02678 | best_loss=0.02678
Epoch 1/80: current_loss=0.02672 | best_loss=0.02672
Epoch 2/80: current_loss=0.02674 | best_loss=0.02672
Epoch 3/80: current_loss=0.02683 | best_loss=0.02672
Epoch 4/80: current_loss=0.02726 | best_loss=0.02672
Epoch 5/80: current_loss=0.02690 | best_loss=0.02672
Epoch 6/80: current_loss=0.02676 | best_loss=0.02672
Epoch 7/80: current_loss=0.02726 | best_loss=0.02672
Epoch 8/80: current_loss=0.02686 | best_loss=0.02672
Epoch 9/80: current_loss=0.02683 | best_loss=0.02672
Epoch 10/80: current_loss=0.02752 | best_loss=0.02672
Epoch 11/80: current_loss=0.02677 | best_loss=0.02672
Epoch 12/80: current_loss=0.02713 | best_loss=0.02672
Epoch 13/80: current_loss=0.02756 | best_loss=0.02672
Epoch 14/80: current_loss=0.02728 | best_loss=0.02672
Epoch 15/80: current_loss=0.02679 | best_loss=0.02672
Epoch 16/80: current_loss=0.02700 | best_loss=0.02672
Epoch 17/80: current_loss=0.02698 | best_loss=0.02672
Epoch 18/80: current_loss=0.02672 | best_loss=0.02672
Epoch 19/80: current_loss=0.02691 | best_loss=0.02672
Epoch 20/80: current_loss=0.02707 | best_loss=0.02672
Epoch 21/80: current_loss=0.02711 | best_loss=0.02672
Epoch 22/80: current_loss=0.02676 | best_loss=0.02672
Epoch 23/80: current_loss=0.02680 | best_loss=0.02672
Epoch 24/80: current_loss=0.02684 | best_loss=0.02672
Epoch 25/80: current_loss=0.02686 | best_loss=0.02672
Epoch 26/80: current_loss=0.02688 | best_loss=0.02672
Epoch 27/80: current_loss=0.02679 | best_loss=0.02672
Epoch 28/80: current_loss=0.02678 | best_loss=0.02672
Epoch 29/80: current_loss=0.02679 | best_loss=0.02672
Epoch 30/80: current_loss=0.02722 | best_loss=0.02672
Epoch 31/80: current_loss=0.02678 | best_loss=0.02672
Epoch 32/80: current_loss=0.02677 | best_loss=0.02672
Epoch 33/80: current_loss=0.02685 | best_loss=0.02672
Epoch 34/80: current_loss=0.02679 | best_loss=0.02672
Epoch 35/80: current_loss=0.02679 | best_loss=0.02672
Epoch 36/80: current_loss=0.02678 | best_loss=0.02672
Epoch 37/80: current_loss=0.02677 | best_loss=0.02672
Epoch 38/80: current_loss=0.02724 | best_loss=0.02672
Early Stopping at epoch 38
      explained_var=0.00190 | mse_loss=0.02494
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02971 | best_loss=0.02971
Epoch 1/80: current_loss=0.02975 | best_loss=0.02971
Epoch 2/80: current_loss=0.02976 | best_loss=0.02971
Epoch 3/80: current_loss=0.02981 | best_loss=0.02971
Epoch 4/80: current_loss=0.02978 | best_loss=0.02971
Epoch 5/80: current_loss=0.03000 | best_loss=0.02971
Epoch 6/80: current_loss=0.02982 | best_loss=0.02971
Epoch 7/80: current_loss=0.03037 | best_loss=0.02971
Epoch 8/80: current_loss=0.02969 | best_loss=0.02969
Epoch 9/80: current_loss=0.03083 | best_loss=0.02969
Epoch 10/80: current_loss=0.03021 | best_loss=0.02969
Epoch 11/80: current_loss=0.02980 | best_loss=0.02969
Epoch 12/80: current_loss=0.02980 | best_loss=0.02969
Epoch 13/80: current_loss=0.02978 | best_loss=0.02969
Epoch 14/80: current_loss=0.02967 | best_loss=0.02967
Epoch 15/80: current_loss=0.02972 | best_loss=0.02967
Epoch 16/80: current_loss=0.03012 | best_loss=0.02967
Epoch 17/80: current_loss=0.02990 | best_loss=0.02967
Epoch 18/80: current_loss=0.02969 | best_loss=0.02967
Epoch 19/80: current_loss=0.02973 | best_loss=0.02967
Epoch 20/80: current_loss=0.02993 | best_loss=0.02967
Epoch 21/80: current_loss=0.02968 | best_loss=0.02967
Epoch 22/80: current_loss=0.02979 | best_loss=0.02967
Epoch 23/80: current_loss=0.02991 | best_loss=0.02967
Epoch 24/80: current_loss=0.02977 | best_loss=0.02967
Epoch 25/80: current_loss=0.03002 | best_loss=0.02967
Epoch 26/80: current_loss=0.02973 | best_loss=0.02967
Epoch 27/80: current_loss=0.02970 | best_loss=0.02967
Epoch 28/80: current_loss=0.02968 | best_loss=0.02967
Epoch 29/80: current_loss=0.02980 | best_loss=0.02967
Epoch 30/80: current_loss=0.02972 | best_loss=0.02967
Epoch 31/80: current_loss=0.02990 | best_loss=0.02967
Epoch 32/80: current_loss=0.02970 | best_loss=0.02967
Epoch 33/80: current_loss=0.02973 | best_loss=0.02967
Epoch 34/80: current_loss=0.02974 | best_loss=0.02967
Early Stopping at epoch 34
      explained_var=0.00248 | mse_loss=0.02869
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00121| avg_loss=0.02690
----------------------------------------------


----------------------------------------------
Params for Trial 97
{'learning_rate': 0.0001, 'weight_decay': 0.00042154386655936547, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.04179 | best_loss=0.04179
Epoch 1/80: current_loss=0.03866 | best_loss=0.03866
Epoch 2/80: current_loss=0.03631 | best_loss=0.03631
Epoch 3/80: current_loss=0.03428 | best_loss=0.03428
Epoch 4/80: current_loss=0.03299 | best_loss=0.03299
Epoch 5/80: current_loss=0.03195 | best_loss=0.03195
Epoch 6/80: current_loss=0.03108 | best_loss=0.03108
Epoch 7/80: current_loss=0.03057 | best_loss=0.03057
Epoch 8/80: current_loss=0.03032 | best_loss=0.03032
Epoch 9/80: current_loss=0.02996 | best_loss=0.02996
Epoch 10/80: current_loss=0.02952 | best_loss=0.02952
Epoch 11/80: current_loss=0.02945 | best_loss=0.02945
Epoch 12/80: current_loss=0.02932 | best_loss=0.02932
Epoch 13/80: current_loss=0.02926 | best_loss=0.02926
Epoch 14/80: current_loss=0.02923 | best_loss=0.02923
Epoch 15/80: current_loss=0.02911 | best_loss=0.02911
Epoch 16/80: current_loss=0.02915 | best_loss=0.02911
Epoch 17/80: current_loss=0.02901 | best_loss=0.02901
Epoch 18/80: current_loss=0.02907 | best_loss=0.02901
Epoch 19/80: current_loss=0.02896 | best_loss=0.02896
Epoch 20/80: current_loss=0.02895 | best_loss=0.02895
Epoch 21/80: current_loss=0.02916 | best_loss=0.02895
Epoch 22/80: current_loss=0.02897 | best_loss=0.02895
Epoch 23/80: current_loss=0.02893 | best_loss=0.02893
Epoch 24/80: current_loss=0.02892 | best_loss=0.02892
Epoch 25/80: current_loss=0.02907 | best_loss=0.02892
Epoch 26/80: current_loss=0.02897 | best_loss=0.02892
Epoch 27/80: current_loss=0.02900 | best_loss=0.02892
Epoch 28/80: current_loss=0.02904 | best_loss=0.02892
Epoch 29/80: current_loss=0.02899 | best_loss=0.02892
Epoch 30/80: current_loss=0.02895 | best_loss=0.02892
Epoch 31/80: current_loss=0.02907 | best_loss=0.02892
Epoch 32/80: current_loss=0.02902 | best_loss=0.02892
Epoch 33/80: current_loss=0.02905 | best_loss=0.02892
Epoch 34/80: current_loss=0.02928 | best_loss=0.02892
Epoch 35/80: current_loss=0.02915 | best_loss=0.02892
Epoch 36/80: current_loss=0.02903 | best_loss=0.02892
Epoch 37/80: current_loss=0.02900 | best_loss=0.02892
Epoch 38/80: current_loss=0.02896 | best_loss=0.02892
Epoch 39/80: current_loss=0.02898 | best_loss=0.02892
Epoch 40/80: current_loss=0.02912 | best_loss=0.02892
Epoch 41/80: current_loss=0.02903 | best_loss=0.02892
Epoch 42/80: current_loss=0.02904 | best_loss=0.02892
Epoch 43/80: current_loss=0.02904 | best_loss=0.02892
Epoch 44/80: current_loss=0.02904 | best_loss=0.02892
Early Stopping at epoch 44
      explained_var=0.00282 | mse_loss=0.02809

----------------------------------------------
Params for Trial 98
{'learning_rate': 0.0001, 'weight_decay': 0.000732505132290603, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.03178 | best_loss=0.03178
Epoch 1/80: current_loss=0.03062 | best_loss=0.03062
Epoch 2/80: current_loss=0.02986 | best_loss=0.02986
Epoch 3/80: current_loss=0.02967 | best_loss=0.02967
Epoch 4/80: current_loss=0.02956 | best_loss=0.02956
Epoch 5/80: current_loss=0.02901 | best_loss=0.02901
Epoch 6/80: current_loss=0.02894 | best_loss=0.02894
Epoch 7/80: current_loss=0.02897 | best_loss=0.02894
Epoch 8/80: current_loss=0.02922 | best_loss=0.02894
Epoch 9/80: current_loss=0.02889 | best_loss=0.02889
Epoch 10/80: current_loss=0.02887 | best_loss=0.02887
Epoch 11/80: current_loss=0.02887 | best_loss=0.02887
Epoch 12/80: current_loss=0.02904 | best_loss=0.02887
Epoch 13/80: current_loss=0.02887 | best_loss=0.02887
Epoch 14/80: current_loss=0.02883 | best_loss=0.02883
Epoch 15/80: current_loss=0.02901 | best_loss=0.02883
Epoch 16/80: current_loss=0.02942 | best_loss=0.02883
Epoch 17/80: current_loss=0.02950 | best_loss=0.02883
Epoch 18/80: current_loss=0.02891 | best_loss=0.02883
Epoch 19/80: current_loss=0.02890 | best_loss=0.02883
Epoch 20/80: current_loss=0.02886 | best_loss=0.02883
Epoch 21/80: current_loss=0.02893 | best_loss=0.02883
Epoch 22/80: current_loss=0.02911 | best_loss=0.02883
Epoch 23/80: current_loss=0.02922 | best_loss=0.02883
Epoch 24/80: current_loss=0.02897 | best_loss=0.02883
Epoch 25/80: current_loss=0.02909 | best_loss=0.02883
Epoch 26/80: current_loss=0.02898 | best_loss=0.02883
Epoch 27/80: current_loss=0.02949 | best_loss=0.02883
Epoch 28/80: current_loss=0.02908 | best_loss=0.02883
Epoch 29/80: current_loss=0.02926 | best_loss=0.02883
Epoch 30/80: current_loss=0.02909 | best_loss=0.02883
Epoch 31/80: current_loss=0.02901 | best_loss=0.02883
Epoch 32/80: current_loss=0.02973 | best_loss=0.02883
Epoch 33/80: current_loss=0.02965 | best_loss=0.02883
Epoch 34/80: current_loss=0.02909 | best_loss=0.02883
Early Stopping at epoch 34
      explained_var=0.00658 | mse_loss=0.02796
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02542 | best_loss=0.02542
Epoch 1/80: current_loss=0.02507 | best_loss=0.02507
Epoch 2/80: current_loss=0.02496 | best_loss=0.02496
Epoch 3/80: current_loss=0.02497 | best_loss=0.02496
Epoch 4/80: current_loss=0.02494 | best_loss=0.02494
Epoch 5/80: current_loss=0.02492 | best_loss=0.02492
Epoch 6/80: current_loss=0.02499 | best_loss=0.02492
Epoch 7/80: current_loss=0.02505 | best_loss=0.02492
Epoch 8/80: current_loss=0.02527 | best_loss=0.02492
Epoch 9/80: current_loss=0.02500 | best_loss=0.02492
Epoch 10/80: current_loss=0.02507 | best_loss=0.02492
Epoch 11/80: current_loss=0.02503 | best_loss=0.02492
Epoch 12/80: current_loss=0.02513 | best_loss=0.02492
Epoch 13/80: current_loss=0.02521 | best_loss=0.02492
Epoch 14/80: current_loss=0.02495 | best_loss=0.02492
Epoch 15/80: current_loss=0.02502 | best_loss=0.02492
Epoch 16/80: current_loss=0.02513 | best_loss=0.02492
Epoch 17/80: current_loss=0.02510 | best_loss=0.02492
Epoch 18/80: current_loss=0.02491 | best_loss=0.02491
Epoch 19/80: current_loss=0.02489 | best_loss=0.02489
Epoch 20/80: current_loss=0.02521 | best_loss=0.02489
Epoch 21/80: current_loss=0.02484 | best_loss=0.02484
Epoch 22/80: current_loss=0.02508 | best_loss=0.02484
Epoch 23/80: current_loss=0.02497 | best_loss=0.02484
Epoch 24/80: current_loss=0.02495 | best_loss=0.02484
Epoch 25/80: current_loss=0.02492 | best_loss=0.02484
Epoch 26/80: current_loss=0.02490 | best_loss=0.02484
Epoch 27/80: current_loss=0.02488 | best_loss=0.02484
Epoch 28/80: current_loss=0.02498 | best_loss=0.02484
Epoch 29/80: current_loss=0.02489 | best_loss=0.02484
Epoch 30/80: current_loss=0.02500 | best_loss=0.02484
Epoch 31/80: current_loss=0.02548 | best_loss=0.02484
Epoch 32/80: current_loss=0.02500 | best_loss=0.02484
Epoch 33/80: current_loss=0.02483 | best_loss=0.02483
Epoch 34/80: current_loss=0.02483 | best_loss=0.02483
Epoch 35/80: current_loss=0.02498 | best_loss=0.02483
Epoch 36/80: current_loss=0.02485 | best_loss=0.02483
Epoch 37/80: current_loss=0.02486 | best_loss=0.02483
Epoch 38/80: current_loss=0.02487 | best_loss=0.02483
Epoch 39/80: current_loss=0.02484 | best_loss=0.02483
Epoch 40/80: current_loss=0.02492 | best_loss=0.02483
Epoch 41/80: current_loss=0.02549 | best_loss=0.02483
Epoch 42/80: current_loss=0.02500 | best_loss=0.02483
Epoch 43/80: current_loss=0.02484 | best_loss=0.02483
Epoch 44/80: current_loss=0.02496 | best_loss=0.02483
Epoch 45/80: current_loss=0.02501 | best_loss=0.02483
Epoch 46/80: current_loss=0.02492 | best_loss=0.02483
Epoch 47/80: current_loss=0.02489 | best_loss=0.02483
Epoch 48/80: current_loss=0.02493 | best_loss=0.02483
Epoch 49/80: current_loss=0.02493 | best_loss=0.02483
Epoch 50/80: current_loss=0.02493 | best_loss=0.02483
Epoch 51/80: current_loss=0.02496 | best_loss=0.02483
Epoch 52/80: current_loss=0.02549 | best_loss=0.02483
Epoch 53/80: current_loss=0.02509 | best_loss=0.02483
Early Stopping at epoch 53
      explained_var=0.00288 | mse_loss=0.02516
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02822 | best_loss=0.02822
Epoch 1/80: current_loss=0.02874 | best_loss=0.02822
Epoch 2/80: current_loss=0.02930 | best_loss=0.02822
Epoch 3/80: current_loss=0.02770 | best_loss=0.02770
Epoch 4/80: current_loss=0.02832 | best_loss=0.02770
Epoch 5/80: current_loss=0.02884 | best_loss=0.02770
Epoch 6/80: current_loss=0.02827 | best_loss=0.02770
Epoch 7/80: current_loss=0.02730 | best_loss=0.02730
Epoch 8/80: current_loss=0.02762 | best_loss=0.02730
Epoch 9/80: current_loss=0.02773 | best_loss=0.02730
Epoch 10/80: current_loss=0.02824 | best_loss=0.02730
Epoch 11/80: current_loss=0.02758 | best_loss=0.02730
Epoch 12/80: current_loss=0.02741 | best_loss=0.02730
Epoch 13/80: current_loss=0.02737 | best_loss=0.02730
Epoch 14/80: current_loss=0.02795 | best_loss=0.02730
Epoch 15/80: current_loss=0.02853 | best_loss=0.02730
Epoch 16/80: current_loss=0.02757 | best_loss=0.02730
Epoch 17/80: current_loss=0.02729 | best_loss=0.02729
Epoch 18/80: current_loss=0.02793 | best_loss=0.02729
Epoch 19/80: current_loss=0.02803 | best_loss=0.02729
Epoch 20/80: current_loss=0.02773 | best_loss=0.02729
Epoch 21/80: current_loss=0.02757 | best_loss=0.02729
Epoch 22/80: current_loss=0.02735 | best_loss=0.02729
Epoch 23/80: current_loss=0.02740 | best_loss=0.02729
Epoch 24/80: current_loss=0.02779 | best_loss=0.02729
Epoch 25/80: current_loss=0.02752 | best_loss=0.02729
Epoch 26/80: current_loss=0.02768 | best_loss=0.02729
Epoch 27/80: current_loss=0.02740 | best_loss=0.02729
Epoch 28/80: current_loss=0.02785 | best_loss=0.02729
Epoch 29/80: current_loss=0.02772 | best_loss=0.02729
Epoch 30/80: current_loss=0.02737 | best_loss=0.02729
Epoch 31/80: current_loss=0.02774 | best_loss=0.02729
Epoch 32/80: current_loss=0.02802 | best_loss=0.02729
Epoch 33/80: current_loss=0.02767 | best_loss=0.02729
Epoch 34/80: current_loss=0.02752 | best_loss=0.02729
Epoch 35/80: current_loss=0.02764 | best_loss=0.02729
Epoch 36/80: current_loss=0.02795 | best_loss=0.02729
Epoch 37/80: current_loss=0.02817 | best_loss=0.02729
Early Stopping at epoch 37
      explained_var=-0.00494 | mse_loss=0.02773
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.02681 | best_loss=0.02681
Epoch 1/80: current_loss=0.02691 | best_loss=0.02681
Epoch 2/80: current_loss=0.02684 | best_loss=0.02681
Epoch 3/80: current_loss=0.02682 | best_loss=0.02681
Epoch 4/80: current_loss=0.02678 | best_loss=0.02678
Epoch 5/80: current_loss=0.02704 | best_loss=0.02678
Epoch 6/80: current_loss=0.02687 | best_loss=0.02678
Epoch 7/80: current_loss=0.02684 | best_loss=0.02678
Epoch 8/80: current_loss=0.02676 | best_loss=0.02676
Epoch 9/80: current_loss=0.02676 | best_loss=0.02676
Epoch 10/80: current_loss=0.02677 | best_loss=0.02676
Epoch 11/80: current_loss=0.02683 | best_loss=0.02676
Epoch 12/80: current_loss=0.02683 | best_loss=0.02676
Epoch 13/80: current_loss=0.02687 | best_loss=0.02676
Epoch 14/80: current_loss=0.02679 | best_loss=0.02676
Epoch 15/80: current_loss=0.02678 | best_loss=0.02676
Epoch 16/80: current_loss=0.02693 | best_loss=0.02676
Epoch 17/80: current_loss=0.02680 | best_loss=0.02676
Epoch 18/80: current_loss=0.02690 | best_loss=0.02676
Epoch 19/80: current_loss=0.02692 | best_loss=0.02676
Epoch 20/80: current_loss=0.02722 | best_loss=0.02676
Epoch 21/80: current_loss=0.02687 | best_loss=0.02676
Epoch 22/80: current_loss=0.02685 | best_loss=0.02676
Epoch 23/80: current_loss=0.02685 | best_loss=0.02676
Epoch 24/80: current_loss=0.02689 | best_loss=0.02676
Epoch 25/80: current_loss=0.02690 | best_loss=0.02676
Epoch 26/80: current_loss=0.02686 | best_loss=0.02676
Epoch 27/80: current_loss=0.02703 | best_loss=0.02676
Epoch 28/80: current_loss=0.02693 | best_loss=0.02676
Early Stopping at epoch 28
      explained_var=0.00034 | mse_loss=0.02497

----------------------------------------------
Params for Trial 99
{'learning_rate': 1e-05, 'weight_decay': 0.00048474671773398307, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=0.18748 | best_loss=0.18748
Epoch 1/80: current_loss=0.12629 | best_loss=0.12629
Epoch 2/80: current_loss=0.07978 | best_loss=0.07978
Epoch 3/80: current_loss=0.05109 | best_loss=0.05109
Epoch 4/80: current_loss=0.04000 | best_loss=0.04000
Epoch 5/80: current_loss=0.03782 | best_loss=0.03782
Epoch 6/80: current_loss=0.03741 | best_loss=0.03741
Epoch 7/80: current_loss=0.03692 | best_loss=0.03692
Epoch 8/80: current_loss=0.03653 | best_loss=0.03653
Epoch 9/80: current_loss=0.03610 | best_loss=0.03610
Epoch 10/80: current_loss=0.03577 | best_loss=0.03577
Epoch 11/80: current_loss=0.03543 | best_loss=0.03543
Epoch 12/80: current_loss=0.03511 | best_loss=0.03511
Epoch 13/80: current_loss=0.03475 | best_loss=0.03475
Epoch 14/80: current_loss=0.03444 | best_loss=0.03444
Epoch 15/80: current_loss=0.03413 | best_loss=0.03413
Epoch 16/80: current_loss=0.03388 | best_loss=0.03388
Epoch 17/80: current_loss=0.03361 | best_loss=0.03361
Epoch 18/80: current_loss=0.03335 | best_loss=0.03335
Epoch 19/80: current_loss=0.03311 | best_loss=0.03311
Epoch 20/80: current_loss=0.03283 | best_loss=0.03283
Epoch 21/80: current_loss=0.03265 | best_loss=0.03265
Epoch 22/80: current_loss=0.03238 | best_loss=0.03238
Epoch 23/80: current_loss=0.03221 | best_loss=0.03221
Epoch 24/80: current_loss=0.03201 | best_loss=0.03201
Epoch 25/80: current_loss=0.03182 | best_loss=0.03182
Epoch 26/80: current_loss=0.03169 | best_loss=0.03169
Epoch 27/80: current_loss=0.03162 | best_loss=0.03162
Epoch 28/80: current_loss=0.03142 | best_loss=0.03142
Epoch 29/80: current_loss=0.03130 | best_loss=0.03130
Epoch 30/80: current_loss=0.03124 | best_loss=0.03124
Epoch 31/80: current_loss=0.03110 | best_loss=0.03110
Epoch 32/80: current_loss=0.03095 | best_loss=0.03095
Epoch 33/80: current_loss=0.03082 | best_loss=0.03082
Epoch 34/80: current_loss=0.03072 | best_loss=0.03072
Epoch 35/80: current_loss=0.03063 | best_loss=0.03063
Epoch 36/80: current_loss=0.03049 | best_loss=0.03049
Epoch 37/80: current_loss=0.03047 | best_loss=0.03047
Epoch 38/80: current_loss=0.03037 | best_loss=0.03037
Epoch 39/80: current_loss=0.03030 | best_loss=0.03030
Epoch 40/80: current_loss=0.03023 | best_loss=0.03023
Epoch 41/80: current_loss=0.03010 | best_loss=0.03010
Epoch 42/80: current_loss=0.03005 | best_loss=0.03005
Epoch 43/80: current_loss=0.02999 | best_loss=0.02999
Epoch 44/80: current_loss=0.02992 | best_loss=0.02992
Epoch 45/80: current_loss=0.02989 | best_loss=0.02989
Epoch 46/80: current_loss=0.02984 | best_loss=0.02984
Epoch 47/80: current_loss=0.02976 | best_loss=0.02976
Epoch 48/80: current_loss=0.02973 | best_loss=0.02973
Epoch 49/80: current_loss=0.02967 | best_loss=0.02967
Epoch 50/80: current_loss=0.02961 | best_loss=0.02961
Epoch 51/80: current_loss=0.02956 | best_loss=0.02956
Epoch 52/80: current_loss=0.02955 | best_loss=0.02955
Epoch 53/80: current_loss=0.02960 | best_loss=0.02955
Epoch 54/80: current_loss=0.02949 | best_loss=0.02949
Epoch 55/80: current_loss=0.02946 | best_loss=0.02946
Epoch 56/80: current_loss=0.02949 | best_loss=0.02946
Epoch 57/80: current_loss=0.02947 | best_loss=0.02946
Epoch 58/80: current_loss=0.02941 | best_loss=0.02941
Epoch 59/80: current_loss=0.02942 | best_loss=0.02941
Epoch 60/80: current_loss=0.02944 | best_loss=0.02941
Epoch 61/80: current_loss=0.02943 | best_loss=0.02941
Epoch 62/80: current_loss=0.02936 | best_loss=0.02936
Epoch 63/80: current_loss=0.02934 | best_loss=0.02934
Epoch 64/80: current_loss=0.02932 | best_loss=0.02932
Epoch 65/80: current_loss=0.02932 | best_loss=0.02932
Epoch 66/80: current_loss=0.02933 | best_loss=0.02932
Epoch 67/80: current_loss=0.02930 | best_loss=0.02930
Epoch 68/80: current_loss=0.02927 | best_loss=0.02927
Epoch 69/80: current_loss=0.02926 | best_loss=0.02926
Epoch 70/80: current_loss=0.02933 | best_loss=0.02926
Epoch 71/80: current_loss=0.02923 | best_loss=0.02923
Epoch 72/80: current_loss=0.02922 | best_loss=0.02922
Epoch 73/80: current_loss=0.02921 | best_loss=0.02921
Epoch 74/80: current_loss=0.02919 | best_loss=0.02919
Epoch 75/80: current_loss=0.02916 | best_loss=0.02916
Epoch 76/80: current_loss=0.02915 | best_loss=0.02915
Epoch 77/80: current_loss=0.02914 | best_loss=0.02914
Epoch 78/80: current_loss=0.02915 | best_loss=0.02914
Epoch 79/80: current_loss=0.02917 | best_loss=0.02914
      explained_var=-0.00328 | mse_loss=0.02825
Optuna study finished, study statistics:
  Finished trials:  100
  Pruned trials:  37
  Completed trials:  63
  Best Trial:  82
  Value:  0.026824730491197896
  AVG stopping:  11
  Params: 
    learning_rate: 0.0001
    weight_decay: 9.833205478141933e-06
    n_layers: 2
    hidden_size: 128
    dropout: 0.4
----------------------------------------------

Check best params: {'learning_rate': 0.0001, 'weight_decay': 9.833205478141933e-06, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4, 'avg_epochs': 11}
--------------------------------------------------------------
Test CNN results: avg_loss=0.0307, avg_expvar=-0.0275, avg_r2score=-0.0411, avg_mae=0.1347
--------------------------------------------------------------
