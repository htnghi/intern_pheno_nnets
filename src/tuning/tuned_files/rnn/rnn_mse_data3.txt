[I 2024-01-06 02:43:00,258] A new study created in memory with name: cnn_mseloss_data3
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:43:45,972] Trial 0 finished with value: 9.062811545126952 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 9.062811545126952.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:46:51,938] Trial 1 finished with value: 11.045586531405922 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}. Best is trial 0 with value: 9.062811545126952.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:51:27,686] Trial 2 finished with value: 8.825221859039079 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:51:55,220] Trial 3 finished with value: 8.845621185794254 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:53:27,989] Trial 4 finished with value: 8.844323795980358 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:54:45,824] Trial 5 finished with value: 8.843982611468968 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:56:43,164] Trial 6 finished with value: 8.853155905429986 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:59:11,455] Trial 7 finished with value: 9.230257155957684 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:00:26,652] Trial 8 finished with value: 12.493466038097285 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:07:05,662] Trial 9 finished with value: 9.181304251856329 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:14:30,356] Trial 10 finished with value: 8.85000169842262 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0030741327184301173, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:18:40,142] Trial 11 finished with value: 8.845409050768362 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.002554434662107642, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 2 with value: 8.825221859039079.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:22:46,473] Trial 12 finished with value: 8.814384623355066 and parameters: {'learning_rate': 0.001, 'weight_decay': 8.686045909808823e-05, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 12 with value: 8.814384623355066.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:26:41,842] Trial 13 finished with value: 8.805694261360655 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.00010813129554321693, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 13 with value: 8.805694261360655.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:30:00,177] Trial 14 finished with value: 8.824723731087648 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.00012708348122822342, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 13 with value: 8.805694261360655.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:33:54,521] Trial 15 finished with value: 8.822968704409641 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0025771573441501217, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 13 with value: 8.805694261360655.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:39:07,702] Trial 16 finished with value: 8.819072107785791 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0015549447682670203, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 13 with value: 8.805694261360655.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:42:21,447] Trial 17 finished with value: 8.82117197002255 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.003599382439907362, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 13 with value: 8.805694261360655.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:45:33,803] Trial 18 finished with value: 8.78369361284282 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0014939019685960917, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 18 with value: 8.78369361284282.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:50:13,499] Trial 19 finished with value: 8.82661419800898 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0019321913361300055, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 18 with value: 8.78369361284282.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:54:25,999] Trial 20 finished with value: 8.811260720593067 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0035585075505975682, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 18 with value: 8.78369361284282.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 03:58:26,581] Trial 21 finished with value: 8.791341634162231 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.003497249185495035, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 18 with value: 8.78369361284282.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:02:54,466] Trial 22 finished with value: 8.784533510040719 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0013780298752642903, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 18 with value: 8.78369361284282.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:04:37,435] Trial 23 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:07:31,857] Trial 24 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:08:44,171] Trial 25 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:09:48,645] Trial 26 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:10:32,438] Trial 27 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:12:08,713] Trial 28 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:12:29,839] Trial 29 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:16:18,801] Trial 30 finished with value: 8.782256377799268 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0010446243951466672, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 30 with value: 8.782256377799268.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:16:50,156] Trial 31 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:20:28,037] Trial 32 finished with value: 8.797374714883203 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0029379297212884423, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 30 with value: 8.782256377799268.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:22:04,010] Trial 33 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:25:47,535] Trial 34 finished with value: 8.767803574156 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0009941751263749209, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:26:40,841] Trial 35 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:27:10,000] Trial 36 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:31:37,995] Trial 37 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:32:36,770] Trial 38 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:35:04,292] Trial 39 finished with value: 8.854174074259287 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0005049159454277711, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.5}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:36:06,117] Trial 40 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:40:06,622] Trial 41 finished with value: 8.794384010299911 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.001559753180127209, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:40:39,165] Trial 42 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:41:39,506] Trial 43 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:44:57,804] Trial 44 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:47:29,442] Trial 45 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:51:25,014] Trial 46 finished with value: 8.794644207645481 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.002123023848387107, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:51:50,281] Trial 47 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:52:25,209] Trial 48 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:54:49,634] Trial 49 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:55:31,702] Trial 50 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:58:10,455] Trial 51 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 04:58:50,200] Trial 52 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:00:07,004] Trial 53 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:02:24,548] Trial 54 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:06:40,644] Trial 55 finished with value: 8.812890311804782 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0024399139710516867, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:09:06,322] Trial 56 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:11:26,200] Trial 57 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:13:02,157] Trial 58 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:16:09,600] Trial 59 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:16:46,911] Trial 60 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:17:19,438] Trial 61 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:19:19,435] Trial 62 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:20:02,703] Trial 63 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:23:04,095] Trial 64 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:26:23,480] Trial 65 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:28:33,714] Trial 66 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:29:46,189] Trial 67 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:31:02,403] Trial 68 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:32:27,593] Trial 69 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:33:48,033] Trial 70 finished with value: 8.812028456126567 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0006073727406846086, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:34:46,880] Trial 71 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:36:44,672] Trial 72 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:37:58,124] Trial 73 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:41:44,012] Trial 74 finished with value: 8.788836884187742 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0014484023816759754, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:43:21,362] Trial 75 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:44:57,429] Trial 76 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:45:42,585] Trial 77 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:49:32,032] Trial 78 finished with value: 8.80024595562406 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0007501706541449324, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.5}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:50:45,719] Trial 79 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:51:20,782] Trial 80 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:55:15,309] Trial 81 finished with value: 8.805541881183942 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.002801919826550663, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:56:37,109] Trial 82 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 05:58:47,315] Trial 83 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:00:02,988] Trial 84 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:03:28,741] Trial 85 finished with value: 8.80165258984114 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.00089931251399769, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:07:01,658] Trial 86 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:08:01,937] Trial 87 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:10:42,688] Trial 88 finished with value: 8.810115052565541 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0011360581648137958, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:13:31,879] Trial 89 finished with value: 8.814534563091339 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.005689163299418702, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:16:52,105] Trial 90 finished with value: 8.783023501224065 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0021209309251191827, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 34 with value: 8.767803574156.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:17:51,124] Trial 91 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:20:59,572] Trial 92 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:21:39,373] Trial 93 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:22:50,353] Trial 94 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:24:26,564] Trial 95 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:28:31,132] Trial 96 finished with value: 8.756229876085499 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0009540954229792735, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 96 with value: 8.756229876085499.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:29:44,746] Trial 97 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:32:41,806] Trial 98 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 06:33:07,169] Trial 99 pruned. 
GPU CUDA available, using GPU for training the models.
-----------------------------------------------
Input arguments: 
   + data_dir: /home/ra56kop/nnets_genomic_prediction/src
   + model: RNN
   + tuned: 1
   + minmax_scale: 0
   + standa_scale: 0
   + pca_fitting: 0
   + dataset: pheno_3
   + gpucuda: 2
   + data_variants: [0, 0, 0, 3]
-----------------------------------------------

---------------------------------------------------------
Tuning RNN with dataset pheno-3
---------------------------------------------------------


----------------------------------------------
Params for Trial 0
{'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=77.30633 | best_loss=77.30633
Epoch 1/80: current_loss=74.34553 | best_loss=74.34553
Epoch 2/80: current_loss=69.60848 | best_loss=69.60848
Epoch 3/80: current_loss=62.81094 | best_loss=62.81094
Epoch 4/80: current_loss=56.10222 | best_loss=56.10222
Epoch 5/80: current_loss=50.69986 | best_loss=50.69986
Epoch 6/80: current_loss=46.64743 | best_loss=46.64743
Epoch 7/80: current_loss=43.42024 | best_loss=43.42024
Epoch 8/80: current_loss=40.70963 | best_loss=40.70963
Epoch 9/80: current_loss=38.35160 | best_loss=38.35160
Epoch 10/80: current_loss=36.28873 | best_loss=36.28873
Epoch 11/80: current_loss=34.39463 | best_loss=34.39463
Epoch 12/80: current_loss=32.68357 | best_loss=32.68357
Epoch 13/80: current_loss=31.12015 | best_loss=31.12015
Epoch 14/80: current_loss=29.67688 | best_loss=29.67688
Epoch 15/80: current_loss=28.32262 | best_loss=28.32262
Epoch 16/80: current_loss=27.07108 | best_loss=27.07108
Epoch 17/80: current_loss=25.91366 | best_loss=25.91366
Epoch 18/80: current_loss=24.82898 | best_loss=24.82898
Epoch 19/80: current_loss=23.80359 | best_loss=23.80359
Epoch 20/80: current_loss=22.86617 | best_loss=22.86617
Epoch 21/80: current_loss=21.96731 | best_loss=21.96731
Epoch 22/80: current_loss=21.13205 | best_loss=21.13205
Epoch 23/80: current_loss=20.33535 | best_loss=20.33535
Epoch 24/80: current_loss=19.61517 | best_loss=19.61517
Epoch 25/80: current_loss=18.94257 | best_loss=18.94257
Epoch 26/80: current_loss=18.31158 | best_loss=18.31158
Epoch 27/80: current_loss=17.71979 | best_loss=17.71979
Epoch 28/80: current_loss=17.16726 | best_loss=17.16726
Epoch 29/80: current_loss=16.65887 | best_loss=16.65887
Epoch 30/80: current_loss=16.17888 | best_loss=16.17888
Epoch 31/80: current_loss=15.73528 | best_loss=15.73528
Epoch 32/80: current_loss=15.31531 | best_loss=15.31531
Epoch 33/80: current_loss=14.93230 | best_loss=14.93230
Epoch 34/80: current_loss=14.56740 | best_loss=14.56740
Epoch 35/80: current_loss=14.22758 | best_loss=14.22758
Epoch 36/80: current_loss=13.91954 | best_loss=13.91954
Epoch 37/80: current_loss=13.63763 | best_loss=13.63763
Epoch 38/80: current_loss=13.35862 | best_loss=13.35862
Epoch 39/80: current_loss=13.12055 | best_loss=13.12055
Epoch 40/80: current_loss=12.89109 | best_loss=12.89109
Epoch 41/80: current_loss=12.69313 | best_loss=12.69313
Epoch 42/80: current_loss=12.49276 | best_loss=12.49276
Epoch 43/80: current_loss=12.31399 | best_loss=12.31399
Epoch 44/80: current_loss=12.15347 | best_loss=12.15347
Epoch 45/80: current_loss=12.00669 | best_loss=12.00669
Epoch 46/80: current_loss=11.86781 | best_loss=11.86781
Epoch 47/80: current_loss=11.74682 | best_loss=11.74682
Epoch 48/80: current_loss=11.63950 | best_loss=11.63950
Epoch 49/80: current_loss=11.53359 | best_loss=11.53359
Epoch 50/80: current_loss=11.43610 | best_loss=11.43610
Epoch 51/80: current_loss=11.35084 | best_loss=11.35084
Epoch 52/80: current_loss=11.26743 | best_loss=11.26743
Epoch 53/80: current_loss=11.19950 | best_loss=11.19950
Epoch 54/80: current_loss=11.13299 | best_loss=11.13299
Epoch 55/80: current_loss=11.07785 | best_loss=11.07785
Epoch 56/80: current_loss=11.03073 | best_loss=11.03073
Epoch 57/80: current_loss=10.98397 | best_loss=10.98397
Epoch 58/80: current_loss=10.94284 | best_loss=10.94284
Epoch 59/80: current_loss=10.89879 | best_loss=10.89879
Epoch 60/80: current_loss=10.85803 | best_loss=10.85803
Epoch 61/80: current_loss=10.82885 | best_loss=10.82885
Epoch 62/80: current_loss=10.79573 | best_loss=10.79573
Epoch 63/80: current_loss=10.76822 | best_loss=10.76822
Epoch 64/80: current_loss=10.73799 | best_loss=10.73799
Epoch 65/80: current_loss=10.71198 | best_loss=10.71198
Epoch 66/80: current_loss=10.69016 | best_loss=10.69016
Epoch 67/80: current_loss=10.66773 | best_loss=10.66773
Epoch 68/80: current_loss=10.64995 | best_loss=10.64995
Epoch 69/80: current_loss=10.63226 | best_loss=10.63226
Epoch 70/80: current_loss=10.61641 | best_loss=10.61641
Epoch 71/80: current_loss=10.60038 | best_loss=10.60038
Epoch 72/80: current_loss=10.58829 | best_loss=10.58829
Epoch 73/80: current_loss=10.57620 | best_loss=10.57620
Epoch 74/80: current_loss=10.56098 | best_loss=10.56098
Epoch 75/80: current_loss=10.54936 | best_loss=10.54936
Epoch 76/80: current_loss=10.53843 | best_loss=10.53843
Epoch 77/80: current_loss=10.52754 | best_loss=10.52754
Epoch 78/80: current_loss=10.51933 | best_loss=10.51933
Epoch 79/80: current_loss=10.50733 | best_loss=10.50733
      explained_var=-0.04373 | mse_loss=10.02539
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.20333 | best_loss=9.20333
Epoch 1/80: current_loss=9.18483 | best_loss=9.18483
Epoch 2/80: current_loss=9.16740 | best_loss=9.16740
Epoch 3/80: current_loss=9.14929 | best_loss=9.14929
Epoch 4/80: current_loss=9.12806 | best_loss=9.12806
Epoch 5/80: current_loss=9.11648 | best_loss=9.11648
Epoch 6/80: current_loss=9.09356 | best_loss=9.09356
Epoch 7/80: current_loss=9.08460 | best_loss=9.08460
Epoch 8/80: current_loss=9.07430 | best_loss=9.07430
Epoch 9/80: current_loss=9.06345 | best_loss=9.06345
Epoch 10/80: current_loss=9.06081 | best_loss=9.06081
Epoch 11/80: current_loss=9.04464 | best_loss=9.04464
Epoch 12/80: current_loss=9.03372 | best_loss=9.03372
Epoch 13/80: current_loss=9.02527 | best_loss=9.02527
Epoch 14/80: current_loss=9.00457 | best_loss=9.00457
Epoch 15/80: current_loss=8.99048 | best_loss=8.99048
Epoch 16/80: current_loss=8.98764 | best_loss=8.98764
Epoch 17/80: current_loss=8.96936 | best_loss=8.96936
Epoch 18/80: current_loss=8.95778 | best_loss=8.95778
Epoch 19/80: current_loss=8.95971 | best_loss=8.95778
Epoch 20/80: current_loss=8.94554 | best_loss=8.94554
Epoch 21/80: current_loss=8.93892 | best_loss=8.93892
Epoch 22/80: current_loss=8.91907 | best_loss=8.91907
Epoch 23/80: current_loss=8.90486 | best_loss=8.90486
Epoch 24/80: current_loss=8.89697 | best_loss=8.89697
Epoch 25/80: current_loss=8.88743 | best_loss=8.88743
Epoch 26/80: current_loss=8.87455 | best_loss=8.87455
Epoch 27/80: current_loss=8.86481 | best_loss=8.86481
Epoch 28/80: current_loss=8.86185 | best_loss=8.86185
Epoch 29/80: current_loss=8.85082 | best_loss=8.85082
Epoch 30/80: current_loss=8.84824 | best_loss=8.84824
Epoch 31/80: current_loss=8.83789 | best_loss=8.83789
Epoch 32/80: current_loss=8.83504 | best_loss=8.83504
Epoch 33/80: current_loss=8.82940 | best_loss=8.82940
Epoch 34/80: current_loss=8.81595 | best_loss=8.81595
Epoch 35/80: current_loss=8.81021 | best_loss=8.81021
Epoch 36/80: current_loss=8.79979 | best_loss=8.79979
Epoch 37/80: current_loss=8.79565 | best_loss=8.79565
Epoch 38/80: current_loss=8.78711 | best_loss=8.78711
Epoch 39/80: current_loss=8.79148 | best_loss=8.78711
Epoch 40/80: current_loss=8.79108 | best_loss=8.78711
Epoch 41/80: current_loss=8.77906 | best_loss=8.77906
Epoch 42/80: current_loss=8.76886 | best_loss=8.76886
Epoch 43/80: current_loss=8.76974 | best_loss=8.76886
Epoch 44/80: current_loss=8.76432 | best_loss=8.76432
Epoch 45/80: current_loss=8.75945 | best_loss=8.75945
Epoch 46/80: current_loss=8.74466 | best_loss=8.74466
Epoch 47/80: current_loss=8.73952 | best_loss=8.73952
Epoch 48/80: current_loss=8.74312 | best_loss=8.73952
Epoch 49/80: current_loss=8.75602 | best_loss=8.73952
Epoch 50/80: current_loss=8.76315 | best_loss=8.73952
Epoch 51/80: current_loss=8.75141 | best_loss=8.73952
Epoch 52/80: current_loss=8.75341 | best_loss=8.73952
Epoch 53/80: current_loss=8.74476 | best_loss=8.73952
Epoch 54/80: current_loss=8.75675 | best_loss=8.73952
Epoch 55/80: current_loss=8.74660 | best_loss=8.73952
Epoch 56/80: current_loss=8.75219 | best_loss=8.73952
Epoch 57/80: current_loss=8.73427 | best_loss=8.73427
Epoch 58/80: current_loss=8.72649 | best_loss=8.72649
Epoch 59/80: current_loss=8.73312 | best_loss=8.72649
Epoch 60/80: current_loss=8.70872 | best_loss=8.70872
Epoch 61/80: current_loss=8.69532 | best_loss=8.69532
Epoch 62/80: current_loss=8.69683 | best_loss=8.69532
Epoch 63/80: current_loss=8.69265 | best_loss=8.69265
Epoch 64/80: current_loss=8.68937 | best_loss=8.68937
Epoch 65/80: current_loss=8.68797 | best_loss=8.68797
Epoch 66/80: current_loss=8.68613 | best_loss=8.68613
Epoch 67/80: current_loss=8.69748 | best_loss=8.68613
Epoch 68/80: current_loss=8.68212 | best_loss=8.68212
Epoch 69/80: current_loss=8.67078 | best_loss=8.67078
Epoch 70/80: current_loss=8.66545 | best_loss=8.66545
Epoch 71/80: current_loss=8.65837 | best_loss=8.65837
Epoch 72/80: current_loss=8.65640 | best_loss=8.65640
Epoch 73/80: current_loss=8.68174 | best_loss=8.65640
Epoch 74/80: current_loss=8.68181 | best_loss=8.65640
Epoch 75/80: current_loss=8.67849 | best_loss=8.65640
Epoch 76/80: current_loss=8.66866 | best_loss=8.65640
Epoch 77/80: current_loss=8.66038 | best_loss=8.65640
Epoch 78/80: current_loss=8.64128 | best_loss=8.64128
Epoch 79/80: current_loss=8.64437 | best_loss=8.64128
      explained_var=-0.00662 | mse_loss=8.72702
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.02531 | best_loss=10.02531
Epoch 1/80: current_loss=10.02299 | best_loss=10.02299
Epoch 2/80: current_loss=10.02651 | best_loss=10.02299
Epoch 3/80: current_loss=10.02350 | best_loss=10.02299
Epoch 4/80: current_loss=10.01196 | best_loss=10.01196
Epoch 5/80: current_loss=10.02132 | best_loss=10.01196
Epoch 6/80: current_loss=10.04328 | best_loss=10.01196
Epoch 7/80: current_loss=10.02678 | best_loss=10.01196
Epoch 8/80: current_loss=10.01864 | best_loss=10.01196
Epoch 9/80: current_loss=10.00536 | best_loss=10.00536
Epoch 10/80: current_loss=9.98373 | best_loss=9.98373
Epoch 11/80: current_loss=9.98350 | best_loss=9.98350
Epoch 12/80: current_loss=10.00453 | best_loss=9.98350
Epoch 13/80: current_loss=9.99348 | best_loss=9.98350
Epoch 14/80: current_loss=10.00680 | best_loss=9.98350
Epoch 15/80: current_loss=10.00225 | best_loss=9.98350
Epoch 16/80: current_loss=9.99788 | best_loss=9.98350
Epoch 17/80: current_loss=9.98138 | best_loss=9.98138
Epoch 18/80: current_loss=9.95216 | best_loss=9.95216
Epoch 19/80: current_loss=9.93664 | best_loss=9.93664
Epoch 20/80: current_loss=9.93335 | best_loss=9.93335
Epoch 21/80: current_loss=9.96139 | best_loss=9.93335
Epoch 22/80: current_loss=9.95847 | best_loss=9.93335
Epoch 23/80: current_loss=9.95515 | best_loss=9.93335
Epoch 24/80: current_loss=9.93121 | best_loss=9.93121
Epoch 25/80: current_loss=9.94651 | best_loss=9.93121
Epoch 26/80: current_loss=9.95201 | best_loss=9.93121
Epoch 27/80: current_loss=9.96794 | best_loss=9.93121
Epoch 28/80: current_loss=9.97231 | best_loss=9.93121
Epoch 29/80: current_loss=9.94207 | best_loss=9.93121
Epoch 30/80: current_loss=9.93868 | best_loss=9.93121
Epoch 31/80: current_loss=9.91930 | best_loss=9.91930
Epoch 32/80: current_loss=9.90176 | best_loss=9.90176
Epoch 33/80: current_loss=9.89291 | best_loss=9.89291
Epoch 34/80: current_loss=9.90334 | best_loss=9.89291
Epoch 35/80: current_loss=9.89195 | best_loss=9.89195
Epoch 36/80: current_loss=9.90758 | best_loss=9.89195
Epoch 37/80: current_loss=9.89247 | best_loss=9.89195
Epoch 38/80: current_loss=9.87197 | best_loss=9.87197
Epoch 39/80: current_loss=9.89451 | best_loss=9.87197
Epoch 40/80: current_loss=9.89401 | best_loss=9.87197
Epoch 41/80: current_loss=9.88003 | best_loss=9.87197
Epoch 42/80: current_loss=9.93007 | best_loss=9.87197
Epoch 43/80: current_loss=9.87878 | best_loss=9.87197
Epoch 44/80: current_loss=9.87139 | best_loss=9.87139
Epoch 45/80: current_loss=9.85245 | best_loss=9.85245
Epoch 46/80: current_loss=9.86731 | best_loss=9.85245
Epoch 47/80: current_loss=9.89365 | best_loss=9.85245
Epoch 48/80: current_loss=9.88129 | best_loss=9.85245
Epoch 49/80: current_loss=9.86598 | best_loss=9.85245
Epoch 50/80: current_loss=9.87493 | best_loss=9.85245
Epoch 51/80: current_loss=9.87397 | best_loss=9.85245
Epoch 52/80: current_loss=9.85966 | best_loss=9.85245
Epoch 53/80: current_loss=9.86869 | best_loss=9.85245
Epoch 54/80: current_loss=9.85120 | best_loss=9.85120
Epoch 55/80: current_loss=9.83368 | best_loss=9.83368
Epoch 56/80: current_loss=9.81086 | best_loss=9.81086
Epoch 57/80: current_loss=9.80098 | best_loss=9.80098
Epoch 58/80: current_loss=9.85125 | best_loss=9.80098
Epoch 59/80: current_loss=9.83146 | best_loss=9.80098
Epoch 60/80: current_loss=9.85877 | best_loss=9.80098
Epoch 61/80: current_loss=9.85212 | best_loss=9.80098
Epoch 62/80: current_loss=9.84023 | best_loss=9.80098
Epoch 63/80: current_loss=9.84249 | best_loss=9.80098
Epoch 64/80: current_loss=9.86895 | best_loss=9.80098
Epoch 65/80: current_loss=9.84690 | best_loss=9.80098
Epoch 66/80: current_loss=9.83401 | best_loss=9.80098
Epoch 67/80: current_loss=9.82083 | best_loss=9.80098
Epoch 68/80: current_loss=9.82300 | best_loss=9.80098
Epoch 69/80: current_loss=9.80713 | best_loss=9.80098
Epoch 70/80: current_loss=9.83628 | best_loss=9.80098
Epoch 71/80: current_loss=9.80267 | best_loss=9.80098
Epoch 72/80: current_loss=9.80028 | best_loss=9.80028
Epoch 73/80: current_loss=9.82073 | best_loss=9.80028
Epoch 74/80: current_loss=9.82133 | best_loss=9.80028
Epoch 75/80: current_loss=9.81247 | best_loss=9.80028
Epoch 76/80: current_loss=9.78299 | best_loss=9.78299
Epoch 77/80: current_loss=9.79433 | best_loss=9.78299
Epoch 78/80: current_loss=9.77646 | best_loss=9.77646
Epoch 79/80: current_loss=9.76905 | best_loss=9.76905
      explained_var=-0.02534 | mse_loss=9.96745
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.91338 | best_loss=8.91338
Epoch 1/80: current_loss=8.91978 | best_loss=8.91338
Epoch 2/80: current_loss=8.91508 | best_loss=8.91338
Epoch 3/80: current_loss=8.90960 | best_loss=8.90960
Epoch 4/80: current_loss=8.90350 | best_loss=8.90350
Epoch 5/80: current_loss=8.90219 | best_loss=8.90219
Epoch 6/80: current_loss=8.90295 | best_loss=8.90219
Epoch 7/80: current_loss=8.90298 | best_loss=8.90219
Epoch 8/80: current_loss=8.90314 | best_loss=8.90219
Epoch 9/80: current_loss=8.90390 | best_loss=8.90219
Epoch 10/80: current_loss=8.90800 | best_loss=8.90219
Epoch 11/80: current_loss=8.90758 | best_loss=8.90219
Epoch 12/80: current_loss=8.90690 | best_loss=8.90219
Epoch 13/80: current_loss=8.91000 | best_loss=8.90219
Epoch 14/80: current_loss=8.90704 | best_loss=8.90219
Epoch 15/80: current_loss=8.90873 | best_loss=8.90219
Epoch 16/80: current_loss=8.90964 | best_loss=8.90219
Epoch 17/80: current_loss=8.91042 | best_loss=8.90219
Epoch 18/80: current_loss=8.91066 | best_loss=8.90219
Epoch 19/80: current_loss=8.91470 | best_loss=8.90219
Epoch 20/80: current_loss=8.90758 | best_loss=8.90219
Epoch 21/80: current_loss=8.91397 | best_loss=8.90219
Epoch 22/80: current_loss=8.91572 | best_loss=8.90219
Epoch 23/80: current_loss=8.91385 | best_loss=8.90219
Epoch 24/80: current_loss=8.91042 | best_loss=8.90219
Epoch 25/80: current_loss=8.90989 | best_loss=8.90219
Early Stopping at epoch 25
      explained_var=0.00185 | mse_loss=8.35046
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.53016 | best_loss=8.53016
Epoch 1/80: current_loss=8.52500 | best_loss=8.52500
Epoch 2/80: current_loss=8.53639 | best_loss=8.52500
Epoch 3/80: current_loss=8.54437 | best_loss=8.52500
Epoch 4/80: current_loss=8.53718 | best_loss=8.52500
Epoch 5/80: current_loss=8.55372 | best_loss=8.52500
Epoch 6/80: current_loss=8.54901 | best_loss=8.52500
Epoch 7/80: current_loss=8.55578 | best_loss=8.52500
Epoch 8/80: current_loss=8.54189 | best_loss=8.52500
Epoch 9/80: current_loss=8.53943 | best_loss=8.52500
Epoch 10/80: current_loss=8.53910 | best_loss=8.52500
Epoch 11/80: current_loss=8.53807 | best_loss=8.52500
Epoch 12/80: current_loss=8.53575 | best_loss=8.52500
Epoch 13/80: current_loss=8.55022 | best_loss=8.52500
Epoch 14/80: current_loss=8.53984 | best_loss=8.52500
Epoch 15/80: current_loss=8.55099 | best_loss=8.52500
Epoch 16/80: current_loss=8.53728 | best_loss=8.52500
Epoch 17/80: current_loss=8.54178 | best_loss=8.52500
Epoch 18/80: current_loss=8.54212 | best_loss=8.52500
Epoch 19/80: current_loss=8.54237 | best_loss=8.52500
Epoch 20/80: current_loss=8.54987 | best_loss=8.52500
Epoch 21/80: current_loss=8.55612 | best_loss=8.52500
Early Stopping at epoch 21
      explained_var=-0.00202 | mse_loss=8.24374
----------------------------------------------
Average early_stopping_point: 49| avg_exp_var=-0.01517| avg_loss=9.06281
----------------------------------------------


----------------------------------------------
Params for Trial 1
{'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=79.73940 | best_loss=79.73940
Epoch 1/80: current_loss=79.06303 | best_loss=79.06303
Epoch 2/80: current_loss=78.37659 | best_loss=78.37659
Epoch 3/80: current_loss=77.66004 | best_loss=77.66004
Epoch 4/80: current_loss=76.88874 | best_loss=76.88874
Epoch 5/80: current_loss=76.04156 | best_loss=76.04156
Epoch 6/80: current_loss=75.08352 | best_loss=75.08352
Epoch 7/80: current_loss=73.97340 | best_loss=73.97340
Epoch 8/80: current_loss=72.67223 | best_loss=72.67223
Epoch 9/80: current_loss=71.09552 | best_loss=71.09552
Epoch 10/80: current_loss=69.22120 | best_loss=69.22120
Epoch 11/80: current_loss=67.04227 | best_loss=67.04227
Epoch 12/80: current_loss=64.63182 | best_loss=64.63182
Epoch 13/80: current_loss=62.12871 | best_loss=62.12871
Epoch 14/80: current_loss=59.63905 | best_loss=59.63905
Epoch 15/80: current_loss=57.21548 | best_loss=57.21548
Epoch 16/80: current_loss=54.91328 | best_loss=54.91328
Epoch 17/80: current_loss=52.75082 | best_loss=52.75082
Epoch 18/80: current_loss=50.73700 | best_loss=50.73700
Epoch 19/80: current_loss=48.88269 | best_loss=48.88269
Epoch 20/80: current_loss=47.16693 | best_loss=47.16693
Epoch 21/80: current_loss=45.61973 | best_loss=45.61973
Epoch 22/80: current_loss=44.19433 | best_loss=44.19433
Epoch 23/80: current_loss=42.88966 | best_loss=42.88966
Epoch 24/80: current_loss=41.69525 | best_loss=41.69525
Epoch 25/80: current_loss=40.59247 | best_loss=40.59247
Epoch 26/80: current_loss=39.58079 | best_loss=39.58079
Epoch 27/80: current_loss=38.64986 | best_loss=38.64986
Epoch 28/80: current_loss=37.78231 | best_loss=37.78231
Epoch 29/80: current_loss=36.95996 | best_loss=36.95996
Epoch 30/80: current_loss=36.19383 | best_loss=36.19383
Epoch 31/80: current_loss=35.47452 | best_loss=35.47452
Epoch 32/80: current_loss=34.80277 | best_loss=34.80277
Epoch 33/80: current_loss=34.16669 | best_loss=34.16669
Epoch 34/80: current_loss=33.55610 | best_loss=33.55610
Epoch 35/80: current_loss=32.98282 | best_loss=32.98282
Epoch 36/80: current_loss=32.43018 | best_loss=32.43018
Epoch 37/80: current_loss=31.90052 | best_loss=31.90052
Epoch 38/80: current_loss=31.39514 | best_loss=31.39514
Epoch 39/80: current_loss=30.90454 | best_loss=30.90454
Epoch 40/80: current_loss=30.43443 | best_loss=30.43443
Epoch 41/80: current_loss=29.97888 | best_loss=29.97888
Epoch 42/80: current_loss=29.54358 | best_loss=29.54358
Epoch 43/80: current_loss=29.12467 | best_loss=29.12467
Epoch 44/80: current_loss=28.70873 | best_loss=28.70873
Epoch 45/80: current_loss=28.31349 | best_loss=28.31349
Epoch 46/80: current_loss=27.92533 | best_loss=27.92533
Epoch 47/80: current_loss=27.54934 | best_loss=27.54934
Epoch 48/80: current_loss=27.18173 | best_loss=27.18173
Epoch 49/80: current_loss=26.83120 | best_loss=26.83120
Epoch 50/80: current_loss=26.49237 | best_loss=26.49237
Epoch 51/80: current_loss=26.15981 | best_loss=26.15981
Epoch 52/80: current_loss=25.83675 | best_loss=25.83675
Epoch 53/80: current_loss=25.52619 | best_loss=25.52619
Epoch 54/80: current_loss=25.22211 | best_loss=25.22211
Epoch 55/80: current_loss=24.92587 | best_loss=24.92587
Epoch 56/80: current_loss=24.63930 | best_loss=24.63930
Epoch 57/80: current_loss=24.35838 | best_loss=24.35838
Epoch 58/80: current_loss=24.08658 | best_loss=24.08658
Epoch 59/80: current_loss=23.82220 | best_loss=23.82220
Epoch 60/80: current_loss=23.56497 | best_loss=23.56497
Epoch 61/80: current_loss=23.31370 | best_loss=23.31370
Epoch 62/80: current_loss=23.06815 | best_loss=23.06815
Epoch 63/80: current_loss=22.82521 | best_loss=22.82521
Epoch 64/80: current_loss=22.59111 | best_loss=22.59111
Epoch 65/80: current_loss=22.36260 | best_loss=22.36260
Epoch 66/80: current_loss=22.13164 | best_loss=22.13164
Epoch 67/80: current_loss=21.90885 | best_loss=21.90885
Epoch 68/80: current_loss=21.69606 | best_loss=21.69606
Epoch 69/80: current_loss=21.48248 | best_loss=21.48248
Epoch 70/80: current_loss=21.27692 | best_loss=21.27692
Epoch 71/80: current_loss=21.06938 | best_loss=21.06938
Epoch 72/80: current_loss=20.86999 | best_loss=20.86999
Epoch 73/80: current_loss=20.67392 | best_loss=20.67392
Epoch 74/80: current_loss=20.48079 | best_loss=20.48079
Epoch 75/80: current_loss=20.28852 | best_loss=20.28852
Epoch 76/80: current_loss=20.10218 | best_loss=20.10218
Epoch 77/80: current_loss=19.91919 | best_loss=19.91919
Epoch 78/80: current_loss=19.73950 | best_loss=19.73950
Epoch 79/80: current_loss=19.56107 | best_loss=19.56107
      explained_var=-0.03219 | mse_loss=18.63654
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=19.10887 | best_loss=19.10887
Epoch 1/80: current_loss=18.84496 | best_loss=18.84496
Epoch 2/80: current_loss=18.59958 | best_loss=18.59958
Epoch 3/80: current_loss=18.36216 | best_loss=18.36216
Epoch 4/80: current_loss=18.13623 | best_loss=18.13623
Epoch 5/80: current_loss=17.91230 | best_loss=17.91230
Epoch 6/80: current_loss=17.70144 | best_loss=17.70144
Epoch 7/80: current_loss=17.49875 | best_loss=17.49875
Epoch 8/80: current_loss=17.30662 | best_loss=17.30662
Epoch 9/80: current_loss=17.11096 | best_loss=17.11096
Epoch 10/80: current_loss=16.92974 | best_loss=16.92974
Epoch 11/80: current_loss=16.74719 | best_loss=16.74719
Epoch 12/80: current_loss=16.56909 | best_loss=16.56909
Epoch 13/80: current_loss=16.39379 | best_loss=16.39379
Epoch 14/80: current_loss=16.22198 | best_loss=16.22198
Epoch 15/80: current_loss=16.05669 | best_loss=16.05669
Epoch 16/80: current_loss=15.89432 | best_loss=15.89432
Epoch 17/80: current_loss=15.73443 | best_loss=15.73443
Epoch 18/80: current_loss=15.57964 | best_loss=15.57964
Epoch 19/80: current_loss=15.42377 | best_loss=15.42377
Epoch 20/80: current_loss=15.27405 | best_loss=15.27405
Epoch 21/80: current_loss=15.12520 | best_loss=15.12520
Epoch 22/80: current_loss=14.98048 | best_loss=14.98048
Epoch 23/80: current_loss=14.83647 | best_loss=14.83647
Epoch 24/80: current_loss=14.69586 | best_loss=14.69586
Epoch 25/80: current_loss=14.55445 | best_loss=14.55445
Epoch 26/80: current_loss=14.41971 | best_loss=14.41971
Epoch 27/80: current_loss=14.28733 | best_loss=14.28733
Epoch 28/80: current_loss=14.15905 | best_loss=14.15905
Epoch 29/80: current_loss=14.03215 | best_loss=14.03215
Epoch 30/80: current_loss=13.90508 | best_loss=13.90508
Epoch 31/80: current_loss=13.78101 | best_loss=13.78101
Epoch 32/80: current_loss=13.66162 | best_loss=13.66162
Epoch 33/80: current_loss=13.54214 | best_loss=13.54214
Epoch 34/80: current_loss=13.42443 | best_loss=13.42443
Epoch 35/80: current_loss=13.31200 | best_loss=13.31200
Epoch 36/80: current_loss=13.19911 | best_loss=13.19911
Epoch 37/80: current_loss=13.09076 | best_loss=13.09076
Epoch 38/80: current_loss=12.98165 | best_loss=12.98165
Epoch 39/80: current_loss=12.87621 | best_loss=12.87621
Epoch 40/80: current_loss=12.77202 | best_loss=12.77202
Epoch 41/80: current_loss=12.66915 | best_loss=12.66915
Epoch 42/80: current_loss=12.56822 | best_loss=12.56822
Epoch 43/80: current_loss=12.47117 | best_loss=12.47117
Epoch 44/80: current_loss=12.37639 | best_loss=12.37639
Epoch 45/80: current_loss=12.28020 | best_loss=12.28020
Epoch 46/80: current_loss=12.18621 | best_loss=12.18621
Epoch 47/80: current_loss=12.09599 | best_loss=12.09599
Epoch 48/80: current_loss=12.00441 | best_loss=12.00441
Epoch 49/80: current_loss=11.91804 | best_loss=11.91804
Epoch 50/80: current_loss=11.82918 | best_loss=11.82918
Epoch 51/80: current_loss=11.74792 | best_loss=11.74792
Epoch 52/80: current_loss=11.66214 | best_loss=11.66214
Epoch 53/80: current_loss=11.58141 | best_loss=11.58141
Epoch 54/80: current_loss=11.50204 | best_loss=11.50204
Epoch 55/80: current_loss=11.42004 | best_loss=11.42004
Epoch 56/80: current_loss=11.34410 | best_loss=11.34410
Epoch 57/80: current_loss=11.26935 | best_loss=11.26935
Epoch 58/80: current_loss=11.19551 | best_loss=11.19551
Epoch 59/80: current_loss=11.12235 | best_loss=11.12235
Epoch 60/80: current_loss=11.05018 | best_loss=11.05018
Epoch 61/80: current_loss=10.97989 | best_loss=10.97989
Epoch 62/80: current_loss=10.91305 | best_loss=10.91305
Epoch 63/80: current_loss=10.84695 | best_loss=10.84695
Epoch 64/80: current_loss=10.78005 | best_loss=10.78005
Epoch 65/80: current_loss=10.71630 | best_loss=10.71630
Epoch 66/80: current_loss=10.65372 | best_loss=10.65372
Epoch 67/80: current_loss=10.59280 | best_loss=10.59280
Epoch 68/80: current_loss=10.53248 | best_loss=10.53248
Epoch 69/80: current_loss=10.47168 | best_loss=10.47168
Epoch 70/80: current_loss=10.41503 | best_loss=10.41503
Epoch 71/80: current_loss=10.35596 | best_loss=10.35596
Epoch 72/80: current_loss=10.30205 | best_loss=10.30205
Epoch 73/80: current_loss=10.24609 | best_loss=10.24609
Epoch 74/80: current_loss=10.19337 | best_loss=10.19337
Epoch 75/80: current_loss=10.14112 | best_loss=10.14112
Epoch 76/80: current_loss=10.08838 | best_loss=10.08838
Epoch 77/80: current_loss=10.03995 | best_loss=10.03995
Epoch 78/80: current_loss=9.99147 | best_loss=9.99147
Epoch 79/80: current_loss=9.94462 | best_loss=9.94462
      explained_var=-0.02098 | mse_loss=9.97050
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=11.81285 | best_loss=11.81285
Epoch 1/80: current_loss=11.73923 | best_loss=11.73923
Epoch 2/80: current_loss=11.66495 | best_loss=11.66495
Epoch 3/80: current_loss=11.59071 | best_loss=11.59071
Epoch 4/80: current_loss=11.52263 | best_loss=11.52263
Epoch 5/80: current_loss=11.45895 | best_loss=11.45895
Epoch 6/80: current_loss=11.39678 | best_loss=11.39678
Epoch 7/80: current_loss=11.33670 | best_loss=11.33670
Epoch 8/80: current_loss=11.27480 | best_loss=11.27480
Epoch 9/80: current_loss=11.21827 | best_loss=11.21827
Epoch 10/80: current_loss=11.16569 | best_loss=11.16569
Epoch 11/80: current_loss=11.11263 | best_loss=11.11263
Epoch 12/80: current_loss=11.06217 | best_loss=11.06217
Epoch 13/80: current_loss=11.01158 | best_loss=11.01158
Epoch 14/80: current_loss=10.96299 | best_loss=10.96299
Epoch 15/80: current_loss=10.91902 | best_loss=10.91902
Epoch 16/80: current_loss=10.87493 | best_loss=10.87493
Epoch 17/80: current_loss=10.83006 | best_loss=10.83006
Epoch 18/80: current_loss=10.78870 | best_loss=10.78870
Epoch 19/80: current_loss=10.74840 | best_loss=10.74840
Epoch 20/80: current_loss=10.70927 | best_loss=10.70927
Epoch 21/80: current_loss=10.67794 | best_loss=10.67794
Epoch 22/80: current_loss=10.64187 | best_loss=10.64187
Epoch 23/80: current_loss=10.60521 | best_loss=10.60521
Epoch 24/80: current_loss=10.57491 | best_loss=10.57491
Epoch 25/80: current_loss=10.54481 | best_loss=10.54481
Epoch 26/80: current_loss=10.51348 | best_loss=10.51348
Epoch 27/80: current_loss=10.48392 | best_loss=10.48392
Epoch 28/80: current_loss=10.45106 | best_loss=10.45106
Epoch 29/80: current_loss=10.42597 | best_loss=10.42597
Epoch 30/80: current_loss=10.39903 | best_loss=10.39903
Epoch 31/80: current_loss=10.37438 | best_loss=10.37438
Epoch 32/80: current_loss=10.35038 | best_loss=10.35038
Epoch 33/80: current_loss=10.32500 | best_loss=10.32500
Epoch 34/80: current_loss=10.30130 | best_loss=10.30130
Epoch 35/80: current_loss=10.27878 | best_loss=10.27878
Epoch 36/80: current_loss=10.25723 | best_loss=10.25723
Epoch 37/80: current_loss=10.23637 | best_loss=10.23637
Epoch 38/80: current_loss=10.21643 | best_loss=10.21643
Epoch 39/80: current_loss=10.19630 | best_loss=10.19630
Epoch 40/80: current_loss=10.17664 | best_loss=10.17664
Epoch 41/80: current_loss=10.15774 | best_loss=10.15774
Epoch 42/80: current_loss=10.14069 | best_loss=10.14069
Epoch 43/80: current_loss=10.12519 | best_loss=10.12519
Epoch 44/80: current_loss=10.11054 | best_loss=10.11054
Epoch 45/80: current_loss=10.09424 | best_loss=10.09424
Epoch 46/80: current_loss=10.07982 | best_loss=10.07982
Epoch 47/80: current_loss=10.06633 | best_loss=10.06633
Epoch 48/80: current_loss=10.05215 | best_loss=10.05215
Epoch 49/80: current_loss=10.03820 | best_loss=10.03820
Epoch 50/80: current_loss=10.02656 | best_loss=10.02656
Epoch 51/80: current_loss=10.01562 | best_loss=10.01562
Epoch 52/80: current_loss=10.00145 | best_loss=10.00145
Epoch 53/80: current_loss=9.98858 | best_loss=9.98858
Epoch 54/80: current_loss=9.97586 | best_loss=9.97586
Epoch 55/80: current_loss=9.96577 | best_loss=9.96577
Epoch 56/80: current_loss=9.95324 | best_loss=9.95324
Epoch 57/80: current_loss=9.94391 | best_loss=9.94391
Epoch 58/80: current_loss=9.93476 | best_loss=9.93476
Epoch 59/80: current_loss=9.92372 | best_loss=9.92372
Epoch 60/80: current_loss=9.91462 | best_loss=9.91462
Epoch 61/80: current_loss=9.90787 | best_loss=9.90787
Epoch 62/80: current_loss=9.89707 | best_loss=9.89707
Epoch 63/80: current_loss=9.88876 | best_loss=9.88876
Epoch 64/80: current_loss=9.88188 | best_loss=9.88188
Epoch 65/80: current_loss=9.87041 | best_loss=9.87041
Epoch 66/80: current_loss=9.86241 | best_loss=9.86241
Epoch 67/80: current_loss=9.85453 | best_loss=9.85453
Epoch 68/80: current_loss=9.85007 | best_loss=9.85007
Epoch 69/80: current_loss=9.84060 | best_loss=9.84060
Epoch 70/80: current_loss=9.83342 | best_loss=9.83342
Epoch 71/80: current_loss=9.82609 | best_loss=9.82609
Epoch 72/80: current_loss=9.81976 | best_loss=9.81976
Epoch 73/80: current_loss=9.81169 | best_loss=9.81169
Epoch 74/80: current_loss=9.80687 | best_loss=9.80687
Epoch 75/80: current_loss=9.80031 | best_loss=9.80031
Epoch 76/80: current_loss=9.79481 | best_loss=9.79481
Epoch 77/80: current_loss=9.79019 | best_loss=9.79019
Epoch 78/80: current_loss=9.78585 | best_loss=9.78585
Epoch 79/80: current_loss=9.78111 | best_loss=9.78111
      explained_var=-0.04533 | mse_loss=10.01232
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.91309 | best_loss=8.91309
Epoch 1/80: current_loss=8.91240 | best_loss=8.91240
Epoch 2/80: current_loss=8.91169 | best_loss=8.91169
Epoch 3/80: current_loss=8.91111 | best_loss=8.91111
Epoch 4/80: current_loss=8.91057 | best_loss=8.91057
Epoch 5/80: current_loss=8.91007 | best_loss=8.91007
Epoch 6/80: current_loss=8.90986 | best_loss=8.90986
Epoch 7/80: current_loss=8.90967 | best_loss=8.90967
Epoch 8/80: current_loss=8.90943 | best_loss=8.90943
Epoch 9/80: current_loss=8.90926 | best_loss=8.90926
Epoch 10/80: current_loss=8.90919 | best_loss=8.90919
Epoch 11/80: current_loss=8.90919 | best_loss=8.90919
Epoch 12/80: current_loss=8.90925 | best_loss=8.90919
Epoch 13/80: current_loss=8.90912 | best_loss=8.90912
Epoch 14/80: current_loss=8.90908 | best_loss=8.90908
Epoch 15/80: current_loss=8.90946 | best_loss=8.90908
Epoch 16/80: current_loss=8.90970 | best_loss=8.90908
Epoch 17/80: current_loss=8.90965 | best_loss=8.90908
Epoch 18/80: current_loss=8.90979 | best_loss=8.90908
Epoch 19/80: current_loss=8.91009 | best_loss=8.90908
Epoch 20/80: current_loss=8.91061 | best_loss=8.90908
Epoch 21/80: current_loss=8.91072 | best_loss=8.90908
Epoch 22/80: current_loss=8.91110 | best_loss=8.90908
Epoch 23/80: current_loss=8.91133 | best_loss=8.90908
Epoch 24/80: current_loss=8.91179 | best_loss=8.90908
Epoch 25/80: current_loss=8.91171 | best_loss=8.90908
Epoch 26/80: current_loss=8.91250 | best_loss=8.90908
Epoch 27/80: current_loss=8.91273 | best_loss=8.90908
Epoch 28/80: current_loss=8.91311 | best_loss=8.90908
Epoch 29/80: current_loss=8.91345 | best_loss=8.90908
Epoch 30/80: current_loss=8.91358 | best_loss=8.90908
Epoch 31/80: current_loss=8.91350 | best_loss=8.90908
Epoch 32/80: current_loss=8.91365 | best_loss=8.90908
Epoch 33/80: current_loss=8.91391 | best_loss=8.90908
Epoch 34/80: current_loss=8.91445 | best_loss=8.90908
Early Stopping at epoch 34
      explained_var=-0.00319 | mse_loss=8.38930
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.51956 | best_loss=8.51956
Epoch 1/80: current_loss=8.51884 | best_loss=8.51884
Epoch 2/80: current_loss=8.51832 | best_loss=8.51832
Epoch 3/80: current_loss=8.51806 | best_loss=8.51806
Epoch 4/80: current_loss=8.51794 | best_loss=8.51794
Epoch 5/80: current_loss=8.51721 | best_loss=8.51721
Epoch 6/80: current_loss=8.51665 | best_loss=8.51665
Epoch 7/80: current_loss=8.51634 | best_loss=8.51634
Epoch 8/80: current_loss=8.51607 | best_loss=8.51607
Epoch 9/80: current_loss=8.51574 | best_loss=8.51574
Epoch 10/80: current_loss=8.51536 | best_loss=8.51536
Epoch 11/80: current_loss=8.51464 | best_loss=8.51464
Epoch 12/80: current_loss=8.51422 | best_loss=8.51422
Epoch 13/80: current_loss=8.51368 | best_loss=8.51368
Epoch 14/80: current_loss=8.51303 | best_loss=8.51303
Epoch 15/80: current_loss=8.51232 | best_loss=8.51232
Epoch 16/80: current_loss=8.51217 | best_loss=8.51217
Epoch 17/80: current_loss=8.51124 | best_loss=8.51124
Epoch 18/80: current_loss=8.51075 | best_loss=8.51075
Epoch 19/80: current_loss=8.51065 | best_loss=8.51065
Epoch 20/80: current_loss=8.51007 | best_loss=8.51007
Epoch 21/80: current_loss=8.50988 | best_loss=8.50988
Epoch 22/80: current_loss=8.50951 | best_loss=8.50951
Epoch 23/80: current_loss=8.50886 | best_loss=8.50886
Epoch 24/80: current_loss=8.50841 | best_loss=8.50841
Epoch 25/80: current_loss=8.50806 | best_loss=8.50806
Epoch 26/80: current_loss=8.50760 | best_loss=8.50760
Epoch 27/80: current_loss=8.50714 | best_loss=8.50714
Epoch 28/80: current_loss=8.50676 | best_loss=8.50676
Epoch 29/80: current_loss=8.50661 | best_loss=8.50661
Epoch 30/80: current_loss=8.50630 | best_loss=8.50630
Epoch 31/80: current_loss=8.50627 | best_loss=8.50627
Epoch 32/80: current_loss=8.50591 | best_loss=8.50591
Epoch 33/80: current_loss=8.50516 | best_loss=8.50516
Epoch 34/80: current_loss=8.50547 | best_loss=8.50516
Epoch 35/80: current_loss=8.50527 | best_loss=8.50516
Epoch 36/80: current_loss=8.50460 | best_loss=8.50460
Epoch 37/80: current_loss=8.50456 | best_loss=8.50456
Epoch 38/80: current_loss=8.50420 | best_loss=8.50420
Epoch 39/80: current_loss=8.50428 | best_loss=8.50420
Epoch 40/80: current_loss=8.50368 | best_loss=8.50368
Epoch 41/80: current_loss=8.50343 | best_loss=8.50343
Epoch 42/80: current_loss=8.50342 | best_loss=8.50342
Epoch 43/80: current_loss=8.50343 | best_loss=8.50342
Epoch 44/80: current_loss=8.50330 | best_loss=8.50330
Epoch 45/80: current_loss=8.50284 | best_loss=8.50284
Epoch 46/80: current_loss=8.50256 | best_loss=8.50256
Epoch 47/80: current_loss=8.50227 | best_loss=8.50227
Epoch 48/80: current_loss=8.50172 | best_loss=8.50172
Epoch 49/80: current_loss=8.50142 | best_loss=8.50142
Epoch 50/80: current_loss=8.50155 | best_loss=8.50142
Epoch 51/80: current_loss=8.50156 | best_loss=8.50142
Epoch 52/80: current_loss=8.50155 | best_loss=8.50142
Epoch 53/80: current_loss=8.50157 | best_loss=8.50142
Epoch 54/80: current_loss=8.50149 | best_loss=8.50142
Epoch 55/80: current_loss=8.50105 | best_loss=8.50105
Epoch 56/80: current_loss=8.50091 | best_loss=8.50091
Epoch 57/80: current_loss=8.50056 | best_loss=8.50056
Epoch 58/80: current_loss=8.50002 | best_loss=8.50002
Epoch 59/80: current_loss=8.49987 | best_loss=8.49987
Epoch 60/80: current_loss=8.49986 | best_loss=8.49986
Epoch 61/80: current_loss=8.49946 | best_loss=8.49946
Epoch 62/80: current_loss=8.49912 | best_loss=8.49912
Epoch 63/80: current_loss=8.49898 | best_loss=8.49898
Epoch 64/80: current_loss=8.49846 | best_loss=8.49846
Epoch 65/80: current_loss=8.49821 | best_loss=8.49821
Epoch 66/80: current_loss=8.49815 | best_loss=8.49815
Epoch 67/80: current_loss=8.49806 | best_loss=8.49806
Epoch 68/80: current_loss=8.49767 | best_loss=8.49767
Epoch 69/80: current_loss=8.49745 | best_loss=8.49745
Epoch 70/80: current_loss=8.49738 | best_loss=8.49738
Epoch 71/80: current_loss=8.49725 | best_loss=8.49725
Epoch 72/80: current_loss=8.49706 | best_loss=8.49706
Epoch 73/80: current_loss=8.49704 | best_loss=8.49704
Epoch 74/80: current_loss=8.49687 | best_loss=8.49687
Epoch 75/80: current_loss=8.49672 | best_loss=8.49672
Epoch 76/80: current_loss=8.49665 | best_loss=8.49665
Epoch 77/80: current_loss=8.49683 | best_loss=8.49665
Epoch 78/80: current_loss=8.49676 | best_loss=8.49665
Epoch 79/80: current_loss=8.49620 | best_loss=8.49620
      explained_var=-0.00227 | mse_loss=8.21928
----------------------------------------------
Average early_stopping_point: 66| avg_exp_var=-0.02079| avg_loss=11.04559
----------------------------------------------


----------------------------------------------
Params for Trial 2
{'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=14.08455 | best_loss=14.08455
Epoch 1/80: current_loss=12.74557 | best_loss=12.74557
Epoch 2/80: current_loss=12.02270 | best_loss=12.02270
Epoch 3/80: current_loss=11.60427 | best_loss=11.60427
Epoch 4/80: current_loss=11.30916 | best_loss=11.30916
Epoch 5/80: current_loss=11.11589 | best_loss=11.11589
Epoch 6/80: current_loss=10.92600 | best_loss=10.92600
Epoch 7/80: current_loss=10.76807 | best_loss=10.76807
Epoch 8/80: current_loss=10.66648 | best_loss=10.66648
Epoch 9/80: current_loss=10.58164 | best_loss=10.58164
Epoch 10/80: current_loss=10.50311 | best_loss=10.50311
Epoch 11/80: current_loss=10.42144 | best_loss=10.42144
Epoch 12/80: current_loss=10.35658 | best_loss=10.35658
Epoch 13/80: current_loss=10.31227 | best_loss=10.31227
Epoch 14/80: current_loss=10.25624 | best_loss=10.25624
Epoch 15/80: current_loss=10.23390 | best_loss=10.23390
Epoch 16/80: current_loss=10.13766 | best_loss=10.13766
Epoch 17/80: current_loss=10.09980 | best_loss=10.09980
Epoch 18/80: current_loss=10.06395 | best_loss=10.06395
Epoch 19/80: current_loss=10.03547 | best_loss=10.03547
Epoch 20/80: current_loss=10.04297 | best_loss=10.03547
Epoch 21/80: current_loss=9.98246 | best_loss=9.98246
Epoch 22/80: current_loss=9.99248 | best_loss=9.98246
Epoch 23/80: current_loss=9.98275 | best_loss=9.98246
Epoch 24/80: current_loss=9.95846 | best_loss=9.95846
Epoch 25/80: current_loss=9.95317 | best_loss=9.95317
Epoch 26/80: current_loss=9.94891 | best_loss=9.94891
Epoch 27/80: current_loss=9.94250 | best_loss=9.94250
Epoch 28/80: current_loss=9.98505 | best_loss=9.94250
Epoch 29/80: current_loss=9.95234 | best_loss=9.94250
Epoch 30/80: current_loss=9.97122 | best_loss=9.94250
Epoch 31/80: current_loss=9.91963 | best_loss=9.91963
Epoch 32/80: current_loss=9.91514 | best_loss=9.91514
Epoch 33/80: current_loss=9.91420 | best_loss=9.91420
Epoch 34/80: current_loss=9.90903 | best_loss=9.90903
Epoch 35/80: current_loss=9.90945 | best_loss=9.90903
Epoch 36/80: current_loss=9.91219 | best_loss=9.90903
Epoch 37/80: current_loss=9.91420 | best_loss=9.90903
Epoch 38/80: current_loss=9.96612 | best_loss=9.90903
Epoch 39/80: current_loss=9.91368 | best_loss=9.90903
Epoch 40/80: current_loss=9.92940 | best_loss=9.90903
Epoch 41/80: current_loss=9.94902 | best_loss=9.90903
Epoch 42/80: current_loss=9.95109 | best_loss=9.90903
Epoch 43/80: current_loss=9.93737 | best_loss=9.90903
Epoch 44/80: current_loss=9.93514 | best_loss=9.90903
Epoch 45/80: current_loss=9.93122 | best_loss=9.90903
Epoch 46/80: current_loss=9.94531 | best_loss=9.90903
Epoch 47/80: current_loss=9.93152 | best_loss=9.90903
Epoch 48/80: current_loss=9.92742 | best_loss=9.90903
Epoch 49/80: current_loss=9.92929 | best_loss=9.90903
Epoch 50/80: current_loss=9.98090 | best_loss=9.90903
Epoch 51/80: current_loss=9.93530 | best_loss=9.90903
Epoch 52/80: current_loss=9.95028 | best_loss=9.90903
Epoch 53/80: current_loss=9.96583 | best_loss=9.90903
Epoch 54/80: current_loss=9.92531 | best_loss=9.90903
Early Stopping at epoch 54
      explained_var=0.00061 | mse_loss=9.57825
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.46767 | best_loss=8.46767
Epoch 1/80: current_loss=8.53445 | best_loss=8.46767
Epoch 2/80: current_loss=8.51779 | best_loss=8.46767
Epoch 3/80: current_loss=8.47673 | best_loss=8.46767
Epoch 4/80: current_loss=8.47338 | best_loss=8.46767
Epoch 5/80: current_loss=8.49926 | best_loss=8.46767
Epoch 6/80: current_loss=8.54254 | best_loss=8.46767
Epoch 7/80: current_loss=8.47926 | best_loss=8.46767
Epoch 8/80: current_loss=8.51917 | best_loss=8.46767
Epoch 9/80: current_loss=8.51947 | best_loss=8.46767
Epoch 10/80: current_loss=8.48101 | best_loss=8.46767
Epoch 11/80: current_loss=8.49802 | best_loss=8.46767
Epoch 12/80: current_loss=8.50593 | best_loss=8.46767
Epoch 13/80: current_loss=8.48079 | best_loss=8.46767
Epoch 14/80: current_loss=8.50217 | best_loss=8.46767
Epoch 15/80: current_loss=8.49698 | best_loss=8.46767
Epoch 16/80: current_loss=8.50821 | best_loss=8.46767
Epoch 17/80: current_loss=8.49922 | best_loss=8.46767
Epoch 18/80: current_loss=8.49816 | best_loss=8.46767
Epoch 19/80: current_loss=8.50569 | best_loss=8.46767
Epoch 20/80: current_loss=8.50880 | best_loss=8.46767
Early Stopping at epoch 20
      explained_var=-0.00004 | mse_loss=8.57714
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.38611 | best_loss=9.38611
Epoch 1/80: current_loss=9.42518 | best_loss=9.38611
Epoch 2/80: current_loss=9.33767 | best_loss=9.33767
Epoch 3/80: current_loss=9.42298 | best_loss=9.33767
Epoch 4/80: current_loss=9.40018 | best_loss=9.33767
Epoch 5/80: current_loss=9.38888 | best_loss=9.33767
Epoch 6/80: current_loss=9.36824 | best_loss=9.33767
Epoch 7/80: current_loss=9.39440 | best_loss=9.33767
Epoch 8/80: current_loss=9.39712 | best_loss=9.33767
Epoch 9/80: current_loss=9.37907 | best_loss=9.33767
Epoch 10/80: current_loss=9.42498 | best_loss=9.33767
Epoch 11/80: current_loss=9.35438 | best_loss=9.33767
Epoch 12/80: current_loss=9.36558 | best_loss=9.33767
Epoch 13/80: current_loss=9.41920 | best_loss=9.33767
Epoch 14/80: current_loss=9.30140 | best_loss=9.30140
Epoch 15/80: current_loss=9.43563 | best_loss=9.30140
Epoch 16/80: current_loss=9.37402 | best_loss=9.30140
Epoch 17/80: current_loss=9.35876 | best_loss=9.30140
Epoch 18/80: current_loss=9.39570 | best_loss=9.30140
Epoch 19/80: current_loss=9.45133 | best_loss=9.30140
Epoch 20/80: current_loss=9.33555 | best_loss=9.30140
Epoch 21/80: current_loss=9.44589 | best_loss=9.30140
Epoch 22/80: current_loss=9.39873 | best_loss=9.30140
Epoch 23/80: current_loss=9.37740 | best_loss=9.30140
Epoch 24/80: current_loss=9.44574 | best_loss=9.30140
Epoch 25/80: current_loss=9.46597 | best_loss=9.30140
Epoch 26/80: current_loss=9.42579 | best_loss=9.30140
Epoch 27/80: current_loss=9.37698 | best_loss=9.30140
Epoch 28/80: current_loss=9.44990 | best_loss=9.30140
Epoch 29/80: current_loss=9.40179 | best_loss=9.30140
Epoch 30/80: current_loss=9.46947 | best_loss=9.30140
Epoch 31/80: current_loss=9.39042 | best_loss=9.30140
Epoch 32/80: current_loss=9.39636 | best_loss=9.30140
Epoch 33/80: current_loss=9.39820 | best_loss=9.30140
Epoch 34/80: current_loss=9.37557 | best_loss=9.30140
Early Stopping at epoch 34
      explained_var=0.00185 | mse_loss=9.41792
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.99657 | best_loss=8.99657
Epoch 1/80: current_loss=9.00171 | best_loss=8.99657
Epoch 2/80: current_loss=8.99707 | best_loss=8.99657
Epoch 3/80: current_loss=9.02408 | best_loss=8.99657
Epoch 4/80: current_loss=9.04081 | best_loss=8.99657
Epoch 5/80: current_loss=9.05161 | best_loss=8.99657
Epoch 6/80: current_loss=8.98416 | best_loss=8.98416
Epoch 7/80: current_loss=8.96736 | best_loss=8.96736
Epoch 8/80: current_loss=8.94632 | best_loss=8.94632
Epoch 9/80: current_loss=9.04405 | best_loss=8.94632
Epoch 10/80: current_loss=8.96040 | best_loss=8.94632
Epoch 11/80: current_loss=8.95506 | best_loss=8.94632
Epoch 12/80: current_loss=8.95720 | best_loss=8.94632
Epoch 13/80: current_loss=8.97852 | best_loss=8.94632
Epoch 14/80: current_loss=8.98180 | best_loss=8.94632
Epoch 15/80: current_loss=8.97007 | best_loss=8.94632
Epoch 16/80: current_loss=8.97234 | best_loss=8.94632
Epoch 17/80: current_loss=8.96170 | best_loss=8.94632
Epoch 18/80: current_loss=8.98657 | best_loss=8.94632
Epoch 19/80: current_loss=8.98224 | best_loss=8.94632
Epoch 20/80: current_loss=8.97876 | best_loss=8.94632
Epoch 21/80: current_loss=8.96492 | best_loss=8.94632
Epoch 22/80: current_loss=8.96349 | best_loss=8.94632
Epoch 23/80: current_loss=8.96079 | best_loss=8.94632
Epoch 24/80: current_loss=8.99056 | best_loss=8.94632
Epoch 25/80: current_loss=8.97643 | best_loss=8.94632
Epoch 26/80: current_loss=8.99192 | best_loss=8.94632
Epoch 27/80: current_loss=9.00003 | best_loss=8.94632
Epoch 28/80: current_loss=8.96456 | best_loss=8.94632
Early Stopping at epoch 28
      explained_var=0.00099 | mse_loss=8.35393
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.49596 | best_loss=8.49596
Epoch 1/80: current_loss=8.49571 | best_loss=8.49571
Epoch 2/80: current_loss=8.49150 | best_loss=8.49150
Epoch 3/80: current_loss=8.48467 | best_loss=8.48467
Epoch 4/80: current_loss=8.48870 | best_loss=8.48467
Epoch 5/80: current_loss=8.49377 | best_loss=8.48467
Epoch 6/80: current_loss=8.48090 | best_loss=8.48090
Epoch 7/80: current_loss=8.47900 | best_loss=8.47900
Epoch 8/80: current_loss=8.48444 | best_loss=8.47900
Epoch 9/80: current_loss=8.48530 | best_loss=8.47900
Epoch 10/80: current_loss=8.48496 | best_loss=8.47900
Epoch 11/80: current_loss=8.48714 | best_loss=8.47900
Epoch 12/80: current_loss=8.49500 | best_loss=8.47900
Epoch 13/80: current_loss=8.49104 | best_loss=8.47900
Epoch 14/80: current_loss=8.49802 | best_loss=8.47900
Epoch 15/80: current_loss=8.49717 | best_loss=8.47900
Epoch 16/80: current_loss=8.49547 | best_loss=8.47900
Epoch 17/80: current_loss=8.50197 | best_loss=8.47900
Epoch 18/80: current_loss=8.50590 | best_loss=8.47900
Epoch 19/80: current_loss=8.50748 | best_loss=8.47900
Epoch 20/80: current_loss=8.49706 | best_loss=8.47900
Epoch 21/80: current_loss=8.49449 | best_loss=8.47900
Epoch 22/80: current_loss=8.49712 | best_loss=8.47900
Epoch 23/80: current_loss=8.48864 | best_loss=8.47900
Epoch 24/80: current_loss=8.49002 | best_loss=8.47900
Epoch 25/80: current_loss=8.49527 | best_loss=8.47900
Epoch 26/80: current_loss=8.49100 | best_loss=8.47900
Epoch 27/80: current_loss=8.49353 | best_loss=8.47900
Early Stopping at epoch 27
      explained_var=0.00020 | mse_loss=8.19887
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=0.00072| avg_loss=8.82522
----------------------------------------------


----------------------------------------------
Params for Trial 3
{'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=43.91398 | best_loss=43.91398
Epoch 1/80: current_loss=22.71967 | best_loss=22.71967
Epoch 2/80: current_loss=15.48902 | best_loss=15.48902
Epoch 3/80: current_loss=12.54962 | best_loss=12.54962
Epoch 4/80: current_loss=11.34134 | best_loss=11.34134
Epoch 5/80: current_loss=10.93457 | best_loss=10.93457
Epoch 6/80: current_loss=10.77993 | best_loss=10.77993
Epoch 7/80: current_loss=10.68565 | best_loss=10.68565
Epoch 8/80: current_loss=10.61970 | best_loss=10.61970
Epoch 9/80: current_loss=10.56224 | best_loss=10.56224
Epoch 10/80: current_loss=10.51266 | best_loss=10.51266
Epoch 11/80: current_loss=10.46662 | best_loss=10.46662
Epoch 12/80: current_loss=10.42479 | best_loss=10.42479
Epoch 13/80: current_loss=10.38302 | best_loss=10.38302
Epoch 14/80: current_loss=10.35004 | best_loss=10.35004
Epoch 15/80: current_loss=10.31983 | best_loss=10.31983
Epoch 16/80: current_loss=10.28835 | best_loss=10.28835
Epoch 17/80: current_loss=10.25175 | best_loss=10.25175
Epoch 18/80: current_loss=10.22704 | best_loss=10.22704
Epoch 19/80: current_loss=10.20079 | best_loss=10.20079
Epoch 20/80: current_loss=10.17702 | best_loss=10.17702
Epoch 21/80: current_loss=10.15337 | best_loss=10.15337
Epoch 22/80: current_loss=10.13787 | best_loss=10.13787
Epoch 23/80: current_loss=10.12052 | best_loss=10.12052
Epoch 24/80: current_loss=10.10469 | best_loss=10.10469
Epoch 25/80: current_loss=10.08832 | best_loss=10.08832
Epoch 26/80: current_loss=10.07583 | best_loss=10.07583
Epoch 27/80: current_loss=10.06666 | best_loss=10.06666
Epoch 28/80: current_loss=10.06263 | best_loss=10.06263
Epoch 29/80: current_loss=10.05273 | best_loss=10.05273
Epoch 30/80: current_loss=10.03620 | best_loss=10.03620
Epoch 31/80: current_loss=10.02253 | best_loss=10.02253
Epoch 32/80: current_loss=10.01764 | best_loss=10.01764
Epoch 33/80: current_loss=10.00821 | best_loss=10.00821
Epoch 34/80: current_loss=9.99882 | best_loss=9.99882
Epoch 35/80: current_loss=9.99014 | best_loss=9.99014
Epoch 36/80: current_loss=9.98261 | best_loss=9.98261
Epoch 37/80: current_loss=9.97427 | best_loss=9.97427
Epoch 38/80: current_loss=9.96809 | best_loss=9.96809
Epoch 39/80: current_loss=9.96251 | best_loss=9.96251
Epoch 40/80: current_loss=9.96207 | best_loss=9.96207
Epoch 41/80: current_loss=9.95149 | best_loss=9.95149
Epoch 42/80: current_loss=9.95001 | best_loss=9.95001
Epoch 43/80: current_loss=9.94441 | best_loss=9.94441
Epoch 44/80: current_loss=9.93989 | best_loss=9.93989
Epoch 45/80: current_loss=9.93504 | best_loss=9.93504
Epoch 46/80: current_loss=9.93170 | best_loss=9.93170
Epoch 47/80: current_loss=9.92949 | best_loss=9.92949
Epoch 48/80: current_loss=9.92940 | best_loss=9.92940
Epoch 49/80: current_loss=9.92162 | best_loss=9.92162
Epoch 50/80: current_loss=9.91979 | best_loss=9.91979
Epoch 51/80: current_loss=9.92491 | best_loss=9.91979
Epoch 52/80: current_loss=9.91770 | best_loss=9.91770
Epoch 53/80: current_loss=9.91490 | best_loss=9.91490
Epoch 54/80: current_loss=9.91239 | best_loss=9.91239
Epoch 55/80: current_loss=9.90962 | best_loss=9.90962
Epoch 56/80: current_loss=9.90837 | best_loss=9.90837
Epoch 57/80: current_loss=9.90670 | best_loss=9.90670
Epoch 58/80: current_loss=9.90746 | best_loss=9.90670
Epoch 59/80: current_loss=9.91021 | best_loss=9.90670
Epoch 60/80: current_loss=9.90201 | best_loss=9.90201
Epoch 61/80: current_loss=9.89834 | best_loss=9.89834
Epoch 62/80: current_loss=9.90116 | best_loss=9.89834
Epoch 63/80: current_loss=9.89512 | best_loss=9.89512
Epoch 64/80: current_loss=9.89427 | best_loss=9.89427
Epoch 65/80: current_loss=9.89630 | best_loss=9.89427
Epoch 66/80: current_loss=9.89120 | best_loss=9.89120
Epoch 67/80: current_loss=9.89014 | best_loss=9.89014
Epoch 68/80: current_loss=9.89823 | best_loss=9.89014
Epoch 69/80: current_loss=9.92521 | best_loss=9.89014
Epoch 70/80: current_loss=9.88466 | best_loss=9.88466
Epoch 71/80: current_loss=9.88809 | best_loss=9.88466
Epoch 72/80: current_loss=9.88475 | best_loss=9.88466
Epoch 73/80: current_loss=9.87491 | best_loss=9.87491
Epoch 74/80: current_loss=9.87474 | best_loss=9.87474
Epoch 75/80: current_loss=9.87924 | best_loss=9.87474
Epoch 76/80: current_loss=9.89886 | best_loss=9.87474
Epoch 77/80: current_loss=9.88123 | best_loss=9.87474
Epoch 78/80: current_loss=9.87362 | best_loss=9.87362
Epoch 79/80: current_loss=9.87335 | best_loss=9.87335
      explained_var=0.00309 | mse_loss=9.55168
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.51903 | best_loss=8.51903
Epoch 1/80: current_loss=8.49051 | best_loss=8.49051
Epoch 2/80: current_loss=8.57594 | best_loss=8.49051
Epoch 3/80: current_loss=8.49136 | best_loss=8.49051
Epoch 4/80: current_loss=8.47826 | best_loss=8.47826
Epoch 5/80: current_loss=8.50887 | best_loss=8.47826
Epoch 6/80: current_loss=8.48349 | best_loss=8.47826
Epoch 7/80: current_loss=8.54736 | best_loss=8.47826
Epoch 8/80: current_loss=8.46552 | best_loss=8.46552
Epoch 9/80: current_loss=8.50947 | best_loss=8.46552
Epoch 10/80: current_loss=8.49156 | best_loss=8.46552
Epoch 11/80: current_loss=8.48276 | best_loss=8.46552
Epoch 12/80: current_loss=8.50538 | best_loss=8.46552
Epoch 13/80: current_loss=8.49807 | best_loss=8.46552
Epoch 14/80: current_loss=8.53497 | best_loss=8.46552
Epoch 15/80: current_loss=8.47350 | best_loss=8.46552
Epoch 16/80: current_loss=8.48289 | best_loss=8.46552
Epoch 17/80: current_loss=8.53243 | best_loss=8.46552
Epoch 18/80: current_loss=8.53441 | best_loss=8.46552
Epoch 19/80: current_loss=8.49049 | best_loss=8.46552
Epoch 20/80: current_loss=8.60022 | best_loss=8.46552
Epoch 21/80: current_loss=8.47548 | best_loss=8.46552
Epoch 22/80: current_loss=8.53325 | best_loss=8.46552
Epoch 23/80: current_loss=8.52398 | best_loss=8.46552
Epoch 24/80: current_loss=8.49084 | best_loss=8.46552
Epoch 25/80: current_loss=8.49645 | best_loss=8.46552
Epoch 26/80: current_loss=8.52642 | best_loss=8.46552
Epoch 27/80: current_loss=8.53660 | best_loss=8.46552
Epoch 28/80: current_loss=8.48035 | best_loss=8.46552
Early Stopping at epoch 28
      explained_var=0.00154 | mse_loss=8.56308
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.52626 | best_loss=9.52626
Epoch 1/80: current_loss=9.58637 | best_loss=9.52626
Epoch 2/80: current_loss=9.41253 | best_loss=9.41253
Epoch 3/80: current_loss=9.51018 | best_loss=9.41253
Epoch 4/80: current_loss=9.52365 | best_loss=9.41253
Epoch 5/80: current_loss=9.64593 | best_loss=9.41253
Epoch 6/80: current_loss=9.51665 | best_loss=9.41253
Epoch 7/80: current_loss=9.54988 | best_loss=9.41253
Epoch 8/80: current_loss=9.50329 | best_loss=9.41253
Epoch 9/80: current_loss=9.61671 | best_loss=9.41253
Epoch 10/80: current_loss=9.55587 | best_loss=9.41253
Epoch 11/80: current_loss=9.70396 | best_loss=9.41253
Epoch 12/80: current_loss=9.62818 | best_loss=9.41253
Epoch 13/80: current_loss=9.43214 | best_loss=9.41253
Epoch 14/80: current_loss=9.52138 | best_loss=9.41253
Epoch 15/80: current_loss=9.54818 | best_loss=9.41253
Epoch 16/80: current_loss=9.61812 | best_loss=9.41253
Epoch 17/80: current_loss=9.57197 | best_loss=9.41253
Epoch 18/80: current_loss=9.57582 | best_loss=9.41253
Epoch 19/80: current_loss=9.58074 | best_loss=9.41253
Epoch 20/80: current_loss=9.51514 | best_loss=9.41253
Epoch 21/80: current_loss=9.49869 | best_loss=9.41253
Epoch 22/80: current_loss=9.47422 | best_loss=9.41253
Early Stopping at epoch 22
      explained_var=-0.01274 | mse_loss=9.58290
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.92803 | best_loss=8.92803
Epoch 1/80: current_loss=8.93155 | best_loss=8.92803
Epoch 2/80: current_loss=8.93117 | best_loss=8.92803
Epoch 3/80: current_loss=8.92814 | best_loss=8.92803
Epoch 4/80: current_loss=8.94526 | best_loss=8.92803
Epoch 5/80: current_loss=8.93400 | best_loss=8.92803
Epoch 6/80: current_loss=8.93470 | best_loss=8.92803
Epoch 7/80: current_loss=8.94499 | best_loss=8.92803
Epoch 8/80: current_loss=8.94948 | best_loss=8.92803
Epoch 9/80: current_loss=8.93241 | best_loss=8.92803
Epoch 10/80: current_loss=8.93981 | best_loss=8.92803
Epoch 11/80: current_loss=8.93010 | best_loss=8.92803
Epoch 12/80: current_loss=9.00797 | best_loss=8.92803
Epoch 13/80: current_loss=8.93126 | best_loss=8.92803
Epoch 14/80: current_loss=8.92879 | best_loss=8.92803
Epoch 15/80: current_loss=8.97756 | best_loss=8.92803
Epoch 16/80: current_loss=8.94651 | best_loss=8.92803
Epoch 17/80: current_loss=8.93417 | best_loss=8.92803
Epoch 18/80: current_loss=8.97567 | best_loss=8.92803
Epoch 19/80: current_loss=8.93872 | best_loss=8.92803
Epoch 20/80: current_loss=8.98233 | best_loss=8.92803
Early Stopping at epoch 20
      explained_var=0.00276 | mse_loss=8.33880
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.58270 | best_loss=8.58270
Epoch 1/80: current_loss=8.47480 | best_loss=8.47480
Epoch 2/80: current_loss=8.48388 | best_loss=8.47480
Epoch 3/80: current_loss=8.47669 | best_loss=8.47480
Epoch 4/80: current_loss=8.47713 | best_loss=8.47480
Epoch 5/80: current_loss=8.50324 | best_loss=8.47480
Epoch 6/80: current_loss=8.50454 | best_loss=8.47480
Epoch 7/80: current_loss=8.47956 | best_loss=8.47480
Epoch 8/80: current_loss=8.50043 | best_loss=8.47480
Epoch 9/80: current_loss=8.48584 | best_loss=8.47480
Epoch 10/80: current_loss=8.47823 | best_loss=8.47480
Epoch 11/80: current_loss=8.47646 | best_loss=8.47480
Epoch 12/80: current_loss=8.47366 | best_loss=8.47366
Epoch 13/80: current_loss=8.48216 | best_loss=8.47366
Epoch 14/80: current_loss=8.49271 | best_loss=8.47366
Epoch 15/80: current_loss=8.48145 | best_loss=8.47366
Epoch 16/80: current_loss=8.47704 | best_loss=8.47366
Epoch 17/80: current_loss=8.49041 | best_loss=8.47366
Epoch 18/80: current_loss=8.49347 | best_loss=8.47366
Epoch 19/80: current_loss=8.48082 | best_loss=8.47366
Epoch 20/80: current_loss=8.47373 | best_loss=8.47366
Epoch 21/80: current_loss=8.50780 | best_loss=8.47366
Epoch 22/80: current_loss=8.47840 | best_loss=8.47366
Epoch 23/80: current_loss=8.48600 | best_loss=8.47366
Epoch 24/80: current_loss=8.48015 | best_loss=8.47366
Epoch 25/80: current_loss=8.48856 | best_loss=8.47366
Epoch 26/80: current_loss=8.47653 | best_loss=8.47366
Epoch 27/80: current_loss=8.50013 | best_loss=8.47366
Epoch 28/80: current_loss=8.47513 | best_loss=8.47366
Epoch 29/80: current_loss=8.50480 | best_loss=8.47366
Epoch 30/80: current_loss=8.53383 | best_loss=8.47366
Epoch 31/80: current_loss=8.49397 | best_loss=8.47366
Epoch 32/80: current_loss=8.48380 | best_loss=8.47366
Early Stopping at epoch 32
      explained_var=0.00108 | mse_loss=8.19164
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=-0.00085| avg_loss=8.84562
----------------------------------------------


----------------------------------------------
Params for Trial 4
{'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=33.96000 | best_loss=33.96000
Epoch 1/80: current_loss=20.32330 | best_loss=20.32330
Epoch 2/80: current_loss=14.20449 | best_loss=14.20449
Epoch 3/80: current_loss=11.51747 | best_loss=11.51747
Epoch 4/80: current_loss=10.46176 | best_loss=10.46176
Epoch 5/80: current_loss=10.07434 | best_loss=10.07434
Epoch 6/80: current_loss=9.95633 | best_loss=9.95633
Epoch 7/80: current_loss=9.92376 | best_loss=9.92376
Epoch 8/80: current_loss=9.91252 | best_loss=9.91252
Epoch 9/80: current_loss=9.90555 | best_loss=9.90555
Epoch 10/80: current_loss=9.90882 | best_loss=9.90555
Epoch 11/80: current_loss=9.91531 | best_loss=9.90555
Epoch 12/80: current_loss=9.89980 | best_loss=9.89980
Epoch 13/80: current_loss=9.89512 | best_loss=9.89512
Epoch 14/80: current_loss=9.89824 | best_loss=9.89512
Epoch 15/80: current_loss=9.89970 | best_loss=9.89512
Epoch 16/80: current_loss=9.89598 | best_loss=9.89512
Epoch 17/80: current_loss=9.89963 | best_loss=9.89512
Epoch 18/80: current_loss=9.89353 | best_loss=9.89353
Epoch 19/80: current_loss=9.89535 | best_loss=9.89353
Epoch 20/80: current_loss=9.90138 | best_loss=9.89353
Epoch 21/80: current_loss=9.89690 | best_loss=9.89353
Epoch 22/80: current_loss=9.89521 | best_loss=9.89353
Epoch 23/80: current_loss=9.90290 | best_loss=9.89353
Epoch 24/80: current_loss=9.89458 | best_loss=9.89353
Epoch 25/80: current_loss=9.90291 | best_loss=9.89353
Epoch 26/80: current_loss=9.89662 | best_loss=9.89353
Epoch 27/80: current_loss=9.89281 | best_loss=9.89281
Epoch 28/80: current_loss=9.89771 | best_loss=9.89281
Epoch 29/80: current_loss=9.89178 | best_loss=9.89178
Epoch 30/80: current_loss=9.89147 | best_loss=9.89147
Epoch 31/80: current_loss=9.89648 | best_loss=9.89147
Epoch 32/80: current_loss=9.89340 | best_loss=9.89147
Epoch 33/80: current_loss=9.89259 | best_loss=9.89147
Epoch 34/80: current_loss=9.90519 | best_loss=9.89147
Epoch 35/80: current_loss=9.89807 | best_loss=9.89147
Epoch 36/80: current_loss=9.89164 | best_loss=9.89147
Epoch 37/80: current_loss=9.89735 | best_loss=9.89147
Epoch 38/80: current_loss=9.89293 | best_loss=9.89147
Epoch 39/80: current_loss=9.89575 | best_loss=9.89147
Epoch 40/80: current_loss=9.90504 | best_loss=9.89147
Epoch 41/80: current_loss=9.90077 | best_loss=9.89147
Epoch 42/80: current_loss=9.90543 | best_loss=9.89147
Epoch 43/80: current_loss=9.90479 | best_loss=9.89147
Epoch 44/80: current_loss=9.91494 | best_loss=9.89147
Epoch 45/80: current_loss=9.89859 | best_loss=9.89147
Epoch 46/80: current_loss=9.90281 | best_loss=9.89147
Epoch 47/80: current_loss=9.90914 | best_loss=9.89147
Epoch 48/80: current_loss=9.90135 | best_loss=9.89147
Epoch 49/80: current_loss=9.89628 | best_loss=9.89147
Epoch 50/80: current_loss=9.89103 | best_loss=9.89103
Epoch 51/80: current_loss=9.89821 | best_loss=9.89103
Epoch 52/80: current_loss=9.88835 | best_loss=9.88835
Epoch 53/80: current_loss=9.91377 | best_loss=9.88835
Epoch 54/80: current_loss=9.90522 | best_loss=9.88835
Epoch 55/80: current_loss=9.89217 | best_loss=9.88835
Epoch 56/80: current_loss=9.89579 | best_loss=9.88835
Epoch 57/80: current_loss=9.89023 | best_loss=9.88835
Epoch 58/80: current_loss=9.90624 | best_loss=9.88835
Epoch 59/80: current_loss=9.90586 | best_loss=9.88835
Epoch 60/80: current_loss=9.88964 | best_loss=9.88835
Epoch 61/80: current_loss=9.89238 | best_loss=9.88835
Epoch 62/80: current_loss=9.90209 | best_loss=9.88835
Epoch 63/80: current_loss=9.89019 | best_loss=9.88835
Epoch 64/80: current_loss=9.89148 | best_loss=9.88835
Epoch 65/80: current_loss=9.91164 | best_loss=9.88835
Epoch 66/80: current_loss=9.90305 | best_loss=9.88835
Epoch 67/80: current_loss=9.88928 | best_loss=9.88835
Epoch 68/80: current_loss=9.90154 | best_loss=9.88835
Epoch 69/80: current_loss=9.91422 | best_loss=9.88835
Epoch 70/80: current_loss=9.90918 | best_loss=9.88835
Epoch 71/80: current_loss=9.91645 | best_loss=9.88835
Epoch 72/80: current_loss=9.88974 | best_loss=9.88835
Early Stopping at epoch 72
      explained_var=0.00227 | mse_loss=9.55921
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.48645 | best_loss=8.48645
Epoch 1/80: current_loss=8.48610 | best_loss=8.48610
Epoch 2/80: current_loss=8.50283 | best_loss=8.48610
Epoch 3/80: current_loss=8.50653 | best_loss=8.48610
Epoch 4/80: current_loss=8.50212 | best_loss=8.48610
Epoch 5/80: current_loss=8.47996 | best_loss=8.47996
Epoch 6/80: current_loss=8.47463 | best_loss=8.47463
Epoch 7/80: current_loss=8.53791 | best_loss=8.47463
Epoch 8/80: current_loss=8.49570 | best_loss=8.47463
Epoch 9/80: current_loss=8.51027 | best_loss=8.47463
Epoch 10/80: current_loss=8.47599 | best_loss=8.47463
Epoch 11/80: current_loss=8.47241 | best_loss=8.47241
Epoch 12/80: current_loss=8.51636 | best_loss=8.47241
Epoch 13/80: current_loss=8.48069 | best_loss=8.47241
Epoch 14/80: current_loss=8.50801 | best_loss=8.47241
Epoch 15/80: current_loss=8.50634 | best_loss=8.47241
Epoch 16/80: current_loss=8.53323 | best_loss=8.47241
Epoch 17/80: current_loss=8.48731 | best_loss=8.47241
Epoch 18/80: current_loss=8.47955 | best_loss=8.47241
Epoch 19/80: current_loss=8.61970 | best_loss=8.47241
Epoch 20/80: current_loss=8.49913 | best_loss=8.47241
Epoch 21/80: current_loss=8.48688 | best_loss=8.47241
Epoch 22/80: current_loss=8.47613 | best_loss=8.47241
Epoch 23/80: current_loss=8.48422 | best_loss=8.47241
Epoch 24/80: current_loss=8.48648 | best_loss=8.47241
Epoch 25/80: current_loss=8.52181 | best_loss=8.47241
Epoch 26/80: current_loss=8.49247 | best_loss=8.47241
Epoch 27/80: current_loss=8.51226 | best_loss=8.47241
Epoch 28/80: current_loss=8.47367 | best_loss=8.47241
Epoch 29/80: current_loss=8.46836 | best_loss=8.46836
Epoch 30/80: current_loss=8.51294 | best_loss=8.46836
Epoch 31/80: current_loss=8.48743 | best_loss=8.46836
Epoch 32/80: current_loss=8.51204 | best_loss=8.46836
Epoch 33/80: current_loss=8.48536 | best_loss=8.46836
Epoch 34/80: current_loss=8.47864 | best_loss=8.46836
Epoch 35/80: current_loss=8.49180 | best_loss=8.46836
Epoch 36/80: current_loss=8.47711 | best_loss=8.46836
Epoch 37/80: current_loss=8.51112 | best_loss=8.46836
Epoch 38/80: current_loss=8.49142 | best_loss=8.46836
Epoch 39/80: current_loss=8.49825 | best_loss=8.46836
Epoch 40/80: current_loss=8.49825 | best_loss=8.46836
Epoch 41/80: current_loss=8.49103 | best_loss=8.46836
Epoch 42/80: current_loss=8.48429 | best_loss=8.46836
Epoch 43/80: current_loss=8.48677 | best_loss=8.46836
Epoch 44/80: current_loss=8.48617 | best_loss=8.46836
Epoch 45/80: current_loss=8.51450 | best_loss=8.46836
Epoch 46/80: current_loss=8.47762 | best_loss=8.46836
Epoch 47/80: current_loss=8.48580 | best_loss=8.46836
Epoch 48/80: current_loss=8.47687 | best_loss=8.46836
Epoch 49/80: current_loss=8.50817 | best_loss=8.46836
Early Stopping at epoch 49
      explained_var=0.00150 | mse_loss=8.56338
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.41834 | best_loss=9.41834
Epoch 1/80: current_loss=9.51517 | best_loss=9.41834
Epoch 2/80: current_loss=9.54103 | best_loss=9.41834
Epoch 3/80: current_loss=9.57270 | best_loss=9.41834
Epoch 4/80: current_loss=9.41382 | best_loss=9.41382
Epoch 5/80: current_loss=9.59266 | best_loss=9.41382
Epoch 6/80: current_loss=9.47152 | best_loss=9.41382
Epoch 7/80: current_loss=9.58043 | best_loss=9.41382
Epoch 8/80: current_loss=9.40563 | best_loss=9.40563
Epoch 9/80: current_loss=9.49322 | best_loss=9.40563
Epoch 10/80: current_loss=9.47419 | best_loss=9.40563
Epoch 11/80: current_loss=9.52874 | best_loss=9.40563
Epoch 12/80: current_loss=9.51354 | best_loss=9.40563
Epoch 13/80: current_loss=9.45282 | best_loss=9.40563
Epoch 14/80: current_loss=9.43806 | best_loss=9.40563
Epoch 15/80: current_loss=9.43689 | best_loss=9.40563
Epoch 16/80: current_loss=9.42670 | best_loss=9.40563
Epoch 17/80: current_loss=9.45380 | best_loss=9.40563
Epoch 18/80: current_loss=9.51864 | best_loss=9.40563
Epoch 19/80: current_loss=9.49010 | best_loss=9.40563
Epoch 20/80: current_loss=9.68226 | best_loss=9.40563
Epoch 21/80: current_loss=9.53602 | best_loss=9.40563
Epoch 22/80: current_loss=9.52603 | best_loss=9.40563
Epoch 23/80: current_loss=9.50041 | best_loss=9.40563
Epoch 24/80: current_loss=9.42862 | best_loss=9.40563
Epoch 25/80: current_loss=9.61883 | best_loss=9.40563
Epoch 26/80: current_loss=9.49228 | best_loss=9.40563
Epoch 27/80: current_loss=9.46698 | best_loss=9.40563
Epoch 28/80: current_loss=9.53626 | best_loss=9.40563
Early Stopping at epoch 28
      explained_var=-0.00662 | mse_loss=9.55284
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.96226 | best_loss=8.96226
Epoch 1/80: current_loss=8.97255 | best_loss=8.96226
Epoch 2/80: current_loss=8.94477 | best_loss=8.94477
Epoch 3/80: current_loss=9.01143 | best_loss=8.94477
Epoch 4/80: current_loss=8.92965 | best_loss=8.92965
Epoch 5/80: current_loss=8.95292 | best_loss=8.92965
Epoch 6/80: current_loss=8.93365 | best_loss=8.92965
Epoch 7/80: current_loss=8.96151 | best_loss=8.92965
Epoch 8/80: current_loss=8.95279 | best_loss=8.92965
Epoch 9/80: current_loss=8.93710 | best_loss=8.92965
Epoch 10/80: current_loss=8.94970 | best_loss=8.92965
Epoch 11/80: current_loss=8.99830 | best_loss=8.92965
Epoch 12/80: current_loss=8.93869 | best_loss=8.92965
Epoch 13/80: current_loss=8.94560 | best_loss=8.92965
Epoch 14/80: current_loss=8.96279 | best_loss=8.92965
Epoch 15/80: current_loss=9.02251 | best_loss=8.92965
Epoch 16/80: current_loss=8.96401 | best_loss=8.92965
Epoch 17/80: current_loss=8.94598 | best_loss=8.92965
Epoch 18/80: current_loss=8.92694 | best_loss=8.92694
Epoch 19/80: current_loss=8.94017 | best_loss=8.92694
Epoch 20/80: current_loss=8.93006 | best_loss=8.92694
Epoch 21/80: current_loss=8.97715 | best_loss=8.92694
Epoch 22/80: current_loss=8.93707 | best_loss=8.92694
Epoch 23/80: current_loss=8.94694 | best_loss=8.92694
Epoch 24/80: current_loss=8.95221 | best_loss=8.92694
Epoch 25/80: current_loss=8.96239 | best_loss=8.92694
Epoch 26/80: current_loss=8.93400 | best_loss=8.92694
Epoch 27/80: current_loss=8.94852 | best_loss=8.92694
Epoch 28/80: current_loss=8.95989 | best_loss=8.92694
Epoch 29/80: current_loss=8.96008 | best_loss=8.92694
Epoch 30/80: current_loss=8.94028 | best_loss=8.92694
Epoch 31/80: current_loss=8.95384 | best_loss=8.92694
Epoch 32/80: current_loss=8.99109 | best_loss=8.92694
Epoch 33/80: current_loss=8.95291 | best_loss=8.92694
Epoch 34/80: current_loss=8.94294 | best_loss=8.92694
Epoch 35/80: current_loss=8.96380 | best_loss=8.92694
Epoch 36/80: current_loss=8.95340 | best_loss=8.92694
Epoch 37/80: current_loss=8.93721 | best_loss=8.92694
Epoch 38/80: current_loss=8.95060 | best_loss=8.92694
Early Stopping at epoch 38
      explained_var=0.00183 | mse_loss=8.34665
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.48273 | best_loss=8.48273
Epoch 1/80: current_loss=8.49366 | best_loss=8.48273
Epoch 2/80: current_loss=8.48714 | best_loss=8.48273
Epoch 3/80: current_loss=8.49802 | best_loss=8.48273
Epoch 4/80: current_loss=8.48388 | best_loss=8.48273
Epoch 5/80: current_loss=8.48114 | best_loss=8.48114
Epoch 6/80: current_loss=8.48120 | best_loss=8.48114
Epoch 7/80: current_loss=8.48196 | best_loss=8.48114
Epoch 8/80: current_loss=8.49680 | best_loss=8.48114
Epoch 9/80: current_loss=8.48281 | best_loss=8.48114
Epoch 10/80: current_loss=8.48250 | best_loss=8.48114
Epoch 11/80: current_loss=8.48239 | best_loss=8.48114
Epoch 12/80: current_loss=8.48804 | best_loss=8.48114
Epoch 13/80: current_loss=8.48520 | best_loss=8.48114
Epoch 14/80: current_loss=8.48308 | best_loss=8.48114
Epoch 15/80: current_loss=8.49354 | best_loss=8.48114
Epoch 16/80: current_loss=8.49097 | best_loss=8.48114
Epoch 17/80: current_loss=8.48360 | best_loss=8.48114
Epoch 18/80: current_loss=8.48244 | best_loss=8.48114
Epoch 19/80: current_loss=8.48626 | best_loss=8.48114
Epoch 20/80: current_loss=8.48594 | best_loss=8.48114
Epoch 21/80: current_loss=8.48123 | best_loss=8.48114
Epoch 22/80: current_loss=8.48343 | best_loss=8.48114
Epoch 23/80: current_loss=8.49071 | best_loss=8.48114
Epoch 24/80: current_loss=8.48750 | best_loss=8.48114
Epoch 25/80: current_loss=8.48497 | best_loss=8.48114
Early Stopping at epoch 25
      explained_var=0.00012 | mse_loss=8.19954
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=-0.00018| avg_loss=8.84432
----------------------------------------------


----------------------------------------------
Params for Trial 5
{'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=12.24320 | best_loss=12.24320
Epoch 1/80: current_loss=11.71383 | best_loss=11.71383
Epoch 2/80: current_loss=11.44441 | best_loss=11.44441
Epoch 3/80: current_loss=11.26806 | best_loss=11.26806
Epoch 4/80: current_loss=11.11115 | best_loss=11.11115
Epoch 5/80: current_loss=11.02072 | best_loss=11.02072
Epoch 6/80: current_loss=10.90406 | best_loss=10.90406
Epoch 7/80: current_loss=10.85764 | best_loss=10.85764
Epoch 8/80: current_loss=10.82374 | best_loss=10.82374
Epoch 9/80: current_loss=10.73187 | best_loss=10.73187
Epoch 10/80: current_loss=10.67463 | best_loss=10.67463
Epoch 11/80: current_loss=10.58608 | best_loss=10.58608
Epoch 12/80: current_loss=10.52397 | best_loss=10.52397
Epoch 13/80: current_loss=10.47611 | best_loss=10.47611
Epoch 14/80: current_loss=10.40522 | best_loss=10.40522
Epoch 15/80: current_loss=10.35086 | best_loss=10.35086
Epoch 16/80: current_loss=10.31404 | best_loss=10.31404
Epoch 17/80: current_loss=10.28117 | best_loss=10.28117
Epoch 18/80: current_loss=10.29645 | best_loss=10.28117
Epoch 19/80: current_loss=10.23454 | best_loss=10.23454
Epoch 20/80: current_loss=10.27687 | best_loss=10.23454
Epoch 21/80: current_loss=10.13931 | best_loss=10.13931
Epoch 22/80: current_loss=10.10214 | best_loss=10.10214
Epoch 23/80: current_loss=10.09641 | best_loss=10.09641
Epoch 24/80: current_loss=10.03770 | best_loss=10.03770
Epoch 25/80: current_loss=10.09011 | best_loss=10.03770
Epoch 26/80: current_loss=10.02732 | best_loss=10.02732
Epoch 27/80: current_loss=10.28300 | best_loss=10.02732
Epoch 28/80: current_loss=9.98215 | best_loss=9.98215
Epoch 29/80: current_loss=9.96419 | best_loss=9.96419
Epoch 30/80: current_loss=10.01424 | best_loss=9.96419
Epoch 31/80: current_loss=10.17790 | best_loss=9.96419
Epoch 32/80: current_loss=10.03897 | best_loss=9.96419
Epoch 33/80: current_loss=9.93270 | best_loss=9.93270
Epoch 34/80: current_loss=9.91677 | best_loss=9.91677
Epoch 35/80: current_loss=9.93420 | best_loss=9.91677
Epoch 36/80: current_loss=9.90678 | best_loss=9.90678
Epoch 37/80: current_loss=9.93165 | best_loss=9.90678
Epoch 38/80: current_loss=9.92405 | best_loss=9.90678
Epoch 39/80: current_loss=9.90303 | best_loss=9.90303
Epoch 40/80: current_loss=10.13652 | best_loss=9.90303
Epoch 41/80: current_loss=10.12385 | best_loss=9.90303
Epoch 42/80: current_loss=9.99695 | best_loss=9.90303
Epoch 43/80: current_loss=9.93447 | best_loss=9.90303
Epoch 44/80: current_loss=10.22694 | best_loss=9.90303
Epoch 45/80: current_loss=10.33376 | best_loss=9.90303
Epoch 46/80: current_loss=9.89371 | best_loss=9.89371
Epoch 47/80: current_loss=9.97229 | best_loss=9.89371
Epoch 48/80: current_loss=9.90373 | best_loss=9.89371
Epoch 49/80: current_loss=9.89057 | best_loss=9.89057
Epoch 50/80: current_loss=9.90398 | best_loss=9.89057
Epoch 51/80: current_loss=9.92809 | best_loss=9.89057
Epoch 52/80: current_loss=9.93834 | best_loss=9.89057
Epoch 53/80: current_loss=9.89661 | best_loss=9.89057
Epoch 54/80: current_loss=9.90119 | best_loss=9.89057
Epoch 55/80: current_loss=9.93393 | best_loss=9.89057
Epoch 56/80: current_loss=9.97141 | best_loss=9.89057
Epoch 57/80: current_loss=10.05913 | best_loss=9.89057
Epoch 58/80: current_loss=9.98087 | best_loss=9.89057
Epoch 59/80: current_loss=9.92649 | best_loss=9.89057
Epoch 60/80: current_loss=10.11388 | best_loss=9.89057
Epoch 61/80: current_loss=9.87607 | best_loss=9.87607
Epoch 62/80: current_loss=9.87616 | best_loss=9.87607
Epoch 63/80: current_loss=9.93807 | best_loss=9.87607
Epoch 64/80: current_loss=9.88746 | best_loss=9.87607
Epoch 65/80: current_loss=10.08701 | best_loss=9.87607
Epoch 66/80: current_loss=9.88204 | best_loss=9.87607
Epoch 67/80: current_loss=9.89464 | best_loss=9.87607
Epoch 68/80: current_loss=9.89402 | best_loss=9.87607
Epoch 69/80: current_loss=9.90840 | best_loss=9.87607
Epoch 70/80: current_loss=10.07468 | best_loss=9.87607
Epoch 71/80: current_loss=9.89101 | best_loss=9.87607
Epoch 72/80: current_loss=9.90578 | best_loss=9.87607
Epoch 73/80: current_loss=9.93535 | best_loss=9.87607
Epoch 74/80: current_loss=9.88246 | best_loss=9.87607
Epoch 75/80: current_loss=9.91524 | best_loss=9.87607
Epoch 76/80: current_loss=9.91422 | best_loss=9.87607
Epoch 77/80: current_loss=10.22377 | best_loss=9.87607
Epoch 78/80: current_loss=9.88720 | best_loss=9.87607
Epoch 79/80: current_loss=9.91042 | best_loss=9.87607
      explained_var=0.00269 | mse_loss=9.55633
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.52374 | best_loss=8.52374
Epoch 1/80: current_loss=8.52350 | best_loss=8.52350
Epoch 2/80: current_loss=8.51309 | best_loss=8.51309
Epoch 3/80: current_loss=8.52537 | best_loss=8.51309
Epoch 4/80: current_loss=8.55411 | best_loss=8.51309
Epoch 5/80: current_loss=8.53160 | best_loss=8.51309
Epoch 6/80: current_loss=8.48385 | best_loss=8.48385
Epoch 7/80: current_loss=8.46890 | best_loss=8.46890
Epoch 8/80: current_loss=8.48891 | best_loss=8.46890
Epoch 9/80: current_loss=8.49580 | best_loss=8.46890
Epoch 10/80: current_loss=8.55793 | best_loss=8.46890
Epoch 11/80: current_loss=8.52021 | best_loss=8.46890
Epoch 12/80: current_loss=8.47152 | best_loss=8.46890
Epoch 13/80: current_loss=8.55273 | best_loss=8.46890
Epoch 14/80: current_loss=8.52091 | best_loss=8.46890
Epoch 15/80: current_loss=8.48480 | best_loss=8.46890
Epoch 16/80: current_loss=8.58180 | best_loss=8.46890
Epoch 17/80: current_loss=8.48908 | best_loss=8.46890
Epoch 18/80: current_loss=8.51515 | best_loss=8.46890
Epoch 19/80: current_loss=8.45384 | best_loss=8.45384
Epoch 20/80: current_loss=8.54223 | best_loss=8.45384
Epoch 21/80: current_loss=8.46925 | best_loss=8.45384
Epoch 22/80: current_loss=8.46156 | best_loss=8.45384
Epoch 23/80: current_loss=8.48528 | best_loss=8.45384
Epoch 24/80: current_loss=8.55243 | best_loss=8.45384
Epoch 25/80: current_loss=8.46011 | best_loss=8.45384
Epoch 26/80: current_loss=8.65676 | best_loss=8.45384
Epoch 27/80: current_loss=8.45924 | best_loss=8.45384
Epoch 28/80: current_loss=8.46007 | best_loss=8.45384
Epoch 29/80: current_loss=8.52608 | best_loss=8.45384
Epoch 30/80: current_loss=8.52790 | best_loss=8.45384
Epoch 31/80: current_loss=8.62545 | best_loss=8.45384
Epoch 32/80: current_loss=8.52927 | best_loss=8.45384
Epoch 33/80: current_loss=8.50722 | best_loss=8.45384
Epoch 34/80: current_loss=8.50609 | best_loss=8.45384
Epoch 35/80: current_loss=8.50990 | best_loss=8.45384
Epoch 36/80: current_loss=8.48972 | best_loss=8.45384
Epoch 37/80: current_loss=8.51478 | best_loss=8.45384
Epoch 38/80: current_loss=8.47902 | best_loss=8.45384
Epoch 39/80: current_loss=8.47448 | best_loss=8.45384
Early Stopping at epoch 39
      explained_var=-0.00038 | mse_loss=8.58029
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.34582 | best_loss=9.34582
Epoch 1/80: current_loss=9.41005 | best_loss=9.34582
Epoch 2/80: current_loss=9.50368 | best_loss=9.34582
Epoch 3/80: current_loss=9.57691 | best_loss=9.34582
Epoch 4/80: current_loss=9.93908 | best_loss=9.34582
Epoch 5/80: current_loss=9.49440 | best_loss=9.34582
Epoch 6/80: current_loss=9.46853 | best_loss=9.34582
Epoch 7/80: current_loss=9.40825 | best_loss=9.34582
Epoch 8/80: current_loss=9.51836 | best_loss=9.34582
Epoch 9/80: current_loss=9.40348 | best_loss=9.34582
Epoch 10/80: current_loss=9.55456 | best_loss=9.34582
Epoch 11/80: current_loss=9.46492 | best_loss=9.34582
Epoch 12/80: current_loss=9.52932 | best_loss=9.34582
Epoch 13/80: current_loss=9.40056 | best_loss=9.34582
Epoch 14/80: current_loss=9.45271 | best_loss=9.34582
Epoch 15/80: current_loss=9.43552 | best_loss=9.34582
Epoch 16/80: current_loss=9.45883 | best_loss=9.34582
Epoch 17/80: current_loss=9.57645 | best_loss=9.34582
Epoch 18/80: current_loss=9.66349 | best_loss=9.34582
Epoch 19/80: current_loss=9.50003 | best_loss=9.34582
Epoch 20/80: current_loss=9.51133 | best_loss=9.34582
Early Stopping at epoch 20
      explained_var=-0.01395 | mse_loss=9.51484
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.12271 | best_loss=9.12271
Epoch 1/80: current_loss=8.96285 | best_loss=8.96285
Epoch 2/80: current_loss=8.94112 | best_loss=8.94112
Epoch 3/80: current_loss=8.94899 | best_loss=8.94112
Epoch 4/80: current_loss=8.92160 | best_loss=8.92160
Epoch 5/80: current_loss=9.11323 | best_loss=8.92160
Epoch 6/80: current_loss=8.93371 | best_loss=8.92160
Epoch 7/80: current_loss=8.92939 | best_loss=8.92160
Epoch 8/80: current_loss=8.92480 | best_loss=8.92160
Epoch 9/80: current_loss=8.92810 | best_loss=8.92160
Epoch 10/80: current_loss=9.00401 | best_loss=8.92160
Epoch 11/80: current_loss=8.93145 | best_loss=8.92160
Epoch 12/80: current_loss=8.91691 | best_loss=8.91691
Epoch 13/80: current_loss=8.92965 | best_loss=8.91691
Epoch 14/80: current_loss=9.05385 | best_loss=8.91691
Epoch 15/80: current_loss=8.93583 | best_loss=8.91691
Epoch 16/80: current_loss=9.08905 | best_loss=8.91691
Epoch 17/80: current_loss=9.06903 | best_loss=8.91691
Epoch 18/80: current_loss=8.94859 | best_loss=8.91691
Epoch 19/80: current_loss=9.06258 | best_loss=8.91691
Epoch 20/80: current_loss=9.01578 | best_loss=8.91691
Epoch 21/80: current_loss=8.99799 | best_loss=8.91691
Epoch 22/80: current_loss=8.94630 | best_loss=8.91691
Epoch 23/80: current_loss=9.19418 | best_loss=8.91691
Epoch 24/80: current_loss=8.92073 | best_loss=8.91691
Epoch 25/80: current_loss=9.06730 | best_loss=8.91691
Epoch 26/80: current_loss=8.94888 | best_loss=8.91691
Epoch 27/80: current_loss=8.91424 | best_loss=8.91424
Epoch 28/80: current_loss=9.07082 | best_loss=8.91424
Epoch 29/80: current_loss=8.90849 | best_loss=8.90849
Epoch 30/80: current_loss=8.91367 | best_loss=8.90849
Epoch 31/80: current_loss=8.93642 | best_loss=8.90849
Epoch 32/80: current_loss=8.91256 | best_loss=8.90849
Epoch 33/80: current_loss=9.06139 | best_loss=8.90849
Epoch 34/80: current_loss=8.91612 | best_loss=8.90849
Epoch 35/80: current_loss=8.91266 | best_loss=8.90849
Epoch 36/80: current_loss=8.93248 | best_loss=8.90849
Epoch 37/80: current_loss=8.92332 | best_loss=8.90849
Epoch 38/80: current_loss=8.98571 | best_loss=8.90849
Epoch 39/80: current_loss=8.95344 | best_loss=8.90849
Epoch 40/80: current_loss=8.91201 | best_loss=8.90849
Epoch 41/80: current_loss=8.92496 | best_loss=8.90849
Epoch 42/80: current_loss=8.91734 | best_loss=8.90849
Epoch 43/80: current_loss=8.93424 | best_loss=8.90849
Epoch 44/80: current_loss=8.92322 | best_loss=8.90849
Epoch 45/80: current_loss=8.90591 | best_loss=8.90591
Epoch 46/80: current_loss=9.03917 | best_loss=8.90591
Epoch 47/80: current_loss=8.96158 | best_loss=8.90591
Epoch 48/80: current_loss=8.91174 | best_loss=8.90591
Epoch 49/80: current_loss=8.95126 | best_loss=8.90591
Epoch 50/80: current_loss=8.90620 | best_loss=8.90591
Epoch 51/80: current_loss=8.92232 | best_loss=8.90591
Epoch 52/80: current_loss=9.02858 | best_loss=8.90591
Epoch 53/80: current_loss=9.01420 | best_loss=8.90591
Epoch 54/80: current_loss=8.97672 | best_loss=8.90591
Epoch 55/80: current_loss=8.92703 | best_loss=8.90591
Epoch 56/80: current_loss=8.94832 | best_loss=8.90591
Epoch 57/80: current_loss=8.98344 | best_loss=8.90591
Epoch 58/80: current_loss=8.94435 | best_loss=8.90591
Epoch 59/80: current_loss=8.95432 | best_loss=8.90591
Epoch 60/80: current_loss=8.92526 | best_loss=8.90591
Epoch 61/80: current_loss=9.00051 | best_loss=8.90591
Epoch 62/80: current_loss=8.93863 | best_loss=8.90591
Epoch 63/80: current_loss=8.92475 | best_loss=8.90591
Epoch 64/80: current_loss=8.94358 | best_loss=8.90591
Epoch 65/80: current_loss=9.15362 | best_loss=8.90591
Early Stopping at epoch 65
      explained_var=0.00173 | mse_loss=8.34740
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.53703 | best_loss=8.53703
Epoch 1/80: current_loss=8.49284 | best_loss=8.49284
Epoch 2/80: current_loss=8.49648 | best_loss=8.49284
Epoch 3/80: current_loss=8.50457 | best_loss=8.49284
Epoch 4/80: current_loss=8.54148 | best_loss=8.49284
Epoch 5/80: current_loss=8.55789 | best_loss=8.49284
Epoch 6/80: current_loss=8.54120 | best_loss=8.49284
Epoch 7/80: current_loss=8.51196 | best_loss=8.49284
Epoch 8/80: current_loss=8.49340 | best_loss=8.49284
Epoch 9/80: current_loss=8.52659 | best_loss=8.49284
Epoch 10/80: current_loss=8.58268 | best_loss=8.49284
Epoch 11/80: current_loss=8.53393 | best_loss=8.49284
Epoch 12/80: current_loss=8.52382 | best_loss=8.49284
Epoch 13/80: current_loss=8.65934 | best_loss=8.49284
Epoch 14/80: current_loss=8.45956 | best_loss=8.45956
Epoch 15/80: current_loss=8.50369 | best_loss=8.45956
Epoch 16/80: current_loss=8.53568 | best_loss=8.45956
Epoch 17/80: current_loss=8.51766 | best_loss=8.45956
Epoch 18/80: current_loss=8.48848 | best_loss=8.45956
Epoch 19/80: current_loss=8.55846 | best_loss=8.45956
Epoch 20/80: current_loss=8.50562 | best_loss=8.45956
Epoch 21/80: current_loss=8.49915 | best_loss=8.45956
Epoch 22/80: current_loss=8.51460 | best_loss=8.45956
Epoch 23/80: current_loss=8.55945 | best_loss=8.45956
Epoch 24/80: current_loss=8.47048 | best_loss=8.45956
Epoch 25/80: current_loss=8.46590 | best_loss=8.45956
Epoch 26/80: current_loss=8.46569 | best_loss=8.45956
Epoch 27/80: current_loss=8.48218 | best_loss=8.45956
Epoch 28/80: current_loss=8.52182 | best_loss=8.45956
Epoch 29/80: current_loss=8.50156 | best_loss=8.45956
Epoch 30/80: current_loss=8.56024 | best_loss=8.45956
Epoch 31/80: current_loss=8.52847 | best_loss=8.45956
Epoch 32/80: current_loss=8.50991 | best_loss=8.45956
Epoch 33/80: current_loss=8.47469 | best_loss=8.45956
Epoch 34/80: current_loss=8.56609 | best_loss=8.45956
Early Stopping at epoch 34
      explained_var=-0.00127 | mse_loss=8.22105
----------------------------------------------
Average early_stopping_point: 31| avg_exp_var=-0.00223| avg_loss=8.84398
----------------------------------------------


----------------------------------------------
Params for Trial 6
{'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=69.42405 | best_loss=69.42405
Epoch 1/80: current_loss=48.25023 | best_loss=48.25023
Epoch 2/80: current_loss=34.67242 | best_loss=34.67242
Epoch 3/80: current_loss=28.38907 | best_loss=28.38907
Epoch 4/80: current_loss=24.38753 | best_loss=24.38753
Epoch 5/80: current_loss=21.65167 | best_loss=21.65167
Epoch 6/80: current_loss=19.61818 | best_loss=19.61818
Epoch 7/80: current_loss=17.97975 | best_loss=17.97975
Epoch 8/80: current_loss=16.64908 | best_loss=16.64908
Epoch 9/80: current_loss=15.52442 | best_loss=15.52442
Epoch 10/80: current_loss=14.60938 | best_loss=14.60938
Epoch 11/80: current_loss=13.81451 | best_loss=13.81451
Epoch 12/80: current_loss=13.16391 | best_loss=13.16391
Epoch 13/80: current_loss=12.61056 | best_loss=12.61056
Epoch 14/80: current_loss=12.16638 | best_loss=12.16638
Epoch 15/80: current_loss=11.79773 | best_loss=11.79773
Epoch 16/80: current_loss=11.49736 | best_loss=11.49736
Epoch 17/80: current_loss=11.25045 | best_loss=11.25045
Epoch 18/80: current_loss=11.05255 | best_loss=11.05255
Epoch 19/80: current_loss=10.89035 | best_loss=10.89035
Epoch 20/80: current_loss=10.75063 | best_loss=10.75063
Epoch 21/80: current_loss=10.64288 | best_loss=10.64288
Epoch 22/80: current_loss=10.55882 | best_loss=10.55882
Epoch 23/80: current_loss=10.48302 | best_loss=10.48302
Epoch 24/80: current_loss=10.41864 | best_loss=10.41864
Epoch 25/80: current_loss=10.37357 | best_loss=10.37357
Epoch 26/80: current_loss=10.33056 | best_loss=10.33056
Epoch 27/80: current_loss=10.29753 | best_loss=10.29753
Epoch 28/80: current_loss=10.26547 | best_loss=10.26547
Epoch 29/80: current_loss=10.23870 | best_loss=10.23870
Epoch 30/80: current_loss=10.21686 | best_loss=10.21686
Epoch 31/80: current_loss=10.19447 | best_loss=10.19447
Epoch 32/80: current_loss=10.17385 | best_loss=10.17385
Epoch 33/80: current_loss=10.15730 | best_loss=10.15730
Epoch 34/80: current_loss=10.14125 | best_loss=10.14125
Epoch 35/80: current_loss=10.12303 | best_loss=10.12303
Epoch 36/80: current_loss=10.11091 | best_loss=10.11091
Epoch 37/80: current_loss=10.09829 | best_loss=10.09829
Epoch 38/80: current_loss=10.08320 | best_loss=10.08320
Epoch 39/80: current_loss=10.06709 | best_loss=10.06709
Epoch 40/80: current_loss=10.05713 | best_loss=10.05713
Epoch 41/80: current_loss=10.04793 | best_loss=10.04793
Epoch 42/80: current_loss=10.04042 | best_loss=10.04042
Epoch 43/80: current_loss=10.02797 | best_loss=10.02797
Epoch 44/80: current_loss=10.01685 | best_loss=10.01685
Epoch 45/80: current_loss=10.01314 | best_loss=10.01314
Epoch 46/80: current_loss=9.99977 | best_loss=9.99977
Epoch 47/80: current_loss=9.99194 | best_loss=9.99194
Epoch 48/80: current_loss=9.98445 | best_loss=9.98445
Epoch 49/80: current_loss=9.97960 | best_loss=9.97960
Epoch 50/80: current_loss=9.97176 | best_loss=9.97176
Epoch 51/80: current_loss=9.96811 | best_loss=9.96811
Epoch 52/80: current_loss=9.96085 | best_loss=9.96085
Epoch 53/80: current_loss=9.96033 | best_loss=9.96033
Epoch 54/80: current_loss=9.95842 | best_loss=9.95842
Epoch 55/80: current_loss=9.95202 | best_loss=9.95202
Epoch 56/80: current_loss=9.94852 | best_loss=9.94852
Epoch 57/80: current_loss=9.94756 | best_loss=9.94756
Epoch 58/80: current_loss=9.94081 | best_loss=9.94081
Epoch 59/80: current_loss=9.93624 | best_loss=9.93624
Epoch 60/80: current_loss=9.93756 | best_loss=9.93624
Epoch 61/80: current_loss=9.93450 | best_loss=9.93450
Epoch 62/80: current_loss=9.93117 | best_loss=9.93117
Epoch 63/80: current_loss=9.93285 | best_loss=9.93117
Epoch 64/80: current_loss=9.92895 | best_loss=9.92895
Epoch 65/80: current_loss=9.93139 | best_loss=9.92895
Epoch 66/80: current_loss=9.92572 | best_loss=9.92572
Epoch 67/80: current_loss=9.92537 | best_loss=9.92537
Epoch 68/80: current_loss=9.92313 | best_loss=9.92313
Epoch 69/80: current_loss=9.92057 | best_loss=9.92057
Epoch 70/80: current_loss=9.92197 | best_loss=9.92057
Epoch 71/80: current_loss=9.92157 | best_loss=9.92057
Epoch 72/80: current_loss=9.91884 | best_loss=9.91884
Epoch 73/80: current_loss=9.91682 | best_loss=9.91682
Epoch 74/80: current_loss=9.91270 | best_loss=9.91270
Epoch 75/80: current_loss=9.91757 | best_loss=9.91270
Epoch 76/80: current_loss=9.91508 | best_loss=9.91270
Epoch 77/80: current_loss=9.91659 | best_loss=9.91270
Epoch 78/80: current_loss=9.91862 | best_loss=9.91270
Epoch 79/80: current_loss=9.91369 | best_loss=9.91270
      explained_var=0.00212 | mse_loss=9.58648
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.45726 | best_loss=8.45726
Epoch 1/80: current_loss=8.45815 | best_loss=8.45726
Epoch 2/80: current_loss=8.46438 | best_loss=8.45726
Epoch 3/80: current_loss=8.46230 | best_loss=8.45726
Epoch 4/80: current_loss=8.46583 | best_loss=8.45726
Epoch 5/80: current_loss=8.46681 | best_loss=8.45726
Epoch 6/80: current_loss=8.46711 | best_loss=8.45726
Epoch 7/80: current_loss=8.46995 | best_loss=8.45726
Epoch 8/80: current_loss=8.46859 | best_loss=8.45726
Epoch 9/80: current_loss=8.47250 | best_loss=8.45726
Epoch 10/80: current_loss=8.47596 | best_loss=8.45726
Epoch 11/80: current_loss=8.47601 | best_loss=8.45726
Epoch 12/80: current_loss=8.47874 | best_loss=8.45726
Epoch 13/80: current_loss=8.47954 | best_loss=8.45726
Epoch 14/80: current_loss=8.48092 | best_loss=8.45726
Epoch 15/80: current_loss=8.48249 | best_loss=8.45726
Epoch 16/80: current_loss=8.48101 | best_loss=8.45726
Epoch 17/80: current_loss=8.48035 | best_loss=8.45726
Epoch 18/80: current_loss=8.48334 | best_loss=8.45726
Epoch 19/80: current_loss=8.48360 | best_loss=8.45726
Epoch 20/80: current_loss=8.48555 | best_loss=8.45726
Early Stopping at epoch 20
      explained_var=0.00179 | mse_loss=8.56175
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.41417 | best_loss=9.41417
Epoch 1/80: current_loss=9.42811 | best_loss=9.41417
Epoch 2/80: current_loss=9.43441 | best_loss=9.41417
Epoch 3/80: current_loss=9.43231 | best_loss=9.41417
Epoch 4/80: current_loss=9.43790 | best_loss=9.41417
Epoch 5/80: current_loss=9.44560 | best_loss=9.41417
Epoch 6/80: current_loss=9.42730 | best_loss=9.41417
Epoch 7/80: current_loss=9.43733 | best_loss=9.41417
Epoch 8/80: current_loss=9.43548 | best_loss=9.41417
Epoch 9/80: current_loss=9.42590 | best_loss=9.41417
Epoch 10/80: current_loss=9.42676 | best_loss=9.41417
Epoch 11/80: current_loss=9.43485 | best_loss=9.41417
Epoch 12/80: current_loss=9.43045 | best_loss=9.41417
Epoch 13/80: current_loss=9.43881 | best_loss=9.41417
Epoch 14/80: current_loss=9.42180 | best_loss=9.41417
Epoch 15/80: current_loss=9.43129 | best_loss=9.41417
Epoch 16/80: current_loss=9.42236 | best_loss=9.41417
Epoch 17/80: current_loss=9.42795 | best_loss=9.41417
Epoch 18/80: current_loss=9.42558 | best_loss=9.41417
Epoch 19/80: current_loss=9.42455 | best_loss=9.41417
Epoch 20/80: current_loss=9.42159 | best_loss=9.41417
Early Stopping at epoch 20
      explained_var=-0.00775 | mse_loss=9.56431
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.95488 | best_loss=8.95488
Epoch 1/80: current_loss=8.96360 | best_loss=8.95488
Epoch 2/80: current_loss=8.96716 | best_loss=8.95488
Epoch 3/80: current_loss=8.96419 | best_loss=8.95488
Epoch 4/80: current_loss=8.97450 | best_loss=8.95488
Epoch 5/80: current_loss=8.96818 | best_loss=8.95488
Epoch 6/80: current_loss=8.97558 | best_loss=8.95488
Epoch 7/80: current_loss=8.96267 | best_loss=8.95488
Epoch 8/80: current_loss=8.98341 | best_loss=8.95488
Epoch 9/80: current_loss=8.97590 | best_loss=8.95488
Epoch 10/80: current_loss=8.97494 | best_loss=8.95488
Epoch 11/80: current_loss=8.97523 | best_loss=8.95488
Epoch 12/80: current_loss=8.97389 | best_loss=8.95488
Epoch 13/80: current_loss=8.96883 | best_loss=8.95488
Epoch 14/80: current_loss=8.97161 | best_loss=8.95488
Epoch 15/80: current_loss=8.96629 | best_loss=8.95488
Epoch 16/80: current_loss=8.97048 | best_loss=8.95488
Epoch 17/80: current_loss=8.97410 | best_loss=8.95488
Epoch 18/80: current_loss=8.97058 | best_loss=8.95488
Epoch 19/80: current_loss=8.96923 | best_loss=8.95488
Epoch 20/80: current_loss=8.97134 | best_loss=8.95488
Early Stopping at epoch 20
      explained_var=0.00100 | mse_loss=8.36185
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.48524 | best_loss=8.48524
Epoch 1/80: current_loss=8.48511 | best_loss=8.48511
Epoch 2/80: current_loss=8.48507 | best_loss=8.48507
Epoch 3/80: current_loss=8.48560 | best_loss=8.48507
Epoch 4/80: current_loss=8.48733 | best_loss=8.48507
Epoch 5/80: current_loss=8.48567 | best_loss=8.48507
Epoch 6/80: current_loss=8.48570 | best_loss=8.48507
Epoch 7/80: current_loss=8.48569 | best_loss=8.48507
Epoch 8/80: current_loss=8.48535 | best_loss=8.48507
Epoch 9/80: current_loss=8.48546 | best_loss=8.48507
Epoch 10/80: current_loss=8.48646 | best_loss=8.48507
Epoch 11/80: current_loss=8.48545 | best_loss=8.48507
Epoch 12/80: current_loss=8.48536 | best_loss=8.48507
Epoch 13/80: current_loss=8.48560 | best_loss=8.48507
Epoch 14/80: current_loss=8.48537 | best_loss=8.48507
Epoch 15/80: current_loss=8.48484 | best_loss=8.48484
Epoch 16/80: current_loss=8.48492 | best_loss=8.48484
Epoch 17/80: current_loss=8.48476 | best_loss=8.48476
Epoch 18/80: current_loss=8.48455 | best_loss=8.48455
Epoch 19/80: current_loss=8.48477 | best_loss=8.48455
Epoch 20/80: current_loss=8.48431 | best_loss=8.48431
Epoch 21/80: current_loss=8.48439 | best_loss=8.48431
Epoch 22/80: current_loss=8.48402 | best_loss=8.48402
Epoch 23/80: current_loss=8.48409 | best_loss=8.48402
Epoch 24/80: current_loss=8.48407 | best_loss=8.48402
Epoch 25/80: current_loss=8.48339 | best_loss=8.48339
Epoch 26/80: current_loss=8.48436 | best_loss=8.48339
Epoch 27/80: current_loss=8.48283 | best_loss=8.48283
Epoch 28/80: current_loss=8.48238 | best_loss=8.48238
Epoch 29/80: current_loss=8.48265 | best_loss=8.48238
Epoch 30/80: current_loss=8.48179 | best_loss=8.48179
Epoch 31/80: current_loss=8.48178 | best_loss=8.48178
Epoch 32/80: current_loss=8.48168 | best_loss=8.48168
Epoch 33/80: current_loss=8.48224 | best_loss=8.48168
Epoch 34/80: current_loss=8.48222 | best_loss=8.48168
Epoch 35/80: current_loss=8.48164 | best_loss=8.48164
Epoch 36/80: current_loss=8.48154 | best_loss=8.48154
Epoch 37/80: current_loss=8.48146 | best_loss=8.48146
Epoch 38/80: current_loss=8.48117 | best_loss=8.48117
Epoch 39/80: current_loss=8.48104 | best_loss=8.48104
Epoch 40/80: current_loss=8.48037 | best_loss=8.48037
Epoch 41/80: current_loss=8.48080 | best_loss=8.48037
Epoch 42/80: current_loss=8.48020 | best_loss=8.48020
Epoch 43/80: current_loss=8.47973 | best_loss=8.47973
Epoch 44/80: current_loss=8.48035 | best_loss=8.47973
Epoch 45/80: current_loss=8.47938 | best_loss=8.47938
Epoch 46/80: current_loss=8.47977 | best_loss=8.47938
Epoch 47/80: current_loss=8.47994 | best_loss=8.47938
Epoch 48/80: current_loss=8.47911 | best_loss=8.47911
Epoch 49/80: current_loss=8.47947 | best_loss=8.47911
Epoch 50/80: current_loss=8.47922 | best_loss=8.47911
Epoch 51/80: current_loss=8.47888 | best_loss=8.47888
Epoch 52/80: current_loss=8.47882 | best_loss=8.47882
Epoch 53/80: current_loss=8.47940 | best_loss=8.47882
Epoch 54/80: current_loss=8.48059 | best_loss=8.47882
Epoch 55/80: current_loss=8.47873 | best_loss=8.47873
Epoch 56/80: current_loss=8.47909 | best_loss=8.47873
Epoch 57/80: current_loss=8.47914 | best_loss=8.47873
Epoch 58/80: current_loss=8.47886 | best_loss=8.47873
Epoch 59/80: current_loss=8.48078 | best_loss=8.47873
Epoch 60/80: current_loss=8.47872 | best_loss=8.47872
Epoch 61/80: current_loss=8.47870 | best_loss=8.47870
Epoch 62/80: current_loss=8.47914 | best_loss=8.47870
Epoch 63/80: current_loss=8.47883 | best_loss=8.47870
Epoch 64/80: current_loss=8.47845 | best_loss=8.47845
Epoch 65/80: current_loss=8.47947 | best_loss=8.47845
Epoch 66/80: current_loss=8.47888 | best_loss=8.47845
Epoch 67/80: current_loss=8.47975 | best_loss=8.47845
Epoch 68/80: current_loss=8.47835 | best_loss=8.47835
Epoch 69/80: current_loss=8.47931 | best_loss=8.47835
Epoch 70/80: current_loss=8.47849 | best_loss=8.47835
Epoch 71/80: current_loss=8.47859 | best_loss=8.47835
Epoch 72/80: current_loss=8.47849 | best_loss=8.47835
Epoch 73/80: current_loss=8.47837 | best_loss=8.47835
Epoch 74/80: current_loss=8.47797 | best_loss=8.47797
Epoch 75/80: current_loss=8.47798 | best_loss=8.47797
Epoch 76/80: current_loss=8.47807 | best_loss=8.47797
Epoch 77/80: current_loss=8.47781 | best_loss=8.47781
Epoch 78/80: current_loss=8.47825 | best_loss=8.47781
Epoch 79/80: current_loss=8.47797 | best_loss=8.47781
      explained_var=0.00112 | mse_loss=8.19140
----------------------------------------------
Average early_stopping_point: 32| avg_exp_var=-0.00034| avg_loss=8.85316
----------------------------------------------


----------------------------------------------
Params for Trial 7
{'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=29.02940 | best_loss=29.02940
Epoch 1/80: current_loss=13.33576 | best_loss=13.33576
Epoch 2/80: current_loss=12.45640 | best_loss=12.45640
Epoch 3/80: current_loss=11.83733 | best_loss=11.83733
Epoch 4/80: current_loss=12.13636 | best_loss=11.83733
Epoch 5/80: current_loss=12.71657 | best_loss=11.83733
Epoch 6/80: current_loss=10.71686 | best_loss=10.71686
Epoch 7/80: current_loss=21.49811 | best_loss=10.71686
Epoch 8/80: current_loss=16.60508 | best_loss=10.71686
Epoch 9/80: current_loss=10.65909 | best_loss=10.65909
Epoch 10/80: current_loss=11.43196 | best_loss=10.65909
Epoch 11/80: current_loss=11.31742 | best_loss=10.65909
Epoch 12/80: current_loss=25.53762 | best_loss=10.65909
Epoch 13/80: current_loss=17.35520 | best_loss=10.65909
Epoch 14/80: current_loss=12.14337 | best_loss=10.65909
Epoch 15/80: current_loss=19.96345 | best_loss=10.65909
Epoch 16/80: current_loss=13.46583 | best_loss=10.65909
Epoch 17/80: current_loss=10.03429 | best_loss=10.03429
Epoch 18/80: current_loss=9.93114 | best_loss=9.93114
Epoch 19/80: current_loss=9.81432 | best_loss=9.81432
Epoch 20/80: current_loss=19.02111 | best_loss=9.81432
Epoch 21/80: current_loss=21.00435 | best_loss=9.81432
Epoch 22/80: current_loss=24.44801 | best_loss=9.81432
Epoch 23/80: current_loss=11.50715 | best_loss=9.81432
Epoch 24/80: current_loss=31.24386 | best_loss=9.81432
Epoch 25/80: current_loss=12.18736 | best_loss=9.81432
Epoch 26/80: current_loss=14.70853 | best_loss=9.81432
Epoch 27/80: current_loss=16.45757 | best_loss=9.81432
Epoch 28/80: current_loss=22.09343 | best_loss=9.81432
Epoch 29/80: current_loss=13.68661 | best_loss=9.81432
Epoch 30/80: current_loss=23.56165 | best_loss=9.81432
Epoch 31/80: current_loss=18.18269 | best_loss=9.81432
Epoch 32/80: current_loss=21.33310 | best_loss=9.81432
Epoch 33/80: current_loss=36.09610 | best_loss=9.81432
Epoch 34/80: current_loss=12.41581 | best_loss=9.81432
Epoch 35/80: current_loss=14.16532 | best_loss=9.81432
Epoch 36/80: current_loss=19.56608 | best_loss=9.81432
Epoch 37/80: current_loss=29.46798 | best_loss=9.81432
Epoch 38/80: current_loss=32.85968 | best_loss=9.81432
Epoch 39/80: current_loss=13.78280 | best_loss=9.81432
Early Stopping at epoch 39
      explained_var=0.00795 | mse_loss=9.70085
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=22.03985 | best_loss=22.03985
Epoch 1/80: current_loss=12.62580 | best_loss=12.62580
Epoch 2/80: current_loss=19.13805 | best_loss=12.62580
Epoch 3/80: current_loss=20.00219 | best_loss=12.62580
Epoch 4/80: current_loss=19.34975 | best_loss=12.62580
Epoch 5/80: current_loss=11.59258 | best_loss=11.59258
Epoch 6/80: current_loss=13.42625 | best_loss=11.59258
Epoch 7/80: current_loss=16.20158 | best_loss=11.59258
Epoch 8/80: current_loss=12.83947 | best_loss=11.59258
Epoch 9/80: current_loss=12.16552 | best_loss=11.59258
Epoch 10/80: current_loss=16.61114 | best_loss=11.59258
Epoch 11/80: current_loss=20.88054 | best_loss=11.59258
Epoch 12/80: current_loss=10.85944 | best_loss=10.85944
Epoch 13/80: current_loss=12.26799 | best_loss=10.85944
Epoch 14/80: current_loss=18.20616 | best_loss=10.85944
Epoch 15/80: current_loss=9.00715 | best_loss=9.00715
Epoch 16/80: current_loss=28.70813 | best_loss=9.00715
Epoch 17/80: current_loss=13.87727 | best_loss=9.00715
Epoch 18/80: current_loss=10.09940 | best_loss=9.00715
Epoch 19/80: current_loss=14.76969 | best_loss=9.00715
Epoch 20/80: current_loss=10.00326 | best_loss=9.00715
Epoch 21/80: current_loss=15.90099 | best_loss=9.00715
Epoch 22/80: current_loss=8.83358 | best_loss=8.83358
Epoch 23/80: current_loss=30.83906 | best_loss=8.83358
Epoch 24/80: current_loss=11.55326 | best_loss=8.83358
Epoch 25/80: current_loss=10.45521 | best_loss=8.83358
Epoch 26/80: current_loss=40.96905 | best_loss=8.83358
Epoch 27/80: current_loss=11.66569 | best_loss=8.83358
Epoch 28/80: current_loss=10.94435 | best_loss=8.83358
Epoch 29/80: current_loss=12.05326 | best_loss=8.83358
Epoch 30/80: current_loss=9.65425 | best_loss=8.83358
Epoch 31/80: current_loss=9.06428 | best_loss=8.83358
Epoch 32/80: current_loss=8.89068 | best_loss=8.83358
Epoch 33/80: current_loss=10.35765 | best_loss=8.83358
Epoch 34/80: current_loss=9.26707 | best_loss=8.83358
Epoch 35/80: current_loss=11.15073 | best_loss=8.83358
Epoch 36/80: current_loss=12.30651 | best_loss=8.83358
Epoch 37/80: current_loss=12.45772 | best_loss=8.83358
Epoch 38/80: current_loss=11.52499 | best_loss=8.83358
Epoch 39/80: current_loss=9.81328 | best_loss=8.83358
Epoch 40/80: current_loss=8.63333 | best_loss=8.63333
Epoch 41/80: current_loss=9.28606 | best_loss=8.63333
Epoch 42/80: current_loss=9.27819 | best_loss=8.63333
Epoch 43/80: current_loss=17.69411 | best_loss=8.63333
Epoch 44/80: current_loss=17.80062 | best_loss=8.63333
Epoch 45/80: current_loss=9.61988 | best_loss=8.63333
Epoch 46/80: current_loss=9.60082 | best_loss=8.63333
Epoch 47/80: current_loss=29.50370 | best_loss=8.63333
Epoch 48/80: current_loss=9.60818 | best_loss=8.63333
Epoch 49/80: current_loss=17.94052 | best_loss=8.63333
Epoch 50/80: current_loss=16.46465 | best_loss=8.63333
Epoch 51/80: current_loss=9.52974 | best_loss=8.63333
Epoch 52/80: current_loss=15.01160 | best_loss=8.63333
Epoch 53/80: current_loss=8.64519 | best_loss=8.63333
Epoch 54/80: current_loss=28.39278 | best_loss=8.63333
Epoch 55/80: current_loss=15.81735 | best_loss=8.63333
Epoch 56/80: current_loss=11.94509 | best_loss=8.63333
Epoch 57/80: current_loss=18.00131 | best_loss=8.63333
Epoch 58/80: current_loss=8.69860 | best_loss=8.63333
Epoch 59/80: current_loss=11.51600 | best_loss=8.63333
Epoch 60/80: current_loss=10.31758 | best_loss=8.63333
Early Stopping at epoch 60
      explained_var=0.00304 | mse_loss=8.55152
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=23.54681 | best_loss=23.54681
Epoch 1/80: current_loss=12.53159 | best_loss=12.53159
Epoch 2/80: current_loss=11.97663 | best_loss=11.97663
Epoch 3/80: current_loss=21.43222 | best_loss=11.97663
Epoch 4/80: current_loss=16.32294 | best_loss=11.97663
Epoch 5/80: current_loss=12.70287 | best_loss=11.97663
Epoch 6/80: current_loss=12.52833 | best_loss=11.97663
Epoch 7/80: current_loss=18.49072 | best_loss=11.97663
Epoch 8/80: current_loss=15.14818 | best_loss=11.97663
Epoch 9/80: current_loss=18.59331 | best_loss=11.97663
Epoch 10/80: current_loss=13.61324 | best_loss=11.97663
Epoch 11/80: current_loss=26.25623 | best_loss=11.97663
Epoch 12/80: current_loss=11.44203 | best_loss=11.44203
Epoch 13/80: current_loss=17.59721 | best_loss=11.44203
Epoch 14/80: current_loss=12.59101 | best_loss=11.44203
Epoch 15/80: current_loss=14.71585 | best_loss=11.44203
Epoch 16/80: current_loss=15.33790 | best_loss=11.44203
Epoch 17/80: current_loss=10.51033 | best_loss=10.51033
Epoch 18/80: current_loss=26.72611 | best_loss=10.51033
Epoch 19/80: current_loss=10.79051 | best_loss=10.51033
Epoch 20/80: current_loss=12.09251 | best_loss=10.51033
Epoch 21/80: current_loss=11.27548 | best_loss=10.51033
Epoch 22/80: current_loss=22.46349 | best_loss=10.51033
Epoch 23/80: current_loss=16.16176 | best_loss=10.51033
Epoch 24/80: current_loss=17.23956 | best_loss=10.51033
Epoch 25/80: current_loss=15.46908 | best_loss=10.51033
Epoch 26/80: current_loss=20.76572 | best_loss=10.51033
Epoch 27/80: current_loss=12.62938 | best_loss=10.51033
Epoch 28/80: current_loss=13.66133 | best_loss=10.51033
Epoch 29/80: current_loss=16.86471 | best_loss=10.51033
Epoch 30/80: current_loss=12.05528 | best_loss=10.51033
Epoch 31/80: current_loss=17.96434 | best_loss=10.51033
Epoch 32/80: current_loss=10.58403 | best_loss=10.51033
Epoch 33/80: current_loss=20.12288 | best_loss=10.51033
Epoch 34/80: current_loss=15.68058 | best_loss=10.51033
Epoch 35/80: current_loss=25.70640 | best_loss=10.51033
Epoch 36/80: current_loss=17.45232 | best_loss=10.51033
Epoch 37/80: current_loss=9.87446 | best_loss=9.87446
Epoch 38/80: current_loss=16.56461 | best_loss=9.87446
Epoch 39/80: current_loss=11.00648 | best_loss=9.87446
Epoch 40/80: current_loss=13.67528 | best_loss=9.87446
Epoch 41/80: current_loss=16.62425 | best_loss=9.87446
Epoch 42/80: current_loss=19.23635 | best_loss=9.87446
Epoch 43/80: current_loss=10.61410 | best_loss=9.87446
Epoch 44/80: current_loss=23.45218 | best_loss=9.87446
Epoch 45/80: current_loss=9.80205 | best_loss=9.80205
Epoch 46/80: current_loss=11.49570 | best_loss=9.80205
Epoch 47/80: current_loss=15.10413 | best_loss=9.80205
Epoch 48/80: current_loss=14.93377 | best_loss=9.80205
Epoch 49/80: current_loss=16.87154 | best_loss=9.80205
Epoch 50/80: current_loss=13.08856 | best_loss=9.80205
Epoch 51/80: current_loss=25.50557 | best_loss=9.80205
Epoch 52/80: current_loss=13.08987 | best_loss=9.80205
Epoch 53/80: current_loss=16.33460 | best_loss=9.80205
Epoch 54/80: current_loss=13.13375 | best_loss=9.80205
Epoch 55/80: current_loss=12.86689 | best_loss=9.80205
Epoch 56/80: current_loss=14.08315 | best_loss=9.80205
Epoch 57/80: current_loss=16.74547 | best_loss=9.80205
Epoch 58/80: current_loss=12.12847 | best_loss=9.80205
Epoch 59/80: current_loss=30.14869 | best_loss=9.80205
Epoch 60/80: current_loss=18.72938 | best_loss=9.80205
Epoch 61/80: current_loss=24.40187 | best_loss=9.80205
Epoch 62/80: current_loss=10.44479 | best_loss=9.80205
Epoch 63/80: current_loss=39.78290 | best_loss=9.80205
Epoch 64/80: current_loss=10.69016 | best_loss=9.80205
Epoch 65/80: current_loss=10.48885 | best_loss=9.80205
Early Stopping at epoch 65
      explained_var=-0.06296 | mse_loss=9.98528
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=12.48163 | best_loss=12.48163
Epoch 1/80: current_loss=9.46804 | best_loss=9.46804
Epoch 2/80: current_loss=11.90397 | best_loss=9.46804
Epoch 3/80: current_loss=10.32158 | best_loss=9.46804
Epoch 4/80: current_loss=24.76970 | best_loss=9.46804
Epoch 5/80: current_loss=10.06512 | best_loss=9.46804
Epoch 6/80: current_loss=12.31988 | best_loss=9.46804
Epoch 7/80: current_loss=10.43741 | best_loss=9.46804
Epoch 8/80: current_loss=10.60660 | best_loss=9.46804
Epoch 9/80: current_loss=14.30211 | best_loss=9.46804
Epoch 10/80: current_loss=15.54002 | best_loss=9.46804
Epoch 11/80: current_loss=14.30997 | best_loss=9.46804
Epoch 12/80: current_loss=14.08438 | best_loss=9.46804
Epoch 13/80: current_loss=12.53891 | best_loss=9.46804
Epoch 14/80: current_loss=15.69785 | best_loss=9.46804
Epoch 15/80: current_loss=21.15636 | best_loss=9.46804
Epoch 16/80: current_loss=14.71402 | best_loss=9.46804
Epoch 17/80: current_loss=10.35234 | best_loss=9.46804
Epoch 18/80: current_loss=10.68302 | best_loss=9.46804
Epoch 19/80: current_loss=15.58644 | best_loss=9.46804
Epoch 20/80: current_loss=11.90872 | best_loss=9.46804
Epoch 21/80: current_loss=29.40538 | best_loss=9.46804
Early Stopping at epoch 21
      explained_var=-0.08342 | mse_loss=9.06063
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.72555 | best_loss=9.72555
Epoch 1/80: current_loss=9.66932 | best_loss=9.66932
Epoch 2/80: current_loss=10.66618 | best_loss=9.66932
Epoch 3/80: current_loss=12.91759 | best_loss=9.66932
Epoch 4/80: current_loss=8.95371 | best_loss=8.95371
Epoch 5/80: current_loss=9.96714 | best_loss=8.95371
Epoch 6/80: current_loss=10.17152 | best_loss=8.95371
Epoch 7/80: current_loss=9.08078 | best_loss=8.95371
Epoch 8/80: current_loss=10.70393 | best_loss=8.95371
Epoch 9/80: current_loss=14.62471 | best_loss=8.95371
Epoch 10/80: current_loss=9.06881 | best_loss=8.95371
Epoch 11/80: current_loss=15.23635 | best_loss=8.95371
Epoch 12/80: current_loss=19.54014 | best_loss=8.95371
Epoch 13/80: current_loss=13.29845 | best_loss=8.95371
Epoch 14/80: current_loss=11.78447 | best_loss=8.95371
Epoch 15/80: current_loss=20.95037 | best_loss=8.95371
Epoch 16/80: current_loss=9.77417 | best_loss=8.95371
Epoch 17/80: current_loss=13.24002 | best_loss=8.95371
Epoch 18/80: current_loss=22.29282 | best_loss=8.95371
Epoch 19/80: current_loss=9.13897 | best_loss=8.95371
Epoch 20/80: current_loss=12.35337 | best_loss=8.95371
Epoch 21/80: current_loss=39.15832 | best_loss=8.95371
Epoch 22/80: current_loss=13.34902 | best_loss=8.95371
Epoch 23/80: current_loss=23.23626 | best_loss=8.95371
Epoch 24/80: current_loss=16.55582 | best_loss=8.95371
Early Stopping at epoch 24
      explained_var=-0.02642 | mse_loss=8.85301
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=-0.03236| avg_loss=9.23026
----------------------------------------------


----------------------------------------------
Params for Trial 8
{'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=74.98650 | best_loss=74.98650
Epoch 1/80: current_loss=74.58043 | best_loss=74.58043
Epoch 2/80: current_loss=74.15692 | best_loss=74.15692
Epoch 3/80: current_loss=73.70888 | best_loss=73.70888
Epoch 4/80: current_loss=73.22967 | best_loss=73.22967
Epoch 5/80: current_loss=72.72085 | best_loss=72.72085
Epoch 6/80: current_loss=72.16149 | best_loss=72.16149
Epoch 7/80: current_loss=71.54037 | best_loss=71.54037
Epoch 8/80: current_loss=70.86025 | best_loss=70.86025
Epoch 9/80: current_loss=70.09050 | best_loss=70.09050
Epoch 10/80: current_loss=69.21911 | best_loss=69.21911
Epoch 11/80: current_loss=68.22624 | best_loss=68.22624
Epoch 12/80: current_loss=67.09417 | best_loss=67.09417
Epoch 13/80: current_loss=65.80607 | best_loss=65.80607
Epoch 14/80: current_loss=64.37754 | best_loss=64.37754
Epoch 15/80: current_loss=62.82472 | best_loss=62.82472
Epoch 16/80: current_loss=61.19671 | best_loss=61.19671
Epoch 17/80: current_loss=59.56140 | best_loss=59.56140
Epoch 18/80: current_loss=57.95392 | best_loss=57.95392
Epoch 19/80: current_loss=56.38943 | best_loss=56.38943
Epoch 20/80: current_loss=54.90143 | best_loss=54.90143
Epoch 21/80: current_loss=53.51436 | best_loss=53.51436
Epoch 22/80: current_loss=52.19516 | best_loss=52.19516
Epoch 23/80: current_loss=50.96227 | best_loss=50.96227
Epoch 24/80: current_loss=49.80179 | best_loss=49.80179
Epoch 25/80: current_loss=48.69446 | best_loss=48.69446
Epoch 26/80: current_loss=47.65007 | best_loss=47.65007
Epoch 27/80: current_loss=46.66134 | best_loss=46.66134
Epoch 28/80: current_loss=45.72717 | best_loss=45.72717
Epoch 29/80: current_loss=44.82647 | best_loss=44.82647
Epoch 30/80: current_loss=43.97631 | best_loss=43.97631
Epoch 31/80: current_loss=43.15600 | best_loss=43.15600
Epoch 32/80: current_loss=42.37538 | best_loss=42.37538
Epoch 33/80: current_loss=41.61679 | best_loss=41.61679
Epoch 34/80: current_loss=40.88907 | best_loss=40.88907
Epoch 35/80: current_loss=40.19325 | best_loss=40.19325
Epoch 36/80: current_loss=39.52073 | best_loss=39.52073
Epoch 37/80: current_loss=38.87128 | best_loss=38.87128
Epoch 38/80: current_loss=38.24726 | best_loss=38.24726
Epoch 39/80: current_loss=37.64485 | best_loss=37.64485
Epoch 40/80: current_loss=37.05009 | best_loss=37.05009
Epoch 41/80: current_loss=36.47547 | best_loss=36.47547
Epoch 42/80: current_loss=35.91616 | best_loss=35.91616
Epoch 43/80: current_loss=35.37753 | best_loss=35.37753
Epoch 44/80: current_loss=34.85248 | best_loss=34.85248
Epoch 45/80: current_loss=34.33371 | best_loss=34.33371
Epoch 46/80: current_loss=33.83313 | best_loss=33.83313
Epoch 47/80: current_loss=33.34121 | best_loss=33.34121
Epoch 48/80: current_loss=32.86617 | best_loss=32.86617
Epoch 49/80: current_loss=32.39559 | best_loss=32.39559
Epoch 50/80: current_loss=31.94020 | best_loss=31.94020
Epoch 51/80: current_loss=31.50053 | best_loss=31.50053
Epoch 52/80: current_loss=31.06239 | best_loss=31.06239
Epoch 53/80: current_loss=30.63766 | best_loss=30.63766
Epoch 54/80: current_loss=30.21544 | best_loss=30.21544
Epoch 55/80: current_loss=29.80886 | best_loss=29.80886
Epoch 56/80: current_loss=29.41026 | best_loss=29.41026
Epoch 57/80: current_loss=29.01792 | best_loss=29.01792
Epoch 58/80: current_loss=28.63495 | best_loss=28.63495
Epoch 59/80: current_loss=28.26107 | best_loss=28.26107
Epoch 60/80: current_loss=27.90043 | best_loss=27.90043
Epoch 61/80: current_loss=27.54153 | best_loss=27.54153
Epoch 62/80: current_loss=27.19399 | best_loss=27.19399
Epoch 63/80: current_loss=26.85328 | best_loss=26.85328
Epoch 64/80: current_loss=26.52243 | best_loss=26.52243
Epoch 65/80: current_loss=26.19734 | best_loss=26.19734
Epoch 66/80: current_loss=25.87738 | best_loss=25.87738
Epoch 67/80: current_loss=25.56569 | best_loss=25.56569
Epoch 68/80: current_loss=25.26160 | best_loss=25.26160
Epoch 69/80: current_loss=24.96565 | best_loss=24.96565
Epoch 70/80: current_loss=24.67432 | best_loss=24.67432
Epoch 71/80: current_loss=24.38690 | best_loss=24.38690
Epoch 72/80: current_loss=24.10938 | best_loss=24.10938
Epoch 73/80: current_loss=23.84320 | best_loss=23.84320
Epoch 74/80: current_loss=23.57255 | best_loss=23.57255
Epoch 75/80: current_loss=23.31249 | best_loss=23.31249
Epoch 76/80: current_loss=23.05799 | best_loss=23.05799
Epoch 77/80: current_loss=22.81312 | best_loss=22.81312
Epoch 78/80: current_loss=22.57147 | best_loss=22.57147
Epoch 79/80: current_loss=22.33399 | best_loss=22.33399
      explained_var=-0.08228 | mse_loss=21.04696
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=21.93888 | best_loss=21.93888
Epoch 1/80: current_loss=21.61750 | best_loss=21.61750
Epoch 2/80: current_loss=21.30482 | best_loss=21.30482
Epoch 3/80: current_loss=21.01217 | best_loss=21.01217
Epoch 4/80: current_loss=20.72949 | best_loss=20.72949
Epoch 5/80: current_loss=20.45258 | best_loss=20.45258
Epoch 6/80: current_loss=20.19082 | best_loss=20.19082
Epoch 7/80: current_loss=19.94108 | best_loss=19.94108
Epoch 8/80: current_loss=19.70241 | best_loss=19.70241
Epoch 9/80: current_loss=19.47258 | best_loss=19.47258
Epoch 10/80: current_loss=19.24409 | best_loss=19.24409
Epoch 11/80: current_loss=19.02239 | best_loss=19.02239
Epoch 12/80: current_loss=18.81242 | best_loss=18.81242
Epoch 13/80: current_loss=18.60673 | best_loss=18.60673
Epoch 14/80: current_loss=18.40793 | best_loss=18.40793
Epoch 15/80: current_loss=18.20859 | best_loss=18.20859
Epoch 16/80: current_loss=18.01937 | best_loss=18.01937
Epoch 17/80: current_loss=17.83236 | best_loss=17.83236
Epoch 18/80: current_loss=17.65231 | best_loss=17.65231
Epoch 19/80: current_loss=17.47583 | best_loss=17.47583
Epoch 20/80: current_loss=17.30000 | best_loss=17.30000
Epoch 21/80: current_loss=17.13347 | best_loss=17.13347
Epoch 22/80: current_loss=16.97258 | best_loss=16.97258
Epoch 23/80: current_loss=16.81823 | best_loss=16.81823
Epoch 24/80: current_loss=16.66261 | best_loss=16.66261
Epoch 25/80: current_loss=16.51014 | best_loss=16.51014
Epoch 26/80: current_loss=16.36118 | best_loss=16.36118
Epoch 27/80: current_loss=16.21791 | best_loss=16.21791
Epoch 28/80: current_loss=16.07400 | best_loss=16.07400
Epoch 29/80: current_loss=15.93712 | best_loss=15.93712
Epoch 30/80: current_loss=15.79780 | best_loss=15.79780
Epoch 31/80: current_loss=15.66587 | best_loss=15.66587
Epoch 32/80: current_loss=15.53267 | best_loss=15.53267
Epoch 33/80: current_loss=15.40476 | best_loss=15.40476
Epoch 34/80: current_loss=15.28189 | best_loss=15.28189
Epoch 35/80: current_loss=15.15828 | best_loss=15.15828
Epoch 36/80: current_loss=15.03922 | best_loss=15.03922
Epoch 37/80: current_loss=14.91943 | best_loss=14.91943
Epoch 38/80: current_loss=14.80447 | best_loss=14.80447
Epoch 39/80: current_loss=14.69134 | best_loss=14.69134
Epoch 40/80: current_loss=14.57689 | best_loss=14.57689
Epoch 41/80: current_loss=14.47116 | best_loss=14.47116
Epoch 42/80: current_loss=14.36308 | best_loss=14.36308
Epoch 43/80: current_loss=14.25911 | best_loss=14.25911
Epoch 44/80: current_loss=14.15549 | best_loss=14.15549
Epoch 45/80: current_loss=14.05483 | best_loss=14.05483
Epoch 46/80: current_loss=13.95508 | best_loss=13.95508
Epoch 47/80: current_loss=13.85694 | best_loss=13.85694
Epoch 48/80: current_loss=13.76009 | best_loss=13.76009
Epoch 49/80: current_loss=13.66579 | best_loss=13.66579
Epoch 50/80: current_loss=13.57268 | best_loss=13.57268
Epoch 51/80: current_loss=13.48436 | best_loss=13.48436
Epoch 52/80: current_loss=13.39496 | best_loss=13.39496
Epoch 53/80: current_loss=13.30669 | best_loss=13.30669
Epoch 54/80: current_loss=13.22327 | best_loss=13.22327
Epoch 55/80: current_loss=13.13785 | best_loss=13.13785
Epoch 56/80: current_loss=13.05346 | best_loss=13.05346
Epoch 57/80: current_loss=12.97665 | best_loss=12.97665
Epoch 58/80: current_loss=12.89792 | best_loss=12.89792
Epoch 59/80: current_loss=12.82243 | best_loss=12.82243
Epoch 60/80: current_loss=12.74574 | best_loss=12.74574
Epoch 61/80: current_loss=12.67203 | best_loss=12.67203
Epoch 62/80: current_loss=12.59807 | best_loss=12.59807
Epoch 63/80: current_loss=12.52670 | best_loss=12.52670
Epoch 64/80: current_loss=12.45842 | best_loss=12.45842
Epoch 65/80: current_loss=12.39091 | best_loss=12.39091
Epoch 66/80: current_loss=12.32277 | best_loss=12.32277
Epoch 67/80: current_loss=12.25579 | best_loss=12.25579
Epoch 68/80: current_loss=12.19020 | best_loss=12.19020
Epoch 69/80: current_loss=12.12934 | best_loss=12.12934
Epoch 70/80: current_loss=12.06714 | best_loss=12.06714
Epoch 71/80: current_loss=12.00740 | best_loss=12.00740
Epoch 72/80: current_loss=11.94535 | best_loss=11.94535
Epoch 73/80: current_loss=11.88516 | best_loss=11.88516
Epoch 74/80: current_loss=11.83059 | best_loss=11.83059
Epoch 75/80: current_loss=11.77536 | best_loss=11.77536
Epoch 76/80: current_loss=11.71889 | best_loss=11.71889
Epoch 77/80: current_loss=11.66656 | best_loss=11.66656
Epoch 78/80: current_loss=11.61472 | best_loss=11.61472
Epoch 79/80: current_loss=11.56221 | best_loss=11.56221
      explained_var=-0.11963 | mse_loss=11.15695
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=13.82125 | best_loss=13.82125
Epoch 1/80: current_loss=13.74949 | best_loss=13.74949
Epoch 2/80: current_loss=13.67807 | best_loss=13.67807
Epoch 3/80: current_loss=13.61134 | best_loss=13.61134
Epoch 4/80: current_loss=13.54577 | best_loss=13.54577
Epoch 5/80: current_loss=13.48670 | best_loss=13.48670
Epoch 6/80: current_loss=13.42899 | best_loss=13.42899
Epoch 7/80: current_loss=13.37012 | best_loss=13.37012
Epoch 8/80: current_loss=13.31535 | best_loss=13.31535
Epoch 9/80: current_loss=13.25984 | best_loss=13.25984
Epoch 10/80: current_loss=13.20849 | best_loss=13.20849
Epoch 11/80: current_loss=13.15426 | best_loss=13.15426
Epoch 12/80: current_loss=13.10782 | best_loss=13.10782
Epoch 13/80: current_loss=13.05952 | best_loss=13.05952
Epoch 14/80: current_loss=13.01285 | best_loss=13.01285
Epoch 15/80: current_loss=12.97032 | best_loss=12.97032
Epoch 16/80: current_loss=12.92807 | best_loss=12.92807
Epoch 17/80: current_loss=12.88483 | best_loss=12.88483
Epoch 18/80: current_loss=12.84473 | best_loss=12.84473
Epoch 19/80: current_loss=12.80879 | best_loss=12.80879
Epoch 20/80: current_loss=12.77089 | best_loss=12.77089
Epoch 21/80: current_loss=12.73544 | best_loss=12.73544
Epoch 22/80: current_loss=12.69683 | best_loss=12.69683
Epoch 23/80: current_loss=12.66431 | best_loss=12.66431
Epoch 24/80: current_loss=12.63478 | best_loss=12.63478
Epoch 25/80: current_loss=12.60015 | best_loss=12.60015
Epoch 26/80: current_loss=12.57039 | best_loss=12.57039
Epoch 27/80: current_loss=12.53988 | best_loss=12.53988
Epoch 28/80: current_loss=12.51216 | best_loss=12.51216
Epoch 29/80: current_loss=12.48531 | best_loss=12.48531
Epoch 30/80: current_loss=12.45725 | best_loss=12.45725
Epoch 31/80: current_loss=12.42894 | best_loss=12.42894
Epoch 32/80: current_loss=12.40462 | best_loss=12.40462
Epoch 33/80: current_loss=12.38085 | best_loss=12.38085
Epoch 34/80: current_loss=12.36054 | best_loss=12.36054
Epoch 35/80: current_loss=12.34018 | best_loss=12.34018
Epoch 36/80: current_loss=12.31666 | best_loss=12.31666
Epoch 37/80: current_loss=12.29888 | best_loss=12.29888
Epoch 38/80: current_loss=12.27805 | best_loss=12.27805
Epoch 39/80: current_loss=12.25962 | best_loss=12.25962
Epoch 40/80: current_loss=12.24052 | best_loss=12.24052
Epoch 41/80: current_loss=12.22089 | best_loss=12.22089
Epoch 42/80: current_loss=12.20301 | best_loss=12.20301
Epoch 43/80: current_loss=12.18649 | best_loss=12.18649
Epoch 44/80: current_loss=12.16922 | best_loss=12.16922
Epoch 45/80: current_loss=12.15486 | best_loss=12.15486
Epoch 46/80: current_loss=12.13700 | best_loss=12.13700
Epoch 47/80: current_loss=12.12300 | best_loss=12.12300
Epoch 48/80: current_loss=12.10699 | best_loss=12.10699
Epoch 49/80: current_loss=12.09178 | best_loss=12.09178
Epoch 50/80: current_loss=12.07697 | best_loss=12.07697
Epoch 51/80: current_loss=12.06374 | best_loss=12.06374
Epoch 52/80: current_loss=12.05404 | best_loss=12.05404
Epoch 53/80: current_loss=12.03973 | best_loss=12.03973
Epoch 54/80: current_loss=12.02628 | best_loss=12.02628
Epoch 55/80: current_loss=12.01552 | best_loss=12.01552
Epoch 56/80: current_loss=12.00431 | best_loss=12.00431
Epoch 57/80: current_loss=11.99414 | best_loss=11.99414
Epoch 58/80: current_loss=11.98397 | best_loss=11.98397
Epoch 59/80: current_loss=11.97303 | best_loss=11.97303
Epoch 60/80: current_loss=11.96240 | best_loss=11.96240
Epoch 61/80: current_loss=11.95255 | best_loss=11.95255
Epoch 62/80: current_loss=11.94315 | best_loss=11.94315
Epoch 63/80: current_loss=11.93347 | best_loss=11.93347
Epoch 64/80: current_loss=11.92156 | best_loss=11.92156
Epoch 65/80: current_loss=11.91192 | best_loss=11.91192
Epoch 66/80: current_loss=11.90041 | best_loss=11.90041
Epoch 67/80: current_loss=11.89161 | best_loss=11.89161
Epoch 68/80: current_loss=11.88356 | best_loss=11.88356
Epoch 69/80: current_loss=11.87286 | best_loss=11.87286
Epoch 70/80: current_loss=11.86354 | best_loss=11.86354
Epoch 71/80: current_loss=11.85335 | best_loss=11.85335
Epoch 72/80: current_loss=11.84542 | best_loss=11.84542
Epoch 73/80: current_loss=11.83764 | best_loss=11.83764
Epoch 74/80: current_loss=11.82931 | best_loss=11.82931
Epoch 75/80: current_loss=11.82041 | best_loss=11.82041
Epoch 76/80: current_loss=11.81001 | best_loss=11.81001
Epoch 77/80: current_loss=11.80209 | best_loss=11.80209
Epoch 78/80: current_loss=11.79359 | best_loss=11.79359
Epoch 79/80: current_loss=11.78494 | best_loss=11.78494
      explained_var=-0.24226 | mse_loss=12.06333
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.80016 | best_loss=9.80016
Epoch 1/80: current_loss=9.79556 | best_loss=9.79556
Epoch 2/80: current_loss=9.79123 | best_loss=9.79123
Epoch 3/80: current_loss=9.78688 | best_loss=9.78688
Epoch 4/80: current_loss=9.78321 | best_loss=9.78321
Epoch 5/80: current_loss=9.77973 | best_loss=9.77973
Epoch 6/80: current_loss=9.77619 | best_loss=9.77619
Epoch 7/80: current_loss=9.77205 | best_loss=9.77205
Epoch 8/80: current_loss=9.76796 | best_loss=9.76796
Epoch 9/80: current_loss=9.76444 | best_loss=9.76444
Epoch 10/80: current_loss=9.76092 | best_loss=9.76092
Epoch 11/80: current_loss=9.75769 | best_loss=9.75769
Epoch 12/80: current_loss=9.75425 | best_loss=9.75425
Epoch 13/80: current_loss=9.75109 | best_loss=9.75109
Epoch 14/80: current_loss=9.74798 | best_loss=9.74798
Epoch 15/80: current_loss=9.74497 | best_loss=9.74497
Epoch 16/80: current_loss=9.74149 | best_loss=9.74149
Epoch 17/80: current_loss=9.73849 | best_loss=9.73849
Epoch 18/80: current_loss=9.73518 | best_loss=9.73518
Epoch 19/80: current_loss=9.73215 | best_loss=9.73215
Epoch 20/80: current_loss=9.72946 | best_loss=9.72946
Epoch 21/80: current_loss=9.72644 | best_loss=9.72644
Epoch 22/80: current_loss=9.72375 | best_loss=9.72375
Epoch 23/80: current_loss=9.72129 | best_loss=9.72129
Epoch 24/80: current_loss=9.71831 | best_loss=9.71831
Epoch 25/80: current_loss=9.71577 | best_loss=9.71577
Epoch 26/80: current_loss=9.71328 | best_loss=9.71328
Epoch 27/80: current_loss=9.71065 | best_loss=9.71065
Epoch 28/80: current_loss=9.70794 | best_loss=9.70794
Epoch 29/80: current_loss=9.70501 | best_loss=9.70501
Epoch 30/80: current_loss=9.70247 | best_loss=9.70247
Epoch 31/80: current_loss=9.69998 | best_loss=9.69998
Epoch 32/80: current_loss=9.69727 | best_loss=9.69727
Epoch 33/80: current_loss=9.69470 | best_loss=9.69470
Epoch 34/80: current_loss=9.69197 | best_loss=9.69197
Epoch 35/80: current_loss=9.68945 | best_loss=9.68945
Epoch 36/80: current_loss=9.68717 | best_loss=9.68717
Epoch 37/80: current_loss=9.68483 | best_loss=9.68483
Epoch 38/80: current_loss=9.68276 | best_loss=9.68276
Epoch 39/80: current_loss=9.68049 | best_loss=9.68049
Epoch 40/80: current_loss=9.67844 | best_loss=9.67844
Epoch 41/80: current_loss=9.67669 | best_loss=9.67669
Epoch 42/80: current_loss=9.67418 | best_loss=9.67418
Epoch 43/80: current_loss=9.67168 | best_loss=9.67168
Epoch 44/80: current_loss=9.66936 | best_loss=9.66936
Epoch 45/80: current_loss=9.66729 | best_loss=9.66729
Epoch 46/80: current_loss=9.66494 | best_loss=9.66494
Epoch 47/80: current_loss=9.66220 | best_loss=9.66220
Epoch 48/80: current_loss=9.65991 | best_loss=9.65991
Epoch 49/80: current_loss=9.65741 | best_loss=9.65741
Epoch 50/80: current_loss=9.65488 | best_loss=9.65488
Epoch 51/80: current_loss=9.65258 | best_loss=9.65258
Epoch 52/80: current_loss=9.65023 | best_loss=9.65023
Epoch 53/80: current_loss=9.64767 | best_loss=9.64767
Epoch 54/80: current_loss=9.64544 | best_loss=9.64544
Epoch 55/80: current_loss=9.64288 | best_loss=9.64288
Epoch 56/80: current_loss=9.64040 | best_loss=9.64040
Epoch 57/80: current_loss=9.63816 | best_loss=9.63816
Epoch 58/80: current_loss=9.63619 | best_loss=9.63619
Epoch 59/80: current_loss=9.63409 | best_loss=9.63409
Epoch 60/80: current_loss=9.63195 | best_loss=9.63195
Epoch 61/80: current_loss=9.62955 | best_loss=9.62955
Epoch 62/80: current_loss=9.62739 | best_loss=9.62739
Epoch 63/80: current_loss=9.62522 | best_loss=9.62522
Epoch 64/80: current_loss=9.62296 | best_loss=9.62296
Epoch 65/80: current_loss=9.62046 | best_loss=9.62046
Epoch 66/80: current_loss=9.61831 | best_loss=9.61831
Epoch 67/80: current_loss=9.61580 | best_loss=9.61580
Epoch 68/80: current_loss=9.61315 | best_loss=9.61315
Epoch 69/80: current_loss=9.61079 | best_loss=9.61079
Epoch 70/80: current_loss=9.60842 | best_loss=9.60842
Epoch 71/80: current_loss=9.60619 | best_loss=9.60619
Epoch 72/80: current_loss=9.60401 | best_loss=9.60401
Epoch 73/80: current_loss=9.60202 | best_loss=9.60202
Epoch 74/80: current_loss=9.60012 | best_loss=9.60012
Epoch 75/80: current_loss=9.59815 | best_loss=9.59815
Epoch 76/80: current_loss=9.59611 | best_loss=9.59611
Epoch 77/80: current_loss=9.59403 | best_loss=9.59403
Epoch 78/80: current_loss=9.59160 | best_loss=9.59160
Epoch 79/80: current_loss=9.58966 | best_loss=9.58966
      explained_var=-0.09614 | mse_loss=9.17030
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.64931 | best_loss=9.64931
Epoch 1/80: current_loss=9.64632 | best_loss=9.64632
Epoch 2/80: current_loss=9.64397 | best_loss=9.64397
Epoch 3/80: current_loss=9.64020 | best_loss=9.64020
Epoch 4/80: current_loss=9.63667 | best_loss=9.63667
Epoch 5/80: current_loss=9.63419 | best_loss=9.63419
Epoch 6/80: current_loss=9.63023 | best_loss=9.63023
Epoch 7/80: current_loss=9.62627 | best_loss=9.62627
Epoch 8/80: current_loss=9.62301 | best_loss=9.62301
Epoch 9/80: current_loss=9.62020 | best_loss=9.62020
Epoch 10/80: current_loss=9.61632 | best_loss=9.61632
Epoch 11/80: current_loss=9.61282 | best_loss=9.61282
Epoch 12/80: current_loss=9.60960 | best_loss=9.60960
Epoch 13/80: current_loss=9.60645 | best_loss=9.60645
Epoch 14/80: current_loss=9.60327 | best_loss=9.60327
Epoch 15/80: current_loss=9.60038 | best_loss=9.60038
Epoch 16/80: current_loss=9.59686 | best_loss=9.59686
Epoch 17/80: current_loss=9.59317 | best_loss=9.59317
Epoch 18/80: current_loss=9.58960 | best_loss=9.58960
Epoch 19/80: current_loss=9.58721 | best_loss=9.58721
Epoch 20/80: current_loss=9.58367 | best_loss=9.58367
Epoch 21/80: current_loss=9.58086 | best_loss=9.58086
Epoch 22/80: current_loss=9.57865 | best_loss=9.57865
Epoch 23/80: current_loss=9.57556 | best_loss=9.57556
Epoch 24/80: current_loss=9.57293 | best_loss=9.57293
Epoch 25/80: current_loss=9.57002 | best_loss=9.57002
Epoch 26/80: current_loss=9.56724 | best_loss=9.56724
Epoch 27/80: current_loss=9.56433 | best_loss=9.56433
Epoch 28/80: current_loss=9.56117 | best_loss=9.56117
Epoch 29/80: current_loss=9.55908 | best_loss=9.55908
Epoch 30/80: current_loss=9.55608 | best_loss=9.55608
Epoch 31/80: current_loss=9.55301 | best_loss=9.55301
Epoch 32/80: current_loss=9.55117 | best_loss=9.55117
Epoch 33/80: current_loss=9.54898 | best_loss=9.54898
Epoch 34/80: current_loss=9.54603 | best_loss=9.54603
Epoch 35/80: current_loss=9.54339 | best_loss=9.54339
Epoch 36/80: current_loss=9.53975 | best_loss=9.53975
Epoch 37/80: current_loss=9.53674 | best_loss=9.53674
Epoch 38/80: current_loss=9.53440 | best_loss=9.53440
Epoch 39/80: current_loss=9.53235 | best_loss=9.53235
Epoch 40/80: current_loss=9.52903 | best_loss=9.52903
Epoch 41/80: current_loss=9.52678 | best_loss=9.52678
Epoch 42/80: current_loss=9.52400 | best_loss=9.52400
Epoch 43/80: current_loss=9.52080 | best_loss=9.52080
Epoch 44/80: current_loss=9.51860 | best_loss=9.51860
Epoch 45/80: current_loss=9.51612 | best_loss=9.51612
Epoch 46/80: current_loss=9.51351 | best_loss=9.51351
Epoch 47/80: current_loss=9.51093 | best_loss=9.51093
Epoch 48/80: current_loss=9.50804 | best_loss=9.50804
Epoch 49/80: current_loss=9.50552 | best_loss=9.50552
Epoch 50/80: current_loss=9.50394 | best_loss=9.50394
Epoch 51/80: current_loss=9.50034 | best_loss=9.50034
Epoch 52/80: current_loss=9.49762 | best_loss=9.49762
Epoch 53/80: current_loss=9.49449 | best_loss=9.49449
Epoch 54/80: current_loss=9.49178 | best_loss=9.49178
Epoch 55/80: current_loss=9.48927 | best_loss=9.48927
Epoch 56/80: current_loss=9.48689 | best_loss=9.48689
Epoch 57/80: current_loss=9.48500 | best_loss=9.48500
Epoch 58/80: current_loss=9.48279 | best_loss=9.48279
Epoch 59/80: current_loss=9.47995 | best_loss=9.47995
Epoch 60/80: current_loss=9.47694 | best_loss=9.47694
Epoch 61/80: current_loss=9.47538 | best_loss=9.47538
Epoch 62/80: current_loss=9.47190 | best_loss=9.47190
Epoch 63/80: current_loss=9.46965 | best_loss=9.46965
Epoch 64/80: current_loss=9.46618 | best_loss=9.46618
Epoch 65/80: current_loss=9.46340 | best_loss=9.46340
Epoch 66/80: current_loss=9.46086 | best_loss=9.46086
Epoch 67/80: current_loss=9.45845 | best_loss=9.45845
Epoch 68/80: current_loss=9.45586 | best_loss=9.45586
Epoch 69/80: current_loss=9.45366 | best_loss=9.45366
Epoch 70/80: current_loss=9.45200 | best_loss=9.45200
Epoch 71/80: current_loss=9.44907 | best_loss=9.44907
Epoch 72/80: current_loss=9.44670 | best_loss=9.44670
Epoch 73/80: current_loss=9.44445 | best_loss=9.44445
Epoch 74/80: current_loss=9.44208 | best_loss=9.44208
Epoch 75/80: current_loss=9.43972 | best_loss=9.43972
Epoch 76/80: current_loss=9.43760 | best_loss=9.43760
Epoch 77/80: current_loss=9.43454 | best_loss=9.43454
Epoch 78/80: current_loss=9.43212 | best_loss=9.43212
Epoch 79/80: current_loss=9.42899 | best_loss=9.42899
      explained_var=-0.09778 | mse_loss=9.02979
----------------------------------------------
Average early_stopping_point: 80| avg_exp_var=-0.12762| avg_loss=12.49347
----------------------------------------------


----------------------------------------------
Params for Trial 9
{'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=28.75957 | best_loss=28.75957
Epoch 1/80: current_loss=15.05536 | best_loss=15.05536
Epoch 2/80: current_loss=10.44942 | best_loss=10.44942
Epoch 3/80: current_loss=20.06605 | best_loss=10.44942
Epoch 4/80: current_loss=10.77702 | best_loss=10.44942
Epoch 5/80: current_loss=12.18185 | best_loss=10.44942
Epoch 6/80: current_loss=16.63687 | best_loss=10.44942
Epoch 7/80: current_loss=15.71118 | best_loss=10.44942
Epoch 8/80: current_loss=12.11360 | best_loss=10.44942
Epoch 9/80: current_loss=11.25021 | best_loss=10.44942
Epoch 10/80: current_loss=10.09632 | best_loss=10.09632
Epoch 11/80: current_loss=10.81211 | best_loss=10.09632
Epoch 12/80: current_loss=12.40247 | best_loss=10.09632
Epoch 13/80: current_loss=10.02296 | best_loss=10.02296
Epoch 14/80: current_loss=15.32980 | best_loss=10.02296
Epoch 15/80: current_loss=13.33029 | best_loss=10.02296
Epoch 16/80: current_loss=18.05585 | best_loss=10.02296
Epoch 17/80: current_loss=17.83649 | best_loss=10.02296
Epoch 18/80: current_loss=17.28452 | best_loss=10.02296
Epoch 19/80: current_loss=12.16324 | best_loss=10.02296
Epoch 20/80: current_loss=16.66406 | best_loss=10.02296
Epoch 21/80: current_loss=18.32163 | best_loss=10.02296
Epoch 22/80: current_loss=23.77077 | best_loss=10.02296
Epoch 23/80: current_loss=13.61134 | best_loss=10.02296
Epoch 24/80: current_loss=14.00331 | best_loss=10.02296
Epoch 25/80: current_loss=16.35699 | best_loss=10.02296
Epoch 26/80: current_loss=14.42243 | best_loss=10.02296
Epoch 27/80: current_loss=76.41067 | best_loss=10.02296
Epoch 28/80: current_loss=14.34063 | best_loss=10.02296
Epoch 29/80: current_loss=10.52753 | best_loss=10.02296
Epoch 30/80: current_loss=10.17898 | best_loss=10.02296
Epoch 31/80: current_loss=11.09796 | best_loss=10.02296
Epoch 32/80: current_loss=16.40714 | best_loss=10.02296
Epoch 33/80: current_loss=12.38571 | best_loss=10.02296
Early Stopping at epoch 33
      explained_var=-0.03139 | mse_loss=9.90567
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.69341 | best_loss=10.69341
Epoch 1/80: current_loss=14.01002 | best_loss=10.69341
Epoch 2/80: current_loss=11.48587 | best_loss=10.69341
Epoch 3/80: current_loss=9.53309 | best_loss=9.53309
Epoch 4/80: current_loss=11.55904 | best_loss=9.53309
Epoch 5/80: current_loss=9.55257 | best_loss=9.53309
Epoch 6/80: current_loss=14.97703 | best_loss=9.53309
Epoch 7/80: current_loss=9.30468 | best_loss=9.30468
Epoch 8/80: current_loss=13.12333 | best_loss=9.30468
Epoch 9/80: current_loss=12.88930 | best_loss=9.30468
Epoch 10/80: current_loss=10.63286 | best_loss=9.30468
Epoch 11/80: current_loss=10.43382 | best_loss=9.30468
Epoch 12/80: current_loss=10.49318 | best_loss=9.30468
Epoch 13/80: current_loss=9.89876 | best_loss=9.30468
Epoch 14/80: current_loss=13.86697 | best_loss=9.30468
Epoch 15/80: current_loss=9.31516 | best_loss=9.30468
Epoch 16/80: current_loss=19.99239 | best_loss=9.30468
Epoch 17/80: current_loss=11.40414 | best_loss=9.30468
Epoch 18/80: current_loss=19.04337 | best_loss=9.30468
Epoch 19/80: current_loss=12.56837 | best_loss=9.30468
Epoch 20/80: current_loss=12.36498 | best_loss=9.30468
Epoch 21/80: current_loss=16.05089 | best_loss=9.30468
Epoch 22/80: current_loss=11.29879 | best_loss=9.30468
Epoch 23/80: current_loss=11.87668 | best_loss=9.30468
Epoch 24/80: current_loss=15.09138 | best_loss=9.30468
Epoch 25/80: current_loss=12.35218 | best_loss=9.30468
Epoch 26/80: current_loss=11.01842 | best_loss=9.30468
Epoch 27/80: current_loss=11.96817 | best_loss=9.30468
Early Stopping at epoch 27
      explained_var=-0.10141 | mse_loss=9.45372
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=16.71502 | best_loss=16.71502
Epoch 1/80: current_loss=15.79532 | best_loss=15.79532
Epoch 2/80: current_loss=18.93184 | best_loss=15.79532
Epoch 3/80: current_loss=11.43284 | best_loss=11.43284
Epoch 4/80: current_loss=12.49890 | best_loss=11.43284
Epoch 5/80: current_loss=10.50049 | best_loss=10.50049
Epoch 6/80: current_loss=13.31209 | best_loss=10.50049
Epoch 7/80: current_loss=17.43037 | best_loss=10.50049
Epoch 8/80: current_loss=24.83463 | best_loss=10.50049
Epoch 9/80: current_loss=21.81075 | best_loss=10.50049
Epoch 10/80: current_loss=21.44556 | best_loss=10.50049
Epoch 11/80: current_loss=10.23343 | best_loss=10.23343
Epoch 12/80: current_loss=22.73670 | best_loss=10.23343
Epoch 13/80: current_loss=21.97043 | best_loss=10.23343
Epoch 14/80: current_loss=13.91036 | best_loss=10.23343
Epoch 15/80: current_loss=10.16584 | best_loss=10.16584
Epoch 16/80: current_loss=11.84723 | best_loss=10.16584
Epoch 17/80: current_loss=18.71344 | best_loss=10.16584
Epoch 18/80: current_loss=13.49090 | best_loss=10.16584
Epoch 19/80: current_loss=10.68034 | best_loss=10.16584
Epoch 20/80: current_loss=10.44789 | best_loss=10.16584
Epoch 21/80: current_loss=12.67709 | best_loss=10.16584
Epoch 22/80: current_loss=15.08885 | best_loss=10.16584
Epoch 23/80: current_loss=12.57183 | best_loss=10.16584
Epoch 24/80: current_loss=11.28108 | best_loss=10.16584
Epoch 25/80: current_loss=9.95482 | best_loss=9.95482
Epoch 26/80: current_loss=24.54999 | best_loss=9.95482
Epoch 27/80: current_loss=9.69503 | best_loss=9.69503
Epoch 28/80: current_loss=26.24269 | best_loss=9.69503
Epoch 29/80: current_loss=10.04138 | best_loss=9.69503
Epoch 30/80: current_loss=11.54924 | best_loss=9.69503
Epoch 31/80: current_loss=18.11321 | best_loss=9.69503
Epoch 32/80: current_loss=27.88502 | best_loss=9.69503
Epoch 33/80: current_loss=9.98937 | best_loss=9.69503
Epoch 34/80: current_loss=11.70348 | best_loss=9.69503
Epoch 35/80: current_loss=17.13202 | best_loss=9.69503
Epoch 36/80: current_loss=23.67537 | best_loss=9.69503
Epoch 37/80: current_loss=9.99155 | best_loss=9.69503
Epoch 38/80: current_loss=11.20818 | best_loss=9.69503
Epoch 39/80: current_loss=9.86570 | best_loss=9.69503
Epoch 40/80: current_loss=10.03880 | best_loss=9.69503
Epoch 41/80: current_loss=15.04778 | best_loss=9.69503
Epoch 42/80: current_loss=13.98990 | best_loss=9.69503
Epoch 43/80: current_loss=9.67901 | best_loss=9.67901
Epoch 44/80: current_loss=9.80613 | best_loss=9.67901
Epoch 45/80: current_loss=12.69169 | best_loss=9.67901
Epoch 46/80: current_loss=28.50228 | best_loss=9.67901
Epoch 47/80: current_loss=14.70424 | best_loss=9.67901
Epoch 48/80: current_loss=13.32651 | best_loss=9.67901
Epoch 49/80: current_loss=10.54657 | best_loss=9.67901
Epoch 50/80: current_loss=11.64184 | best_loss=9.67901
Epoch 51/80: current_loss=10.70482 | best_loss=9.67901
Epoch 52/80: current_loss=9.95521 | best_loss=9.67901
Epoch 53/80: current_loss=12.10153 | best_loss=9.67901
Epoch 54/80: current_loss=12.10605 | best_loss=9.67901
Epoch 55/80: current_loss=18.22762 | best_loss=9.67901
Epoch 56/80: current_loss=10.61179 | best_loss=9.67901
Epoch 57/80: current_loss=12.69085 | best_loss=9.67901
Epoch 58/80: current_loss=11.79883 | best_loss=9.67901
Epoch 59/80: current_loss=17.64349 | best_loss=9.67901
Epoch 60/80: current_loss=23.90689 | best_loss=9.67901
Epoch 61/80: current_loss=11.41096 | best_loss=9.67901
Epoch 62/80: current_loss=12.75886 | best_loss=9.67901
Epoch 63/80: current_loss=10.76412 | best_loss=9.67901
Early Stopping at epoch 63
      explained_var=-0.01567 | mse_loss=9.76863
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=11.67827 | best_loss=11.67827
Epoch 1/80: current_loss=10.42530 | best_loss=10.42530
Epoch 2/80: current_loss=11.08984 | best_loss=10.42530
Epoch 3/80: current_loss=10.50917 | best_loss=10.42530
Epoch 4/80: current_loss=10.43118 | best_loss=10.42530
Epoch 5/80: current_loss=10.31723 | best_loss=10.31723
Epoch 6/80: current_loss=10.48790 | best_loss=10.31723
Epoch 7/80: current_loss=13.10020 | best_loss=10.31723
Epoch 8/80: current_loss=11.01237 | best_loss=10.31723
Epoch 9/80: current_loss=10.02624 | best_loss=10.02624
Epoch 10/80: current_loss=17.71052 | best_loss=10.02624
Epoch 11/80: current_loss=17.74631 | best_loss=10.02624
Epoch 12/80: current_loss=12.16107 | best_loss=10.02624
Epoch 13/80: current_loss=17.78117 | best_loss=10.02624
Epoch 14/80: current_loss=11.03273 | best_loss=10.02624
Epoch 15/80: current_loss=9.73075 | best_loss=9.73075
Epoch 16/80: current_loss=10.06239 | best_loss=9.73075
Epoch 17/80: current_loss=8.96200 | best_loss=8.96200
Epoch 18/80: current_loss=14.03613 | best_loss=8.96200
Epoch 19/80: current_loss=9.40531 | best_loss=8.96200
Epoch 20/80: current_loss=17.38191 | best_loss=8.96200
Epoch 21/80: current_loss=10.46827 | best_loss=8.96200
Epoch 22/80: current_loss=9.99870 | best_loss=8.96200
Epoch 23/80: current_loss=10.30630 | best_loss=8.96200
Epoch 24/80: current_loss=11.25974 | best_loss=8.96200
Epoch 25/80: current_loss=9.74039 | best_loss=8.96200
Epoch 26/80: current_loss=10.85140 | best_loss=8.96200
Epoch 27/80: current_loss=13.10806 | best_loss=8.96200
Epoch 28/80: current_loss=12.40864 | best_loss=8.96200
Epoch 29/80: current_loss=14.14953 | best_loss=8.96200
Epoch 30/80: current_loss=10.03933 | best_loss=8.96200
Epoch 31/80: current_loss=17.81706 | best_loss=8.96200
Epoch 32/80: current_loss=16.95256 | best_loss=8.96200
Epoch 33/80: current_loss=38.07611 | best_loss=8.96200
Epoch 34/80: current_loss=53.68528 | best_loss=8.96200
Epoch 35/80: current_loss=14.75113 | best_loss=8.96200
Epoch 36/80: current_loss=20.70229 | best_loss=8.96200
Epoch 37/80: current_loss=20.23877 | best_loss=8.96200
Early Stopping at epoch 37
      explained_var=-0.01107 | mse_loss=8.53425
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=14.33565 | best_loss=14.33565
Epoch 1/80: current_loss=14.39060 | best_loss=14.33565
Epoch 2/80: current_loss=11.09309 | best_loss=11.09309
Epoch 3/80: current_loss=12.55186 | best_loss=11.09309
Epoch 4/80: current_loss=9.17212 | best_loss=9.17212
Epoch 5/80: current_loss=11.44952 | best_loss=9.17212
Epoch 6/80: current_loss=11.24297 | best_loss=9.17212
Epoch 7/80: current_loss=10.46455 | best_loss=9.17212
Epoch 8/80: current_loss=10.73679 | best_loss=9.17212
Epoch 9/80: current_loss=21.08585 | best_loss=9.17212
Epoch 10/80: current_loss=12.92490 | best_loss=9.17212
Epoch 11/80: current_loss=9.21256 | best_loss=9.17212
Epoch 12/80: current_loss=11.88419 | best_loss=9.17212
Epoch 13/80: current_loss=32.90770 | best_loss=9.17212
Epoch 14/80: current_loss=10.77066 | best_loss=9.17212
Epoch 15/80: current_loss=11.52195 | best_loss=9.17212
Epoch 16/80: current_loss=14.97678 | best_loss=9.17212
Epoch 17/80: current_loss=8.90473 | best_loss=8.90473
Epoch 18/80: current_loss=12.85753 | best_loss=8.90473
Epoch 19/80: current_loss=8.80014 | best_loss=8.80014
Epoch 20/80: current_loss=24.10516 | best_loss=8.80014
Epoch 21/80: current_loss=24.35100 | best_loss=8.80014
Epoch 22/80: current_loss=12.17610 | best_loss=8.80014
Epoch 23/80: current_loss=8.59345 | best_loss=8.59345
Epoch 24/80: current_loss=23.85964 | best_loss=8.59345
Epoch 25/80: current_loss=12.40736 | best_loss=8.59345
Epoch 26/80: current_loss=10.65283 | best_loss=8.59345
Epoch 27/80: current_loss=9.29150 | best_loss=8.59345
Epoch 28/80: current_loss=13.86551 | best_loss=8.59345
Epoch 29/80: current_loss=14.41227 | best_loss=8.59345
Epoch 30/80: current_loss=9.07275 | best_loss=8.59345
Epoch 31/80: current_loss=16.17402 | best_loss=8.59345
Epoch 32/80: current_loss=19.04523 | best_loss=8.59345
Epoch 33/80: current_loss=10.94107 | best_loss=8.59345
Epoch 34/80: current_loss=12.66050 | best_loss=8.59345
Epoch 35/80: current_loss=11.85890 | best_loss=8.59345
Epoch 36/80: current_loss=18.97966 | best_loss=8.59345
Epoch 37/80: current_loss=8.75094 | best_loss=8.59345
Epoch 38/80: current_loss=11.89701 | best_loss=8.59345
Epoch 39/80: current_loss=9.59454 | best_loss=8.59345
Epoch 40/80: current_loss=16.05201 | best_loss=8.59345
Epoch 41/80: current_loss=9.12773 | best_loss=8.59345
Epoch 42/80: current_loss=10.01544 | best_loss=8.59345
Epoch 43/80: current_loss=8.58289 | best_loss=8.58289
Epoch 44/80: current_loss=8.54885 | best_loss=8.54885
Epoch 45/80: current_loss=9.71167 | best_loss=8.54885
Epoch 46/80: current_loss=16.49882 | best_loss=8.54885
Epoch 47/80: current_loss=21.14830 | best_loss=8.54885
Epoch 48/80: current_loss=12.30234 | best_loss=8.54885
Epoch 49/80: current_loss=8.83973 | best_loss=8.54885
Epoch 50/80: current_loss=16.30668 | best_loss=8.54885
Epoch 51/80: current_loss=52.95662 | best_loss=8.54885
Epoch 52/80: current_loss=38.59347 | best_loss=8.54885
Epoch 53/80: current_loss=9.21557 | best_loss=8.54885
Epoch 54/80: current_loss=9.06251 | best_loss=8.54885
Epoch 55/80: current_loss=16.73358 | best_loss=8.54885
Epoch 56/80: current_loss=10.45349 | best_loss=8.54885
Epoch 57/80: current_loss=54.39408 | best_loss=8.54885
Epoch 58/80: current_loss=18.65370 | best_loss=8.54885
Epoch 59/80: current_loss=24.27279 | best_loss=8.54885
Epoch 60/80: current_loss=8.65945 | best_loss=8.54885
Epoch 61/80: current_loss=8.90562 | best_loss=8.54885
Epoch 62/80: current_loss=8.53613 | best_loss=8.53613
Epoch 63/80: current_loss=12.78607 | best_loss=8.53613
Epoch 64/80: current_loss=8.61946 | best_loss=8.53613
Epoch 65/80: current_loss=13.64106 | best_loss=8.53613
Epoch 66/80: current_loss=21.85956 | best_loss=8.53613
Epoch 67/80: current_loss=21.31493 | best_loss=8.53613
Epoch 68/80: current_loss=24.56793 | best_loss=8.53613
Epoch 69/80: current_loss=11.59627 | best_loss=8.53613
Epoch 70/80: current_loss=8.90095 | best_loss=8.53613
Epoch 71/80: current_loss=13.09312 | best_loss=8.53613
Epoch 72/80: current_loss=9.39573 | best_loss=8.53613
Epoch 73/80: current_loss=10.63962 | best_loss=8.53613
Epoch 74/80: current_loss=18.80872 | best_loss=8.53613
Epoch 75/80: current_loss=19.36305 | best_loss=8.53613
Epoch 76/80: current_loss=27.36140 | best_loss=8.53613
Epoch 77/80: current_loss=9.07114 | best_loss=8.53613
Epoch 78/80: current_loss=8.78170 | best_loss=8.53613
Epoch 79/80: current_loss=24.41698 | best_loss=8.53613
      explained_var=-0.00334 | mse_loss=8.24425
----------------------------------------------
Average early_stopping_point: 32| avg_exp_var=-0.03258| avg_loss=9.18130
----------------------------------------------


----------------------------------------------
Params for Trial 10
{'learning_rate': 0.01, 'weight_decay': 0.0030741327184301173, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.16170 | best_loss=10.16170
Epoch 1/80: current_loss=9.91033 | best_loss=9.91033
Epoch 2/80: current_loss=11.93719 | best_loss=9.91033
Epoch 3/80: current_loss=11.84948 | best_loss=9.91033
Epoch 4/80: current_loss=9.92685 | best_loss=9.91033
Epoch 5/80: current_loss=10.17226 | best_loss=9.91033
Epoch 6/80: current_loss=9.91683 | best_loss=9.91033
Epoch 7/80: current_loss=10.05417 | best_loss=9.91033
Epoch 8/80: current_loss=10.04047 | best_loss=9.91033
Epoch 9/80: current_loss=10.17877 | best_loss=9.91033
Epoch 10/80: current_loss=9.95483 | best_loss=9.91033
Epoch 11/80: current_loss=10.78304 | best_loss=9.91033
Epoch 12/80: current_loss=10.21525 | best_loss=9.91033
Epoch 13/80: current_loss=10.69002 | best_loss=9.91033
Epoch 14/80: current_loss=12.48883 | best_loss=9.91033
Epoch 15/80: current_loss=10.96189 | best_loss=9.91033
Epoch 16/80: current_loss=9.89694 | best_loss=9.89694
Epoch 17/80: current_loss=9.92750 | best_loss=9.89694
Epoch 18/80: current_loss=10.53624 | best_loss=9.89694
Epoch 19/80: current_loss=9.93501 | best_loss=9.89694
Epoch 20/80: current_loss=9.95584 | best_loss=9.89694
Epoch 21/80: current_loss=9.90827 | best_loss=9.89694
Epoch 22/80: current_loss=11.23501 | best_loss=9.89694
Epoch 23/80: current_loss=10.82783 | best_loss=9.89694
Epoch 24/80: current_loss=12.06950 | best_loss=9.89694
Epoch 25/80: current_loss=10.01075 | best_loss=9.89694
Epoch 26/80: current_loss=9.91440 | best_loss=9.89694
Epoch 27/80: current_loss=11.89599 | best_loss=9.89694
Epoch 28/80: current_loss=10.23227 | best_loss=9.89694
Epoch 29/80: current_loss=9.93182 | best_loss=9.89694
Epoch 30/80: current_loss=9.94937 | best_loss=9.89694
Epoch 31/80: current_loss=9.91548 | best_loss=9.89694
Epoch 32/80: current_loss=9.92706 | best_loss=9.89694
Epoch 33/80: current_loss=10.60190 | best_loss=9.89694
Epoch 34/80: current_loss=9.92426 | best_loss=9.89694
Epoch 35/80: current_loss=9.90403 | best_loss=9.89694
Epoch 36/80: current_loss=9.98933 | best_loss=9.89694
Early Stopping at epoch 36
      explained_var=0.00180 | mse_loss=9.55992
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.93823 | best_loss=8.93823
Epoch 1/80: current_loss=10.46604 | best_loss=8.93823
Epoch 2/80: current_loss=9.60399 | best_loss=8.93823
Epoch 3/80: current_loss=8.50905 | best_loss=8.50905
Epoch 4/80: current_loss=8.65227 | best_loss=8.50905
Epoch 5/80: current_loss=10.04693 | best_loss=8.50905
Epoch 6/80: current_loss=8.69150 | best_loss=8.50905
Epoch 7/80: current_loss=8.74878 | best_loss=8.50905
Epoch 8/80: current_loss=8.79407 | best_loss=8.50905
Epoch 9/80: current_loss=9.04077 | best_loss=8.50905
Epoch 10/80: current_loss=9.08280 | best_loss=8.50905
Epoch 11/80: current_loss=9.05723 | best_loss=8.50905
Epoch 12/80: current_loss=8.68294 | best_loss=8.50905
Epoch 13/80: current_loss=11.18511 | best_loss=8.50905
Epoch 14/80: current_loss=8.62607 | best_loss=8.50905
Epoch 15/80: current_loss=8.64015 | best_loss=8.50905
Epoch 16/80: current_loss=8.68362 | best_loss=8.50905
Epoch 17/80: current_loss=8.99521 | best_loss=8.50905
Epoch 18/80: current_loss=8.96820 | best_loss=8.50905
Epoch 19/80: current_loss=8.47469 | best_loss=8.47469
Epoch 20/80: current_loss=9.02472 | best_loss=8.47469
Epoch 21/80: current_loss=8.94134 | best_loss=8.47469
Epoch 22/80: current_loss=9.77698 | best_loss=8.47469
Epoch 23/80: current_loss=8.88196 | best_loss=8.47469
Epoch 24/80: current_loss=23.36381 | best_loss=8.47469
Epoch 25/80: current_loss=40.63671 | best_loss=8.47469
Epoch 26/80: current_loss=29.43032 | best_loss=8.47469
Epoch 27/80: current_loss=22.55327 | best_loss=8.47469
Epoch 28/80: current_loss=18.16083 | best_loss=8.47469
Epoch 29/80: current_loss=15.56513 | best_loss=8.47469
Epoch 30/80: current_loss=13.31243 | best_loss=8.47469
Epoch 31/80: current_loss=12.05817 | best_loss=8.47469
Epoch 32/80: current_loss=11.03411 | best_loss=8.47469
Epoch 33/80: current_loss=10.36114 | best_loss=8.47469
Epoch 34/80: current_loss=10.66765 | best_loss=8.47469
Epoch 35/80: current_loss=9.77578 | best_loss=8.47469
Epoch 36/80: current_loss=8.96139 | best_loss=8.47469
Epoch 37/80: current_loss=10.57399 | best_loss=8.47469
Epoch 38/80: current_loss=8.74757 | best_loss=8.47469
Epoch 39/80: current_loss=8.88307 | best_loss=8.47469
Early Stopping at epoch 39
      explained_var=-0.00435 | mse_loss=8.63416
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.18183 | best_loss=10.18183
Epoch 1/80: current_loss=10.35384 | best_loss=10.18183
Epoch 2/80: current_loss=10.81906 | best_loss=10.18183
Epoch 3/80: current_loss=9.88556 | best_loss=9.88556
Epoch 4/80: current_loss=11.26766 | best_loss=9.88556
Epoch 5/80: current_loss=9.34700 | best_loss=9.34700
Epoch 6/80: current_loss=9.67513 | best_loss=9.34700
Epoch 7/80: current_loss=9.63832 | best_loss=9.34700
Epoch 8/80: current_loss=9.71672 | best_loss=9.34700
Epoch 9/80: current_loss=10.63707 | best_loss=9.34700
Epoch 10/80: current_loss=9.56379 | best_loss=9.34700
Epoch 11/80: current_loss=10.86065 | best_loss=9.34700
Epoch 12/80: current_loss=9.33490 | best_loss=9.33490
Epoch 13/80: current_loss=9.64547 | best_loss=9.33490
Epoch 14/80: current_loss=9.75644 | best_loss=9.33490
Epoch 15/80: current_loss=9.97977 | best_loss=9.33490
Epoch 16/80: current_loss=10.75737 | best_loss=9.33490
Epoch 17/80: current_loss=9.51687 | best_loss=9.33490
Epoch 18/80: current_loss=10.09241 | best_loss=9.33490
Epoch 19/80: current_loss=9.41466 | best_loss=9.33490
Epoch 20/80: current_loss=9.89937 | best_loss=9.33490
Epoch 21/80: current_loss=10.18988 | best_loss=9.33490
Epoch 22/80: current_loss=11.15708 | best_loss=9.33490
Epoch 23/80: current_loss=10.06687 | best_loss=9.33490
Epoch 24/80: current_loss=9.75198 | best_loss=9.33490
Epoch 25/80: current_loss=10.76524 | best_loss=9.33490
Epoch 26/80: current_loss=9.56286 | best_loss=9.33490
Epoch 27/80: current_loss=9.90628 | best_loss=9.33490
Epoch 28/80: current_loss=9.63987 | best_loss=9.33490
Epoch 29/80: current_loss=11.94251 | best_loss=9.33490
Epoch 30/80: current_loss=10.08077 | best_loss=9.33490
Epoch 31/80: current_loss=13.93999 | best_loss=9.33490
Epoch 32/80: current_loss=9.51569 | best_loss=9.33490
Early Stopping at epoch 32
      explained_var=-0.00001 | mse_loss=9.39361
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.44301 | best_loss=9.44301
Epoch 1/80: current_loss=9.18833 | best_loss=9.18833
Epoch 2/80: current_loss=9.80684 | best_loss=9.18833
Epoch 3/80: current_loss=8.92992 | best_loss=8.92992
Epoch 4/80: current_loss=9.13798 | best_loss=8.92992
Epoch 5/80: current_loss=10.79763 | best_loss=8.92992
Epoch 6/80: current_loss=12.91390 | best_loss=8.92992
Epoch 7/80: current_loss=9.08155 | best_loss=8.92992
Epoch 8/80: current_loss=9.70860 | best_loss=8.92992
Epoch 9/80: current_loss=9.17831 | best_loss=8.92992
Epoch 10/80: current_loss=9.14517 | best_loss=8.92992
Epoch 11/80: current_loss=9.49025 | best_loss=8.92992
Epoch 12/80: current_loss=9.10023 | best_loss=8.92992
Epoch 13/80: current_loss=9.63363 | best_loss=8.92992
Epoch 14/80: current_loss=8.95415 | best_loss=8.92992
Epoch 15/80: current_loss=12.37817 | best_loss=8.92992
Epoch 16/80: current_loss=10.25173 | best_loss=8.92992
Epoch 17/80: current_loss=9.03419 | best_loss=8.92992
Epoch 18/80: current_loss=9.23528 | best_loss=8.92992
Epoch 19/80: current_loss=9.64281 | best_loss=8.92992
Epoch 20/80: current_loss=9.95355 | best_loss=8.92992
Epoch 21/80: current_loss=9.16494 | best_loss=8.92992
Epoch 22/80: current_loss=9.31480 | best_loss=8.92992
Epoch 23/80: current_loss=13.48431 | best_loss=8.92992
Early Stopping at epoch 23
      explained_var=-0.00602 | mse_loss=8.47066
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50494 | best_loss=8.50494
Epoch 1/80: current_loss=9.17680 | best_loss=8.50494
Epoch 2/80: current_loss=8.50640 | best_loss=8.50494
Epoch 3/80: current_loss=10.59880 | best_loss=8.50494
Epoch 4/80: current_loss=9.81559 | best_loss=8.50494
Epoch 5/80: current_loss=8.83394 | best_loss=8.50494
Epoch 6/80: current_loss=8.55281 | best_loss=8.50494
Epoch 7/80: current_loss=9.11438 | best_loss=8.50494
Epoch 8/80: current_loss=9.80186 | best_loss=8.50494
Epoch 9/80: current_loss=8.76469 | best_loss=8.50494
Epoch 10/80: current_loss=8.47052 | best_loss=8.47052
Epoch 11/80: current_loss=9.18327 | best_loss=8.47052
Epoch 12/80: current_loss=9.27079 | best_loss=8.47052
Epoch 13/80: current_loss=9.06090 | best_loss=8.47052
Epoch 14/80: current_loss=8.51030 | best_loss=8.47052
Epoch 15/80: current_loss=10.58876 | best_loss=8.47052
Epoch 16/80: current_loss=9.22798 | best_loss=8.47052
Epoch 17/80: current_loss=8.77501 | best_loss=8.47052
Epoch 18/80: current_loss=8.61458 | best_loss=8.47052
Epoch 19/80: current_loss=9.21730 | best_loss=8.47052
Epoch 20/80: current_loss=8.97019 | best_loss=8.47052
Epoch 21/80: current_loss=8.74064 | best_loss=8.47052
Epoch 22/80: current_loss=8.68786 | best_loss=8.47052
Epoch 23/80: current_loss=9.54319 | best_loss=8.47052
Epoch 24/80: current_loss=8.63088 | best_loss=8.47052
Epoch 25/80: current_loss=10.02179 | best_loss=8.47052
Epoch 26/80: current_loss=9.34254 | best_loss=8.47052
Epoch 27/80: current_loss=9.80218 | best_loss=8.47052
Epoch 28/80: current_loss=10.95620 | best_loss=8.47052
Epoch 29/80: current_loss=9.21684 | best_loss=8.47052
Epoch 30/80: current_loss=8.68814 | best_loss=8.47052
Early Stopping at epoch 30
      explained_var=0.00107 | mse_loss=8.19166
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=-0.00150| avg_loss=8.85000
----------------------------------------------


----------------------------------------------
Params for Trial 11
{'learning_rate': 0.0001, 'weight_decay': 0.002554434662107642, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=36.81088 | best_loss=36.81088
Epoch 1/80: current_loss=18.28492 | best_loss=18.28492
Epoch 2/80: current_loss=14.42670 | best_loss=14.42670
Epoch 3/80: current_loss=12.64113 | best_loss=12.64113
Epoch 4/80: current_loss=11.63212 | best_loss=11.63212
Epoch 5/80: current_loss=11.05296 | best_loss=11.05296
Epoch 6/80: current_loss=10.72302 | best_loss=10.72302
Epoch 7/80: current_loss=10.53274 | best_loss=10.53274
Epoch 8/80: current_loss=10.39597 | best_loss=10.39597
Epoch 9/80: current_loss=10.30171 | best_loss=10.30171
Epoch 10/80: current_loss=10.23921 | best_loss=10.23921
Epoch 11/80: current_loss=10.18212 | best_loss=10.18212
Epoch 12/80: current_loss=10.13932 | best_loss=10.13932
Epoch 13/80: current_loss=10.09951 | best_loss=10.09951
Epoch 14/80: current_loss=10.07286 | best_loss=10.07286
Epoch 15/80: current_loss=10.04933 | best_loss=10.04933
Epoch 16/80: current_loss=10.02174 | best_loss=10.02174
Epoch 17/80: current_loss=9.99905 | best_loss=9.99905
Epoch 18/80: current_loss=9.97698 | best_loss=9.97698
Epoch 19/80: current_loss=9.96023 | best_loss=9.96023
Epoch 20/80: current_loss=9.94899 | best_loss=9.94899
Epoch 21/80: current_loss=9.94338 | best_loss=9.94338
Epoch 22/80: current_loss=9.94159 | best_loss=9.94159
Epoch 23/80: current_loss=9.93564 | best_loss=9.93564
Epoch 24/80: current_loss=9.93512 | best_loss=9.93512
Epoch 25/80: current_loss=9.92237 | best_loss=9.92237
Epoch 26/80: current_loss=9.91913 | best_loss=9.91913
Epoch 27/80: current_loss=9.91576 | best_loss=9.91576
Epoch 28/80: current_loss=9.91393 | best_loss=9.91393
Epoch 29/80: current_loss=9.91937 | best_loss=9.91393
Epoch 30/80: current_loss=9.91946 | best_loss=9.91393
Epoch 31/80: current_loss=9.91595 | best_loss=9.91393
Epoch 32/80: current_loss=9.91757 | best_loss=9.91393
Epoch 33/80: current_loss=9.91361 | best_loss=9.91361
Epoch 34/80: current_loss=9.91694 | best_loss=9.91361
Epoch 35/80: current_loss=9.91688 | best_loss=9.91361
Epoch 36/80: current_loss=9.91773 | best_loss=9.91361
Epoch 37/80: current_loss=9.91462 | best_loss=9.91361
Epoch 38/80: current_loss=9.90546 | best_loss=9.90546
Epoch 39/80: current_loss=9.91441 | best_loss=9.90546
Epoch 40/80: current_loss=9.91746 | best_loss=9.90546
Epoch 41/80: current_loss=9.91195 | best_loss=9.90546
Epoch 42/80: current_loss=9.91556 | best_loss=9.90546
Epoch 43/80: current_loss=9.91072 | best_loss=9.90546
Epoch 44/80: current_loss=9.90578 | best_loss=9.90546
Epoch 45/80: current_loss=9.91385 | best_loss=9.90546
Epoch 46/80: current_loss=9.90842 | best_loss=9.90546
Epoch 47/80: current_loss=9.91103 | best_loss=9.90546
Epoch 48/80: current_loss=9.91272 | best_loss=9.90546
Epoch 49/80: current_loss=9.91182 | best_loss=9.90546
Epoch 50/80: current_loss=9.91678 | best_loss=9.90546
Epoch 51/80: current_loss=9.91729 | best_loss=9.90546
Epoch 52/80: current_loss=9.91284 | best_loss=9.90546
Epoch 53/80: current_loss=9.91679 | best_loss=9.90546
Epoch 54/80: current_loss=9.90913 | best_loss=9.90546
Epoch 55/80: current_loss=9.91067 | best_loss=9.90546
Epoch 56/80: current_loss=9.91434 | best_loss=9.90546
Epoch 57/80: current_loss=9.90919 | best_loss=9.90546
Epoch 58/80: current_loss=9.91773 | best_loss=9.90546
Early Stopping at epoch 58
      explained_var=0.00247 | mse_loss=9.58279
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.47992 | best_loss=8.47992
Epoch 1/80: current_loss=8.47437 | best_loss=8.47437
Epoch 2/80: current_loss=8.47072 | best_loss=8.47072
Epoch 3/80: current_loss=8.47418 | best_loss=8.47072
Epoch 4/80: current_loss=8.47461 | best_loss=8.47072
Epoch 5/80: current_loss=8.47876 | best_loss=8.47072
Epoch 6/80: current_loss=8.47978 | best_loss=8.47072
Epoch 7/80: current_loss=8.48373 | best_loss=8.47072
Epoch 8/80: current_loss=8.48885 | best_loss=8.47072
Epoch 9/80: current_loss=8.48648 | best_loss=8.47072
Epoch 10/80: current_loss=8.48127 | best_loss=8.47072
Epoch 11/80: current_loss=8.48364 | best_loss=8.47072
Epoch 12/80: current_loss=8.48879 | best_loss=8.47072
Epoch 13/80: current_loss=8.48925 | best_loss=8.47072
Epoch 14/80: current_loss=8.48059 | best_loss=8.47072
Epoch 15/80: current_loss=8.48059 | best_loss=8.47072
Epoch 16/80: current_loss=8.48721 | best_loss=8.47072
Epoch 17/80: current_loss=8.48398 | best_loss=8.47072
Epoch 18/80: current_loss=8.48467 | best_loss=8.47072
Epoch 19/80: current_loss=8.48680 | best_loss=8.47072
Epoch 20/80: current_loss=8.49234 | best_loss=8.47072
Epoch 21/80: current_loss=8.49327 | best_loss=8.47072
Epoch 22/80: current_loss=8.50137 | best_loss=8.47072
Early Stopping at epoch 22
      explained_var=0.00215 | mse_loss=8.55914
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.41180 | best_loss=9.41180
Epoch 1/80: current_loss=9.42476 | best_loss=9.41180
Epoch 2/80: current_loss=9.41677 | best_loss=9.41180
Epoch 3/80: current_loss=9.39535 | best_loss=9.39535
Epoch 4/80: current_loss=9.39709 | best_loss=9.39535
Epoch 5/80: current_loss=9.40938 | best_loss=9.39535
Epoch 6/80: current_loss=9.42211 | best_loss=9.39535
Epoch 7/80: current_loss=9.42928 | best_loss=9.39535
Epoch 8/80: current_loss=9.41784 | best_loss=9.39535
Epoch 9/80: current_loss=9.42778 | best_loss=9.39535
Epoch 10/80: current_loss=9.41918 | best_loss=9.39535
Epoch 11/80: current_loss=9.40365 | best_loss=9.39535
Epoch 12/80: current_loss=9.42330 | best_loss=9.39535
Epoch 13/80: current_loss=9.43576 | best_loss=9.39535
Epoch 14/80: current_loss=9.40947 | best_loss=9.39535
Epoch 15/80: current_loss=9.45289 | best_loss=9.39535
Epoch 16/80: current_loss=9.41944 | best_loss=9.39535
Epoch 17/80: current_loss=9.42721 | best_loss=9.39535
Epoch 18/80: current_loss=9.42953 | best_loss=9.39535
Epoch 19/80: current_loss=9.38039 | best_loss=9.38039
Epoch 20/80: current_loss=9.45144 | best_loss=9.38039
Epoch 21/80: current_loss=9.40872 | best_loss=9.38039
Epoch 22/80: current_loss=9.40332 | best_loss=9.38039
Epoch 23/80: current_loss=9.41584 | best_loss=9.38039
Epoch 24/80: current_loss=9.43739 | best_loss=9.38039
Epoch 25/80: current_loss=9.41258 | best_loss=9.38039
Epoch 26/80: current_loss=9.41186 | best_loss=9.38039
Epoch 27/80: current_loss=9.39580 | best_loss=9.38039
Epoch 28/80: current_loss=9.43948 | best_loss=9.38039
Epoch 29/80: current_loss=9.42755 | best_loss=9.38039
Epoch 30/80: current_loss=9.42561 | best_loss=9.38039
Epoch 31/80: current_loss=9.40467 | best_loss=9.38039
Epoch 32/80: current_loss=9.39070 | best_loss=9.38039
Epoch 33/80: current_loss=9.43317 | best_loss=9.38039
Epoch 34/80: current_loss=9.43558 | best_loss=9.38039
Epoch 35/80: current_loss=9.41929 | best_loss=9.38039
Epoch 36/80: current_loss=9.40797 | best_loss=9.38039
Epoch 37/80: current_loss=9.43528 | best_loss=9.38039
Epoch 38/80: current_loss=9.43503 | best_loss=9.38039
Epoch 39/80: current_loss=9.42747 | best_loss=9.38039
Early Stopping at epoch 39
      explained_var=-0.00592 | mse_loss=9.52526
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.97721 | best_loss=8.97721
Epoch 1/80: current_loss=8.97451 | best_loss=8.97451
Epoch 2/80: current_loss=8.97564 | best_loss=8.97451
Epoch 3/80: current_loss=8.96276 | best_loss=8.96276
Epoch 4/80: current_loss=8.97574 | best_loss=8.96276
Epoch 5/80: current_loss=8.97528 | best_loss=8.96276
Epoch 6/80: current_loss=8.97528 | best_loss=8.96276
Epoch 7/80: current_loss=8.97644 | best_loss=8.96276
Epoch 8/80: current_loss=8.97032 | best_loss=8.96276
Epoch 9/80: current_loss=8.96711 | best_loss=8.96276
Epoch 10/80: current_loss=8.98213 | best_loss=8.96276
Epoch 11/80: current_loss=8.97467 | best_loss=8.96276
Epoch 12/80: current_loss=8.97710 | best_loss=8.96276
Epoch 13/80: current_loss=8.94713 | best_loss=8.94713
Epoch 14/80: current_loss=8.97375 | best_loss=8.94713
Epoch 15/80: current_loss=8.96667 | best_loss=8.94713
Epoch 16/80: current_loss=8.96837 | best_loss=8.94713
Epoch 17/80: current_loss=8.97252 | best_loss=8.94713
Epoch 18/80: current_loss=8.97322 | best_loss=8.94713
Epoch 19/80: current_loss=8.97079 | best_loss=8.94713
Epoch 20/80: current_loss=8.97560 | best_loss=8.94713
Epoch 21/80: current_loss=8.97480 | best_loss=8.94713
Epoch 22/80: current_loss=8.96815 | best_loss=8.94713
Epoch 23/80: current_loss=8.95545 | best_loss=8.94713
Epoch 24/80: current_loss=8.96948 | best_loss=8.94713
Epoch 25/80: current_loss=8.97326 | best_loss=8.94713
Epoch 26/80: current_loss=8.96489 | best_loss=8.94713
Epoch 27/80: current_loss=8.96029 | best_loss=8.94713
Epoch 28/80: current_loss=8.97569 | best_loss=8.94713
Epoch 29/80: current_loss=8.98556 | best_loss=8.94713
Epoch 30/80: current_loss=8.97328 | best_loss=8.94713
Epoch 31/80: current_loss=8.97307 | best_loss=8.94713
Epoch 32/80: current_loss=8.98990 | best_loss=8.94713
Epoch 33/80: current_loss=8.98445 | best_loss=8.94713
Early Stopping at epoch 33
      explained_var=0.00134 | mse_loss=8.36133
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.48605 | best_loss=8.48605
Epoch 1/80: current_loss=8.48588 | best_loss=8.48588
Epoch 2/80: current_loss=8.48617 | best_loss=8.48588
Epoch 3/80: current_loss=8.48754 | best_loss=8.48588
Epoch 4/80: current_loss=8.48647 | best_loss=8.48588
Epoch 5/80: current_loss=8.48714 | best_loss=8.48588
Epoch 6/80: current_loss=8.48594 | best_loss=8.48588
Epoch 7/80: current_loss=8.48555 | best_loss=8.48555
Epoch 8/80: current_loss=8.48572 | best_loss=8.48555
Epoch 9/80: current_loss=8.48671 | best_loss=8.48555
Epoch 10/80: current_loss=8.48504 | best_loss=8.48504
Epoch 11/80: current_loss=8.48514 | best_loss=8.48504
Epoch 12/80: current_loss=8.48776 | best_loss=8.48504
Epoch 13/80: current_loss=8.48552 | best_loss=8.48504
Epoch 14/80: current_loss=8.48568 | best_loss=8.48504
Epoch 15/80: current_loss=8.48569 | best_loss=8.48504
Epoch 16/80: current_loss=8.48431 | best_loss=8.48431
Epoch 17/80: current_loss=8.48509 | best_loss=8.48431
Epoch 18/80: current_loss=8.48448 | best_loss=8.48431
Epoch 19/80: current_loss=8.48461 | best_loss=8.48431
Epoch 20/80: current_loss=8.48416 | best_loss=8.48416
Epoch 21/80: current_loss=8.48391 | best_loss=8.48391
Epoch 22/80: current_loss=8.48365 | best_loss=8.48365
Epoch 23/80: current_loss=8.48362 | best_loss=8.48362
Epoch 24/80: current_loss=8.48400 | best_loss=8.48362
Epoch 25/80: current_loss=8.48427 | best_loss=8.48362
Epoch 26/80: current_loss=8.48291 | best_loss=8.48291
Epoch 27/80: current_loss=8.48295 | best_loss=8.48291
Epoch 28/80: current_loss=8.48446 | best_loss=8.48291
Epoch 29/80: current_loss=8.48332 | best_loss=8.48291
Epoch 30/80: current_loss=8.48458 | best_loss=8.48291
Epoch 31/80: current_loss=8.48290 | best_loss=8.48290
Epoch 32/80: current_loss=8.48332 | best_loss=8.48290
Epoch 33/80: current_loss=8.48376 | best_loss=8.48290
Epoch 34/80: current_loss=8.48351 | best_loss=8.48290
Epoch 35/80: current_loss=8.48373 | best_loss=8.48290
Epoch 36/80: current_loss=8.48484 | best_loss=8.48290
Epoch 37/80: current_loss=8.48315 | best_loss=8.48290
Epoch 38/80: current_loss=8.48309 | best_loss=8.48290
Epoch 39/80: current_loss=8.48327 | best_loss=8.48290
Epoch 40/80: current_loss=8.48413 | best_loss=8.48290
Epoch 41/80: current_loss=8.48485 | best_loss=8.48290
Epoch 42/80: current_loss=8.48507 | best_loss=8.48290
Epoch 43/80: current_loss=8.48374 | best_loss=8.48290
Epoch 44/80: current_loss=8.48341 | best_loss=8.48290
Epoch 45/80: current_loss=8.48823 | best_loss=8.48290
Epoch 46/80: current_loss=8.48636 | best_loss=8.48290
Epoch 47/80: current_loss=8.48462 | best_loss=8.48290
Epoch 48/80: current_loss=8.48453 | best_loss=8.48290
Epoch 49/80: current_loss=8.48408 | best_loss=8.48290
Epoch 50/80: current_loss=8.48497 | best_loss=8.48290
Epoch 51/80: current_loss=8.48373 | best_loss=8.48290
Early Stopping at epoch 51
      explained_var=0.00027 | mse_loss=8.19853
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00006| avg_loss=8.84541
----------------------------------------------


----------------------------------------------
Params for Trial 12
{'learning_rate': 0.001, 'weight_decay': 8.686045909808823e-05, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.16986 | best_loss=10.16986
Epoch 1/80: current_loss=9.92600 | best_loss=9.92600
Epoch 2/80: current_loss=9.90124 | best_loss=9.90124
Epoch 3/80: current_loss=9.97702 | best_loss=9.90124
Epoch 4/80: current_loss=9.94414 | best_loss=9.90124
Epoch 5/80: current_loss=9.88980 | best_loss=9.88980
Epoch 6/80: current_loss=9.89143 | best_loss=9.88980
Epoch 7/80: current_loss=9.90658 | best_loss=9.88980
Epoch 8/80: current_loss=9.89228 | best_loss=9.88980
Epoch 9/80: current_loss=9.91263 | best_loss=9.88980
Epoch 10/80: current_loss=9.92161 | best_loss=9.88980
Epoch 11/80: current_loss=9.90956 | best_loss=9.88980
Epoch 12/80: current_loss=9.92933 | best_loss=9.88980
Epoch 13/80: current_loss=9.90432 | best_loss=9.88980
Epoch 14/80: current_loss=9.95235 | best_loss=9.88980
Epoch 15/80: current_loss=9.89946 | best_loss=9.88980
Epoch 16/80: current_loss=9.92720 | best_loss=9.88980
Epoch 17/80: current_loss=9.90928 | best_loss=9.88980
Epoch 18/80: current_loss=9.98609 | best_loss=9.88980
Epoch 19/80: current_loss=9.91952 | best_loss=9.88980
Epoch 20/80: current_loss=9.97255 | best_loss=9.88980
Epoch 21/80: current_loss=9.99612 | best_loss=9.88980
Epoch 22/80: current_loss=9.94919 | best_loss=9.88980
Epoch 23/80: current_loss=9.90246 | best_loss=9.88980
Epoch 24/80: current_loss=9.90199 | best_loss=9.88980
Epoch 25/80: current_loss=9.90432 | best_loss=9.88980
Early Stopping at epoch 25
      explained_var=0.00237 | mse_loss=9.55993
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.55129 | best_loss=8.55129
Epoch 1/80: current_loss=8.50977 | best_loss=8.50977
Epoch 2/80: current_loss=8.52989 | best_loss=8.50977
Epoch 3/80: current_loss=8.53536 | best_loss=8.50977
Epoch 4/80: current_loss=8.50704 | best_loss=8.50704
Epoch 5/80: current_loss=8.51446 | best_loss=8.50704
Epoch 6/80: current_loss=8.51256 | best_loss=8.50704
Epoch 7/80: current_loss=8.57730 | best_loss=8.50704
Epoch 8/80: current_loss=8.55744 | best_loss=8.50704
Epoch 9/80: current_loss=8.51858 | best_loss=8.50704
Epoch 10/80: current_loss=8.51337 | best_loss=8.50704
Epoch 11/80: current_loss=8.52352 | best_loss=8.50704
Epoch 12/80: current_loss=8.53093 | best_loss=8.50704
Epoch 13/80: current_loss=8.55216 | best_loss=8.50704
Epoch 14/80: current_loss=8.55309 | best_loss=8.50704
Epoch 15/80: current_loss=8.51217 | best_loss=8.50704
Epoch 16/80: current_loss=8.52013 | best_loss=8.50704
Epoch 17/80: current_loss=8.51679 | best_loss=8.50704
Epoch 18/80: current_loss=8.51951 | best_loss=8.50704
Epoch 19/80: current_loss=8.51738 | best_loss=8.50704
Epoch 20/80: current_loss=8.59342 | best_loss=8.50704
Epoch 21/80: current_loss=8.51477 | best_loss=8.50704
Epoch 22/80: current_loss=8.59618 | best_loss=8.50704
Epoch 23/80: current_loss=8.53920 | best_loss=8.50704
Epoch 24/80: current_loss=8.53319 | best_loss=8.50704
Early Stopping at epoch 24
      explained_var=0.00038 | mse_loss=8.57317
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.40390 | best_loss=9.40390
Epoch 1/80: current_loss=9.34429 | best_loss=9.34429
Epoch 2/80: current_loss=9.40180 | best_loss=9.34429
Epoch 3/80: current_loss=9.78586 | best_loss=9.34429
Epoch 4/80: current_loss=9.41595 | best_loss=9.34429
Epoch 5/80: current_loss=9.48222 | best_loss=9.34429
Epoch 6/80: current_loss=9.58138 | best_loss=9.34429
Epoch 7/80: current_loss=9.57343 | best_loss=9.34429
Epoch 8/80: current_loss=9.45605 | best_loss=9.34429
Epoch 9/80: current_loss=9.36495 | best_loss=9.34429
Epoch 10/80: current_loss=9.59159 | best_loss=9.34429
Epoch 11/80: current_loss=10.10184 | best_loss=9.34429
Epoch 12/80: current_loss=9.47776 | best_loss=9.34429
Epoch 13/80: current_loss=9.39505 | best_loss=9.34429
Epoch 14/80: current_loss=9.53547 | best_loss=9.34429
Epoch 15/80: current_loss=9.29842 | best_loss=9.29842
Epoch 16/80: current_loss=9.41494 | best_loss=9.29842
Epoch 17/80: current_loss=9.38497 | best_loss=9.29842
Epoch 18/80: current_loss=9.30569 | best_loss=9.29842
Epoch 19/80: current_loss=9.56054 | best_loss=9.29842
Epoch 20/80: current_loss=9.68365 | best_loss=9.29842
Epoch 21/80: current_loss=9.52536 | best_loss=9.29842
Epoch 22/80: current_loss=9.36508 | best_loss=9.29842
Epoch 23/80: current_loss=9.25505 | best_loss=9.25505
Epoch 24/80: current_loss=9.39807 | best_loss=9.25505
Epoch 25/80: current_loss=9.41608 | best_loss=9.25505
Epoch 26/80: current_loss=9.39353 | best_loss=9.25505
Epoch 27/80: current_loss=9.48921 | best_loss=9.25505
Epoch 28/80: current_loss=9.44346 | best_loss=9.25505
Epoch 29/80: current_loss=9.54507 | best_loss=9.25505
Epoch 30/80: current_loss=9.36539 | best_loss=9.25505
Epoch 31/80: current_loss=9.32382 | best_loss=9.25505
Epoch 32/80: current_loss=9.52074 | best_loss=9.25505
Epoch 33/80: current_loss=9.32530 | best_loss=9.25505
Epoch 34/80: current_loss=9.30916 | best_loss=9.25505
Epoch 35/80: current_loss=9.26068 | best_loss=9.25505
Epoch 36/80: current_loss=9.25709 | best_loss=9.25505
Epoch 37/80: current_loss=9.34669 | best_loss=9.25505
Epoch 38/80: current_loss=9.33792 | best_loss=9.25505
Epoch 39/80: current_loss=9.74347 | best_loss=9.25505
Epoch 40/80: current_loss=9.50938 | best_loss=9.25505
Epoch 41/80: current_loss=9.69621 | best_loss=9.25505
Epoch 42/80: current_loss=9.39576 | best_loss=9.25505
Epoch 43/80: current_loss=9.52586 | best_loss=9.25505
Early Stopping at epoch 43
      explained_var=0.00057 | mse_loss=9.37760
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.98918 | best_loss=8.98918
Epoch 1/80: current_loss=8.97864 | best_loss=8.97864
Epoch 2/80: current_loss=8.96586 | best_loss=8.96586
Epoch 3/80: current_loss=8.97117 | best_loss=8.96586
Epoch 4/80: current_loss=8.99095 | best_loss=8.96586
Epoch 5/80: current_loss=8.96537 | best_loss=8.96537
Epoch 6/80: current_loss=9.00569 | best_loss=8.96537
Epoch 7/80: current_loss=8.96931 | best_loss=8.96537
Epoch 8/80: current_loss=8.96848 | best_loss=8.96537
Epoch 9/80: current_loss=9.09781 | best_loss=8.96537
Epoch 10/80: current_loss=9.10275 | best_loss=8.96537
Epoch 11/80: current_loss=8.98217 | best_loss=8.96537
Epoch 12/80: current_loss=8.96035 | best_loss=8.96035
Epoch 13/80: current_loss=9.00644 | best_loss=8.96035
Epoch 14/80: current_loss=8.95717 | best_loss=8.95717
Epoch 15/80: current_loss=8.96126 | best_loss=8.95717
Epoch 16/80: current_loss=8.98373 | best_loss=8.95717
Epoch 17/80: current_loss=9.08143 | best_loss=8.95717
Epoch 18/80: current_loss=8.99317 | best_loss=8.95717
Epoch 19/80: current_loss=8.97063 | best_loss=8.95717
Epoch 20/80: current_loss=9.05729 | best_loss=8.95717
Epoch 21/80: current_loss=8.95286 | best_loss=8.95286
Epoch 22/80: current_loss=8.98931 | best_loss=8.95286
Epoch 23/80: current_loss=9.07757 | best_loss=8.95286
Epoch 24/80: current_loss=9.00727 | best_loss=8.95286
Epoch 25/80: current_loss=8.96174 | best_loss=8.95286
Epoch 26/80: current_loss=9.02794 | best_loss=8.95286
Epoch 27/80: current_loss=9.04762 | best_loss=8.95286
Epoch 28/80: current_loss=9.17234 | best_loss=8.95286
Epoch 29/80: current_loss=9.01439 | best_loss=8.95286
Epoch 30/80: current_loss=9.05017 | best_loss=8.95286
Epoch 31/80: current_loss=9.00230 | best_loss=8.95286
Epoch 32/80: current_loss=9.06557 | best_loss=8.95286
Epoch 33/80: current_loss=8.98575 | best_loss=8.95286
Epoch 34/80: current_loss=8.95402 | best_loss=8.95286
Epoch 35/80: current_loss=9.01349 | best_loss=8.95286
Epoch 36/80: current_loss=8.96985 | best_loss=8.95286
Epoch 37/80: current_loss=9.00567 | best_loss=8.95286
Epoch 38/80: current_loss=9.02286 | best_loss=8.95286
Epoch 39/80: current_loss=8.97648 | best_loss=8.95286
Epoch 40/80: current_loss=8.99884 | best_loss=8.95286
Epoch 41/80: current_loss=8.98146 | best_loss=8.95286
Early Stopping at epoch 41
      explained_var=0.00052 | mse_loss=8.36209
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.51017 | best_loss=8.51017
Epoch 1/80: current_loss=8.50294 | best_loss=8.50294
Epoch 2/80: current_loss=8.50341 | best_loss=8.50294
Epoch 3/80: current_loss=8.50873 | best_loss=8.50294
Epoch 4/80: current_loss=8.57737 | best_loss=8.50294
Epoch 5/80: current_loss=8.51765 | best_loss=8.50294
Epoch 6/80: current_loss=8.49919 | best_loss=8.49919
Epoch 7/80: current_loss=8.50301 | best_loss=8.49919
Epoch 8/80: current_loss=8.49613 | best_loss=8.49613
Epoch 9/80: current_loss=8.49477 | best_loss=8.49477
Epoch 10/80: current_loss=8.49778 | best_loss=8.49477
Epoch 11/80: current_loss=8.49430 | best_loss=8.49430
Epoch 12/80: current_loss=8.54478 | best_loss=8.49430
Epoch 13/80: current_loss=8.49515 | best_loss=8.49430
Epoch 14/80: current_loss=8.49712 | best_loss=8.49430
Epoch 15/80: current_loss=8.49553 | best_loss=8.49430
Epoch 16/80: current_loss=8.51681 | best_loss=8.49430
Epoch 17/80: current_loss=8.50138 | best_loss=8.49430
Epoch 18/80: current_loss=8.50891 | best_loss=8.49430
Epoch 19/80: current_loss=8.49367 | best_loss=8.49367
Epoch 20/80: current_loss=8.49353 | best_loss=8.49353
Epoch 21/80: current_loss=8.59870 | best_loss=8.49353
Epoch 22/80: current_loss=8.52002 | best_loss=8.49353
Epoch 23/80: current_loss=8.51456 | best_loss=8.49353
Epoch 24/80: current_loss=8.50764 | best_loss=8.49353
Epoch 25/80: current_loss=8.54681 | best_loss=8.49353
Epoch 26/80: current_loss=8.50645 | best_loss=8.49353
Epoch 27/80: current_loss=8.65423 | best_loss=8.49353
Epoch 28/80: current_loss=8.50334 | best_loss=8.49353
Epoch 29/80: current_loss=8.57641 | best_loss=8.49353
Epoch 30/80: current_loss=8.55916 | best_loss=8.49353
Epoch 31/80: current_loss=8.50543 | best_loss=8.49353
Epoch 32/80: current_loss=8.50834 | best_loss=8.49353
Epoch 33/80: current_loss=8.49748 | best_loss=8.49353
Epoch 34/80: current_loss=8.50991 | best_loss=8.49353
Epoch 35/80: current_loss=8.61221 | best_loss=8.49353
Epoch 36/80: current_loss=8.49157 | best_loss=8.49157
Epoch 37/80: current_loss=8.53683 | best_loss=8.49157
Epoch 38/80: current_loss=8.50154 | best_loss=8.49157
Epoch 39/80: current_loss=8.52259 | best_loss=8.49157
Epoch 40/80: current_loss=8.50180 | best_loss=8.49157
Epoch 41/80: current_loss=8.56151 | best_loss=8.49157
Epoch 42/80: current_loss=8.51325 | best_loss=8.49157
Epoch 43/80: current_loss=8.49669 | best_loss=8.49157
Epoch 44/80: current_loss=8.57856 | best_loss=8.49157
Epoch 45/80: current_loss=8.50824 | best_loss=8.49157
Epoch 46/80: current_loss=8.50924 | best_loss=8.49157
Epoch 47/80: current_loss=8.49117 | best_loss=8.49117
Epoch 48/80: current_loss=8.55865 | best_loss=8.49117
Epoch 49/80: current_loss=8.49290 | best_loss=8.49117
Epoch 50/80: current_loss=8.56966 | best_loss=8.49117
Epoch 51/80: current_loss=8.53667 | best_loss=8.49117
Epoch 52/80: current_loss=8.50191 | best_loss=8.49117
Epoch 53/80: current_loss=8.52790 | best_loss=8.49117
Epoch 54/80: current_loss=8.49571 | best_loss=8.49117
Epoch 55/80: current_loss=8.49705 | best_loss=8.49117
Epoch 56/80: current_loss=8.49654 | best_loss=8.49117
Epoch 57/80: current_loss=8.50273 | best_loss=8.49117
Epoch 58/80: current_loss=8.50101 | best_loss=8.49117
Epoch 59/80: current_loss=8.50709 | best_loss=8.49117
Epoch 60/80: current_loss=8.49701 | best_loss=8.49117
Epoch 61/80: current_loss=8.56745 | best_loss=8.49117
Epoch 62/80: current_loss=8.49834 | best_loss=8.49117
Epoch 63/80: current_loss=8.50001 | best_loss=8.49117
Epoch 64/80: current_loss=8.50533 | best_loss=8.49117
Epoch 65/80: current_loss=8.49752 | best_loss=8.49117
Epoch 66/80: current_loss=8.51122 | best_loss=8.49117
Epoch 67/80: current_loss=8.54527 | best_loss=8.49117
Early Stopping at epoch 67
      explained_var=0.00027 | mse_loss=8.19913
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00082| avg_loss=8.81438
----------------------------------------------


----------------------------------------------
Params for Trial 13
{'learning_rate': 0.01, 'weight_decay': 0.00010813129554321693, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91100 | best_loss=9.91100
Epoch 1/80: current_loss=10.51551 | best_loss=9.91100
Epoch 2/80: current_loss=10.15231 | best_loss=9.91100
Epoch 3/80: current_loss=10.23981 | best_loss=9.91100
Epoch 4/80: current_loss=9.97453 | best_loss=9.91100
Epoch 5/80: current_loss=10.29204 | best_loss=9.91100
Epoch 6/80: current_loss=10.02277 | best_loss=9.91100
Epoch 7/80: current_loss=10.23429 | best_loss=9.91100
Epoch 8/80: current_loss=10.01758 | best_loss=9.91100
Epoch 9/80: current_loss=10.40314 | best_loss=9.91100
Epoch 10/80: current_loss=9.97989 | best_loss=9.91100
Epoch 11/80: current_loss=10.04743 | best_loss=9.91100
Epoch 12/80: current_loss=10.00626 | best_loss=9.91100
Epoch 13/80: current_loss=10.03904 | best_loss=9.91100
Epoch 14/80: current_loss=10.00911 | best_loss=9.91100
Epoch 15/80: current_loss=10.16553 | best_loss=9.91100
Epoch 16/80: current_loss=10.83575 | best_loss=9.91100
Epoch 17/80: current_loss=9.90521 | best_loss=9.90521
Epoch 18/80: current_loss=9.89992 | best_loss=9.89992
Epoch 19/80: current_loss=9.89965 | best_loss=9.89965
Epoch 20/80: current_loss=10.49648 | best_loss=9.89965
Epoch 21/80: current_loss=9.98424 | best_loss=9.89965
Epoch 22/80: current_loss=10.04506 | best_loss=9.89965
Epoch 23/80: current_loss=9.97763 | best_loss=9.89965
Epoch 24/80: current_loss=10.25310 | best_loss=9.89965
Epoch 25/80: current_loss=9.98846 | best_loss=9.89965
Epoch 26/80: current_loss=9.95475 | best_loss=9.89965
Epoch 27/80: current_loss=9.92202 | best_loss=9.89965
Epoch 28/80: current_loss=9.94766 | best_loss=9.89965
Epoch 29/80: current_loss=10.25205 | best_loss=9.89965
Epoch 30/80: current_loss=9.92915 | best_loss=9.89965
Epoch 31/80: current_loss=9.94640 | best_loss=9.89965
Epoch 32/80: current_loss=10.58758 | best_loss=9.89965
Epoch 33/80: current_loss=10.08830 | best_loss=9.89965
Epoch 34/80: current_loss=10.41618 | best_loss=9.89965
Epoch 35/80: current_loss=10.07323 | best_loss=9.89965
Epoch 36/80: current_loss=10.15762 | best_loss=9.89965
Epoch 37/80: current_loss=9.89813 | best_loss=9.89813
Epoch 38/80: current_loss=10.96689 | best_loss=9.89813
Epoch 39/80: current_loss=9.96676 | best_loss=9.89813
Epoch 40/80: current_loss=10.34193 | best_loss=9.89813
Epoch 41/80: current_loss=11.47567 | best_loss=9.89813
Epoch 42/80: current_loss=9.91536 | best_loss=9.89813
Epoch 43/80: current_loss=10.10631 | best_loss=9.89813
Epoch 44/80: current_loss=10.04012 | best_loss=9.89813
Epoch 45/80: current_loss=9.90814 | best_loss=9.89813
Epoch 46/80: current_loss=9.98596 | best_loss=9.89813
Epoch 47/80: current_loss=9.91247 | best_loss=9.89813
Epoch 48/80: current_loss=10.35868 | best_loss=9.89813
Epoch 49/80: current_loss=9.94529 | best_loss=9.89813
Epoch 50/80: current_loss=9.91316 | best_loss=9.89813
Epoch 51/80: current_loss=9.90751 | best_loss=9.89813
Epoch 52/80: current_loss=9.97499 | best_loss=9.89813
Epoch 53/80: current_loss=9.93710 | best_loss=9.89813
Epoch 54/80: current_loss=9.94619 | best_loss=9.89813
Epoch 55/80: current_loss=10.12554 | best_loss=9.89813
Epoch 56/80: current_loss=10.22151 | best_loss=9.89813
Epoch 57/80: current_loss=9.93598 | best_loss=9.89813
Early Stopping at epoch 57
      explained_var=0.00129 | mse_loss=9.57216
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.55104 | best_loss=8.55104
Epoch 1/80: current_loss=8.52238 | best_loss=8.52238
Epoch 2/80: current_loss=9.10238 | best_loss=8.52238
Epoch 3/80: current_loss=8.48404 | best_loss=8.48404
Epoch 4/80: current_loss=8.49839 | best_loss=8.48404
Epoch 5/80: current_loss=8.58710 | best_loss=8.48404
Epoch 6/80: current_loss=8.68386 | best_loss=8.48404
Epoch 7/80: current_loss=8.72949 | best_loss=8.48404
Epoch 8/80: current_loss=8.59959 | best_loss=8.48404
Epoch 9/80: current_loss=8.93477 | best_loss=8.48404
Epoch 10/80: current_loss=8.51281 | best_loss=8.48404
Epoch 11/80: current_loss=8.52183 | best_loss=8.48404
Epoch 12/80: current_loss=8.75891 | best_loss=8.48404
Epoch 13/80: current_loss=9.42077 | best_loss=8.48404
Epoch 14/80: current_loss=8.57335 | best_loss=8.48404
Epoch 15/80: current_loss=8.51950 | best_loss=8.48404
Epoch 16/80: current_loss=8.48936 | best_loss=8.48404
Epoch 17/80: current_loss=8.48556 | best_loss=8.48404
Epoch 18/80: current_loss=8.48074 | best_loss=8.48074
Epoch 19/80: current_loss=8.51081 | best_loss=8.48074
Epoch 20/80: current_loss=8.82162 | best_loss=8.48074
Epoch 21/80: current_loss=8.84761 | best_loss=8.48074
Epoch 22/80: current_loss=8.61333 | best_loss=8.48074
Epoch 23/80: current_loss=8.54255 | best_loss=8.48074
Epoch 24/80: current_loss=8.54485 | best_loss=8.48074
Epoch 25/80: current_loss=8.51814 | best_loss=8.48074
Epoch 26/80: current_loss=8.68144 | best_loss=8.48074
Epoch 27/80: current_loss=8.54026 | best_loss=8.48074
Epoch 28/80: current_loss=8.48522 | best_loss=8.48074
Epoch 29/80: current_loss=8.46995 | best_loss=8.46995
Epoch 30/80: current_loss=9.50539 | best_loss=8.46995
Epoch 31/80: current_loss=8.48211 | best_loss=8.46995
Epoch 32/80: current_loss=8.68961 | best_loss=8.46995
Epoch 33/80: current_loss=8.68454 | best_loss=8.46995
Epoch 34/80: current_loss=9.01044 | best_loss=8.46995
Epoch 35/80: current_loss=9.36209 | best_loss=8.46995
Epoch 36/80: current_loss=8.51744 | best_loss=8.46995
Epoch 37/80: current_loss=8.60015 | best_loss=8.46995
Epoch 38/80: current_loss=8.60538 | best_loss=8.46995
Epoch 39/80: current_loss=8.54079 | best_loss=8.46995
Epoch 40/80: current_loss=9.12174 | best_loss=8.46995
Epoch 41/80: current_loss=8.66042 | best_loss=8.46995
Epoch 42/80: current_loss=8.62290 | best_loss=8.46995
Epoch 43/80: current_loss=8.62539 | best_loss=8.46995
Epoch 44/80: current_loss=8.56127 | best_loss=8.46995
Epoch 45/80: current_loss=8.65022 | best_loss=8.46995
Epoch 46/80: current_loss=9.01893 | best_loss=8.46995
Epoch 47/80: current_loss=8.52596 | best_loss=8.46995
Epoch 48/80: current_loss=8.51037 | best_loss=8.46995
Epoch 49/80: current_loss=8.97721 | best_loss=8.46995
Early Stopping at epoch 49
      explained_var=0.00261 | mse_loss=8.55963
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.41426 | best_loss=9.41426
Epoch 1/80: current_loss=9.32179 | best_loss=9.32179
Epoch 2/80: current_loss=9.54204 | best_loss=9.32179
Epoch 3/80: current_loss=9.32257 | best_loss=9.32179
Epoch 4/80: current_loss=9.38125 | best_loss=9.32179
Epoch 5/80: current_loss=9.91170 | best_loss=9.32179
Epoch 6/80: current_loss=9.30532 | best_loss=9.30532
Epoch 7/80: current_loss=9.37045 | best_loss=9.30532
Epoch 8/80: current_loss=10.70601 | best_loss=9.30532
Epoch 9/80: current_loss=9.92740 | best_loss=9.30532
Epoch 10/80: current_loss=9.67267 | best_loss=9.30532
Epoch 11/80: current_loss=9.28495 | best_loss=9.28495
Epoch 12/80: current_loss=9.41055 | best_loss=9.28495
Epoch 13/80: current_loss=9.42728 | best_loss=9.28495
Epoch 14/80: current_loss=9.22839 | best_loss=9.22839
Epoch 15/80: current_loss=9.33773 | best_loss=9.22839
Epoch 16/80: current_loss=10.69846 | best_loss=9.22839
Epoch 17/80: current_loss=9.27788 | best_loss=9.22839
Epoch 18/80: current_loss=10.31406 | best_loss=9.22839
Epoch 19/80: current_loss=9.79129 | best_loss=9.22839
Epoch 20/80: current_loss=9.80377 | best_loss=9.22839
Epoch 21/80: current_loss=9.35380 | best_loss=9.22839
Epoch 22/80: current_loss=9.46672 | best_loss=9.22839
Epoch 23/80: current_loss=9.61322 | best_loss=9.22839
Epoch 24/80: current_loss=9.99980 | best_loss=9.22839
Epoch 25/80: current_loss=9.29153 | best_loss=9.22839
Epoch 26/80: current_loss=9.29937 | best_loss=9.22839
Epoch 27/80: current_loss=9.75676 | best_loss=9.22839
Epoch 28/80: current_loss=10.58720 | best_loss=9.22839
Epoch 29/80: current_loss=9.59315 | best_loss=9.22839
Epoch 30/80: current_loss=9.74418 | best_loss=9.22839
Epoch 31/80: current_loss=10.06782 | best_loss=9.22839
Epoch 32/80: current_loss=9.34640 | best_loss=9.22839
Epoch 33/80: current_loss=12.71733 | best_loss=9.22839
Epoch 34/80: current_loss=10.00112 | best_loss=9.22839
Early Stopping at epoch 34
      explained_var=0.00606 | mse_loss=9.32620
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.28584 | best_loss=9.28584
Epoch 1/80: current_loss=9.15679 | best_loss=9.15679
Epoch 2/80: current_loss=9.99665 | best_loss=9.15679
Epoch 3/80: current_loss=8.99761 | best_loss=8.99761
Epoch 4/80: current_loss=8.93120 | best_loss=8.93120
Epoch 5/80: current_loss=9.29576 | best_loss=8.93120
Epoch 6/80: current_loss=9.27744 | best_loss=8.93120
Epoch 7/80: current_loss=9.06337 | best_loss=8.93120
Epoch 8/80: current_loss=9.18604 | best_loss=8.93120
Epoch 9/80: current_loss=9.63181 | best_loss=8.93120
Epoch 10/80: current_loss=8.90926 | best_loss=8.90926
Epoch 11/80: current_loss=9.06825 | best_loss=8.90926
Epoch 12/80: current_loss=9.32226 | best_loss=8.90926
Epoch 13/80: current_loss=9.24478 | best_loss=8.90926
Epoch 14/80: current_loss=8.97180 | best_loss=8.90926
Epoch 15/80: current_loss=8.93929 | best_loss=8.90926
Epoch 16/80: current_loss=9.03389 | best_loss=8.90926
Epoch 17/80: current_loss=8.94487 | best_loss=8.90926
Epoch 18/80: current_loss=9.13076 | best_loss=8.90926
Epoch 19/80: current_loss=9.84784 | best_loss=8.90926
Epoch 20/80: current_loss=9.28955 | best_loss=8.90926
Epoch 21/80: current_loss=9.03692 | best_loss=8.90926
Epoch 22/80: current_loss=9.97567 | best_loss=8.90926
Epoch 23/80: current_loss=9.62332 | best_loss=8.90926
Epoch 24/80: current_loss=8.96528 | best_loss=8.90926
Epoch 25/80: current_loss=9.53614 | best_loss=8.90926
Epoch 26/80: current_loss=9.14441 | best_loss=8.90926
Epoch 27/80: current_loss=8.99888 | best_loss=8.90926
Epoch 28/80: current_loss=9.22896 | best_loss=8.90926
Epoch 29/80: current_loss=9.02591 | best_loss=8.90926
Epoch 30/80: current_loss=9.03548 | best_loss=8.90926
Early Stopping at epoch 30
      explained_var=0.00194 | mse_loss=8.34629
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.54546 | best_loss=8.54546
Epoch 1/80: current_loss=8.52541 | best_loss=8.52541
Epoch 2/80: current_loss=8.97580 | best_loss=8.52541
Epoch 3/80: current_loss=8.69049 | best_loss=8.52541
Epoch 4/80: current_loss=8.82083 | best_loss=8.52541
Epoch 5/80: current_loss=8.66948 | best_loss=8.52541
Epoch 6/80: current_loss=8.79972 | best_loss=8.52541
Epoch 7/80: current_loss=8.64300 | best_loss=8.52541
Epoch 8/80: current_loss=9.16679 | best_loss=8.52541
Epoch 9/80: current_loss=8.58634 | best_loss=8.52541
Epoch 10/80: current_loss=8.61248 | best_loss=8.52541
Epoch 11/80: current_loss=8.80181 | best_loss=8.52541
Epoch 12/80: current_loss=9.04902 | best_loss=8.52541
Epoch 13/80: current_loss=8.68406 | best_loss=8.52541
Epoch 14/80: current_loss=8.94055 | best_loss=8.52541
Epoch 15/80: current_loss=8.52909 | best_loss=8.52541
Epoch 16/80: current_loss=8.53313 | best_loss=8.52541
Epoch 17/80: current_loss=8.66320 | best_loss=8.52541
Epoch 18/80: current_loss=8.63145 | best_loss=8.52541
Epoch 19/80: current_loss=8.52857 | best_loss=8.52541
Epoch 20/80: current_loss=8.56178 | best_loss=8.52541
Epoch 21/80: current_loss=9.40146 | best_loss=8.52541
Early Stopping at epoch 21
      explained_var=0.00020 | mse_loss=8.22419
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00242| avg_loss=8.80569
----------------------------------------------


----------------------------------------------
Params for Trial 14
{'learning_rate': 0.01, 'weight_decay': 0.00012708348122822342, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.90885 | best_loss=9.90885
Epoch 1/80: current_loss=10.38728 | best_loss=9.90885
Epoch 2/80: current_loss=9.93939 | best_loss=9.90885
Epoch 3/80: current_loss=9.90690 | best_loss=9.90690
Epoch 4/80: current_loss=10.69573 | best_loss=9.90690
Epoch 5/80: current_loss=9.97062 | best_loss=9.90690
Epoch 6/80: current_loss=10.51916 | best_loss=9.90690
Epoch 7/80: current_loss=9.99346 | best_loss=9.90690
Epoch 8/80: current_loss=10.19342 | best_loss=9.90690
Epoch 9/80: current_loss=9.98604 | best_loss=9.90690
Epoch 10/80: current_loss=9.93263 | best_loss=9.90690
Epoch 11/80: current_loss=10.54377 | best_loss=9.90690
Epoch 12/80: current_loss=9.90840 | best_loss=9.90690
Epoch 13/80: current_loss=9.92886 | best_loss=9.90690
Epoch 14/80: current_loss=9.92192 | best_loss=9.90690
Epoch 15/80: current_loss=9.95065 | best_loss=9.90690
Epoch 16/80: current_loss=11.25194 | best_loss=9.90690
Epoch 17/80: current_loss=9.91059 | best_loss=9.90690
Epoch 18/80: current_loss=9.97103 | best_loss=9.90690
Epoch 19/80: current_loss=9.90756 | best_loss=9.90690
Epoch 20/80: current_loss=10.07881 | best_loss=9.90690
Epoch 21/80: current_loss=9.92243 | best_loss=9.90690
Epoch 22/80: current_loss=9.93198 | best_loss=9.90690
Epoch 23/80: current_loss=10.68006 | best_loss=9.90690
Early Stopping at epoch 23
      explained_var=0.00009 | mse_loss=9.57670
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.57907 | best_loss=8.57907
Epoch 1/80: current_loss=8.51261 | best_loss=8.51261
Epoch 2/80: current_loss=8.54114 | best_loss=8.51261
Epoch 3/80: current_loss=8.56814 | best_loss=8.51261
Epoch 4/80: current_loss=8.51412 | best_loss=8.51261
Epoch 5/80: current_loss=8.52135 | best_loss=8.51261
Epoch 6/80: current_loss=8.50484 | best_loss=8.50484
Epoch 7/80: current_loss=8.63303 | best_loss=8.50484
Epoch 8/80: current_loss=8.59262 | best_loss=8.50484
Epoch 9/80: current_loss=8.52791 | best_loss=8.50484
Epoch 10/80: current_loss=8.62753 | best_loss=8.50484
Epoch 11/80: current_loss=8.79927 | best_loss=8.50484
Epoch 12/80: current_loss=8.56956 | best_loss=8.50484
Epoch 13/80: current_loss=8.72381 | best_loss=8.50484
Epoch 14/80: current_loss=8.51111 | best_loss=8.50484
Epoch 15/80: current_loss=8.73675 | best_loss=8.50484
Epoch 16/80: current_loss=8.62266 | best_loss=8.50484
Epoch 17/80: current_loss=8.83803 | best_loss=8.50484
Epoch 18/80: current_loss=8.98360 | best_loss=8.50484
Epoch 19/80: current_loss=8.63522 | best_loss=8.50484
Epoch 20/80: current_loss=8.97359 | best_loss=8.50484
Epoch 21/80: current_loss=8.62797 | best_loss=8.50484
Epoch 22/80: current_loss=8.65359 | best_loss=8.50484
Epoch 23/80: current_loss=8.52315 | best_loss=8.50484
Epoch 24/80: current_loss=8.52843 | best_loss=8.50484
Epoch 25/80: current_loss=8.73864 | best_loss=8.50484
Epoch 26/80: current_loss=8.68172 | best_loss=8.50484
Early Stopping at epoch 26
      explained_var=0.00069 | mse_loss=8.57085
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.35258 | best_loss=9.35258
Epoch 1/80: current_loss=9.45659 | best_loss=9.35258
Epoch 2/80: current_loss=9.53968 | best_loss=9.35258
Epoch 3/80: current_loss=9.30356 | best_loss=9.30356
Epoch 4/80: current_loss=9.44782 | best_loss=9.30356
Epoch 5/80: current_loss=10.92175 | best_loss=9.30356
Epoch 6/80: current_loss=10.69683 | best_loss=9.30356
Epoch 7/80: current_loss=9.35855 | best_loss=9.30356
Epoch 8/80: current_loss=9.87491 | best_loss=9.30356
Epoch 9/80: current_loss=9.79013 | best_loss=9.30356
Epoch 10/80: current_loss=9.77129 | best_loss=9.30356
Epoch 11/80: current_loss=11.74776 | best_loss=9.30356
Epoch 12/80: current_loss=9.33223 | best_loss=9.30356
Epoch 13/80: current_loss=10.20489 | best_loss=9.30356
Epoch 14/80: current_loss=9.34277 | best_loss=9.30356
Epoch 15/80: current_loss=10.16823 | best_loss=9.30356
Epoch 16/80: current_loss=9.28059 | best_loss=9.28059
Epoch 17/80: current_loss=9.34377 | best_loss=9.28059
Epoch 18/80: current_loss=9.47764 | best_loss=9.28059
Epoch 19/80: current_loss=10.01576 | best_loss=9.28059
Epoch 20/80: current_loss=9.51241 | best_loss=9.28059
Epoch 21/80: current_loss=9.33829 | best_loss=9.28059
Epoch 22/80: current_loss=9.55254 | best_loss=9.28059
Epoch 23/80: current_loss=9.31961 | best_loss=9.28059
Epoch 24/80: current_loss=9.44684 | best_loss=9.28059
Epoch 25/80: current_loss=10.27727 | best_loss=9.28059
Epoch 26/80: current_loss=9.57665 | best_loss=9.28059
Epoch 27/80: current_loss=9.34577 | best_loss=9.28059
Epoch 28/80: current_loss=10.34620 | best_loss=9.28059
Epoch 29/80: current_loss=9.30517 | best_loss=9.28059
Epoch 30/80: current_loss=9.38834 | best_loss=9.28059
Epoch 31/80: current_loss=9.52149 | best_loss=9.28059
Epoch 32/80: current_loss=9.33657 | best_loss=9.28059
Epoch 33/80: current_loss=10.30074 | best_loss=9.28059
Epoch 34/80: current_loss=9.35101 | best_loss=9.28059
Epoch 35/80: current_loss=9.32802 | best_loss=9.28059
Epoch 36/80: current_loss=9.37474 | best_loss=9.28059
Early Stopping at epoch 36
      explained_var=-0.00424 | mse_loss=9.42265
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.96399 | best_loss=8.96399
Epoch 1/80: current_loss=9.17263 | best_loss=8.96399
Epoch 2/80: current_loss=9.04734 | best_loss=8.96399
Epoch 3/80: current_loss=8.97222 | best_loss=8.96399
Epoch 4/80: current_loss=8.96788 | best_loss=8.96399
Epoch 5/80: current_loss=9.59079 | best_loss=8.96399
Epoch 6/80: current_loss=8.93344 | best_loss=8.93344
Epoch 7/80: current_loss=9.79857 | best_loss=8.93344
Epoch 8/80: current_loss=9.18329 | best_loss=8.93344
Epoch 9/80: current_loss=9.17966 | best_loss=8.93344
Epoch 10/80: current_loss=9.81461 | best_loss=8.93344
Epoch 11/80: current_loss=9.45241 | best_loss=8.93344
Epoch 12/80: current_loss=9.02652 | best_loss=8.93344
Epoch 13/80: current_loss=9.59343 | best_loss=8.93344
Epoch 14/80: current_loss=9.32669 | best_loss=8.93344
Epoch 15/80: current_loss=9.07505 | best_loss=8.93344
Epoch 16/80: current_loss=9.19055 | best_loss=8.93344
Epoch 17/80: current_loss=8.93099 | best_loss=8.93099
Epoch 18/80: current_loss=12.36270 | best_loss=8.93099
Epoch 19/80: current_loss=9.16785 | best_loss=8.93099
Epoch 20/80: current_loss=8.94029 | best_loss=8.93099
Epoch 21/80: current_loss=9.60168 | best_loss=8.93099
Epoch 22/80: current_loss=8.95286 | best_loss=8.93099
Epoch 23/80: current_loss=8.93545 | best_loss=8.93099
Epoch 24/80: current_loss=8.93706 | best_loss=8.93099
Epoch 25/80: current_loss=9.07176 | best_loss=8.93099
Epoch 26/80: current_loss=8.91209 | best_loss=8.91209
Epoch 27/80: current_loss=8.96532 | best_loss=8.91209
Epoch 28/80: current_loss=9.14758 | best_loss=8.91209
Epoch 29/80: current_loss=9.13620 | best_loss=8.91209
Epoch 30/80: current_loss=9.06500 | best_loss=8.91209
Epoch 31/80: current_loss=10.09849 | best_loss=8.91209
Epoch 32/80: current_loss=8.96773 | best_loss=8.91209
Epoch 33/80: current_loss=9.51965 | best_loss=8.91209
Epoch 34/80: current_loss=9.05891 | best_loss=8.91209
Epoch 35/80: current_loss=9.33355 | best_loss=8.91209
Epoch 36/80: current_loss=9.05254 | best_loss=8.91209
Epoch 37/80: current_loss=8.96771 | best_loss=8.91209
Epoch 38/80: current_loss=9.37062 | best_loss=8.91209
Epoch 39/80: current_loss=9.97688 | best_loss=8.91209
Epoch 40/80: current_loss=8.97238 | best_loss=8.91209
Epoch 41/80: current_loss=9.13092 | best_loss=8.91209
Epoch 42/80: current_loss=8.93130 | best_loss=8.91209
Epoch 43/80: current_loss=8.95076 | best_loss=8.91209
Epoch 44/80: current_loss=9.23172 | best_loss=8.91209
Epoch 45/80: current_loss=9.06875 | best_loss=8.91209
Epoch 46/80: current_loss=9.34934 | best_loss=8.91209
Early Stopping at epoch 46
      explained_var=0.00085 | mse_loss=8.35662
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.59672 | best_loss=8.59672
Epoch 1/80: current_loss=8.57467 | best_loss=8.57467
Epoch 2/80: current_loss=8.55036 | best_loss=8.55036
Epoch 3/80: current_loss=8.80983 | best_loss=8.55036
Epoch 4/80: current_loss=8.50648 | best_loss=8.50648
Epoch 5/80: current_loss=8.49626 | best_loss=8.49626
Epoch 6/80: current_loss=8.49411 | best_loss=8.49411
Epoch 7/80: current_loss=8.74295 | best_loss=8.49411
Epoch 8/80: current_loss=8.77196 | best_loss=8.49411
Epoch 9/80: current_loss=8.48468 | best_loss=8.48468
Epoch 10/80: current_loss=8.82299 | best_loss=8.48468
Epoch 11/80: current_loss=8.49828 | best_loss=8.48468
Epoch 12/80: current_loss=9.10675 | best_loss=8.48468
Epoch 13/80: current_loss=9.10944 | best_loss=8.48468
Epoch 14/80: current_loss=8.55946 | best_loss=8.48468
Epoch 15/80: current_loss=8.77058 | best_loss=8.48468
Epoch 16/80: current_loss=8.54298 | best_loss=8.48468
Epoch 17/80: current_loss=8.71615 | best_loss=8.48468
Epoch 18/80: current_loss=8.58142 | best_loss=8.48468
Epoch 19/80: current_loss=8.80973 | best_loss=8.48468
Epoch 20/80: current_loss=8.97812 | best_loss=8.48468
Epoch 21/80: current_loss=8.73779 | best_loss=8.48468
Epoch 22/80: current_loss=8.72466 | best_loss=8.48468
Epoch 23/80: current_loss=8.66834 | best_loss=8.48468
Epoch 24/80: current_loss=8.65374 | best_loss=8.48468
Epoch 25/80: current_loss=8.71860 | best_loss=8.48468
Epoch 26/80: current_loss=8.68825 | best_loss=8.48468
Epoch 27/80: current_loss=9.20843 | best_loss=8.48468
Epoch 28/80: current_loss=8.85632 | best_loss=8.48468
Epoch 29/80: current_loss=8.52951 | best_loss=8.48468
Early Stopping at epoch 29
      explained_var=0.00045 | mse_loss=8.19679
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=-0.00043| avg_loss=8.82472
----------------------------------------------


----------------------------------------------
Params for Trial 15
{'learning_rate': 0.01, 'weight_decay': 0.0025771573441501217, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.28576 | best_loss=10.28576
Epoch 1/80: current_loss=10.05494 | best_loss=10.05494
Epoch 2/80: current_loss=10.54085 | best_loss=10.05494
Epoch 3/80: current_loss=10.59565 | best_loss=10.05494
Epoch 4/80: current_loss=10.10410 | best_loss=10.05494
Epoch 5/80: current_loss=9.91187 | best_loss=9.91187
Epoch 6/80: current_loss=9.90945 | best_loss=9.90945
Epoch 7/80: current_loss=9.96168 | best_loss=9.90945
Epoch 8/80: current_loss=10.10727 | best_loss=9.90945
Epoch 9/80: current_loss=11.07198 | best_loss=9.90945
Epoch 10/80: current_loss=9.90974 | best_loss=9.90945
Epoch 11/80: current_loss=10.28948 | best_loss=9.90945
Epoch 12/80: current_loss=10.23047 | best_loss=9.90945
Epoch 13/80: current_loss=9.91398 | best_loss=9.90945
Epoch 14/80: current_loss=9.95493 | best_loss=9.90945
Epoch 15/80: current_loss=9.95141 | best_loss=9.90945
Epoch 16/80: current_loss=9.89641 | best_loss=9.89641
Epoch 17/80: current_loss=10.28295 | best_loss=9.89641
Epoch 18/80: current_loss=10.10193 | best_loss=9.89641
Epoch 19/80: current_loss=10.07991 | best_loss=9.89641
Epoch 20/80: current_loss=10.01262 | best_loss=9.89641
Epoch 21/80: current_loss=9.94572 | best_loss=9.89641
Epoch 22/80: current_loss=10.57396 | best_loss=9.89641
Epoch 23/80: current_loss=10.12137 | best_loss=9.89641
Epoch 24/80: current_loss=10.14266 | best_loss=9.89641
Epoch 25/80: current_loss=10.46547 | best_loss=9.89641
Epoch 26/80: current_loss=9.90269 | best_loss=9.89641
Epoch 27/80: current_loss=10.07152 | best_loss=9.89641
Epoch 28/80: current_loss=10.02343 | best_loss=9.89641
Epoch 29/80: current_loss=10.44337 | best_loss=9.89641
Epoch 30/80: current_loss=10.28274 | best_loss=9.89641
Epoch 31/80: current_loss=10.62147 | best_loss=9.89641
Epoch 32/80: current_loss=9.97406 | best_loss=9.89641
Epoch 33/80: current_loss=10.37181 | best_loss=9.89641
Epoch 34/80: current_loss=10.40175 | best_loss=9.89641
Epoch 35/80: current_loss=10.67889 | best_loss=9.89641
Epoch 36/80: current_loss=9.92003 | best_loss=9.89641
Early Stopping at epoch 36
      explained_var=0.00291 | mse_loss=9.56901
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.77879 | best_loss=8.77879
Epoch 1/80: current_loss=8.73702 | best_loss=8.73702
Epoch 2/80: current_loss=8.93079 | best_loss=8.73702
Epoch 3/80: current_loss=8.52916 | best_loss=8.52916
Epoch 4/80: current_loss=8.52378 | best_loss=8.52378
Epoch 5/80: current_loss=8.54259 | best_loss=8.52378
Epoch 6/80: current_loss=8.58151 | best_loss=8.52378
Epoch 7/80: current_loss=9.25850 | best_loss=8.52378
Epoch 8/80: current_loss=8.96832 | best_loss=8.52378
Epoch 9/80: current_loss=8.93957 | best_loss=8.52378
Epoch 10/80: current_loss=9.12616 | best_loss=8.52378
Epoch 11/80: current_loss=9.02225 | best_loss=8.52378
Epoch 12/80: current_loss=8.71359 | best_loss=8.52378
Epoch 13/80: current_loss=8.53876 | best_loss=8.52378
Epoch 14/80: current_loss=8.66001 | best_loss=8.52378
Epoch 15/80: current_loss=8.52882 | best_loss=8.52378
Epoch 16/80: current_loss=8.59700 | best_loss=8.52378
Epoch 17/80: current_loss=9.16895 | best_loss=8.52378
Epoch 18/80: current_loss=8.55731 | best_loss=8.52378
Epoch 19/80: current_loss=9.08843 | best_loss=8.52378
Epoch 20/80: current_loss=8.53289 | best_loss=8.52378
Epoch 21/80: current_loss=8.72294 | best_loss=8.52378
Epoch 22/80: current_loss=8.99555 | best_loss=8.52378
Epoch 23/80: current_loss=8.51140 | best_loss=8.51140
Epoch 24/80: current_loss=8.58551 | best_loss=8.51140
Epoch 25/80: current_loss=8.93971 | best_loss=8.51140
Epoch 26/80: current_loss=8.55030 | best_loss=8.51140
Epoch 27/80: current_loss=8.80756 | best_loss=8.51140
Epoch 28/80: current_loss=8.88376 | best_loss=8.51140
Epoch 29/80: current_loss=8.52220 | best_loss=8.51140
Epoch 30/80: current_loss=8.59037 | best_loss=8.51140
Epoch 31/80: current_loss=8.70959 | best_loss=8.51140
Epoch 32/80: current_loss=8.91340 | best_loss=8.51140
Epoch 33/80: current_loss=8.47917 | best_loss=8.47917
Epoch 34/80: current_loss=8.48250 | best_loss=8.47917
Epoch 35/80: current_loss=8.51664 | best_loss=8.47917
Epoch 36/80: current_loss=8.88959 | best_loss=8.47917
Epoch 37/80: current_loss=9.13450 | best_loss=8.47917
Epoch 38/80: current_loss=8.50798 | best_loss=8.47917
Epoch 39/80: current_loss=8.52577 | best_loss=8.47917
Epoch 40/80: current_loss=8.50881 | best_loss=8.47917
Epoch 41/80: current_loss=8.53100 | best_loss=8.47917
Epoch 42/80: current_loss=8.51558 | best_loss=8.47917
Epoch 43/80: current_loss=8.60643 | best_loss=8.47917
Epoch 44/80: current_loss=8.63507 | best_loss=8.47917
Epoch 45/80: current_loss=8.53292 | best_loss=8.47917
Epoch 46/80: current_loss=8.50685 | best_loss=8.47917
Epoch 47/80: current_loss=8.52729 | best_loss=8.47917
Epoch 48/80: current_loss=8.72695 | best_loss=8.47917
Epoch 49/80: current_loss=8.51063 | best_loss=8.47917
Epoch 50/80: current_loss=8.50355 | best_loss=8.47917
Epoch 51/80: current_loss=9.26095 | best_loss=8.47917
Epoch 52/80: current_loss=8.66400 | best_loss=8.47917
Epoch 53/80: current_loss=9.07601 | best_loss=8.47917
Early Stopping at epoch 53
      explained_var=0.00233 | mse_loss=8.56009
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.30944 | best_loss=9.30944
Epoch 1/80: current_loss=10.30239 | best_loss=9.30944
Epoch 2/80: current_loss=9.28839 | best_loss=9.28839
Epoch 3/80: current_loss=9.65114 | best_loss=9.28839
Epoch 4/80: current_loss=10.28939 | best_loss=9.28839
Epoch 5/80: current_loss=9.84478 | best_loss=9.28839
Epoch 6/80: current_loss=9.46940 | best_loss=9.28839
Epoch 7/80: current_loss=9.73871 | best_loss=9.28839
Epoch 8/80: current_loss=9.31227 | best_loss=9.28839
Epoch 9/80: current_loss=9.45817 | best_loss=9.28839
Epoch 10/80: current_loss=10.15792 | best_loss=9.28839
Epoch 11/80: current_loss=10.18043 | best_loss=9.28839
Epoch 12/80: current_loss=9.62763 | best_loss=9.28839
Epoch 13/80: current_loss=9.43130 | best_loss=9.28839
Epoch 14/80: current_loss=10.08288 | best_loss=9.28839
Epoch 15/80: current_loss=9.74191 | best_loss=9.28839
Epoch 16/80: current_loss=10.09273 | best_loss=9.28839
Epoch 17/80: current_loss=9.52994 | best_loss=9.28839
Epoch 18/80: current_loss=10.88761 | best_loss=9.28839
Epoch 19/80: current_loss=9.89012 | best_loss=9.28839
Epoch 20/80: current_loss=9.79451 | best_loss=9.28839
Epoch 21/80: current_loss=9.57545 | best_loss=9.28839
Epoch 22/80: current_loss=9.43709 | best_loss=9.28839
Early Stopping at epoch 22
      explained_var=-0.00390 | mse_loss=9.41941
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.11430 | best_loss=10.11430
Epoch 1/80: current_loss=9.07250 | best_loss=9.07250
Epoch 2/80: current_loss=9.04553 | best_loss=9.04553
Epoch 3/80: current_loss=9.26583 | best_loss=9.04553
Epoch 4/80: current_loss=9.12190 | best_loss=9.04553
Epoch 5/80: current_loss=9.24968 | best_loss=9.04553
Epoch 6/80: current_loss=9.04001 | best_loss=9.04001
Epoch 7/80: current_loss=9.44284 | best_loss=9.04001
Epoch 8/80: current_loss=9.20473 | best_loss=9.04001
Epoch 9/80: current_loss=9.09487 | best_loss=9.04001
Epoch 10/80: current_loss=8.95474 | best_loss=8.95474
Epoch 11/80: current_loss=9.37999 | best_loss=8.95474
Epoch 12/80: current_loss=9.41314 | best_loss=8.95474
Epoch 13/80: current_loss=9.61655 | best_loss=8.95474
Epoch 14/80: current_loss=8.98399 | best_loss=8.95474
Epoch 15/80: current_loss=8.97124 | best_loss=8.95474
Epoch 16/80: current_loss=9.00372 | best_loss=8.95474
Epoch 17/80: current_loss=9.26297 | best_loss=8.95474
Epoch 18/80: current_loss=9.33317 | best_loss=8.95474
Epoch 19/80: current_loss=9.06685 | best_loss=8.95474
Epoch 20/80: current_loss=9.36169 | best_loss=8.95474
Epoch 21/80: current_loss=9.43647 | best_loss=8.95474
Epoch 22/80: current_loss=9.34121 | best_loss=8.95474
Epoch 23/80: current_loss=9.40284 | best_loss=8.95474
Epoch 24/80: current_loss=9.56340 | best_loss=8.95474
Epoch 25/80: current_loss=8.90287 | best_loss=8.90287
Epoch 26/80: current_loss=9.22126 | best_loss=8.90287
Epoch 27/80: current_loss=8.95517 | best_loss=8.90287
Epoch 28/80: current_loss=12.19809 | best_loss=8.90287
Epoch 29/80: current_loss=9.00415 | best_loss=8.90287
Epoch 30/80: current_loss=10.21611 | best_loss=8.90287
Epoch 31/80: current_loss=9.08431 | best_loss=8.90287
Epoch 32/80: current_loss=9.06039 | best_loss=8.90287
Epoch 33/80: current_loss=9.06620 | best_loss=8.90287
Epoch 34/80: current_loss=9.54246 | best_loss=8.90287
Epoch 35/80: current_loss=8.95096 | best_loss=8.90287
Epoch 36/80: current_loss=8.91613 | best_loss=8.90287
Epoch 37/80: current_loss=9.10332 | best_loss=8.90287
Epoch 38/80: current_loss=9.27544 | best_loss=8.90287
Epoch 39/80: current_loss=8.95103 | best_loss=8.90287
Epoch 40/80: current_loss=8.98560 | best_loss=8.90287
Epoch 41/80: current_loss=8.93074 | best_loss=8.90287
Epoch 42/80: current_loss=10.01906 | best_loss=8.90287
Epoch 43/80: current_loss=9.70086 | best_loss=8.90287
Epoch 44/80: current_loss=10.43741 | best_loss=8.90287
Epoch 45/80: current_loss=9.14188 | best_loss=8.90287
Early Stopping at epoch 45
      explained_var=0.00325 | mse_loss=8.35448
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.99982 | best_loss=8.99982
Epoch 1/80: current_loss=8.56844 | best_loss=8.56844
Epoch 2/80: current_loss=8.82794 | best_loss=8.56844
Epoch 3/80: current_loss=8.64484 | best_loss=8.56844
Epoch 4/80: current_loss=8.54421 | best_loss=8.54421
Epoch 5/80: current_loss=8.51056 | best_loss=8.51056
Epoch 6/80: current_loss=8.57441 | best_loss=8.51056
Epoch 7/80: current_loss=8.57775 | best_loss=8.51056
Epoch 8/80: current_loss=8.92299 | best_loss=8.51056
Epoch 9/80: current_loss=8.51043 | best_loss=8.51043
Epoch 10/80: current_loss=8.52347 | best_loss=8.51043
Epoch 11/80: current_loss=8.51659 | best_loss=8.51043
Epoch 12/80: current_loss=8.54145 | best_loss=8.51043
Epoch 13/80: current_loss=8.68654 | best_loss=8.51043
Epoch 14/80: current_loss=8.47786 | best_loss=8.47786
Epoch 15/80: current_loss=8.51440 | best_loss=8.47786
Epoch 16/80: current_loss=9.12802 | best_loss=8.47786
Epoch 17/80: current_loss=8.64969 | best_loss=8.47786
Epoch 18/80: current_loss=8.51930 | best_loss=8.47786
Epoch 19/80: current_loss=8.76054 | best_loss=8.47786
Epoch 20/80: current_loss=8.78736 | best_loss=8.47786
Epoch 21/80: current_loss=9.49403 | best_loss=8.47786
Epoch 22/80: current_loss=8.88756 | best_loss=8.47786
Epoch 23/80: current_loss=8.51284 | best_loss=8.47786
Epoch 24/80: current_loss=8.51678 | best_loss=8.47786
Epoch 25/80: current_loss=8.82152 | best_loss=8.47786
Epoch 26/80: current_loss=8.47839 | best_loss=8.47786
Epoch 27/80: current_loss=8.80542 | best_loss=8.47786
Epoch 28/80: current_loss=8.71472 | best_loss=8.47786
Epoch 29/80: current_loss=8.68157 | best_loss=8.47786
Epoch 30/80: current_loss=8.50542 | best_loss=8.47786
Epoch 31/80: current_loss=8.96895 | best_loss=8.47786
Epoch 32/80: current_loss=8.61355 | best_loss=8.47786
Epoch 33/80: current_loss=8.84949 | best_loss=8.47786
Epoch 34/80: current_loss=8.83844 | best_loss=8.47786
Early Stopping at epoch 34
      explained_var=0.00130 | mse_loss=8.21186
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00118| avg_loss=8.82297
----------------------------------------------


----------------------------------------------
Params for Trial 16
{'learning_rate': 0.01, 'weight_decay': 0.0015549447682670203, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.15540 | best_loss=10.15540
Epoch 1/80: current_loss=10.06654 | best_loss=10.06654
Epoch 2/80: current_loss=10.01063 | best_loss=10.01063
Epoch 3/80: current_loss=9.92009 | best_loss=9.92009
Epoch 4/80: current_loss=9.91982 | best_loss=9.91982
Epoch 5/80: current_loss=10.19477 | best_loss=9.91982
Epoch 6/80: current_loss=10.22194 | best_loss=9.91982
Epoch 7/80: current_loss=9.99431 | best_loss=9.91982
Epoch 8/80: current_loss=9.92033 | best_loss=9.91982
Epoch 9/80: current_loss=10.70675 | best_loss=9.91982
Epoch 10/80: current_loss=10.06980 | best_loss=9.91982
Epoch 11/80: current_loss=11.06056 | best_loss=9.91982
Epoch 12/80: current_loss=9.90364 | best_loss=9.90364
Epoch 13/80: current_loss=9.90575 | best_loss=9.90364
Epoch 14/80: current_loss=10.44592 | best_loss=9.90364
Epoch 15/80: current_loss=10.19605 | best_loss=9.90364
Epoch 16/80: current_loss=10.07693 | best_loss=9.90364
Epoch 17/80: current_loss=9.95292 | best_loss=9.90364
Epoch 18/80: current_loss=9.99591 | best_loss=9.90364
Epoch 19/80: current_loss=9.91501 | best_loss=9.90364
Epoch 20/80: current_loss=9.92138 | best_loss=9.90364
Epoch 21/80: current_loss=9.91524 | best_loss=9.90364
Epoch 22/80: current_loss=10.69166 | best_loss=9.90364
Epoch 23/80: current_loss=10.03550 | best_loss=9.90364
Epoch 24/80: current_loss=9.90663 | best_loss=9.90364
Epoch 25/80: current_loss=9.92802 | best_loss=9.90364
Epoch 26/80: current_loss=10.28012 | best_loss=9.90364
Epoch 27/80: current_loss=9.90362 | best_loss=9.90362
Epoch 28/80: current_loss=10.14283 | best_loss=9.90362
Epoch 29/80: current_loss=9.97119 | best_loss=9.90362
Epoch 30/80: current_loss=9.91079 | best_loss=9.90362
Epoch 31/80: current_loss=11.27935 | best_loss=9.90362
Epoch 32/80: current_loss=10.57217 | best_loss=9.90362
Epoch 33/80: current_loss=11.48022 | best_loss=9.90362
Epoch 34/80: current_loss=10.20430 | best_loss=9.90362
Epoch 35/80: current_loss=9.90506 | best_loss=9.90362
Epoch 36/80: current_loss=10.03320 | best_loss=9.90362
Epoch 37/80: current_loss=10.92640 | best_loss=9.90362
Epoch 38/80: current_loss=10.01193 | best_loss=9.90362
Epoch 39/80: current_loss=10.07292 | best_loss=9.90362
Epoch 40/80: current_loss=10.44040 | best_loss=9.90362
Epoch 41/80: current_loss=9.90309 | best_loss=9.90309
Epoch 42/80: current_loss=10.03604 | best_loss=9.90309
Epoch 43/80: current_loss=10.15897 | best_loss=9.90309
Epoch 44/80: current_loss=10.21085 | best_loss=9.90309
Epoch 45/80: current_loss=11.05852 | best_loss=9.90309
Epoch 46/80: current_loss=10.11020 | best_loss=9.90309
Epoch 47/80: current_loss=9.89130 | best_loss=9.89130
Epoch 48/80: current_loss=9.90657 | best_loss=9.89130
Epoch 49/80: current_loss=9.92948 | best_loss=9.89130
Epoch 50/80: current_loss=10.28571 | best_loss=9.89130
Epoch 51/80: current_loss=10.40362 | best_loss=9.89130
Epoch 52/80: current_loss=10.07915 | best_loss=9.89130
Epoch 53/80: current_loss=10.30052 | best_loss=9.89130
Epoch 54/80: current_loss=10.17650 | best_loss=9.89130
Epoch 55/80: current_loss=9.95763 | best_loss=9.89130
Epoch 56/80: current_loss=9.89806 | best_loss=9.89130
Epoch 57/80: current_loss=9.91361 | best_loss=9.89130
Epoch 58/80: current_loss=9.94897 | best_loss=9.89130
Epoch 59/80: current_loss=10.48742 | best_loss=9.89130
Epoch 60/80: current_loss=9.92707 | best_loss=9.89130
Epoch 61/80: current_loss=10.19769 | best_loss=9.89130
Epoch 62/80: current_loss=9.88857 | best_loss=9.88857
Epoch 63/80: current_loss=10.01968 | best_loss=9.88857
Epoch 64/80: current_loss=9.94269 | best_loss=9.88857
Epoch 65/80: current_loss=10.52463 | best_loss=9.88857
Epoch 66/80: current_loss=9.94659 | best_loss=9.88857
Epoch 67/80: current_loss=10.20919 | best_loss=9.88857
Epoch 68/80: current_loss=10.04317 | best_loss=9.88857
Epoch 69/80: current_loss=10.92661 | best_loss=9.88857
Epoch 70/80: current_loss=9.91276 | best_loss=9.88857
Epoch 71/80: current_loss=10.00151 | best_loss=9.88857
Epoch 72/80: current_loss=9.90713 | best_loss=9.88857
Epoch 73/80: current_loss=9.91093 | best_loss=9.88857
Epoch 74/80: current_loss=9.98028 | best_loss=9.88857
Epoch 75/80: current_loss=10.02039 | best_loss=9.88857
Epoch 76/80: current_loss=9.97967 | best_loss=9.88857
Epoch 77/80: current_loss=9.96649 | best_loss=9.88857
Epoch 78/80: current_loss=10.20710 | best_loss=9.88857
Epoch 79/80: current_loss=10.48203 | best_loss=9.88857
      explained_var=0.00241 | mse_loss=9.55403
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.49902 | best_loss=8.49902
Epoch 1/80: current_loss=8.63180 | best_loss=8.49902
Epoch 2/80: current_loss=8.48136 | best_loss=8.48136
Epoch 3/80: current_loss=9.93617 | best_loss=8.48136
Epoch 4/80: current_loss=8.52538 | best_loss=8.48136
Epoch 5/80: current_loss=9.26865 | best_loss=8.48136
Epoch 6/80: current_loss=9.13512 | best_loss=8.48136
Epoch 7/80: current_loss=8.67138 | best_loss=8.48136
Epoch 8/80: current_loss=8.55464 | best_loss=8.48136
Epoch 9/80: current_loss=9.46067 | best_loss=8.48136
Epoch 10/80: current_loss=8.54910 | best_loss=8.48136
Epoch 11/80: current_loss=8.58425 | best_loss=8.48136
Epoch 12/80: current_loss=8.59519 | best_loss=8.48136
Epoch 13/80: current_loss=8.56953 | best_loss=8.48136
Epoch 14/80: current_loss=8.59804 | best_loss=8.48136
Epoch 15/80: current_loss=9.08590 | best_loss=8.48136
Epoch 16/80: current_loss=8.48304 | best_loss=8.48136
Epoch 17/80: current_loss=8.58199 | best_loss=8.48136
Epoch 18/80: current_loss=8.82930 | best_loss=8.48136
Epoch 19/80: current_loss=8.53801 | best_loss=8.48136
Epoch 20/80: current_loss=9.06572 | best_loss=8.48136
Epoch 21/80: current_loss=8.51689 | best_loss=8.48136
Epoch 22/80: current_loss=8.94632 | best_loss=8.48136
Early Stopping at epoch 22
      explained_var=-0.00140 | mse_loss=8.60229
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.10050 | best_loss=10.10050
Epoch 1/80: current_loss=10.32124 | best_loss=10.10050
Epoch 2/80: current_loss=9.48390 | best_loss=9.48390
Epoch 3/80: current_loss=9.55843 | best_loss=9.48390
Epoch 4/80: current_loss=9.79644 | best_loss=9.48390
Epoch 5/80: current_loss=10.73686 | best_loss=9.48390
Epoch 6/80: current_loss=9.50384 | best_loss=9.48390
Epoch 7/80: current_loss=9.45116 | best_loss=9.45116
Epoch 8/80: current_loss=9.40445 | best_loss=9.40445
Epoch 9/80: current_loss=9.51134 | best_loss=9.40445
Epoch 10/80: current_loss=9.74068 | best_loss=9.40445
Epoch 11/80: current_loss=9.44539 | best_loss=9.40445
Epoch 12/80: current_loss=9.25119 | best_loss=9.25119
Epoch 13/80: current_loss=9.73294 | best_loss=9.25119
Epoch 14/80: current_loss=9.47748 | best_loss=9.25119
Epoch 15/80: current_loss=9.26579 | best_loss=9.25119
Epoch 16/80: current_loss=9.59576 | best_loss=9.25119
Epoch 17/80: current_loss=9.26960 | best_loss=9.25119
Epoch 18/80: current_loss=9.40017 | best_loss=9.25119
Epoch 19/80: current_loss=9.26456 | best_loss=9.25119
Epoch 20/80: current_loss=9.43544 | best_loss=9.25119
Epoch 21/80: current_loss=9.33733 | best_loss=9.25119
Epoch 22/80: current_loss=9.72100 | best_loss=9.25119
Epoch 23/80: current_loss=9.28279 | best_loss=9.25119
Epoch 24/80: current_loss=9.27227 | best_loss=9.25119
Epoch 25/80: current_loss=9.58707 | best_loss=9.25119
Epoch 26/80: current_loss=10.14403 | best_loss=9.25119
Epoch 27/80: current_loss=9.42606 | best_loss=9.25119
Epoch 28/80: current_loss=9.33004 | best_loss=9.25119
Epoch 29/80: current_loss=9.46766 | best_loss=9.25119
Epoch 30/80: current_loss=9.24750 | best_loss=9.24750
Epoch 31/80: current_loss=9.41593 | best_loss=9.24750
Epoch 32/80: current_loss=9.43186 | best_loss=9.24750
Epoch 33/80: current_loss=9.50682 | best_loss=9.24750
Epoch 34/80: current_loss=9.25595 | best_loss=9.24750
Epoch 35/80: current_loss=9.53954 | best_loss=9.24750
Epoch 36/80: current_loss=9.56585 | best_loss=9.24750
Epoch 37/80: current_loss=9.35380 | best_loss=9.24750
Epoch 38/80: current_loss=9.33264 | best_loss=9.24750
Epoch 39/80: current_loss=9.44780 | best_loss=9.24750
Epoch 40/80: current_loss=9.84046 | best_loss=9.24750
Epoch 41/80: current_loss=11.04135 | best_loss=9.24750
Epoch 42/80: current_loss=9.37765 | best_loss=9.24750
Epoch 43/80: current_loss=10.43656 | best_loss=9.24750
Epoch 44/80: current_loss=9.31212 | best_loss=9.24750
Epoch 45/80: current_loss=9.76888 | best_loss=9.24750
Epoch 46/80: current_loss=9.82820 | best_loss=9.24750
Epoch 47/80: current_loss=9.25815 | best_loss=9.24750
Epoch 48/80: current_loss=9.31179 | best_loss=9.24750
Epoch 49/80: current_loss=9.43814 | best_loss=9.24750
Epoch 50/80: current_loss=9.33334 | best_loss=9.24750
Early Stopping at epoch 50
      explained_var=0.00411 | mse_loss=9.35479
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.98158 | best_loss=8.98158
Epoch 1/80: current_loss=8.98053 | best_loss=8.98053
Epoch 2/80: current_loss=8.99708 | best_loss=8.98053
Epoch 3/80: current_loss=8.99593 | best_loss=8.98053
Epoch 4/80: current_loss=9.01196 | best_loss=8.98053
Epoch 5/80: current_loss=8.98006 | best_loss=8.98006
Epoch 6/80: current_loss=8.94632 | best_loss=8.94632
Epoch 7/80: current_loss=9.05854 | best_loss=8.94632
Epoch 8/80: current_loss=8.96566 | best_loss=8.94632
Epoch 9/80: current_loss=8.96799 | best_loss=8.94632
Epoch 10/80: current_loss=8.98850 | best_loss=8.94632
Epoch 11/80: current_loss=9.18902 | best_loss=8.94632
Epoch 12/80: current_loss=9.25087 | best_loss=8.94632
Epoch 13/80: current_loss=8.96775 | best_loss=8.94632
Epoch 14/80: current_loss=9.36815 | best_loss=8.94632
Epoch 15/80: current_loss=8.96147 | best_loss=8.94632
Epoch 16/80: current_loss=8.96747 | best_loss=8.94632
Epoch 17/80: current_loss=8.95871 | best_loss=8.94632
Epoch 18/80: current_loss=9.27004 | best_loss=8.94632
Epoch 19/80: current_loss=9.02362 | best_loss=8.94632
Epoch 20/80: current_loss=8.95263 | best_loss=8.94632
Epoch 21/80: current_loss=9.13072 | best_loss=8.94632
Epoch 22/80: current_loss=8.95608 | best_loss=8.94632
Epoch 23/80: current_loss=8.99455 | best_loss=8.94632
Epoch 24/80: current_loss=9.00906 | best_loss=8.94632
Epoch 25/80: current_loss=9.18302 | best_loss=8.94632
Epoch 26/80: current_loss=8.95580 | best_loss=8.94632
Early Stopping at epoch 26
      explained_var=0.00174 | mse_loss=8.35735
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50505 | best_loss=8.50505
Epoch 1/80: current_loss=8.83241 | best_loss=8.50505
Epoch 2/80: current_loss=8.61801 | best_loss=8.50505
Epoch 3/80: current_loss=8.87024 | best_loss=8.50505
Epoch 4/80: current_loss=8.51016 | best_loss=8.50505
Epoch 5/80: current_loss=9.85661 | best_loss=8.50505
Epoch 6/80: current_loss=8.47481 | best_loss=8.47481
Epoch 7/80: current_loss=8.50182 | best_loss=8.47481
Epoch 8/80: current_loss=9.09366 | best_loss=8.47481
Epoch 9/80: current_loss=10.38664 | best_loss=8.47481
Epoch 10/80: current_loss=9.12416 | best_loss=8.47481
Epoch 11/80: current_loss=8.51274 | best_loss=8.47481
Epoch 12/80: current_loss=8.52849 | best_loss=8.47481
Epoch 13/80: current_loss=8.51304 | best_loss=8.47481
Epoch 14/80: current_loss=9.18569 | best_loss=8.47481
Epoch 15/80: current_loss=8.93115 | best_loss=8.47481
Epoch 16/80: current_loss=8.69303 | best_loss=8.47481
Epoch 17/80: current_loss=8.87820 | best_loss=8.47481
Epoch 18/80: current_loss=8.55179 | best_loss=8.47481
Epoch 19/80: current_loss=8.56462 | best_loss=8.47481
Epoch 20/80: current_loss=8.75661 | best_loss=8.47481
Epoch 21/80: current_loss=9.05308 | best_loss=8.47481
Epoch 22/80: current_loss=8.62273 | best_loss=8.47481
Epoch 23/80: current_loss=9.28647 | best_loss=8.47481
Epoch 24/80: current_loss=8.69310 | best_loss=8.47481
Epoch 25/80: current_loss=8.83967 | best_loss=8.47481
Epoch 26/80: current_loss=8.47100 | best_loss=8.47100
Epoch 27/80: current_loss=8.59680 | best_loss=8.47100
Epoch 28/80: current_loss=8.65486 | best_loss=8.47100
Epoch 29/80: current_loss=8.90496 | best_loss=8.47100
Epoch 30/80: current_loss=9.15291 | best_loss=8.47100
Epoch 31/80: current_loss=8.66574 | best_loss=8.47100
Epoch 32/80: current_loss=8.56383 | best_loss=8.47100
Epoch 33/80: current_loss=8.61719 | best_loss=8.47100
Epoch 34/80: current_loss=8.93140 | best_loss=8.47100
Epoch 35/80: current_loss=8.83378 | best_loss=8.47100
Epoch 36/80: current_loss=8.86776 | best_loss=8.47100
Epoch 37/80: current_loss=8.94607 | best_loss=8.47100
Epoch 38/80: current_loss=8.45635 | best_loss=8.45635
Epoch 39/80: current_loss=9.46520 | best_loss=8.45635
Epoch 40/80: current_loss=8.80580 | best_loss=8.45635
Epoch 41/80: current_loss=9.53947 | best_loss=8.45635
Epoch 42/80: current_loss=8.53043 | best_loss=8.45635
Epoch 43/80: current_loss=8.54001 | best_loss=8.45635
Epoch 44/80: current_loss=8.79788 | best_loss=8.45635
Epoch 45/80: current_loss=8.79149 | best_loss=8.45635
Epoch 46/80: current_loss=8.64839 | best_loss=8.45635
Epoch 47/80: current_loss=8.99495 | best_loss=8.45635
Epoch 48/80: current_loss=8.54024 | best_loss=8.45635
Epoch 49/80: current_loss=9.00463 | best_loss=8.45635
Epoch 50/80: current_loss=8.75911 | best_loss=8.45635
Epoch 51/80: current_loss=8.49452 | best_loss=8.45635
Epoch 52/80: current_loss=8.70444 | best_loss=8.45635
Epoch 53/80: current_loss=8.59455 | best_loss=8.45635
Epoch 54/80: current_loss=8.44881 | best_loss=8.44881
Epoch 55/80: current_loss=9.56230 | best_loss=8.44881
Epoch 56/80: current_loss=9.06027 | best_loss=8.44881
Epoch 57/80: current_loss=8.52253 | best_loss=8.44881
Epoch 58/80: current_loss=8.50652 | best_loss=8.44881
Epoch 59/80: current_loss=8.55088 | best_loss=8.44881
Epoch 60/80: current_loss=9.36145 | best_loss=8.44881
Epoch 61/80: current_loss=8.83608 | best_loss=8.44881
Epoch 62/80: current_loss=8.78287 | best_loss=8.44881
Epoch 63/80: current_loss=8.86158 | best_loss=8.44881
Epoch 64/80: current_loss=8.43258 | best_loss=8.43258
Epoch 65/80: current_loss=9.06920 | best_loss=8.43258
Epoch 66/80: current_loss=8.51248 | best_loss=8.43258
Epoch 67/80: current_loss=9.10180 | best_loss=8.43258
Epoch 68/80: current_loss=8.80597 | best_loss=8.43258
Epoch 69/80: current_loss=8.70834 | best_loss=8.43258
Epoch 70/80: current_loss=8.68592 | best_loss=8.43258
Epoch 71/80: current_loss=9.33091 | best_loss=8.43258
Epoch 72/80: current_loss=8.74086 | best_loss=8.43258
Epoch 73/80: current_loss=9.43740 | best_loss=8.43258
Epoch 74/80: current_loss=8.86962 | best_loss=8.43258
Epoch 75/80: current_loss=8.66106 | best_loss=8.43258
Epoch 76/80: current_loss=8.47200 | best_loss=8.43258
Epoch 77/80: current_loss=8.69103 | best_loss=8.43258
Epoch 78/80: current_loss=9.02225 | best_loss=8.43258
Epoch 79/80: current_loss=8.47898 | best_loss=8.43258
      explained_var=-0.00301 | mse_loss=8.22690
----------------------------------------------
Average early_stopping_point: 39| avg_exp_var=0.00077| avg_loss=8.81907
----------------------------------------------


----------------------------------------------
Params for Trial 17
{'learning_rate': 0.001, 'weight_decay': 0.003599382439907362, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.21843 | best_loss=10.21843
Epoch 1/80: current_loss=9.92938 | best_loss=9.92938
Epoch 2/80: current_loss=9.93845 | best_loss=9.92938
Epoch 3/80: current_loss=9.97354 | best_loss=9.92938
Epoch 4/80: current_loss=9.91243 | best_loss=9.91243
Epoch 5/80: current_loss=9.89597 | best_loss=9.89597
Epoch 6/80: current_loss=9.93383 | best_loss=9.89597
Epoch 7/80: current_loss=9.89758 | best_loss=9.89597
Epoch 8/80: current_loss=9.89573 | best_loss=9.89573
Epoch 9/80: current_loss=9.89610 | best_loss=9.89573
Epoch 10/80: current_loss=9.92112 | best_loss=9.89573
Epoch 11/80: current_loss=9.89542 | best_loss=9.89542
Epoch 12/80: current_loss=10.00883 | best_loss=9.89542
Epoch 13/80: current_loss=9.92926 | best_loss=9.89542
Epoch 14/80: current_loss=9.90084 | best_loss=9.89542
Epoch 15/80: current_loss=9.90796 | best_loss=9.89542
Epoch 16/80: current_loss=9.95327 | best_loss=9.89542
Epoch 17/80: current_loss=9.90939 | best_loss=9.89542
Epoch 18/80: current_loss=9.95671 | best_loss=9.89542
Epoch 19/80: current_loss=9.92259 | best_loss=9.89542
Epoch 20/80: current_loss=9.91628 | best_loss=9.89542
Epoch 21/80: current_loss=9.89627 | best_loss=9.89542
Epoch 22/80: current_loss=9.91376 | best_loss=9.89542
Epoch 23/80: current_loss=9.99333 | best_loss=9.89542
Epoch 24/80: current_loss=9.91685 | best_loss=9.89542
Epoch 25/80: current_loss=9.89883 | best_loss=9.89542
Epoch 26/80: current_loss=9.90518 | best_loss=9.89542
Epoch 27/80: current_loss=9.95790 | best_loss=9.89542
Epoch 28/80: current_loss=10.00727 | best_loss=9.89542
Epoch 29/80: current_loss=9.90500 | best_loss=9.89542
Epoch 30/80: current_loss=9.94251 | best_loss=9.89542
Epoch 31/80: current_loss=9.89963 | best_loss=9.89542
Early Stopping at epoch 31
      explained_var=0.00147 | mse_loss=9.56738
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.49209 | best_loss=8.49209
Epoch 1/80: current_loss=8.52123 | best_loss=8.49209
Epoch 2/80: current_loss=8.56113 | best_loss=8.49209
Epoch 3/80: current_loss=8.49604 | best_loss=8.49209
Epoch 4/80: current_loss=8.49279 | best_loss=8.49209
Epoch 5/80: current_loss=8.49854 | best_loss=8.49209
Epoch 6/80: current_loss=8.49920 | best_loss=8.49209
Epoch 7/80: current_loss=8.50150 | best_loss=8.49209
Epoch 8/80: current_loss=8.50717 | best_loss=8.49209
Epoch 9/80: current_loss=8.49812 | best_loss=8.49209
Epoch 10/80: current_loss=8.51165 | best_loss=8.49209
Epoch 11/80: current_loss=8.49077 | best_loss=8.49077
Epoch 12/80: current_loss=8.54514 | best_loss=8.49077
Epoch 13/80: current_loss=8.59292 | best_loss=8.49077
Epoch 14/80: current_loss=8.59007 | best_loss=8.49077
Epoch 15/80: current_loss=8.54773 | best_loss=8.49077
Epoch 16/80: current_loss=8.50066 | best_loss=8.49077
Epoch 17/80: current_loss=8.53128 | best_loss=8.49077
Epoch 18/80: current_loss=8.47755 | best_loss=8.47755
Epoch 19/80: current_loss=8.53734 | best_loss=8.47755
Epoch 20/80: current_loss=8.48459 | best_loss=8.47755
Epoch 21/80: current_loss=8.50232 | best_loss=8.47755
Epoch 22/80: current_loss=8.56429 | best_loss=8.47755
Epoch 23/80: current_loss=8.49546 | best_loss=8.47755
Epoch 24/80: current_loss=8.49667 | best_loss=8.47755
Epoch 25/80: current_loss=8.49368 | best_loss=8.47755
Epoch 26/80: current_loss=8.51711 | best_loss=8.47755
Epoch 27/80: current_loss=8.51413 | best_loss=8.47755
Epoch 28/80: current_loss=8.61037 | best_loss=8.47755
Epoch 29/80: current_loss=8.52686 | best_loss=8.47755
Epoch 30/80: current_loss=8.52314 | best_loss=8.47755
Epoch 31/80: current_loss=8.55124 | best_loss=8.47755
Epoch 32/80: current_loss=8.52103 | best_loss=8.47755
Epoch 33/80: current_loss=8.48702 | best_loss=8.47755
Epoch 34/80: current_loss=8.50412 | best_loss=8.47755
Epoch 35/80: current_loss=8.53269 | best_loss=8.47755
Epoch 36/80: current_loss=8.58473 | best_loss=8.47755
Epoch 37/80: current_loss=8.57084 | best_loss=8.47755
Epoch 38/80: current_loss=8.52633 | best_loss=8.47755
Early Stopping at epoch 38
      explained_var=0.00170 | mse_loss=8.56194
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.39533 | best_loss=9.39533
Epoch 1/80: current_loss=9.54879 | best_loss=9.39533
Epoch 2/80: current_loss=9.35050 | best_loss=9.35050
Epoch 3/80: current_loss=9.41546 | best_loss=9.35050
Epoch 4/80: current_loss=9.51219 | best_loss=9.35050
Epoch 5/80: current_loss=9.43595 | best_loss=9.35050
Epoch 6/80: current_loss=9.46731 | best_loss=9.35050
Epoch 7/80: current_loss=9.41298 | best_loss=9.35050
Epoch 8/80: current_loss=9.64207 | best_loss=9.35050
Epoch 9/80: current_loss=9.83873 | best_loss=9.35050
Epoch 10/80: current_loss=9.69373 | best_loss=9.35050
Epoch 11/80: current_loss=9.54940 | best_loss=9.35050
Epoch 12/80: current_loss=9.36505 | best_loss=9.35050
Epoch 13/80: current_loss=9.46196 | best_loss=9.35050
Epoch 14/80: current_loss=9.60648 | best_loss=9.35050
Epoch 15/80: current_loss=9.53902 | best_loss=9.35050
Epoch 16/80: current_loss=9.56917 | best_loss=9.35050
Epoch 17/80: current_loss=9.39002 | best_loss=9.35050
Epoch 18/80: current_loss=9.34533 | best_loss=9.34533
Epoch 19/80: current_loss=9.34800 | best_loss=9.34533
Epoch 20/80: current_loss=9.35707 | best_loss=9.34533
Epoch 21/80: current_loss=9.43232 | best_loss=9.34533
Epoch 22/80: current_loss=9.44381 | best_loss=9.34533
Epoch 23/80: current_loss=9.29904 | best_loss=9.29904
Epoch 24/80: current_loss=9.34210 | best_loss=9.29904
Epoch 25/80: current_loss=9.53092 | best_loss=9.29904
Epoch 26/80: current_loss=9.34622 | best_loss=9.29904
Epoch 27/80: current_loss=9.64141 | best_loss=9.29904
Epoch 28/80: current_loss=9.43280 | best_loss=9.29904
Epoch 29/80: current_loss=9.40819 | best_loss=9.29904
Epoch 30/80: current_loss=9.54135 | best_loss=9.29904
Epoch 31/80: current_loss=9.35847 | best_loss=9.29904
Epoch 32/80: current_loss=9.43230 | best_loss=9.29904
Epoch 33/80: current_loss=9.45375 | best_loss=9.29904
Epoch 34/80: current_loss=9.68103 | best_loss=9.29904
Epoch 35/80: current_loss=9.36801 | best_loss=9.29904
Epoch 36/80: current_loss=9.55435 | best_loss=9.29904
Epoch 37/80: current_loss=9.52096 | best_loss=9.29904
Epoch 38/80: current_loss=9.52154 | best_loss=9.29904
Epoch 39/80: current_loss=9.47772 | best_loss=9.29904
Epoch 40/80: current_loss=9.30439 | best_loss=9.29904
Epoch 41/80: current_loss=9.45779 | best_loss=9.29904
Epoch 42/80: current_loss=9.38692 | best_loss=9.29904
Epoch 43/80: current_loss=9.73284 | best_loss=9.29904
Early Stopping at epoch 43
      explained_var=0.00122 | mse_loss=9.41845
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.93678 | best_loss=8.93678
Epoch 1/80: current_loss=8.93149 | best_loss=8.93149
Epoch 2/80: current_loss=8.91893 | best_loss=8.91893
Epoch 3/80: current_loss=9.14358 | best_loss=8.91893
Epoch 4/80: current_loss=8.95382 | best_loss=8.91893
Epoch 5/80: current_loss=8.98491 | best_loss=8.91893
Epoch 6/80: current_loss=8.95630 | best_loss=8.91893
Epoch 7/80: current_loss=9.04406 | best_loss=8.91893
Epoch 8/80: current_loss=9.00997 | best_loss=8.91893
Epoch 9/80: current_loss=8.96993 | best_loss=8.91893
Epoch 10/80: current_loss=9.01089 | best_loss=8.91893
Epoch 11/80: current_loss=8.95812 | best_loss=8.91893
Epoch 12/80: current_loss=9.01628 | best_loss=8.91893
Epoch 13/80: current_loss=8.99307 | best_loss=8.91893
Epoch 14/80: current_loss=9.17185 | best_loss=8.91893
Epoch 15/80: current_loss=8.92879 | best_loss=8.91893
Epoch 16/80: current_loss=8.94325 | best_loss=8.91893
Epoch 17/80: current_loss=9.01402 | best_loss=8.91893
Epoch 18/80: current_loss=9.03550 | best_loss=8.91893
Epoch 19/80: current_loss=8.94781 | best_loss=8.91893
Epoch 20/80: current_loss=8.98766 | best_loss=8.91893
Epoch 21/80: current_loss=8.95045 | best_loss=8.91893
Epoch 22/80: current_loss=8.94890 | best_loss=8.91893
Early Stopping at epoch 22
      explained_var=0.00095 | mse_loss=8.35731
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.48528 | best_loss=8.48528
Epoch 1/80: current_loss=8.51543 | best_loss=8.48528
Epoch 2/80: current_loss=8.48270 | best_loss=8.48270
Epoch 3/80: current_loss=8.50466 | best_loss=8.48270
Epoch 4/80: current_loss=8.52794 | best_loss=8.48270
Epoch 5/80: current_loss=8.48791 | best_loss=8.48270
Epoch 6/80: current_loss=8.49043 | best_loss=8.48270
Epoch 7/80: current_loss=8.48529 | best_loss=8.48270
Epoch 8/80: current_loss=8.49575 | best_loss=8.48270
Epoch 9/80: current_loss=8.48511 | best_loss=8.48270
Epoch 10/80: current_loss=8.52913 | best_loss=8.48270
Epoch 11/80: current_loss=8.48307 | best_loss=8.48270
Epoch 12/80: current_loss=8.52617 | best_loss=8.48270
Epoch 13/80: current_loss=8.48787 | best_loss=8.48270
Epoch 14/80: current_loss=8.74652 | best_loss=8.48270
Epoch 15/80: current_loss=8.52810 | best_loss=8.48270
Epoch 16/80: current_loss=8.49517 | best_loss=8.48270
Epoch 17/80: current_loss=8.51965 | best_loss=8.48270
Epoch 18/80: current_loss=8.49237 | best_loss=8.48270
Epoch 19/80: current_loss=8.54754 | best_loss=8.48270
Epoch 20/80: current_loss=8.49056 | best_loss=8.48270
Epoch 21/80: current_loss=8.49144 | best_loss=8.48270
Epoch 22/80: current_loss=8.60007 | best_loss=8.48270
Early Stopping at epoch 22
      explained_var=-0.00001 | mse_loss=8.20077
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00107| avg_loss=8.82117
----------------------------------------------


----------------------------------------------
Params for Trial 18
{'learning_rate': 0.01, 'weight_decay': 0.0014939019685960917, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.05044 | best_loss=10.05044
Epoch 1/80: current_loss=10.11484 | best_loss=10.05044
Epoch 2/80: current_loss=10.13189 | best_loss=10.05044
Epoch 3/80: current_loss=9.94483 | best_loss=9.94483
Epoch 4/80: current_loss=10.21267 | best_loss=9.94483
Epoch 5/80: current_loss=10.84789 | best_loss=9.94483
Epoch 6/80: current_loss=10.11865 | best_loss=9.94483
Epoch 7/80: current_loss=10.32646 | best_loss=9.94483
Epoch 8/80: current_loss=9.98194 | best_loss=9.94483
Epoch 9/80: current_loss=10.15407 | best_loss=9.94483
Epoch 10/80: current_loss=9.90821 | best_loss=9.90821
Epoch 11/80: current_loss=11.31449 | best_loss=9.90821
Epoch 12/80: current_loss=10.32467 | best_loss=9.90821
Epoch 13/80: current_loss=9.98457 | best_loss=9.90821
Epoch 14/80: current_loss=9.90440 | best_loss=9.90440
Epoch 15/80: current_loss=10.21857 | best_loss=9.90440
Epoch 16/80: current_loss=9.92302 | best_loss=9.90440
Epoch 17/80: current_loss=10.43530 | best_loss=9.90440
Epoch 18/80: current_loss=9.97013 | best_loss=9.90440
Epoch 19/80: current_loss=10.00551 | best_loss=9.90440
Epoch 20/80: current_loss=9.98595 | best_loss=9.90440
Epoch 21/80: current_loss=9.91396 | best_loss=9.90440
Epoch 22/80: current_loss=10.07008 | best_loss=9.90440
Epoch 23/80: current_loss=9.92064 | best_loss=9.90440
Epoch 24/80: current_loss=10.13695 | best_loss=9.90440
Epoch 25/80: current_loss=10.07490 | best_loss=9.90440
Epoch 26/80: current_loss=10.19342 | best_loss=9.90440
Epoch 27/80: current_loss=10.16670 | best_loss=9.90440
Epoch 28/80: current_loss=10.11526 | best_loss=9.90440
Epoch 29/80: current_loss=11.33127 | best_loss=9.90440
Epoch 30/80: current_loss=9.99774 | best_loss=9.90440
Epoch 31/80: current_loss=9.94284 | best_loss=9.90440
Epoch 32/80: current_loss=10.76623 | best_loss=9.90440
Epoch 33/80: current_loss=10.00061 | best_loss=9.90440
Epoch 34/80: current_loss=9.93162 | best_loss=9.90440
Early Stopping at epoch 34
      explained_var=0.00045 | mse_loss=9.57250
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.46926 | best_loss=8.46926
Epoch 1/80: current_loss=8.48741 | best_loss=8.46926
Epoch 2/80: current_loss=8.64658 | best_loss=8.46926
Epoch 3/80: current_loss=8.51466 | best_loss=8.46926
Epoch 4/80: current_loss=8.57867 | best_loss=8.46926
Epoch 5/80: current_loss=8.50606 | best_loss=8.46926
Epoch 6/80: current_loss=8.56275 | best_loss=8.46926
Epoch 7/80: current_loss=8.70141 | best_loss=8.46926
Epoch 8/80: current_loss=9.66482 | best_loss=8.46926
Epoch 9/80: current_loss=8.71546 | best_loss=8.46926
Epoch 10/80: current_loss=8.53897 | best_loss=8.46926
Epoch 11/80: current_loss=8.52583 | best_loss=8.46926
Epoch 12/80: current_loss=8.47761 | best_loss=8.46926
Epoch 13/80: current_loss=8.48811 | best_loss=8.46926
Epoch 14/80: current_loss=9.26124 | best_loss=8.46926
Epoch 15/80: current_loss=8.86842 | best_loss=8.46926
Epoch 16/80: current_loss=8.65980 | best_loss=8.46926
Epoch 17/80: current_loss=8.95431 | best_loss=8.46926
Epoch 18/80: current_loss=8.62930 | best_loss=8.46926
Epoch 19/80: current_loss=8.56537 | best_loss=8.46926
Epoch 20/80: current_loss=9.15965 | best_loss=8.46926
Early Stopping at epoch 20
      explained_var=0.00279 | mse_loss=8.55734
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.34648 | best_loss=9.34648
Epoch 1/80: current_loss=9.52052 | best_loss=9.34648
Epoch 2/80: current_loss=10.43451 | best_loss=9.34648
Epoch 3/80: current_loss=9.84523 | best_loss=9.34648
Epoch 4/80: current_loss=9.84383 | best_loss=9.34648
Epoch 5/80: current_loss=10.19792 | best_loss=9.34648
Epoch 6/80: current_loss=9.40790 | best_loss=9.34648
Epoch 7/80: current_loss=9.31202 | best_loss=9.31202
Epoch 8/80: current_loss=10.23602 | best_loss=9.31202
Epoch 9/80: current_loss=9.85653 | best_loss=9.31202
Epoch 10/80: current_loss=9.38230 | best_loss=9.31202
Epoch 11/80: current_loss=9.46101 | best_loss=9.31202
Epoch 12/80: current_loss=9.68712 | best_loss=9.31202
Epoch 13/80: current_loss=10.16814 | best_loss=9.31202
Epoch 14/80: current_loss=9.70937 | best_loss=9.31202
Epoch 15/80: current_loss=9.63761 | best_loss=9.31202
Epoch 16/80: current_loss=9.44420 | best_loss=9.31202
Epoch 17/80: current_loss=9.25871 | best_loss=9.25871
Epoch 18/80: current_loss=9.88774 | best_loss=9.25871
Epoch 19/80: current_loss=9.99672 | best_loss=9.25871
Epoch 20/80: current_loss=9.65498 | best_loss=9.25871
Epoch 21/80: current_loss=9.43833 | best_loss=9.25871
Epoch 22/80: current_loss=9.28701 | best_loss=9.25871
Epoch 23/80: current_loss=9.44104 | best_loss=9.25871
Epoch 24/80: current_loss=9.56608 | best_loss=9.25871
Epoch 25/80: current_loss=9.75481 | best_loss=9.25871
Epoch 26/80: current_loss=9.39407 | best_loss=9.25871
Epoch 27/80: current_loss=10.07293 | best_loss=9.25871
Epoch 28/80: current_loss=9.46976 | best_loss=9.25871
Epoch 29/80: current_loss=9.70791 | best_loss=9.25871
Epoch 30/80: current_loss=9.42316 | best_loss=9.25871
Epoch 31/80: current_loss=10.41441 | best_loss=9.25871
Epoch 32/80: current_loss=9.42249 | best_loss=9.25871
Epoch 33/80: current_loss=9.71444 | best_loss=9.25871
Epoch 34/80: current_loss=10.75242 | best_loss=9.25871
Epoch 35/80: current_loss=10.19477 | best_loss=9.25871
Epoch 36/80: current_loss=10.72002 | best_loss=9.25871
Epoch 37/80: current_loss=10.45689 | best_loss=9.25871
Early Stopping at epoch 37
      explained_var=0.00227 | mse_loss=9.35630
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.34329 | best_loss=9.34329
Epoch 1/80: current_loss=8.93783 | best_loss=8.93783
Epoch 2/80: current_loss=9.22474 | best_loss=8.93783
Epoch 3/80: current_loss=9.91769 | best_loss=8.93783
Epoch 4/80: current_loss=8.94414 | best_loss=8.93783
Epoch 5/80: current_loss=9.47574 | best_loss=8.93783
Epoch 6/80: current_loss=9.03838 | best_loss=8.93783
Epoch 7/80: current_loss=9.27146 | best_loss=8.93783
Epoch 8/80: current_loss=9.02908 | best_loss=8.93783
Epoch 9/80: current_loss=9.29969 | best_loss=8.93783
Epoch 10/80: current_loss=9.14294 | best_loss=8.93783
Epoch 11/80: current_loss=9.73340 | best_loss=8.93783
Epoch 12/80: current_loss=9.39601 | best_loss=8.93783
Epoch 13/80: current_loss=9.25071 | best_loss=8.93783
Epoch 14/80: current_loss=9.04572 | best_loss=8.93783
Epoch 15/80: current_loss=9.00965 | best_loss=8.93783
Epoch 16/80: current_loss=9.23537 | best_loss=8.93783
Epoch 17/80: current_loss=9.20792 | best_loss=8.93783
Epoch 18/80: current_loss=9.24823 | best_loss=8.93783
Epoch 19/80: current_loss=8.97021 | best_loss=8.93783
Epoch 20/80: current_loss=9.56867 | best_loss=8.93783
Epoch 21/80: current_loss=9.15125 | best_loss=8.93783
Early Stopping at epoch 21
      explained_var=0.00037 | mse_loss=8.35889
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.06387 | best_loss=9.06387
Epoch 1/80: current_loss=9.35063 | best_loss=9.06387
Epoch 2/80: current_loss=8.98639 | best_loss=8.98639
Epoch 3/80: current_loss=8.48921 | best_loss=8.48921
Epoch 4/80: current_loss=8.75164 | best_loss=8.48921
Epoch 5/80: current_loss=8.66860 | best_loss=8.48921
Epoch 6/80: current_loss=8.49912 | best_loss=8.48921
Epoch 7/80: current_loss=8.62154 | best_loss=8.48921
Epoch 8/80: current_loss=8.53515 | best_loss=8.48921
Epoch 9/80: current_loss=9.00516 | best_loss=8.48921
Epoch 10/80: current_loss=8.78973 | best_loss=8.48921
Epoch 11/80: current_loss=8.98109 | best_loss=8.48921
Epoch 12/80: current_loss=8.58541 | best_loss=8.48921
Epoch 13/80: current_loss=8.63442 | best_loss=8.48921
Epoch 14/80: current_loss=8.51352 | best_loss=8.48921
Epoch 15/80: current_loss=8.52688 | best_loss=8.48921
Epoch 16/80: current_loss=8.56415 | best_loss=8.48921
Epoch 17/80: current_loss=8.66202 | best_loss=8.48921
Epoch 18/80: current_loss=8.46779 | best_loss=8.46779
Epoch 19/80: current_loss=8.65589 | best_loss=8.46779
Epoch 20/80: current_loss=8.42100 | best_loss=8.42100
Epoch 21/80: current_loss=8.64274 | best_loss=8.42100
Epoch 22/80: current_loss=8.64684 | best_loss=8.42100
Epoch 23/80: current_loss=8.37387 | best_loss=8.37387
Epoch 24/80: current_loss=8.48437 | best_loss=8.37387
Epoch 25/80: current_loss=8.37663 | best_loss=8.37387
Epoch 26/80: current_loss=8.82441 | best_loss=8.37387
Epoch 27/80: current_loss=16.75759 | best_loss=8.37387
Epoch 28/80: current_loss=8.78136 | best_loss=8.37387
Epoch 29/80: current_loss=8.53129 | best_loss=8.37387
Epoch 30/80: current_loss=8.91712 | best_loss=8.37387
Epoch 31/80: current_loss=8.90123 | best_loss=8.37387
Epoch 32/80: current_loss=8.69963 | best_loss=8.37387
Epoch 33/80: current_loss=8.68289 | best_loss=8.37387
Epoch 34/80: current_loss=8.94438 | best_loss=8.37387
Epoch 35/80: current_loss=9.15075 | best_loss=8.37387
Epoch 36/80: current_loss=8.43071 | best_loss=8.37387
Epoch 37/80: current_loss=8.70142 | best_loss=8.37387
Epoch 38/80: current_loss=8.88554 | best_loss=8.37387
Epoch 39/80: current_loss=8.52028 | best_loss=8.37387
Epoch 40/80: current_loss=8.75395 | best_loss=8.37387
Epoch 41/80: current_loss=8.61640 | best_loss=8.37387
Epoch 42/80: current_loss=8.70426 | best_loss=8.37387
Epoch 43/80: current_loss=9.44075 | best_loss=8.37387
Early Stopping at epoch 43
      explained_var=0.01552 | mse_loss=8.07345
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00428| avg_loss=8.78369
----------------------------------------------


----------------------------------------------
Params for Trial 19
{'learning_rate': 0.01, 'weight_decay': 0.0019321913361300055, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.02967 | best_loss=10.02967
Epoch 1/80: current_loss=9.91443 | best_loss=9.91443
Epoch 2/80: current_loss=9.91256 | best_loss=9.91256
Epoch 3/80: current_loss=9.93422 | best_loss=9.91256
Epoch 4/80: current_loss=9.96751 | best_loss=9.91256
Epoch 5/80: current_loss=9.90541 | best_loss=9.90541
Epoch 6/80: current_loss=9.94061 | best_loss=9.90541
Epoch 7/80: current_loss=10.53850 | best_loss=9.90541
Epoch 8/80: current_loss=10.01044 | best_loss=9.90541
Epoch 9/80: current_loss=11.62526 | best_loss=9.90541
Epoch 10/80: current_loss=10.06662 | best_loss=9.90541
Epoch 11/80: current_loss=9.90531 | best_loss=9.90531
Epoch 12/80: current_loss=10.40688 | best_loss=9.90531
Epoch 13/80: current_loss=9.90589 | best_loss=9.90531
Epoch 14/80: current_loss=10.14992 | best_loss=9.90531
Epoch 15/80: current_loss=9.90833 | best_loss=9.90531
Epoch 16/80: current_loss=9.93997 | best_loss=9.90531
Epoch 17/80: current_loss=9.97685 | best_loss=9.90531
Epoch 18/80: current_loss=9.94036 | best_loss=9.90531
Epoch 19/80: current_loss=10.02959 | best_loss=9.90531
Epoch 20/80: current_loss=9.90204 | best_loss=9.90204
Epoch 21/80: current_loss=9.91912 | best_loss=9.90204
Epoch 22/80: current_loss=10.10462 | best_loss=9.90204
Epoch 23/80: current_loss=10.92914 | best_loss=9.90204
Epoch 24/80: current_loss=10.25909 | best_loss=9.90204
Epoch 25/80: current_loss=10.45363 | best_loss=9.90204
Epoch 26/80: current_loss=9.99584 | best_loss=9.90204
Epoch 27/80: current_loss=10.22712 | best_loss=9.90204
Epoch 28/80: current_loss=10.30716 | best_loss=9.90204
Epoch 29/80: current_loss=9.89437 | best_loss=9.89437
Epoch 30/80: current_loss=10.18182 | best_loss=9.89437
Epoch 31/80: current_loss=9.97227 | best_loss=9.89437
Epoch 32/80: current_loss=10.06002 | best_loss=9.89437
Epoch 33/80: current_loss=10.07523 | best_loss=9.89437
Epoch 34/80: current_loss=10.09672 | best_loss=9.89437
Epoch 35/80: current_loss=10.55499 | best_loss=9.89437
Epoch 36/80: current_loss=10.26056 | best_loss=9.89437
Epoch 37/80: current_loss=9.89714 | best_loss=9.89437
Epoch 38/80: current_loss=10.00131 | best_loss=9.89437
Epoch 39/80: current_loss=10.10795 | best_loss=9.89437
Epoch 40/80: current_loss=10.04173 | best_loss=9.89437
Epoch 41/80: current_loss=10.07367 | best_loss=9.89437
Epoch 42/80: current_loss=9.91973 | best_loss=9.89437
Epoch 43/80: current_loss=10.67181 | best_loss=9.89437
Epoch 44/80: current_loss=10.09103 | best_loss=9.89437
Epoch 45/80: current_loss=10.56367 | best_loss=9.89437
Epoch 46/80: current_loss=11.67287 | best_loss=9.89437
Epoch 47/80: current_loss=9.96502 | best_loss=9.89437
Epoch 48/80: current_loss=10.31664 | best_loss=9.89437
Epoch 49/80: current_loss=9.90556 | best_loss=9.89437
Early Stopping at epoch 49
      explained_var=0.00167 | mse_loss=9.56742
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.51849 | best_loss=8.51849
Epoch 1/80: current_loss=8.60931 | best_loss=8.51849
Epoch 2/80: current_loss=8.54933 | best_loss=8.51849
Epoch 3/80: current_loss=8.55804 | best_loss=8.51849
Epoch 4/80: current_loss=8.77071 | best_loss=8.51849
Epoch 5/80: current_loss=8.82629 | best_loss=8.51849
Epoch 6/80: current_loss=8.49993 | best_loss=8.49993
Epoch 7/80: current_loss=9.14804 | best_loss=8.49993
Epoch 8/80: current_loss=8.57113 | best_loss=8.49993
Epoch 9/80: current_loss=9.19893 | best_loss=8.49993
Epoch 10/80: current_loss=8.50081 | best_loss=8.49993
Epoch 11/80: current_loss=8.49880 | best_loss=8.49880
Epoch 12/80: current_loss=8.61854 | best_loss=8.49880
Epoch 13/80: current_loss=8.50088 | best_loss=8.49880
Epoch 14/80: current_loss=8.48952 | best_loss=8.48952
Epoch 15/80: current_loss=9.28153 | best_loss=8.48952
Epoch 16/80: current_loss=8.49797 | best_loss=8.48952
Epoch 17/80: current_loss=8.61795 | best_loss=8.48952
Epoch 18/80: current_loss=8.51106 | best_loss=8.48952
Epoch 19/80: current_loss=8.76904 | best_loss=8.48952
Epoch 20/80: current_loss=8.87152 | best_loss=8.48952
Epoch 21/80: current_loss=8.75334 | best_loss=8.48952
Epoch 22/80: current_loss=8.51993 | best_loss=8.48952
Epoch 23/80: current_loss=8.82748 | best_loss=8.48952
Epoch 24/80: current_loss=8.51034 | best_loss=8.48952
Epoch 25/80: current_loss=8.51166 | best_loss=8.48952
Epoch 26/80: current_loss=8.71716 | best_loss=8.48952
Epoch 27/80: current_loss=8.85665 | best_loss=8.48952
Epoch 28/80: current_loss=8.91962 | best_loss=8.48952
Epoch 29/80: current_loss=8.56402 | best_loss=8.48952
Epoch 30/80: current_loss=8.49314 | best_loss=8.48952
Epoch 31/80: current_loss=8.59256 | best_loss=8.48952
Epoch 32/80: current_loss=8.46171 | best_loss=8.46171
Epoch 33/80: current_loss=8.52167 | best_loss=8.46171
Epoch 34/80: current_loss=8.53798 | best_loss=8.46171
Epoch 35/80: current_loss=8.58248 | best_loss=8.46171
Epoch 36/80: current_loss=8.46791 | best_loss=8.46171
Epoch 37/80: current_loss=8.64364 | best_loss=8.46171
Epoch 38/80: current_loss=8.51305 | best_loss=8.46171
Epoch 39/80: current_loss=8.54740 | best_loss=8.46171
Epoch 40/80: current_loss=8.50906 | best_loss=8.46171
Epoch 41/80: current_loss=8.90764 | best_loss=8.46171
Epoch 42/80: current_loss=8.50327 | best_loss=8.46171
Epoch 43/80: current_loss=8.46647 | best_loss=8.46171
Epoch 44/80: current_loss=8.72906 | best_loss=8.46171
Epoch 45/80: current_loss=8.56328 | best_loss=8.46171
Epoch 46/80: current_loss=8.51127 | best_loss=8.46171
Epoch 47/80: current_loss=8.51989 | best_loss=8.46171
Epoch 48/80: current_loss=8.92160 | best_loss=8.46171
Epoch 49/80: current_loss=8.55455 | best_loss=8.46171
Epoch 50/80: current_loss=8.51006 | best_loss=8.46171
Epoch 51/80: current_loss=8.55200 | best_loss=8.46171
Epoch 52/80: current_loss=8.67542 | best_loss=8.46171
Early Stopping at epoch 52
      explained_var=0.00283 | mse_loss=8.56402
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.55115 | best_loss=9.55115
Epoch 1/80: current_loss=9.79952 | best_loss=9.55115
Epoch 2/80: current_loss=9.60558 | best_loss=9.55115
Epoch 3/80: current_loss=9.59783 | best_loss=9.55115
Epoch 4/80: current_loss=10.06194 | best_loss=9.55115
Epoch 5/80: current_loss=9.69379 | best_loss=9.55115
Epoch 6/80: current_loss=9.28355 | best_loss=9.28355
Epoch 7/80: current_loss=9.32019 | best_loss=9.28355
Epoch 8/80: current_loss=9.68409 | best_loss=9.28355
Epoch 9/80: current_loss=9.83391 | best_loss=9.28355
Epoch 10/80: current_loss=9.43545 | best_loss=9.28355
Epoch 11/80: current_loss=9.63666 | best_loss=9.28355
Epoch 12/80: current_loss=9.69645 | best_loss=9.28355
Epoch 13/80: current_loss=9.56012 | best_loss=9.28355
Epoch 14/80: current_loss=9.48968 | best_loss=9.28355
Epoch 15/80: current_loss=9.31897 | best_loss=9.28355
Epoch 16/80: current_loss=10.01980 | best_loss=9.28355
Epoch 17/80: current_loss=10.70480 | best_loss=9.28355
Epoch 18/80: current_loss=9.62853 | best_loss=9.28355
Epoch 19/80: current_loss=9.32307 | best_loss=9.28355
Epoch 20/80: current_loss=9.53101 | best_loss=9.28355
Epoch 21/80: current_loss=9.77900 | best_loss=9.28355
Epoch 22/80: current_loss=9.31696 | best_loss=9.28355
Epoch 23/80: current_loss=9.77609 | best_loss=9.28355
Epoch 24/80: current_loss=9.65883 | best_loss=9.28355
Epoch 25/80: current_loss=9.52758 | best_loss=9.28355
Epoch 26/80: current_loss=9.99464 | best_loss=9.28355
Early Stopping at epoch 26
      explained_var=-0.00204 | mse_loss=9.42470
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.86495 | best_loss=9.86495
Epoch 1/80: current_loss=9.67751 | best_loss=9.67751
Epoch 2/80: current_loss=9.53422 | best_loss=9.53422
Epoch 3/80: current_loss=10.44785 | best_loss=9.53422
Epoch 4/80: current_loss=9.03092 | best_loss=9.03092
Epoch 5/80: current_loss=9.30236 | best_loss=9.03092
Epoch 6/80: current_loss=9.99845 | best_loss=9.03092
Epoch 7/80: current_loss=9.60941 | best_loss=9.03092
Epoch 8/80: current_loss=9.05213 | best_loss=9.03092
Epoch 9/80: current_loss=8.95983 | best_loss=8.95983
Epoch 10/80: current_loss=8.89696 | best_loss=8.89696
Epoch 11/80: current_loss=8.93595 | best_loss=8.89696
Epoch 12/80: current_loss=9.05745 | best_loss=8.89696
Epoch 13/80: current_loss=9.33186 | best_loss=8.89696
Epoch 14/80: current_loss=9.18047 | best_loss=8.89696
Epoch 15/80: current_loss=9.90142 | best_loss=8.89696
Epoch 16/80: current_loss=8.93520 | best_loss=8.89696
Epoch 17/80: current_loss=9.01188 | best_loss=8.89696
Epoch 18/80: current_loss=9.57260 | best_loss=8.89696
Epoch 19/80: current_loss=9.02846 | best_loss=8.89696
Epoch 20/80: current_loss=9.37214 | best_loss=8.89696
Epoch 21/80: current_loss=8.95963 | best_loss=8.89696
Epoch 22/80: current_loss=8.95743 | best_loss=8.89696
Epoch 23/80: current_loss=9.05412 | best_loss=8.89696
Epoch 24/80: current_loss=9.76589 | best_loss=8.89696
Epoch 25/80: current_loss=9.12426 | best_loss=8.89696
Epoch 26/80: current_loss=9.16785 | best_loss=8.89696
Epoch 27/80: current_loss=9.00907 | best_loss=8.89696
Epoch 28/80: current_loss=9.73097 | best_loss=8.89696
Epoch 29/80: current_loss=9.07903 | best_loss=8.89696
Epoch 30/80: current_loss=9.19754 | best_loss=8.89696
Early Stopping at epoch 30
      explained_var=-0.00155 | mse_loss=8.37784
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.52933 | best_loss=8.52933
Epoch 1/80: current_loss=8.53019 | best_loss=8.52933
Epoch 2/80: current_loss=8.59229 | best_loss=8.52933
Epoch 3/80: current_loss=8.66711 | best_loss=8.52933
Epoch 4/80: current_loss=8.65882 | best_loss=8.52933
Epoch 5/80: current_loss=8.59410 | best_loss=8.52933
Epoch 6/80: current_loss=8.55059 | best_loss=8.52933
Epoch 7/80: current_loss=8.70879 | best_loss=8.52933
Epoch 8/80: current_loss=8.55160 | best_loss=8.52933
Epoch 9/80: current_loss=8.55317 | best_loss=8.52933
Epoch 10/80: current_loss=8.86410 | best_loss=8.52933
Epoch 11/80: current_loss=8.88451 | best_loss=8.52933
Epoch 12/80: current_loss=8.77911 | best_loss=8.52933
Epoch 13/80: current_loss=9.38381 | best_loss=8.52933
Epoch 14/80: current_loss=8.63836 | best_loss=8.52933
Epoch 15/80: current_loss=9.65358 | best_loss=8.52933
Epoch 16/80: current_loss=9.99791 | best_loss=8.52933
Epoch 17/80: current_loss=9.25406 | best_loss=8.52933
Epoch 18/80: current_loss=8.63019 | best_loss=8.52933
Epoch 19/80: current_loss=8.49070 | best_loss=8.49070
Epoch 20/80: current_loss=8.76652 | best_loss=8.49070
Epoch 21/80: current_loss=8.60716 | best_loss=8.49070
Epoch 22/80: current_loss=9.16856 | best_loss=8.49070
Epoch 23/80: current_loss=8.81609 | best_loss=8.49070
Epoch 24/80: current_loss=8.53703 | best_loss=8.49070
Epoch 25/80: current_loss=8.55891 | best_loss=8.49070
Epoch 26/80: current_loss=8.56892 | best_loss=8.49070
Epoch 27/80: current_loss=8.55688 | best_loss=8.49070
Epoch 28/80: current_loss=8.51963 | best_loss=8.49070
Epoch 29/80: current_loss=8.65824 | best_loss=8.49070
Epoch 30/80: current_loss=8.52475 | best_loss=8.49070
Epoch 31/80: current_loss=9.73051 | best_loss=8.49070
Epoch 32/80: current_loss=8.51324 | best_loss=8.49070
Epoch 33/80: current_loss=8.82570 | best_loss=8.49070
Epoch 34/80: current_loss=8.86321 | best_loss=8.49070
Epoch 35/80: current_loss=8.48571 | best_loss=8.48571
Epoch 36/80: current_loss=8.59647 | best_loss=8.48571
Epoch 37/80: current_loss=8.58112 | best_loss=8.48571
Epoch 38/80: current_loss=8.57794 | best_loss=8.48571
Epoch 39/80: current_loss=8.79946 | best_loss=8.48571
Epoch 40/80: current_loss=9.11598 | best_loss=8.48571
Epoch 41/80: current_loss=9.04574 | best_loss=8.48571
Epoch 42/80: current_loss=8.50372 | best_loss=8.48571
Epoch 43/80: current_loss=8.52860 | best_loss=8.48571
Epoch 44/80: current_loss=8.53123 | best_loss=8.48571
Epoch 45/80: current_loss=8.90886 | best_loss=8.48571
Epoch 46/80: current_loss=8.88350 | best_loss=8.48571
Epoch 47/80: current_loss=8.61342 | best_loss=8.48571
Epoch 48/80: current_loss=8.60227 | best_loss=8.48571
Epoch 49/80: current_loss=8.48808 | best_loss=8.48571
Epoch 50/80: current_loss=8.77194 | best_loss=8.48571
Epoch 51/80: current_loss=8.48229 | best_loss=8.48229
Epoch 52/80: current_loss=9.02028 | best_loss=8.48229
Epoch 53/80: current_loss=8.69222 | best_loss=8.48229
Epoch 54/80: current_loss=9.48174 | best_loss=8.48229
Epoch 55/80: current_loss=8.93145 | best_loss=8.48229
Epoch 56/80: current_loss=8.64231 | best_loss=8.48229
Epoch 57/80: current_loss=8.83393 | best_loss=8.48229
Epoch 58/80: current_loss=8.58073 | best_loss=8.48229
Epoch 59/80: current_loss=8.50025 | best_loss=8.48229
Epoch 60/80: current_loss=8.75731 | best_loss=8.48229
Epoch 61/80: current_loss=8.57505 | best_loss=8.48229
Epoch 62/80: current_loss=10.06171 | best_loss=8.48229
Epoch 63/80: current_loss=8.87835 | best_loss=8.48229
Epoch 64/80: current_loss=8.66307 | best_loss=8.48229
Epoch 65/80: current_loss=9.12300 | best_loss=8.48229
Epoch 66/80: current_loss=9.01705 | best_loss=8.48229
Epoch 67/80: current_loss=8.90272 | best_loss=8.48229
Epoch 68/80: current_loss=9.31510 | best_loss=8.48229
Epoch 69/80: current_loss=8.71954 | best_loss=8.48229
Epoch 70/80: current_loss=8.72389 | best_loss=8.48229
Epoch 71/80: current_loss=9.41174 | best_loss=8.48229
Early Stopping at epoch 71
      explained_var=0.00027 | mse_loss=8.19909
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=0.00023| avg_loss=8.82661
----------------------------------------------


----------------------------------------------
Params for Trial 20
{'learning_rate': 0.01, 'weight_decay': 0.0035585075505975682, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.05669 | best_loss=10.05669
Epoch 1/80: current_loss=10.14115 | best_loss=10.05669
Epoch 2/80: current_loss=10.61897 | best_loss=10.05669
Epoch 3/80: current_loss=10.22432 | best_loss=10.05669
Epoch 4/80: current_loss=10.02964 | best_loss=10.02964
Epoch 5/80: current_loss=9.94645 | best_loss=9.94645
Epoch 6/80: current_loss=10.45216 | best_loss=9.94645
Epoch 7/80: current_loss=10.01396 | best_loss=9.94645
Epoch 8/80: current_loss=10.36140 | best_loss=9.94645
Epoch 9/80: current_loss=10.52112 | best_loss=9.94645
Epoch 10/80: current_loss=10.01198 | best_loss=9.94645
Epoch 11/80: current_loss=10.28303 | best_loss=9.94645
Epoch 12/80: current_loss=11.37164 | best_loss=9.94645
Epoch 13/80: current_loss=10.07888 | best_loss=9.94645
Epoch 14/80: current_loss=10.23259 | best_loss=9.94645
Epoch 15/80: current_loss=11.40020 | best_loss=9.94645
Epoch 16/80: current_loss=9.90544 | best_loss=9.90544
Epoch 17/80: current_loss=10.01873 | best_loss=9.90544
Epoch 18/80: current_loss=10.04658 | best_loss=9.90544
Epoch 19/80: current_loss=10.11542 | best_loss=9.90544
Epoch 20/80: current_loss=9.94414 | best_loss=9.90544
Epoch 21/80: current_loss=10.00036 | best_loss=9.90544
Epoch 22/80: current_loss=9.93498 | best_loss=9.90544
Epoch 23/80: current_loss=9.97987 | best_loss=9.90544
Epoch 24/80: current_loss=10.73490 | best_loss=9.90544
Epoch 25/80: current_loss=9.89137 | best_loss=9.89137
Epoch 26/80: current_loss=10.09950 | best_loss=9.89137
Epoch 27/80: current_loss=9.95193 | best_loss=9.89137
Epoch 28/80: current_loss=10.65116 | best_loss=9.89137
Epoch 29/80: current_loss=10.07116 | best_loss=9.89137
Epoch 30/80: current_loss=9.95460 | best_loss=9.89137
Epoch 31/80: current_loss=10.10483 | best_loss=9.89137
Epoch 32/80: current_loss=9.95306 | best_loss=9.89137
Epoch 33/80: current_loss=12.18282 | best_loss=9.89137
Epoch 34/80: current_loss=10.38866 | best_loss=9.89137
Epoch 35/80: current_loss=9.90676 | best_loss=9.89137
Epoch 36/80: current_loss=10.34094 | best_loss=9.89137
Epoch 37/80: current_loss=10.32707 | best_loss=9.89137
Epoch 38/80: current_loss=9.90961 | best_loss=9.89137
Epoch 39/80: current_loss=9.94247 | best_loss=9.89137
Epoch 40/80: current_loss=10.15123 | best_loss=9.89137
Epoch 41/80: current_loss=11.25042 | best_loss=9.89137
Epoch 42/80: current_loss=10.02737 | best_loss=9.89137
Epoch 43/80: current_loss=9.92679 | best_loss=9.89137
Epoch 44/80: current_loss=9.90918 | best_loss=9.89137
Epoch 45/80: current_loss=9.97600 | best_loss=9.89137
Early Stopping at epoch 45
      explained_var=0.00199 | mse_loss=9.56197
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.62791 | best_loss=8.62791
Epoch 1/80: current_loss=8.62218 | best_loss=8.62218
Epoch 2/80: current_loss=8.56245 | best_loss=8.56245
Epoch 3/80: current_loss=8.61266 | best_loss=8.56245
Epoch 4/80: current_loss=8.50138 | best_loss=8.50138
Epoch 5/80: current_loss=8.85710 | best_loss=8.50138
Epoch 6/80: current_loss=9.14387 | best_loss=8.50138
Epoch 7/80: current_loss=8.54633 | best_loss=8.50138
Epoch 8/80: current_loss=8.48872 | best_loss=8.48872
Epoch 9/80: current_loss=8.53064 | best_loss=8.48872
Epoch 10/80: current_loss=8.54450 | best_loss=8.48872
Epoch 11/80: current_loss=8.63849 | best_loss=8.48872
Epoch 12/80: current_loss=8.67013 | best_loss=8.48872
Epoch 13/80: current_loss=8.52261 | best_loss=8.48872
Epoch 14/80: current_loss=8.64859 | best_loss=8.48872
Epoch 15/80: current_loss=8.71262 | best_loss=8.48872
Epoch 16/80: current_loss=8.58511 | best_loss=8.48872
Epoch 17/80: current_loss=8.66178 | best_loss=8.48872
Epoch 18/80: current_loss=8.51071 | best_loss=8.48872
Epoch 19/80: current_loss=8.62186 | best_loss=8.48872
Epoch 20/80: current_loss=8.47542 | best_loss=8.47542
Epoch 21/80: current_loss=9.04050 | best_loss=8.47542
Epoch 22/80: current_loss=8.48072 | best_loss=8.47542
Epoch 23/80: current_loss=8.86456 | best_loss=8.47542
Epoch 24/80: current_loss=8.59848 | best_loss=8.47542
Epoch 25/80: current_loss=8.69139 | best_loss=8.47542
Epoch 26/80: current_loss=8.80925 | best_loss=8.47542
Epoch 27/80: current_loss=8.64036 | best_loss=8.47542
Epoch 28/80: current_loss=8.51126 | best_loss=8.47542
Epoch 29/80: current_loss=8.51913 | best_loss=8.47542
Epoch 30/80: current_loss=8.48682 | best_loss=8.47542
Epoch 31/80: current_loss=8.60644 | best_loss=8.47542
Epoch 32/80: current_loss=8.70025 | best_loss=8.47542
Epoch 33/80: current_loss=8.93097 | best_loss=8.47542
Epoch 34/80: current_loss=8.70305 | best_loss=8.47542
Epoch 35/80: current_loss=8.53668 | best_loss=8.47542
Epoch 36/80: current_loss=8.55203 | best_loss=8.47542
Epoch 37/80: current_loss=8.64904 | best_loss=8.47542
Epoch 38/80: current_loss=8.71665 | best_loss=8.47542
Epoch 39/80: current_loss=8.52725 | best_loss=8.47542
Epoch 40/80: current_loss=8.52037 | best_loss=8.47542
Early Stopping at epoch 40
      explained_var=0.00222 | mse_loss=8.56518
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.04731 | best_loss=10.04731
Epoch 1/80: current_loss=9.34301 | best_loss=9.34301
Epoch 2/80: current_loss=10.38414 | best_loss=9.34301
Epoch 3/80: current_loss=9.59954 | best_loss=9.34301
Epoch 4/80: current_loss=9.30649 | best_loss=9.30649
Epoch 5/80: current_loss=9.29456 | best_loss=9.29456
Epoch 6/80: current_loss=10.13349 | best_loss=9.29456
Epoch 7/80: current_loss=9.65021 | best_loss=9.29456
Epoch 8/80: current_loss=9.31685 | best_loss=9.29456
Epoch 9/80: current_loss=10.14990 | best_loss=9.29456
Epoch 10/80: current_loss=9.39300 | best_loss=9.29456
Epoch 11/80: current_loss=9.46920 | best_loss=9.29456
Epoch 12/80: current_loss=9.25251 | best_loss=9.25251
Epoch 13/80: current_loss=9.35034 | best_loss=9.25251
Epoch 14/80: current_loss=10.16605 | best_loss=9.25251
Epoch 15/80: current_loss=9.74401 | best_loss=9.25251
Epoch 16/80: current_loss=10.73361 | best_loss=9.25251
Epoch 17/80: current_loss=12.24484 | best_loss=9.25251
Epoch 18/80: current_loss=10.35276 | best_loss=9.25251
Epoch 19/80: current_loss=9.39788 | best_loss=9.25251
Epoch 20/80: current_loss=9.28134 | best_loss=9.25251
Epoch 21/80: current_loss=9.84705 | best_loss=9.25251
Epoch 22/80: current_loss=9.29908 | best_loss=9.25251
Epoch 23/80: current_loss=9.72287 | best_loss=9.25251
Epoch 24/80: current_loss=9.79006 | best_loss=9.25251
Epoch 25/80: current_loss=10.14701 | best_loss=9.25251
Epoch 26/80: current_loss=9.27884 | best_loss=9.25251
Epoch 27/80: current_loss=9.31542 | best_loss=9.25251
Epoch 28/80: current_loss=9.25226 | best_loss=9.25226
Epoch 29/80: current_loss=9.50529 | best_loss=9.25226
Epoch 30/80: current_loss=9.29490 | best_loss=9.25226
Epoch 31/80: current_loss=9.32359 | best_loss=9.25226
Epoch 32/80: current_loss=9.43362 | best_loss=9.25226
Epoch 33/80: current_loss=9.34522 | best_loss=9.25226
Epoch 34/80: current_loss=9.58211 | best_loss=9.25226
Epoch 35/80: current_loss=9.29364 | best_loss=9.25226
Epoch 36/80: current_loss=9.28885 | best_loss=9.25226
Epoch 37/80: current_loss=9.87366 | best_loss=9.25226
Epoch 38/80: current_loss=9.42556 | best_loss=9.25226
Epoch 39/80: current_loss=9.42422 | best_loss=9.25226
Epoch 40/80: current_loss=9.65305 | best_loss=9.25226
Epoch 41/80: current_loss=10.01757 | best_loss=9.25226
Epoch 42/80: current_loss=9.40232 | best_loss=9.25226
Epoch 43/80: current_loss=9.43061 | best_loss=9.25226
Epoch 44/80: current_loss=9.32850 | best_loss=9.25226
Epoch 45/80: current_loss=9.47279 | best_loss=9.25226
Epoch 46/80: current_loss=9.71732 | best_loss=9.25226
Epoch 47/80: current_loss=9.32256 | best_loss=9.25226
Epoch 48/80: current_loss=9.63223 | best_loss=9.25226
Early Stopping at epoch 48
      explained_var=0.00096 | mse_loss=9.37330
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.24554 | best_loss=9.24554
Epoch 1/80: current_loss=8.96994 | best_loss=8.96994
Epoch 2/80: current_loss=9.02210 | best_loss=8.96994
Epoch 3/80: current_loss=9.00502 | best_loss=8.96994
Epoch 4/80: current_loss=8.99648 | best_loss=8.96994
Epoch 5/80: current_loss=8.99277 | best_loss=8.96994
Epoch 6/80: current_loss=9.29975 | best_loss=8.96994
Epoch 7/80: current_loss=8.99776 | best_loss=8.96994
Epoch 8/80: current_loss=9.37576 | best_loss=8.96994
Epoch 9/80: current_loss=9.57615 | best_loss=8.96994
Epoch 10/80: current_loss=8.92160 | best_loss=8.92160
Epoch 11/80: current_loss=9.26504 | best_loss=8.92160
Epoch 12/80: current_loss=9.53978 | best_loss=8.92160
Epoch 13/80: current_loss=9.97478 | best_loss=8.92160
Epoch 14/80: current_loss=9.23744 | best_loss=8.92160
Epoch 15/80: current_loss=9.13993 | best_loss=8.92160
Epoch 16/80: current_loss=9.04659 | best_loss=8.92160
Epoch 17/80: current_loss=9.06387 | best_loss=8.92160
Epoch 18/80: current_loss=9.05830 | best_loss=8.92160
Epoch 19/80: current_loss=9.57102 | best_loss=8.92160
Epoch 20/80: current_loss=9.21568 | best_loss=8.92160
Epoch 21/80: current_loss=9.32967 | best_loss=8.92160
Epoch 22/80: current_loss=9.06724 | best_loss=8.92160
Epoch 23/80: current_loss=9.04668 | best_loss=8.92160
Epoch 24/80: current_loss=8.94919 | best_loss=8.92160
Epoch 25/80: current_loss=9.46821 | best_loss=8.92160
Epoch 26/80: current_loss=9.59095 | best_loss=8.92160
Epoch 27/80: current_loss=9.19357 | best_loss=8.92160
Epoch 28/80: current_loss=9.04674 | best_loss=8.92160
Epoch 29/80: current_loss=9.24088 | best_loss=8.92160
Epoch 30/80: current_loss=9.30149 | best_loss=8.92160
Early Stopping at epoch 30
      explained_var=0.00399 | mse_loss=8.34190
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.81421 | best_loss=8.81421
Epoch 1/80: current_loss=8.71372 | best_loss=8.71372
Epoch 2/80: current_loss=8.97903 | best_loss=8.71372
Epoch 3/80: current_loss=8.54359 | best_loss=8.54359
Epoch 4/80: current_loss=10.02053 | best_loss=8.54359
Epoch 5/80: current_loss=8.92773 | best_loss=8.54359
Epoch 6/80: current_loss=8.69376 | best_loss=8.54359
Epoch 7/80: current_loss=8.71180 | best_loss=8.54359
Epoch 8/80: current_loss=8.72516 | best_loss=8.54359
Epoch 9/80: current_loss=8.81422 | best_loss=8.54359
Epoch 10/80: current_loss=8.75803 | best_loss=8.54359
Epoch 11/80: current_loss=8.99134 | best_loss=8.54359
Epoch 12/80: current_loss=8.64754 | best_loss=8.54359
Epoch 13/80: current_loss=8.72814 | best_loss=8.54359
Epoch 14/80: current_loss=8.55148 | best_loss=8.54359
Epoch 15/80: current_loss=8.88343 | best_loss=8.54359
Epoch 16/80: current_loss=8.61314 | best_loss=8.54359
Epoch 17/80: current_loss=8.52344 | best_loss=8.52344
Epoch 18/80: current_loss=8.60562 | best_loss=8.52344
Epoch 19/80: current_loss=8.56395 | best_loss=8.52344
Epoch 20/80: current_loss=8.49278 | best_loss=8.49278
Epoch 21/80: current_loss=8.60041 | best_loss=8.49278
Epoch 22/80: current_loss=8.45186 | best_loss=8.45186
Epoch 23/80: current_loss=8.63112 | best_loss=8.45186
Epoch 24/80: current_loss=8.49362 | best_loss=8.45186
Epoch 25/80: current_loss=8.74103 | best_loss=8.45186
Epoch 26/80: current_loss=8.47875 | best_loss=8.45186
Epoch 27/80: current_loss=9.04829 | best_loss=8.45186
Epoch 28/80: current_loss=9.32801 | best_loss=8.45186
Epoch 29/80: current_loss=9.07530 | best_loss=8.45186
Epoch 30/80: current_loss=8.80981 | best_loss=8.45186
Epoch 31/80: current_loss=8.58324 | best_loss=8.45186
Epoch 32/80: current_loss=8.90094 | best_loss=8.45186
Epoch 33/80: current_loss=8.47133 | best_loss=8.45186
Epoch 34/80: current_loss=9.13116 | best_loss=8.45186
Epoch 35/80: current_loss=8.57363 | best_loss=8.45186
Epoch 36/80: current_loss=8.79318 | best_loss=8.45186
Epoch 37/80: current_loss=9.09828 | best_loss=8.45186
Epoch 38/80: current_loss=8.57238 | best_loss=8.45186
Epoch 39/80: current_loss=8.91829 | best_loss=8.45186
Epoch 40/80: current_loss=8.80641 | best_loss=8.45186
Epoch 41/80: current_loss=9.10703 | best_loss=8.45186
Epoch 42/80: current_loss=8.75466 | best_loss=8.45186
Early Stopping at epoch 42
      explained_var=0.00107 | mse_loss=8.21396
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.00205| avg_loss=8.81126
----------------------------------------------


----------------------------------------------
Params for Trial 21
{'learning_rate': 0.01, 'weight_decay': 0.003497249185495035, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.92893 | best_loss=9.92893
Epoch 1/80: current_loss=10.59091 | best_loss=9.92893
Epoch 2/80: current_loss=9.91880 | best_loss=9.91880
Epoch 3/80: current_loss=10.34256 | best_loss=9.91880
Epoch 4/80: current_loss=10.52121 | best_loss=9.91880
Epoch 5/80: current_loss=9.89162 | best_loss=9.89162
Epoch 6/80: current_loss=9.90642 | best_loss=9.89162
Epoch 7/80: current_loss=9.90552 | best_loss=9.89162
Epoch 8/80: current_loss=9.91400 | best_loss=9.89162
Epoch 9/80: current_loss=9.99614 | best_loss=9.89162
Epoch 10/80: current_loss=9.90878 | best_loss=9.89162
Epoch 11/80: current_loss=10.03505 | best_loss=9.89162
Epoch 12/80: current_loss=10.44269 | best_loss=9.89162
Epoch 13/80: current_loss=10.09042 | best_loss=9.89162
Epoch 14/80: current_loss=9.96329 | best_loss=9.89162
Epoch 15/80: current_loss=10.62864 | best_loss=9.89162
Epoch 16/80: current_loss=10.06717 | best_loss=9.89162
Epoch 17/80: current_loss=10.07764 | best_loss=9.89162
Epoch 18/80: current_loss=10.24277 | best_loss=9.89162
Epoch 19/80: current_loss=10.39548 | best_loss=9.89162
Epoch 20/80: current_loss=10.07840 | best_loss=9.89162
Epoch 21/80: current_loss=9.93446 | best_loss=9.89162
Epoch 22/80: current_loss=10.87900 | best_loss=9.89162
Epoch 23/80: current_loss=10.35018 | best_loss=9.89162
Epoch 24/80: current_loss=9.91378 | best_loss=9.89162
Epoch 25/80: current_loss=10.10048 | best_loss=9.89162
Early Stopping at epoch 25
      explained_var=0.00198 | mse_loss=9.56167
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.74511 | best_loss=8.74511
Epoch 1/80: current_loss=8.62881 | best_loss=8.62881
Epoch 2/80: current_loss=8.72746 | best_loss=8.62881
Epoch 3/80: current_loss=9.03156 | best_loss=8.62881
Epoch 4/80: current_loss=8.52072 | best_loss=8.52072
Epoch 5/80: current_loss=8.53386 | best_loss=8.52072
Epoch 6/80: current_loss=8.49073 | best_loss=8.49073
Epoch 7/80: current_loss=8.52002 | best_loss=8.49073
Epoch 8/80: current_loss=8.50548 | best_loss=8.49073
Epoch 9/80: current_loss=8.78750 | best_loss=8.49073
Epoch 10/80: current_loss=8.62946 | best_loss=8.49073
Epoch 11/80: current_loss=8.56845 | best_loss=8.49073
Epoch 12/80: current_loss=9.21910 | best_loss=8.49073
Epoch 13/80: current_loss=9.22283 | best_loss=8.49073
Epoch 14/80: current_loss=8.99197 | best_loss=8.49073
Epoch 15/80: current_loss=8.76681 | best_loss=8.49073
Epoch 16/80: current_loss=8.62514 | best_loss=8.49073
Epoch 17/80: current_loss=8.62673 | best_loss=8.49073
Epoch 18/80: current_loss=8.57043 | best_loss=8.49073
Epoch 19/80: current_loss=8.51977 | best_loss=8.49073
Epoch 20/80: current_loss=8.58623 | best_loss=8.49073
Epoch 21/80: current_loss=10.17449 | best_loss=8.49073
Epoch 22/80: current_loss=8.49441 | best_loss=8.49073
Epoch 23/80: current_loss=8.50640 | best_loss=8.49073
Epoch 24/80: current_loss=9.75951 | best_loss=8.49073
Epoch 25/80: current_loss=8.70539 | best_loss=8.49073
Epoch 26/80: current_loss=8.45769 | best_loss=8.45769
Epoch 27/80: current_loss=8.95285 | best_loss=8.45769
Epoch 28/80: current_loss=8.48305 | best_loss=8.45769
Epoch 29/80: current_loss=8.54875 | best_loss=8.45769
Epoch 30/80: current_loss=9.45648 | best_loss=8.45769
Epoch 31/80: current_loss=8.51593 | best_loss=8.45769
Epoch 32/80: current_loss=10.07148 | best_loss=8.45769
Epoch 33/80: current_loss=8.61352 | best_loss=8.45769
Epoch 34/80: current_loss=8.51102 | best_loss=8.45769
Epoch 35/80: current_loss=8.69138 | best_loss=8.45769
Epoch 36/80: current_loss=8.47285 | best_loss=8.45769
Epoch 37/80: current_loss=8.45514 | best_loss=8.45514
Epoch 38/80: current_loss=8.60805 | best_loss=8.45514
Epoch 39/80: current_loss=8.67585 | best_loss=8.45514
Epoch 40/80: current_loss=8.71372 | best_loss=8.45514
Epoch 41/80: current_loss=8.54185 | best_loss=8.45514
Epoch 42/80: current_loss=8.49905 | best_loss=8.45514
Epoch 43/80: current_loss=8.75228 | best_loss=8.45514
Epoch 44/80: current_loss=8.55702 | best_loss=8.45514
Epoch 45/80: current_loss=8.69795 | best_loss=8.45514
Epoch 46/80: current_loss=9.11946 | best_loss=8.45514
Epoch 47/80: current_loss=8.80437 | best_loss=8.45514
Epoch 48/80: current_loss=8.52074 | best_loss=8.45514
Epoch 49/80: current_loss=8.65955 | best_loss=8.45514
Epoch 50/80: current_loss=8.51468 | best_loss=8.45514
Epoch 51/80: current_loss=8.70526 | best_loss=8.45514
Epoch 52/80: current_loss=8.48856 | best_loss=8.45514
Epoch 53/80: current_loss=8.81940 | best_loss=8.45514
Epoch 54/80: current_loss=8.96112 | best_loss=8.45514
Epoch 55/80: current_loss=8.56191 | best_loss=8.45514
Epoch 56/80: current_loss=8.59223 | best_loss=8.45514
Epoch 57/80: current_loss=8.48748 | best_loss=8.45514
Early Stopping at epoch 57
      explained_var=0.00278 | mse_loss=8.55455
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.47248 | best_loss=9.47248
Epoch 1/80: current_loss=9.81066 | best_loss=9.47248
Epoch 2/80: current_loss=9.60755 | best_loss=9.47248
Epoch 3/80: current_loss=9.39769 | best_loss=9.39769
Epoch 4/80: current_loss=9.27464 | best_loss=9.27464
Epoch 5/80: current_loss=10.05906 | best_loss=9.27464
Epoch 6/80: current_loss=9.55535 | best_loss=9.27464
Epoch 7/80: current_loss=9.26495 | best_loss=9.26495
Epoch 8/80: current_loss=9.60196 | best_loss=9.26495
Epoch 9/80: current_loss=9.66729 | best_loss=9.26495
Epoch 10/80: current_loss=9.39107 | best_loss=9.26495
Epoch 11/80: current_loss=10.11880 | best_loss=9.26495
Epoch 12/80: current_loss=9.97343 | best_loss=9.26495
Epoch 13/80: current_loss=9.32304 | best_loss=9.26495
Epoch 14/80: current_loss=9.35794 | best_loss=9.26495
Epoch 15/80: current_loss=9.51578 | best_loss=9.26495
Epoch 16/80: current_loss=9.25278 | best_loss=9.25278
Epoch 17/80: current_loss=9.52728 | best_loss=9.25278
Epoch 18/80: current_loss=9.38677 | best_loss=9.25278
Epoch 19/80: current_loss=9.31301 | best_loss=9.25278
Epoch 20/80: current_loss=9.25333 | best_loss=9.25278
Epoch 21/80: current_loss=9.46722 | best_loss=9.25278
Epoch 22/80: current_loss=10.00114 | best_loss=9.25278
Epoch 23/80: current_loss=9.46660 | best_loss=9.25278
Epoch 24/80: current_loss=9.92675 | best_loss=9.25278
Epoch 25/80: current_loss=9.75265 | best_loss=9.25278
Epoch 26/80: current_loss=9.51995 | best_loss=9.25278
Epoch 27/80: current_loss=9.29248 | best_loss=9.25278
Epoch 28/80: current_loss=9.31592 | best_loss=9.25278
Epoch 29/80: current_loss=9.35733 | best_loss=9.25278
Epoch 30/80: current_loss=9.52819 | best_loss=9.25278
Epoch 31/80: current_loss=9.41119 | best_loss=9.25278
Epoch 32/80: current_loss=10.07808 | best_loss=9.25278
Epoch 33/80: current_loss=9.52927 | best_loss=9.25278
Epoch 34/80: current_loss=9.44932 | best_loss=9.25278
Epoch 35/80: current_loss=9.28775 | best_loss=9.25278
Epoch 36/80: current_loss=9.34724 | best_loss=9.25278
Early Stopping at epoch 36
      explained_var=0.00191 | mse_loss=9.36598
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.92763 | best_loss=8.92763
Epoch 1/80: current_loss=9.03318 | best_loss=8.92763
Epoch 2/80: current_loss=8.93957 | best_loss=8.92763
Epoch 3/80: current_loss=9.19492 | best_loss=8.92763
Epoch 4/80: current_loss=9.09902 | best_loss=8.92763
Epoch 5/80: current_loss=9.23284 | best_loss=8.92763
Epoch 6/80: current_loss=9.11501 | best_loss=8.92763
Epoch 7/80: current_loss=8.99440 | best_loss=8.92763
Epoch 8/80: current_loss=11.92675 | best_loss=8.92763
Epoch 9/80: current_loss=9.34824 | best_loss=8.92763
Epoch 10/80: current_loss=9.24921 | best_loss=8.92763
Epoch 11/80: current_loss=8.97339 | best_loss=8.92763
Epoch 12/80: current_loss=9.02993 | best_loss=8.92763
Epoch 13/80: current_loss=9.79206 | best_loss=8.92763
Epoch 14/80: current_loss=8.95794 | best_loss=8.92763
Epoch 15/80: current_loss=8.91968 | best_loss=8.91968
Epoch 16/80: current_loss=8.96653 | best_loss=8.91968
Epoch 17/80: current_loss=9.01980 | best_loss=8.91968
Epoch 18/80: current_loss=9.34620 | best_loss=8.91968
Epoch 19/80: current_loss=9.07793 | best_loss=8.91968
Epoch 20/80: current_loss=9.01082 | best_loss=8.91968
Epoch 21/80: current_loss=9.50183 | best_loss=8.91968
Epoch 22/80: current_loss=9.30057 | best_loss=8.91968
Epoch 23/80: current_loss=8.96969 | best_loss=8.91968
Epoch 24/80: current_loss=8.98490 | best_loss=8.91968
Epoch 25/80: current_loss=8.97025 | best_loss=8.91968
Epoch 26/80: current_loss=9.05483 | best_loss=8.91968
Epoch 27/80: current_loss=8.95819 | best_loss=8.91968
Epoch 28/80: current_loss=8.94629 | best_loss=8.91968
Epoch 29/80: current_loss=9.01112 | best_loss=8.91968
Epoch 30/80: current_loss=8.97621 | best_loss=8.91968
Epoch 31/80: current_loss=9.09466 | best_loss=8.91968
Epoch 32/80: current_loss=8.98337 | best_loss=8.91968
Epoch 33/80: current_loss=9.70072 | best_loss=8.91968
Epoch 34/80: current_loss=9.96545 | best_loss=8.91968
Epoch 35/80: current_loss=9.23197 | best_loss=8.91968
Early Stopping at epoch 35
      explained_var=0.00112 | mse_loss=8.35468
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.54358 | best_loss=8.54358
Epoch 1/80: current_loss=8.54654 | best_loss=8.54358
Epoch 2/80: current_loss=8.58134 | best_loss=8.54358
Epoch 3/80: current_loss=8.49029 | best_loss=8.49029
Epoch 4/80: current_loss=9.06672 | best_loss=8.49029
Epoch 5/80: current_loss=8.65948 | best_loss=8.49029
Epoch 6/80: current_loss=8.92202 | best_loss=8.49029
Epoch 7/80: current_loss=8.54174 | best_loss=8.49029
Epoch 8/80: current_loss=9.29247 | best_loss=8.49029
Epoch 9/80: current_loss=8.84778 | best_loss=8.49029
Epoch 10/80: current_loss=8.71584 | best_loss=8.49029
Epoch 11/80: current_loss=8.86679 | best_loss=8.49029
Epoch 12/80: current_loss=8.54511 | best_loss=8.49029
Epoch 13/80: current_loss=8.58560 | best_loss=8.49029
Epoch 14/80: current_loss=8.79144 | best_loss=8.49029
Epoch 15/80: current_loss=8.73574 | best_loss=8.49029
Epoch 16/80: current_loss=8.52490 | best_loss=8.49029
Epoch 17/80: current_loss=8.57215 | best_loss=8.49029
Epoch 18/80: current_loss=8.58544 | best_loss=8.49029
Epoch 19/80: current_loss=8.65158 | best_loss=8.49029
Epoch 20/80: current_loss=8.56618 | best_loss=8.49029
Epoch 21/80: current_loss=8.75698 | best_loss=8.49029
Epoch 22/80: current_loss=8.39475 | best_loss=8.39475
Epoch 23/80: current_loss=8.57104 | best_loss=8.39475
Epoch 24/80: current_loss=9.07099 | best_loss=8.39475
Epoch 25/80: current_loss=8.51103 | best_loss=8.39475
Epoch 26/80: current_loss=8.77921 | best_loss=8.39475
Epoch 27/80: current_loss=8.44064 | best_loss=8.39475
Epoch 28/80: current_loss=9.00580 | best_loss=8.39475
Epoch 29/80: current_loss=9.04422 | best_loss=8.39475
Epoch 30/80: current_loss=8.78081 | best_loss=8.39475
Epoch 31/80: current_loss=8.75867 | best_loss=8.39475
Epoch 32/80: current_loss=8.52353 | best_loss=8.39475
Epoch 33/80: current_loss=9.42064 | best_loss=8.39475
Epoch 34/80: current_loss=8.54278 | best_loss=8.39475
Epoch 35/80: current_loss=8.52571 | best_loss=8.39475
Epoch 36/80: current_loss=8.82578 | best_loss=8.39475
Epoch 37/80: current_loss=8.59552 | best_loss=8.39475
Epoch 38/80: current_loss=8.71995 | best_loss=8.39475
Epoch 39/80: current_loss=8.69635 | best_loss=8.39475
Epoch 40/80: current_loss=9.61686 | best_loss=8.39475
Epoch 41/80: current_loss=8.49359 | best_loss=8.39475
Epoch 42/80: current_loss=8.48561 | best_loss=8.39475
Early Stopping at epoch 42
      explained_var=0.01414 | mse_loss=8.11982
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00439| avg_loss=8.79134
----------------------------------------------


----------------------------------------------
Params for Trial 22
{'learning_rate': 0.01, 'weight_decay': 0.0013780298752642903, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.90081 | best_loss=9.90081
Epoch 1/80: current_loss=10.28670 | best_loss=9.90081
Epoch 2/80: current_loss=10.64359 | best_loss=9.90081
Epoch 3/80: current_loss=10.18499 | best_loss=9.90081
Epoch 4/80: current_loss=10.23346 | best_loss=9.90081
Epoch 5/80: current_loss=9.94369 | best_loss=9.90081
Epoch 6/80: current_loss=10.05868 | best_loss=9.90081
Epoch 7/80: current_loss=10.04185 | best_loss=9.90081
Epoch 8/80: current_loss=9.90603 | best_loss=9.90081
Epoch 9/80: current_loss=10.29495 | best_loss=9.90081
Epoch 10/80: current_loss=10.15323 | best_loss=9.90081
Epoch 11/80: current_loss=9.93224 | best_loss=9.90081
Epoch 12/80: current_loss=9.88662 | best_loss=9.88662
Epoch 13/80: current_loss=10.13636 | best_loss=9.88662
Epoch 14/80: current_loss=9.95953 | best_loss=9.88662
Epoch 15/80: current_loss=10.14839 | best_loss=9.88662
Epoch 16/80: current_loss=9.89218 | best_loss=9.88662
Epoch 17/80: current_loss=10.26379 | best_loss=9.88662
Epoch 18/80: current_loss=10.74871 | best_loss=9.88662
Epoch 19/80: current_loss=9.90945 | best_loss=9.88662
Epoch 20/80: current_loss=9.92228 | best_loss=9.88662
Epoch 21/80: current_loss=9.93042 | best_loss=9.88662
Epoch 22/80: current_loss=10.25355 | best_loss=9.88662
Epoch 23/80: current_loss=9.91839 | best_loss=9.88662
Epoch 24/80: current_loss=9.93046 | best_loss=9.88662
Epoch 25/80: current_loss=9.90352 | best_loss=9.88662
Epoch 26/80: current_loss=9.91043 | best_loss=9.88662
Epoch 27/80: current_loss=10.23393 | best_loss=9.88662
Epoch 28/80: current_loss=10.17434 | best_loss=9.88662
Epoch 29/80: current_loss=9.90630 | best_loss=9.88662
Epoch 30/80: current_loss=9.96323 | best_loss=9.88662
Epoch 31/80: current_loss=9.92128 | best_loss=9.88662
Epoch 32/80: current_loss=9.98248 | best_loss=9.88662
Early Stopping at epoch 32
      explained_var=0.00266 | mse_loss=9.55317
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.52543 | best_loss=8.52543
Epoch 1/80: current_loss=8.60565 | best_loss=8.52543
Epoch 2/80: current_loss=8.75587 | best_loss=8.52543
Epoch 3/80: current_loss=8.70915 | best_loss=8.52543
Epoch 4/80: current_loss=8.89280 | best_loss=8.52543
Epoch 5/80: current_loss=8.51947 | best_loss=8.51947
Epoch 6/80: current_loss=8.52453 | best_loss=8.51947
Epoch 7/80: current_loss=8.54594 | best_loss=8.51947
Epoch 8/80: current_loss=8.62505 | best_loss=8.51947
Epoch 9/80: current_loss=8.52938 | best_loss=8.51947
Epoch 10/80: current_loss=8.55886 | best_loss=8.51947
Epoch 11/80: current_loss=8.56070 | best_loss=8.51947
Epoch 12/80: current_loss=8.50233 | best_loss=8.50233
Epoch 13/80: current_loss=9.87911 | best_loss=8.50233
Epoch 14/80: current_loss=8.96613 | best_loss=8.50233
Epoch 15/80: current_loss=8.60148 | best_loss=8.50233
Epoch 16/80: current_loss=8.72902 | best_loss=8.50233
Epoch 17/80: current_loss=8.64809 | best_loss=8.50233
Epoch 18/80: current_loss=8.86847 | best_loss=8.50233
Epoch 19/80: current_loss=8.55399 | best_loss=8.50233
Epoch 20/80: current_loss=9.09800 | best_loss=8.50233
Epoch 21/80: current_loss=8.51248 | best_loss=8.50233
Epoch 22/80: current_loss=8.59851 | best_loss=8.50233
Epoch 23/80: current_loss=8.97775 | best_loss=8.50233
Epoch 24/80: current_loss=8.80334 | best_loss=8.50233
Epoch 25/80: current_loss=8.69396 | best_loss=8.50233
Epoch 26/80: current_loss=9.15474 | best_loss=8.50233
Epoch 27/80: current_loss=8.83746 | best_loss=8.50233
Epoch 28/80: current_loss=8.74798 | best_loss=8.50233
Epoch 29/80: current_loss=8.85418 | best_loss=8.50233
Epoch 30/80: current_loss=8.53450 | best_loss=8.50233
Epoch 31/80: current_loss=8.45691 | best_loss=8.45691
Epoch 32/80: current_loss=8.82690 | best_loss=8.45691
Epoch 33/80: current_loss=8.62862 | best_loss=8.45691
Epoch 34/80: current_loss=9.09834 | best_loss=8.45691
Epoch 35/80: current_loss=8.67506 | best_loss=8.45691
Epoch 36/80: current_loss=8.66319 | best_loss=8.45691
Epoch 37/80: current_loss=9.90862 | best_loss=8.45691
Epoch 38/80: current_loss=8.49458 | best_loss=8.45691
Epoch 39/80: current_loss=8.67480 | best_loss=8.45691
Epoch 40/80: current_loss=8.34805 | best_loss=8.34805
Epoch 41/80: current_loss=8.58627 | best_loss=8.34805
Epoch 42/80: current_loss=8.51128 | best_loss=8.34805
Epoch 43/80: current_loss=8.47631 | best_loss=8.34805
Epoch 44/80: current_loss=8.56556 | best_loss=8.34805
Epoch 45/80: current_loss=8.47453 | best_loss=8.34805
Epoch 46/80: current_loss=8.49473 | best_loss=8.34805
Epoch 47/80: current_loss=8.65844 | best_loss=8.34805
Epoch 48/80: current_loss=8.73984 | best_loss=8.34805
Epoch 49/80: current_loss=8.49049 | best_loss=8.34805
Epoch 50/80: current_loss=8.60170 | best_loss=8.34805
Epoch 51/80: current_loss=8.66233 | best_loss=8.34805
Epoch 52/80: current_loss=9.02280 | best_loss=8.34805
Epoch 53/80: current_loss=8.58238 | best_loss=8.34805
Epoch 54/80: current_loss=8.51332 | best_loss=8.34805
Epoch 55/80: current_loss=9.37421 | best_loss=8.34805
Epoch 56/80: current_loss=8.48061 | best_loss=8.34805
Epoch 57/80: current_loss=8.49322 | best_loss=8.34805
Epoch 58/80: current_loss=8.51378 | best_loss=8.34805
Epoch 59/80: current_loss=8.96471 | best_loss=8.34805
Epoch 60/80: current_loss=9.14569 | best_loss=8.34805
Early Stopping at epoch 60
      explained_var=0.00579 | mse_loss=8.52698
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.48970 | best_loss=9.48970
Epoch 1/80: current_loss=9.32745 | best_loss=9.32745
Epoch 2/80: current_loss=10.33644 | best_loss=9.32745
Epoch 3/80: current_loss=9.35458 | best_loss=9.32745
Epoch 4/80: current_loss=9.43339 | best_loss=9.32745
Epoch 5/80: current_loss=9.57231 | best_loss=9.32745
Epoch 6/80: current_loss=9.48447 | best_loss=9.32745
Epoch 7/80: current_loss=10.13767 | best_loss=9.32745
Epoch 8/80: current_loss=9.79745 | best_loss=9.32745
Epoch 9/80: current_loss=9.64796 | best_loss=9.32745
Epoch 10/80: current_loss=9.89387 | best_loss=9.32745
Epoch 11/80: current_loss=9.56971 | best_loss=9.32745
Epoch 12/80: current_loss=9.21660 | best_loss=9.21660
Epoch 13/80: current_loss=9.64291 | best_loss=9.21660
Epoch 14/80: current_loss=9.53711 | best_loss=9.21660
Epoch 15/80: current_loss=9.36869 | best_loss=9.21660
Epoch 16/80: current_loss=12.03735 | best_loss=9.21660
Epoch 17/80: current_loss=9.35128 | best_loss=9.21660
Epoch 18/80: current_loss=9.36567 | best_loss=9.21660
Epoch 19/80: current_loss=9.35281 | best_loss=9.21660
Epoch 20/80: current_loss=9.35366 | best_loss=9.21660
Epoch 21/80: current_loss=9.28991 | best_loss=9.21660
Epoch 22/80: current_loss=9.47696 | best_loss=9.21660
Epoch 23/80: current_loss=9.33149 | best_loss=9.21660
Epoch 24/80: current_loss=9.46355 | best_loss=9.21660
Epoch 25/80: current_loss=9.65986 | best_loss=9.21660
Epoch 26/80: current_loss=9.56056 | best_loss=9.21660
Epoch 27/80: current_loss=9.62284 | best_loss=9.21660
Epoch 28/80: current_loss=9.61400 | best_loss=9.21660
Epoch 29/80: current_loss=9.69068 | best_loss=9.21660
Epoch 30/80: current_loss=9.91161 | best_loss=9.21660
Epoch 31/80: current_loss=9.54830 | best_loss=9.21660
Epoch 32/80: current_loss=9.49130 | best_loss=9.21660
Early Stopping at epoch 32
      explained_var=0.00757 | mse_loss=9.30817
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.99615 | best_loss=8.99615
Epoch 1/80: current_loss=8.93878 | best_loss=8.93878
Epoch 2/80: current_loss=9.73369 | best_loss=8.93878
Epoch 3/80: current_loss=8.95499 | best_loss=8.93878
Epoch 4/80: current_loss=9.20449 | best_loss=8.93878
Epoch 5/80: current_loss=9.22103 | best_loss=8.93878
Epoch 6/80: current_loss=9.23660 | best_loss=8.93878
Epoch 7/80: current_loss=9.10484 | best_loss=8.93878
Epoch 8/80: current_loss=8.89744 | best_loss=8.89744
Epoch 9/80: current_loss=9.20303 | best_loss=8.89744
Epoch 10/80: current_loss=9.41529 | best_loss=8.89744
Epoch 11/80: current_loss=8.87371 | best_loss=8.87371
Epoch 12/80: current_loss=9.34611 | best_loss=8.87371
Epoch 13/80: current_loss=9.22391 | best_loss=8.87371
Epoch 14/80: current_loss=9.41672 | best_loss=8.87371
Epoch 15/80: current_loss=9.16394 | best_loss=8.87371
Epoch 16/80: current_loss=8.99331 | best_loss=8.87371
Epoch 17/80: current_loss=9.06274 | best_loss=8.87371
Epoch 18/80: current_loss=9.12934 | best_loss=8.87371
Epoch 19/80: current_loss=9.24082 | best_loss=8.87371
Epoch 20/80: current_loss=9.80678 | best_loss=8.87371
Epoch 21/80: current_loss=9.17711 | best_loss=8.87371
Epoch 22/80: current_loss=9.55611 | best_loss=8.87371
Epoch 23/80: current_loss=9.10859 | best_loss=8.87371
Epoch 24/80: current_loss=9.03855 | best_loss=8.87371
Epoch 25/80: current_loss=8.89750 | best_loss=8.87371
Epoch 26/80: current_loss=8.96986 | best_loss=8.87371
Epoch 27/80: current_loss=9.02241 | best_loss=8.87371
Epoch 28/80: current_loss=9.14571 | best_loss=8.87371
Epoch 29/80: current_loss=8.86973 | best_loss=8.86973
Epoch 30/80: current_loss=8.91365 | best_loss=8.86973
Epoch 31/80: current_loss=9.08977 | best_loss=8.86973
Epoch 32/80: current_loss=9.01430 | best_loss=8.86973
Epoch 33/80: current_loss=9.91745 | best_loss=8.86973
Epoch 34/80: current_loss=9.02351 | best_loss=8.86973
Epoch 35/80: current_loss=8.94722 | best_loss=8.86973
Epoch 36/80: current_loss=9.60179 | best_loss=8.86973
Epoch 37/80: current_loss=9.13377 | best_loss=8.86973
Epoch 38/80: current_loss=9.08582 | best_loss=8.86973
Epoch 39/80: current_loss=9.12554 | best_loss=8.86973
Epoch 40/80: current_loss=9.71144 | best_loss=8.86973
Epoch 41/80: current_loss=8.98715 | best_loss=8.86973
Epoch 42/80: current_loss=8.98361 | best_loss=8.86973
Epoch 43/80: current_loss=8.96111 | best_loss=8.86973
Epoch 44/80: current_loss=9.05046 | best_loss=8.86973
Epoch 45/80: current_loss=9.35235 | best_loss=8.86973
Epoch 46/80: current_loss=9.29246 | best_loss=8.86973
Epoch 47/80: current_loss=9.08474 | best_loss=8.86973
Epoch 48/80: current_loss=8.99153 | best_loss=8.86973
Epoch 49/80: current_loss=8.99911 | best_loss=8.86973
Early Stopping at epoch 49
      explained_var=0.00358 | mse_loss=8.33734
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.71569 | best_loss=8.71569
Epoch 1/80: current_loss=8.53956 | best_loss=8.53956
Epoch 2/80: current_loss=9.83364 | best_loss=8.53956
Epoch 3/80: current_loss=8.61457 | best_loss=8.53956
Epoch 4/80: current_loss=9.09136 | best_loss=8.53956
Epoch 5/80: current_loss=8.52701 | best_loss=8.52701
Epoch 6/80: current_loss=8.78176 | best_loss=8.52701
Epoch 7/80: current_loss=8.74596 | best_loss=8.52701
Epoch 8/80: current_loss=9.00117 | best_loss=8.52701
Epoch 9/80: current_loss=8.52764 | best_loss=8.52701
Epoch 10/80: current_loss=8.66278 | best_loss=8.52701
Epoch 11/80: current_loss=8.54412 | best_loss=8.52701
Epoch 12/80: current_loss=8.58023 | best_loss=8.52701
Epoch 13/80: current_loss=8.74001 | best_loss=8.52701
Epoch 14/80: current_loss=8.82147 | best_loss=8.52701
Epoch 15/80: current_loss=8.50904 | best_loss=8.50904
Epoch 16/80: current_loss=8.58223 | best_loss=8.50904
Epoch 17/80: current_loss=9.41474 | best_loss=8.50904
Epoch 18/80: current_loss=8.51178 | best_loss=8.50904
Epoch 19/80: current_loss=8.62676 | best_loss=8.50904
Epoch 20/80: current_loss=10.76876 | best_loss=8.50904
Epoch 21/80: current_loss=9.23836 | best_loss=8.50904
Epoch 22/80: current_loss=8.57894 | best_loss=8.50904
Epoch 23/80: current_loss=8.65542 | best_loss=8.50904
Epoch 24/80: current_loss=8.51598 | best_loss=8.50904
Epoch 25/80: current_loss=8.48602 | best_loss=8.48602
Epoch 26/80: current_loss=8.51219 | best_loss=8.48602
Epoch 27/80: current_loss=8.54280 | best_loss=8.48602
Epoch 28/80: current_loss=8.55179 | best_loss=8.48602
Epoch 29/80: current_loss=9.27723 | best_loss=8.48602
Epoch 30/80: current_loss=8.83182 | best_loss=8.48602
Epoch 31/80: current_loss=8.99767 | best_loss=8.48602
Epoch 32/80: current_loss=8.74324 | best_loss=8.48602
Epoch 33/80: current_loss=8.56247 | best_loss=8.48602
Epoch 34/80: current_loss=8.53096 | best_loss=8.48602
Epoch 35/80: current_loss=8.49361 | best_loss=8.48602
Epoch 36/80: current_loss=8.67115 | best_loss=8.48602
Epoch 37/80: current_loss=8.57284 | best_loss=8.48602
Epoch 38/80: current_loss=8.82091 | best_loss=8.48602
Epoch 39/80: current_loss=9.58744 | best_loss=8.48602
Epoch 40/80: current_loss=8.51376 | best_loss=8.48602
Epoch 41/80: current_loss=8.61368 | best_loss=8.48602
Epoch 42/80: current_loss=8.92600 | best_loss=8.48602
Epoch 43/80: current_loss=8.51169 | best_loss=8.48602
Epoch 44/80: current_loss=8.66097 | best_loss=8.48602
Epoch 45/80: current_loss=9.04433 | best_loss=8.48602
Early Stopping at epoch 45
      explained_var=0.00043 | mse_loss=8.19701
----------------------------------------------
Average early_stopping_point: 23| avg_exp_var=0.00401| avg_loss=8.78453
----------------------------------------------


----------------------------------------------
Params for Trial 23
{'learning_rate': 0.01, 'weight_decay': 0.004113527257951045, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.01928 | best_loss=10.01928
Epoch 1/80: current_loss=9.91164 | best_loss=9.91164
Epoch 2/80: current_loss=11.12888 | best_loss=9.91164
Epoch 3/80: current_loss=9.92164 | best_loss=9.91164
Epoch 4/80: current_loss=9.90484 | best_loss=9.90484
Epoch 5/80: current_loss=10.14791 | best_loss=9.90484
Epoch 6/80: current_loss=9.90201 | best_loss=9.90201
Epoch 7/80: current_loss=10.06621 | best_loss=9.90201
Epoch 8/80: current_loss=10.01138 | best_loss=9.90201
Epoch 9/80: current_loss=10.10226 | best_loss=9.90201
Epoch 10/80: current_loss=9.91361 | best_loss=9.90201
Epoch 11/80: current_loss=9.93404 | best_loss=9.90201
Epoch 12/80: current_loss=10.40311 | best_loss=9.90201
Epoch 13/80: current_loss=10.28214 | best_loss=9.90201
Epoch 14/80: current_loss=10.48059 | best_loss=9.90201
Epoch 15/80: current_loss=9.89677 | best_loss=9.89677
Epoch 16/80: current_loss=10.46769 | best_loss=9.89677
Epoch 17/80: current_loss=9.89778 | best_loss=9.89677
Epoch 18/80: current_loss=9.94303 | best_loss=9.89677
Epoch 19/80: current_loss=9.95523 | best_loss=9.89677
Epoch 20/80: current_loss=9.93846 | best_loss=9.89677
Epoch 21/80: current_loss=10.21706 | best_loss=9.89677
Epoch 22/80: current_loss=9.95217 | best_loss=9.89677
Epoch 23/80: current_loss=9.91682 | best_loss=9.89677
Epoch 24/80: current_loss=9.89796 | best_loss=9.89677
Epoch 25/80: current_loss=9.91621 | best_loss=9.89677
Epoch 26/80: current_loss=10.37282 | best_loss=9.89677
Epoch 27/80: current_loss=9.88888 | best_loss=9.88888
Epoch 28/80: current_loss=9.89777 | best_loss=9.88888
Epoch 29/80: current_loss=10.34585 | best_loss=9.88888
Epoch 30/80: current_loss=10.18845 | best_loss=9.88888
Epoch 31/80: current_loss=10.02233 | best_loss=9.88888
Epoch 32/80: current_loss=10.14484 | best_loss=9.88888
Epoch 33/80: current_loss=10.47760 | best_loss=9.88888
Epoch 34/80: current_loss=9.95424 | best_loss=9.88888
Epoch 35/80: current_loss=10.10431 | best_loss=9.88888
Epoch 36/80: current_loss=10.10765 | best_loss=9.88888
Epoch 37/80: current_loss=9.91101 | best_loss=9.88888
Epoch 38/80: current_loss=9.96601 | best_loss=9.88888
Epoch 39/80: current_loss=9.95159 | best_loss=9.88888
Epoch 40/80: current_loss=9.92394 | best_loss=9.88888
Epoch 41/80: current_loss=9.95180 | best_loss=9.88888
Epoch 42/80: current_loss=10.05191 | best_loss=9.88888
Epoch 43/80: current_loss=10.03998 | best_loss=9.88888
Epoch 44/80: current_loss=9.97880 | best_loss=9.88888
Epoch 45/80: current_loss=9.97694 | best_loss=9.88888
Epoch 46/80: current_loss=9.93884 | best_loss=9.88888
Epoch 47/80: current_loss=9.90064 | best_loss=9.88888
Early Stopping at epoch 47
      explained_var=0.00239 | mse_loss=9.55975
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.49530 | best_loss=8.49530
Epoch 1/80: current_loss=8.51384 | best_loss=8.49530
Epoch 2/80: current_loss=8.97415 | best_loss=8.49530
Epoch 3/80: current_loss=8.51409 | best_loss=8.49530
Epoch 4/80: current_loss=8.72120 | best_loss=8.49530
Epoch 5/80: current_loss=8.48225 | best_loss=8.48225
Epoch 6/80: current_loss=8.46000 | best_loss=8.46000
Epoch 7/80: current_loss=8.87694 | best_loss=8.46000
Epoch 8/80: current_loss=8.50170 | best_loss=8.46000
Epoch 9/80: current_loss=8.55984 | best_loss=8.46000
Epoch 10/80: current_loss=8.68798 | best_loss=8.46000
Epoch 11/80: current_loss=8.50392 | best_loss=8.46000
Epoch 12/80: current_loss=8.66221 | best_loss=8.46000
Epoch 13/80: current_loss=9.66419 | best_loss=8.46000
Epoch 14/80: current_loss=8.57678 | best_loss=8.46000
Epoch 15/80: current_loss=8.58133 | best_loss=8.46000
Epoch 16/80: current_loss=8.54053 | best_loss=8.46000
Epoch 17/80: current_loss=8.70215 | best_loss=8.46000
Epoch 18/80: current_loss=8.51212 | best_loss=8.46000
Epoch 19/80: current_loss=8.82735 | best_loss=8.46000
Epoch 20/80: current_loss=8.51544 | best_loss=8.46000
Epoch 21/80: current_loss=8.51659 | best_loss=8.46000
Epoch 22/80: current_loss=8.89943 | best_loss=8.46000
Epoch 23/80: current_loss=8.47373 | best_loss=8.46000
Epoch 24/80: current_loss=9.06804 | best_loss=8.46000
Epoch 25/80: current_loss=8.57270 | best_loss=8.46000
Epoch 26/80: current_loss=8.97660 | best_loss=8.46000
Early Stopping at epoch 26
      explained_var=0.00170 | mse_loss=8.56736
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.69566 | best_loss=9.69566
Epoch 1/80: current_loss=9.38046 | best_loss=9.38046
Epoch 2/80: current_loss=9.98125 | best_loss=9.38046
Epoch 3/80: current_loss=9.39138 | best_loss=9.38046
Epoch 4/80: current_loss=9.28867 | best_loss=9.28867
Epoch 5/80: current_loss=9.37076 | best_loss=9.28867
Epoch 6/80: current_loss=9.24229 | best_loss=9.24229
Epoch 7/80: current_loss=9.44621 | best_loss=9.24229
Epoch 8/80: current_loss=9.29963 | best_loss=9.24229
Epoch 9/80: current_loss=10.72169 | best_loss=9.24229
Epoch 10/80: current_loss=9.30533 | best_loss=9.24229
Epoch 11/80: current_loss=10.49129 | best_loss=9.24229
Epoch 12/80: current_loss=9.48247 | best_loss=9.24229
Epoch 13/80: current_loss=10.95913 | best_loss=9.24229
Epoch 14/80: current_loss=9.79267 | best_loss=9.24229
Epoch 15/80: current_loss=9.55727 | best_loss=9.24229
Epoch 16/80: current_loss=9.45664 | best_loss=9.24229
Epoch 17/80: current_loss=9.59545 | best_loss=9.24229
Epoch 18/80: current_loss=9.32706 | best_loss=9.24229
Epoch 19/80: current_loss=9.61164 | best_loss=9.24229
Epoch 20/80: current_loss=9.46823 | best_loss=9.24229
Epoch 21/80: current_loss=9.27345 | best_loss=9.24229
Epoch 22/80: current_loss=9.56240 | best_loss=9.24229
Epoch 23/80: current_loss=10.20525 | best_loss=9.24229
Epoch 24/80: current_loss=9.60559 | best_loss=9.24229
Epoch 25/80: current_loss=10.16152 | best_loss=9.24229
Epoch 26/80: current_loss=9.28737 | best_loss=9.24229
Early Stopping at epoch 26
      explained_var=0.00398 | mse_loss=9.35032
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.40324 | best_loss=9.40324
Epoch 1/80: current_loss=9.00384 | best_loss=9.00384
Epoch 2/80: current_loss=9.05686 | best_loss=9.00384
Epoch 3/80: current_loss=9.18065 | best_loss=9.00384
Epoch 4/80: current_loss=8.93789 | best_loss=8.93789
Epoch 5/80: current_loss=8.99574 | best_loss=8.93789
Epoch 6/80: current_loss=9.28475 | best_loss=8.93789
Epoch 7/80: current_loss=9.19738 | best_loss=8.93789
Epoch 8/80: current_loss=10.62466 | best_loss=8.93789
Epoch 9/80: current_loss=9.91310 | best_loss=8.93789
Epoch 10/80: current_loss=9.07655 | best_loss=8.93789
Epoch 11/80: current_loss=9.05196 | best_loss=8.93789
Epoch 12/80: current_loss=9.01197 | best_loss=8.93789
Epoch 13/80: current_loss=8.99358 | best_loss=8.93789
Epoch 14/80: current_loss=9.05950 | best_loss=8.93789
Epoch 15/80: current_loss=9.92028 | best_loss=8.93789
Epoch 16/80: current_loss=9.12338 | best_loss=8.93789
Epoch 17/80: current_loss=9.01387 | best_loss=8.93789
Epoch 18/80: current_loss=9.47131 | best_loss=8.93789
Epoch 19/80: current_loss=9.00146 | best_loss=8.93789
Epoch 20/80: current_loss=9.49619 | best_loss=8.93789
Epoch 21/80: current_loss=9.02054 | best_loss=8.93789
Epoch 22/80: current_loss=9.05953 | best_loss=8.93789
Epoch 23/80: current_loss=9.21979 | best_loss=8.93789
Epoch 24/80: current_loss=8.97240 | best_loss=8.93789
Early Stopping at epoch 24
      explained_var=-0.00056 | mse_loss=8.39328

----------------------------------------------
Params for Trial 24
{'learning_rate': 0.01, 'weight_decay': 0.0015132885551484472, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.95163 | best_loss=9.95163
Epoch 1/80: current_loss=9.96601 | best_loss=9.95163
Epoch 2/80: current_loss=10.58792 | best_loss=9.95163
Epoch 3/80: current_loss=10.16290 | best_loss=9.95163
Epoch 4/80: current_loss=10.03575 | best_loss=9.95163
Epoch 5/80: current_loss=9.97510 | best_loss=9.95163
Epoch 6/80: current_loss=9.90587 | best_loss=9.90587
Epoch 7/80: current_loss=10.34135 | best_loss=9.90587
Epoch 8/80: current_loss=9.96529 | best_loss=9.90587
Epoch 9/80: current_loss=9.92548 | best_loss=9.90587
Epoch 10/80: current_loss=9.95495 | best_loss=9.90587
Epoch 11/80: current_loss=9.90994 | best_loss=9.90587
Epoch 12/80: current_loss=10.50042 | best_loss=9.90587
Epoch 13/80: current_loss=10.34401 | best_loss=9.90587
Epoch 14/80: current_loss=9.91756 | best_loss=9.90587
Epoch 15/80: current_loss=11.25453 | best_loss=9.90587
Epoch 16/80: current_loss=10.00521 | best_loss=9.90587
Epoch 17/80: current_loss=9.98905 | best_loss=9.90587
Epoch 18/80: current_loss=10.06314 | best_loss=9.90587
Epoch 19/80: current_loss=9.89799 | best_loss=9.89799
Epoch 20/80: current_loss=9.90032 | best_loss=9.89799
Epoch 21/80: current_loss=10.66134 | best_loss=9.89799
Epoch 22/80: current_loss=9.96296 | best_loss=9.89799
Epoch 23/80: current_loss=9.94526 | best_loss=9.89799
Epoch 24/80: current_loss=9.92825 | best_loss=9.89799
Epoch 25/80: current_loss=9.89822 | best_loss=9.89799
Epoch 26/80: current_loss=9.92022 | best_loss=9.89799
Epoch 27/80: current_loss=9.93696 | best_loss=9.89799
Epoch 28/80: current_loss=9.94906 | best_loss=9.89799
Epoch 29/80: current_loss=9.98606 | best_loss=9.89799
Epoch 30/80: current_loss=10.17006 | best_loss=9.89799
Epoch 31/80: current_loss=10.62491 | best_loss=9.89799
Epoch 32/80: current_loss=9.93772 | best_loss=9.89799
Epoch 33/80: current_loss=9.95616 | best_loss=9.89799
Epoch 34/80: current_loss=9.92725 | best_loss=9.89799
Epoch 35/80: current_loss=9.91942 | best_loss=9.89799
Epoch 36/80: current_loss=9.99613 | best_loss=9.89799
Epoch 37/80: current_loss=10.21951 | best_loss=9.89799
Epoch 38/80: current_loss=10.00097 | best_loss=9.89799
Epoch 39/80: current_loss=10.01284 | best_loss=9.89799
Early Stopping at epoch 39
      explained_var=0.00114 | mse_loss=9.56614
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.68322 | best_loss=8.68322
Epoch 1/80: current_loss=8.89903 | best_loss=8.68322
Epoch 2/80: current_loss=8.58604 | best_loss=8.58604
Epoch 3/80: current_loss=9.09305 | best_loss=8.58604
Epoch 4/80: current_loss=8.55252 | best_loss=8.55252
Epoch 5/80: current_loss=8.50450 | best_loss=8.50450
Epoch 6/80: current_loss=8.68538 | best_loss=8.50450
Epoch 7/80: current_loss=8.57272 | best_loss=8.50450
Epoch 8/80: current_loss=8.73948 | best_loss=8.50450
Epoch 9/80: current_loss=8.54252 | best_loss=8.50450
Epoch 10/80: current_loss=8.55554 | best_loss=8.50450
Epoch 11/80: current_loss=8.61868 | best_loss=8.50450
Epoch 12/80: current_loss=8.75437 | best_loss=8.50450
Epoch 13/80: current_loss=8.82131 | best_loss=8.50450
Epoch 14/80: current_loss=8.61535 | best_loss=8.50450
Epoch 15/80: current_loss=8.94083 | best_loss=8.50450
Epoch 16/80: current_loss=8.60184 | best_loss=8.50450
Epoch 17/80: current_loss=8.56266 | best_loss=8.50450
Epoch 18/80: current_loss=8.52890 | best_loss=8.50450
Epoch 19/80: current_loss=8.59969 | best_loss=8.50450
Epoch 20/80: current_loss=8.49040 | best_loss=8.49040
Epoch 21/80: current_loss=8.48945 | best_loss=8.48945
Epoch 22/80: current_loss=8.72106 | best_loss=8.48945
Epoch 23/80: current_loss=8.82946 | best_loss=8.48945
Epoch 24/80: current_loss=13.01623 | best_loss=8.48945
Epoch 25/80: current_loss=8.93449 | best_loss=8.48945
Epoch 26/80: current_loss=8.61514 | best_loss=8.48945
Epoch 27/80: current_loss=8.89675 | best_loss=8.48945
Epoch 28/80: current_loss=8.51405 | best_loss=8.48945
Epoch 29/80: current_loss=8.54478 | best_loss=8.48945
Epoch 30/80: current_loss=8.76685 | best_loss=8.48945
Epoch 31/80: current_loss=8.51822 | best_loss=8.48945
Epoch 32/80: current_loss=8.65629 | best_loss=8.48945
Epoch 33/80: current_loss=8.70816 | best_loss=8.48945
Epoch 34/80: current_loss=8.56016 | best_loss=8.48945
Epoch 35/80: current_loss=8.96417 | best_loss=8.48945
Epoch 36/80: current_loss=8.68581 | best_loss=8.48945
Epoch 37/80: current_loss=8.94894 | best_loss=8.48945
Epoch 38/80: current_loss=9.46088 | best_loss=8.48945
Epoch 39/80: current_loss=8.49949 | best_loss=8.48945
Epoch 40/80: current_loss=9.17309 | best_loss=8.48945
Epoch 41/80: current_loss=8.54901 | best_loss=8.48945
Early Stopping at epoch 41
      explained_var=0.00169 | mse_loss=8.56232
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.25549 | best_loss=9.25549
Epoch 1/80: current_loss=10.04751 | best_loss=9.25549
Epoch 2/80: current_loss=9.68208 | best_loss=9.25549
Epoch 3/80: current_loss=9.54549 | best_loss=9.25549
Epoch 4/80: current_loss=9.31548 | best_loss=9.25549
Epoch 5/80: current_loss=9.33098 | best_loss=9.25549
Epoch 6/80: current_loss=9.52158 | best_loss=9.25549
Epoch 7/80: current_loss=9.38166 | best_loss=9.25549
Epoch 8/80: current_loss=10.57021 | best_loss=9.25549
Epoch 9/80: current_loss=9.26951 | best_loss=9.25549
Epoch 10/80: current_loss=9.78882 | best_loss=9.25549
Epoch 11/80: current_loss=9.32649 | best_loss=9.25549
Epoch 12/80: current_loss=9.62643 | best_loss=9.25549
Epoch 13/80: current_loss=9.55771 | best_loss=9.25549
Epoch 14/80: current_loss=9.55017 | best_loss=9.25549
Epoch 15/80: current_loss=9.37941 | best_loss=9.25549
Epoch 16/80: current_loss=9.65640 | best_loss=9.25549
Epoch 17/80: current_loss=10.01820 | best_loss=9.25549
Epoch 18/80: current_loss=9.35881 | best_loss=9.25549
Epoch 19/80: current_loss=9.69815 | best_loss=9.25549
Epoch 20/80: current_loss=9.50276 | best_loss=9.25549
Early Stopping at epoch 20
      explained_var=-0.00045 | mse_loss=9.38200
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.15342 | best_loss=9.15342
Epoch 1/80: current_loss=9.02603 | best_loss=9.02603
Epoch 2/80: current_loss=8.98751 | best_loss=8.98751
Epoch 3/80: current_loss=9.07417 | best_loss=8.98751
Epoch 4/80: current_loss=9.87537 | best_loss=8.98751
Epoch 5/80: current_loss=9.08246 | best_loss=8.98751
Epoch 6/80: current_loss=9.50680 | best_loss=8.98751
Epoch 7/80: current_loss=9.01676 | best_loss=8.98751
Epoch 8/80: current_loss=8.95734 | best_loss=8.95734
Epoch 9/80: current_loss=9.32212 | best_loss=8.95734
Epoch 10/80: current_loss=9.04686 | best_loss=8.95734
Epoch 11/80: current_loss=8.97212 | best_loss=8.95734
Epoch 12/80: current_loss=9.23516 | best_loss=8.95734
Epoch 13/80: current_loss=9.13671 | best_loss=8.95734
Epoch 14/80: current_loss=9.19593 | best_loss=8.95734
Epoch 15/80: current_loss=9.14491 | best_loss=8.95734
Epoch 16/80: current_loss=9.27343 | best_loss=8.95734
Epoch 17/80: current_loss=9.10025 | best_loss=8.95734
Epoch 18/80: current_loss=8.98426 | best_loss=8.95734
Epoch 19/80: current_loss=9.05666 | best_loss=8.95734
Epoch 20/80: current_loss=9.04660 | best_loss=8.95734
Epoch 21/80: current_loss=8.95194 | best_loss=8.95194
Epoch 22/80: current_loss=9.10130 | best_loss=8.95194
Epoch 23/80: current_loss=8.97303 | best_loss=8.95194
Epoch 24/80: current_loss=10.12226 | best_loss=8.95194
Epoch 25/80: current_loss=9.03500 | best_loss=8.95194
Epoch 26/80: current_loss=8.95943 | best_loss=8.95194
Epoch 27/80: current_loss=9.07695 | best_loss=8.95194
Epoch 28/80: current_loss=9.42729 | best_loss=8.95194
Epoch 29/80: current_loss=8.98219 | best_loss=8.95194
Epoch 30/80: current_loss=9.17526 | best_loss=8.95194
Epoch 31/80: current_loss=9.23938 | best_loss=8.95194
Epoch 32/80: current_loss=10.21780 | best_loss=8.95194
Epoch 33/80: current_loss=9.24720 | best_loss=8.95194
Epoch 34/80: current_loss=9.05741 | best_loss=8.95194
Epoch 35/80: current_loss=9.12727 | best_loss=8.95194
Epoch 36/80: current_loss=9.03387 | best_loss=8.95194
Epoch 37/80: current_loss=9.09487 | best_loss=8.95194
Epoch 38/80: current_loss=9.19379 | best_loss=8.95194
Epoch 39/80: current_loss=9.56611 | best_loss=8.95194
Epoch 40/80: current_loss=9.54496 | best_loss=8.95194
Epoch 41/80: current_loss=9.10176 | best_loss=8.95194
Early Stopping at epoch 41
      explained_var=0.00065 | mse_loss=8.36531

----------------------------------------------
Params for Trial 25
{'learning_rate': 0.01, 'weight_decay': 0.0024475054149537935, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.06050 | best_loss=10.06050
Epoch 1/80: current_loss=9.90964 | best_loss=9.90964
Epoch 2/80: current_loss=9.99622 | best_loss=9.90964
Epoch 3/80: current_loss=9.93758 | best_loss=9.90964
Epoch 4/80: current_loss=9.91288 | best_loss=9.90964
Epoch 5/80: current_loss=9.91442 | best_loss=9.90964
Epoch 6/80: current_loss=9.92943 | best_loss=9.90964
Epoch 7/80: current_loss=10.04017 | best_loss=9.90964
Epoch 8/80: current_loss=9.91349 | best_loss=9.90964
Epoch 9/80: current_loss=9.93778 | best_loss=9.90964
Epoch 10/80: current_loss=10.22826 | best_loss=9.90964
Epoch 11/80: current_loss=10.03929 | best_loss=9.90964
Epoch 12/80: current_loss=10.44464 | best_loss=9.90964
Epoch 13/80: current_loss=10.26084 | best_loss=9.90964
Epoch 14/80: current_loss=9.96221 | best_loss=9.90964
Epoch 15/80: current_loss=9.90804 | best_loss=9.90804
Epoch 16/80: current_loss=9.91738 | best_loss=9.90804
Epoch 17/80: current_loss=9.89816 | best_loss=9.89816
Epoch 18/80: current_loss=9.96108 | best_loss=9.89816
Epoch 19/80: current_loss=10.05389 | best_loss=9.89816
Epoch 20/80: current_loss=9.91196 | best_loss=9.89816
Epoch 21/80: current_loss=10.73414 | best_loss=9.89816
Epoch 22/80: current_loss=9.97010 | best_loss=9.89816
Epoch 23/80: current_loss=9.90469 | best_loss=9.89816
Epoch 24/80: current_loss=10.00681 | best_loss=9.89816
Epoch 25/80: current_loss=9.90247 | best_loss=9.89816
Epoch 26/80: current_loss=10.10924 | best_loss=9.89816
Epoch 27/80: current_loss=9.97141 | best_loss=9.89816
Epoch 28/80: current_loss=10.39170 | best_loss=9.89816
Epoch 29/80: current_loss=9.96920 | best_loss=9.89816
Epoch 30/80: current_loss=9.95992 | best_loss=9.89816
Epoch 31/80: current_loss=10.00291 | best_loss=9.89816
Epoch 32/80: current_loss=10.10440 | best_loss=9.89816
Epoch 33/80: current_loss=10.07231 | best_loss=9.89816
Epoch 34/80: current_loss=9.94490 | best_loss=9.89816
Epoch 35/80: current_loss=10.96878 | best_loss=9.89816
Epoch 36/80: current_loss=10.57435 | best_loss=9.89816
Epoch 37/80: current_loss=10.67349 | best_loss=9.89816
Early Stopping at epoch 37
      explained_var=0.00149 | mse_loss=9.57351
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.64096 | best_loss=8.64096
Epoch 1/80: current_loss=8.49303 | best_loss=8.49303
Epoch 2/80: current_loss=8.83684 | best_loss=8.49303
Epoch 3/80: current_loss=8.50944 | best_loss=8.49303
Epoch 4/80: current_loss=8.51351 | best_loss=8.49303
Epoch 5/80: current_loss=8.78867 | best_loss=8.49303
Epoch 6/80: current_loss=8.88472 | best_loss=8.49303
Epoch 7/80: current_loss=8.52402 | best_loss=8.49303
Epoch 8/80: current_loss=8.97935 | best_loss=8.49303
Epoch 9/80: current_loss=8.61492 | best_loss=8.49303
Epoch 10/80: current_loss=8.53794 | best_loss=8.49303
Epoch 11/80: current_loss=8.66038 | best_loss=8.49303
Epoch 12/80: current_loss=8.50686 | best_loss=8.49303
Epoch 13/80: current_loss=8.53034 | best_loss=8.49303
Epoch 14/80: current_loss=8.84635 | best_loss=8.49303
Epoch 15/80: current_loss=8.78603 | best_loss=8.49303
Epoch 16/80: current_loss=8.66492 | best_loss=8.49303
Epoch 17/80: current_loss=8.52970 | best_loss=8.49303
Epoch 18/80: current_loss=8.52308 | best_loss=8.49303
Epoch 19/80: current_loss=8.77740 | best_loss=8.49303
Epoch 20/80: current_loss=8.81497 | best_loss=8.49303
Epoch 21/80: current_loss=9.04333 | best_loss=8.49303
Early Stopping at epoch 21
      explained_var=0.00232 | mse_loss=8.58279

----------------------------------------------
Params for Trial 26
{'learning_rate': 1e-05, 'weight_decay': 0.006814869993263544, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=74.61341 | best_loss=74.61341
Epoch 1/80: current_loss=73.02671 | best_loss=73.02671
Epoch 2/80: current_loss=71.07264 | best_loss=71.07264
Epoch 3/80: current_loss=68.37503 | best_loss=68.37503
Epoch 4/80: current_loss=64.39738 | best_loss=64.39738
Epoch 5/80: current_loss=58.49414 | best_loss=58.49414
Epoch 6/80: current_loss=51.17938 | best_loss=51.17938
Epoch 7/80: current_loss=44.56196 | best_loss=44.56196
Epoch 8/80: current_loss=39.39529 | best_loss=39.39529
Epoch 9/80: current_loss=35.44615 | best_loss=35.44615
Epoch 10/80: current_loss=32.40852 | best_loss=32.40852
Epoch 11/80: current_loss=29.96927 | best_loss=29.96927
Epoch 12/80: current_loss=27.98333 | best_loss=27.98333
Epoch 13/80: current_loss=26.33161 | best_loss=26.33161
Epoch 14/80: current_loss=24.94232 | best_loss=24.94232
Epoch 15/80: current_loss=23.76646 | best_loss=23.76646
Epoch 16/80: current_loss=22.76225 | best_loss=22.76225
Epoch 17/80: current_loss=21.86417 | best_loss=21.86417
Epoch 18/80: current_loss=21.09868 | best_loss=21.09868
Epoch 19/80: current_loss=20.40396 | best_loss=20.40396
Epoch 20/80: current_loss=19.78346 | best_loss=19.78346
Epoch 21/80: current_loss=19.20793 | best_loss=19.20793
Epoch 22/80: current_loss=18.69755 | best_loss=18.69755
Epoch 23/80: current_loss=18.21921 | best_loss=18.21921
Epoch 24/80: current_loss=17.76431 | best_loss=17.76431
Epoch 25/80: current_loss=17.36287 | best_loss=17.36287
Epoch 26/80: current_loss=16.98611 | best_loss=16.98611
Epoch 27/80: current_loss=16.64442 | best_loss=16.64442
Epoch 28/80: current_loss=16.31629 | best_loss=16.31629
Epoch 29/80: current_loss=16.00889 | best_loss=16.00889
Epoch 30/80: current_loss=15.71535 | best_loss=15.71535
Epoch 31/80: current_loss=15.46095 | best_loss=15.46095
Epoch 32/80: current_loss=15.21014 | best_loss=15.21014
Epoch 33/80: current_loss=14.97456 | best_loss=14.97456
Epoch 34/80: current_loss=14.74991 | best_loss=14.74991
Epoch 35/80: current_loss=14.54489 | best_loss=14.54489
Epoch 36/80: current_loss=14.34775 | best_loss=14.34775
Epoch 37/80: current_loss=14.16098 | best_loss=14.16098
Epoch 38/80: current_loss=13.99207 | best_loss=13.99207
Epoch 39/80: current_loss=13.82558 | best_loss=13.82558
Epoch 40/80: current_loss=13.66585 | best_loss=13.66585
Epoch 41/80: current_loss=13.51328 | best_loss=13.51328
Epoch 42/80: current_loss=13.37035 | best_loss=13.37035
Epoch 43/80: current_loss=13.23993 | best_loss=13.23993
Epoch 44/80: current_loss=13.11202 | best_loss=13.11202
Epoch 45/80: current_loss=12.99400 | best_loss=12.99400
Epoch 46/80: current_loss=12.87707 | best_loss=12.87707
Epoch 47/80: current_loss=12.76702 | best_loss=12.76702
Epoch 48/80: current_loss=12.66323 | best_loss=12.66323
Epoch 49/80: current_loss=12.56747 | best_loss=12.56747
Epoch 50/80: current_loss=12.47514 | best_loss=12.47514
Epoch 51/80: current_loss=12.38620 | best_loss=12.38620
Epoch 52/80: current_loss=12.30214 | best_loss=12.30214
Epoch 53/80: current_loss=12.22417 | best_loss=12.22417
Epoch 54/80: current_loss=12.14568 | best_loss=12.14568
Epoch 55/80: current_loss=12.07129 | best_loss=12.07129
Epoch 56/80: current_loss=12.00146 | best_loss=12.00146
Epoch 57/80: current_loss=11.93338 | best_loss=11.93338
Epoch 58/80: current_loss=11.87376 | best_loss=11.87376
Epoch 59/80: current_loss=11.81227 | best_loss=11.81227
Epoch 60/80: current_loss=11.75825 | best_loss=11.75825
Epoch 61/80: current_loss=11.70357 | best_loss=11.70357
Epoch 62/80: current_loss=11.65223 | best_loss=11.65223
Epoch 63/80: current_loss=11.60390 | best_loss=11.60390
Epoch 64/80: current_loss=11.56112 | best_loss=11.56112
Epoch 65/80: current_loss=11.51532 | best_loss=11.51532
Epoch 66/80: current_loss=11.47385 | best_loss=11.47385
Epoch 67/80: current_loss=11.43310 | best_loss=11.43310
Epoch 68/80: current_loss=11.39536 | best_loss=11.39536
Epoch 69/80: current_loss=11.36035 | best_loss=11.36035
Epoch 70/80: current_loss=11.32456 | best_loss=11.32456
Epoch 71/80: current_loss=11.29266 | best_loss=11.29266
Epoch 72/80: current_loss=11.26240 | best_loss=11.26240
Epoch 73/80: current_loss=11.23642 | best_loss=11.23642
Epoch 74/80: current_loss=11.20803 | best_loss=11.20803
Epoch 75/80: current_loss=11.18321 | best_loss=11.18321
Epoch 76/80: current_loss=11.16011 | best_loss=11.16011
Epoch 77/80: current_loss=11.13640 | best_loss=11.13640
Epoch 78/80: current_loss=11.11387 | best_loss=11.11387
Epoch 79/80: current_loss=11.09209 | best_loss=11.09209
      explained_var=-0.08914 | mse_loss=10.48630

----------------------------------------------
Params for Trial 27
{'learning_rate': 0.01, 'weight_decay': 0.0010558124996944266, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.00440 | best_loss=10.00440
Epoch 1/80: current_loss=9.90902 | best_loss=9.90902
Epoch 2/80: current_loss=9.98399 | best_loss=9.90902
Epoch 3/80: current_loss=9.90935 | best_loss=9.90902
Epoch 4/80: current_loss=9.98298 | best_loss=9.90902
Epoch 5/80: current_loss=9.97601 | best_loss=9.90902
Epoch 6/80: current_loss=9.92221 | best_loss=9.90902
Epoch 7/80: current_loss=9.90239 | best_loss=9.90239
Epoch 8/80: current_loss=10.00373 | best_loss=9.90239
Epoch 9/80: current_loss=9.98883 | best_loss=9.90239
Epoch 10/80: current_loss=9.99886 | best_loss=9.90239
Epoch 11/80: current_loss=9.94806 | best_loss=9.90239
Epoch 12/80: current_loss=9.98154 | best_loss=9.90239
Epoch 13/80: current_loss=10.16703 | best_loss=9.90239
Epoch 14/80: current_loss=10.01009 | best_loss=9.90239
Epoch 15/80: current_loss=9.90742 | best_loss=9.90239
Epoch 16/80: current_loss=9.90095 | best_loss=9.90095
Epoch 17/80: current_loss=9.90483 | best_loss=9.90095
Epoch 18/80: current_loss=9.93540 | best_loss=9.90095
Epoch 19/80: current_loss=9.90788 | best_loss=9.90095
Epoch 20/80: current_loss=9.90898 | best_loss=9.90095
Epoch 21/80: current_loss=9.95722 | best_loss=9.90095
Epoch 22/80: current_loss=9.90991 | best_loss=9.90095
Epoch 23/80: current_loss=10.14416 | best_loss=9.90095
Epoch 24/80: current_loss=9.97216 | best_loss=9.90095
Epoch 25/80: current_loss=9.91771 | best_loss=9.90095
Epoch 26/80: current_loss=9.91220 | best_loss=9.90095
Epoch 27/80: current_loss=9.96414 | best_loss=9.90095
Epoch 28/80: current_loss=9.91158 | best_loss=9.90095
Epoch 29/80: current_loss=10.03354 | best_loss=9.90095
Epoch 30/80: current_loss=10.02211 | best_loss=9.90095
Epoch 31/80: current_loss=9.92569 | best_loss=9.90095
Epoch 32/80: current_loss=9.90926 | best_loss=9.90095
Epoch 33/80: current_loss=10.07898 | best_loss=9.90095
Epoch 34/80: current_loss=9.91703 | best_loss=9.90095
Epoch 35/80: current_loss=10.06744 | best_loss=9.90095
Epoch 36/80: current_loss=9.94236 | best_loss=9.90095
Early Stopping at epoch 36
      explained_var=0.00092 | mse_loss=9.56790
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.76970 | best_loss=8.76970
Epoch 1/80: current_loss=8.79027 | best_loss=8.76970
Epoch 2/80: current_loss=8.64619 | best_loss=8.64619
Epoch 3/80: current_loss=8.50665 | best_loss=8.50665
Epoch 4/80: current_loss=8.51647 | best_loss=8.50665
Epoch 5/80: current_loss=8.63352 | best_loss=8.50665
Epoch 6/80: current_loss=8.59942 | best_loss=8.50665
Epoch 7/80: current_loss=8.54164 | best_loss=8.50665
Epoch 8/80: current_loss=8.95101 | best_loss=8.50665
Epoch 9/80: current_loss=8.55451 | best_loss=8.50665
Epoch 10/80: current_loss=8.50595 | best_loss=8.50595
Epoch 11/80: current_loss=8.49863 | best_loss=8.49863
Epoch 12/80: current_loss=8.50714 | best_loss=8.49863
Epoch 13/80: current_loss=8.51676 | best_loss=8.49863
Epoch 14/80: current_loss=9.11635 | best_loss=8.49863
Epoch 15/80: current_loss=8.63331 | best_loss=8.49863
Epoch 16/80: current_loss=8.70111 | best_loss=8.49863
Epoch 17/80: current_loss=8.70080 | best_loss=8.49863
Epoch 18/80: current_loss=8.51471 | best_loss=8.49863
Epoch 19/80: current_loss=8.52352 | best_loss=8.49863
Epoch 20/80: current_loss=8.52224 | best_loss=8.49863
Epoch 21/80: current_loss=8.53814 | best_loss=8.49863
Epoch 22/80: current_loss=8.67846 | best_loss=8.49863
Epoch 23/80: current_loss=8.75861 | best_loss=8.49863
Epoch 24/80: current_loss=8.61566 | best_loss=8.49863
Epoch 25/80: current_loss=8.53704 | best_loss=8.49863
Epoch 26/80: current_loss=8.58345 | best_loss=8.49863
Epoch 27/80: current_loss=8.64031 | best_loss=8.49863
Epoch 28/80: current_loss=8.51499 | best_loss=8.49863
Epoch 29/80: current_loss=8.54419 | best_loss=8.49863
Epoch 30/80: current_loss=8.50643 | best_loss=8.49863
Epoch 31/80: current_loss=8.53050 | best_loss=8.49863
Early Stopping at epoch 31
      explained_var=0.00042 | mse_loss=8.57627

----------------------------------------------
Params for Trial 28
{'learning_rate': 0.1, 'weight_decay': 0.0038662913852269623, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.38774 | best_loss=10.38774
Epoch 1/80: current_loss=12.02170 | best_loss=10.38774
Epoch 2/80: current_loss=9.89201 | best_loss=9.89201
Epoch 3/80: current_loss=10.72278 | best_loss=9.89201
Epoch 4/80: current_loss=10.52973 | best_loss=9.89201
Epoch 5/80: current_loss=10.44180 | best_loss=9.89201
Epoch 6/80: current_loss=11.31794 | best_loss=9.89201
Epoch 7/80: current_loss=10.73010 | best_loss=9.89201
Epoch 8/80: current_loss=10.29039 | best_loss=9.89201
Epoch 9/80: current_loss=10.90915 | best_loss=9.89201
Epoch 10/80: current_loss=10.57991 | best_loss=9.89201
Epoch 11/80: current_loss=10.06951 | best_loss=9.89201
Epoch 12/80: current_loss=10.58397 | best_loss=9.89201
Epoch 13/80: current_loss=12.42065 | best_loss=9.89201
Epoch 14/80: current_loss=9.94457 | best_loss=9.89201
Epoch 15/80: current_loss=10.20466 | best_loss=9.89201
Epoch 16/80: current_loss=11.65479 | best_loss=9.89201
Epoch 17/80: current_loss=9.93380 | best_loss=9.89201
Epoch 18/80: current_loss=10.46115 | best_loss=9.89201
Epoch 19/80: current_loss=10.30263 | best_loss=9.89201
Epoch 20/80: current_loss=9.93250 | best_loss=9.89201
Epoch 21/80: current_loss=10.64489 | best_loss=9.89201
Epoch 22/80: current_loss=10.13470 | best_loss=9.89201
Early Stopping at epoch 22
      explained_var=0.00230 | mse_loss=9.55486
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.66266 | best_loss=9.66266
Epoch 1/80: current_loss=8.92042 | best_loss=8.92042
Epoch 2/80: current_loss=8.88827 | best_loss=8.88827
Epoch 3/80: current_loss=9.41973 | best_loss=8.88827
Epoch 4/80: current_loss=8.91064 | best_loss=8.88827
Epoch 5/80: current_loss=10.81106 | best_loss=8.88827
Epoch 6/80: current_loss=8.66054 | best_loss=8.66054
Epoch 7/80: current_loss=10.21624 | best_loss=8.66054
Epoch 8/80: current_loss=8.95932 | best_loss=8.66054
Epoch 9/80: current_loss=8.87817 | best_loss=8.66054
Epoch 10/80: current_loss=10.06076 | best_loss=8.66054
Epoch 11/80: current_loss=8.94266 | best_loss=8.66054
Epoch 12/80: current_loss=8.55523 | best_loss=8.55523
Epoch 13/80: current_loss=8.73799 | best_loss=8.55523
Epoch 14/80: current_loss=9.28716 | best_loss=8.55523
Epoch 15/80: current_loss=8.60147 | best_loss=8.55523
Epoch 16/80: current_loss=13.20649 | best_loss=8.55523
Epoch 17/80: current_loss=9.60232 | best_loss=8.55523
Epoch 18/80: current_loss=10.05124 | best_loss=8.55523
Epoch 19/80: current_loss=10.84470 | best_loss=8.55523
Epoch 20/80: current_loss=8.55074 | best_loss=8.55074
Epoch 21/80: current_loss=8.98280 | best_loss=8.55074
Epoch 22/80: current_loss=8.46660 | best_loss=8.46660
Epoch 23/80: current_loss=9.03705 | best_loss=8.46660
Epoch 24/80: current_loss=8.49191 | best_loss=8.46660
Epoch 25/80: current_loss=8.66921 | best_loss=8.46660
Epoch 26/80: current_loss=8.51198 | best_loss=8.46660
Epoch 27/80: current_loss=8.98528 | best_loss=8.46660
Epoch 28/80: current_loss=8.90904 | best_loss=8.46660
Epoch 29/80: current_loss=10.95260 | best_loss=8.46660
Epoch 30/80: current_loss=9.47909 | best_loss=8.46660
Epoch 31/80: current_loss=9.18384 | best_loss=8.46660
Epoch 32/80: current_loss=9.67149 | best_loss=8.46660
Epoch 33/80: current_loss=8.91659 | best_loss=8.46660
Epoch 34/80: current_loss=8.53383 | best_loss=8.46660
Epoch 35/80: current_loss=14.73835 | best_loss=8.46660
Epoch 36/80: current_loss=8.79188 | best_loss=8.46660
Epoch 37/80: current_loss=9.61009 | best_loss=8.46660
Epoch 38/80: current_loss=12.81166 | best_loss=8.46660
Epoch 39/80: current_loss=8.51419 | best_loss=8.46660
Epoch 40/80: current_loss=11.06500 | best_loss=8.46660
Epoch 41/80: current_loss=8.88784 | best_loss=8.46660
Epoch 42/80: current_loss=9.04933 | best_loss=8.46660
Early Stopping at epoch 42
      explained_var=0.00261 | mse_loss=8.55742
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.35590 | best_loss=10.35590
Epoch 1/80: current_loss=9.99210 | best_loss=9.99210
Epoch 2/80: current_loss=9.45796 | best_loss=9.45796
Epoch 3/80: current_loss=9.23472 | best_loss=9.23472
Epoch 4/80: current_loss=11.76741 | best_loss=9.23472
Epoch 5/80: current_loss=10.36442 | best_loss=9.23472
Epoch 6/80: current_loss=10.34729 | best_loss=9.23472
Epoch 7/80: current_loss=10.84565 | best_loss=9.23472
Epoch 8/80: current_loss=9.84486 | best_loss=9.23472
Epoch 9/80: current_loss=10.06874 | best_loss=9.23472
Epoch 10/80: current_loss=9.62258 | best_loss=9.23472
Epoch 11/80: current_loss=9.54630 | best_loss=9.23472
Epoch 12/80: current_loss=10.50614 | best_loss=9.23472
Epoch 13/80: current_loss=11.34118 | best_loss=9.23472
Epoch 14/80: current_loss=9.68696 | best_loss=9.23472
Epoch 15/80: current_loss=9.36295 | best_loss=9.23472
Epoch 16/80: current_loss=13.48769 | best_loss=9.23472
Epoch 17/80: current_loss=10.04157 | best_loss=9.23472
Epoch 18/80: current_loss=10.92218 | best_loss=9.23472
Epoch 19/80: current_loss=9.51811 | best_loss=9.23472
Epoch 20/80: current_loss=9.82954 | best_loss=9.23472
Epoch 21/80: current_loss=9.78156 | best_loss=9.23472
Epoch 22/80: current_loss=9.64233 | best_loss=9.23472
Epoch 23/80: current_loss=10.53091 | best_loss=9.23472
Early Stopping at epoch 23
      explained_var=0.01373 | mse_loss=9.24889
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.15384 | best_loss=10.15384
Epoch 1/80: current_loss=10.33738 | best_loss=10.15384
Epoch 2/80: current_loss=18.15673 | best_loss=10.15384
Epoch 3/80: current_loss=8.99003 | best_loss=8.99003
Epoch 4/80: current_loss=9.01814 | best_loss=8.99003
Epoch 5/80: current_loss=9.84865 | best_loss=8.99003
Epoch 6/80: current_loss=10.67964 | best_loss=8.99003
Epoch 7/80: current_loss=9.30504 | best_loss=8.99003
Epoch 8/80: current_loss=11.03567 | best_loss=8.99003
Epoch 9/80: current_loss=8.96856 | best_loss=8.96856
Epoch 10/80: current_loss=9.15504 | best_loss=8.96856
Epoch 11/80: current_loss=9.47816 | best_loss=8.96856
Epoch 12/80: current_loss=9.16734 | best_loss=8.96856
Epoch 13/80: current_loss=8.95975 | best_loss=8.95975
Epoch 14/80: current_loss=9.05889 | best_loss=8.95975
Epoch 15/80: current_loss=9.31775 | best_loss=8.95975
Epoch 16/80: current_loss=9.53415 | best_loss=8.95975
Epoch 17/80: current_loss=11.36876 | best_loss=8.95975
Epoch 18/80: current_loss=9.04922 | best_loss=8.95975
Epoch 19/80: current_loss=9.08250 | best_loss=8.95975
Epoch 20/80: current_loss=9.02069 | best_loss=8.95975
Epoch 21/80: current_loss=9.10843 | best_loss=8.95975
Epoch 22/80: current_loss=9.07844 | best_loss=8.95975
Epoch 23/80: current_loss=9.58666 | best_loss=8.95975
Epoch 24/80: current_loss=10.16009 | best_loss=8.95975
Epoch 25/80: current_loss=9.35152 | best_loss=8.95975
Epoch 26/80: current_loss=9.79060 | best_loss=8.95975
Epoch 27/80: current_loss=9.56609 | best_loss=8.95975
Epoch 28/80: current_loss=9.47890 | best_loss=8.95975
Epoch 29/80: current_loss=11.83001 | best_loss=8.95975
Epoch 30/80: current_loss=9.84089 | best_loss=8.95975
Epoch 31/80: current_loss=9.06157 | best_loss=8.95975
Epoch 32/80: current_loss=10.03471 | best_loss=8.95975
Epoch 33/80: current_loss=11.73347 | best_loss=8.95975
Early Stopping at epoch 33
      explained_var=0.00113 | mse_loss=8.39547

----------------------------------------------
Params for Trial 29
{'learning_rate': 0.01, 'weight_decay': 0.0019064644459241495, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.02253 | best_loss=10.02253
Epoch 1/80: current_loss=9.89098 | best_loss=9.89098
Epoch 2/80: current_loss=9.98072 | best_loss=9.89098
Epoch 3/80: current_loss=9.91439 | best_loss=9.89098
Epoch 4/80: current_loss=9.94547 | best_loss=9.89098
Epoch 5/80: current_loss=9.90802 | best_loss=9.89098
Epoch 6/80: current_loss=9.93313 | best_loss=9.89098
Epoch 7/80: current_loss=9.92601 | best_loss=9.89098
Epoch 8/80: current_loss=9.91343 | best_loss=9.89098
Epoch 9/80: current_loss=9.92061 | best_loss=9.89098
Epoch 10/80: current_loss=9.98394 | best_loss=9.89098
Epoch 11/80: current_loss=9.92439 | best_loss=9.89098
Epoch 12/80: current_loss=10.01661 | best_loss=9.89098
Epoch 13/80: current_loss=9.97365 | best_loss=9.89098
Epoch 14/80: current_loss=9.89912 | best_loss=9.89098
Epoch 15/80: current_loss=9.92635 | best_loss=9.89098
Epoch 16/80: current_loss=10.12455 | best_loss=9.89098
Epoch 17/80: current_loss=9.92297 | best_loss=9.89098
Epoch 18/80: current_loss=9.89822 | best_loss=9.89098
Epoch 19/80: current_loss=9.91393 | best_loss=9.89098
Epoch 20/80: current_loss=9.93811 | best_loss=9.89098
Epoch 21/80: current_loss=9.96609 | best_loss=9.89098
Early Stopping at epoch 21
      explained_var=0.00204 | mse_loss=9.56219
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.51578 | best_loss=8.51578
Epoch 1/80: current_loss=8.66089 | best_loss=8.51578
Epoch 2/80: current_loss=8.52179 | best_loss=8.51578
Epoch 3/80: current_loss=8.62108 | best_loss=8.51578
Epoch 4/80: current_loss=8.52653 | best_loss=8.51578
Epoch 5/80: current_loss=8.48698 | best_loss=8.48698
Epoch 6/80: current_loss=8.56034 | best_loss=8.48698
Epoch 7/80: current_loss=8.79814 | best_loss=8.48698
Epoch 8/80: current_loss=8.49950 | best_loss=8.48698
Epoch 9/80: current_loss=8.56944 | best_loss=8.48698
Epoch 10/80: current_loss=8.51421 | best_loss=8.48698
Epoch 11/80: current_loss=8.51723 | best_loss=8.48698
Epoch 12/80: current_loss=8.55678 | best_loss=8.48698
Epoch 13/80: current_loss=8.58679 | best_loss=8.48698
Epoch 14/80: current_loss=8.51007 | best_loss=8.48698
Epoch 15/80: current_loss=8.93425 | best_loss=8.48698
Epoch 16/80: current_loss=8.63139 | best_loss=8.48698
Epoch 17/80: current_loss=8.87480 | best_loss=8.48698
Epoch 18/80: current_loss=8.58277 | best_loss=8.48698
Epoch 19/80: current_loss=8.53433 | best_loss=8.48698
Epoch 20/80: current_loss=8.62237 | best_loss=8.48698
Epoch 21/80: current_loss=9.59124 | best_loss=8.48698
Epoch 22/80: current_loss=8.73393 | best_loss=8.48698
Epoch 23/80: current_loss=8.77672 | best_loss=8.48698
Epoch 24/80: current_loss=8.73860 | best_loss=8.48698
Epoch 25/80: current_loss=8.52075 | best_loss=8.48698
Early Stopping at epoch 25
      explained_var=0.00036 | mse_loss=8.58137

----------------------------------------------
Params for Trial 30
{'learning_rate': 0.01, 'weight_decay': 0.0010446243951466672, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.23486 | best_loss=10.23486
Epoch 1/80: current_loss=10.08361 | best_loss=10.08361
Epoch 2/80: current_loss=10.06492 | best_loss=10.06492
Epoch 3/80: current_loss=10.00257 | best_loss=10.00257
Epoch 4/80: current_loss=9.92268 | best_loss=9.92268
Epoch 5/80: current_loss=9.91757 | best_loss=9.91757
Epoch 6/80: current_loss=10.14645 | best_loss=9.91757
Epoch 7/80: current_loss=10.66073 | best_loss=9.91757
Epoch 8/80: current_loss=9.90970 | best_loss=9.90970
Epoch 9/80: current_loss=9.89895 | best_loss=9.89895
Epoch 10/80: current_loss=10.04406 | best_loss=9.89895
Epoch 11/80: current_loss=9.95276 | best_loss=9.89895
Epoch 12/80: current_loss=10.41308 | best_loss=9.89895
Epoch 13/80: current_loss=10.30144 | best_loss=9.89895
Epoch 14/80: current_loss=9.99489 | best_loss=9.89895
Epoch 15/80: current_loss=9.90968 | best_loss=9.89895
Epoch 16/80: current_loss=10.33091 | best_loss=9.89895
Epoch 17/80: current_loss=9.93888 | best_loss=9.89895
Epoch 18/80: current_loss=10.91482 | best_loss=9.89895
Epoch 19/80: current_loss=9.89911 | best_loss=9.89895
Epoch 20/80: current_loss=9.95222 | best_loss=9.89895
Epoch 21/80: current_loss=9.91456 | best_loss=9.89895
Epoch 22/80: current_loss=10.08037 | best_loss=9.89895
Epoch 23/80: current_loss=9.91164 | best_loss=9.89895
Epoch 24/80: current_loss=9.97345 | best_loss=9.89895
Epoch 25/80: current_loss=10.66389 | best_loss=9.89895
Epoch 26/80: current_loss=9.92488 | best_loss=9.89895
Epoch 27/80: current_loss=10.17868 | best_loss=9.89895
Epoch 28/80: current_loss=10.09797 | best_loss=9.89895
Epoch 29/80: current_loss=10.01551 | best_loss=9.89895
Early Stopping at epoch 29
      explained_var=0.00136 | mse_loss=9.57409
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.53164 | best_loss=8.53164
Epoch 1/80: current_loss=8.75372 | best_loss=8.53164
Epoch 2/80: current_loss=8.57474 | best_loss=8.53164
Epoch 3/80: current_loss=8.61327 | best_loss=8.53164
Epoch 4/80: current_loss=8.56571 | best_loss=8.53164
Epoch 5/80: current_loss=8.58557 | best_loss=8.53164
Epoch 6/80: current_loss=8.50773 | best_loss=8.50773
Epoch 7/80: current_loss=8.76787 | best_loss=8.50773
Epoch 8/80: current_loss=8.60024 | best_loss=8.50773
Epoch 9/80: current_loss=9.43007 | best_loss=8.50773
Epoch 10/80: current_loss=9.58632 | best_loss=8.50773
Epoch 11/80: current_loss=8.98348 | best_loss=8.50773
Epoch 12/80: current_loss=10.87105 | best_loss=8.50773
Epoch 13/80: current_loss=8.53783 | best_loss=8.50773
Epoch 14/80: current_loss=8.43050 | best_loss=8.43050
Epoch 15/80: current_loss=9.42446 | best_loss=8.43050
Epoch 16/80: current_loss=8.74138 | best_loss=8.43050
Epoch 17/80: current_loss=8.46622 | best_loss=8.43050
Epoch 18/80: current_loss=8.47661 | best_loss=8.43050
Epoch 19/80: current_loss=8.85518 | best_loss=8.43050
Epoch 20/80: current_loss=8.48240 | best_loss=8.43050
Epoch 21/80: current_loss=8.52407 | best_loss=8.43050
Epoch 22/80: current_loss=8.75050 | best_loss=8.43050
Epoch 23/80: current_loss=8.50579 | best_loss=8.43050
Epoch 24/80: current_loss=8.64987 | best_loss=8.43050
Epoch 25/80: current_loss=8.37932 | best_loss=8.37932
Epoch 26/80: current_loss=8.95403 | best_loss=8.37932
Epoch 27/80: current_loss=8.90336 | best_loss=8.37932
Epoch 28/80: current_loss=8.37061 | best_loss=8.37061
Epoch 29/80: current_loss=9.16744 | best_loss=8.37061
Epoch 30/80: current_loss=9.07196 | best_loss=8.37061
Epoch 31/80: current_loss=8.41755 | best_loss=8.37061
Epoch 32/80: current_loss=9.24952 | best_loss=8.37061
Epoch 33/80: current_loss=8.94187 | best_loss=8.37061
Epoch 34/80: current_loss=8.96333 | best_loss=8.37061
Epoch 35/80: current_loss=9.39329 | best_loss=8.37061
Epoch 36/80: current_loss=10.18132 | best_loss=8.37061
Epoch 37/80: current_loss=9.15026 | best_loss=8.37061
Epoch 38/80: current_loss=11.26233 | best_loss=8.37061
Epoch 39/80: current_loss=8.73226 | best_loss=8.37061
Epoch 40/80: current_loss=8.66064 | best_loss=8.37061
Epoch 41/80: current_loss=9.37804 | best_loss=8.37061
Epoch 42/80: current_loss=8.72995 | best_loss=8.37061
Epoch 43/80: current_loss=9.27584 | best_loss=8.37061
Epoch 44/80: current_loss=8.73583 | best_loss=8.37061
Epoch 45/80: current_loss=8.85532 | best_loss=8.37061
Epoch 46/80: current_loss=8.49626 | best_loss=8.37061
Epoch 47/80: current_loss=9.08262 | best_loss=8.37061
Epoch 48/80: current_loss=10.52152 | best_loss=8.37061
Early Stopping at epoch 48
      explained_var=0.00966 | mse_loss=8.50184
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.52030 | best_loss=9.52030
Epoch 1/80: current_loss=9.96260 | best_loss=9.52030
Epoch 2/80: current_loss=9.55605 | best_loss=9.52030
Epoch 3/80: current_loss=9.57154 | best_loss=9.52030
Epoch 4/80: current_loss=9.41496 | best_loss=9.41496
Epoch 5/80: current_loss=9.47560 | best_loss=9.41496
Epoch 6/80: current_loss=9.51432 | best_loss=9.41496
Epoch 7/80: current_loss=9.40548 | best_loss=9.40548
Epoch 8/80: current_loss=9.52972 | best_loss=9.40548
Epoch 9/80: current_loss=9.63209 | best_loss=9.40548
Epoch 10/80: current_loss=9.59898 | best_loss=9.40548
Epoch 11/80: current_loss=9.38456 | best_loss=9.38456
Epoch 12/80: current_loss=9.40310 | best_loss=9.38456
Epoch 13/80: current_loss=9.50939 | best_loss=9.38456
Epoch 14/80: current_loss=9.23781 | best_loss=9.23781
Epoch 15/80: current_loss=9.55431 | best_loss=9.23781
Epoch 16/80: current_loss=9.40642 | best_loss=9.23781
Epoch 17/80: current_loss=9.48693 | best_loss=9.23781
Epoch 18/80: current_loss=10.47582 | best_loss=9.23781
Epoch 19/80: current_loss=9.44019 | best_loss=9.23781
Epoch 20/80: current_loss=9.52568 | best_loss=9.23781
Epoch 21/80: current_loss=9.45445 | best_loss=9.23781
Epoch 22/80: current_loss=9.54550 | best_loss=9.23781
Epoch 23/80: current_loss=9.81766 | best_loss=9.23781
Epoch 24/80: current_loss=9.67926 | best_loss=9.23781
Epoch 25/80: current_loss=9.80496 | best_loss=9.23781
Epoch 26/80: current_loss=9.84673 | best_loss=9.23781
Epoch 27/80: current_loss=9.35875 | best_loss=9.23781
Epoch 28/80: current_loss=9.44738 | best_loss=9.23781
Epoch 29/80: current_loss=9.46496 | best_loss=9.23781
Epoch 30/80: current_loss=9.71182 | best_loss=9.23781
Epoch 31/80: current_loss=9.51074 | best_loss=9.23781
Epoch 32/80: current_loss=9.53914 | best_loss=9.23781
Epoch 33/80: current_loss=9.65939 | best_loss=9.23781
Epoch 34/80: current_loss=9.60632 | best_loss=9.23781
Early Stopping at epoch 34
      explained_var=0.01288 | mse_loss=9.32469
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.26311 | best_loss=9.26311
Epoch 1/80: current_loss=8.79960 | best_loss=8.79960
Epoch 2/80: current_loss=8.94452 | best_loss=8.79960
Epoch 3/80: current_loss=9.34226 | best_loss=8.79960
Epoch 4/80: current_loss=9.29967 | best_loss=8.79960
Epoch 5/80: current_loss=9.22314 | best_loss=8.79960
Epoch 6/80: current_loss=9.52933 | best_loss=8.79960
Epoch 7/80: current_loss=9.55865 | best_loss=8.79960
Epoch 8/80: current_loss=8.90594 | best_loss=8.79960
Epoch 9/80: current_loss=9.44318 | best_loss=8.79960
Epoch 10/80: current_loss=9.20144 | best_loss=8.79960
Epoch 11/80: current_loss=9.27366 | best_loss=8.79960
Epoch 12/80: current_loss=8.83710 | best_loss=8.79960
Epoch 13/80: current_loss=8.97636 | best_loss=8.79960
Epoch 14/80: current_loss=9.02095 | best_loss=8.79960
Epoch 15/80: current_loss=9.72035 | best_loss=8.79960
Epoch 16/80: current_loss=9.58865 | best_loss=8.79960
Epoch 17/80: current_loss=9.42121 | best_loss=8.79960
Epoch 18/80: current_loss=9.06105 | best_loss=8.79960
Epoch 19/80: current_loss=9.13883 | best_loss=8.79960
Epoch 20/80: current_loss=8.94559 | best_loss=8.79960
Epoch 21/80: current_loss=9.06109 | best_loss=8.79960
Early Stopping at epoch 21
      explained_var=0.00584 | mse_loss=8.31310
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.64499 | best_loss=8.64499
Epoch 1/80: current_loss=8.52127 | best_loss=8.52127
Epoch 2/80: current_loss=8.71826 | best_loss=8.52127
Epoch 3/80: current_loss=9.17144 | best_loss=8.52127
Epoch 4/80: current_loss=8.86093 | best_loss=8.52127
Epoch 5/80: current_loss=8.49396 | best_loss=8.49396
Epoch 6/80: current_loss=8.48415 | best_loss=8.48415
Epoch 7/80: current_loss=8.48430 | best_loss=8.48415
Epoch 8/80: current_loss=8.72727 | best_loss=8.48415
Epoch 9/80: current_loss=9.04552 | best_loss=8.48415
Epoch 10/80: current_loss=8.50046 | best_loss=8.48415
Epoch 11/80: current_loss=8.56322 | best_loss=8.48415
Epoch 12/80: current_loss=8.60414 | best_loss=8.48415
Epoch 13/80: current_loss=8.55431 | best_loss=8.48415
Epoch 14/80: current_loss=8.53915 | best_loss=8.48415
Epoch 15/80: current_loss=8.62556 | best_loss=8.48415
Epoch 16/80: current_loss=8.78117 | best_loss=8.48415
Epoch 17/80: current_loss=8.53168 | best_loss=8.48415
Epoch 18/80: current_loss=8.48292 | best_loss=8.48292
Epoch 19/80: current_loss=8.63514 | best_loss=8.48292
Epoch 20/80: current_loss=8.73080 | best_loss=8.48292
Epoch 21/80: current_loss=8.56404 | best_loss=8.48292
Epoch 22/80: current_loss=8.55242 | best_loss=8.48292
Epoch 23/80: current_loss=8.52759 | best_loss=8.48292
Epoch 24/80: current_loss=8.52348 | best_loss=8.48292
Epoch 25/80: current_loss=8.61525 | best_loss=8.48292
Epoch 26/80: current_loss=8.94602 | best_loss=8.48292
Epoch 27/80: current_loss=8.51598 | best_loss=8.48292
Epoch 28/80: current_loss=8.49128 | best_loss=8.48292
Epoch 29/80: current_loss=9.01877 | best_loss=8.48292
Epoch 30/80: current_loss=9.03955 | best_loss=8.48292
Epoch 31/80: current_loss=8.56796 | best_loss=8.48292
Epoch 32/80: current_loss=9.24711 | best_loss=8.48292
Epoch 33/80: current_loss=8.48173 | best_loss=8.48173
Epoch 34/80: current_loss=9.14375 | best_loss=8.48173
Epoch 35/80: current_loss=8.92963 | best_loss=8.48173
Epoch 36/80: current_loss=8.53865 | best_loss=8.48173
Epoch 37/80: current_loss=8.57741 | best_loss=8.48173
Epoch 38/80: current_loss=8.52776 | best_loss=8.48173
Epoch 39/80: current_loss=8.63610 | best_loss=8.48173
Epoch 40/80: current_loss=8.49907 | best_loss=8.48173
Epoch 41/80: current_loss=8.49483 | best_loss=8.48173
Epoch 42/80: current_loss=8.53387 | best_loss=8.48173
Epoch 43/80: current_loss=8.57592 | best_loss=8.48173
Epoch 44/80: current_loss=9.10181 | best_loss=8.48173
Epoch 45/80: current_loss=8.72473 | best_loss=8.48173
Epoch 46/80: current_loss=8.57429 | best_loss=8.48173
Epoch 47/80: current_loss=8.56597 | best_loss=8.48173
Epoch 48/80: current_loss=8.58505 | best_loss=8.48173
Epoch 49/80: current_loss=8.70385 | best_loss=8.48173
Epoch 50/80: current_loss=9.21060 | best_loss=8.48173
Epoch 51/80: current_loss=9.12681 | best_loss=8.48173
Epoch 52/80: current_loss=9.24426 | best_loss=8.48173
Epoch 53/80: current_loss=8.51118 | best_loss=8.48173
Early Stopping at epoch 53
      explained_var=0.00064 | mse_loss=8.19755
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00608| avg_loss=8.78226
----------------------------------------------


----------------------------------------------
Params for Trial 31
{'learning_rate': 0.01, 'weight_decay': 0.0010778297874458219, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91780 | best_loss=9.91780
Epoch 1/80: current_loss=10.47531 | best_loss=9.91780
Epoch 2/80: current_loss=9.92442 | best_loss=9.91780
Epoch 3/80: current_loss=9.91940 | best_loss=9.91780
Epoch 4/80: current_loss=10.43518 | best_loss=9.91780
Epoch 5/80: current_loss=9.90651 | best_loss=9.90651
Epoch 6/80: current_loss=10.98968 | best_loss=9.90651
Epoch 7/80: current_loss=9.90767 | best_loss=9.90651
Epoch 8/80: current_loss=9.91681 | best_loss=9.90651
Epoch 9/80: current_loss=9.94037 | best_loss=9.90651
Epoch 10/80: current_loss=10.84371 | best_loss=9.90651
Epoch 11/80: current_loss=9.90771 | best_loss=9.90651
Epoch 12/80: current_loss=9.97155 | best_loss=9.90651
Epoch 13/80: current_loss=10.18684 | best_loss=9.90651
Epoch 14/80: current_loss=9.98693 | best_loss=9.90651
Epoch 15/80: current_loss=10.12508 | best_loss=9.90651
Epoch 16/80: current_loss=10.06103 | best_loss=9.90651
Epoch 17/80: current_loss=10.11140 | best_loss=9.90651
Epoch 18/80: current_loss=9.93538 | best_loss=9.90651
Epoch 19/80: current_loss=9.94173 | best_loss=9.90651
Epoch 20/80: current_loss=10.19328 | best_loss=9.90651
Epoch 21/80: current_loss=9.99368 | best_loss=9.90651
Epoch 22/80: current_loss=9.94652 | best_loss=9.90651
Epoch 23/80: current_loss=9.94650 | best_loss=9.90651
Epoch 24/80: current_loss=10.13289 | best_loss=9.90651
Epoch 25/80: current_loss=9.91161 | best_loss=9.90651
Early Stopping at epoch 25
      explained_var=0.00037 | mse_loss=9.58138

----------------------------------------------
Params for Trial 32
{'learning_rate': 0.01, 'weight_decay': 0.0029379297212884423, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.06131 | best_loss=10.06131
Epoch 1/80: current_loss=10.03018 | best_loss=10.03018
Epoch 2/80: current_loss=9.93764 | best_loss=9.93764
Epoch 3/80: current_loss=10.09424 | best_loss=9.93764
Epoch 4/80: current_loss=10.03090 | best_loss=9.93764
Epoch 5/80: current_loss=10.15018 | best_loss=9.93764
Epoch 6/80: current_loss=9.90852 | best_loss=9.90852
Epoch 7/80: current_loss=9.90634 | best_loss=9.90634
Epoch 8/80: current_loss=9.90426 | best_loss=9.90426
Epoch 9/80: current_loss=10.78263 | best_loss=9.90426
Epoch 10/80: current_loss=10.03427 | best_loss=9.90426
Epoch 11/80: current_loss=10.56270 | best_loss=9.90426
Epoch 12/80: current_loss=10.22532 | best_loss=9.90426
Epoch 13/80: current_loss=10.22092 | best_loss=9.90426
Epoch 14/80: current_loss=9.89718 | best_loss=9.89718
Epoch 15/80: current_loss=10.16810 | best_loss=9.89718
Epoch 16/80: current_loss=10.27963 | best_loss=9.89718
Epoch 17/80: current_loss=10.03943 | best_loss=9.89718
Epoch 18/80: current_loss=10.00651 | best_loss=9.89718
Epoch 19/80: current_loss=9.93098 | best_loss=9.89718
Epoch 20/80: current_loss=9.91039 | best_loss=9.89718
Epoch 21/80: current_loss=10.14255 | best_loss=9.89718
Epoch 22/80: current_loss=10.02380 | best_loss=9.89718
Epoch 23/80: current_loss=10.56453 | best_loss=9.89718
Epoch 24/80: current_loss=10.69187 | best_loss=9.89718
Epoch 25/80: current_loss=10.15495 | best_loss=9.89718
Epoch 26/80: current_loss=10.15674 | best_loss=9.89718
Epoch 27/80: current_loss=9.93795 | best_loss=9.89718
Epoch 28/80: current_loss=10.16209 | best_loss=9.89718
Epoch 29/80: current_loss=10.67024 | best_loss=9.89718
Epoch 30/80: current_loss=9.90711 | best_loss=9.89718
Epoch 31/80: current_loss=10.01263 | best_loss=9.89718
Epoch 32/80: current_loss=10.32308 | best_loss=9.89718
Epoch 33/80: current_loss=10.17630 | best_loss=9.89718
Epoch 34/80: current_loss=9.91500 | best_loss=9.89718
Early Stopping at epoch 34
      explained_var=0.00123 | mse_loss=9.56935
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.88845 | best_loss=8.88845
Epoch 1/80: current_loss=8.50221 | best_loss=8.50221
Epoch 2/80: current_loss=8.53746 | best_loss=8.50221
Epoch 3/80: current_loss=8.60809 | best_loss=8.50221
Epoch 4/80: current_loss=8.51899 | best_loss=8.50221
Epoch 5/80: current_loss=8.55156 | best_loss=8.50221
Epoch 6/80: current_loss=8.76737 | best_loss=8.50221
Epoch 7/80: current_loss=8.73590 | best_loss=8.50221
Epoch 8/80: current_loss=8.79141 | best_loss=8.50221
Epoch 9/80: current_loss=8.51702 | best_loss=8.50221
Epoch 10/80: current_loss=8.58982 | best_loss=8.50221
Epoch 11/80: current_loss=8.68457 | best_loss=8.50221
Epoch 12/80: current_loss=8.45772 | best_loss=8.45772
Epoch 13/80: current_loss=8.42940 | best_loss=8.42940
Epoch 14/80: current_loss=8.56468 | best_loss=8.42940
Epoch 15/80: current_loss=8.71098 | best_loss=8.42940
Epoch 16/80: current_loss=8.39904 | best_loss=8.39904
Epoch 17/80: current_loss=8.52807 | best_loss=8.39904
Epoch 18/80: current_loss=8.49097 | best_loss=8.39904
Epoch 19/80: current_loss=8.63069 | best_loss=8.39904
Epoch 20/80: current_loss=8.51326 | best_loss=8.39904
Epoch 21/80: current_loss=9.20217 | best_loss=8.39904
Epoch 22/80: current_loss=8.44289 | best_loss=8.39904
Epoch 23/80: current_loss=8.50885 | best_loss=8.39904
Epoch 24/80: current_loss=8.54792 | best_loss=8.39904
Epoch 25/80: current_loss=9.43941 | best_loss=8.39904
Epoch 26/80: current_loss=8.56922 | best_loss=8.39904
Epoch 27/80: current_loss=8.45772 | best_loss=8.39904
Epoch 28/80: current_loss=8.72479 | best_loss=8.39904
Epoch 29/80: current_loss=8.51783 | best_loss=8.39904
Epoch 30/80: current_loss=8.52337 | best_loss=8.39904
Epoch 31/80: current_loss=8.54186 | best_loss=8.39904
Epoch 32/80: current_loss=8.67864 | best_loss=8.39904
Epoch 33/80: current_loss=8.61049 | best_loss=8.39904
Epoch 34/80: current_loss=8.97153 | best_loss=8.39904
Epoch 35/80: current_loss=9.22324 | best_loss=8.39904
Epoch 36/80: current_loss=8.64362 | best_loss=8.39904
Early Stopping at epoch 36
      explained_var=0.01168 | mse_loss=8.49767
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.30023 | best_loss=9.30023
Epoch 1/80: current_loss=9.56241 | best_loss=9.30023
Epoch 2/80: current_loss=9.92774 | best_loss=9.30023
Epoch 3/80: current_loss=9.76564 | best_loss=9.30023
Epoch 4/80: current_loss=9.48852 | best_loss=9.30023
Epoch 5/80: current_loss=9.24238 | best_loss=9.24238
Epoch 6/80: current_loss=9.40215 | best_loss=9.24238
Epoch 7/80: current_loss=9.40001 | best_loss=9.24238
Epoch 8/80: current_loss=9.63255 | best_loss=9.24238
Epoch 9/80: current_loss=9.42996 | best_loss=9.24238
Epoch 10/80: current_loss=9.98664 | best_loss=9.24238
Epoch 11/80: current_loss=9.27908 | best_loss=9.24238
Epoch 12/80: current_loss=9.27621 | best_loss=9.24238
Epoch 13/80: current_loss=9.34211 | best_loss=9.24238
Epoch 14/80: current_loss=9.69161 | best_loss=9.24238
Epoch 15/80: current_loss=9.39324 | best_loss=9.24238
Epoch 16/80: current_loss=9.27133 | best_loss=9.24238
Epoch 17/80: current_loss=9.64904 | best_loss=9.24238
Epoch 18/80: current_loss=9.73058 | best_loss=9.24238
Epoch 19/80: current_loss=9.37998 | best_loss=9.24238
Epoch 20/80: current_loss=9.64253 | best_loss=9.24238
Epoch 21/80: current_loss=9.30366 | best_loss=9.24238
Epoch 22/80: current_loss=9.56943 | best_loss=9.24238
Epoch 23/80: current_loss=9.68559 | best_loss=9.24238
Epoch 24/80: current_loss=9.32126 | best_loss=9.24238
Epoch 25/80: current_loss=9.56920 | best_loss=9.24238
Early Stopping at epoch 25
      explained_var=0.00229 | mse_loss=9.35765
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.36457 | best_loss=9.36457
Epoch 1/80: current_loss=10.23464 | best_loss=9.36457
Epoch 2/80: current_loss=9.02826 | best_loss=9.02826
Epoch 3/80: current_loss=9.03257 | best_loss=9.02826
Epoch 4/80: current_loss=9.00974 | best_loss=9.00974
Epoch 5/80: current_loss=9.08473 | best_loss=9.00974
Epoch 6/80: current_loss=9.36602 | best_loss=9.00974
Epoch 7/80: current_loss=8.98750 | best_loss=8.98750
Epoch 8/80: current_loss=9.41825 | best_loss=8.98750
Epoch 9/80: current_loss=9.03526 | best_loss=8.98750
Epoch 10/80: current_loss=9.11237 | best_loss=8.98750
Epoch 11/80: current_loss=8.98866 | best_loss=8.98750
Epoch 12/80: current_loss=9.39492 | best_loss=8.98750
Epoch 13/80: current_loss=8.96776 | best_loss=8.96776
Epoch 14/80: current_loss=8.96470 | best_loss=8.96470
Epoch 15/80: current_loss=9.06282 | best_loss=8.96470
Epoch 16/80: current_loss=9.08827 | best_loss=8.96470
Epoch 17/80: current_loss=8.97130 | best_loss=8.96470
Epoch 18/80: current_loss=9.33688 | best_loss=8.96470
Epoch 19/80: current_loss=9.10586 | best_loss=8.96470
Epoch 20/80: current_loss=9.06655 | best_loss=8.96470
Epoch 21/80: current_loss=8.98035 | best_loss=8.96470
Epoch 22/80: current_loss=8.96677 | best_loss=8.96470
Epoch 23/80: current_loss=9.00345 | best_loss=8.96470
Epoch 24/80: current_loss=8.93012 | best_loss=8.93012
Epoch 25/80: current_loss=10.04139 | best_loss=8.93012
Epoch 26/80: current_loss=8.99451 | best_loss=8.93012
Epoch 27/80: current_loss=9.14103 | best_loss=8.93012
Epoch 28/80: current_loss=9.02535 | best_loss=8.93012
Epoch 29/80: current_loss=9.01034 | best_loss=8.93012
Epoch 30/80: current_loss=9.08485 | best_loss=8.93012
Epoch 31/80: current_loss=9.00341 | best_loss=8.93012
Epoch 32/80: current_loss=9.01359 | best_loss=8.93012
Epoch 33/80: current_loss=8.97125 | best_loss=8.93012
Epoch 34/80: current_loss=9.06819 | best_loss=8.93012
Epoch 35/80: current_loss=9.11339 | best_loss=8.93012
Epoch 36/80: current_loss=9.36559 | best_loss=8.93012
Epoch 37/80: current_loss=8.95223 | best_loss=8.93012
Epoch 38/80: current_loss=10.09930 | best_loss=8.93012
Epoch 39/80: current_loss=8.96536 | best_loss=8.93012
Epoch 40/80: current_loss=9.52777 | best_loss=8.93012
Epoch 41/80: current_loss=8.94555 | best_loss=8.93012
Epoch 42/80: current_loss=8.99905 | best_loss=8.93012
Epoch 43/80: current_loss=9.50895 | best_loss=8.93012
Epoch 44/80: current_loss=9.00258 | best_loss=8.93012
Early Stopping at epoch 44
      explained_var=0.00111 | mse_loss=8.35589
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.49705 | best_loss=8.49705
Epoch 1/80: current_loss=8.73784 | best_loss=8.49705
Epoch 2/80: current_loss=8.79472 | best_loss=8.49705
Epoch 3/80: current_loss=8.70133 | best_loss=8.49705
Epoch 4/80: current_loss=8.99514 | best_loss=8.49705
Epoch 5/80: current_loss=8.85237 | best_loss=8.49705
Epoch 6/80: current_loss=8.53584 | best_loss=8.49705
Epoch 7/80: current_loss=8.49359 | best_loss=8.49359
Epoch 8/80: current_loss=8.60473 | best_loss=8.49359
Epoch 9/80: current_loss=8.67252 | best_loss=8.49359
Epoch 10/80: current_loss=8.85324 | best_loss=8.49359
Epoch 11/80: current_loss=8.68770 | best_loss=8.49359
Epoch 12/80: current_loss=8.85298 | best_loss=8.49359
Epoch 13/80: current_loss=8.53442 | best_loss=8.49359
Epoch 14/80: current_loss=8.60384 | best_loss=8.49359
Epoch 15/80: current_loss=8.75940 | best_loss=8.49359
Epoch 16/80: current_loss=8.60647 | best_loss=8.49359
Epoch 17/80: current_loss=8.46888 | best_loss=8.46888
Epoch 18/80: current_loss=8.48172 | best_loss=8.46888
Epoch 19/80: current_loss=8.53732 | best_loss=8.46888
Epoch 20/80: current_loss=8.77009 | best_loss=8.46888
Epoch 21/80: current_loss=8.50427 | best_loss=8.46888
Epoch 22/80: current_loss=8.75595 | best_loss=8.46888
Epoch 23/80: current_loss=8.74498 | best_loss=8.46888
Epoch 24/80: current_loss=8.64874 | best_loss=8.46888
Epoch 25/80: current_loss=8.79307 | best_loss=8.46888
Epoch 26/80: current_loss=8.76891 | best_loss=8.46888
Epoch 27/80: current_loss=9.14844 | best_loss=8.46888
Epoch 28/80: current_loss=8.63434 | best_loss=8.46888
Epoch 29/80: current_loss=8.95952 | best_loss=8.46888
Epoch 30/80: current_loss=8.63598 | best_loss=8.46888
Epoch 31/80: current_loss=8.66956 | best_loss=8.46888
Epoch 32/80: current_loss=9.85252 | best_loss=8.46888
Epoch 33/80: current_loss=8.69757 | best_loss=8.46888
Epoch 34/80: current_loss=8.50800 | best_loss=8.46888
Epoch 35/80: current_loss=8.78207 | best_loss=8.46888
Epoch 36/80: current_loss=8.76270 | best_loss=8.46888
Epoch 37/80: current_loss=8.82563 | best_loss=8.46888
Early Stopping at epoch 37
      explained_var=-0.00024 | mse_loss=8.20631
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.00322| avg_loss=8.79737
----------------------------------------------


----------------------------------------------
Params for Trial 33
{'learning_rate': 1e-05, 'weight_decay': 0.0019852221406124067, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=75.15695 | best_loss=75.15695
Epoch 1/80: current_loss=73.10727 | best_loss=73.10727
Epoch 2/80: current_loss=70.28043 | best_loss=70.28043
Epoch 3/80: current_loss=65.87322 | best_loss=65.87322
Epoch 4/80: current_loss=59.23429 | best_loss=59.23429
Epoch 5/80: current_loss=51.39775 | best_loss=51.39775
Epoch 6/80: current_loss=44.55866 | best_loss=44.55866
Epoch 7/80: current_loss=39.35268 | best_loss=39.35268
Epoch 8/80: current_loss=35.26946 | best_loss=35.26946
Epoch 9/80: current_loss=32.04967 | best_loss=32.04967
Epoch 10/80: current_loss=29.38651 | best_loss=29.38651
Epoch 11/80: current_loss=27.15240 | best_loss=27.15240
Epoch 12/80: current_loss=25.30213 | best_loss=25.30213
Epoch 13/80: current_loss=23.72053 | best_loss=23.72053
Epoch 14/80: current_loss=22.36853 | best_loss=22.36853
Epoch 15/80: current_loss=21.19512 | best_loss=21.19512
Epoch 16/80: current_loss=20.19659 | best_loss=20.19659
Epoch 17/80: current_loss=19.32359 | best_loss=19.32359
Epoch 18/80: current_loss=18.53512 | best_loss=18.53512
Epoch 19/80: current_loss=17.84973 | best_loss=17.84973
Epoch 20/80: current_loss=17.24387 | best_loss=17.24387
Epoch 21/80: current_loss=16.70636 | best_loss=16.70636
Epoch 22/80: current_loss=16.22839 | best_loss=16.22839
Epoch 23/80: current_loss=15.80511 | best_loss=15.80511
Epoch 24/80: current_loss=15.41350 | best_loss=15.41350
Epoch 25/80: current_loss=15.05596 | best_loss=15.05596
Epoch 26/80: current_loss=14.73792 | best_loss=14.73792
Epoch 27/80: current_loss=14.44724 | best_loss=14.44724
Epoch 28/80: current_loss=14.17403 | best_loss=14.17403
Epoch 29/80: current_loss=13.92443 | best_loss=13.92443
Epoch 30/80: current_loss=13.69548 | best_loss=13.69548
Epoch 31/80: current_loss=13.48369 | best_loss=13.48369
Epoch 32/80: current_loss=13.29026 | best_loss=13.29026
Epoch 33/80: current_loss=13.10677 | best_loss=13.10677
Epoch 34/80: current_loss=12.93650 | best_loss=12.93650
Epoch 35/80: current_loss=12.77348 | best_loss=12.77348
Epoch 36/80: current_loss=12.62301 | best_loss=12.62301
Epoch 37/80: current_loss=12.48569 | best_loss=12.48569
Epoch 38/80: current_loss=12.35625 | best_loss=12.35625
Epoch 39/80: current_loss=12.23613 | best_loss=12.23613
Epoch 40/80: current_loss=12.11880 | best_loss=12.11880
Epoch 41/80: current_loss=12.00893 | best_loss=12.00893
Epoch 42/80: current_loss=11.90859 | best_loss=11.90859
Epoch 43/80: current_loss=11.81394 | best_loss=11.81394
Epoch 44/80: current_loss=11.72119 | best_loss=11.72119
Epoch 45/80: current_loss=11.63725 | best_loss=11.63725
Epoch 46/80: current_loss=11.55573 | best_loss=11.55573
Epoch 47/80: current_loss=11.47854 | best_loss=11.47854
Epoch 48/80: current_loss=11.40729 | best_loss=11.40729
Epoch 49/80: current_loss=11.33992 | best_loss=11.33992
Epoch 50/80: current_loss=11.28232 | best_loss=11.28232
Epoch 51/80: current_loss=11.21826 | best_loss=11.21826
Epoch 52/80: current_loss=11.16721 | best_loss=11.16721
Epoch 53/80: current_loss=11.11683 | best_loss=11.11683
Epoch 54/80: current_loss=11.06698 | best_loss=11.06698
Epoch 55/80: current_loss=11.02081 | best_loss=11.02081
Epoch 56/80: current_loss=10.97830 | best_loss=10.97830
Epoch 57/80: current_loss=10.93565 | best_loss=10.93565
Epoch 58/80: current_loss=10.89501 | best_loss=10.89501
Epoch 59/80: current_loss=10.85838 | best_loss=10.85838
Epoch 60/80: current_loss=10.82587 | best_loss=10.82587
Epoch 61/80: current_loss=10.79376 | best_loss=10.79376
Epoch 62/80: current_loss=10.76397 | best_loss=10.76397
Epoch 63/80: current_loss=10.73562 | best_loss=10.73562
Epoch 64/80: current_loss=10.70785 | best_loss=10.70785
Epoch 65/80: current_loss=10.68195 | best_loss=10.68195
Epoch 66/80: current_loss=10.65724 | best_loss=10.65724
Epoch 67/80: current_loss=10.63243 | best_loss=10.63243
Epoch 68/80: current_loss=10.60988 | best_loss=10.60988
Epoch 69/80: current_loss=10.58701 | best_loss=10.58701
Epoch 70/80: current_loss=10.56734 | best_loss=10.56734
Epoch 71/80: current_loss=10.54808 | best_loss=10.54808
Epoch 72/80: current_loss=10.52906 | best_loss=10.52906
Epoch 73/80: current_loss=10.51115 | best_loss=10.51115
Epoch 74/80: current_loss=10.49370 | best_loss=10.49370
Epoch 75/80: current_loss=10.47836 | best_loss=10.47836
Epoch 76/80: current_loss=10.46412 | best_loss=10.46412
Epoch 77/80: current_loss=10.45070 | best_loss=10.45070
Epoch 78/80: current_loss=10.43580 | best_loss=10.43580
Epoch 79/80: current_loss=10.42183 | best_loss=10.42183
      explained_var=-0.03987 | mse_loss=9.96338

----------------------------------------------
Params for Trial 34
{'learning_rate': 0.01, 'weight_decay': 0.0009941751263749209, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.96907 | best_loss=9.96907
Epoch 1/80: current_loss=10.13349 | best_loss=9.96907
Epoch 2/80: current_loss=10.16516 | best_loss=9.96907
Epoch 3/80: current_loss=10.02196 | best_loss=9.96907
Epoch 4/80: current_loss=9.98010 | best_loss=9.96907
Epoch 5/80: current_loss=9.95942 | best_loss=9.95942
Epoch 6/80: current_loss=10.06637 | best_loss=9.95942
Epoch 7/80: current_loss=10.38795 | best_loss=9.95942
Epoch 8/80: current_loss=10.21069 | best_loss=9.95942
Epoch 9/80: current_loss=9.89949 | best_loss=9.89949
Epoch 10/80: current_loss=9.89056 | best_loss=9.89056
Epoch 11/80: current_loss=9.90414 | best_loss=9.89056
Epoch 12/80: current_loss=10.08353 | best_loss=9.89056
Epoch 13/80: current_loss=9.92489 | best_loss=9.89056
Epoch 14/80: current_loss=10.20702 | best_loss=9.89056
Epoch 15/80: current_loss=10.73389 | best_loss=9.89056
Epoch 16/80: current_loss=9.96171 | best_loss=9.89056
Epoch 17/80: current_loss=10.02775 | best_loss=9.89056
Epoch 18/80: current_loss=9.91336 | best_loss=9.89056
Epoch 19/80: current_loss=10.19723 | best_loss=9.89056
Epoch 20/80: current_loss=9.97273 | best_loss=9.89056
Epoch 21/80: current_loss=9.96377 | best_loss=9.89056
Epoch 22/80: current_loss=10.27259 | best_loss=9.89056
Epoch 23/80: current_loss=10.26234 | best_loss=9.89056
Epoch 24/80: current_loss=9.90985 | best_loss=9.89056
Epoch 25/80: current_loss=9.90942 | best_loss=9.89056
Epoch 26/80: current_loss=9.91454 | best_loss=9.89056
Epoch 27/80: current_loss=9.90277 | best_loss=9.89056
Epoch 28/80: current_loss=9.92020 | best_loss=9.89056
Epoch 29/80: current_loss=10.06599 | best_loss=9.89056
Epoch 30/80: current_loss=9.90410 | best_loss=9.89056
Early Stopping at epoch 30
      explained_var=0.00207 | mse_loss=9.55755
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.71954 | best_loss=8.71954
Epoch 1/80: current_loss=8.53976 | best_loss=8.53976
Epoch 2/80: current_loss=8.52073 | best_loss=8.52073
Epoch 3/80: current_loss=8.73999 | best_loss=8.52073
Epoch 4/80: current_loss=8.60876 | best_loss=8.52073
Epoch 5/80: current_loss=8.50227 | best_loss=8.50227
Epoch 6/80: current_loss=8.48210 | best_loss=8.48210
Epoch 7/80: current_loss=8.97766 | best_loss=8.48210
Epoch 8/80: current_loss=8.45837 | best_loss=8.45837
Epoch 9/80: current_loss=8.37760 | best_loss=8.37760
Epoch 10/80: current_loss=9.16826 | best_loss=8.37760
Epoch 11/80: current_loss=8.61082 | best_loss=8.37760
Epoch 12/80: current_loss=8.52544 | best_loss=8.37760
Epoch 13/80: current_loss=8.60051 | best_loss=8.37760
Epoch 14/80: current_loss=8.54484 | best_loss=8.37760
Epoch 15/80: current_loss=8.46553 | best_loss=8.37760
Epoch 16/80: current_loss=8.54903 | best_loss=8.37760
Epoch 17/80: current_loss=8.68115 | best_loss=8.37760
Epoch 18/80: current_loss=8.79148 | best_loss=8.37760
Epoch 19/80: current_loss=8.39716 | best_loss=8.37760
Epoch 20/80: current_loss=8.53703 | best_loss=8.37760
Epoch 21/80: current_loss=8.53575 | best_loss=8.37760
Epoch 22/80: current_loss=8.63086 | best_loss=8.37760
Epoch 23/80: current_loss=8.90705 | best_loss=8.37760
Epoch 24/80: current_loss=8.76255 | best_loss=8.37760
Epoch 25/80: current_loss=8.76818 | best_loss=8.37760
Epoch 26/80: current_loss=8.59167 | best_loss=8.37760
Epoch 27/80: current_loss=8.65213 | best_loss=8.37760
Epoch 28/80: current_loss=8.53586 | best_loss=8.37760
Epoch 29/80: current_loss=9.86988 | best_loss=8.37760
Early Stopping at epoch 29
      explained_var=0.01021 | mse_loss=8.50553
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.67042 | best_loss=9.67042
Epoch 1/80: current_loss=9.43306 | best_loss=9.43306
Epoch 2/80: current_loss=10.19194 | best_loss=9.43306
Epoch 3/80: current_loss=11.10872 | best_loss=9.43306
Epoch 4/80: current_loss=9.58831 | best_loss=9.43306
Epoch 5/80: current_loss=9.34004 | best_loss=9.34004
Epoch 6/80: current_loss=9.98806 | best_loss=9.34004
Epoch 7/80: current_loss=9.41317 | best_loss=9.34004
Epoch 8/80: current_loss=10.36540 | best_loss=9.34004
Epoch 9/80: current_loss=9.72306 | best_loss=9.34004
Epoch 10/80: current_loss=9.59975 | best_loss=9.34004
Epoch 11/80: current_loss=9.43709 | best_loss=9.34004
Epoch 12/80: current_loss=9.34266 | best_loss=9.34004
Epoch 13/80: current_loss=11.03137 | best_loss=9.34004
Epoch 14/80: current_loss=10.68926 | best_loss=9.34004
Epoch 15/80: current_loss=9.58223 | best_loss=9.34004
Epoch 16/80: current_loss=9.33665 | best_loss=9.33665
Epoch 17/80: current_loss=9.67503 | best_loss=9.33665
Epoch 18/80: current_loss=9.29346 | best_loss=9.29346
Epoch 19/80: current_loss=9.42186 | best_loss=9.29346
Epoch 20/80: current_loss=9.97989 | best_loss=9.29346
Epoch 21/80: current_loss=9.30862 | best_loss=9.29346
Epoch 22/80: current_loss=9.50363 | best_loss=9.29346
Epoch 23/80: current_loss=9.98848 | best_loss=9.29346
Epoch 24/80: current_loss=9.95219 | best_loss=9.29346
Epoch 25/80: current_loss=9.26768 | best_loss=9.26768
Epoch 26/80: current_loss=9.35456 | best_loss=9.26768
Epoch 27/80: current_loss=9.53039 | best_loss=9.26768
Epoch 28/80: current_loss=11.08123 | best_loss=9.26768
Epoch 29/80: current_loss=9.42890 | best_loss=9.26768
Epoch 30/80: current_loss=10.06065 | best_loss=9.26768
Epoch 31/80: current_loss=10.43977 | best_loss=9.26768
Epoch 32/80: current_loss=9.64710 | best_loss=9.26768
Epoch 33/80: current_loss=10.12332 | best_loss=9.26768
Epoch 34/80: current_loss=10.46566 | best_loss=9.26768
Epoch 35/80: current_loss=10.08495 | best_loss=9.26768
Epoch 36/80: current_loss=9.82521 | best_loss=9.26768
Epoch 37/80: current_loss=9.75648 | best_loss=9.26768
Epoch 38/80: current_loss=10.54046 | best_loss=9.26768
Epoch 39/80: current_loss=10.01220 | best_loss=9.26768
Epoch 40/80: current_loss=10.02848 | best_loss=9.26768
Epoch 41/80: current_loss=10.75592 | best_loss=9.26768
Epoch 42/80: current_loss=9.73074 | best_loss=9.26768
Epoch 43/80: current_loss=10.08525 | best_loss=9.26768
Epoch 44/80: current_loss=9.57507 | best_loss=9.26768
Epoch 45/80: current_loss=9.59426 | best_loss=9.26768
Early Stopping at epoch 45
      explained_var=0.00482 | mse_loss=9.36686
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.03073 | best_loss=9.03073
Epoch 1/80: current_loss=9.10973 | best_loss=9.03073
Epoch 2/80: current_loss=10.92142 | best_loss=9.03073
Epoch 3/80: current_loss=9.01791 | best_loss=9.01791
Epoch 4/80: current_loss=9.26125 | best_loss=9.01791
Epoch 5/80: current_loss=9.15329 | best_loss=9.01791
Epoch 6/80: current_loss=8.98094 | best_loss=8.98094
Epoch 7/80: current_loss=9.01220 | best_loss=8.98094
Epoch 8/80: current_loss=9.30652 | best_loss=8.98094
Epoch 9/80: current_loss=8.97240 | best_loss=8.97240
Epoch 10/80: current_loss=9.81113 | best_loss=8.97240
Epoch 11/80: current_loss=9.32101 | best_loss=8.97240
Epoch 12/80: current_loss=9.06912 | best_loss=8.97240
Epoch 13/80: current_loss=10.72686 | best_loss=8.97240
Epoch 14/80: current_loss=9.10376 | best_loss=8.97240
Epoch 15/80: current_loss=9.14782 | best_loss=8.97240
Epoch 16/80: current_loss=9.44291 | best_loss=8.97240
Epoch 17/80: current_loss=8.95317 | best_loss=8.95317
Epoch 18/80: current_loss=8.94363 | best_loss=8.94363
Epoch 19/80: current_loss=9.25527 | best_loss=8.94363
Epoch 20/80: current_loss=9.05204 | best_loss=8.94363
Epoch 21/80: current_loss=8.89212 | best_loss=8.89212
Epoch 22/80: current_loss=9.39929 | best_loss=8.89212
Epoch 23/80: current_loss=9.08309 | best_loss=8.89212
Epoch 24/80: current_loss=9.03686 | best_loss=8.89212
Epoch 25/80: current_loss=9.23246 | best_loss=8.89212
Epoch 26/80: current_loss=9.21455 | best_loss=8.89212
Epoch 27/80: current_loss=9.30900 | best_loss=8.89212
Epoch 28/80: current_loss=9.21798 | best_loss=8.89212
Epoch 29/80: current_loss=9.16502 | best_loss=8.89212
Epoch 30/80: current_loss=9.31127 | best_loss=8.89212
Epoch 31/80: current_loss=9.26500 | best_loss=8.89212
Epoch 32/80: current_loss=9.05033 | best_loss=8.89212
Epoch 33/80: current_loss=8.94847 | best_loss=8.89212
Epoch 34/80: current_loss=9.02819 | best_loss=8.89212
Epoch 35/80: current_loss=9.11288 | best_loss=8.89212
Epoch 36/80: current_loss=9.30114 | best_loss=8.89212
Epoch 37/80: current_loss=8.91945 | best_loss=8.89212
Epoch 38/80: current_loss=9.54025 | best_loss=8.89212
Epoch 39/80: current_loss=9.10381 | best_loss=8.89212
Epoch 40/80: current_loss=9.00154 | best_loss=8.89212
Epoch 41/80: current_loss=9.21464 | best_loss=8.89212
Early Stopping at epoch 41
      explained_var=0.00571 | mse_loss=8.32961
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50097 | best_loss=8.50097
Epoch 1/80: current_loss=8.43285 | best_loss=8.43285
Epoch 2/80: current_loss=8.64799 | best_loss=8.43285
Epoch 3/80: current_loss=8.91852 | best_loss=8.43285
Epoch 4/80: current_loss=8.45548 | best_loss=8.43285
Epoch 5/80: current_loss=8.61437 | best_loss=8.43285
Epoch 6/80: current_loss=8.51621 | best_loss=8.43285
Epoch 7/80: current_loss=8.48600 | best_loss=8.43285
Epoch 8/80: current_loss=8.44760 | best_loss=8.43285
Epoch 9/80: current_loss=8.86201 | best_loss=8.43285
Epoch 10/80: current_loss=9.53563 | best_loss=8.43285
Epoch 11/80: current_loss=8.48568 | best_loss=8.43285
Epoch 12/80: current_loss=8.64581 | best_loss=8.43285
Epoch 13/80: current_loss=8.45054 | best_loss=8.43285
Epoch 14/80: current_loss=8.36529 | best_loss=8.36529
Epoch 15/80: current_loss=8.60707 | best_loss=8.36529
Epoch 16/80: current_loss=8.34617 | best_loss=8.34617
Epoch 17/80: current_loss=8.56077 | best_loss=8.34617
Epoch 18/80: current_loss=8.65253 | best_loss=8.34617
Epoch 19/80: current_loss=8.68897 | best_loss=8.34617
Epoch 20/80: current_loss=8.58466 | best_loss=8.34617
Epoch 21/80: current_loss=8.52478 | best_loss=8.34617
Epoch 22/80: current_loss=8.94064 | best_loss=8.34617
Epoch 23/80: current_loss=8.78907 | best_loss=8.34617
Epoch 24/80: current_loss=8.67322 | best_loss=8.34617
Epoch 25/80: current_loss=8.53194 | best_loss=8.34617
Epoch 26/80: current_loss=8.90503 | best_loss=8.34617
Epoch 27/80: current_loss=8.57143 | best_loss=8.34617
Epoch 28/80: current_loss=8.61738 | best_loss=8.34617
Epoch 29/80: current_loss=8.80622 | best_loss=8.34617
Epoch 30/80: current_loss=8.64509 | best_loss=8.34617
Epoch 31/80: current_loss=8.53672 | best_loss=8.34617
Epoch 32/80: current_loss=8.59251 | best_loss=8.34617
Epoch 33/80: current_loss=8.57324 | best_loss=8.34617
Epoch 34/80: current_loss=8.73173 | best_loss=8.34617
Epoch 35/80: current_loss=8.66134 | best_loss=8.34617
Epoch 36/80: current_loss=8.57393 | best_loss=8.34617
Early Stopping at epoch 36
      explained_var=0.01483 | mse_loss=8.07945
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.00753| avg_loss=8.76780
----------------------------------------------


----------------------------------------------
Params for Trial 35
{'learning_rate': 0.0001, 'weight_decay': 0.0008109684701319025, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=70.67601 | best_loss=70.67601
Epoch 1/80: current_loss=51.24199 | best_loss=51.24199
Epoch 2/80: current_loss=35.88204 | best_loss=35.88204
Epoch 3/80: current_loss=28.43251 | best_loss=28.43251
Epoch 4/80: current_loss=24.15271 | best_loss=24.15271
Epoch 5/80: current_loss=21.34098 | best_loss=21.34098
Epoch 6/80: current_loss=19.26838 | best_loss=19.26838
Epoch 7/80: current_loss=17.67812 | best_loss=17.67812
Epoch 8/80: current_loss=16.34718 | best_loss=16.34718
Epoch 9/80: current_loss=15.27226 | best_loss=15.27226
Epoch 10/80: current_loss=14.38048 | best_loss=14.38048
Epoch 11/80: current_loss=13.64403 | best_loss=13.64403
Epoch 12/80: current_loss=13.03501 | best_loss=13.03501
Epoch 13/80: current_loss=12.52270 | best_loss=12.52270
Epoch 14/80: current_loss=12.09468 | best_loss=12.09468
Epoch 15/80: current_loss=11.75017 | best_loss=11.75017
Epoch 16/80: current_loss=11.47936 | best_loss=11.47936
Epoch 17/80: current_loss=11.23855 | best_loss=11.23855
Epoch 18/80: current_loss=11.04540 | best_loss=11.04540
Epoch 19/80: current_loss=10.89392 | best_loss=10.89392
Epoch 20/80: current_loss=10.77294 | best_loss=10.77294
Epoch 21/80: current_loss=10.67363 | best_loss=10.67363
Epoch 22/80: current_loss=10.59718 | best_loss=10.59718
Epoch 23/80: current_loss=10.53037 | best_loss=10.53037
Epoch 24/80: current_loss=10.47278 | best_loss=10.47278
Epoch 25/80: current_loss=10.42214 | best_loss=10.42214
Epoch 26/80: current_loss=10.37794 | best_loss=10.37794
Epoch 27/80: current_loss=10.34063 | best_loss=10.34063
Epoch 28/80: current_loss=10.30639 | best_loss=10.30639
Epoch 29/80: current_loss=10.27630 | best_loss=10.27630
Epoch 30/80: current_loss=10.25130 | best_loss=10.25130
Epoch 31/80: current_loss=10.22933 | best_loss=10.22933
Epoch 32/80: current_loss=10.20596 | best_loss=10.20596
Epoch 33/80: current_loss=10.18578 | best_loss=10.18578
Epoch 34/80: current_loss=10.16632 | best_loss=10.16632
Epoch 35/80: current_loss=10.14879 | best_loss=10.14879
Epoch 36/80: current_loss=10.13334 | best_loss=10.13334
Epoch 37/80: current_loss=10.12248 | best_loss=10.12248
Epoch 38/80: current_loss=10.10929 | best_loss=10.10929
Epoch 39/80: current_loss=10.09408 | best_loss=10.09408
Epoch 40/80: current_loss=10.08101 | best_loss=10.08101
Epoch 41/80: current_loss=10.06870 | best_loss=10.06870
Epoch 42/80: current_loss=10.05615 | best_loss=10.05615
Epoch 43/80: current_loss=10.04503 | best_loss=10.04503
Epoch 44/80: current_loss=10.03451 | best_loss=10.03451
Epoch 45/80: current_loss=10.02608 | best_loss=10.02608
Epoch 46/80: current_loss=10.01636 | best_loss=10.01636
Epoch 47/80: current_loss=10.00821 | best_loss=10.00821
Epoch 48/80: current_loss=10.00023 | best_loss=10.00023
Epoch 49/80: current_loss=9.99389 | best_loss=9.99389
Epoch 50/80: current_loss=9.98779 | best_loss=9.98779
Epoch 51/80: current_loss=9.98279 | best_loss=9.98279
Epoch 52/80: current_loss=9.97885 | best_loss=9.97885
Epoch 53/80: current_loss=9.97298 | best_loss=9.97298
Epoch 54/80: current_loss=9.96479 | best_loss=9.96479
Epoch 55/80: current_loss=9.96161 | best_loss=9.96161
Epoch 56/80: current_loss=9.95692 | best_loss=9.95692
Epoch 57/80: current_loss=9.95077 | best_loss=9.95077
Epoch 58/80: current_loss=9.94618 | best_loss=9.94618
Epoch 59/80: current_loss=9.94247 | best_loss=9.94247
Epoch 60/80: current_loss=9.94030 | best_loss=9.94030
Epoch 61/80: current_loss=9.93794 | best_loss=9.93794
Epoch 62/80: current_loss=9.93275 | best_loss=9.93275
Epoch 63/80: current_loss=9.92947 | best_loss=9.92947
Epoch 64/80: current_loss=9.92588 | best_loss=9.92588
Epoch 65/80: current_loss=9.92508 | best_loss=9.92508
Epoch 66/80: current_loss=9.92232 | best_loss=9.92232
Epoch 67/80: current_loss=9.91956 | best_loss=9.91956
Epoch 68/80: current_loss=9.91647 | best_loss=9.91647
Epoch 69/80: current_loss=9.91327 | best_loss=9.91327
Epoch 70/80: current_loss=9.90998 | best_loss=9.90998
Epoch 71/80: current_loss=9.91037 | best_loss=9.90998
Epoch 72/80: current_loss=9.90792 | best_loss=9.90792
Epoch 73/80: current_loss=9.90811 | best_loss=9.90792
Epoch 74/80: current_loss=9.90779 | best_loss=9.90779
Epoch 75/80: current_loss=9.90524 | best_loss=9.90524
Epoch 76/80: current_loss=9.90558 | best_loss=9.90524
Epoch 77/80: current_loss=9.90380 | best_loss=9.90380
Epoch 78/80: current_loss=9.90509 | best_loss=9.90380
Epoch 79/80: current_loss=9.90547 | best_loss=9.90380
      explained_var=0.00180 | mse_loss=9.57044
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.46967 | best_loss=8.46967
Epoch 1/80: current_loss=8.47442 | best_loss=8.46967
Epoch 2/80: current_loss=8.47575 | best_loss=8.46967
Epoch 3/80: current_loss=8.48575 | best_loss=8.46967
Epoch 4/80: current_loss=8.48617 | best_loss=8.46967
Epoch 5/80: current_loss=8.48345 | best_loss=8.46967
Epoch 6/80: current_loss=8.47927 | best_loss=8.46967
Epoch 7/80: current_loss=8.48173 | best_loss=8.46967
Epoch 8/80: current_loss=8.48754 | best_loss=8.46967
Epoch 9/80: current_loss=8.49016 | best_loss=8.46967
Epoch 10/80: current_loss=8.48920 | best_loss=8.46967
Epoch 11/80: current_loss=8.48544 | best_loss=8.46967
Epoch 12/80: current_loss=8.48759 | best_loss=8.46967
Epoch 13/80: current_loss=8.49206 | best_loss=8.46967
Epoch 14/80: current_loss=8.48911 | best_loss=8.46967
Epoch 15/80: current_loss=8.49536 | best_loss=8.46967
Epoch 16/80: current_loss=8.49168 | best_loss=8.46967
Epoch 17/80: current_loss=8.48852 | best_loss=8.46967
Epoch 18/80: current_loss=8.48526 | best_loss=8.46967
Epoch 19/80: current_loss=8.49270 | best_loss=8.46967
Epoch 20/80: current_loss=8.49780 | best_loss=8.46967
Early Stopping at epoch 20
      explained_var=0.00112 | mse_loss=8.57556

----------------------------------------------
Params for Trial 36
{'learning_rate': 0.01, 'weight_decay': 0.0012947321176852565, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.26345 | best_loss=10.26345
Epoch 1/80: current_loss=9.91153 | best_loss=9.91153
Epoch 2/80: current_loss=9.94216 | best_loss=9.91153
Epoch 3/80: current_loss=10.01919 | best_loss=9.91153
Epoch 4/80: current_loss=9.90779 | best_loss=9.90779
Epoch 5/80: current_loss=9.90854 | best_loss=9.90779
Epoch 6/80: current_loss=9.91624 | best_loss=9.90779
Epoch 7/80: current_loss=9.94131 | best_loss=9.90779
Epoch 8/80: current_loss=9.92382 | best_loss=9.90779
Epoch 9/80: current_loss=9.92761 | best_loss=9.90779
Epoch 10/80: current_loss=9.94009 | best_loss=9.90779
Epoch 11/80: current_loss=9.92621 | best_loss=9.90779
Epoch 12/80: current_loss=9.91917 | best_loss=9.90779
Epoch 13/80: current_loss=9.93293 | best_loss=9.90779
Epoch 14/80: current_loss=10.00155 | best_loss=9.90779
Epoch 15/80: current_loss=9.91429 | best_loss=9.90779
Epoch 16/80: current_loss=9.96976 | best_loss=9.90779
Epoch 17/80: current_loss=9.96823 | best_loss=9.90779
Epoch 18/80: current_loss=10.07247 | best_loss=9.90779
Epoch 19/80: current_loss=9.90446 | best_loss=9.90446
Epoch 20/80: current_loss=10.09895 | best_loss=9.90446
Epoch 21/80: current_loss=9.91201 | best_loss=9.90446
Epoch 22/80: current_loss=10.07259 | best_loss=9.90446
Epoch 23/80: current_loss=9.90916 | best_loss=9.90446
Epoch 24/80: current_loss=9.98353 | best_loss=9.90446
Epoch 25/80: current_loss=9.90325 | best_loss=9.90325
Epoch 26/80: current_loss=9.96059 | best_loss=9.90325
Epoch 27/80: current_loss=9.90673 | best_loss=9.90325
Epoch 28/80: current_loss=9.90699 | best_loss=9.90325
Epoch 29/80: current_loss=9.97285 | best_loss=9.90325
Epoch 30/80: current_loss=9.94942 | best_loss=9.90325
Epoch 31/80: current_loss=10.03809 | best_loss=9.90325
Epoch 32/80: current_loss=10.01586 | best_loss=9.90325
Epoch 33/80: current_loss=9.92395 | best_loss=9.90325
Epoch 34/80: current_loss=9.90696 | best_loss=9.90325
Epoch 35/80: current_loss=10.00924 | best_loss=9.90325
Epoch 36/80: current_loss=9.92774 | best_loss=9.90325
Epoch 37/80: current_loss=10.07908 | best_loss=9.90325
Epoch 38/80: current_loss=9.97797 | best_loss=9.90325
Epoch 39/80: current_loss=9.92039 | best_loss=9.90325
Epoch 40/80: current_loss=9.90544 | best_loss=9.90325
Epoch 41/80: current_loss=9.93202 | best_loss=9.90325
Epoch 42/80: current_loss=9.91235 | best_loss=9.90325
Epoch 43/80: current_loss=9.90401 | best_loss=9.90325
Epoch 44/80: current_loss=9.90938 | best_loss=9.90325
Epoch 45/80: current_loss=9.90555 | best_loss=9.90325
Early Stopping at epoch 45
      explained_var=0.00063 | mse_loss=9.57712

----------------------------------------------
Params for Trial 37
{'learning_rate': 0.01, 'weight_decay': 0.0006242417358635384, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=11.08127 | best_loss=11.08127
Epoch 1/80: current_loss=11.76444 | best_loss=11.08127
Epoch 2/80: current_loss=10.00914 | best_loss=10.00914
Epoch 3/80: current_loss=10.02611 | best_loss=10.00914
Epoch 4/80: current_loss=10.57752 | best_loss=10.00914
Epoch 5/80: current_loss=9.89542 | best_loss=9.89542
Epoch 6/80: current_loss=11.15029 | best_loss=9.89542
Epoch 7/80: current_loss=10.06075 | best_loss=9.89542
Epoch 8/80: current_loss=10.07367 | best_loss=9.89542
Epoch 9/80: current_loss=10.20795 | best_loss=9.89542
Epoch 10/80: current_loss=10.73631 | best_loss=9.89542
Epoch 11/80: current_loss=9.90928 | best_loss=9.89542
Epoch 12/80: current_loss=10.16186 | best_loss=9.89542
Epoch 13/80: current_loss=14.16229 | best_loss=9.89542
Epoch 14/80: current_loss=12.79275 | best_loss=9.89542
Epoch 15/80: current_loss=10.46905 | best_loss=9.89542
Epoch 16/80: current_loss=10.02972 | best_loss=9.89542
Epoch 17/80: current_loss=11.49965 | best_loss=9.89542
Epoch 18/80: current_loss=10.08281 | best_loss=9.89542
Epoch 19/80: current_loss=10.08596 | best_loss=9.89542
Epoch 20/80: current_loss=12.41313 | best_loss=9.89542
Epoch 21/80: current_loss=9.91453 | best_loss=9.89542
Epoch 22/80: current_loss=11.74141 | best_loss=9.89542
Epoch 23/80: current_loss=10.56095 | best_loss=9.89542
Epoch 24/80: current_loss=9.89527 | best_loss=9.89527
Epoch 25/80: current_loss=10.07445 | best_loss=9.89527
Epoch 26/80: current_loss=10.30630 | best_loss=9.89527
Epoch 27/80: current_loss=10.97028 | best_loss=9.89527
Epoch 28/80: current_loss=10.39644 | best_loss=9.89527
Epoch 29/80: current_loss=9.98187 | best_loss=9.89527
Epoch 30/80: current_loss=9.89406 | best_loss=9.89406
Epoch 31/80: current_loss=10.90779 | best_loss=9.89406
Epoch 32/80: current_loss=10.07954 | best_loss=9.89406
Epoch 33/80: current_loss=10.40086 | best_loss=9.89406
Epoch 34/80: current_loss=10.01854 | best_loss=9.89406
Epoch 35/80: current_loss=9.94405 | best_loss=9.89406
Epoch 36/80: current_loss=12.74135 | best_loss=9.89406
Epoch 37/80: current_loss=9.89059 | best_loss=9.89059
Epoch 38/80: current_loss=10.20905 | best_loss=9.89059
Epoch 39/80: current_loss=10.64298 | best_loss=9.89059
Epoch 40/80: current_loss=10.75521 | best_loss=9.89059
Epoch 41/80: current_loss=11.30588 | best_loss=9.89059
Epoch 42/80: current_loss=9.87720 | best_loss=9.87720
Epoch 43/80: current_loss=12.29909 | best_loss=9.87720
Epoch 44/80: current_loss=11.25955 | best_loss=9.87720
Epoch 45/80: current_loss=12.77436 | best_loss=9.87720
Epoch 46/80: current_loss=10.88361 | best_loss=9.87720
Epoch 47/80: current_loss=11.06742 | best_loss=9.87720
Epoch 48/80: current_loss=10.14841 | best_loss=9.87720
Epoch 49/80: current_loss=11.93007 | best_loss=9.87720
Epoch 50/80: current_loss=12.95716 | best_loss=9.87720
Epoch 51/80: current_loss=9.90063 | best_loss=9.87720
Epoch 52/80: current_loss=11.48649 | best_loss=9.87720
Epoch 53/80: current_loss=10.31236 | best_loss=9.87720
Epoch 54/80: current_loss=10.87545 | best_loss=9.87720
Epoch 55/80: current_loss=10.30741 | best_loss=9.87720
Epoch 56/80: current_loss=10.32664 | best_loss=9.87720
Epoch 57/80: current_loss=9.96511 | best_loss=9.87720
Epoch 58/80: current_loss=10.29878 | best_loss=9.87720
Epoch 59/80: current_loss=10.06178 | best_loss=9.87720
Epoch 60/80: current_loss=9.89258 | best_loss=9.87720
Epoch 61/80: current_loss=10.70173 | best_loss=9.87720
Epoch 62/80: current_loss=10.09310 | best_loss=9.87720
Early Stopping at epoch 62
      explained_var=0.00270 | mse_loss=9.56147
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.59637 | best_loss=8.59637
Epoch 1/80: current_loss=10.08283 | best_loss=8.59637
Epoch 2/80: current_loss=9.55527 | best_loss=8.59637
Epoch 3/80: current_loss=8.58330 | best_loss=8.58330
Epoch 4/80: current_loss=9.60411 | best_loss=8.58330
Epoch 5/80: current_loss=10.92491 | best_loss=8.58330
Epoch 6/80: current_loss=8.56375 | best_loss=8.56375
Epoch 7/80: current_loss=10.07879 | best_loss=8.56375
Epoch 8/80: current_loss=8.43562 | best_loss=8.43562
Epoch 9/80: current_loss=8.63178 | best_loss=8.43562
Epoch 10/80: current_loss=8.95531 | best_loss=8.43562
Epoch 11/80: current_loss=8.72282 | best_loss=8.43562
Epoch 12/80: current_loss=8.85988 | best_loss=8.43562
Epoch 13/80: current_loss=8.60820 | best_loss=8.43562
Epoch 14/80: current_loss=8.50262 | best_loss=8.43562
Epoch 15/80: current_loss=8.53931 | best_loss=8.43562
Epoch 16/80: current_loss=8.68664 | best_loss=8.43562
Epoch 17/80: current_loss=10.17727 | best_loss=8.43562
Epoch 18/80: current_loss=9.25955 | best_loss=8.43562
Epoch 19/80: current_loss=8.72698 | best_loss=8.43562
Epoch 20/80: current_loss=8.95809 | best_loss=8.43562
Epoch 21/80: current_loss=8.73049 | best_loss=8.43562
Epoch 22/80: current_loss=9.05925 | best_loss=8.43562
Epoch 23/80: current_loss=8.81468 | best_loss=8.43562
Epoch 24/80: current_loss=8.64641 | best_loss=8.43562
Epoch 25/80: current_loss=8.52162 | best_loss=8.43562
Epoch 26/80: current_loss=9.20003 | best_loss=8.43562
Epoch 27/80: current_loss=13.13512 | best_loss=8.43562
Epoch 28/80: current_loss=10.84093 | best_loss=8.43562
Early Stopping at epoch 28
      explained_var=0.00206 | mse_loss=8.56724
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=11.78047 | best_loss=11.78047
Epoch 1/80: current_loss=11.33185 | best_loss=11.33185
Epoch 2/80: current_loss=9.33967 | best_loss=9.33967
Epoch 3/80: current_loss=10.67643 | best_loss=9.33967
Epoch 4/80: current_loss=27.38722 | best_loss=9.33967
Epoch 5/80: current_loss=9.57214 | best_loss=9.33967
Epoch 6/80: current_loss=11.68760 | best_loss=9.33967
Epoch 7/80: current_loss=9.56427 | best_loss=9.33967
Epoch 8/80: current_loss=10.50556 | best_loss=9.33967
Epoch 9/80: current_loss=10.36511 | best_loss=9.33967
Epoch 10/80: current_loss=9.63265 | best_loss=9.33967
Epoch 11/80: current_loss=9.23790 | best_loss=9.23790
Epoch 12/80: current_loss=11.97954 | best_loss=9.23790
Epoch 13/80: current_loss=9.92016 | best_loss=9.23790
Epoch 14/80: current_loss=13.34585 | best_loss=9.23790
Epoch 15/80: current_loss=9.69613 | best_loss=9.23790
Epoch 16/80: current_loss=9.36320 | best_loss=9.23790
Epoch 17/80: current_loss=10.08418 | best_loss=9.23790
Epoch 18/80: current_loss=9.46010 | best_loss=9.23790
Epoch 19/80: current_loss=9.30283 | best_loss=9.23790
Epoch 20/80: current_loss=9.44079 | best_loss=9.23790
Epoch 21/80: current_loss=9.27931 | best_loss=9.23790
Epoch 22/80: current_loss=9.37182 | best_loss=9.23790
Epoch 23/80: current_loss=12.31900 | best_loss=9.23790
Epoch 24/80: current_loss=11.60426 | best_loss=9.23790
Epoch 25/80: current_loss=10.78298 | best_loss=9.23790
Epoch 26/80: current_loss=10.20425 | best_loss=9.23790
Epoch 27/80: current_loss=10.35547 | best_loss=9.23790
Epoch 28/80: current_loss=9.26544 | best_loss=9.23790
Epoch 29/80: current_loss=9.67042 | best_loss=9.23790
Epoch 30/80: current_loss=9.29724 | best_loss=9.23790
Epoch 31/80: current_loss=9.33429 | best_loss=9.23790
Early Stopping at epoch 31
      explained_var=0.00983 | mse_loss=9.28624
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.15603 | best_loss=9.15603
Epoch 1/80: current_loss=9.02602 | best_loss=9.02602
Epoch 2/80: current_loss=8.94636 | best_loss=8.94636
Epoch 3/80: current_loss=8.93494 | best_loss=8.93494
Epoch 4/80: current_loss=9.24250 | best_loss=8.93494
Epoch 5/80: current_loss=11.83637 | best_loss=8.93494
Epoch 6/80: current_loss=9.47192 | best_loss=8.93494
Epoch 7/80: current_loss=9.48476 | best_loss=8.93494
Epoch 8/80: current_loss=8.94388 | best_loss=8.93494
Epoch 9/80: current_loss=9.57149 | best_loss=8.93494
Epoch 10/80: current_loss=9.09873 | best_loss=8.93494
Epoch 11/80: current_loss=9.02293 | best_loss=8.93494
Epoch 12/80: current_loss=8.99694 | best_loss=8.93494
Epoch 13/80: current_loss=11.56731 | best_loss=8.93494
Epoch 14/80: current_loss=10.04071 | best_loss=8.93494
Epoch 15/80: current_loss=9.22388 | best_loss=8.93494
Epoch 16/80: current_loss=9.05851 | best_loss=8.93494
Epoch 17/80: current_loss=8.96421 | best_loss=8.93494
Epoch 18/80: current_loss=11.72953 | best_loss=8.93494
Epoch 19/80: current_loss=9.55427 | best_loss=8.93494
Epoch 20/80: current_loss=8.92635 | best_loss=8.92635
Epoch 21/80: current_loss=9.16452 | best_loss=8.92635
Epoch 22/80: current_loss=9.00631 | best_loss=8.92635
Epoch 23/80: current_loss=9.78673 | best_loss=8.92635
Epoch 24/80: current_loss=9.29869 | best_loss=8.92635
Epoch 25/80: current_loss=9.17983 | best_loss=8.92635
Epoch 26/80: current_loss=12.98338 | best_loss=8.92635
Epoch 27/80: current_loss=8.94546 | best_loss=8.92635
Epoch 28/80: current_loss=9.05914 | best_loss=8.92635
Epoch 29/80: current_loss=12.96399 | best_loss=8.92635
Epoch 30/80: current_loss=10.10539 | best_loss=8.92635
Epoch 31/80: current_loss=9.24759 | best_loss=8.92635
Epoch 32/80: current_loss=9.86900 | best_loss=8.92635
Epoch 33/80: current_loss=9.07323 | best_loss=8.92635
Epoch 34/80: current_loss=9.24465 | best_loss=8.92635
Epoch 35/80: current_loss=9.10339 | best_loss=8.92635
Epoch 36/80: current_loss=9.25835 | best_loss=8.92635
Epoch 37/80: current_loss=10.10038 | best_loss=8.92635
Epoch 38/80: current_loss=11.12079 | best_loss=8.92635
Epoch 39/80: current_loss=10.10046 | best_loss=8.92635
Epoch 40/80: current_loss=9.20289 | best_loss=8.92635
Early Stopping at epoch 40
      explained_var=0.00180 | mse_loss=8.36545

----------------------------------------------
Params for Trial 38
{'learning_rate': 0.1, 'weight_decay': 0.002273137603089439, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.00141 | best_loss=10.00141
Epoch 1/80: current_loss=10.17982 | best_loss=10.00141
Epoch 2/80: current_loss=9.91120 | best_loss=9.91120
Epoch 3/80: current_loss=9.90456 | best_loss=9.90456
Epoch 4/80: current_loss=24.95703 | best_loss=9.90456
Epoch 5/80: current_loss=10.50754 | best_loss=9.90456
Epoch 6/80: current_loss=11.04953 | best_loss=9.90456
Epoch 7/80: current_loss=10.02778 | best_loss=9.90456
Epoch 8/80: current_loss=11.27686 | best_loss=9.90456
Epoch 9/80: current_loss=9.90016 | best_loss=9.90016
Epoch 10/80: current_loss=11.24884 | best_loss=9.90016
Epoch 11/80: current_loss=11.90519 | best_loss=9.90016
Epoch 12/80: current_loss=9.89953 | best_loss=9.89953
Epoch 13/80: current_loss=10.12446 | best_loss=9.89953
Epoch 14/80: current_loss=10.04686 | best_loss=9.89953
Epoch 15/80: current_loss=11.06030 | best_loss=9.89953
Epoch 16/80: current_loss=10.74413 | best_loss=9.89953
Epoch 17/80: current_loss=10.32990 | best_loss=9.89953
Epoch 18/80: current_loss=9.89532 | best_loss=9.89532
Epoch 19/80: current_loss=9.91503 | best_loss=9.89532
Epoch 20/80: current_loss=10.28905 | best_loss=9.89532
Epoch 21/80: current_loss=9.89835 | best_loss=9.89532
Epoch 22/80: current_loss=10.04667 | best_loss=9.89532
Epoch 23/80: current_loss=10.30232 | best_loss=9.89532
Epoch 24/80: current_loss=10.17704 | best_loss=9.89532
Epoch 25/80: current_loss=10.34420 | best_loss=9.89532
Epoch 26/80: current_loss=22.35745 | best_loss=9.89532
Epoch 27/80: current_loss=10.25975 | best_loss=9.89532
Epoch 28/80: current_loss=9.88110 | best_loss=9.88110
Epoch 29/80: current_loss=10.83013 | best_loss=9.88110
Epoch 30/80: current_loss=13.22302 | best_loss=9.88110
Epoch 31/80: current_loss=12.65856 | best_loss=9.88110
Epoch 32/80: current_loss=10.13761 | best_loss=9.88110
Epoch 33/80: current_loss=10.31056 | best_loss=9.88110
Epoch 34/80: current_loss=11.28111 | best_loss=9.88110
Epoch 35/80: current_loss=10.13919 | best_loss=9.88110
Epoch 36/80: current_loss=10.60199 | best_loss=9.88110
Epoch 37/80: current_loss=12.08193 | best_loss=9.88110
Epoch 38/80: current_loss=14.66585 | best_loss=9.88110
Epoch 39/80: current_loss=10.94065 | best_loss=9.88110
Epoch 40/80: current_loss=11.60935 | best_loss=9.88110
Epoch 41/80: current_loss=12.14182 | best_loss=9.88110
Epoch 42/80: current_loss=10.34806 | best_loss=9.88110
Epoch 43/80: current_loss=9.92943 | best_loss=9.88110
Epoch 44/80: current_loss=13.30917 | best_loss=9.88110
Epoch 45/80: current_loss=9.89099 | best_loss=9.88110
Epoch 46/80: current_loss=11.25071 | best_loss=9.88110
Epoch 47/80: current_loss=9.90791 | best_loss=9.88110
Epoch 48/80: current_loss=11.74698 | best_loss=9.88110
Early Stopping at epoch 48
      explained_var=0.00149 | mse_loss=9.61462

----------------------------------------------
Params for Trial 39
{'learning_rate': 0.0001, 'weight_decay': 0.0005049159454277711, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=56.84994 | best_loss=56.84994
Epoch 1/80: current_loss=25.76628 | best_loss=25.76628
Epoch 2/80: current_loss=17.95029 | best_loss=17.95029
Epoch 3/80: current_loss=14.85443 | best_loss=14.85443
Epoch 4/80: current_loss=13.22153 | best_loss=13.22153
Epoch 5/80: current_loss=12.28935 | best_loss=12.28935
Epoch 6/80: current_loss=11.76568 | best_loss=11.76568
Epoch 7/80: current_loss=11.45829 | best_loss=11.45829
Epoch 8/80: current_loss=11.28550 | best_loss=11.28550
Epoch 9/80: current_loss=11.14626 | best_loss=11.14626
Epoch 10/80: current_loss=11.04173 | best_loss=11.04173
Epoch 11/80: current_loss=10.95946 | best_loss=10.95946
Epoch 12/80: current_loss=10.89903 | best_loss=10.89903
Epoch 13/80: current_loss=10.83442 | best_loss=10.83442
Epoch 14/80: current_loss=10.78180 | best_loss=10.78180
Epoch 15/80: current_loss=10.73009 | best_loss=10.73009
Epoch 16/80: current_loss=10.68425 | best_loss=10.68425
Epoch 17/80: current_loss=10.65403 | best_loss=10.65403
Epoch 18/80: current_loss=10.60883 | best_loss=10.60883
Epoch 19/80: current_loss=10.56719 | best_loss=10.56719
Epoch 20/80: current_loss=10.52206 | best_loss=10.52206
Epoch 21/80: current_loss=10.48421 | best_loss=10.48421
Epoch 22/80: current_loss=10.45066 | best_loss=10.45066
Epoch 23/80: current_loss=10.41495 | best_loss=10.41495
Epoch 24/80: current_loss=10.38778 | best_loss=10.38778
Epoch 25/80: current_loss=10.34863 | best_loss=10.34863
Epoch 26/80: current_loss=10.31614 | best_loss=10.31614
Epoch 27/80: current_loss=10.29244 | best_loss=10.29244
Epoch 28/80: current_loss=10.25693 | best_loss=10.25693
Epoch 29/80: current_loss=10.22865 | best_loss=10.22865
Epoch 30/80: current_loss=10.20453 | best_loss=10.20453
Epoch 31/80: current_loss=10.18300 | best_loss=10.18300
Epoch 32/80: current_loss=10.16806 | best_loss=10.16806
Epoch 33/80: current_loss=10.14279 | best_loss=10.14279
Epoch 34/80: current_loss=10.13072 | best_loss=10.13072
Epoch 35/80: current_loss=10.11353 | best_loss=10.11353
Epoch 36/80: current_loss=10.09756 | best_loss=10.09756
Epoch 37/80: current_loss=10.07823 | best_loss=10.07823
Epoch 38/80: current_loss=10.07021 | best_loss=10.07021
Epoch 39/80: current_loss=10.05316 | best_loss=10.05316
Epoch 40/80: current_loss=10.04113 | best_loss=10.04113
Epoch 41/80: current_loss=10.02934 | best_loss=10.02934
Epoch 42/80: current_loss=10.01799 | best_loss=10.01799
Epoch 43/80: current_loss=10.01365 | best_loss=10.01365
Epoch 44/80: current_loss=9.99884 | best_loss=9.99884
Epoch 45/80: current_loss=9.99635 | best_loss=9.99635
Epoch 46/80: current_loss=9.98214 | best_loss=9.98214
Epoch 47/80: current_loss=9.97047 | best_loss=9.97047
Epoch 48/80: current_loss=9.96148 | best_loss=9.96148
Epoch 49/80: current_loss=9.95623 | best_loss=9.95623
Epoch 50/80: current_loss=9.95250 | best_loss=9.95250
Epoch 51/80: current_loss=9.94525 | best_loss=9.94525
Epoch 52/80: current_loss=9.93895 | best_loss=9.93895
Epoch 53/80: current_loss=9.93576 | best_loss=9.93576
Epoch 54/80: current_loss=9.93167 | best_loss=9.93167
Epoch 55/80: current_loss=9.92903 | best_loss=9.92903
Epoch 56/80: current_loss=9.92581 | best_loss=9.92581
Epoch 57/80: current_loss=9.92799 | best_loss=9.92581
Epoch 58/80: current_loss=9.91815 | best_loss=9.91815
Epoch 59/80: current_loss=9.91723 | best_loss=9.91723
Epoch 60/80: current_loss=9.91948 | best_loss=9.91723
Epoch 61/80: current_loss=9.92345 | best_loss=9.91723
Epoch 62/80: current_loss=9.92197 | best_loss=9.91723
Epoch 63/80: current_loss=9.91479 | best_loss=9.91479
Epoch 64/80: current_loss=9.91431 | best_loss=9.91431
Epoch 65/80: current_loss=9.90773 | best_loss=9.90773
Epoch 66/80: current_loss=9.90681 | best_loss=9.90681
Epoch 67/80: current_loss=9.90762 | best_loss=9.90681
Epoch 68/80: current_loss=9.90695 | best_loss=9.90681
Epoch 69/80: current_loss=9.90977 | best_loss=9.90681
Epoch 70/80: current_loss=9.90581 | best_loss=9.90581
Epoch 71/80: current_loss=9.90436 | best_loss=9.90436
Epoch 72/80: current_loss=9.90146 | best_loss=9.90146
Epoch 73/80: current_loss=9.90512 | best_loss=9.90146
Epoch 74/80: current_loss=9.90900 | best_loss=9.90146
Epoch 75/80: current_loss=9.89848 | best_loss=9.89848
Epoch 76/80: current_loss=9.90090 | best_loss=9.89848
Epoch 77/80: current_loss=9.89538 | best_loss=9.89538
Epoch 78/80: current_loss=9.89781 | best_loss=9.89538
Epoch 79/80: current_loss=9.90117 | best_loss=9.89538
      explained_var=0.00206 | mse_loss=9.56356
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.47199 | best_loss=8.47199
Epoch 1/80: current_loss=8.47133 | best_loss=8.47133
Epoch 2/80: current_loss=8.47638 | best_loss=8.47133
Epoch 3/80: current_loss=8.47644 | best_loss=8.47133
Epoch 4/80: current_loss=8.47966 | best_loss=8.47133
Epoch 5/80: current_loss=8.48064 | best_loss=8.47133
Epoch 6/80: current_loss=8.47819 | best_loss=8.47133
Epoch 7/80: current_loss=8.47506 | best_loss=8.47133
Epoch 8/80: current_loss=8.47408 | best_loss=8.47133
Epoch 9/80: current_loss=8.47692 | best_loss=8.47133
Epoch 10/80: current_loss=8.48426 | best_loss=8.47133
Epoch 11/80: current_loss=8.48608 | best_loss=8.47133
Epoch 12/80: current_loss=8.48628 | best_loss=8.47133
Epoch 13/80: current_loss=8.48655 | best_loss=8.47133
Epoch 14/80: current_loss=8.49107 | best_loss=8.47133
Epoch 15/80: current_loss=8.49093 | best_loss=8.47133
Epoch 16/80: current_loss=8.49151 | best_loss=8.47133
Epoch 17/80: current_loss=8.48740 | best_loss=8.47133
Epoch 18/80: current_loss=8.48830 | best_loss=8.47133
Epoch 19/80: current_loss=8.49553 | best_loss=8.47133
Epoch 20/80: current_loss=8.49471 | best_loss=8.47133
Epoch 21/80: current_loss=8.49170 | best_loss=8.47133
Early Stopping at epoch 21
      explained_var=0.00151 | mse_loss=8.57105
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.47826 | best_loss=9.47826
Epoch 1/80: current_loss=9.46786 | best_loss=9.46786
Epoch 2/80: current_loss=9.47369 | best_loss=9.46786
Epoch 3/80: current_loss=9.46903 | best_loss=9.46786
Epoch 4/80: current_loss=9.47718 | best_loss=9.46786
Epoch 5/80: current_loss=9.45000 | best_loss=9.45000
Epoch 6/80: current_loss=9.45422 | best_loss=9.45000
Epoch 7/80: current_loss=9.45794 | best_loss=9.45000
Epoch 8/80: current_loss=9.45456 | best_loss=9.45000
Epoch 9/80: current_loss=9.48720 | best_loss=9.45000
Epoch 10/80: current_loss=9.46799 | best_loss=9.45000
Epoch 11/80: current_loss=9.48312 | best_loss=9.45000
Epoch 12/80: current_loss=9.46659 | best_loss=9.45000
Epoch 13/80: current_loss=9.45734 | best_loss=9.45000
Epoch 14/80: current_loss=9.44386 | best_loss=9.44386
Epoch 15/80: current_loss=9.43362 | best_loss=9.43362
Epoch 16/80: current_loss=9.45402 | best_loss=9.43362
Epoch 17/80: current_loss=9.46000 | best_loss=9.43362
Epoch 18/80: current_loss=9.45634 | best_loss=9.43362
Epoch 19/80: current_loss=9.50601 | best_loss=9.43362
Epoch 20/80: current_loss=9.48175 | best_loss=9.43362
Epoch 21/80: current_loss=9.49510 | best_loss=9.43362
Epoch 22/80: current_loss=9.44426 | best_loss=9.43362
Epoch 23/80: current_loss=9.47882 | best_loss=9.43362
Epoch 24/80: current_loss=9.46222 | best_loss=9.43362
Epoch 25/80: current_loss=9.47309 | best_loss=9.43362
Epoch 26/80: current_loss=9.45763 | best_loss=9.43362
Epoch 27/80: current_loss=9.48272 | best_loss=9.43362
Epoch 28/80: current_loss=9.44596 | best_loss=9.43362
Epoch 29/80: current_loss=9.46141 | best_loss=9.43362
Epoch 30/80: current_loss=9.43852 | best_loss=9.43362
Epoch 31/80: current_loss=9.45950 | best_loss=9.43362
Epoch 32/80: current_loss=9.47144 | best_loss=9.43362
Epoch 33/80: current_loss=9.46718 | best_loss=9.43362
Epoch 34/80: current_loss=9.47736 | best_loss=9.43362
Epoch 35/80: current_loss=9.45992 | best_loss=9.43362
Early Stopping at epoch 35
      explained_var=-0.00604 | mse_loss=9.57795
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.95219 | best_loss=8.95219
Epoch 1/80: current_loss=8.95790 | best_loss=8.95219
Epoch 2/80: current_loss=8.95883 | best_loss=8.95219
Epoch 3/80: current_loss=8.95840 | best_loss=8.95219
Epoch 4/80: current_loss=8.95586 | best_loss=8.95219
Epoch 5/80: current_loss=8.96675 | best_loss=8.95219
Epoch 6/80: current_loss=8.96437 | best_loss=8.95219
Epoch 7/80: current_loss=8.96167 | best_loss=8.95219
Epoch 8/80: current_loss=8.95969 | best_loss=8.95219
Epoch 9/80: current_loss=8.96187 | best_loss=8.95219
Epoch 10/80: current_loss=8.96165 | best_loss=8.95219
Epoch 11/80: current_loss=8.95449 | best_loss=8.95219
Epoch 12/80: current_loss=8.96591 | best_loss=8.95219
Epoch 13/80: current_loss=8.96569 | best_loss=8.95219
Epoch 14/80: current_loss=8.97107 | best_loss=8.95219
Epoch 15/80: current_loss=8.96683 | best_loss=8.95219
Epoch 16/80: current_loss=8.95822 | best_loss=8.95219
Epoch 17/80: current_loss=8.95702 | best_loss=8.95219
Epoch 18/80: current_loss=8.95901 | best_loss=8.95219
Epoch 19/80: current_loss=8.97182 | best_loss=8.95219
Epoch 20/80: current_loss=8.96304 | best_loss=8.95219
Early Stopping at epoch 20
      explained_var=0.00079 | mse_loss=8.35833
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.48858 | best_loss=8.48858
Epoch 1/80: current_loss=8.48747 | best_loss=8.48747
Epoch 2/80: current_loss=8.48687 | best_loss=8.48687
Epoch 3/80: current_loss=8.48805 | best_loss=8.48687
Epoch 4/80: current_loss=8.48926 | best_loss=8.48687
Epoch 5/80: current_loss=8.48799 | best_loss=8.48687
Epoch 6/80: current_loss=8.48935 | best_loss=8.48687
Epoch 7/80: current_loss=8.48700 | best_loss=8.48687
Epoch 8/80: current_loss=8.48967 | best_loss=8.48687
Epoch 9/80: current_loss=8.48829 | best_loss=8.48687
Epoch 10/80: current_loss=8.49224 | best_loss=8.48687
Epoch 11/80: current_loss=8.48987 | best_loss=8.48687
Epoch 12/80: current_loss=8.49004 | best_loss=8.48687
Epoch 13/80: current_loss=8.48746 | best_loss=8.48687
Epoch 14/80: current_loss=8.49085 | best_loss=8.48687
Epoch 15/80: current_loss=8.48833 | best_loss=8.48687
Epoch 16/80: current_loss=8.49038 | best_loss=8.48687
Epoch 17/80: current_loss=8.48943 | best_loss=8.48687
Epoch 18/80: current_loss=8.49094 | best_loss=8.48687
Epoch 19/80: current_loss=8.49505 | best_loss=8.48687
Epoch 20/80: current_loss=8.48902 | best_loss=8.48687
Epoch 21/80: current_loss=8.48986 | best_loss=8.48687
Epoch 22/80: current_loss=8.49045 | best_loss=8.48687
Early Stopping at epoch 22
      explained_var=0.00006 | mse_loss=8.19999
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=-0.00032| avg_loss=8.85417
----------------------------------------------


----------------------------------------------
Params for Trial 40
{'learning_rate': 1e-05, 'weight_decay': 0.009764968251243388, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=74.57129 | best_loss=74.57129
Epoch 1/80: current_loss=73.51037 | best_loss=73.51037
Epoch 2/80: current_loss=72.40357 | best_loss=72.40357
Epoch 3/80: current_loss=71.17431 | best_loss=71.17431
Epoch 4/80: current_loss=69.75770 | best_loss=69.75770
Epoch 5/80: current_loss=68.09184 | best_loss=68.09184
Epoch 6/80: current_loss=66.09943 | best_loss=66.09943
Epoch 7/80: current_loss=63.72575 | best_loss=63.72575
Epoch 8/80: current_loss=60.94826 | best_loss=60.94826
Epoch 9/80: current_loss=57.80902 | best_loss=57.80902
Epoch 10/80: current_loss=54.46589 | best_loss=54.46589
Epoch 11/80: current_loss=51.10011 | best_loss=51.10011
Epoch 12/80: current_loss=47.97591 | best_loss=47.97591
Epoch 13/80: current_loss=45.17316 | best_loss=45.17316
Epoch 14/80: current_loss=42.71428 | best_loss=42.71428
Epoch 15/80: current_loss=40.59485 | best_loss=40.59485
Epoch 16/80: current_loss=38.75303 | best_loss=38.75303
Epoch 17/80: current_loss=37.15189 | best_loss=37.15189
Epoch 18/80: current_loss=35.72388 | best_loss=35.72388
Epoch 19/80: current_loss=34.46687 | best_loss=34.46687
Epoch 20/80: current_loss=33.33370 | best_loss=33.33370
Epoch 21/80: current_loss=32.31813 | best_loss=32.31813
Epoch 22/80: current_loss=31.38699 | best_loss=31.38699
Epoch 23/80: current_loss=30.54434 | best_loss=30.54434
Epoch 24/80: current_loss=29.77261 | best_loss=29.77261
Epoch 25/80: current_loss=29.05455 | best_loss=29.05455
Epoch 26/80: current_loss=28.38706 | best_loss=28.38706
Epoch 27/80: current_loss=27.76850 | best_loss=27.76850
Epoch 28/80: current_loss=27.19175 | best_loss=27.19175
Epoch 29/80: current_loss=26.64888 | best_loss=26.64888
Epoch 30/80: current_loss=26.13469 | best_loss=26.13469
Epoch 31/80: current_loss=25.65812 | best_loss=25.65812
Epoch 32/80: current_loss=25.20602 | best_loss=25.20602
Epoch 33/80: current_loss=24.78259 | best_loss=24.78259
Epoch 34/80: current_loss=24.37276 | best_loss=24.37276
Epoch 35/80: current_loss=23.98451 | best_loss=23.98451
Epoch 36/80: current_loss=23.61696 | best_loss=23.61696
Epoch 37/80: current_loss=23.26552 | best_loss=23.26552
Epoch 38/80: current_loss=22.93068 | best_loss=22.93068
Epoch 39/80: current_loss=22.60977 | best_loss=22.60977
Epoch 40/80: current_loss=22.29863 | best_loss=22.29863
Epoch 41/80: current_loss=22.00202 | best_loss=22.00202
Epoch 42/80: current_loss=21.70930 | best_loss=21.70930
Epoch 43/80: current_loss=21.43359 | best_loss=21.43359
Epoch 44/80: current_loss=21.16003 | best_loss=21.16003
Epoch 45/80: current_loss=20.89854 | best_loss=20.89854
Epoch 46/80: current_loss=20.64630 | best_loss=20.64630
Epoch 47/80: current_loss=20.40139 | best_loss=20.40139
Epoch 48/80: current_loss=20.16457 | best_loss=20.16457
Epoch 49/80: current_loss=19.93045 | best_loss=19.93045
Epoch 50/80: current_loss=19.70793 | best_loss=19.70793
Epoch 51/80: current_loss=19.48650 | best_loss=19.48650
Epoch 52/80: current_loss=19.27363 | best_loss=19.27363
Epoch 53/80: current_loss=19.07018 | best_loss=19.07018
Epoch 54/80: current_loss=18.87041 | best_loss=18.87041
Epoch 55/80: current_loss=18.67268 | best_loss=18.67268
Epoch 56/80: current_loss=18.47933 | best_loss=18.47933
Epoch 57/80: current_loss=18.29480 | best_loss=18.29480
Epoch 58/80: current_loss=18.11360 | best_loss=18.11360
Epoch 59/80: current_loss=17.93060 | best_loss=17.93060
Epoch 60/80: current_loss=17.75603 | best_loss=17.75603
Epoch 61/80: current_loss=17.58191 | best_loss=17.58191
Epoch 62/80: current_loss=17.41350 | best_loss=17.41350
Epoch 63/80: current_loss=17.24789 | best_loss=17.24789
Epoch 64/80: current_loss=17.08584 | best_loss=17.08584
Epoch 65/80: current_loss=16.92409 | best_loss=16.92409
Epoch 66/80: current_loss=16.77068 | best_loss=16.77068
Epoch 67/80: current_loss=16.61491 | best_loss=16.61491
Epoch 68/80: current_loss=16.46457 | best_loss=16.46457
Epoch 69/80: current_loss=16.32031 | best_loss=16.32031
Epoch 70/80: current_loss=16.17142 | best_loss=16.17142
Epoch 71/80: current_loss=16.02913 | best_loss=16.02913
Epoch 72/80: current_loss=15.88913 | best_loss=15.88913
Epoch 73/80: current_loss=15.75094 | best_loss=15.75094
Epoch 74/80: current_loss=15.61369 | best_loss=15.61369
Epoch 75/80: current_loss=15.47940 | best_loss=15.47940
Epoch 76/80: current_loss=15.34876 | best_loss=15.34876
Epoch 77/80: current_loss=15.21832 | best_loss=15.21832
Epoch 78/80: current_loss=15.09543 | best_loss=15.09543
Epoch 79/80: current_loss=14.96992 | best_loss=14.96992
      explained_var=-0.01940 | mse_loss=14.27077

----------------------------------------------
Params for Trial 41
{'learning_rate': 0.01, 'weight_decay': 0.001559753180127209, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.92358 | best_loss=9.92358
Epoch 1/80: current_loss=9.97288 | best_loss=9.92358
Epoch 2/80: current_loss=10.04361 | best_loss=9.92358
Epoch 3/80: current_loss=9.93437 | best_loss=9.92358
Epoch 4/80: current_loss=9.92198 | best_loss=9.92198
Epoch 5/80: current_loss=10.30694 | best_loss=9.92198
Epoch 6/80: current_loss=9.96428 | best_loss=9.92198
Epoch 7/80: current_loss=9.94389 | best_loss=9.92198
Epoch 8/80: current_loss=10.50445 | best_loss=9.92198
Epoch 9/80: current_loss=9.97704 | best_loss=9.92198
Epoch 10/80: current_loss=10.20079 | best_loss=9.92198
Epoch 11/80: current_loss=10.11304 | best_loss=9.92198
Epoch 12/80: current_loss=11.08691 | best_loss=9.92198
Epoch 13/80: current_loss=10.21515 | best_loss=9.92198
Epoch 14/80: current_loss=9.90540 | best_loss=9.90540
Epoch 15/80: current_loss=10.27398 | best_loss=9.90540
Epoch 16/80: current_loss=10.38422 | best_loss=9.90540
Epoch 17/80: current_loss=10.12125 | best_loss=9.90540
Epoch 18/80: current_loss=10.01679 | best_loss=9.90540
Epoch 19/80: current_loss=10.45219 | best_loss=9.90540
Epoch 20/80: current_loss=9.93261 | best_loss=9.90540
Epoch 21/80: current_loss=10.71140 | best_loss=9.90540
Epoch 22/80: current_loss=9.91734 | best_loss=9.90540
Epoch 23/80: current_loss=9.90463 | best_loss=9.90463
Epoch 24/80: current_loss=10.01056 | best_loss=9.90463
Epoch 25/80: current_loss=9.89236 | best_loss=9.89236
Epoch 26/80: current_loss=9.92698 | best_loss=9.89236
Epoch 27/80: current_loss=10.09636 | best_loss=9.89236
Epoch 28/80: current_loss=10.05099 | best_loss=9.89236
Epoch 29/80: current_loss=9.89816 | best_loss=9.89236
Epoch 30/80: current_loss=9.89596 | best_loss=9.89236
Epoch 31/80: current_loss=9.96405 | best_loss=9.89236
Epoch 32/80: current_loss=10.04704 | best_loss=9.89236
Epoch 33/80: current_loss=10.80076 | best_loss=9.89236
Epoch 34/80: current_loss=10.09699 | best_loss=9.89236
Epoch 35/80: current_loss=10.36623 | best_loss=9.89236
Epoch 36/80: current_loss=9.88902 | best_loss=9.88902
Epoch 37/80: current_loss=10.20331 | best_loss=9.88902
Epoch 38/80: current_loss=10.02003 | best_loss=9.88902
Epoch 39/80: current_loss=10.02622 | best_loss=9.88902
Epoch 40/80: current_loss=9.91975 | best_loss=9.88902
Epoch 41/80: current_loss=9.97432 | best_loss=9.88902
Epoch 42/80: current_loss=9.98092 | best_loss=9.88902
Epoch 43/80: current_loss=9.97806 | best_loss=9.88902
Epoch 44/80: current_loss=10.19394 | best_loss=9.88902
Epoch 45/80: current_loss=9.93940 | best_loss=9.88902
Epoch 46/80: current_loss=10.91213 | best_loss=9.88902
Epoch 47/80: current_loss=10.74265 | best_loss=9.88902
Epoch 48/80: current_loss=9.90275 | best_loss=9.88902
Epoch 49/80: current_loss=9.92714 | best_loss=9.88902
Epoch 50/80: current_loss=9.94341 | best_loss=9.88902
Epoch 51/80: current_loss=9.91313 | best_loss=9.88902
Epoch 52/80: current_loss=9.90389 | best_loss=9.88902
Epoch 53/80: current_loss=10.26756 | best_loss=9.88902
Epoch 54/80: current_loss=10.00592 | best_loss=9.88902
Epoch 55/80: current_loss=10.17326 | best_loss=9.88902
Epoch 56/80: current_loss=10.57723 | best_loss=9.88902
Early Stopping at epoch 56
      explained_var=0.00223 | mse_loss=9.55662
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.55588 | best_loss=8.55588
Epoch 1/80: current_loss=8.72516 | best_loss=8.55588
Epoch 2/80: current_loss=8.57596 | best_loss=8.55588
Epoch 3/80: current_loss=8.68359 | best_loss=8.55588
Epoch 4/80: current_loss=8.45449 | best_loss=8.45449
Epoch 5/80: current_loss=8.90474 | best_loss=8.45449
Epoch 6/80: current_loss=8.47538 | best_loss=8.45449
Epoch 7/80: current_loss=8.52686 | best_loss=8.45449
Epoch 8/80: current_loss=8.79356 | best_loss=8.45449
Epoch 9/80: current_loss=8.76798 | best_loss=8.45449
Epoch 10/80: current_loss=8.54737 | best_loss=8.45449
Epoch 11/80: current_loss=8.56815 | best_loss=8.45449
Epoch 12/80: current_loss=8.64125 | best_loss=8.45449
Epoch 13/80: current_loss=8.49627 | best_loss=8.45449
Epoch 14/80: current_loss=8.51285 | best_loss=8.45449
Epoch 15/80: current_loss=8.51865 | best_loss=8.45449
Epoch 16/80: current_loss=8.75193 | best_loss=8.45449
Epoch 17/80: current_loss=8.53909 | best_loss=8.45449
Epoch 18/80: current_loss=8.56444 | best_loss=8.45449
Epoch 19/80: current_loss=8.53951 | best_loss=8.45449
Epoch 20/80: current_loss=8.66250 | best_loss=8.45449
Epoch 21/80: current_loss=8.66046 | best_loss=8.45449
Epoch 22/80: current_loss=8.59823 | best_loss=8.45449
Epoch 23/80: current_loss=8.54267 | best_loss=8.45449
Epoch 24/80: current_loss=8.54414 | best_loss=8.45449
Early Stopping at epoch 24
      explained_var=0.00311 | mse_loss=8.55022
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.63924 | best_loss=9.63924
Epoch 1/80: current_loss=9.35258 | best_loss=9.35258
Epoch 2/80: current_loss=9.58899 | best_loss=9.35258
Epoch 3/80: current_loss=9.37525 | best_loss=9.35258
Epoch 4/80: current_loss=9.27803 | best_loss=9.27803
Epoch 5/80: current_loss=9.33473 | best_loss=9.27803
Epoch 6/80: current_loss=9.73902 | best_loss=9.27803
Epoch 7/80: current_loss=9.71177 | best_loss=9.27803
Epoch 8/80: current_loss=9.28647 | best_loss=9.27803
Epoch 9/80: current_loss=9.37749 | best_loss=9.27803
Epoch 10/80: current_loss=9.33273 | best_loss=9.27803
Epoch 11/80: current_loss=9.26219 | best_loss=9.26219
Epoch 12/80: current_loss=10.89187 | best_loss=9.26219
Epoch 13/80: current_loss=9.34421 | best_loss=9.26219
Epoch 14/80: current_loss=9.42251 | best_loss=9.26219
Epoch 15/80: current_loss=9.31087 | best_loss=9.26219
Epoch 16/80: current_loss=9.29441 | best_loss=9.26219
Epoch 17/80: current_loss=9.30516 | best_loss=9.26219
Epoch 18/80: current_loss=9.82938 | best_loss=9.26219
Epoch 19/80: current_loss=9.26945 | best_loss=9.26219
Epoch 20/80: current_loss=9.44681 | best_loss=9.26219
Epoch 21/80: current_loss=9.92623 | best_loss=9.26219
Epoch 22/80: current_loss=9.76129 | best_loss=9.26219
Epoch 23/80: current_loss=9.68050 | best_loss=9.26219
Epoch 24/80: current_loss=9.27007 | best_loss=9.26219
Epoch 25/80: current_loss=9.46204 | best_loss=9.26219
Epoch 26/80: current_loss=9.28271 | best_loss=9.26219
Epoch 27/80: current_loss=9.36889 | best_loss=9.26219
Epoch 28/80: current_loss=10.62737 | best_loss=9.26219
Epoch 29/80: current_loss=9.37573 | best_loss=9.26219
Epoch 30/80: current_loss=9.59872 | best_loss=9.26219
Epoch 31/80: current_loss=9.83391 | best_loss=9.26219
Early Stopping at epoch 31
      explained_var=-0.00162 | mse_loss=9.39399
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.20068 | best_loss=9.20068
Epoch 1/80: current_loss=9.23989 | best_loss=9.20068
Epoch 2/80: current_loss=8.96098 | best_loss=8.96098
Epoch 3/80: current_loss=9.01731 | best_loss=8.96098
Epoch 4/80: current_loss=8.97868 | best_loss=8.96098
Epoch 5/80: current_loss=10.48616 | best_loss=8.96098
Epoch 6/80: current_loss=8.91231 | best_loss=8.91231
Epoch 7/80: current_loss=9.10814 | best_loss=8.91231
Epoch 8/80: current_loss=9.04288 | best_loss=8.91231
Epoch 9/80: current_loss=8.94155 | best_loss=8.91231
Epoch 10/80: current_loss=8.94800 | best_loss=8.91231
Epoch 11/80: current_loss=9.24173 | best_loss=8.91231
Epoch 12/80: current_loss=8.91216 | best_loss=8.91216
Epoch 13/80: current_loss=9.04717 | best_loss=8.91216
Epoch 14/80: current_loss=8.98009 | best_loss=8.91216
Epoch 15/80: current_loss=10.45008 | best_loss=8.91216
Epoch 16/80: current_loss=9.04011 | best_loss=8.91216
Epoch 17/80: current_loss=9.14827 | best_loss=8.91216
Epoch 18/80: current_loss=9.00217 | best_loss=8.91216
Epoch 19/80: current_loss=8.95768 | best_loss=8.91216
Epoch 20/80: current_loss=8.99961 | best_loss=8.91216
Epoch 21/80: current_loss=8.94791 | best_loss=8.91216
Epoch 22/80: current_loss=8.94322 | best_loss=8.91216
Epoch 23/80: current_loss=9.40979 | best_loss=8.91216
Epoch 24/80: current_loss=9.21282 | best_loss=8.91216
Epoch 25/80: current_loss=9.02999 | best_loss=8.91216
Epoch 26/80: current_loss=9.02377 | best_loss=8.91216
Epoch 27/80: current_loss=8.98523 | best_loss=8.91216
Epoch 28/80: current_loss=9.11556 | best_loss=8.91216
Epoch 29/80: current_loss=9.71468 | best_loss=8.91216
Epoch 30/80: current_loss=8.93193 | best_loss=8.91216
Epoch 31/80: current_loss=9.01964 | best_loss=8.91216
Epoch 32/80: current_loss=9.03668 | best_loss=8.91216
Early Stopping at epoch 32
      explained_var=0.00274 | mse_loss=8.34569
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.72469 | best_loss=9.72469
Epoch 1/80: current_loss=8.53650 | best_loss=8.53650
Epoch 2/80: current_loss=8.48216 | best_loss=8.48216
Epoch 3/80: current_loss=8.54514 | best_loss=8.48216
Epoch 4/80: current_loss=8.53077 | best_loss=8.48216
Epoch 5/80: current_loss=8.80061 | best_loss=8.48216
Epoch 6/80: current_loss=8.66126 | best_loss=8.48216
Epoch 7/80: current_loss=8.57800 | best_loss=8.48216
Epoch 8/80: current_loss=9.47101 | best_loss=8.48216
Epoch 9/80: current_loss=8.48296 | best_loss=8.48216
Epoch 10/80: current_loss=8.56987 | best_loss=8.48216
Epoch 11/80: current_loss=8.45264 | best_loss=8.45264
Epoch 12/80: current_loss=9.13302 | best_loss=8.45264
Epoch 13/80: current_loss=8.54166 | best_loss=8.45264
Epoch 14/80: current_loss=9.37012 | best_loss=8.45264
Epoch 15/80: current_loss=8.37343 | best_loss=8.37343
Epoch 16/80: current_loss=8.48732 | best_loss=8.37343
Epoch 17/80: current_loss=8.55536 | best_loss=8.37343
Epoch 18/80: current_loss=8.39201 | best_loss=8.37343
Epoch 19/80: current_loss=8.52211 | best_loss=8.37343
Epoch 20/80: current_loss=8.55930 | best_loss=8.37343
Epoch 21/80: current_loss=8.42472 | best_loss=8.37343
Epoch 22/80: current_loss=8.53745 | best_loss=8.37343
Epoch 23/80: current_loss=8.71201 | best_loss=8.37343
Epoch 24/80: current_loss=9.23601 | best_loss=8.37343
Epoch 25/80: current_loss=9.00417 | best_loss=8.37343
Epoch 26/80: current_loss=8.40975 | best_loss=8.37343
Epoch 27/80: current_loss=8.38285 | best_loss=8.37343
Epoch 28/80: current_loss=8.99510 | best_loss=8.37343
Epoch 29/80: current_loss=8.49709 | best_loss=8.37343
Epoch 30/80: current_loss=8.99050 | best_loss=8.37343
Epoch 31/80: current_loss=8.51619 | best_loss=8.37343
Epoch 32/80: current_loss=8.34164 | best_loss=8.34164
Epoch 33/80: current_loss=8.60059 | best_loss=8.34164
Epoch 34/80: current_loss=8.54685 | best_loss=8.34164
Epoch 35/80: current_loss=9.03909 | best_loss=8.34164
Epoch 36/80: current_loss=8.69635 | best_loss=8.34164
Epoch 37/80: current_loss=8.45948 | best_loss=8.34164
Epoch 38/80: current_loss=8.59992 | best_loss=8.34164
Epoch 39/80: current_loss=8.90495 | best_loss=8.34164
Epoch 40/80: current_loss=8.65032 | best_loss=8.34164
Epoch 41/80: current_loss=8.76910 | best_loss=8.34164
Epoch 42/80: current_loss=8.69957 | best_loss=8.34164
Epoch 43/80: current_loss=8.49479 | best_loss=8.34164
Epoch 44/80: current_loss=8.64274 | best_loss=8.34164
Epoch 45/80: current_loss=8.53958 | best_loss=8.34164
Epoch 46/80: current_loss=8.50968 | best_loss=8.34164
Epoch 47/80: current_loss=8.57182 | best_loss=8.34164
Epoch 48/80: current_loss=8.67884 | best_loss=8.34164
Epoch 49/80: current_loss=8.44762 | best_loss=8.34164
Epoch 50/80: current_loss=8.52240 | best_loss=8.34164
Epoch 51/80: current_loss=8.71673 | best_loss=8.34164
Epoch 52/80: current_loss=8.52510 | best_loss=8.34164
Early Stopping at epoch 52
      explained_var=0.00971 | mse_loss=8.12540
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00323| avg_loss=8.79438
----------------------------------------------


----------------------------------------------
Params for Trial 42
{'learning_rate': 0.01, 'weight_decay': 0.0030317865622282152, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91163 | best_loss=9.91163
Epoch 1/80: current_loss=9.91238 | best_loss=9.91163
Epoch 2/80: current_loss=10.02237 | best_loss=9.91163
Epoch 3/80: current_loss=9.90330 | best_loss=9.90330
Epoch 4/80: current_loss=9.93536 | best_loss=9.90330
Epoch 5/80: current_loss=10.18104 | best_loss=9.90330
Epoch 6/80: current_loss=9.90040 | best_loss=9.90040
Epoch 7/80: current_loss=10.07893 | best_loss=9.90040
Epoch 8/80: current_loss=10.44744 | best_loss=9.90040
Epoch 9/80: current_loss=10.18966 | best_loss=9.90040
Epoch 10/80: current_loss=10.11329 | best_loss=9.90040
Epoch 11/80: current_loss=9.91962 | best_loss=9.90040
Epoch 12/80: current_loss=10.85698 | best_loss=9.90040
Epoch 13/80: current_loss=9.97590 | best_loss=9.90040
Epoch 14/80: current_loss=10.15841 | best_loss=9.90040
Epoch 15/80: current_loss=10.09684 | best_loss=9.90040
Epoch 16/80: current_loss=9.91614 | best_loss=9.90040
Epoch 17/80: current_loss=10.00058 | best_loss=9.90040
Epoch 18/80: current_loss=9.92470 | best_loss=9.90040
Epoch 19/80: current_loss=10.67616 | best_loss=9.90040
Epoch 20/80: current_loss=9.91488 | best_loss=9.90040
Epoch 21/80: current_loss=9.97374 | best_loss=9.90040
Epoch 22/80: current_loss=9.91565 | best_loss=9.90040
Epoch 23/80: current_loss=9.91890 | best_loss=9.90040
Epoch 24/80: current_loss=9.90368 | best_loss=9.90040
Epoch 25/80: current_loss=10.66051 | best_loss=9.90040
Epoch 26/80: current_loss=10.19181 | best_loss=9.90040
Early Stopping at epoch 26
      explained_var=0.00095 | mse_loss=9.57401

----------------------------------------------
Params for Trial 43
{'learning_rate': 0.01, 'weight_decay': 0.00457109139470187, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.93067 | best_loss=9.93067
Epoch 1/80: current_loss=9.98081 | best_loss=9.93067
Epoch 2/80: current_loss=10.28053 | best_loss=9.93067
Epoch 3/80: current_loss=9.92531 | best_loss=9.92531
Epoch 4/80: current_loss=9.97997 | best_loss=9.92531
Epoch 5/80: current_loss=9.96460 | best_loss=9.92531
Epoch 6/80: current_loss=9.89112 | best_loss=9.89112
Epoch 7/80: current_loss=10.17277 | best_loss=9.89112
Epoch 8/80: current_loss=9.91052 | best_loss=9.89112
Epoch 9/80: current_loss=11.75559 | best_loss=9.89112
Epoch 10/80: current_loss=9.93057 | best_loss=9.89112
Epoch 11/80: current_loss=9.97981 | best_loss=9.89112
Epoch 12/80: current_loss=10.38141 | best_loss=9.89112
Epoch 13/80: current_loss=10.11117 | best_loss=9.89112
Epoch 14/80: current_loss=10.40393 | best_loss=9.89112
Epoch 15/80: current_loss=10.68476 | best_loss=9.89112
Epoch 16/80: current_loss=10.01480 | best_loss=9.89112
Epoch 17/80: current_loss=9.91859 | best_loss=9.89112
Epoch 18/80: current_loss=10.65173 | best_loss=9.89112
Epoch 19/80: current_loss=10.01426 | best_loss=9.89112
Epoch 20/80: current_loss=10.28228 | best_loss=9.89112
Epoch 21/80: current_loss=9.90242 | best_loss=9.89112
Epoch 22/80: current_loss=9.97735 | best_loss=9.89112
Epoch 23/80: current_loss=10.15864 | best_loss=9.89112
Epoch 24/80: current_loss=9.95029 | best_loss=9.89112
Epoch 25/80: current_loss=10.59010 | best_loss=9.89112
Epoch 26/80: current_loss=10.14979 | best_loss=9.89112
Early Stopping at epoch 26
      explained_var=0.00215 | mse_loss=9.55663
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.67175 | best_loss=8.67175
Epoch 1/80: current_loss=8.80246 | best_loss=8.67175
Epoch 2/80: current_loss=8.45587 | best_loss=8.45587
Epoch 3/80: current_loss=8.84825 | best_loss=8.45587
Epoch 4/80: current_loss=8.57755 | best_loss=8.45587
Epoch 5/80: current_loss=8.86374 | best_loss=8.45587
Epoch 6/80: current_loss=8.92763 | best_loss=8.45587
Epoch 7/80: current_loss=8.68840 | best_loss=8.45587
Epoch 8/80: current_loss=9.09489 | best_loss=8.45587
Epoch 9/80: current_loss=8.48915 | best_loss=8.45587
Epoch 10/80: current_loss=8.56163 | best_loss=8.45587
Epoch 11/80: current_loss=8.66824 | best_loss=8.45587
Epoch 12/80: current_loss=9.28428 | best_loss=8.45587
Epoch 13/80: current_loss=8.50448 | best_loss=8.45587
Epoch 14/80: current_loss=8.52705 | best_loss=8.45587
Epoch 15/80: current_loss=9.74653 | best_loss=8.45587
Epoch 16/80: current_loss=8.68358 | best_loss=8.45587
Epoch 17/80: current_loss=8.94139 | best_loss=8.45587
Epoch 18/80: current_loss=8.64067 | best_loss=8.45587
Epoch 19/80: current_loss=9.75552 | best_loss=8.45587
Epoch 20/80: current_loss=8.72757 | best_loss=8.45587
Epoch 21/80: current_loss=9.15219 | best_loss=8.45587
Epoch 22/80: current_loss=9.49739 | best_loss=8.45587
Early Stopping at epoch 22
      explained_var=0.00973 | mse_loss=8.57689

----------------------------------------------
Params for Trial 44
{'learning_rate': 0.01, 'weight_decay': 0.0057600671306366, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=11.31012 | best_loss=11.31012
Epoch 1/80: current_loss=86.59644 | best_loss=11.31012
Epoch 2/80: current_loss=10.35088 | best_loss=10.35088
Epoch 3/80: current_loss=10.31179 | best_loss=10.31179
Epoch 4/80: current_loss=11.54642 | best_loss=10.31179
Epoch 5/80: current_loss=11.14580 | best_loss=10.31179
Epoch 6/80: current_loss=10.12376 | best_loss=10.12376
Epoch 7/80: current_loss=10.87351 | best_loss=10.12376
Epoch 8/80: current_loss=10.23787 | best_loss=10.12376
Epoch 9/80: current_loss=10.27444 | best_loss=10.12376
Epoch 10/80: current_loss=10.86896 | best_loss=10.12376
Epoch 11/80: current_loss=10.73732 | best_loss=10.12376
Epoch 12/80: current_loss=14.63022 | best_loss=10.12376
Epoch 13/80: current_loss=42.66186 | best_loss=10.12376
Epoch 14/80: current_loss=31.61585 | best_loss=10.12376
Epoch 15/80: current_loss=29.05623 | best_loss=10.12376
Epoch 16/80: current_loss=10.48914 | best_loss=10.12376
Epoch 17/80: current_loss=11.44241 | best_loss=10.12376
Epoch 18/80: current_loss=12.01722 | best_loss=10.12376
Epoch 19/80: current_loss=13.91340 | best_loss=10.12376
Epoch 20/80: current_loss=10.10212 | best_loss=10.10212
Epoch 21/80: current_loss=9.99241 | best_loss=9.99241
Epoch 22/80: current_loss=10.20835 | best_loss=9.99241
Epoch 23/80: current_loss=10.42299 | best_loss=9.99241
Epoch 24/80: current_loss=9.96470 | best_loss=9.96470
Epoch 25/80: current_loss=10.77325 | best_loss=9.96470
Epoch 26/80: current_loss=10.57368 | best_loss=9.96470
Epoch 27/80: current_loss=10.98039 | best_loss=9.96470
Epoch 28/80: current_loss=11.12717 | best_loss=9.96470
Epoch 29/80: current_loss=10.51900 | best_loss=9.96470
Epoch 30/80: current_loss=10.14223 | best_loss=9.96470
Epoch 31/80: current_loss=10.50347 | best_loss=9.96470
Epoch 32/80: current_loss=10.27603 | best_loss=9.96470
Epoch 33/80: current_loss=10.01520 | best_loss=9.96470
Epoch 34/80: current_loss=12.65132 | best_loss=9.96470
Epoch 35/80: current_loss=10.83543 | best_loss=9.96470
Epoch 36/80: current_loss=10.22965 | best_loss=9.96470
Epoch 37/80: current_loss=10.43047 | best_loss=9.96470
Epoch 38/80: current_loss=10.70519 | best_loss=9.96470
Epoch 39/80: current_loss=11.50018 | best_loss=9.96470
Epoch 40/80: current_loss=9.90649 | best_loss=9.90649
Epoch 41/80: current_loss=12.21542 | best_loss=9.90649
Epoch 42/80: current_loss=9.94384 | best_loss=9.90649
Epoch 43/80: current_loss=10.03921 | best_loss=9.90649
Epoch 44/80: current_loss=11.00744 | best_loss=9.90649
Epoch 45/80: current_loss=10.44938 | best_loss=9.90649
Epoch 46/80: current_loss=9.91301 | best_loss=9.90649
Epoch 47/80: current_loss=11.37483 | best_loss=9.90649
Epoch 48/80: current_loss=10.57792 | best_loss=9.90649
Epoch 49/80: current_loss=10.16008 | best_loss=9.90649
Epoch 50/80: current_loss=10.98376 | best_loss=9.90649
Epoch 51/80: current_loss=10.14078 | best_loss=9.90649
Epoch 52/80: current_loss=9.94529 | best_loss=9.90649
Epoch 53/80: current_loss=9.89830 | best_loss=9.89830
Epoch 54/80: current_loss=17.93721 | best_loss=9.89830
Epoch 55/80: current_loss=10.06137 | best_loss=9.89830
Epoch 56/80: current_loss=9.95201 | best_loss=9.89830
Epoch 57/80: current_loss=12.33572 | best_loss=9.89830
Epoch 58/80: current_loss=10.21950 | best_loss=9.89830
Epoch 59/80: current_loss=10.05177 | best_loss=9.89830
Epoch 60/80: current_loss=10.01146 | best_loss=9.89830
Epoch 61/80: current_loss=11.38869 | best_loss=9.89830
Epoch 62/80: current_loss=10.06043 | best_loss=9.89830
Epoch 63/80: current_loss=10.39542 | best_loss=9.89830
Epoch 64/80: current_loss=9.94834 | best_loss=9.89830
Epoch 65/80: current_loss=14.41102 | best_loss=9.89830
Epoch 66/80: current_loss=10.76498 | best_loss=9.89830
Epoch 67/80: current_loss=10.18635 | best_loss=9.89830
Epoch 68/80: current_loss=9.91790 | best_loss=9.89830
Epoch 69/80: current_loss=10.19825 | best_loss=9.89830
Epoch 70/80: current_loss=10.18577 | best_loss=9.89830
Epoch 71/80: current_loss=10.85490 | best_loss=9.89830
Epoch 72/80: current_loss=10.66272 | best_loss=9.89830
Epoch 73/80: current_loss=10.56510 | best_loss=9.89830
Early Stopping at epoch 73
      explained_var=0.00263 | mse_loss=9.57588

----------------------------------------------
Params for Trial 45
{'learning_rate': 0.001, 'weight_decay': 0.000918322814409595, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.21626 | best_loss=10.21626
Epoch 1/80: current_loss=9.90666 | best_loss=9.90666
Epoch 2/80: current_loss=9.91963 | best_loss=9.90666
Epoch 3/80: current_loss=9.89123 | best_loss=9.89123
Epoch 4/80: current_loss=9.96652 | best_loss=9.89123
Epoch 5/80: current_loss=9.89130 | best_loss=9.89123
Epoch 6/80: current_loss=9.97451 | best_loss=9.89123
Epoch 7/80: current_loss=9.90308 | best_loss=9.89123
Epoch 8/80: current_loss=9.92610 | best_loss=9.89123
Epoch 9/80: current_loss=9.89743 | best_loss=9.89123
Epoch 10/80: current_loss=9.92110 | best_loss=9.89123
Epoch 11/80: current_loss=9.90196 | best_loss=9.89123
Epoch 12/80: current_loss=9.90089 | best_loss=9.89123
Epoch 13/80: current_loss=9.92458 | best_loss=9.89123
Epoch 14/80: current_loss=9.89894 | best_loss=9.89123
Epoch 15/80: current_loss=9.89918 | best_loss=9.89123
Epoch 16/80: current_loss=9.91246 | best_loss=9.89123
Epoch 17/80: current_loss=9.89882 | best_loss=9.89123
Epoch 18/80: current_loss=9.94198 | best_loss=9.89123
Epoch 19/80: current_loss=9.89982 | best_loss=9.89123
Epoch 20/80: current_loss=9.90162 | best_loss=9.89123
Epoch 21/80: current_loss=9.90246 | best_loss=9.89123
Epoch 22/80: current_loss=9.95171 | best_loss=9.89123
Epoch 23/80: current_loss=9.90822 | best_loss=9.89123
Early Stopping at epoch 23
      explained_var=0.00255 | mse_loss=9.56101
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50676 | best_loss=8.50676
Epoch 1/80: current_loss=8.53622 | best_loss=8.50676
Epoch 2/80: current_loss=8.51544 | best_loss=8.50676
Epoch 3/80: current_loss=8.52126 | best_loss=8.50676
Epoch 4/80: current_loss=8.50498 | best_loss=8.50498
Epoch 5/80: current_loss=8.71107 | best_loss=8.50498
Epoch 6/80: current_loss=8.55764 | best_loss=8.50498
Epoch 7/80: current_loss=8.48046 | best_loss=8.48046
Epoch 8/80: current_loss=8.49403 | best_loss=8.48046
Epoch 9/80: current_loss=8.71826 | best_loss=8.48046
Epoch 10/80: current_loss=8.50987 | best_loss=8.48046
Epoch 11/80: current_loss=8.51076 | best_loss=8.48046
Epoch 12/80: current_loss=8.53891 | best_loss=8.48046
Epoch 13/80: current_loss=8.55931 | best_loss=8.48046
Epoch 14/80: current_loss=8.46497 | best_loss=8.46497
Epoch 15/80: current_loss=8.50137 | best_loss=8.46497
Epoch 16/80: current_loss=8.53602 | best_loss=8.46497
Epoch 17/80: current_loss=8.53775 | best_loss=8.46497
Epoch 18/80: current_loss=8.50559 | best_loss=8.46497
Epoch 19/80: current_loss=8.58436 | best_loss=8.46497
Epoch 20/80: current_loss=8.55164 | best_loss=8.46497
Epoch 21/80: current_loss=8.59193 | best_loss=8.46497
Epoch 22/80: current_loss=8.53483 | best_loss=8.46497
Epoch 23/80: current_loss=8.58305 | best_loss=8.46497
Epoch 24/80: current_loss=8.50168 | best_loss=8.46497
Epoch 25/80: current_loss=8.49952 | best_loss=8.46497
Epoch 26/80: current_loss=8.48709 | best_loss=8.46497
Epoch 27/80: current_loss=8.52506 | best_loss=8.46497
Epoch 28/80: current_loss=8.53128 | best_loss=8.46497
Epoch 29/80: current_loss=8.50328 | best_loss=8.46497
Epoch 30/80: current_loss=8.51413 | best_loss=8.46497
Epoch 31/80: current_loss=8.53015 | best_loss=8.46497
Epoch 32/80: current_loss=8.53124 | best_loss=8.46497
Epoch 33/80: current_loss=8.50349 | best_loss=8.46497
Epoch 34/80: current_loss=8.52004 | best_loss=8.46497
Early Stopping at epoch 34
      explained_var=0.00238 | mse_loss=8.56343
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.28049 | best_loss=9.28049
Epoch 1/80: current_loss=9.25669 | best_loss=9.25669
Epoch 2/80: current_loss=9.41621 | best_loss=9.25669
Epoch 3/80: current_loss=9.33678 | best_loss=9.25669
Epoch 4/80: current_loss=9.42404 | best_loss=9.25669
Epoch 5/80: current_loss=9.85174 | best_loss=9.25669
Epoch 6/80: current_loss=9.43910 | best_loss=9.25669
Epoch 7/80: current_loss=9.31165 | best_loss=9.25669
Epoch 8/80: current_loss=9.55921 | best_loss=9.25669
Epoch 9/80: current_loss=9.43008 | best_loss=9.25669
Epoch 10/80: current_loss=9.47823 | best_loss=9.25669
Epoch 11/80: current_loss=9.46773 | best_loss=9.25669
Epoch 12/80: current_loss=9.47122 | best_loss=9.25669
Epoch 13/80: current_loss=9.31838 | best_loss=9.25669
Epoch 14/80: current_loss=9.30047 | best_loss=9.25669
Epoch 15/80: current_loss=9.46380 | best_loss=9.25669
Epoch 16/80: current_loss=9.36355 | best_loss=9.25669
Epoch 17/80: current_loss=9.29070 | best_loss=9.25669
Epoch 18/80: current_loss=9.35420 | best_loss=9.25669
Epoch 19/80: current_loss=9.32234 | best_loss=9.25669
Epoch 20/80: current_loss=9.33753 | best_loss=9.25669
Epoch 21/80: current_loss=9.40325 | best_loss=9.25669
Early Stopping at epoch 21
      explained_var=-0.00039 | mse_loss=9.38322
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.97078 | best_loss=8.97078
Epoch 1/80: current_loss=8.95445 | best_loss=8.95445
Epoch 2/80: current_loss=8.96988 | best_loss=8.95445
Epoch 3/80: current_loss=9.11971 | best_loss=8.95445
Epoch 4/80: current_loss=8.96120 | best_loss=8.95445
Epoch 5/80: current_loss=9.04033 | best_loss=8.95445
Epoch 6/80: current_loss=8.97572 | best_loss=8.95445
Epoch 7/80: current_loss=8.99029 | best_loss=8.95445
Epoch 8/80: current_loss=8.94926 | best_loss=8.94926
Epoch 9/80: current_loss=8.97806 | best_loss=8.94926
Epoch 10/80: current_loss=8.95675 | best_loss=8.94926
Epoch 11/80: current_loss=8.95674 | best_loss=8.94926
Epoch 12/80: current_loss=9.03549 | best_loss=8.94926
Epoch 13/80: current_loss=9.00294 | best_loss=8.94926
Epoch 14/80: current_loss=9.01705 | best_loss=8.94926
Epoch 15/80: current_loss=9.09940 | best_loss=8.94926
Epoch 16/80: current_loss=9.12743 | best_loss=8.94926
Epoch 17/80: current_loss=9.05166 | best_loss=8.94926
Epoch 18/80: current_loss=8.98423 | best_loss=8.94926
Epoch 19/80: current_loss=8.96853 | best_loss=8.94926
Epoch 20/80: current_loss=8.95681 | best_loss=8.94926
Epoch 21/80: current_loss=9.02040 | best_loss=8.94926
Epoch 22/80: current_loss=9.01621 | best_loss=8.94926
Epoch 23/80: current_loss=9.02276 | best_loss=8.94926
Epoch 24/80: current_loss=8.94465 | best_loss=8.94465
Epoch 25/80: current_loss=8.95949 | best_loss=8.94465
Epoch 26/80: current_loss=8.98587 | best_loss=8.94465
Epoch 27/80: current_loss=8.96830 | best_loss=8.94465
Epoch 28/80: current_loss=8.97158 | best_loss=8.94465
Epoch 29/80: current_loss=8.97326 | best_loss=8.94465
Epoch 30/80: current_loss=8.98739 | best_loss=8.94465
Epoch 31/80: current_loss=8.98468 | best_loss=8.94465
Epoch 32/80: current_loss=9.07378 | best_loss=8.94465
Epoch 33/80: current_loss=8.95398 | best_loss=8.94465
Epoch 34/80: current_loss=9.01489 | best_loss=8.94465
Epoch 35/80: current_loss=8.97331 | best_loss=8.94465
Epoch 36/80: current_loss=8.96150 | best_loss=8.94465
Epoch 37/80: current_loss=8.98906 | best_loss=8.94465
Epoch 38/80: current_loss=8.95553 | best_loss=8.94465
Epoch 39/80: current_loss=8.97320 | best_loss=8.94465
Epoch 40/80: current_loss=8.95505 | best_loss=8.94465
Epoch 41/80: current_loss=9.02917 | best_loss=8.94465
Epoch 42/80: current_loss=8.96113 | best_loss=8.94465
Epoch 43/80: current_loss=8.97573 | best_loss=8.94465
Epoch 44/80: current_loss=8.96306 | best_loss=8.94465
Early Stopping at epoch 44
      explained_var=0.00115 | mse_loss=8.38056

----------------------------------------------
Params for Trial 46
{'learning_rate': 0.01, 'weight_decay': 0.002123023848387107, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91890 | best_loss=9.91890
Epoch 1/80: current_loss=10.56949 | best_loss=9.91890
Epoch 2/80: current_loss=10.16657 | best_loss=9.91890
Epoch 3/80: current_loss=10.30589 | best_loss=9.91890
Epoch 4/80: current_loss=10.37071 | best_loss=9.91890
Epoch 5/80: current_loss=10.75614 | best_loss=9.91890
Epoch 6/80: current_loss=10.12340 | best_loss=9.91890
Epoch 7/80: current_loss=10.49782 | best_loss=9.91890
Epoch 8/80: current_loss=10.23615 | best_loss=9.91890
Epoch 9/80: current_loss=10.04972 | best_loss=9.91890
Epoch 10/80: current_loss=10.04360 | best_loss=9.91890
Epoch 11/80: current_loss=10.02368 | best_loss=9.91890
Epoch 12/80: current_loss=10.02644 | best_loss=9.91890
Epoch 13/80: current_loss=9.98282 | best_loss=9.91890
Epoch 14/80: current_loss=10.01603 | best_loss=9.91890
Epoch 15/80: current_loss=10.41346 | best_loss=9.91890
Epoch 16/80: current_loss=11.36606 | best_loss=9.91890
Epoch 17/80: current_loss=9.90689 | best_loss=9.90689
Epoch 18/80: current_loss=10.08406 | best_loss=9.90689
Epoch 19/80: current_loss=9.96776 | best_loss=9.90689
Epoch 20/80: current_loss=10.53421 | best_loss=9.90689
Epoch 21/80: current_loss=10.08171 | best_loss=9.90689
Epoch 22/80: current_loss=10.41901 | best_loss=9.90689
Epoch 23/80: current_loss=9.89369 | best_loss=9.89369
Epoch 24/80: current_loss=9.90065 | best_loss=9.89369
Epoch 25/80: current_loss=9.91256 | best_loss=9.89369
Epoch 26/80: current_loss=9.92448 | best_loss=9.89369
Epoch 27/80: current_loss=9.91258 | best_loss=9.89369
Epoch 28/80: current_loss=9.98454 | best_loss=9.89369
Epoch 29/80: current_loss=9.96286 | best_loss=9.89369
Epoch 30/80: current_loss=10.27247 | best_loss=9.89369
Epoch 31/80: current_loss=10.33586 | best_loss=9.89369
Epoch 32/80: current_loss=9.92182 | best_loss=9.89369
Epoch 33/80: current_loss=9.96968 | best_loss=9.89369
Epoch 34/80: current_loss=10.09167 | best_loss=9.89369
Epoch 35/80: current_loss=9.88657 | best_loss=9.88657
Epoch 36/80: current_loss=9.93814 | best_loss=9.88657
Epoch 37/80: current_loss=10.01444 | best_loss=9.88657
Epoch 38/80: current_loss=10.12260 | best_loss=9.88657
Epoch 39/80: current_loss=9.94466 | best_loss=9.88657
Epoch 40/80: current_loss=9.89733 | best_loss=9.88657
Epoch 41/80: current_loss=9.91510 | best_loss=9.88657
Epoch 42/80: current_loss=10.31191 | best_loss=9.88657
Epoch 43/80: current_loss=10.00684 | best_loss=9.88657
Epoch 44/80: current_loss=9.89772 | best_loss=9.88657
Epoch 45/80: current_loss=10.25005 | best_loss=9.88657
Epoch 46/80: current_loss=10.18405 | best_loss=9.88657
Epoch 47/80: current_loss=10.17128 | best_loss=9.88657
Epoch 48/80: current_loss=9.89818 | best_loss=9.88657
Epoch 49/80: current_loss=9.92793 | best_loss=9.88657
Epoch 50/80: current_loss=10.95117 | best_loss=9.88657
Epoch 51/80: current_loss=9.93991 | best_loss=9.88657
Epoch 52/80: current_loss=9.98509 | best_loss=9.88657
Epoch 53/80: current_loss=10.95643 | best_loss=9.88657
Epoch 54/80: current_loss=10.12684 | best_loss=9.88657
Epoch 55/80: current_loss=9.92756 | best_loss=9.88657
Early Stopping at epoch 55
      explained_var=0.00279 | mse_loss=9.55033
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.53700 | best_loss=8.53700
Epoch 1/80: current_loss=9.32935 | best_loss=8.53700
Epoch 2/80: current_loss=8.47017 | best_loss=8.47017
Epoch 3/80: current_loss=8.48702 | best_loss=8.47017
Epoch 4/80: current_loss=8.87498 | best_loss=8.47017
Epoch 5/80: current_loss=9.13160 | best_loss=8.47017
Epoch 6/80: current_loss=8.90948 | best_loss=8.47017
Epoch 7/80: current_loss=8.65087 | best_loss=8.47017
Epoch 8/80: current_loss=8.74605 | best_loss=8.47017
Epoch 9/80: current_loss=8.48826 | best_loss=8.47017
Epoch 10/80: current_loss=9.16263 | best_loss=8.47017
Epoch 11/80: current_loss=8.97375 | best_loss=8.47017
Epoch 12/80: current_loss=8.90277 | best_loss=8.47017
Epoch 13/80: current_loss=9.25840 | best_loss=8.47017
Epoch 14/80: current_loss=8.46447 | best_loss=8.46447
Epoch 15/80: current_loss=8.52102 | best_loss=8.46447
Epoch 16/80: current_loss=8.57609 | best_loss=8.46447
Epoch 17/80: current_loss=8.46767 | best_loss=8.46447
Epoch 18/80: current_loss=59.06816 | best_loss=8.46447
Epoch 19/80: current_loss=8.65761 | best_loss=8.46447
Epoch 20/80: current_loss=8.47596 | best_loss=8.46447
Epoch 21/80: current_loss=8.55317 | best_loss=8.46447
Epoch 22/80: current_loss=8.48563 | best_loss=8.46447
Epoch 23/80: current_loss=8.50190 | best_loss=8.46447
Epoch 24/80: current_loss=8.76157 | best_loss=8.46447
Epoch 25/80: current_loss=8.47675 | best_loss=8.46447
Epoch 26/80: current_loss=8.54318 | best_loss=8.46447
Epoch 27/80: current_loss=8.50004 | best_loss=8.46447
Epoch 28/80: current_loss=8.56408 | best_loss=8.46447
Epoch 29/80: current_loss=8.74573 | best_loss=8.46447
Epoch 30/80: current_loss=8.48576 | best_loss=8.46447
Epoch 31/80: current_loss=8.87489 | best_loss=8.46447
Epoch 32/80: current_loss=8.90247 | best_loss=8.46447
Epoch 33/80: current_loss=8.97967 | best_loss=8.46447
Epoch 34/80: current_loss=9.03850 | best_loss=8.46447
Early Stopping at epoch 34
      explained_var=0.00273 | mse_loss=8.55707
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.28125 | best_loss=9.28125
Epoch 1/80: current_loss=10.36598 | best_loss=9.28125
Epoch 2/80: current_loss=9.69589 | best_loss=9.28125
Epoch 3/80: current_loss=9.23275 | best_loss=9.23275
Epoch 4/80: current_loss=10.83792 | best_loss=9.23275
Epoch 5/80: current_loss=9.30909 | best_loss=9.23275
Epoch 6/80: current_loss=9.43841 | best_loss=9.23275
Epoch 7/80: current_loss=10.86315 | best_loss=9.23275
Epoch 8/80: current_loss=10.15955 | best_loss=9.23275
Epoch 9/80: current_loss=9.32336 | best_loss=9.23275
Epoch 10/80: current_loss=9.29917 | best_loss=9.23275
Epoch 11/80: current_loss=9.40288 | best_loss=9.23275
Epoch 12/80: current_loss=9.33933 | best_loss=9.23275
Epoch 13/80: current_loss=9.52782 | best_loss=9.23275
Epoch 14/80: current_loss=9.53586 | best_loss=9.23275
Epoch 15/80: current_loss=9.58308 | best_loss=9.23275
Epoch 16/80: current_loss=10.00753 | best_loss=9.23275
Epoch 17/80: current_loss=9.41606 | best_loss=9.23275
Epoch 18/80: current_loss=9.34390 | best_loss=9.23275
Epoch 19/80: current_loss=9.96502 | best_loss=9.23275
Epoch 20/80: current_loss=9.99154 | best_loss=9.23275
Epoch 21/80: current_loss=10.41901 | best_loss=9.23275
Epoch 22/80: current_loss=9.27697 | best_loss=9.23275
Epoch 23/80: current_loss=9.44037 | best_loss=9.23275
Early Stopping at epoch 23
      explained_var=0.00447 | mse_loss=9.33841
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.95933 | best_loss=8.95933
Epoch 1/80: current_loss=9.23644 | best_loss=8.95933
Epoch 2/80: current_loss=9.91850 | best_loss=8.95933
Epoch 3/80: current_loss=9.26291 | best_loss=8.95933
Epoch 4/80: current_loss=9.48082 | best_loss=8.95933
Epoch 5/80: current_loss=9.32066 | best_loss=8.95933
Epoch 6/80: current_loss=8.90420 | best_loss=8.90420
Epoch 7/80: current_loss=9.35010 | best_loss=8.90420
Epoch 8/80: current_loss=9.05434 | best_loss=8.90420
Epoch 9/80: current_loss=9.15354 | best_loss=8.90420
Epoch 10/80: current_loss=9.45901 | best_loss=8.90420
Epoch 11/80: current_loss=9.70691 | best_loss=8.90420
Epoch 12/80: current_loss=8.98760 | best_loss=8.90420
Epoch 13/80: current_loss=9.22807 | best_loss=8.90420
Epoch 14/80: current_loss=9.60602 | best_loss=8.90420
Epoch 15/80: current_loss=10.38759 | best_loss=8.90420
Epoch 16/80: current_loss=9.34838 | best_loss=8.90420
Epoch 17/80: current_loss=8.94532 | best_loss=8.90420
Epoch 18/80: current_loss=10.25750 | best_loss=8.90420
Epoch 19/80: current_loss=9.36480 | best_loss=8.90420
Epoch 20/80: current_loss=9.59661 | best_loss=8.90420
Epoch 21/80: current_loss=8.96634 | best_loss=8.90420
Epoch 22/80: current_loss=8.92692 | best_loss=8.90420
Epoch 23/80: current_loss=8.99928 | best_loss=8.90420
Epoch 24/80: current_loss=8.94194 | best_loss=8.90420
Epoch 25/80: current_loss=8.94407 | best_loss=8.90420
Epoch 26/80: current_loss=9.01223 | best_loss=8.90420
Early Stopping at epoch 26
      explained_var=0.00421 | mse_loss=8.33656
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.80496 | best_loss=8.80496
Epoch 1/80: current_loss=8.80022 | best_loss=8.80022
Epoch 2/80: current_loss=8.90756 | best_loss=8.80022
Epoch 3/80: current_loss=9.09488 | best_loss=8.80022
Epoch 4/80: current_loss=8.68929 | best_loss=8.68929
Epoch 5/80: current_loss=9.21541 | best_loss=8.68929
Epoch 6/80: current_loss=8.50254 | best_loss=8.50254
Epoch 7/80: current_loss=8.75464 | best_loss=8.50254
Epoch 8/80: current_loss=9.00749 | best_loss=8.50254
Epoch 9/80: current_loss=8.84374 | best_loss=8.50254
Epoch 10/80: current_loss=8.88009 | best_loss=8.50254
Epoch 11/80: current_loss=8.58400 | best_loss=8.50254
Epoch 12/80: current_loss=8.57539 | best_loss=8.50254
Epoch 13/80: current_loss=8.81251 | best_loss=8.50254
Epoch 14/80: current_loss=8.57150 | best_loss=8.50254
Epoch 15/80: current_loss=8.47863 | best_loss=8.47863
Epoch 16/80: current_loss=8.51347 | best_loss=8.47863
Epoch 17/80: current_loss=8.75795 | best_loss=8.47863
Epoch 18/80: current_loss=8.62532 | best_loss=8.47863
Epoch 19/80: current_loss=10.27153 | best_loss=8.47863
Epoch 20/80: current_loss=8.50256 | best_loss=8.47863
Epoch 21/80: current_loss=8.54165 | best_loss=8.47863
Epoch 22/80: current_loss=8.52272 | best_loss=8.47863
Epoch 23/80: current_loss=9.27342 | best_loss=8.47863
Epoch 24/80: current_loss=8.88793 | best_loss=8.47863
Epoch 25/80: current_loss=8.93631 | best_loss=8.47863
Epoch 26/80: current_loss=8.80002 | best_loss=8.47863
Epoch 27/80: current_loss=8.98883 | best_loss=8.47863
Epoch 28/80: current_loss=8.68000 | best_loss=8.47863
Epoch 29/80: current_loss=8.55639 | best_loss=8.47863
Epoch 30/80: current_loss=8.62308 | best_loss=8.47863
Epoch 31/80: current_loss=8.67037 | best_loss=8.47863
Epoch 32/80: current_loss=8.52456 | best_loss=8.47863
Epoch 33/80: current_loss=8.46815 | best_loss=8.46815
Epoch 34/80: current_loss=8.48400 | best_loss=8.46815
Epoch 35/80: current_loss=9.54589 | best_loss=8.46815
Epoch 36/80: current_loss=8.82468 | best_loss=8.46815
Epoch 37/80: current_loss=8.51105 | best_loss=8.46815
Epoch 38/80: current_loss=8.96231 | best_loss=8.46815
Epoch 39/80: current_loss=8.61223 | best_loss=8.46815
Epoch 40/80: current_loss=9.40276 | best_loss=8.46815
Epoch 41/80: current_loss=8.63369 | best_loss=8.46815
Epoch 42/80: current_loss=8.61480 | best_loss=8.46815
Epoch 43/80: current_loss=8.65076 | best_loss=8.46815
Epoch 44/80: current_loss=9.03390 | best_loss=8.46815
Epoch 45/80: current_loss=9.37093 | best_loss=8.46815
Epoch 46/80: current_loss=8.58661 | best_loss=8.46815
Epoch 47/80: current_loss=8.55014 | best_loss=8.46815
Epoch 48/80: current_loss=8.59047 | best_loss=8.46815
Epoch 49/80: current_loss=8.81821 | best_loss=8.46815
Epoch 50/80: current_loss=8.48055 | best_loss=8.46815
Epoch 51/80: current_loss=8.48499 | best_loss=8.46815
Epoch 52/80: current_loss=8.75133 | best_loss=8.46815
Epoch 53/80: current_loss=9.05081 | best_loss=8.46815
Early Stopping at epoch 53
      explained_var=0.00117 | mse_loss=8.19085
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00307| avg_loss=8.79464
----------------------------------------------


----------------------------------------------
Params for Trial 47
{'learning_rate': 0.1, 'weight_decay': 0.002776348640738428, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.95590 | best_loss=9.95590
Epoch 1/80: current_loss=9.94293 | best_loss=9.94293
Epoch 2/80: current_loss=10.24136 | best_loss=9.94293
Epoch 3/80: current_loss=10.91247 | best_loss=9.94293
Epoch 4/80: current_loss=10.28158 | best_loss=9.94293
Epoch 5/80: current_loss=9.99222 | best_loss=9.94293
Epoch 6/80: current_loss=10.43044 | best_loss=9.94293
Epoch 7/80: current_loss=9.97861 | best_loss=9.94293
Epoch 8/80: current_loss=12.72010 | best_loss=9.94293
Epoch 9/80: current_loss=10.07094 | best_loss=9.94293
Epoch 10/80: current_loss=10.07785 | best_loss=9.94293
Epoch 11/80: current_loss=9.90394 | best_loss=9.90394
Epoch 12/80: current_loss=10.57107 | best_loss=9.90394
Epoch 13/80: current_loss=10.02880 | best_loss=9.90394
Epoch 14/80: current_loss=9.96714 | best_loss=9.90394
Epoch 15/80: current_loss=12.25194 | best_loss=9.90394
Epoch 16/80: current_loss=11.21263 | best_loss=9.90394
Epoch 17/80: current_loss=10.63324 | best_loss=9.90394
Epoch 18/80: current_loss=9.93461 | best_loss=9.90394
Epoch 19/80: current_loss=10.52622 | best_loss=9.90394
Epoch 20/80: current_loss=9.93797 | best_loss=9.90394
Epoch 21/80: current_loss=9.90598 | best_loss=9.90394
Epoch 22/80: current_loss=12.72790 | best_loss=9.90394
Epoch 23/80: current_loss=10.12758 | best_loss=9.90394
Epoch 24/80: current_loss=9.90337 | best_loss=9.90337
Epoch 25/80: current_loss=9.96303 | best_loss=9.90337
Epoch 26/80: current_loss=9.83838 | best_loss=9.83838
Epoch 27/80: current_loss=9.96078 | best_loss=9.83838
Epoch 28/80: current_loss=9.90630 | best_loss=9.83838
Epoch 29/80: current_loss=10.09065 | best_loss=9.83838
Epoch 30/80: current_loss=10.71980 | best_loss=9.83838
Epoch 31/80: current_loss=11.43664 | best_loss=9.83838
Epoch 32/80: current_loss=10.09812 | best_loss=9.83838
Epoch 33/80: current_loss=10.89552 | best_loss=9.83838
Epoch 34/80: current_loss=11.55834 | best_loss=9.83838
Epoch 35/80: current_loss=10.68092 | best_loss=9.83838
Epoch 36/80: current_loss=10.32010 | best_loss=9.83838
Epoch 37/80: current_loss=10.23743 | best_loss=9.83838
Epoch 38/80: current_loss=9.92453 | best_loss=9.83838
Epoch 39/80: current_loss=9.94051 | best_loss=9.83838
Epoch 40/80: current_loss=9.88419 | best_loss=9.83838
Epoch 41/80: current_loss=10.19060 | best_loss=9.83838
Epoch 42/80: current_loss=10.92535 | best_loss=9.83838
Epoch 43/80: current_loss=10.51679 | best_loss=9.83838
Epoch 44/80: current_loss=9.95480 | best_loss=9.83838
Epoch 45/80: current_loss=9.87323 | best_loss=9.83838
Epoch 46/80: current_loss=10.08755 | best_loss=9.83838
Early Stopping at epoch 46
      explained_var=0.00631 | mse_loss=9.51630
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.97529 | best_loss=8.97529
Epoch 1/80: current_loss=8.72865 | best_loss=8.72865
Epoch 2/80: current_loss=10.10897 | best_loss=8.72865
Epoch 3/80: current_loss=9.57352 | best_loss=8.72865
Epoch 4/80: current_loss=9.17436 | best_loss=8.72865
Epoch 5/80: current_loss=9.12932 | best_loss=8.72865
Epoch 6/80: current_loss=10.89390 | best_loss=8.72865
Epoch 7/80: current_loss=8.65804 | best_loss=8.65804
Epoch 8/80: current_loss=9.44008 | best_loss=8.65804
Epoch 9/80: current_loss=8.78487 | best_loss=8.65804
Epoch 10/80: current_loss=10.34622 | best_loss=8.65804
Epoch 11/80: current_loss=9.20223 | best_loss=8.65804
Epoch 12/80: current_loss=8.72733 | best_loss=8.65804
Epoch 13/80: current_loss=8.78155 | best_loss=8.65804
Epoch 14/80: current_loss=8.68289 | best_loss=8.65804
Epoch 15/80: current_loss=9.02226 | best_loss=8.65804
Epoch 16/80: current_loss=8.64442 | best_loss=8.64442
Epoch 17/80: current_loss=8.57663 | best_loss=8.57663
Epoch 18/80: current_loss=8.47467 | best_loss=8.47467
Epoch 19/80: current_loss=8.93957 | best_loss=8.47467
Epoch 20/80: current_loss=9.19422 | best_loss=8.47467
Epoch 21/80: current_loss=8.69247 | best_loss=8.47467
Epoch 22/80: current_loss=9.06929 | best_loss=8.47467
Epoch 23/80: current_loss=9.70040 | best_loss=8.47467
Epoch 24/80: current_loss=9.71233 | best_loss=8.47467
Epoch 25/80: current_loss=8.76049 | best_loss=8.47467
Epoch 26/80: current_loss=8.65555 | best_loss=8.47467
Epoch 27/80: current_loss=8.57719 | best_loss=8.47467
Epoch 28/80: current_loss=8.40359 | best_loss=8.40359
Epoch 29/80: current_loss=8.99270 | best_loss=8.40359
Epoch 30/80: current_loss=8.52489 | best_loss=8.40359
Epoch 31/80: current_loss=8.86306 | best_loss=8.40359
Epoch 32/80: current_loss=8.68473 | best_loss=8.40359
Epoch 33/80: current_loss=8.64553 | best_loss=8.40359
Epoch 34/80: current_loss=11.80861 | best_loss=8.40359
Epoch 35/80: current_loss=8.54215 | best_loss=8.40359
Epoch 36/80: current_loss=8.56591 | best_loss=8.40359
Epoch 37/80: current_loss=8.79709 | best_loss=8.40359
Epoch 38/80: current_loss=8.94507 | best_loss=8.40359
Epoch 39/80: current_loss=10.22946 | best_loss=8.40359
Epoch 40/80: current_loss=8.55385 | best_loss=8.40359
Epoch 41/80: current_loss=9.52697 | best_loss=8.40359
Epoch 42/80: current_loss=8.48423 | best_loss=8.40359
Epoch 43/80: current_loss=8.49069 | best_loss=8.40359
Epoch 44/80: current_loss=10.29897 | best_loss=8.40359
Epoch 45/80: current_loss=8.56970 | best_loss=8.40359
Epoch 46/80: current_loss=8.64299 | best_loss=8.40359
Epoch 47/80: current_loss=8.49798 | best_loss=8.40359
Epoch 48/80: current_loss=8.50370 | best_loss=8.40359
Early Stopping at epoch 48
      explained_var=0.00824 | mse_loss=8.50704
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.40122 | best_loss=9.40122
Epoch 1/80: current_loss=12.14142 | best_loss=9.40122
Epoch 2/80: current_loss=10.36859 | best_loss=9.40122
Epoch 3/80: current_loss=11.30660 | best_loss=9.40122
Epoch 4/80: current_loss=9.26951 | best_loss=9.26951
Epoch 5/80: current_loss=9.50593 | best_loss=9.26951
Epoch 6/80: current_loss=9.46649 | best_loss=9.26951
Epoch 7/80: current_loss=9.18454 | best_loss=9.18454
Epoch 8/80: current_loss=9.40331 | best_loss=9.18454
Epoch 9/80: current_loss=9.23256 | best_loss=9.18454
Epoch 10/80: current_loss=9.38752 | best_loss=9.18454
Epoch 11/80: current_loss=9.37989 | best_loss=9.18454
Epoch 12/80: current_loss=9.35517 | best_loss=9.18454
Epoch 13/80: current_loss=9.54433 | best_loss=9.18454
Epoch 14/80: current_loss=9.79602 | best_loss=9.18454
Epoch 15/80: current_loss=9.36159 | best_loss=9.18454
Epoch 16/80: current_loss=9.66525 | best_loss=9.18454
Epoch 17/80: current_loss=10.07677 | best_loss=9.18454
Epoch 18/80: current_loss=9.25106 | best_loss=9.18454
Epoch 19/80: current_loss=9.93362 | best_loss=9.18454
Epoch 20/80: current_loss=9.46015 | best_loss=9.18454
Epoch 21/80: current_loss=9.54980 | best_loss=9.18454
Epoch 22/80: current_loss=9.38058 | best_loss=9.18454
Epoch 23/80: current_loss=9.22855 | best_loss=9.18454
Epoch 24/80: current_loss=9.50620 | best_loss=9.18454
Epoch 25/80: current_loss=9.19766 | best_loss=9.18454
Epoch 26/80: current_loss=9.33306 | best_loss=9.18454
Epoch 27/80: current_loss=11.77823 | best_loss=9.18454
Early Stopping at epoch 27
      explained_var=0.01140 | mse_loss=9.28865
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.33479 | best_loss=10.33479
Epoch 1/80: current_loss=9.43925 | best_loss=9.43925
Epoch 2/80: current_loss=9.02391 | best_loss=9.02391
Epoch 3/80: current_loss=10.25053 | best_loss=9.02391
Epoch 4/80: current_loss=9.07684 | best_loss=9.02391
Epoch 5/80: current_loss=9.02925 | best_loss=9.02391
Epoch 6/80: current_loss=9.48554 | best_loss=9.02391
Epoch 7/80: current_loss=9.48080 | best_loss=9.02391
Epoch 8/80: current_loss=9.50922 | best_loss=9.02391
Epoch 9/80: current_loss=9.04922 | best_loss=9.02391
Epoch 10/80: current_loss=9.20779 | best_loss=9.02391
Epoch 11/80: current_loss=9.15224 | best_loss=9.02391
Epoch 12/80: current_loss=9.02741 | best_loss=9.02391
Epoch 13/80: current_loss=9.12083 | best_loss=9.02391
Epoch 14/80: current_loss=9.12596 | best_loss=9.02391
Epoch 15/80: current_loss=9.46385 | best_loss=9.02391
Epoch 16/80: current_loss=9.27802 | best_loss=9.02391
Epoch 17/80: current_loss=8.95926 | best_loss=8.95926
Epoch 18/80: current_loss=9.46117 | best_loss=8.95926
Epoch 19/80: current_loss=9.29676 | best_loss=8.95926
Epoch 20/80: current_loss=8.94806 | best_loss=8.94806
Epoch 21/80: current_loss=8.96474 | best_loss=8.94806
Epoch 22/80: current_loss=9.03677 | best_loss=8.94806
Epoch 23/80: current_loss=9.99611 | best_loss=8.94806
Epoch 24/80: current_loss=9.72136 | best_loss=8.94806
Epoch 25/80: current_loss=8.99765 | best_loss=8.94806
Epoch 26/80: current_loss=9.65379 | best_loss=8.94806
Epoch 27/80: current_loss=8.94947 | best_loss=8.94806
Epoch 28/80: current_loss=8.99517 | best_loss=8.94806
Epoch 29/80: current_loss=9.05740 | best_loss=8.94806
Epoch 30/80: current_loss=9.32003 | best_loss=8.94806
Epoch 31/80: current_loss=10.31139 | best_loss=8.94806
Epoch 32/80: current_loss=9.34775 | best_loss=8.94806
Epoch 33/80: current_loss=8.99751 | best_loss=8.94806
Epoch 34/80: current_loss=9.48857 | best_loss=8.94806
Epoch 35/80: current_loss=9.47603 | best_loss=8.94806
Epoch 36/80: current_loss=10.60626 | best_loss=8.94806
Epoch 37/80: current_loss=8.98785 | best_loss=8.94806
Epoch 38/80: current_loss=9.08979 | best_loss=8.94806
Epoch 39/80: current_loss=9.20145 | best_loss=8.94806
Epoch 40/80: current_loss=10.05551 | best_loss=8.94806
Early Stopping at epoch 40
      explained_var=0.00108 | mse_loss=8.37467

----------------------------------------------
Params for Trial 48
{'learning_rate': 0.01, 'weight_decay': 0.000325909419101398, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.33404 | best_loss=10.33404
Epoch 1/80: current_loss=10.00390 | best_loss=10.00390
Epoch 2/80: current_loss=9.96898 | best_loss=9.96898
Epoch 3/80: current_loss=10.11673 | best_loss=9.96898
Epoch 4/80: current_loss=9.96079 | best_loss=9.96079
Epoch 5/80: current_loss=10.03174 | best_loss=9.96079
Epoch 6/80: current_loss=9.96378 | best_loss=9.96079
Epoch 7/80: current_loss=11.02500 | best_loss=9.96079
Epoch 8/80: current_loss=9.90757 | best_loss=9.90757
Epoch 9/80: current_loss=9.96801 | best_loss=9.90757
Epoch 10/80: current_loss=9.90958 | best_loss=9.90757
Epoch 11/80: current_loss=9.91116 | best_loss=9.90757
Epoch 12/80: current_loss=10.04504 | best_loss=9.90757
Epoch 13/80: current_loss=10.57371 | best_loss=9.90757
Epoch 14/80: current_loss=9.99821 | best_loss=9.90757
Epoch 15/80: current_loss=9.96164 | best_loss=9.90757
Epoch 16/80: current_loss=9.91781 | best_loss=9.90757
Epoch 17/80: current_loss=9.93671 | best_loss=9.90757
Epoch 18/80: current_loss=11.07215 | best_loss=9.90757
Epoch 19/80: current_loss=10.69358 | best_loss=9.90757
Epoch 20/80: current_loss=9.93764 | best_loss=9.90757
Epoch 21/80: current_loss=9.97928 | best_loss=9.90757
Epoch 22/80: current_loss=10.18455 | best_loss=9.90757
Epoch 23/80: current_loss=9.94471 | best_loss=9.90757
Epoch 24/80: current_loss=9.91667 | best_loss=9.90757
Epoch 25/80: current_loss=10.02658 | best_loss=9.90757
Epoch 26/80: current_loss=10.33298 | best_loss=9.90757
Epoch 27/80: current_loss=9.93068 | best_loss=9.90757
Epoch 28/80: current_loss=9.98739 | best_loss=9.90757
Early Stopping at epoch 28
      explained_var=0.00006 | mse_loss=9.57652

----------------------------------------------
Params for Trial 49
{'learning_rate': 0.0001, 'weight_decay': 0.0033493587912219935, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=14.36435 | best_loss=14.36435
Epoch 1/80: current_loss=12.97393 | best_loss=12.97393
Epoch 2/80: current_loss=12.13724 | best_loss=12.13724
Epoch 3/80: current_loss=11.72099 | best_loss=11.72099
Epoch 4/80: current_loss=11.25387 | best_loss=11.25387
Epoch 5/80: current_loss=10.99915 | best_loss=10.99915
Epoch 6/80: current_loss=10.83311 | best_loss=10.83311
Epoch 7/80: current_loss=10.71807 | best_loss=10.71807
Epoch 8/80: current_loss=10.59773 | best_loss=10.59773
Epoch 9/80: current_loss=10.53456 | best_loss=10.53456
Epoch 10/80: current_loss=10.42087 | best_loss=10.42087
Epoch 11/80: current_loss=10.36562 | best_loss=10.36562
Epoch 12/80: current_loss=10.27468 | best_loss=10.27468
Epoch 13/80: current_loss=10.21443 | best_loss=10.21443
Epoch 14/80: current_loss=10.15001 | best_loss=10.15001
Epoch 15/80: current_loss=10.11037 | best_loss=10.11037
Epoch 16/80: current_loss=10.10280 | best_loss=10.10280
Epoch 17/80: current_loss=10.03210 | best_loss=10.03210
Epoch 18/80: current_loss=10.01572 | best_loss=10.01572
Epoch 19/80: current_loss=9.99156 | best_loss=9.99156
Epoch 20/80: current_loss=10.00129 | best_loss=9.99156
Epoch 21/80: current_loss=9.95670 | best_loss=9.95670
Epoch 22/80: current_loss=9.92932 | best_loss=9.92932
Epoch 23/80: current_loss=9.92606 | best_loss=9.92606
Epoch 24/80: current_loss=9.93293 | best_loss=9.92606
Epoch 25/80: current_loss=9.90448 | best_loss=9.90448
Epoch 26/80: current_loss=9.96227 | best_loss=9.90448
Epoch 27/80: current_loss=9.93874 | best_loss=9.90448
Epoch 28/80: current_loss=9.93064 | best_loss=9.90448
Epoch 29/80: current_loss=10.00069 | best_loss=9.90448
Epoch 30/80: current_loss=9.92468 | best_loss=9.90448
Epoch 31/80: current_loss=9.90217 | best_loss=9.90217
Epoch 32/80: current_loss=9.95068 | best_loss=9.90217
Epoch 33/80: current_loss=9.96369 | best_loss=9.90217
Epoch 34/80: current_loss=9.94501 | best_loss=9.90217
Epoch 35/80: current_loss=9.93660 | best_loss=9.90217
Epoch 36/80: current_loss=9.92660 | best_loss=9.90217
Epoch 37/80: current_loss=9.93927 | best_loss=9.90217
Epoch 38/80: current_loss=9.95345 | best_loss=9.90217
Epoch 39/80: current_loss=9.91743 | best_loss=9.90217
Epoch 40/80: current_loss=9.89560 | best_loss=9.89560
Epoch 41/80: current_loss=9.90327 | best_loss=9.89560
Epoch 42/80: current_loss=9.97481 | best_loss=9.89560
Epoch 43/80: current_loss=9.91187 | best_loss=9.89560
Epoch 44/80: current_loss=9.88469 | best_loss=9.88469
Epoch 45/80: current_loss=9.93239 | best_loss=9.88469
Epoch 46/80: current_loss=9.97149 | best_loss=9.88469
Epoch 47/80: current_loss=9.92926 | best_loss=9.88469
Epoch 48/80: current_loss=9.95684 | best_loss=9.88469
Epoch 49/80: current_loss=9.93287 | best_loss=9.88469
Epoch 50/80: current_loss=9.93904 | best_loss=9.88469
Epoch 51/80: current_loss=9.97063 | best_loss=9.88469
Epoch 52/80: current_loss=9.91608 | best_loss=9.88469
Epoch 53/80: current_loss=9.94415 | best_loss=9.88469
Epoch 54/80: current_loss=9.94523 | best_loss=9.88469
Epoch 55/80: current_loss=9.96949 | best_loss=9.88469
Epoch 56/80: current_loss=9.93498 | best_loss=9.88469
Epoch 57/80: current_loss=9.96568 | best_loss=9.88469
Epoch 58/80: current_loss=9.92573 | best_loss=9.88469
Epoch 59/80: current_loss=9.98137 | best_loss=9.88469
Epoch 60/80: current_loss=9.97017 | best_loss=9.88469
Epoch 61/80: current_loss=9.91144 | best_loss=9.88469
Epoch 62/80: current_loss=9.94802 | best_loss=9.88469
Epoch 63/80: current_loss=9.90864 | best_loss=9.88469
Epoch 64/80: current_loss=9.94227 | best_loss=9.88469
Early Stopping at epoch 64
      explained_var=0.00225 | mse_loss=9.55632
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50394 | best_loss=8.50394
Epoch 1/80: current_loss=8.56907 | best_loss=8.50394
Epoch 2/80: current_loss=8.46394 | best_loss=8.46394
Epoch 3/80: current_loss=8.53023 | best_loss=8.46394
Epoch 4/80: current_loss=8.59218 | best_loss=8.46394
Epoch 5/80: current_loss=8.53892 | best_loss=8.46394
Epoch 6/80: current_loss=8.55431 | best_loss=8.46394
Epoch 7/80: current_loss=8.48926 | best_loss=8.46394
Epoch 8/80: current_loss=8.50468 | best_loss=8.46394
Epoch 9/80: current_loss=8.49067 | best_loss=8.46394
Epoch 10/80: current_loss=8.46839 | best_loss=8.46394
Epoch 11/80: current_loss=8.52988 | best_loss=8.46394
Epoch 12/80: current_loss=8.50426 | best_loss=8.46394
Epoch 13/80: current_loss=8.47932 | best_loss=8.46394
Epoch 14/80: current_loss=8.52590 | best_loss=8.46394
Epoch 15/80: current_loss=8.50556 | best_loss=8.46394
Epoch 16/80: current_loss=8.54082 | best_loss=8.46394
Epoch 17/80: current_loss=8.49023 | best_loss=8.46394
Epoch 18/80: current_loss=8.50070 | best_loss=8.46394
Epoch 19/80: current_loss=8.47637 | best_loss=8.46394
Epoch 20/80: current_loss=8.51095 | best_loss=8.46394
Epoch 21/80: current_loss=8.47945 | best_loss=8.46394
Epoch 22/80: current_loss=8.49070 | best_loss=8.46394
Early Stopping at epoch 22
      explained_var=-0.00075 | mse_loss=8.58380

----------------------------------------------
Params for Trial 50
{'learning_rate': 0.01, 'weight_decay': 0.0012827624987414728, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.01233 | best_loss=10.01233
Epoch 1/80: current_loss=10.02700 | best_loss=10.01233
Epoch 2/80: current_loss=10.04824 | best_loss=10.01233
Epoch 3/80: current_loss=9.93080 | best_loss=9.93080
Epoch 4/80: current_loss=9.92519 | best_loss=9.92519
Epoch 5/80: current_loss=9.99875 | best_loss=9.92519
Epoch 6/80: current_loss=10.11822 | best_loss=9.92519
Epoch 7/80: current_loss=10.79433 | best_loss=9.92519
Epoch 8/80: current_loss=10.33553 | best_loss=9.92519
Epoch 9/80: current_loss=13.06234 | best_loss=9.92519
Epoch 10/80: current_loss=9.95648 | best_loss=9.92519
Epoch 11/80: current_loss=10.05178 | best_loss=9.92519
Epoch 12/80: current_loss=9.90590 | best_loss=9.90590
Epoch 13/80: current_loss=9.90271 | best_loss=9.90271
Epoch 14/80: current_loss=9.86805 | best_loss=9.86805
Epoch 15/80: current_loss=10.01356 | best_loss=9.86805
Epoch 16/80: current_loss=9.89240 | best_loss=9.86805
Epoch 17/80: current_loss=9.91076 | best_loss=9.86805
Epoch 18/80: current_loss=9.90144 | best_loss=9.86805
Epoch 19/80: current_loss=10.08092 | best_loss=9.86805
Epoch 20/80: current_loss=10.11122 | best_loss=9.86805
Epoch 21/80: current_loss=9.94445 | best_loss=9.86805
Epoch 22/80: current_loss=9.96614 | best_loss=9.86805
Epoch 23/80: current_loss=9.90591 | best_loss=9.86805
Epoch 24/80: current_loss=9.90923 | best_loss=9.86805
Epoch 25/80: current_loss=9.90633 | best_loss=9.86805
Epoch 26/80: current_loss=10.03048 | best_loss=9.86805
Epoch 27/80: current_loss=9.89987 | best_loss=9.86805
Epoch 28/80: current_loss=9.91608 | best_loss=9.86805
Epoch 29/80: current_loss=10.59876 | best_loss=9.86805
Epoch 30/80: current_loss=9.93190 | best_loss=9.86805
Epoch 31/80: current_loss=10.24772 | best_loss=9.86805
Epoch 32/80: current_loss=9.89299 | best_loss=9.86805
Epoch 33/80: current_loss=9.92749 | best_loss=9.86805
Epoch 34/80: current_loss=10.64666 | best_loss=9.86805
Early Stopping at epoch 34
      explained_var=0.00035 | mse_loss=9.57484

----------------------------------------------
Params for Trial 51
{'learning_rate': 0.01, 'weight_decay': 0.0017661865766022242, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=11.32902 | best_loss=11.32902
Epoch 1/80: current_loss=9.99225 | best_loss=9.99225
Epoch 2/80: current_loss=9.93485 | best_loss=9.93485
Epoch 3/80: current_loss=9.90414 | best_loss=9.90414
Epoch 4/80: current_loss=9.99034 | best_loss=9.90414
Epoch 5/80: current_loss=9.93765 | best_loss=9.90414
Epoch 6/80: current_loss=10.17508 | best_loss=9.90414
Epoch 7/80: current_loss=9.99157 | best_loss=9.90414
Epoch 8/80: current_loss=10.32459 | best_loss=9.90414
Epoch 9/80: current_loss=9.91212 | best_loss=9.90414
Epoch 10/80: current_loss=10.31936 | best_loss=9.90414
Epoch 11/80: current_loss=9.90662 | best_loss=9.90414
Epoch 12/80: current_loss=9.96405 | best_loss=9.90414
Epoch 13/80: current_loss=10.02076 | best_loss=9.90414
Epoch 14/80: current_loss=9.89801 | best_loss=9.89801
Epoch 15/80: current_loss=9.90456 | best_loss=9.89801
Epoch 16/80: current_loss=10.11525 | best_loss=9.89801
Epoch 17/80: current_loss=9.90301 | best_loss=9.89801
Epoch 18/80: current_loss=10.54410 | best_loss=9.89801
Epoch 19/80: current_loss=9.94623 | best_loss=9.89801
Epoch 20/80: current_loss=10.21655 | best_loss=9.89801
Epoch 21/80: current_loss=10.67780 | best_loss=9.89801
Epoch 22/80: current_loss=10.36331 | best_loss=9.89801
Epoch 23/80: current_loss=10.09062 | best_loss=9.89801
Epoch 24/80: current_loss=10.70600 | best_loss=9.89801
Epoch 25/80: current_loss=10.02971 | best_loss=9.89801
Epoch 26/80: current_loss=10.03557 | best_loss=9.89801
Epoch 27/80: current_loss=10.19848 | best_loss=9.89801
Epoch 28/80: current_loss=9.74210 | best_loss=9.74210
Epoch 29/80: current_loss=9.85574 | best_loss=9.74210
Epoch 30/80: current_loss=9.90694 | best_loss=9.74210
Epoch 31/80: current_loss=9.98482 | best_loss=9.74210
Epoch 32/80: current_loss=10.08974 | best_loss=9.74210
Epoch 33/80: current_loss=10.04781 | best_loss=9.74210
Epoch 34/80: current_loss=10.03666 | best_loss=9.74210
Epoch 35/80: current_loss=10.53526 | best_loss=9.74210
Epoch 36/80: current_loss=11.03154 | best_loss=9.74210
Epoch 37/80: current_loss=9.90700 | best_loss=9.74210
Epoch 38/80: current_loss=9.93312 | best_loss=9.74210
Epoch 39/80: current_loss=9.88364 | best_loss=9.74210
Epoch 40/80: current_loss=9.88312 | best_loss=9.74210
Epoch 41/80: current_loss=10.04495 | best_loss=9.74210
Epoch 42/80: current_loss=10.14033 | best_loss=9.74210
Epoch 43/80: current_loss=9.97513 | best_loss=9.74210
Epoch 44/80: current_loss=10.29828 | best_loss=9.74210
Epoch 45/80: current_loss=10.06035 | best_loss=9.74210
Epoch 46/80: current_loss=10.10753 | best_loss=9.74210
Epoch 47/80: current_loss=9.93364 | best_loss=9.74210
Epoch 48/80: current_loss=10.19425 | best_loss=9.74210
Early Stopping at epoch 48
      explained_var=0.01025 | mse_loss=9.48084
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.55078 | best_loss=8.55078
Epoch 1/80: current_loss=8.63057 | best_loss=8.55078
Epoch 2/80: current_loss=8.58565 | best_loss=8.55078
Epoch 3/80: current_loss=8.34546 | best_loss=8.34546
Epoch 4/80: current_loss=8.40109 | best_loss=8.34546
Epoch 5/80: current_loss=8.55199 | best_loss=8.34546
Epoch 6/80: current_loss=8.52488 | best_loss=8.34546
Epoch 7/80: current_loss=8.45308 | best_loss=8.34546
Epoch 8/80: current_loss=8.61880 | best_loss=8.34546
Epoch 9/80: current_loss=8.66625 | best_loss=8.34546
Epoch 10/80: current_loss=8.75332 | best_loss=8.34546
Epoch 11/80: current_loss=8.67624 | best_loss=8.34546
Epoch 12/80: current_loss=8.51819 | best_loss=8.34546
Epoch 13/80: current_loss=8.49040 | best_loss=8.34546
Epoch 14/80: current_loss=8.78102 | best_loss=8.34546
Epoch 15/80: current_loss=8.55322 | best_loss=8.34546
Epoch 16/80: current_loss=9.15061 | best_loss=8.34546
Epoch 17/80: current_loss=8.69896 | best_loss=8.34546
Epoch 18/80: current_loss=9.89995 | best_loss=8.34546
Epoch 19/80: current_loss=8.63670 | best_loss=8.34546
Epoch 20/80: current_loss=8.83177 | best_loss=8.34546
Epoch 21/80: current_loss=9.04488 | best_loss=8.34546
Epoch 22/80: current_loss=8.71893 | best_loss=8.34546
Epoch 23/80: current_loss=8.98918 | best_loss=8.34546
Early Stopping at epoch 23
      explained_var=0.00740 | mse_loss=8.53692
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.48345 | best_loss=9.48345
Epoch 1/80: current_loss=10.33311 | best_loss=9.48345
Epoch 2/80: current_loss=10.74765 | best_loss=9.48345
Epoch 3/80: current_loss=9.24015 | best_loss=9.24015
Epoch 4/80: current_loss=10.06137 | best_loss=9.24015
Epoch 5/80: current_loss=10.13597 | best_loss=9.24015
Epoch 6/80: current_loss=10.16878 | best_loss=9.24015
Epoch 7/80: current_loss=9.94403 | best_loss=9.24015
Epoch 8/80: current_loss=9.71074 | best_loss=9.24015
Epoch 9/80: current_loss=9.72880 | best_loss=9.24015
Epoch 10/80: current_loss=9.31450 | best_loss=9.24015
Epoch 11/80: current_loss=9.54984 | best_loss=9.24015
Epoch 12/80: current_loss=10.00905 | best_loss=9.24015
Epoch 13/80: current_loss=10.35236 | best_loss=9.24015
Epoch 14/80: current_loss=9.36274 | best_loss=9.24015
Epoch 15/80: current_loss=9.83382 | best_loss=9.24015
Epoch 16/80: current_loss=9.54530 | best_loss=9.24015
Epoch 17/80: current_loss=10.08189 | best_loss=9.24015
Epoch 18/80: current_loss=9.76216 | best_loss=9.24015
Epoch 19/80: current_loss=9.92919 | best_loss=9.24015
Epoch 20/80: current_loss=9.55946 | best_loss=9.24015
Epoch 21/80: current_loss=9.61550 | best_loss=9.24015
Epoch 22/80: current_loss=9.94230 | best_loss=9.24015
Epoch 23/80: current_loss=9.61851 | best_loss=9.24015
Early Stopping at epoch 23
      explained_var=0.01137 | mse_loss=9.39460
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.15517 | best_loss=9.15517
Epoch 1/80: current_loss=9.06333 | best_loss=9.06333
Epoch 2/80: current_loss=9.03196 | best_loss=9.03196
Epoch 3/80: current_loss=9.62874 | best_loss=9.03196
Epoch 4/80: current_loss=9.05288 | best_loss=9.03196
Epoch 5/80: current_loss=9.11140 | best_loss=9.03196
Epoch 6/80: current_loss=9.07060 | best_loss=9.03196
Epoch 7/80: current_loss=9.31667 | best_loss=9.03196
Epoch 8/80: current_loss=9.00565 | best_loss=9.00565
Epoch 9/80: current_loss=9.55670 | best_loss=9.00565
Epoch 10/80: current_loss=9.68557 | best_loss=9.00565
Epoch 11/80: current_loss=9.10058 | best_loss=9.00565
Epoch 12/80: current_loss=9.42080 | best_loss=9.00565
Epoch 13/80: current_loss=8.97435 | best_loss=8.97435
Epoch 14/80: current_loss=8.93342 | best_loss=8.93342
Epoch 15/80: current_loss=9.01742 | best_loss=8.93342
Epoch 16/80: current_loss=9.17485 | best_loss=8.93342
Epoch 17/80: current_loss=8.93789 | best_loss=8.93342
Epoch 18/80: current_loss=9.30634 | best_loss=8.93342
Epoch 19/80: current_loss=9.02083 | best_loss=8.93342
Epoch 20/80: current_loss=9.01529 | best_loss=8.93342
Epoch 21/80: current_loss=9.20471 | best_loss=8.93342
Epoch 22/80: current_loss=9.04978 | best_loss=8.93342
Epoch 23/80: current_loss=8.96611 | best_loss=8.93342
Epoch 24/80: current_loss=8.99739 | best_loss=8.93342
Epoch 25/80: current_loss=9.02233 | best_loss=8.93342
Epoch 26/80: current_loss=9.06252 | best_loss=8.93342
Epoch 27/80: current_loss=9.32305 | best_loss=8.93342
Epoch 28/80: current_loss=9.23878 | best_loss=8.93342
Epoch 29/80: current_loss=9.19863 | best_loss=8.93342
Epoch 30/80: current_loss=9.00861 | best_loss=8.93342
Epoch 31/80: current_loss=9.03477 | best_loss=8.93342
Epoch 32/80: current_loss=9.13058 | best_loss=8.93342
Epoch 33/80: current_loss=9.15047 | best_loss=8.93342
Epoch 34/80: current_loss=9.41962 | best_loss=8.93342
Early Stopping at epoch 34
      explained_var=-0.00004 | mse_loss=8.37869

----------------------------------------------
Params for Trial 52
{'learning_rate': 0.01, 'weight_decay': 0.001642349322684807, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.02972 | best_loss=10.02972
Epoch 1/80: current_loss=9.90643 | best_loss=9.90643
Epoch 2/80: current_loss=9.91837 | best_loss=9.90643
Epoch 3/80: current_loss=9.91738 | best_loss=9.90643
Epoch 4/80: current_loss=10.14143 | best_loss=9.90643
Epoch 5/80: current_loss=10.27592 | best_loss=9.90643
Epoch 6/80: current_loss=10.14371 | best_loss=9.90643
Epoch 7/80: current_loss=9.93416 | best_loss=9.90643
Epoch 8/80: current_loss=9.93118 | best_loss=9.90643
Epoch 9/80: current_loss=9.98510 | best_loss=9.90643
Epoch 10/80: current_loss=10.03530 | best_loss=9.90643
Epoch 11/80: current_loss=9.93469 | best_loss=9.90643
Epoch 12/80: current_loss=9.90522 | best_loss=9.90522
Epoch 13/80: current_loss=10.04168 | best_loss=9.90522
Epoch 14/80: current_loss=10.11772 | best_loss=9.90522
Epoch 15/80: current_loss=10.93445 | best_loss=9.90522
Epoch 16/80: current_loss=9.94174 | best_loss=9.90522
Epoch 17/80: current_loss=10.01922 | best_loss=9.90522
Epoch 18/80: current_loss=10.12182 | best_loss=9.90522
Epoch 19/80: current_loss=11.29433 | best_loss=9.90522
Epoch 20/80: current_loss=9.96588 | best_loss=9.90522
Epoch 21/80: current_loss=9.95656 | best_loss=9.90522
Epoch 22/80: current_loss=10.19777 | best_loss=9.90522
Epoch 23/80: current_loss=9.90585 | best_loss=9.90522
Epoch 24/80: current_loss=10.02643 | best_loss=9.90522
Epoch 25/80: current_loss=10.93659 | best_loss=9.90522
Epoch 26/80: current_loss=10.39871 | best_loss=9.90522
Epoch 27/80: current_loss=9.91420 | best_loss=9.90522
Epoch 28/80: current_loss=9.93027 | best_loss=9.90522
Epoch 29/80: current_loss=10.08130 | best_loss=9.90522
Epoch 30/80: current_loss=9.91329 | best_loss=9.90522
Epoch 31/80: current_loss=10.19720 | best_loss=9.90522
Epoch 32/80: current_loss=10.17213 | best_loss=9.90522
Early Stopping at epoch 32
      explained_var=0.00055 | mse_loss=9.58032

----------------------------------------------
Params for Trial 53
{'learning_rate': 0.01, 'weight_decay': 0.0013428793954548897, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.06204 | best_loss=10.06204
Epoch 1/80: current_loss=10.08373 | best_loss=10.06204
Epoch 2/80: current_loss=10.91078 | best_loss=10.06204
Epoch 3/80: current_loss=9.90572 | best_loss=9.90572
Epoch 4/80: current_loss=10.01893 | best_loss=9.90572
Epoch 5/80: current_loss=10.24838 | best_loss=9.90572
Epoch 6/80: current_loss=10.17278 | best_loss=9.90572
Epoch 7/80: current_loss=10.15946 | best_loss=9.90572
Epoch 8/80: current_loss=10.01542 | best_loss=9.90572
Epoch 9/80: current_loss=9.91235 | best_loss=9.90572
Epoch 10/80: current_loss=10.79095 | best_loss=9.90572
Epoch 11/80: current_loss=10.01823 | best_loss=9.90572
Epoch 12/80: current_loss=9.91407 | best_loss=9.90572
Epoch 13/80: current_loss=9.90457 | best_loss=9.90457
Epoch 14/80: current_loss=9.97387 | best_loss=9.90457
Epoch 15/80: current_loss=9.90781 | best_loss=9.90457
Epoch 16/80: current_loss=9.93211 | best_loss=9.90457
Epoch 17/80: current_loss=9.96635 | best_loss=9.90457
Epoch 18/80: current_loss=9.94698 | best_loss=9.90457
Epoch 19/80: current_loss=9.97828 | best_loss=9.90457
Epoch 20/80: current_loss=9.94256 | best_loss=9.90457
Epoch 21/80: current_loss=9.90885 | best_loss=9.90457
Epoch 22/80: current_loss=9.94581 | best_loss=9.90457
Epoch 23/80: current_loss=9.91264 | best_loss=9.90457
Epoch 24/80: current_loss=9.90379 | best_loss=9.90379
Epoch 25/80: current_loss=9.91568 | best_loss=9.90379
Epoch 26/80: current_loss=10.10743 | best_loss=9.90379
Epoch 27/80: current_loss=10.66568 | best_loss=9.90379
Epoch 28/80: current_loss=9.90773 | best_loss=9.90379
Epoch 29/80: current_loss=9.92757 | best_loss=9.90379
Epoch 30/80: current_loss=10.27346 | best_loss=9.90379
Epoch 31/80: current_loss=9.96880 | best_loss=9.90379
Epoch 32/80: current_loss=9.91330 | best_loss=9.90379
Epoch 33/80: current_loss=10.41095 | best_loss=9.90379
Epoch 34/80: current_loss=9.91015 | best_loss=9.90379
Epoch 35/80: current_loss=11.75907 | best_loss=9.90379
Epoch 36/80: current_loss=10.25176 | best_loss=9.90379
Epoch 37/80: current_loss=10.05539 | best_loss=9.90379
Epoch 38/80: current_loss=9.90374 | best_loss=9.90374
Epoch 39/80: current_loss=9.90954 | best_loss=9.90374
Epoch 40/80: current_loss=9.90857 | best_loss=9.90374
Epoch 41/80: current_loss=9.92890 | best_loss=9.90374
Epoch 42/80: current_loss=9.96432 | best_loss=9.90374
Epoch 43/80: current_loss=9.90346 | best_loss=9.90346
Epoch 44/80: current_loss=10.25522 | best_loss=9.90346
Epoch 45/80: current_loss=9.96512 | best_loss=9.90346
Epoch 46/80: current_loss=10.58681 | best_loss=9.90346
Epoch 47/80: current_loss=10.05659 | best_loss=9.90346
Epoch 48/80: current_loss=10.36416 | best_loss=9.90346
Epoch 49/80: current_loss=10.05890 | best_loss=9.90346
Epoch 50/80: current_loss=9.95892 | best_loss=9.90346
Epoch 51/80: current_loss=9.93336 | best_loss=9.90346
Epoch 52/80: current_loss=10.27503 | best_loss=9.90346
Epoch 53/80: current_loss=9.97015 | best_loss=9.90346
Epoch 54/80: current_loss=9.91398 | best_loss=9.90346
Epoch 55/80: current_loss=10.32201 | best_loss=9.90346
Epoch 56/80: current_loss=9.90547 | best_loss=9.90346
Epoch 57/80: current_loss=10.91357 | best_loss=9.90346
Epoch 58/80: current_loss=10.01923 | best_loss=9.90346
Epoch 59/80: current_loss=9.93245 | best_loss=9.90346
Epoch 60/80: current_loss=10.16736 | best_loss=9.90346
Epoch 61/80: current_loss=11.06853 | best_loss=9.90346
Epoch 62/80: current_loss=9.91451 | best_loss=9.90346
Epoch 63/80: current_loss=10.53004 | best_loss=9.90346
Early Stopping at epoch 63
      explained_var=0.00099 | mse_loss=9.57988

----------------------------------------------
Params for Trial 54
{'learning_rate': 0.001, 'weight_decay': 0.0007719365634779184, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.09145 | best_loss=10.09145
Epoch 1/80: current_loss=9.91780 | best_loss=9.91780
Epoch 2/80: current_loss=9.92731 | best_loss=9.91780
Epoch 3/80: current_loss=9.89623 | best_loss=9.89623
Epoch 4/80: current_loss=9.89542 | best_loss=9.89542
Epoch 5/80: current_loss=9.89525 | best_loss=9.89525
Epoch 6/80: current_loss=9.98521 | best_loss=9.89525
Epoch 7/80: current_loss=9.90984 | best_loss=9.89525
Epoch 8/80: current_loss=9.93911 | best_loss=9.89525
Epoch 9/80: current_loss=9.92901 | best_loss=9.89525
Epoch 10/80: current_loss=9.92251 | best_loss=9.89525
Epoch 11/80: current_loss=9.91517 | best_loss=9.89525
Epoch 12/80: current_loss=9.92179 | best_loss=9.89525
Epoch 13/80: current_loss=9.92229 | best_loss=9.89525
Epoch 14/80: current_loss=9.90534 | best_loss=9.89525
Epoch 15/80: current_loss=9.91161 | best_loss=9.89525
Epoch 16/80: current_loss=9.94360 | best_loss=9.89525
Epoch 17/80: current_loss=9.90757 | best_loss=9.89525
Epoch 18/80: current_loss=9.92855 | best_loss=9.89525
Epoch 19/80: current_loss=9.89948 | best_loss=9.89525
Epoch 20/80: current_loss=9.90133 | best_loss=9.89525
Epoch 21/80: current_loss=9.91297 | best_loss=9.89525
Epoch 22/80: current_loss=9.94577 | best_loss=9.89525
Epoch 23/80: current_loss=9.91856 | best_loss=9.89525
Epoch 24/80: current_loss=9.91616 | best_loss=9.89525
Epoch 25/80: current_loss=9.89803 | best_loss=9.89525
Early Stopping at epoch 25
      explained_var=0.00249 | mse_loss=9.57103
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50288 | best_loss=8.50288
Epoch 1/80: current_loss=8.52235 | best_loss=8.50288
Epoch 2/80: current_loss=8.52541 | best_loss=8.50288
Epoch 3/80: current_loss=8.51216 | best_loss=8.50288
Epoch 4/80: current_loss=8.54281 | best_loss=8.50288
Epoch 5/80: current_loss=8.58130 | best_loss=8.50288
Epoch 6/80: current_loss=8.60478 | best_loss=8.50288
Epoch 7/80: current_loss=8.50767 | best_loss=8.50288
Epoch 8/80: current_loss=8.51126 | best_loss=8.50288
Epoch 9/80: current_loss=8.52760 | best_loss=8.50288
Epoch 10/80: current_loss=8.56365 | best_loss=8.50288
Epoch 11/80: current_loss=8.59173 | best_loss=8.50288
Epoch 12/80: current_loss=8.55433 | best_loss=8.50288
Epoch 13/80: current_loss=8.50011 | best_loss=8.50011
Epoch 14/80: current_loss=8.63286 | best_loss=8.50011
Epoch 15/80: current_loss=8.54368 | best_loss=8.50011
Epoch 16/80: current_loss=8.49297 | best_loss=8.49297
Epoch 17/80: current_loss=8.48752 | best_loss=8.48752
Epoch 18/80: current_loss=8.49739 | best_loss=8.48752
Epoch 19/80: current_loss=8.49792 | best_loss=8.48752
Epoch 20/80: current_loss=8.50184 | best_loss=8.48752
Epoch 21/80: current_loss=8.51244 | best_loss=8.48752
Epoch 22/80: current_loss=8.50401 | best_loss=8.48752
Epoch 23/80: current_loss=8.55324 | best_loss=8.48752
Epoch 24/80: current_loss=8.54939 | best_loss=8.48752
Epoch 25/80: current_loss=8.48362 | best_loss=8.48362
Epoch 26/80: current_loss=8.59454 | best_loss=8.48362
Epoch 27/80: current_loss=8.55356 | best_loss=8.48362
Epoch 28/80: current_loss=8.52693 | best_loss=8.48362
Epoch 29/80: current_loss=8.64046 | best_loss=8.48362
Epoch 30/80: current_loss=8.57212 | best_loss=8.48362
Epoch 31/80: current_loss=8.76245 | best_loss=8.48362
Epoch 32/80: current_loss=8.61425 | best_loss=8.48362
Epoch 33/80: current_loss=8.52807 | best_loss=8.48362
Epoch 34/80: current_loss=8.51931 | best_loss=8.48362
Epoch 35/80: current_loss=8.51111 | best_loss=8.48362
Epoch 36/80: current_loss=8.53540 | best_loss=8.48362
Epoch 37/80: current_loss=8.50520 | best_loss=8.48362
Epoch 38/80: current_loss=8.61943 | best_loss=8.48362
Epoch 39/80: current_loss=8.51165 | best_loss=8.48362
Epoch 40/80: current_loss=8.55033 | best_loss=8.48362
Epoch 41/80: current_loss=8.49883 | best_loss=8.48362
Epoch 42/80: current_loss=8.54645 | best_loss=8.48362
Epoch 43/80: current_loss=8.72719 | best_loss=8.48362
Epoch 44/80: current_loss=8.56834 | best_loss=8.48362
Epoch 45/80: current_loss=8.51639 | best_loss=8.48362
Early Stopping at epoch 45
      explained_var=0.00122 | mse_loss=8.56615
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.31183 | best_loss=9.31183
Epoch 1/80: current_loss=9.40374 | best_loss=9.31183
Epoch 2/80: current_loss=9.40416 | best_loss=9.31183
Epoch 3/80: current_loss=9.50881 | best_loss=9.31183
Epoch 4/80: current_loss=9.57543 | best_loss=9.31183
Epoch 5/80: current_loss=9.32095 | best_loss=9.31183
Epoch 6/80: current_loss=9.34505 | best_loss=9.31183
Epoch 7/80: current_loss=9.63540 | best_loss=9.31183
Epoch 8/80: current_loss=9.41262 | best_loss=9.31183
Epoch 9/80: current_loss=9.37103 | best_loss=9.31183
Epoch 10/80: current_loss=9.36254 | best_loss=9.31183
Epoch 11/80: current_loss=9.45860 | best_loss=9.31183
Epoch 12/80: current_loss=9.71748 | best_loss=9.31183
Epoch 13/80: current_loss=9.65817 | best_loss=9.31183
Epoch 14/80: current_loss=9.41028 | best_loss=9.31183
Epoch 15/80: current_loss=9.45916 | best_loss=9.31183
Epoch 16/80: current_loss=9.54039 | best_loss=9.31183
Epoch 17/80: current_loss=9.74993 | best_loss=9.31183
Epoch 18/80: current_loss=9.39392 | best_loss=9.31183
Epoch 19/80: current_loss=9.41182 | best_loss=9.31183
Epoch 20/80: current_loss=9.49950 | best_loss=9.31183
Early Stopping at epoch 20
      explained_var=-0.00012 | mse_loss=9.43672
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.95071 | best_loss=8.95071
Epoch 1/80: current_loss=9.13530 | best_loss=8.95071
Epoch 2/80: current_loss=8.99399 | best_loss=8.95071
Epoch 3/80: current_loss=9.01670 | best_loss=8.95071
Epoch 4/80: current_loss=9.00380 | best_loss=8.95071
Epoch 5/80: current_loss=8.95822 | best_loss=8.95071
Epoch 6/80: current_loss=9.00827 | best_loss=8.95071
Epoch 7/80: current_loss=8.97224 | best_loss=8.95071
Epoch 8/80: current_loss=9.01077 | best_loss=8.95071
Epoch 9/80: current_loss=9.06911 | best_loss=8.95071
Epoch 10/80: current_loss=8.99778 | best_loss=8.95071
Epoch 11/80: current_loss=8.99731 | best_loss=8.95071
Epoch 12/80: current_loss=8.98861 | best_loss=8.95071
Epoch 13/80: current_loss=9.13506 | best_loss=8.95071
Epoch 14/80: current_loss=8.97267 | best_loss=8.95071
Epoch 15/80: current_loss=9.08888 | best_loss=8.95071
Epoch 16/80: current_loss=9.06898 | best_loss=8.95071
Epoch 17/80: current_loss=9.04182 | best_loss=8.95071
Epoch 18/80: current_loss=9.04202 | best_loss=8.95071
Epoch 19/80: current_loss=9.03416 | best_loss=8.95071
Epoch 20/80: current_loss=9.05245 | best_loss=8.95071
Early Stopping at epoch 20
      explained_var=0.00089 | mse_loss=8.35982

----------------------------------------------
Params for Trial 55
{'learning_rate': 0.01, 'weight_decay': 0.0024399139710516867, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.70618 | best_loss=10.70618
Epoch 1/80: current_loss=10.21837 | best_loss=10.21837
Epoch 2/80: current_loss=10.12429 | best_loss=10.12429
Epoch 3/80: current_loss=10.02006 | best_loss=10.02006
Epoch 4/80: current_loss=10.01445 | best_loss=10.01445
Epoch 5/80: current_loss=9.98682 | best_loss=9.98682
Epoch 6/80: current_loss=11.53763 | best_loss=9.98682
Epoch 7/80: current_loss=10.30780 | best_loss=9.98682
Epoch 8/80: current_loss=10.12368 | best_loss=9.98682
Epoch 9/80: current_loss=9.90904 | best_loss=9.90904
Epoch 10/80: current_loss=9.94287 | best_loss=9.90904
Epoch 11/80: current_loss=10.14369 | best_loss=9.90904
Epoch 12/80: current_loss=10.59653 | best_loss=9.90904
Epoch 13/80: current_loss=10.12742 | best_loss=9.90904
Epoch 14/80: current_loss=9.90076 | best_loss=9.90076
Epoch 15/80: current_loss=10.11944 | best_loss=9.90076
Epoch 16/80: current_loss=12.61394 | best_loss=9.90076
Epoch 17/80: current_loss=10.14305 | best_loss=9.90076
Epoch 18/80: current_loss=9.96222 | best_loss=9.90076
Epoch 19/80: current_loss=9.97744 | best_loss=9.90076
Epoch 20/80: current_loss=10.00025 | best_loss=9.90076
Epoch 21/80: current_loss=9.99595 | best_loss=9.90076
Epoch 22/80: current_loss=9.95577 | best_loss=9.90076
Epoch 23/80: current_loss=9.90902 | best_loss=9.90076
Epoch 24/80: current_loss=10.11986 | best_loss=9.90076
Epoch 25/80: current_loss=9.91914 | best_loss=9.90076
Epoch 26/80: current_loss=9.92065 | best_loss=9.90076
Epoch 27/80: current_loss=9.91700 | best_loss=9.90076
Epoch 28/80: current_loss=10.19541 | best_loss=9.90076
Epoch 29/80: current_loss=9.90512 | best_loss=9.90076
Epoch 30/80: current_loss=10.14170 | best_loss=9.90076
Epoch 31/80: current_loss=10.03564 | best_loss=9.90076
Epoch 32/80: current_loss=10.40192 | best_loss=9.90076
Epoch 33/80: current_loss=9.91128 | best_loss=9.90076
Epoch 34/80: current_loss=9.89873 | best_loss=9.89873
Epoch 35/80: current_loss=9.90698 | best_loss=9.89873
Epoch 36/80: current_loss=9.89874 | best_loss=9.89873
Epoch 37/80: current_loss=10.05731 | best_loss=9.89873
Epoch 38/80: current_loss=10.07570 | best_loss=9.89873
Epoch 39/80: current_loss=9.92680 | best_loss=9.89873
Epoch 40/80: current_loss=9.97221 | best_loss=9.89873
Epoch 41/80: current_loss=10.23725 | best_loss=9.89873
Epoch 42/80: current_loss=10.00507 | best_loss=9.89873
Epoch 43/80: current_loss=10.49235 | best_loss=9.89873
Epoch 44/80: current_loss=10.25742 | best_loss=9.89873
Epoch 45/80: current_loss=9.99973 | best_loss=9.89873
Epoch 46/80: current_loss=10.00461 | best_loss=9.89873
Epoch 47/80: current_loss=9.94642 | best_loss=9.89873
Epoch 48/80: current_loss=10.03714 | best_loss=9.89873
Epoch 49/80: current_loss=9.93418 | best_loss=9.89873
Epoch 50/80: current_loss=9.90827 | best_loss=9.89873
Epoch 51/80: current_loss=11.05130 | best_loss=9.89873
Epoch 52/80: current_loss=11.23024 | best_loss=9.89873
Epoch 53/80: current_loss=10.06964 | best_loss=9.89873
Epoch 54/80: current_loss=9.89818 | best_loss=9.89818
Epoch 55/80: current_loss=9.93151 | best_loss=9.89818
Epoch 56/80: current_loss=9.91704 | best_loss=9.89818
Epoch 57/80: current_loss=11.50635 | best_loss=9.89818
Epoch 58/80: current_loss=9.93512 | best_loss=9.89818
Epoch 59/80: current_loss=9.92560 | best_loss=9.89818
Epoch 60/80: current_loss=9.90006 | best_loss=9.89818
Epoch 61/80: current_loss=9.89708 | best_loss=9.89708
Epoch 62/80: current_loss=9.96056 | best_loss=9.89708
Epoch 63/80: current_loss=9.88748 | best_loss=9.88748
Epoch 64/80: current_loss=9.90144 | best_loss=9.88748
Epoch 65/80: current_loss=10.03990 | best_loss=9.88748
Epoch 66/80: current_loss=10.30215 | best_loss=9.88748
Epoch 67/80: current_loss=10.07965 | best_loss=9.88748
Epoch 68/80: current_loss=9.91282 | best_loss=9.88748
Epoch 69/80: current_loss=10.07552 | best_loss=9.88748
Epoch 70/80: current_loss=10.41574 | best_loss=9.88748
Epoch 71/80: current_loss=11.02312 | best_loss=9.88748
Epoch 72/80: current_loss=9.90898 | best_loss=9.88748
Epoch 73/80: current_loss=9.90599 | best_loss=9.88748
Epoch 74/80: current_loss=10.18062 | best_loss=9.88748
Epoch 75/80: current_loss=10.18160 | best_loss=9.88748
Epoch 76/80: current_loss=9.89584 | best_loss=9.88748
Epoch 77/80: current_loss=10.95324 | best_loss=9.88748
Epoch 78/80: current_loss=10.11900 | best_loss=9.88748
Epoch 79/80: current_loss=10.20086 | best_loss=9.88748
      explained_var=0.00240 | mse_loss=9.55542
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.89011 | best_loss=8.89011
Epoch 1/80: current_loss=8.68636 | best_loss=8.68636
Epoch 2/80: current_loss=8.47139 | best_loss=8.47139
Epoch 3/80: current_loss=8.49412 | best_loss=8.47139
Epoch 4/80: current_loss=8.55863 | best_loss=8.47139
Epoch 5/80: current_loss=8.97734 | best_loss=8.47139
Epoch 6/80: current_loss=8.52571 | best_loss=8.47139
Epoch 7/80: current_loss=9.21652 | best_loss=8.47139
Epoch 8/80: current_loss=8.79172 | best_loss=8.47139
Epoch 9/80: current_loss=8.69915 | best_loss=8.47139
Epoch 10/80: current_loss=8.51018 | best_loss=8.47139
Epoch 11/80: current_loss=8.53724 | best_loss=8.47139
Epoch 12/80: current_loss=8.55202 | best_loss=8.47139
Epoch 13/80: current_loss=9.64436 | best_loss=8.47139
Epoch 14/80: current_loss=8.64420 | best_loss=8.47139
Epoch 15/80: current_loss=8.47370 | best_loss=8.47139
Epoch 16/80: current_loss=9.14005 | best_loss=8.47139
Epoch 17/80: current_loss=8.59128 | best_loss=8.47139
Epoch 18/80: current_loss=8.60545 | best_loss=8.47139
Epoch 19/80: current_loss=9.41185 | best_loss=8.47139
Epoch 20/80: current_loss=8.54079 | best_loss=8.47139
Epoch 21/80: current_loss=8.85028 | best_loss=8.47139
Epoch 22/80: current_loss=8.70545 | best_loss=8.47139
Early Stopping at epoch 22
      explained_var=0.00263 | mse_loss=8.55762
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.34087 | best_loss=9.34087
Epoch 1/80: current_loss=9.33209 | best_loss=9.33209
Epoch 2/80: current_loss=9.36953 | best_loss=9.33209
Epoch 3/80: current_loss=10.19124 | best_loss=9.33209
Epoch 4/80: current_loss=9.85851 | best_loss=9.33209
Epoch 5/80: current_loss=9.33508 | best_loss=9.33209
Epoch 6/80: current_loss=9.40237 | best_loss=9.33209
Epoch 7/80: current_loss=10.52750 | best_loss=9.33209
Epoch 8/80: current_loss=9.46433 | best_loss=9.33209
Epoch 9/80: current_loss=9.29920 | best_loss=9.29920
Epoch 10/80: current_loss=9.56254 | best_loss=9.29920
Epoch 11/80: current_loss=10.14554 | best_loss=9.29920
Epoch 12/80: current_loss=9.33398 | best_loss=9.29920
Epoch 13/80: current_loss=10.07272 | best_loss=9.29920
Epoch 14/80: current_loss=9.29090 | best_loss=9.29090
Epoch 15/80: current_loss=9.40332 | best_loss=9.29090
Epoch 16/80: current_loss=9.33904 | best_loss=9.29090
Epoch 17/80: current_loss=9.42317 | best_loss=9.29090
Epoch 18/80: current_loss=9.39911 | best_loss=9.29090
Epoch 19/80: current_loss=9.27427 | best_loss=9.27427
Epoch 20/80: current_loss=9.79643 | best_loss=9.27427
Epoch 21/80: current_loss=9.58562 | best_loss=9.27427
Epoch 22/80: current_loss=9.29244 | best_loss=9.27427
Epoch 23/80: current_loss=9.30149 | best_loss=9.27427
Epoch 24/80: current_loss=9.26849 | best_loss=9.26849
Epoch 25/80: current_loss=10.03089 | best_loss=9.26849
Epoch 26/80: current_loss=9.27179 | best_loss=9.26849
Epoch 27/80: current_loss=9.29682 | best_loss=9.26849
Epoch 28/80: current_loss=9.42528 | best_loss=9.26849
Epoch 29/80: current_loss=9.70649 | best_loss=9.26849
Epoch 30/80: current_loss=9.48877 | best_loss=9.26849
Epoch 31/80: current_loss=9.29810 | best_loss=9.26849
Epoch 32/80: current_loss=9.49164 | best_loss=9.26849
Epoch 33/80: current_loss=9.43907 | best_loss=9.26849
Epoch 34/80: current_loss=9.41234 | best_loss=9.26849
Epoch 35/80: current_loss=9.27463 | best_loss=9.26849
Epoch 36/80: current_loss=10.62753 | best_loss=9.26849
Epoch 37/80: current_loss=9.39114 | best_loss=9.26849
Epoch 38/80: current_loss=9.27315 | best_loss=9.26849
Epoch 39/80: current_loss=10.14564 | best_loss=9.26849
Epoch 40/80: current_loss=10.51349 | best_loss=9.26849
Epoch 41/80: current_loss=10.22509 | best_loss=9.26849
Epoch 42/80: current_loss=9.28513 | best_loss=9.26849
Epoch 43/80: current_loss=10.71517 | best_loss=9.26849
Epoch 44/80: current_loss=9.62243 | best_loss=9.26849
Early Stopping at epoch 44
      explained_var=-0.00138 | mse_loss=9.39894
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.95989 | best_loss=8.95989
Epoch 1/80: current_loss=10.16882 | best_loss=8.95989
Epoch 2/80: current_loss=9.85098 | best_loss=8.95989
Epoch 3/80: current_loss=9.09581 | best_loss=8.95989
Epoch 4/80: current_loss=10.08683 | best_loss=8.95989
Epoch 5/80: current_loss=8.95709 | best_loss=8.95709
Epoch 6/80: current_loss=8.99072 | best_loss=8.95709
Epoch 7/80: current_loss=8.95000 | best_loss=8.95000
Epoch 8/80: current_loss=8.95760 | best_loss=8.95000
Epoch 9/80: current_loss=9.25497 | best_loss=8.95000
Epoch 10/80: current_loss=8.98128 | best_loss=8.95000
Epoch 11/80: current_loss=9.19257 | best_loss=8.95000
Epoch 12/80: current_loss=8.94087 | best_loss=8.94087
Epoch 13/80: current_loss=8.96518 | best_loss=8.94087
Epoch 14/80: current_loss=9.02406 | best_loss=8.94087
Epoch 15/80: current_loss=8.98927 | best_loss=8.94087
Epoch 16/80: current_loss=8.98975 | best_loss=8.94087
Epoch 17/80: current_loss=8.93123 | best_loss=8.93123
Epoch 18/80: current_loss=8.99636 | best_loss=8.93123
Epoch 19/80: current_loss=9.15306 | best_loss=8.93123
Epoch 20/80: current_loss=9.06193 | best_loss=8.93123
Epoch 21/80: current_loss=8.97985 | best_loss=8.93123
Epoch 22/80: current_loss=9.43673 | best_loss=8.93123
Epoch 23/80: current_loss=9.36108 | best_loss=8.93123
Epoch 24/80: current_loss=8.95084 | best_loss=8.93123
Epoch 25/80: current_loss=9.75733 | best_loss=8.93123
Epoch 26/80: current_loss=8.96346 | best_loss=8.93123
Epoch 27/80: current_loss=9.11898 | best_loss=8.93123
Epoch 28/80: current_loss=9.19813 | best_loss=8.93123
Epoch 29/80: current_loss=8.97313 | best_loss=8.93123
Epoch 30/80: current_loss=9.05773 | best_loss=8.93123
Epoch 31/80: current_loss=9.56483 | best_loss=8.93123
Epoch 32/80: current_loss=9.01648 | best_loss=8.93123
Epoch 33/80: current_loss=9.20719 | best_loss=8.93123
Epoch 34/80: current_loss=9.02687 | best_loss=8.93123
Epoch 35/80: current_loss=9.05777 | best_loss=8.93123
Epoch 36/80: current_loss=9.66332 | best_loss=8.93123
Epoch 37/80: current_loss=8.98134 | best_loss=8.93123
Early Stopping at epoch 37
      explained_var=0.00107 | mse_loss=8.35332
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.54992 | best_loss=8.54992
Epoch 1/80: current_loss=8.80048 | best_loss=8.54992
Epoch 2/80: current_loss=8.61619 | best_loss=8.54992
Epoch 3/80: current_loss=8.80939 | best_loss=8.54992
Epoch 4/80: current_loss=8.49894 | best_loss=8.49894
Epoch 5/80: current_loss=8.57806 | best_loss=8.49894
Epoch 6/80: current_loss=8.48162 | best_loss=8.48162
Epoch 7/80: current_loss=8.60852 | best_loss=8.48162
Epoch 8/80: current_loss=8.51274 | best_loss=8.48162
Epoch 9/80: current_loss=8.55884 | best_loss=8.48162
Epoch 10/80: current_loss=8.50536 | best_loss=8.48162
Epoch 11/80: current_loss=8.49561 | best_loss=8.48162
Epoch 12/80: current_loss=8.62169 | best_loss=8.48162
Epoch 13/80: current_loss=8.62945 | best_loss=8.48162
Epoch 14/80: current_loss=8.96291 | best_loss=8.48162
Epoch 15/80: current_loss=8.59527 | best_loss=8.48162
Epoch 16/80: current_loss=8.58427 | best_loss=8.48162
Epoch 17/80: current_loss=9.59348 | best_loss=8.48162
Epoch 18/80: current_loss=8.56103 | best_loss=8.48162
Epoch 19/80: current_loss=8.50602 | best_loss=8.48162
Epoch 20/80: current_loss=8.52536 | best_loss=8.48162
Epoch 21/80: current_loss=8.70669 | best_loss=8.48162
Epoch 22/80: current_loss=8.51254 | best_loss=8.48162
Epoch 23/80: current_loss=8.50827 | best_loss=8.48162
Epoch 24/80: current_loss=8.76852 | best_loss=8.48162
Epoch 25/80: current_loss=8.49079 | best_loss=8.48162
Epoch 26/80: current_loss=8.58957 | best_loss=8.48162
Early Stopping at epoch 26
      explained_var=0.00024 | mse_loss=8.19914
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=0.00099| avg_loss=8.81289
----------------------------------------------


----------------------------------------------
Params for Trial 56
{'learning_rate': 0.01, 'weight_decay': 0.00036612952932738724, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.96036 | best_loss=9.96036
Epoch 1/80: current_loss=10.15499 | best_loss=9.96036
Epoch 2/80: current_loss=10.25439 | best_loss=9.96036
Epoch 3/80: current_loss=10.01054 | best_loss=9.96036
Epoch 4/80: current_loss=10.04275 | best_loss=9.96036
Epoch 5/80: current_loss=10.27640 | best_loss=9.96036
Epoch 6/80: current_loss=10.81929 | best_loss=9.96036
Epoch 7/80: current_loss=10.02223 | best_loss=9.96036
Epoch 8/80: current_loss=10.10013 | best_loss=9.96036
Epoch 9/80: current_loss=10.38627 | best_loss=9.96036
Epoch 10/80: current_loss=9.91521 | best_loss=9.91521
Epoch 11/80: current_loss=10.16075 | best_loss=9.91521
Epoch 12/80: current_loss=9.90359 | best_loss=9.90359
Epoch 13/80: current_loss=9.94752 | best_loss=9.90359
Epoch 14/80: current_loss=10.00578 | best_loss=9.90359
Epoch 15/80: current_loss=10.34318 | best_loss=9.90359
Epoch 16/80: current_loss=9.96808 | best_loss=9.90359
Epoch 17/80: current_loss=10.06320 | best_loss=9.90359
Epoch 18/80: current_loss=10.10484 | best_loss=9.90359
Epoch 19/80: current_loss=9.91285 | best_loss=9.90359
Epoch 20/80: current_loss=9.93314 | best_loss=9.90359
Epoch 21/80: current_loss=10.21459 | best_loss=9.90359
Epoch 22/80: current_loss=9.94632 | best_loss=9.90359
Epoch 23/80: current_loss=9.92029 | best_loss=9.90359
Epoch 24/80: current_loss=9.91557 | best_loss=9.90359
Epoch 25/80: current_loss=10.11719 | best_loss=9.90359
Epoch 26/80: current_loss=10.04079 | best_loss=9.90359
Epoch 27/80: current_loss=9.93857 | best_loss=9.90359
Epoch 28/80: current_loss=9.94664 | best_loss=9.90359
Epoch 29/80: current_loss=10.57726 | best_loss=9.90359
Epoch 30/80: current_loss=9.99955 | best_loss=9.90359
Epoch 31/80: current_loss=9.92160 | best_loss=9.90359
Epoch 32/80: current_loss=9.94936 | best_loss=9.90359
Early Stopping at epoch 32
      explained_var=0.00132 | mse_loss=9.56591
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.47733 | best_loss=8.47733
Epoch 1/80: current_loss=8.59912 | best_loss=8.47733
Epoch 2/80: current_loss=8.56300 | best_loss=8.47733
Epoch 3/80: current_loss=9.26577 | best_loss=8.47733
Epoch 4/80: current_loss=9.75963 | best_loss=8.47733
Epoch 5/80: current_loss=8.62686 | best_loss=8.47733
Epoch 6/80: current_loss=8.54791 | best_loss=8.47733
Epoch 7/80: current_loss=8.61709 | best_loss=8.47733
Epoch 8/80: current_loss=9.08037 | best_loss=8.47733
Epoch 9/80: current_loss=8.85235 | best_loss=8.47733
Epoch 10/80: current_loss=8.58759 | best_loss=8.47733
Epoch 11/80: current_loss=8.85142 | best_loss=8.47733
Epoch 12/80: current_loss=8.73188 | best_loss=8.47733
Epoch 13/80: current_loss=8.87093 | best_loss=8.47733
Epoch 14/80: current_loss=8.96857 | best_loss=8.47733
Epoch 15/80: current_loss=8.46352 | best_loss=8.46352
Epoch 16/80: current_loss=8.64713 | best_loss=8.46352
Epoch 17/80: current_loss=8.50676 | best_loss=8.46352
Epoch 18/80: current_loss=8.51173 | best_loss=8.46352
Epoch 19/80: current_loss=9.17497 | best_loss=8.46352
Epoch 20/80: current_loss=8.64362 | best_loss=8.46352
Epoch 21/80: current_loss=8.65631 | best_loss=8.46352
Epoch 22/80: current_loss=8.50812 | best_loss=8.46352
Epoch 23/80: current_loss=8.71163 | best_loss=8.46352
Epoch 24/80: current_loss=8.79041 | best_loss=8.46352
Epoch 25/80: current_loss=8.56931 | best_loss=8.46352
Epoch 26/80: current_loss=8.50250 | best_loss=8.46352
Epoch 27/80: current_loss=8.54834 | best_loss=8.46352
Epoch 28/80: current_loss=9.31129 | best_loss=8.46352
Epoch 29/80: current_loss=8.79563 | best_loss=8.46352
Epoch 30/80: current_loss=8.48748 | best_loss=8.46352
Epoch 31/80: current_loss=8.64269 | best_loss=8.46352
Epoch 32/80: current_loss=8.73188 | best_loss=8.46352
Epoch 33/80: current_loss=8.66676 | best_loss=8.46352
Epoch 34/80: current_loss=8.76538 | best_loss=8.46352
Epoch 35/80: current_loss=9.15286 | best_loss=8.46352
Early Stopping at epoch 35
      explained_var=0.00479 | mse_loss=8.53547
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.76609 | best_loss=10.76609
Epoch 1/80: current_loss=9.33906 | best_loss=9.33906
Epoch 2/80: current_loss=9.28676 | best_loss=9.28676
Epoch 3/80: current_loss=9.32005 | best_loss=9.28676
Epoch 4/80: current_loss=10.39364 | best_loss=9.28676
Epoch 5/80: current_loss=9.49218 | best_loss=9.28676
Epoch 6/80: current_loss=9.29027 | best_loss=9.28676
Epoch 7/80: current_loss=9.40575 | best_loss=9.28676
Epoch 8/80: current_loss=9.62683 | best_loss=9.28676
Epoch 9/80: current_loss=9.35074 | best_loss=9.28676
Epoch 10/80: current_loss=9.76909 | best_loss=9.28676
Epoch 11/80: current_loss=9.37535 | best_loss=9.28676
Epoch 12/80: current_loss=9.99351 | best_loss=9.28676
Epoch 13/80: current_loss=9.42577 | best_loss=9.28676
Epoch 14/80: current_loss=9.29847 | best_loss=9.28676
Epoch 15/80: current_loss=9.37804 | best_loss=9.28676
Epoch 16/80: current_loss=10.51397 | best_loss=9.28676
Epoch 17/80: current_loss=9.41576 | best_loss=9.28676
Epoch 18/80: current_loss=9.30808 | best_loss=9.28676
Epoch 19/80: current_loss=9.32090 | best_loss=9.28676
Epoch 20/80: current_loss=9.47316 | best_loss=9.28676
Epoch 21/80: current_loss=9.31062 | best_loss=9.28676
Epoch 22/80: current_loss=10.34258 | best_loss=9.28676
Early Stopping at epoch 22
      explained_var=-0.00618 | mse_loss=9.43533
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.19929 | best_loss=9.19929
Epoch 1/80: current_loss=9.03316 | best_loss=9.03316
Epoch 2/80: current_loss=8.96709 | best_loss=8.96709
Epoch 3/80: current_loss=8.98175 | best_loss=8.96709
Epoch 4/80: current_loss=8.90121 | best_loss=8.90121
Epoch 5/80: current_loss=8.98546 | best_loss=8.90121
Epoch 6/80: current_loss=9.02132 | best_loss=8.90121
Epoch 7/80: current_loss=9.01968 | best_loss=8.90121
Epoch 8/80: current_loss=8.80867 | best_loss=8.80867
Epoch 9/80: current_loss=9.26857 | best_loss=8.80867
Epoch 10/80: current_loss=9.14315 | best_loss=8.80867
Epoch 11/80: current_loss=9.12217 | best_loss=8.80867
Epoch 12/80: current_loss=9.24722 | best_loss=8.80867
Epoch 13/80: current_loss=9.06473 | best_loss=8.80867
Epoch 14/80: current_loss=8.87998 | best_loss=8.80867
Epoch 15/80: current_loss=8.87819 | best_loss=8.80867
Epoch 16/80: current_loss=8.82525 | best_loss=8.80867
Epoch 17/80: current_loss=9.62751 | best_loss=8.80867
Epoch 18/80: current_loss=9.00548 | best_loss=8.80867
Epoch 19/80: current_loss=9.01309 | best_loss=8.80867
Epoch 20/80: current_loss=9.01839 | best_loss=8.80867
Epoch 21/80: current_loss=8.97037 | best_loss=8.80867
Epoch 22/80: current_loss=8.81363 | best_loss=8.80867
Epoch 23/80: current_loss=8.92560 | best_loss=8.80867
Epoch 24/80: current_loss=10.13967 | best_loss=8.80867
Epoch 25/80: current_loss=8.98604 | best_loss=8.80867
Epoch 26/80: current_loss=9.20174 | best_loss=8.80867
Epoch 27/80: current_loss=8.91763 | best_loss=8.80867
Epoch 28/80: current_loss=8.98151 | best_loss=8.80867
Early Stopping at epoch 28
      explained_var=0.00347 | mse_loss=8.37168

----------------------------------------------
Params for Trial 57
{'learning_rate': 0.01, 'weight_decay': 0.007506797681089372, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.04812 | best_loss=10.04812
Epoch 1/80: current_loss=10.01663 | best_loss=10.01663
Epoch 2/80: current_loss=10.30130 | best_loss=10.01663
Epoch 3/80: current_loss=10.18987 | best_loss=10.01663
Epoch 4/80: current_loss=9.94334 | best_loss=9.94334
Epoch 5/80: current_loss=9.89963 | best_loss=9.89963
Epoch 6/80: current_loss=9.92357 | best_loss=9.89963
Epoch 7/80: current_loss=9.99296 | best_loss=9.89963
Epoch 8/80: current_loss=9.93922 | best_loss=9.89963
Epoch 9/80: current_loss=9.97961 | best_loss=9.89963
Epoch 10/80: current_loss=9.90207 | best_loss=9.89963
Epoch 11/80: current_loss=10.25891 | best_loss=9.89963
Epoch 12/80: current_loss=9.99493 | best_loss=9.89963
Epoch 13/80: current_loss=9.95965 | best_loss=9.89963
Epoch 14/80: current_loss=10.08205 | best_loss=9.89963
Epoch 15/80: current_loss=10.67585 | best_loss=9.89963
Epoch 16/80: current_loss=9.96849 | best_loss=9.89963
Epoch 17/80: current_loss=9.97284 | best_loss=9.89963
Epoch 18/80: current_loss=9.90844 | best_loss=9.89963
Epoch 19/80: current_loss=9.93815 | best_loss=9.89963
Epoch 20/80: current_loss=9.89722 | best_loss=9.89722
Epoch 21/80: current_loss=9.90530 | best_loss=9.89722
Epoch 22/80: current_loss=10.18136 | best_loss=9.89722
Epoch 23/80: current_loss=9.90385 | best_loss=9.89722
Epoch 24/80: current_loss=10.04199 | best_loss=9.89722
Epoch 25/80: current_loss=9.89180 | best_loss=9.89180
Epoch 26/80: current_loss=11.01212 | best_loss=9.89180
Epoch 27/80: current_loss=9.91007 | best_loss=9.89180
Epoch 28/80: current_loss=10.01174 | best_loss=9.89180
Epoch 29/80: current_loss=9.91094 | best_loss=9.89180
Epoch 30/80: current_loss=10.20615 | best_loss=9.89180
Epoch 31/80: current_loss=9.90727 | best_loss=9.89180
Epoch 32/80: current_loss=10.14266 | best_loss=9.89180
Epoch 33/80: current_loss=9.91679 | best_loss=9.89180
Epoch 34/80: current_loss=9.97441 | best_loss=9.89180
Epoch 35/80: current_loss=9.90582 | best_loss=9.89180
Epoch 36/80: current_loss=9.90246 | best_loss=9.89180
Epoch 37/80: current_loss=9.89182 | best_loss=9.89180
Epoch 38/80: current_loss=10.20490 | best_loss=9.89180
Epoch 39/80: current_loss=9.90396 | best_loss=9.89180
Epoch 40/80: current_loss=9.95778 | best_loss=9.89180
Epoch 41/80: current_loss=9.91125 | best_loss=9.89180
Epoch 42/80: current_loss=10.24051 | best_loss=9.89180
Epoch 43/80: current_loss=9.89929 | best_loss=9.89180
Epoch 44/80: current_loss=10.02065 | best_loss=9.89180
Epoch 45/80: current_loss=9.90690 | best_loss=9.89180
Early Stopping at epoch 45
      explained_var=0.00249 | mse_loss=9.55338
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.65744 | best_loss=8.65744
Epoch 1/80: current_loss=8.51698 | best_loss=8.51698
Epoch 2/80: current_loss=8.58329 | best_loss=8.51698
Epoch 3/80: current_loss=8.50359 | best_loss=8.50359
Epoch 4/80: current_loss=8.50937 | best_loss=8.50359
Epoch 5/80: current_loss=8.49808 | best_loss=8.49808
Epoch 6/80: current_loss=8.89701 | best_loss=8.49808
Epoch 7/80: current_loss=8.55751 | best_loss=8.49808
Epoch 8/80: current_loss=8.51060 | best_loss=8.49808
Epoch 9/80: current_loss=8.78542 | best_loss=8.49808
Epoch 10/80: current_loss=8.62870 | best_loss=8.49808
Epoch 11/80: current_loss=8.53138 | best_loss=8.49808
Epoch 12/80: current_loss=8.71680 | best_loss=8.49808
Epoch 13/80: current_loss=8.52478 | best_loss=8.49808
Epoch 14/80: current_loss=8.49203 | best_loss=8.49203
Epoch 15/80: current_loss=9.15548 | best_loss=8.49203
Epoch 16/80: current_loss=8.46400 | best_loss=8.46400
Epoch 17/80: current_loss=8.87908 | best_loss=8.46400
Epoch 18/80: current_loss=8.71522 | best_loss=8.46400
Epoch 19/80: current_loss=8.67753 | best_loss=8.46400
Epoch 20/80: current_loss=8.57709 | best_loss=8.46400
Epoch 21/80: current_loss=8.48737 | best_loss=8.46400
Epoch 22/80: current_loss=8.58811 | best_loss=8.46400
Epoch 23/80: current_loss=8.59702 | best_loss=8.46400
Epoch 24/80: current_loss=8.52284 | best_loss=8.46400
Epoch 25/80: current_loss=8.51056 | best_loss=8.46400
Epoch 26/80: current_loss=8.49177 | best_loss=8.46400
Epoch 27/80: current_loss=8.77065 | best_loss=8.46400
Epoch 28/80: current_loss=8.52703 | best_loss=8.46400
Epoch 29/80: current_loss=8.52235 | best_loss=8.46400
Epoch 30/80: current_loss=8.80717 | best_loss=8.46400
Epoch 31/80: current_loss=8.48342 | best_loss=8.46400
Epoch 32/80: current_loss=8.49185 | best_loss=8.46400
Epoch 33/80: current_loss=8.50387 | best_loss=8.46400
Epoch 34/80: current_loss=8.83186 | best_loss=8.46400
Epoch 35/80: current_loss=8.99148 | best_loss=8.46400
Epoch 36/80: current_loss=8.49323 | best_loss=8.46400
Early Stopping at epoch 36
      explained_var=0.00211 | mse_loss=8.55858
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.33651 | best_loss=9.33651
Epoch 1/80: current_loss=9.49929 | best_loss=9.33651
Epoch 2/80: current_loss=9.76635 | best_loss=9.33651
Epoch 3/80: current_loss=9.29908 | best_loss=9.29908
Epoch 4/80: current_loss=9.46966 | best_loss=9.29908
Epoch 5/80: current_loss=9.68364 | best_loss=9.29908
Epoch 6/80: current_loss=9.87611 | best_loss=9.29908
Epoch 7/80: current_loss=9.31613 | best_loss=9.29908
Epoch 8/80: current_loss=9.40721 | best_loss=9.29908
Epoch 9/80: current_loss=9.55212 | best_loss=9.29908
Epoch 10/80: current_loss=9.33922 | best_loss=9.29908
Epoch 11/80: current_loss=9.76337 | best_loss=9.29908
Epoch 12/80: current_loss=9.41732 | best_loss=9.29908
Epoch 13/80: current_loss=9.31461 | best_loss=9.29908
Epoch 14/80: current_loss=9.52844 | best_loss=9.29908
Epoch 15/80: current_loss=9.55908 | best_loss=9.29908
Epoch 16/80: current_loss=9.97015 | best_loss=9.29908
Epoch 17/80: current_loss=10.06878 | best_loss=9.29908
Epoch 18/80: current_loss=9.77776 | best_loss=9.29908
Epoch 19/80: current_loss=9.37892 | best_loss=9.29908
Epoch 20/80: current_loss=9.92405 | best_loss=9.29908
Epoch 21/80: current_loss=9.26919 | best_loss=9.26919
Epoch 22/80: current_loss=9.27286 | best_loss=9.26919
Epoch 23/80: current_loss=9.42268 | best_loss=9.26919
Epoch 24/80: current_loss=9.28283 | best_loss=9.26919
Epoch 25/80: current_loss=9.46319 | best_loss=9.26919
Epoch 26/80: current_loss=9.42686 | best_loss=9.26919
Epoch 27/80: current_loss=9.98701 | best_loss=9.26919
Epoch 28/80: current_loss=10.01948 | best_loss=9.26919
Epoch 29/80: current_loss=9.70563 | best_loss=9.26919
Epoch 30/80: current_loss=9.60123 | best_loss=9.26919
Epoch 31/80: current_loss=9.26730 | best_loss=9.26730
Epoch 32/80: current_loss=9.32024 | best_loss=9.26730
Epoch 33/80: current_loss=9.88588 | best_loss=9.26730
Epoch 34/80: current_loss=9.43709 | best_loss=9.26730
Epoch 35/80: current_loss=9.31380 | best_loss=9.26730
Epoch 36/80: current_loss=9.35079 | best_loss=9.26730
Epoch 37/80: current_loss=9.80942 | best_loss=9.26730
Epoch 38/80: current_loss=10.03075 | best_loss=9.26730
Epoch 39/80: current_loss=10.29370 | best_loss=9.26730
Epoch 40/80: current_loss=9.62527 | best_loss=9.26730
Epoch 41/80: current_loss=9.54001 | best_loss=9.26730
Epoch 42/80: current_loss=9.82340 | best_loss=9.26730
Epoch 43/80: current_loss=9.74982 | best_loss=9.26730
Epoch 44/80: current_loss=9.27924 | best_loss=9.26730
Epoch 45/80: current_loss=9.42676 | best_loss=9.26730
Epoch 46/80: current_loss=9.51231 | best_loss=9.26730
Epoch 47/80: current_loss=9.30999 | best_loss=9.26730
Epoch 48/80: current_loss=9.26915 | best_loss=9.26730
Epoch 49/80: current_loss=9.92915 | best_loss=9.26730
Epoch 50/80: current_loss=9.31535 | best_loss=9.26730
Epoch 51/80: current_loss=9.38356 | best_loss=9.26730
Early Stopping at epoch 51
      explained_var=-0.00154 | mse_loss=9.39838
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.01778 | best_loss=9.01778
Epoch 1/80: current_loss=8.93781 | best_loss=8.93781
Epoch 2/80: current_loss=9.00872 | best_loss=8.93781
Epoch 3/80: current_loss=8.96217 | best_loss=8.93781
Epoch 4/80: current_loss=9.20173 | best_loss=8.93781
Epoch 5/80: current_loss=8.93671 | best_loss=8.93671
Epoch 6/80: current_loss=9.03774 | best_loss=8.93671
Epoch 7/80: current_loss=8.95310 | best_loss=8.93671
Epoch 8/80: current_loss=9.47564 | best_loss=8.93671
Epoch 9/80: current_loss=8.99164 | best_loss=8.93671
Epoch 10/80: current_loss=9.05399 | best_loss=8.93671
Epoch 11/80: current_loss=9.12716 | best_loss=8.93671
Epoch 12/80: current_loss=9.07057 | best_loss=8.93671
Epoch 13/80: current_loss=9.05339 | best_loss=8.93671
Epoch 14/80: current_loss=8.93743 | best_loss=8.93671
Epoch 15/80: current_loss=9.29717 | best_loss=8.93671
Epoch 16/80: current_loss=8.98119 | best_loss=8.93671
Epoch 17/80: current_loss=8.93197 | best_loss=8.93197
Epoch 18/80: current_loss=8.97701 | best_loss=8.93197
Epoch 19/80: current_loss=9.14182 | best_loss=8.93197
Epoch 20/80: current_loss=9.05813 | best_loss=8.93197
Epoch 21/80: current_loss=8.94756 | best_loss=8.93197
Epoch 22/80: current_loss=9.09832 | best_loss=8.93197
Epoch 23/80: current_loss=8.96641 | best_loss=8.93197
Epoch 24/80: current_loss=9.02905 | best_loss=8.93197
Epoch 25/80: current_loss=8.91516 | best_loss=8.91516
Epoch 26/80: current_loss=9.04335 | best_loss=8.91516
Epoch 27/80: current_loss=9.07752 | best_loss=8.91516
Epoch 28/80: current_loss=8.94456 | best_loss=8.91516
Epoch 29/80: current_loss=8.99241 | best_loss=8.91516
Epoch 30/80: current_loss=9.03946 | best_loss=8.91516
Epoch 31/80: current_loss=8.96892 | best_loss=8.91516
Epoch 32/80: current_loss=8.94845 | best_loss=8.91516
Epoch 33/80: current_loss=9.37264 | best_loss=8.91516
Epoch 34/80: current_loss=9.08994 | best_loss=8.91516
Epoch 35/80: current_loss=9.26953 | best_loss=8.91516
Epoch 36/80: current_loss=9.01778 | best_loss=8.91516
Epoch 37/80: current_loss=9.06383 | best_loss=8.91516
Epoch 38/80: current_loss=9.08732 | best_loss=8.91516
Epoch 39/80: current_loss=8.95286 | best_loss=8.91516
Epoch 40/80: current_loss=8.97423 | best_loss=8.91516
Epoch 41/80: current_loss=9.23342 | best_loss=8.91516
Epoch 42/80: current_loss=8.95274 | best_loss=8.91516
Epoch 43/80: current_loss=9.23380 | best_loss=8.91516
Epoch 44/80: current_loss=8.97123 | best_loss=8.91516
Epoch 45/80: current_loss=9.12334 | best_loss=8.91516
Early Stopping at epoch 45
      explained_var=0.00041 | mse_loss=8.37771

----------------------------------------------
Params for Trial 58
{'learning_rate': 1e-05, 'weight_decay': 0.004339877231701992, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=75.30906 | best_loss=75.30906
Epoch 1/80: current_loss=72.60770 | best_loss=72.60770
Epoch 2/80: current_loss=68.74053 | best_loss=68.74053
Epoch 3/80: current_loss=63.04142 | best_loss=63.04142
Epoch 4/80: current_loss=55.59835 | best_loss=55.59835
Epoch 5/80: current_loss=48.19915 | best_loss=48.19915
Epoch 6/80: current_loss=42.14489 | best_loss=42.14489
Epoch 7/80: current_loss=37.32813 | best_loss=37.32813
Epoch 8/80: current_loss=33.53906 | best_loss=33.53906
Epoch 9/80: current_loss=30.47809 | best_loss=30.47809
Epoch 10/80: current_loss=27.98396 | best_loss=27.98396
Epoch 11/80: current_loss=25.92529 | best_loss=25.92529
Epoch 12/80: current_loss=24.24072 | best_loss=24.24072
Epoch 13/80: current_loss=22.80497 | best_loss=22.80497
Epoch 14/80: current_loss=21.59124 | best_loss=21.59124
Epoch 15/80: current_loss=20.56288 | best_loss=20.56288
Epoch 16/80: current_loss=19.68821 | best_loss=19.68821
Epoch 17/80: current_loss=18.89971 | best_loss=18.89971
Epoch 18/80: current_loss=18.22605 | best_loss=18.22605
Epoch 19/80: current_loss=17.64196 | best_loss=17.64196
Epoch 20/80: current_loss=17.10698 | best_loss=17.10698
Epoch 21/80: current_loss=16.64209 | best_loss=16.64209
Epoch 22/80: current_loss=16.20984 | best_loss=16.20984
Epoch 23/80: current_loss=15.82106 | best_loss=15.82106
Epoch 24/80: current_loss=15.45892 | best_loss=15.45892
Epoch 25/80: current_loss=15.13161 | best_loss=15.13161
Epoch 26/80: current_loss=14.82892 | best_loss=14.82892
Epoch 27/80: current_loss=14.55201 | best_loss=14.55201
Epoch 28/80: current_loss=14.29160 | best_loss=14.29160
Epoch 29/80: current_loss=14.04997 | best_loss=14.04997
Epoch 30/80: current_loss=13.82874 | best_loss=13.82874
Epoch 31/80: current_loss=13.62296 | best_loss=13.62296
Epoch 32/80: current_loss=13.43499 | best_loss=13.43499
Epoch 33/80: current_loss=13.24641 | best_loss=13.24641
Epoch 34/80: current_loss=13.07986 | best_loss=13.07986
Epoch 35/80: current_loss=12.92019 | best_loss=12.92019
Epoch 36/80: current_loss=12.76358 | best_loss=12.76358
Epoch 37/80: current_loss=12.62480 | best_loss=12.62480
Epoch 38/80: current_loss=12.49076 | best_loss=12.49076
Epoch 39/80: current_loss=12.36405 | best_loss=12.36405
Epoch 40/80: current_loss=12.24716 | best_loss=12.24716
Epoch 41/80: current_loss=12.14004 | best_loss=12.14004
Epoch 42/80: current_loss=12.03309 | best_loss=12.03309
Epoch 43/80: current_loss=11.93901 | best_loss=11.93901
Epoch 44/80: current_loss=11.84257 | best_loss=11.84257
Epoch 45/80: current_loss=11.75604 | best_loss=11.75604
Epoch 46/80: current_loss=11.67246 | best_loss=11.67246
Epoch 47/80: current_loss=11.59362 | best_loss=11.59362
Epoch 48/80: current_loss=11.51313 | best_loss=11.51313
Epoch 49/80: current_loss=11.44271 | best_loss=11.44271
Epoch 50/80: current_loss=11.37859 | best_loss=11.37859
Epoch 51/80: current_loss=11.31350 | best_loss=11.31350
Epoch 52/80: current_loss=11.25922 | best_loss=11.25922
Epoch 53/80: current_loss=11.20674 | best_loss=11.20674
Epoch 54/80: current_loss=11.15244 | best_loss=11.15244
Epoch 55/80: current_loss=11.10441 | best_loss=11.10441
Epoch 56/80: current_loss=11.05763 | best_loss=11.05763
Epoch 57/80: current_loss=11.01239 | best_loss=11.01239
Epoch 58/80: current_loss=10.97320 | best_loss=10.97320
Epoch 59/80: current_loss=10.93374 | best_loss=10.93374
Epoch 60/80: current_loss=10.89348 | best_loss=10.89348
Epoch 61/80: current_loss=10.86169 | best_loss=10.86169
Epoch 62/80: current_loss=10.82844 | best_loss=10.82844
Epoch 63/80: current_loss=10.79973 | best_loss=10.79973
Epoch 64/80: current_loss=10.76830 | best_loss=10.76830
Epoch 65/80: current_loss=10.73979 | best_loss=10.73979
Epoch 66/80: current_loss=10.71321 | best_loss=10.71321
Epoch 67/80: current_loss=10.69002 | best_loss=10.69002
Epoch 68/80: current_loss=10.66571 | best_loss=10.66571
Epoch 69/80: current_loss=10.64119 | best_loss=10.64119
Epoch 70/80: current_loss=10.62027 | best_loss=10.62027
Epoch 71/80: current_loss=10.60007 | best_loss=10.60007
Epoch 72/80: current_loss=10.57757 | best_loss=10.57757
Epoch 73/80: current_loss=10.55736 | best_loss=10.55736
Epoch 74/80: current_loss=10.53850 | best_loss=10.53850
Epoch 75/80: current_loss=10.51979 | best_loss=10.51979
Epoch 76/80: current_loss=10.50342 | best_loss=10.50342
Epoch 77/80: current_loss=10.48707 | best_loss=10.48707
Epoch 78/80: current_loss=10.46987 | best_loss=10.46987
Epoch 79/80: current_loss=10.45194 | best_loss=10.45194
      explained_var=-0.04253 | mse_loss=9.98874

----------------------------------------------
Params for Trial 59
{'learning_rate': 0.01, 'weight_decay': 0.00531873072694467, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.95400 | best_loss=9.95400
Epoch 1/80: current_loss=9.92371 | best_loss=9.92371
Epoch 2/80: current_loss=10.16324 | best_loss=9.92371
Epoch 3/80: current_loss=10.02370 | best_loss=9.92371
Epoch 4/80: current_loss=10.05117 | best_loss=9.92371
Epoch 5/80: current_loss=10.21660 | best_loss=9.92371
Epoch 6/80: current_loss=10.12592 | best_loss=9.92371
Epoch 7/80: current_loss=9.90657 | best_loss=9.90657
Epoch 8/80: current_loss=9.94241 | best_loss=9.90657
Epoch 9/80: current_loss=9.92922 | best_loss=9.90657
Epoch 10/80: current_loss=10.51120 | best_loss=9.90657
Epoch 11/80: current_loss=10.24378 | best_loss=9.90657
Epoch 12/80: current_loss=9.91593 | best_loss=9.90657
Epoch 13/80: current_loss=9.92121 | best_loss=9.90657
Epoch 14/80: current_loss=10.02507 | best_loss=9.90657
Epoch 15/80: current_loss=10.05297 | best_loss=9.90657
Epoch 16/80: current_loss=10.00523 | best_loss=9.90657
Epoch 17/80: current_loss=9.89298 | best_loss=9.89298
Epoch 18/80: current_loss=10.04949 | best_loss=9.89298
Epoch 19/80: current_loss=9.89497 | best_loss=9.89298
Epoch 20/80: current_loss=9.90850 | best_loss=9.89298
Epoch 21/80: current_loss=9.89248 | best_loss=9.89248
Epoch 22/80: current_loss=9.99855 | best_loss=9.89248
Epoch 23/80: current_loss=10.18970 | best_loss=9.89248
Epoch 24/80: current_loss=10.24030 | best_loss=9.89248
Epoch 25/80: current_loss=10.75692 | best_loss=9.89248
Epoch 26/80: current_loss=9.94194 | best_loss=9.89248
Epoch 27/80: current_loss=9.93872 | best_loss=9.89248
Epoch 28/80: current_loss=9.91817 | best_loss=9.89248
Epoch 29/80: current_loss=9.89391 | best_loss=9.89248
Epoch 30/80: current_loss=10.07033 | best_loss=9.89248
Epoch 31/80: current_loss=10.19751 | best_loss=9.89248
Epoch 32/80: current_loss=10.97398 | best_loss=9.89248
Epoch 33/80: current_loss=9.92303 | best_loss=9.89248
Epoch 34/80: current_loss=10.87464 | best_loss=9.89248
Epoch 35/80: current_loss=10.27618 | best_loss=9.89248
Epoch 36/80: current_loss=10.29691 | best_loss=9.89248
Epoch 37/80: current_loss=10.30215 | best_loss=9.89248
Epoch 38/80: current_loss=9.89049 | best_loss=9.89049
Epoch 39/80: current_loss=10.49687 | best_loss=9.89049
Epoch 40/80: current_loss=9.95609 | best_loss=9.89049
Epoch 41/80: current_loss=9.94007 | best_loss=9.89049
Epoch 42/80: current_loss=9.94898 | best_loss=9.89049
Epoch 43/80: current_loss=9.92095 | best_loss=9.89049
Epoch 44/80: current_loss=9.97281 | best_loss=9.89049
Epoch 45/80: current_loss=9.95267 | best_loss=9.89049
Epoch 46/80: current_loss=9.90049 | best_loss=9.89049
Epoch 47/80: current_loss=10.04876 | best_loss=9.89049
Epoch 48/80: current_loss=9.93404 | best_loss=9.89049
Epoch 49/80: current_loss=9.97044 | best_loss=9.89049
Epoch 50/80: current_loss=9.97222 | best_loss=9.89049
Epoch 51/80: current_loss=9.91703 | best_loss=9.89049
Epoch 52/80: current_loss=9.90045 | best_loss=9.89049
Epoch 53/80: current_loss=10.16362 | best_loss=9.89049
Epoch 54/80: current_loss=10.31843 | best_loss=9.89049
Epoch 55/80: current_loss=10.16487 | best_loss=9.89049
Epoch 56/80: current_loss=9.92119 | best_loss=9.89049
Epoch 57/80: current_loss=10.06713 | best_loss=9.89049
Epoch 58/80: current_loss=9.94850 | best_loss=9.89049
Early Stopping at epoch 58
      explained_var=0.00246 | mse_loss=9.55501
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.89421 | best_loss=8.89421
Epoch 1/80: current_loss=8.52872 | best_loss=8.52872
Epoch 2/80: current_loss=8.50263 | best_loss=8.50263
Epoch 3/80: current_loss=14.62850 | best_loss=8.50263
Epoch 4/80: current_loss=8.37818 | best_loss=8.37818
Epoch 5/80: current_loss=8.61206 | best_loss=8.37818
Epoch 6/80: current_loss=8.37765 | best_loss=8.37765
Epoch 7/80: current_loss=8.85005 | best_loss=8.37765
Epoch 8/80: current_loss=8.36167 | best_loss=8.36167
Epoch 9/80: current_loss=8.65278 | best_loss=8.36167
Epoch 10/80: current_loss=8.81026 | best_loss=8.36167
Epoch 11/80: current_loss=9.22544 | best_loss=8.36167
Epoch 12/80: current_loss=8.73447 | best_loss=8.36167
Epoch 13/80: current_loss=9.51902 | best_loss=8.36167
Epoch 14/80: current_loss=8.58626 | best_loss=8.36167
Epoch 15/80: current_loss=8.48277 | best_loss=8.36167
Epoch 16/80: current_loss=9.14551 | best_loss=8.36167
Epoch 17/80: current_loss=8.68015 | best_loss=8.36167
Epoch 18/80: current_loss=9.11200 | best_loss=8.36167
Epoch 19/80: current_loss=8.91890 | best_loss=8.36167
Epoch 20/80: current_loss=8.45263 | best_loss=8.36167
Epoch 21/80: current_loss=8.47483 | best_loss=8.36167
Epoch 22/80: current_loss=8.55936 | best_loss=8.36167
Epoch 23/80: current_loss=8.82991 | best_loss=8.36167
Epoch 24/80: current_loss=8.55975 | best_loss=8.36167
Epoch 25/80: current_loss=8.90366 | best_loss=8.36167
Epoch 26/80: current_loss=8.63893 | best_loss=8.36167
Epoch 27/80: current_loss=9.61694 | best_loss=8.36167
Epoch 28/80: current_loss=8.62522 | best_loss=8.36167
Early Stopping at epoch 28
      explained_var=0.00833 | mse_loss=8.51197
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.40963 | best_loss=9.40963
Epoch 1/80: current_loss=9.32947 | best_loss=9.32947
Epoch 2/80: current_loss=9.50874 | best_loss=9.32947
Epoch 3/80: current_loss=9.47895 | best_loss=9.32947
Epoch 4/80: current_loss=9.35814 | best_loss=9.32947
Epoch 5/80: current_loss=9.61870 | best_loss=9.32947
Epoch 6/80: current_loss=9.59530 | best_loss=9.32947
Epoch 7/80: current_loss=10.00454 | best_loss=9.32947
Epoch 8/80: current_loss=12.59347 | best_loss=9.32947
Epoch 9/80: current_loss=9.52057 | best_loss=9.32947
Epoch 10/80: current_loss=10.14679 | best_loss=9.32947
Epoch 11/80: current_loss=9.47984 | best_loss=9.32947
Epoch 12/80: current_loss=9.85228 | best_loss=9.32947
Epoch 13/80: current_loss=10.32233 | best_loss=9.32947
Epoch 14/80: current_loss=10.11269 | best_loss=9.32947
Epoch 15/80: current_loss=11.39681 | best_loss=9.32947
Epoch 16/80: current_loss=9.58988 | best_loss=9.32947
Epoch 17/80: current_loss=9.51599 | best_loss=9.32947
Epoch 18/80: current_loss=9.70360 | best_loss=9.32947
Epoch 19/80: current_loss=9.45540 | best_loss=9.32947
Epoch 20/80: current_loss=9.56162 | best_loss=9.32947
Epoch 21/80: current_loss=9.31269 | best_loss=9.31269
Epoch 22/80: current_loss=9.94759 | best_loss=9.31269
Epoch 23/80: current_loss=9.21149 | best_loss=9.21149
Epoch 24/80: current_loss=9.54491 | best_loss=9.21149
Epoch 25/80: current_loss=9.43456 | best_loss=9.21149
Epoch 26/80: current_loss=9.40771 | best_loss=9.21149
Epoch 27/80: current_loss=9.45782 | best_loss=9.21149
Epoch 28/80: current_loss=9.43405 | best_loss=9.21149
Epoch 29/80: current_loss=10.12115 | best_loss=9.21149
Epoch 30/80: current_loss=10.09901 | best_loss=9.21149
Epoch 31/80: current_loss=9.34941 | best_loss=9.21149
Epoch 32/80: current_loss=9.35445 | best_loss=9.21149
Epoch 33/80: current_loss=10.10313 | best_loss=9.21149
Epoch 34/80: current_loss=9.46658 | best_loss=9.21149
Epoch 35/80: current_loss=9.41346 | best_loss=9.21149
Epoch 36/80: current_loss=9.90887 | best_loss=9.21149
Epoch 37/80: current_loss=10.08399 | best_loss=9.21149
Epoch 38/80: current_loss=9.54645 | best_loss=9.21149
Epoch 39/80: current_loss=9.93969 | best_loss=9.21149
Epoch 40/80: current_loss=9.83538 | best_loss=9.21149
Epoch 41/80: current_loss=9.44408 | best_loss=9.21149
Epoch 42/80: current_loss=9.74488 | best_loss=9.21149
Epoch 43/80: current_loss=9.28881 | best_loss=9.21149
Early Stopping at epoch 43
      explained_var=0.01055 | mse_loss=9.27940
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.37341 | best_loss=9.37341
Epoch 1/80: current_loss=9.00872 | best_loss=9.00872
Epoch 2/80: current_loss=9.33146 | best_loss=9.00872
Epoch 3/80: current_loss=8.94479 | best_loss=8.94479
Epoch 4/80: current_loss=9.10684 | best_loss=8.94479
Epoch 5/80: current_loss=9.02930 | best_loss=8.94479
Epoch 6/80: current_loss=9.06122 | best_loss=8.94479
Epoch 7/80: current_loss=9.11043 | best_loss=8.94479
Epoch 8/80: current_loss=9.12599 | best_loss=8.94479
Epoch 9/80: current_loss=9.26365 | best_loss=8.94479
Epoch 10/80: current_loss=8.99964 | best_loss=8.94479
Epoch 11/80: current_loss=9.24803 | best_loss=8.94479
Epoch 12/80: current_loss=9.01511 | best_loss=8.94479
Epoch 13/80: current_loss=9.19940 | best_loss=8.94479
Epoch 14/80: current_loss=9.61991 | best_loss=8.94479
Epoch 15/80: current_loss=9.08053 | best_loss=8.94479
Epoch 16/80: current_loss=9.12622 | best_loss=8.94479
Epoch 17/80: current_loss=8.95266 | best_loss=8.94479
Epoch 18/80: current_loss=8.96976 | best_loss=8.94479
Epoch 19/80: current_loss=9.86739 | best_loss=8.94479
Epoch 20/80: current_loss=9.15274 | best_loss=8.94479
Epoch 21/80: current_loss=8.98170 | best_loss=8.94479
Epoch 22/80: current_loss=10.63600 | best_loss=8.94479
Epoch 23/80: current_loss=9.45523 | best_loss=8.94479
Early Stopping at epoch 23
      explained_var=0.00545 | mse_loss=8.37290

----------------------------------------------
Params for Trial 60
{'learning_rate': 0.01, 'weight_decay': 0.001674511843150389, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.04700 | best_loss=10.04700
Epoch 1/80: current_loss=10.33205 | best_loss=10.04700
Epoch 2/80: current_loss=10.20373 | best_loss=10.04700
Epoch 3/80: current_loss=9.91030 | best_loss=9.91030
Epoch 4/80: current_loss=10.03517 | best_loss=9.91030
Epoch 5/80: current_loss=9.92057 | best_loss=9.91030
Epoch 6/80: current_loss=10.37217 | best_loss=9.91030
Epoch 7/80: current_loss=9.90742 | best_loss=9.90742
Epoch 8/80: current_loss=10.24803 | best_loss=9.90742
Epoch 9/80: current_loss=10.83655 | best_loss=9.90742
Epoch 10/80: current_loss=9.89852 | best_loss=9.89852
Epoch 11/80: current_loss=9.91994 | best_loss=9.89852
Epoch 12/80: current_loss=10.04374 | best_loss=9.89852
Epoch 13/80: current_loss=10.51792 | best_loss=9.89852
Epoch 14/80: current_loss=10.30945 | best_loss=9.89852
Epoch 15/80: current_loss=10.05386 | best_loss=9.89852
Epoch 16/80: current_loss=9.90143 | best_loss=9.89852
Epoch 17/80: current_loss=9.90341 | best_loss=9.89852
Epoch 18/80: current_loss=9.96707 | best_loss=9.89852
Epoch 19/80: current_loss=9.94471 | best_loss=9.89852
Epoch 20/80: current_loss=10.07268 | best_loss=9.89852
Epoch 21/80: current_loss=10.02514 | best_loss=9.89852
Epoch 22/80: current_loss=10.05696 | best_loss=9.89852
Epoch 23/80: current_loss=9.97898 | best_loss=9.89852
Epoch 24/80: current_loss=9.90097 | best_loss=9.89852
Epoch 25/80: current_loss=9.94341 | best_loss=9.89852
Epoch 26/80: current_loss=9.91592 | best_loss=9.89852
Epoch 27/80: current_loss=9.90731 | best_loss=9.89852
Epoch 28/80: current_loss=9.95955 | best_loss=9.89852
Epoch 29/80: current_loss=10.03409 | best_loss=9.89852
Epoch 30/80: current_loss=10.53131 | best_loss=9.89852
Early Stopping at epoch 30
      explained_var=0.00149 | mse_loss=9.57405

----------------------------------------------
Params for Trial 61
{'learning_rate': 0.01, 'weight_decay': 0.0020892115027639265, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.90633 | best_loss=9.90633
Epoch 1/80: current_loss=10.11942 | best_loss=9.90633
Epoch 2/80: current_loss=9.91184 | best_loss=9.90633
Epoch 3/80: current_loss=10.25234 | best_loss=9.90633
Epoch 4/80: current_loss=10.19798 | best_loss=9.90633
Epoch 5/80: current_loss=9.91937 | best_loss=9.90633
Epoch 6/80: current_loss=9.90009 | best_loss=9.90009
Epoch 7/80: current_loss=9.93858 | best_loss=9.90009
Epoch 8/80: current_loss=9.98430 | best_loss=9.90009
Epoch 9/80: current_loss=10.24253 | best_loss=9.90009
Epoch 10/80: current_loss=10.86455 | best_loss=9.90009
Epoch 11/80: current_loss=10.35853 | best_loss=9.90009
Epoch 12/80: current_loss=9.92345 | best_loss=9.90009
Epoch 13/80: current_loss=9.91728 | best_loss=9.90009
Epoch 14/80: current_loss=10.16633 | best_loss=9.90009
Epoch 15/80: current_loss=10.02965 | best_loss=9.90009
Epoch 16/80: current_loss=10.27413 | best_loss=9.90009
Epoch 17/80: current_loss=9.94211 | best_loss=9.90009
Epoch 18/80: current_loss=9.90724 | best_loss=9.90009
Epoch 19/80: current_loss=10.02327 | best_loss=9.90009
Epoch 20/80: current_loss=9.92834 | best_loss=9.90009
Epoch 21/80: current_loss=10.19752 | best_loss=9.90009
Epoch 22/80: current_loss=9.99487 | best_loss=9.90009
Epoch 23/80: current_loss=9.97111 | best_loss=9.90009
Epoch 24/80: current_loss=10.08178 | best_loss=9.90009
Epoch 25/80: current_loss=10.01368 | best_loss=9.90009
Epoch 26/80: current_loss=9.95828 | best_loss=9.90009
Early Stopping at epoch 26
      explained_var=0.00121 | mse_loss=9.57524

----------------------------------------------
Params for Trial 62
{'learning_rate': 0.01, 'weight_decay': 0.00211301338609456, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91536 | best_loss=9.91536
Epoch 1/80: current_loss=10.09581 | best_loss=9.91536
Epoch 2/80: current_loss=9.92084 | best_loss=9.91536
Epoch 3/80: current_loss=9.97606 | best_loss=9.91536
Epoch 4/80: current_loss=9.91670 | best_loss=9.91536
Epoch 5/80: current_loss=10.01946 | best_loss=9.91536
Epoch 6/80: current_loss=10.48240 | best_loss=9.91536
Epoch 7/80: current_loss=10.04360 | best_loss=9.91536
Epoch 8/80: current_loss=10.51737 | best_loss=9.91536
Epoch 9/80: current_loss=10.00932 | best_loss=9.91536
Epoch 10/80: current_loss=9.97830 | best_loss=9.91536
Epoch 11/80: current_loss=10.10249 | best_loss=9.91536
Epoch 12/80: current_loss=10.43597 | best_loss=9.91536
Epoch 13/80: current_loss=10.06335 | best_loss=9.91536
Epoch 14/80: current_loss=9.90811 | best_loss=9.90811
Epoch 15/80: current_loss=9.94853 | best_loss=9.90811
Epoch 16/80: current_loss=9.91681 | best_loss=9.90811
Epoch 17/80: current_loss=10.21646 | best_loss=9.90811
Epoch 18/80: current_loss=10.02593 | best_loss=9.90811
Epoch 19/80: current_loss=11.13577 | best_loss=9.90811
Epoch 20/80: current_loss=10.01569 | best_loss=9.90811
Epoch 21/80: current_loss=10.66850 | best_loss=9.90811
Epoch 22/80: current_loss=10.19825 | best_loss=9.90811
Epoch 23/80: current_loss=9.90744 | best_loss=9.90744
Epoch 24/80: current_loss=10.04180 | best_loss=9.90744
Epoch 25/80: current_loss=10.21831 | best_loss=9.90744
Epoch 26/80: current_loss=9.91061 | best_loss=9.90744
Epoch 27/80: current_loss=9.91067 | best_loss=9.90744
Epoch 28/80: current_loss=9.95544 | best_loss=9.90744
Epoch 29/80: current_loss=9.91172 | best_loss=9.90744
Epoch 30/80: current_loss=10.05539 | best_loss=9.90744
Epoch 31/80: current_loss=10.03317 | best_loss=9.90744
Epoch 32/80: current_loss=9.95989 | best_loss=9.90744
Epoch 33/80: current_loss=10.16634 | best_loss=9.90744
Epoch 34/80: current_loss=9.89575 | best_loss=9.89575
Epoch 35/80: current_loss=11.18697 | best_loss=9.89575
Epoch 36/80: current_loss=9.91887 | best_loss=9.89575
Epoch 37/80: current_loss=9.98663 | best_loss=9.89575
Epoch 38/80: current_loss=11.52002 | best_loss=9.89575
Epoch 39/80: current_loss=9.97772 | best_loss=9.89575
Epoch 40/80: current_loss=10.06643 | best_loss=9.89575
Epoch 41/80: current_loss=9.96773 | best_loss=9.89575
Epoch 42/80: current_loss=10.15004 | best_loss=9.89575
Epoch 43/80: current_loss=9.89388 | best_loss=9.89388
Epoch 44/80: current_loss=10.83236 | best_loss=9.89388
Epoch 45/80: current_loss=9.92873 | best_loss=9.89388
Epoch 46/80: current_loss=9.89625 | best_loss=9.89388
Epoch 47/80: current_loss=10.04444 | best_loss=9.89388
Epoch 48/80: current_loss=10.53737 | best_loss=9.89388
Epoch 49/80: current_loss=9.94361 | best_loss=9.89388
Epoch 50/80: current_loss=10.03665 | best_loss=9.89388
Epoch 51/80: current_loss=9.90894 | best_loss=9.89388
Epoch 52/80: current_loss=10.09398 | best_loss=9.89388
Epoch 53/80: current_loss=10.19713 | best_loss=9.89388
Epoch 54/80: current_loss=10.02383 | best_loss=9.89388
Epoch 55/80: current_loss=9.88763 | best_loss=9.88763
Epoch 56/80: current_loss=9.92194 | best_loss=9.88763
Epoch 57/80: current_loss=10.05437 | best_loss=9.88763
Epoch 58/80: current_loss=9.92317 | best_loss=9.88763
Epoch 59/80: current_loss=10.02351 | best_loss=9.88763
Epoch 60/80: current_loss=10.21730 | best_loss=9.88763
Epoch 61/80: current_loss=10.61313 | best_loss=9.88763
Epoch 62/80: current_loss=10.04589 | best_loss=9.88763
Epoch 63/80: current_loss=9.91297 | best_loss=9.88763
Epoch 64/80: current_loss=10.08800 | best_loss=9.88763
Epoch 65/80: current_loss=9.91694 | best_loss=9.88763
Epoch 66/80: current_loss=10.44797 | best_loss=9.88763
Epoch 67/80: current_loss=10.44683 | best_loss=9.88763
Epoch 68/80: current_loss=10.18970 | best_loss=9.88763
Epoch 69/80: current_loss=9.97421 | best_loss=9.88763
Epoch 70/80: current_loss=9.98857 | best_loss=9.88763
Epoch 71/80: current_loss=10.13027 | best_loss=9.88763
Epoch 72/80: current_loss=10.48066 | best_loss=9.88763
Epoch 73/80: current_loss=10.03748 | best_loss=9.88763
Epoch 74/80: current_loss=10.22784 | best_loss=9.88763
Epoch 75/80: current_loss=15.74571 | best_loss=9.88763
Early Stopping at epoch 75
      explained_var=0.00246 | mse_loss=9.55473
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.58982 | best_loss=8.58982
Epoch 1/80: current_loss=8.74583 | best_loss=8.58982
Epoch 2/80: current_loss=8.67645 | best_loss=8.58982
Epoch 3/80: current_loss=8.48045 | best_loss=8.48045
Epoch 4/80: current_loss=8.81568 | best_loss=8.48045
Epoch 5/80: current_loss=8.58277 | best_loss=8.48045
Epoch 6/80: current_loss=9.54216 | best_loss=8.48045
Epoch 7/80: current_loss=8.89933 | best_loss=8.48045
Epoch 8/80: current_loss=8.58863 | best_loss=8.48045
Epoch 9/80: current_loss=8.51448 | best_loss=8.48045
Epoch 10/80: current_loss=8.61158 | best_loss=8.48045
Epoch 11/80: current_loss=8.70994 | best_loss=8.48045
Epoch 12/80: current_loss=8.49179 | best_loss=8.48045
Epoch 13/80: current_loss=8.50197 | best_loss=8.48045
Epoch 14/80: current_loss=8.97952 | best_loss=8.48045
Epoch 15/80: current_loss=8.65398 | best_loss=8.48045
Epoch 16/80: current_loss=8.57302 | best_loss=8.48045
Epoch 17/80: current_loss=10.35004 | best_loss=8.48045
Epoch 18/80: current_loss=9.05178 | best_loss=8.48045
Epoch 19/80: current_loss=8.48046 | best_loss=8.48045
Epoch 20/80: current_loss=8.63543 | best_loss=8.48045
Epoch 21/80: current_loss=8.49479 | best_loss=8.48045
Epoch 22/80: current_loss=9.36933 | best_loss=8.48045
Epoch 23/80: current_loss=8.73240 | best_loss=8.48045
Early Stopping at epoch 23
      explained_var=0.00864 | mse_loss=8.64801

----------------------------------------------
Params for Trial 63
{'learning_rate': 0.01, 'weight_decay': 1.3456673211770416e-05, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.14790 | best_loss=10.14790
Epoch 1/80: current_loss=9.93640 | best_loss=9.93640
Epoch 2/80: current_loss=10.04248 | best_loss=9.93640
Epoch 3/80: current_loss=10.13026 | best_loss=9.93640
Epoch 4/80: current_loss=9.95431 | best_loss=9.93640
Epoch 5/80: current_loss=10.23694 | best_loss=9.93640
Epoch 6/80: current_loss=10.20850 | best_loss=9.93640
Epoch 7/80: current_loss=10.39400 | best_loss=9.93640
Epoch 8/80: current_loss=10.02472 | best_loss=9.93640
Epoch 9/80: current_loss=10.43446 | best_loss=9.93640
Epoch 10/80: current_loss=9.91839 | best_loss=9.91839
Epoch 11/80: current_loss=9.91157 | best_loss=9.91157
Epoch 12/80: current_loss=9.90887 | best_loss=9.90887
Epoch 13/80: current_loss=10.18397 | best_loss=9.90887
Epoch 14/80: current_loss=9.99726 | best_loss=9.90887
Epoch 15/80: current_loss=9.90594 | best_loss=9.90594
Epoch 16/80: current_loss=10.88712 | best_loss=9.90594
Epoch 17/80: current_loss=10.09607 | best_loss=9.90594
Epoch 18/80: current_loss=11.13094 | best_loss=9.90594
Epoch 19/80: current_loss=10.18121 | best_loss=9.90594
Epoch 20/80: current_loss=9.92895 | best_loss=9.90594
Epoch 21/80: current_loss=9.91840 | best_loss=9.90594
Epoch 22/80: current_loss=10.21896 | best_loss=9.90594
Epoch 23/80: current_loss=9.95574 | best_loss=9.90594
Epoch 24/80: current_loss=10.16280 | best_loss=9.90594
Epoch 25/80: current_loss=9.92888 | best_loss=9.90594
Epoch 26/80: current_loss=9.98990 | best_loss=9.90594
Epoch 27/80: current_loss=13.34595 | best_loss=9.90594
Epoch 28/80: current_loss=9.94608 | best_loss=9.90594
Epoch 29/80: current_loss=9.91558 | best_loss=9.90594
Epoch 30/80: current_loss=9.91307 | best_loss=9.90594
Epoch 31/80: current_loss=9.97882 | best_loss=9.90594
Epoch 32/80: current_loss=10.04729 | best_loss=9.90594
Epoch 33/80: current_loss=10.20875 | best_loss=9.90594
Epoch 34/80: current_loss=10.15050 | best_loss=9.90594
Epoch 35/80: current_loss=10.07554 | best_loss=9.90594
Early Stopping at epoch 35
      explained_var=0.00074 | mse_loss=9.58263

----------------------------------------------
Params for Trial 64
{'learning_rate': 0.01, 'weight_decay': 0.0010846633924567982, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.92106 | best_loss=9.92106
Epoch 1/80: current_loss=9.91246 | best_loss=9.91246
Epoch 2/80: current_loss=9.99028 | best_loss=9.91246
Epoch 3/80: current_loss=9.98874 | best_loss=9.91246
Epoch 4/80: current_loss=9.91274 | best_loss=9.91246
Epoch 5/80: current_loss=9.89666 | best_loss=9.89666
Epoch 6/80: current_loss=10.06438 | best_loss=9.89666
Epoch 7/80: current_loss=9.89639 | best_loss=9.89639
Epoch 8/80: current_loss=9.91812 | best_loss=9.89639
Epoch 9/80: current_loss=10.14797 | best_loss=9.89639
Epoch 10/80: current_loss=10.05558 | best_loss=9.89639
Epoch 11/80: current_loss=10.42588 | best_loss=9.89639
Epoch 12/80: current_loss=9.93645 | best_loss=9.89639
Epoch 13/80: current_loss=9.91688 | best_loss=9.89639
Epoch 14/80: current_loss=9.90345 | best_loss=9.89639
Epoch 15/80: current_loss=10.33565 | best_loss=9.89639
Epoch 16/80: current_loss=11.17806 | best_loss=9.89639
Epoch 17/80: current_loss=10.25765 | best_loss=9.89639
Epoch 18/80: current_loss=9.95485 | best_loss=9.89639
Epoch 19/80: current_loss=9.95473 | best_loss=9.89639
Epoch 20/80: current_loss=9.90053 | best_loss=9.89639
Epoch 21/80: current_loss=10.08464 | best_loss=9.89639
Epoch 22/80: current_loss=9.91605 | best_loss=9.89639
Epoch 23/80: current_loss=9.93567 | best_loss=9.89639
Epoch 24/80: current_loss=10.44367 | best_loss=9.89639
Epoch 25/80: current_loss=9.90663 | best_loss=9.89639
Epoch 26/80: current_loss=9.97420 | best_loss=9.89639
Epoch 27/80: current_loss=9.91012 | best_loss=9.89639
Early Stopping at epoch 27
      explained_var=0.00148 | mse_loss=9.57003
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.49989 | best_loss=8.49989
Epoch 1/80: current_loss=8.45730 | best_loss=8.45730
Epoch 2/80: current_loss=8.67906 | best_loss=8.45730
Epoch 3/80: current_loss=9.31871 | best_loss=8.45730
Epoch 4/80: current_loss=8.52726 | best_loss=8.45730
Epoch 5/80: current_loss=8.86877 | best_loss=8.45730
Epoch 6/80: current_loss=9.25274 | best_loss=8.45730
Epoch 7/80: current_loss=8.79501 | best_loss=8.45730
Epoch 8/80: current_loss=8.48161 | best_loss=8.45730
Epoch 9/80: current_loss=8.50410 | best_loss=8.45730
Epoch 10/80: current_loss=8.51038 | best_loss=8.45730
Epoch 11/80: current_loss=8.62608 | best_loss=8.45730
Epoch 12/80: current_loss=8.48324 | best_loss=8.45730
Epoch 13/80: current_loss=8.50781 | best_loss=8.45730
Epoch 14/80: current_loss=8.77863 | best_loss=8.45730
Epoch 15/80: current_loss=8.53279 | best_loss=8.45730
Epoch 16/80: current_loss=8.57330 | best_loss=8.45730
Epoch 17/80: current_loss=8.51607 | best_loss=8.45730
Epoch 18/80: current_loss=8.69226 | best_loss=8.45730
Epoch 19/80: current_loss=8.54912 | best_loss=8.45730
Epoch 20/80: current_loss=8.57254 | best_loss=8.45730
Epoch 21/80: current_loss=8.48084 | best_loss=8.45730
Early Stopping at epoch 21
      explained_var=0.00173 | mse_loss=8.56417
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.42780 | best_loss=9.42780
Epoch 1/80: current_loss=9.86148 | best_loss=9.42780
Epoch 2/80: current_loss=9.45137 | best_loss=9.42780
Epoch 3/80: current_loss=10.47133 | best_loss=9.42780
Epoch 4/80: current_loss=9.54358 | best_loss=9.42780
Epoch 5/80: current_loss=9.34772 | best_loss=9.34772
Epoch 6/80: current_loss=9.55494 | best_loss=9.34772
Epoch 7/80: current_loss=9.75794 | best_loss=9.34772
Epoch 8/80: current_loss=9.42186 | best_loss=9.34772
Epoch 9/80: current_loss=10.38616 | best_loss=9.34772
Epoch 10/80: current_loss=9.25318 | best_loss=9.25318
Epoch 11/80: current_loss=11.29536 | best_loss=9.25318
Epoch 12/80: current_loss=9.24933 | best_loss=9.24933
Epoch 13/80: current_loss=9.44194 | best_loss=9.24933
Epoch 14/80: current_loss=9.79592 | best_loss=9.24933
Epoch 15/80: current_loss=9.72200 | best_loss=9.24933
Epoch 16/80: current_loss=9.33639 | best_loss=9.24933
Epoch 17/80: current_loss=9.57374 | best_loss=9.24933
Epoch 18/80: current_loss=10.45670 | best_loss=9.24933
Epoch 19/80: current_loss=9.26969 | best_loss=9.24933
Epoch 20/80: current_loss=9.56033 | best_loss=9.24933
Epoch 21/80: current_loss=9.27010 | best_loss=9.24933
Epoch 22/80: current_loss=10.12458 | best_loss=9.24933
Epoch 23/80: current_loss=9.52026 | best_loss=9.24933
Epoch 24/80: current_loss=10.63935 | best_loss=9.24933
Epoch 25/80: current_loss=9.25414 | best_loss=9.24933
Epoch 26/80: current_loss=9.38958 | best_loss=9.24933
Epoch 27/80: current_loss=9.47137 | best_loss=9.24933
Epoch 28/80: current_loss=9.34043 | best_loss=9.24933
Epoch 29/80: current_loss=9.50836 | best_loss=9.24933
Epoch 30/80: current_loss=9.46305 | best_loss=9.24933
Epoch 31/80: current_loss=10.26845 | best_loss=9.24933
Epoch 32/80: current_loss=9.83067 | best_loss=9.24933
Early Stopping at epoch 32
      explained_var=0.00127 | mse_loss=9.36994
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.97975 | best_loss=8.97975
Epoch 1/80: current_loss=9.27842 | best_loss=8.97975
Epoch 2/80: current_loss=9.15231 | best_loss=8.97975
Epoch 3/80: current_loss=9.15938 | best_loss=8.97975
Epoch 4/80: current_loss=9.58511 | best_loss=8.97975
Epoch 5/80: current_loss=9.02695 | best_loss=8.97975
Epoch 6/80: current_loss=10.25333 | best_loss=8.97975
Epoch 7/80: current_loss=8.99205 | best_loss=8.97975
Epoch 8/80: current_loss=9.11098 | best_loss=8.97975
Epoch 9/80: current_loss=8.96181 | best_loss=8.96181
Epoch 10/80: current_loss=8.93584 | best_loss=8.93584
Epoch 11/80: current_loss=8.93731 | best_loss=8.93584
Epoch 12/80: current_loss=9.13235 | best_loss=8.93584
Epoch 13/80: current_loss=9.03195 | best_loss=8.93584
Epoch 14/80: current_loss=8.99133 | best_loss=8.93584
Epoch 15/80: current_loss=8.96236 | best_loss=8.93584
Epoch 16/80: current_loss=9.29697 | best_loss=8.93584
Epoch 17/80: current_loss=9.20999 | best_loss=8.93584
Epoch 18/80: current_loss=8.97320 | best_loss=8.93584
Epoch 19/80: current_loss=9.25400 | best_loss=8.93584
Epoch 20/80: current_loss=8.93484 | best_loss=8.93484
Epoch 21/80: current_loss=9.29440 | best_loss=8.93484
Epoch 22/80: current_loss=9.25117 | best_loss=8.93484
Epoch 23/80: current_loss=9.48744 | best_loss=8.93484
Epoch 24/80: current_loss=8.94329 | best_loss=8.93484
Epoch 25/80: current_loss=9.28994 | best_loss=8.93484
Epoch 26/80: current_loss=9.60265 | best_loss=8.93484
Epoch 27/80: current_loss=9.63102 | best_loss=8.93484
Epoch 28/80: current_loss=9.04097 | best_loss=8.93484
Epoch 29/80: current_loss=9.50998 | best_loss=8.93484
Epoch 30/80: current_loss=9.03026 | best_loss=8.93484
Epoch 31/80: current_loss=9.06954 | best_loss=8.93484
Epoch 32/80: current_loss=9.35376 | best_loss=8.93484
Epoch 33/80: current_loss=9.00691 | best_loss=8.93484
Epoch 34/80: current_loss=8.96076 | best_loss=8.93484
Epoch 35/80: current_loss=9.71308 | best_loss=8.93484
Epoch 36/80: current_loss=9.02011 | best_loss=8.93484
Epoch 37/80: current_loss=8.96481 | best_loss=8.93484
Epoch 38/80: current_loss=8.93385 | best_loss=8.93385
Epoch 39/80: current_loss=9.84320 | best_loss=8.93385
Epoch 40/80: current_loss=8.97570 | best_loss=8.93385
Epoch 41/80: current_loss=9.62537 | best_loss=8.93385
Epoch 42/80: current_loss=8.96010 | best_loss=8.93385
Epoch 43/80: current_loss=10.74568 | best_loss=8.93385
Epoch 44/80: current_loss=9.46223 | best_loss=8.93385
Epoch 45/80: current_loss=8.98102 | best_loss=8.93385
Epoch 46/80: current_loss=9.41185 | best_loss=8.93385
Epoch 47/80: current_loss=8.90649 | best_loss=8.90649
Epoch 48/80: current_loss=9.15590 | best_loss=8.90649
Epoch 49/80: current_loss=8.97631 | best_loss=8.90649
Epoch 50/80: current_loss=9.19583 | best_loss=8.90649
Epoch 51/80: current_loss=9.01527 | best_loss=8.90649
Epoch 52/80: current_loss=8.96658 | best_loss=8.90649
Epoch 53/80: current_loss=9.18603 | best_loss=8.90649
Epoch 54/80: current_loss=8.95486 | best_loss=8.90649
Epoch 55/80: current_loss=8.95477 | best_loss=8.90649
Epoch 56/80: current_loss=9.02998 | best_loss=8.90649
Epoch 57/80: current_loss=9.64488 | best_loss=8.90649
Epoch 58/80: current_loss=8.95094 | best_loss=8.90649
Epoch 59/80: current_loss=9.36917 | best_loss=8.90649
Epoch 60/80: current_loss=8.97278 | best_loss=8.90649
Epoch 61/80: current_loss=10.16120 | best_loss=8.90649
Epoch 62/80: current_loss=10.27501 | best_loss=8.90649
Epoch 63/80: current_loss=8.95074 | best_loss=8.90649
Epoch 64/80: current_loss=8.93919 | best_loss=8.90649
Epoch 65/80: current_loss=8.97211 | best_loss=8.90649
Epoch 66/80: current_loss=9.02574 | best_loss=8.90649
Epoch 67/80: current_loss=9.01000 | best_loss=8.90649
Early Stopping at epoch 67
      explained_var=0.00004 | mse_loss=8.36566

----------------------------------------------
Params for Trial 65
{'learning_rate': 0.01, 'weight_decay': 0.003343153234990452, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.95053 | best_loss=9.95053
Epoch 1/80: current_loss=9.91779 | best_loss=9.91779
Epoch 2/80: current_loss=9.97097 | best_loss=9.91779
Epoch 3/80: current_loss=10.03821 | best_loss=9.91779
Epoch 4/80: current_loss=10.00474 | best_loss=9.91779
Epoch 5/80: current_loss=10.44754 | best_loss=9.91779
Epoch 6/80: current_loss=9.91246 | best_loss=9.91246
Epoch 7/80: current_loss=10.83796 | best_loss=9.91246
Epoch 8/80: current_loss=10.07257 | best_loss=9.91246
Epoch 9/80: current_loss=10.59977 | best_loss=9.91246
Epoch 10/80: current_loss=10.13877 | best_loss=9.91246
Epoch 11/80: current_loss=9.89729 | best_loss=9.89729
Epoch 12/80: current_loss=9.91555 | best_loss=9.89729
Epoch 13/80: current_loss=10.07379 | best_loss=9.89729
Epoch 14/80: current_loss=9.94284 | best_loss=9.89729
Epoch 15/80: current_loss=9.99889 | best_loss=9.89729
Epoch 16/80: current_loss=9.93434 | best_loss=9.89729
Epoch 17/80: current_loss=10.42641 | best_loss=9.89729
Epoch 18/80: current_loss=9.91625 | best_loss=9.89729
Epoch 19/80: current_loss=9.89154 | best_loss=9.89154
Epoch 20/80: current_loss=10.19616 | best_loss=9.89154
Epoch 21/80: current_loss=9.90565 | best_loss=9.89154
Epoch 22/80: current_loss=10.38648 | best_loss=9.89154
Epoch 23/80: current_loss=10.23495 | best_loss=9.89154
Epoch 24/80: current_loss=10.02034 | best_loss=9.89154
Epoch 25/80: current_loss=9.95175 | best_loss=9.89154
Epoch 26/80: current_loss=9.97235 | best_loss=9.89154
Epoch 27/80: current_loss=9.91289 | best_loss=9.89154
Epoch 28/80: current_loss=10.86739 | best_loss=9.89154
Epoch 29/80: current_loss=9.88894 | best_loss=9.88894
Epoch 30/80: current_loss=10.97165 | best_loss=9.88894
Epoch 31/80: current_loss=10.05602 | best_loss=9.88894
Epoch 32/80: current_loss=9.89609 | best_loss=9.88894
Epoch 33/80: current_loss=10.02116 | best_loss=9.88894
Epoch 34/80: current_loss=10.85673 | best_loss=9.88894
Epoch 35/80: current_loss=11.19867 | best_loss=9.88894
Epoch 36/80: current_loss=9.90941 | best_loss=9.88894
Epoch 37/80: current_loss=10.17138 | best_loss=9.88894
Epoch 38/80: current_loss=9.90895 | best_loss=9.88894
Epoch 39/80: current_loss=9.90306 | best_loss=9.88894
Epoch 40/80: current_loss=9.90321 | best_loss=9.88894
Epoch 41/80: current_loss=9.94309 | best_loss=9.88894
Epoch 42/80: current_loss=10.49561 | best_loss=9.88894
Epoch 43/80: current_loss=10.29559 | best_loss=9.88894
Epoch 44/80: current_loss=9.93632 | best_loss=9.88894
Epoch 45/80: current_loss=10.12656 | best_loss=9.88894
Epoch 46/80: current_loss=9.88642 | best_loss=9.88642
Epoch 47/80: current_loss=9.96093 | best_loss=9.88642
Epoch 48/80: current_loss=10.03145 | best_loss=9.88642
Epoch 49/80: current_loss=10.17259 | best_loss=9.88642
Epoch 50/80: current_loss=10.31102 | best_loss=9.88642
Epoch 51/80: current_loss=9.90015 | best_loss=9.88642
Epoch 52/80: current_loss=9.92196 | best_loss=9.88642
Epoch 53/80: current_loss=9.93043 | best_loss=9.88642
Epoch 54/80: current_loss=10.80619 | best_loss=9.88642
Epoch 55/80: current_loss=11.12082 | best_loss=9.88642
Epoch 56/80: current_loss=10.41514 | best_loss=9.88642
Epoch 57/80: current_loss=9.95088 | best_loss=9.88642
Epoch 58/80: current_loss=9.90569 | best_loss=9.88642
Epoch 59/80: current_loss=9.90830 | best_loss=9.88642
Epoch 60/80: current_loss=10.15723 | best_loss=9.88642
Epoch 61/80: current_loss=9.90468 | best_loss=9.88642
Epoch 62/80: current_loss=10.50380 | best_loss=9.88642
Epoch 63/80: current_loss=9.94620 | best_loss=9.88642
Epoch 64/80: current_loss=10.11891 | best_loss=9.88642
Epoch 65/80: current_loss=10.02977 | best_loss=9.88642
Epoch 66/80: current_loss=9.97876 | best_loss=9.88642
Early Stopping at epoch 66
      explained_var=0.00264 | mse_loss=9.55561
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.82195 | best_loss=8.82195
Epoch 1/80: current_loss=8.85242 | best_loss=8.82195
Epoch 2/80: current_loss=8.55953 | best_loss=8.55953
Epoch 3/80: current_loss=8.58398 | best_loss=8.55953
Epoch 4/80: current_loss=8.70063 | best_loss=8.55953
Epoch 5/80: current_loss=8.51952 | best_loss=8.51952
Epoch 6/80: current_loss=8.83105 | best_loss=8.51952
Epoch 7/80: current_loss=8.57640 | best_loss=8.51952
Epoch 8/80: current_loss=8.46004 | best_loss=8.46004
Epoch 9/80: current_loss=9.24970 | best_loss=8.46004
Epoch 10/80: current_loss=8.62953 | best_loss=8.46004
Epoch 11/80: current_loss=8.55619 | best_loss=8.46004
Epoch 12/80: current_loss=8.67712 | best_loss=8.46004
Epoch 13/80: current_loss=9.51909 | best_loss=8.46004
Epoch 14/80: current_loss=8.75708 | best_loss=8.46004
Epoch 15/80: current_loss=8.81682 | best_loss=8.46004
Epoch 16/80: current_loss=8.99452 | best_loss=8.46004
Epoch 17/80: current_loss=8.57067 | best_loss=8.46004
Epoch 18/80: current_loss=8.61716 | best_loss=8.46004
Epoch 19/80: current_loss=8.76635 | best_loss=8.46004
Epoch 20/80: current_loss=8.74117 | best_loss=8.46004
Epoch 21/80: current_loss=8.74830 | best_loss=8.46004
Epoch 22/80: current_loss=8.92189 | best_loss=8.46004
Epoch 23/80: current_loss=8.98225 | best_loss=8.46004
Epoch 24/80: current_loss=8.67659 | best_loss=8.46004
Epoch 25/80: current_loss=10.00193 | best_loss=8.46004
Epoch 26/80: current_loss=8.52043 | best_loss=8.46004
Epoch 27/80: current_loss=8.49206 | best_loss=8.46004
Epoch 28/80: current_loss=8.54422 | best_loss=8.46004
Early Stopping at epoch 28
      explained_var=0.00860 | mse_loss=8.56812
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.63940 | best_loss=9.63940
Epoch 1/80: current_loss=10.70314 | best_loss=9.63940
Epoch 2/80: current_loss=9.99020 | best_loss=9.63940
Epoch 3/80: current_loss=9.53230 | best_loss=9.53230
Epoch 4/80: current_loss=9.50707 | best_loss=9.50707
Epoch 5/80: current_loss=10.49213 | best_loss=9.50707
Epoch 6/80: current_loss=9.77575 | best_loss=9.50707
Epoch 7/80: current_loss=9.75643 | best_loss=9.50707
Epoch 8/80: current_loss=9.73383 | best_loss=9.50707
Epoch 9/80: current_loss=10.51047 | best_loss=9.50707
Epoch 10/80: current_loss=9.91301 | best_loss=9.50707
Epoch 11/80: current_loss=9.74295 | best_loss=9.50707
Epoch 12/80: current_loss=9.72143 | best_loss=9.50707
Epoch 13/80: current_loss=9.66559 | best_loss=9.50707
Epoch 14/80: current_loss=9.91627 | best_loss=9.50707
Epoch 15/80: current_loss=9.93013 | best_loss=9.50707
Epoch 16/80: current_loss=10.27069 | best_loss=9.50707
Epoch 17/80: current_loss=10.39827 | best_loss=9.50707
Epoch 18/80: current_loss=10.21786 | best_loss=9.50707
Epoch 19/80: current_loss=9.85022 | best_loss=9.50707
Epoch 20/80: current_loss=9.86690 | best_loss=9.50707
Epoch 21/80: current_loss=9.69555 | best_loss=9.50707
Epoch 22/80: current_loss=9.39529 | best_loss=9.39529
Epoch 23/80: current_loss=9.47703 | best_loss=9.39529
Epoch 24/80: current_loss=9.92350 | best_loss=9.39529
Epoch 25/80: current_loss=9.51792 | best_loss=9.39529
Epoch 26/80: current_loss=9.49232 | best_loss=9.39529
Epoch 27/80: current_loss=9.33324 | best_loss=9.33324
Epoch 28/80: current_loss=9.73139 | best_loss=9.33324
Epoch 29/80: current_loss=9.37560 | best_loss=9.33324
Epoch 30/80: current_loss=9.38647 | best_loss=9.33324
Epoch 31/80: current_loss=9.53725 | best_loss=9.33324
Epoch 32/80: current_loss=9.48564 | best_loss=9.33324
Epoch 33/80: current_loss=9.58896 | best_loss=9.33324
Epoch 34/80: current_loss=9.99237 | best_loss=9.33324
Epoch 35/80: current_loss=9.36776 | best_loss=9.33324
Epoch 36/80: current_loss=9.57608 | best_loss=9.33324
Epoch 37/80: current_loss=9.92050 | best_loss=9.33324
Epoch 38/80: current_loss=9.39645 | best_loss=9.33324
Epoch 39/80: current_loss=9.51333 | best_loss=9.33324
Epoch 40/80: current_loss=9.91771 | best_loss=9.33324
Epoch 41/80: current_loss=10.32320 | best_loss=9.33324
Epoch 42/80: current_loss=9.41104 | best_loss=9.33324
Epoch 43/80: current_loss=9.40474 | best_loss=9.33324
Epoch 44/80: current_loss=9.36054 | best_loss=9.33324
Epoch 45/80: current_loss=9.55435 | best_loss=9.33324
Epoch 46/80: current_loss=9.37147 | best_loss=9.33324
Epoch 47/80: current_loss=9.54662 | best_loss=9.33324
Early Stopping at epoch 47
      explained_var=0.00623 | mse_loss=9.43158
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.33520 | best_loss=9.33520
Epoch 1/80: current_loss=8.95318 | best_loss=8.95318
Epoch 2/80: current_loss=9.21662 | best_loss=8.95318
Epoch 3/80: current_loss=9.01236 | best_loss=8.95318
Epoch 4/80: current_loss=9.07938 | best_loss=8.95318
Epoch 5/80: current_loss=9.00519 | best_loss=8.95318
Epoch 6/80: current_loss=9.10900 | best_loss=8.95318
Epoch 7/80: current_loss=8.96178 | best_loss=8.95318
Epoch 8/80: current_loss=9.03406 | best_loss=8.95318
Epoch 9/80: current_loss=9.32101 | best_loss=8.95318
Epoch 10/80: current_loss=9.35135 | best_loss=8.95318
Epoch 11/80: current_loss=10.74469 | best_loss=8.95318
Epoch 12/80: current_loss=9.38307 | best_loss=8.95318
Epoch 13/80: current_loss=9.50791 | best_loss=8.95318
Epoch 14/80: current_loss=9.06288 | best_loss=8.95318
Epoch 15/80: current_loss=9.46074 | best_loss=8.95318
Epoch 16/80: current_loss=9.54510 | best_loss=8.95318
Epoch 17/80: current_loss=8.96305 | best_loss=8.95318
Epoch 18/80: current_loss=9.09287 | best_loss=8.95318
Epoch 19/80: current_loss=9.40092 | best_loss=8.95318
Epoch 20/80: current_loss=9.70160 | best_loss=8.95318
Epoch 21/80: current_loss=9.84639 | best_loss=8.95318
Early Stopping at epoch 21
      explained_var=-0.01090 | mse_loss=8.46656

----------------------------------------------
Params for Trial 66
{'learning_rate': 0.1, 'weight_decay': 0.002642005883052129, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.28040 | best_loss=10.28040
Epoch 1/80: current_loss=9.96057 | best_loss=9.96057
Epoch 2/80: current_loss=9.93211 | best_loss=9.93211
Epoch 3/80: current_loss=10.61184 | best_loss=9.93211
Epoch 4/80: current_loss=9.88489 | best_loss=9.88489
Epoch 5/80: current_loss=10.79323 | best_loss=9.88489
Epoch 6/80: current_loss=9.91485 | best_loss=9.88489
Epoch 7/80: current_loss=9.99897 | best_loss=9.88489
Epoch 8/80: current_loss=9.94604 | best_loss=9.88489
Epoch 9/80: current_loss=10.83196 | best_loss=9.88489
Epoch 10/80: current_loss=9.91533 | best_loss=9.88489
Epoch 11/80: current_loss=10.55725 | best_loss=9.88489
Epoch 12/80: current_loss=10.01086 | best_loss=9.88489
Epoch 13/80: current_loss=12.98604 | best_loss=9.88489
Epoch 14/80: current_loss=11.07643 | best_loss=9.88489
Epoch 15/80: current_loss=11.20076 | best_loss=9.88489
Epoch 16/80: current_loss=10.25255 | best_loss=9.88489
Epoch 17/80: current_loss=13.37802 | best_loss=9.88489
Epoch 18/80: current_loss=10.33712 | best_loss=9.88489
Epoch 19/80: current_loss=10.13416 | best_loss=9.88489
Epoch 20/80: current_loss=10.45908 | best_loss=9.88489
Epoch 21/80: current_loss=11.84499 | best_loss=9.88489
Epoch 22/80: current_loss=9.90080 | best_loss=9.88489
Epoch 23/80: current_loss=10.07192 | best_loss=9.88489
Epoch 24/80: current_loss=10.29821 | best_loss=9.88489
Early Stopping at epoch 24
      explained_var=0.00289 | mse_loss=9.55419
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.57292 | best_loss=8.57292
Epoch 1/80: current_loss=8.61871 | best_loss=8.57292
Epoch 2/80: current_loss=8.56544 | best_loss=8.56544
Epoch 3/80: current_loss=9.64875 | best_loss=8.56544
Epoch 4/80: current_loss=8.75494 | best_loss=8.56544
Epoch 5/80: current_loss=9.73451 | best_loss=8.56544
Epoch 6/80: current_loss=8.48377 | best_loss=8.48377
Epoch 7/80: current_loss=8.50646 | best_loss=8.48377
Epoch 8/80: current_loss=8.85212 | best_loss=8.48377
Epoch 9/80: current_loss=8.49890 | best_loss=8.48377
Epoch 10/80: current_loss=8.61012 | best_loss=8.48377
Epoch 11/80: current_loss=8.85532 | best_loss=8.48377
Epoch 12/80: current_loss=8.76136 | best_loss=8.48377
Epoch 13/80: current_loss=8.57912 | best_loss=8.48377
Epoch 14/80: current_loss=9.40802 | best_loss=8.48377
Epoch 15/80: current_loss=8.47989 | best_loss=8.47989
Epoch 16/80: current_loss=9.11961 | best_loss=8.47989
Epoch 17/80: current_loss=8.65980 | best_loss=8.47989
Epoch 18/80: current_loss=8.60764 | best_loss=8.47989
Epoch 19/80: current_loss=8.85378 | best_loss=8.47989
Epoch 20/80: current_loss=8.61203 | best_loss=8.47989
Epoch 21/80: current_loss=8.46355 | best_loss=8.46355
Epoch 22/80: current_loss=8.90018 | best_loss=8.46355
Epoch 23/80: current_loss=8.57261 | best_loss=8.46355
Epoch 24/80: current_loss=8.53008 | best_loss=8.46355
Epoch 25/80: current_loss=9.80100 | best_loss=8.46355
Epoch 26/80: current_loss=8.94416 | best_loss=8.46355
Epoch 27/80: current_loss=8.80988 | best_loss=8.46355
Epoch 28/80: current_loss=9.00430 | best_loss=8.46355
Epoch 29/80: current_loss=8.57743 | best_loss=8.46355
Epoch 30/80: current_loss=8.52831 | best_loss=8.46355
Epoch 31/80: current_loss=8.45197 | best_loss=8.45197
Epoch 32/80: current_loss=9.36743 | best_loss=8.45197
Epoch 33/80: current_loss=10.71770 | best_loss=8.45197
Epoch 34/80: current_loss=8.86922 | best_loss=8.45197
Epoch 35/80: current_loss=8.75058 | best_loss=8.45197
Epoch 36/80: current_loss=8.52291 | best_loss=8.45197
Epoch 37/80: current_loss=8.83226 | best_loss=8.45197
Epoch 38/80: current_loss=8.58654 | best_loss=8.45197
Epoch 39/80: current_loss=10.49173 | best_loss=8.45197
Epoch 40/80: current_loss=8.86670 | best_loss=8.45197
Epoch 41/80: current_loss=8.78480 | best_loss=8.45197
Epoch 42/80: current_loss=8.53138 | best_loss=8.45197
Epoch 43/80: current_loss=8.62781 | best_loss=8.45197
Epoch 44/80: current_loss=8.56980 | best_loss=8.45197
Epoch 45/80: current_loss=8.74117 | best_loss=8.45197
Epoch 46/80: current_loss=9.52804 | best_loss=8.45197
Epoch 47/80: current_loss=9.20338 | best_loss=8.45197
Epoch 48/80: current_loss=8.86906 | best_loss=8.45197
Epoch 49/80: current_loss=11.28725 | best_loss=8.45197
Epoch 50/80: current_loss=8.52753 | best_loss=8.45197
Epoch 51/80: current_loss=9.08619 | best_loss=8.45197
Early Stopping at epoch 51
      explained_var=0.00258 | mse_loss=8.55921
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.25923 | best_loss=9.25923
Epoch 1/80: current_loss=9.44312 | best_loss=9.25923
Epoch 2/80: current_loss=9.92381 | best_loss=9.25923
Epoch 3/80: current_loss=9.30133 | best_loss=9.25923
Epoch 4/80: current_loss=9.26452 | best_loss=9.25923
Epoch 5/80: current_loss=10.56846 | best_loss=9.25923
Epoch 6/80: current_loss=10.61834 | best_loss=9.25923
Epoch 7/80: current_loss=10.23116 | best_loss=9.25923
Epoch 8/80: current_loss=10.12710 | best_loss=9.25923
Epoch 9/80: current_loss=9.27017 | best_loss=9.25923
Epoch 10/80: current_loss=11.88504 | best_loss=9.25923
Epoch 11/80: current_loss=11.53910 | best_loss=9.25923
Epoch 12/80: current_loss=9.73965 | best_loss=9.25923
Epoch 13/80: current_loss=9.43936 | best_loss=9.25923
Epoch 14/80: current_loss=9.83635 | best_loss=9.25923
Epoch 15/80: current_loss=9.76043 | best_loss=9.25923
Epoch 16/80: current_loss=9.25677 | best_loss=9.25677
Epoch 17/80: current_loss=9.34299 | best_loss=9.25677
Epoch 18/80: current_loss=10.46244 | best_loss=9.25677
Epoch 19/80: current_loss=9.74816 | best_loss=9.25677
Epoch 20/80: current_loss=11.47248 | best_loss=9.25677
Epoch 21/80: current_loss=9.59788 | best_loss=9.25677
Epoch 22/80: current_loss=9.72159 | best_loss=9.25677
Epoch 23/80: current_loss=10.21525 | best_loss=9.25677
Epoch 24/80: current_loss=9.70239 | best_loss=9.25677
Epoch 25/80: current_loss=9.84039 | best_loss=9.25677
Epoch 26/80: current_loss=10.97724 | best_loss=9.25677
Epoch 27/80: current_loss=10.37744 | best_loss=9.25677
Epoch 28/80: current_loss=9.37255 | best_loss=9.25677
Epoch 29/80: current_loss=9.25783 | best_loss=9.25677
Epoch 30/80: current_loss=9.45926 | best_loss=9.25677
Epoch 31/80: current_loss=9.56422 | best_loss=9.25677
Epoch 32/80: current_loss=13.02067 | best_loss=9.25677
Epoch 33/80: current_loss=9.71608 | best_loss=9.25677
Epoch 34/80: current_loss=9.56668 | best_loss=9.25677
Epoch 35/80: current_loss=9.73402 | best_loss=9.25677
Epoch 36/80: current_loss=9.22445 | best_loss=9.22445
Epoch 37/80: current_loss=9.29868 | best_loss=9.22445
Epoch 38/80: current_loss=9.25431 | best_loss=9.22445
Epoch 39/80: current_loss=9.48488 | best_loss=9.22445
Epoch 40/80: current_loss=9.24842 | best_loss=9.22445
Epoch 41/80: current_loss=9.55759 | best_loss=9.22445
Epoch 42/80: current_loss=9.34203 | best_loss=9.22445
Epoch 43/80: current_loss=9.67842 | best_loss=9.22445
Epoch 44/80: current_loss=10.96427 | best_loss=9.22445
Epoch 45/80: current_loss=9.26130 | best_loss=9.22445
Epoch 46/80: current_loss=9.46476 | best_loss=9.22445
Epoch 47/80: current_loss=10.47409 | best_loss=9.22445
Epoch 48/80: current_loss=9.31947 | best_loss=9.22445
Epoch 49/80: current_loss=9.40889 | best_loss=9.22445
Epoch 50/80: current_loss=9.66437 | best_loss=9.22445
Epoch 51/80: current_loss=15.04789 | best_loss=9.22445
Epoch 52/80: current_loss=9.98909 | best_loss=9.22445
Epoch 53/80: current_loss=9.28593 | best_loss=9.22445
Epoch 54/80: current_loss=9.51386 | best_loss=9.22445
Epoch 55/80: current_loss=9.26930 | best_loss=9.22445
Epoch 56/80: current_loss=10.12171 | best_loss=9.22445
Early Stopping at epoch 56
      explained_var=0.00613 | mse_loss=9.32442
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.11478 | best_loss=9.11478
Epoch 1/80: current_loss=9.25709 | best_loss=9.11478
Epoch 2/80: current_loss=9.48793 | best_loss=9.11478
Epoch 3/80: current_loss=9.03775 | best_loss=9.03775
Epoch 4/80: current_loss=9.95696 | best_loss=9.03775
Epoch 5/80: current_loss=9.26264 | best_loss=9.03775
Epoch 6/80: current_loss=9.20722 | best_loss=9.03775
Epoch 7/80: current_loss=9.68322 | best_loss=9.03775
Epoch 8/80: current_loss=9.58335 | best_loss=9.03775
Epoch 9/80: current_loss=9.14602 | best_loss=9.03775
Epoch 10/80: current_loss=9.27935 | best_loss=9.03775
Epoch 11/80: current_loss=10.01431 | best_loss=9.03775
Epoch 12/80: current_loss=9.10508 | best_loss=9.03775
Epoch 13/80: current_loss=9.10727 | best_loss=9.03775
Epoch 14/80: current_loss=9.07154 | best_loss=9.03775
Epoch 15/80: current_loss=9.02340 | best_loss=9.02340
Epoch 16/80: current_loss=9.45835 | best_loss=9.02340
Epoch 17/80: current_loss=8.99467 | best_loss=8.99467
Epoch 18/80: current_loss=9.04359 | best_loss=8.99467
Epoch 19/80: current_loss=9.52274 | best_loss=8.99467
Epoch 20/80: current_loss=10.94610 | best_loss=8.99467
Epoch 21/80: current_loss=9.21462 | best_loss=8.99467
Epoch 22/80: current_loss=9.02496 | best_loss=8.99467
Epoch 23/80: current_loss=9.22084 | best_loss=8.99467
Epoch 24/80: current_loss=8.99926 | best_loss=8.99467
Epoch 25/80: current_loss=9.72993 | best_loss=8.99467
Epoch 26/80: current_loss=8.97062 | best_loss=8.97062
Epoch 27/80: current_loss=9.63353 | best_loss=8.97062
Epoch 28/80: current_loss=9.00106 | best_loss=8.97062
Epoch 29/80: current_loss=9.07980 | best_loss=8.97062
Epoch 30/80: current_loss=9.10957 | best_loss=8.97062
Epoch 31/80: current_loss=12.21727 | best_loss=8.97062
Epoch 32/80: current_loss=8.97542 | best_loss=8.97062
Epoch 33/80: current_loss=9.08782 | best_loss=8.97062
Epoch 34/80: current_loss=9.07135 | best_loss=8.97062
Epoch 35/80: current_loss=9.06887 | best_loss=8.97062
Epoch 36/80: current_loss=9.00303 | best_loss=8.97062
Epoch 37/80: current_loss=8.98175 | best_loss=8.97062
Epoch 38/80: current_loss=10.00021 | best_loss=8.97062
Epoch 39/80: current_loss=9.46772 | best_loss=8.97062
Epoch 40/80: current_loss=9.56619 | best_loss=8.97062
Epoch 41/80: current_loss=9.11558 | best_loss=8.97062
Epoch 42/80: current_loss=9.01148 | best_loss=8.97062
Epoch 43/80: current_loss=9.93304 | best_loss=8.97062
Epoch 44/80: current_loss=9.11548 | best_loss=8.97062
Epoch 45/80: current_loss=8.95691 | best_loss=8.95691
Epoch 46/80: current_loss=9.02418 | best_loss=8.95691
Epoch 47/80: current_loss=9.21796 | best_loss=8.95691
Epoch 48/80: current_loss=9.07520 | best_loss=8.95691
Epoch 49/80: current_loss=11.50021 | best_loss=8.95691
Epoch 50/80: current_loss=11.38229 | best_loss=8.95691
Epoch 51/80: current_loss=8.90442 | best_loss=8.90442
Epoch 52/80: current_loss=8.93573 | best_loss=8.90442
Epoch 53/80: current_loss=9.52671 | best_loss=8.90442
Epoch 54/80: current_loss=9.69158 | best_loss=8.90442
Epoch 55/80: current_loss=10.59600 | best_loss=8.90442
Epoch 56/80: current_loss=9.20945 | best_loss=8.90442
Epoch 57/80: current_loss=9.83234 | best_loss=8.90442
Epoch 58/80: current_loss=9.60408 | best_loss=8.90442
Epoch 59/80: current_loss=11.31948 | best_loss=8.90442
Epoch 60/80: current_loss=9.00850 | best_loss=8.90442
Epoch 61/80: current_loss=9.38559 | best_loss=8.90442
Epoch 62/80: current_loss=9.04983 | best_loss=8.90442
Epoch 63/80: current_loss=9.26430 | best_loss=8.90442
Epoch 64/80: current_loss=8.98679 | best_loss=8.90442
Epoch 65/80: current_loss=9.38013 | best_loss=8.90442
Epoch 66/80: current_loss=8.97547 | best_loss=8.90442
Epoch 67/80: current_loss=9.82366 | best_loss=8.90442
Epoch 68/80: current_loss=9.21259 | best_loss=8.90442
Epoch 69/80: current_loss=8.94995 | best_loss=8.90442
Epoch 70/80: current_loss=9.09566 | best_loss=8.90442
Epoch 71/80: current_loss=11.88370 | best_loss=8.90442
Early Stopping at epoch 71
      explained_var=0.00125 | mse_loss=8.36080

----------------------------------------------
Params for Trial 67
{'learning_rate': 0.001, 'weight_decay': 0.0015934399423989615, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.18482 | best_loss=10.18482
Epoch 1/80: current_loss=9.93178 | best_loss=9.93178
Epoch 2/80: current_loss=9.91065 | best_loss=9.91065
Epoch 3/80: current_loss=9.91848 | best_loss=9.91065
Epoch 4/80: current_loss=9.94024 | best_loss=9.91065
Epoch 5/80: current_loss=9.90187 | best_loss=9.90187
Epoch 6/80: current_loss=9.90530 | best_loss=9.90187
Epoch 7/80: current_loss=9.91053 | best_loss=9.90187
Epoch 8/80: current_loss=10.01038 | best_loss=9.90187
Epoch 9/80: current_loss=9.93728 | best_loss=9.90187
Epoch 10/80: current_loss=9.94094 | best_loss=9.90187
Epoch 11/80: current_loss=10.00014 | best_loss=9.90187
Epoch 12/80: current_loss=9.90341 | best_loss=9.90187
Epoch 13/80: current_loss=9.97232 | best_loss=9.90187
Epoch 14/80: current_loss=9.98032 | best_loss=9.90187
Epoch 15/80: current_loss=9.95804 | best_loss=9.90187
Epoch 16/80: current_loss=9.89454 | best_loss=9.89454
Epoch 17/80: current_loss=9.92751 | best_loss=9.89454
Epoch 18/80: current_loss=9.90704 | best_loss=9.89454
Epoch 19/80: current_loss=9.93051 | best_loss=9.89454
Epoch 20/80: current_loss=9.90345 | best_loss=9.89454
Epoch 21/80: current_loss=9.91172 | best_loss=9.89454
Epoch 22/80: current_loss=9.92673 | best_loss=9.89454
Epoch 23/80: current_loss=9.94502 | best_loss=9.89454
Epoch 24/80: current_loss=9.91767 | best_loss=9.89454
Epoch 25/80: current_loss=9.90936 | best_loss=9.89454
Epoch 26/80: current_loss=9.89627 | best_loss=9.89454
Epoch 27/80: current_loss=9.90237 | best_loss=9.89454
Epoch 28/80: current_loss=9.96672 | best_loss=9.89454
Epoch 29/80: current_loss=9.94968 | best_loss=9.89454
Epoch 30/80: current_loss=10.07199 | best_loss=9.89454
Epoch 31/80: current_loss=9.89807 | best_loss=9.89454
Epoch 32/80: current_loss=9.89663 | best_loss=9.89454
Epoch 33/80: current_loss=9.90787 | best_loss=9.89454
Epoch 34/80: current_loss=9.92585 | best_loss=9.89454
Epoch 35/80: current_loss=9.93338 | best_loss=9.89454
Epoch 36/80: current_loss=9.90188 | best_loss=9.89454
Early Stopping at epoch 36
      explained_var=0.00162 | mse_loss=9.56132
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50842 | best_loss=8.50842
Epoch 1/80: current_loss=8.58917 | best_loss=8.50842
Epoch 2/80: current_loss=8.50308 | best_loss=8.50308
Epoch 3/80: current_loss=8.52527 | best_loss=8.50308
Epoch 4/80: current_loss=8.53423 | best_loss=8.50308
Epoch 5/80: current_loss=8.50336 | best_loss=8.50308
Epoch 6/80: current_loss=8.52280 | best_loss=8.50308
Epoch 7/80: current_loss=8.53662 | best_loss=8.50308
Epoch 8/80: current_loss=8.50431 | best_loss=8.50308
Epoch 9/80: current_loss=8.50419 | best_loss=8.50308
Epoch 10/80: current_loss=8.54399 | best_loss=8.50308
Epoch 11/80: current_loss=8.53875 | best_loss=8.50308
Epoch 12/80: current_loss=8.50392 | best_loss=8.50308
Epoch 13/80: current_loss=8.52217 | best_loss=8.50308
Epoch 14/80: current_loss=8.65868 | best_loss=8.50308
Epoch 15/80: current_loss=8.52001 | best_loss=8.50308
Epoch 16/80: current_loss=8.53352 | best_loss=8.50308
Epoch 17/80: current_loss=8.50904 | best_loss=8.50308
Epoch 18/80: current_loss=8.53085 | best_loss=8.50308
Epoch 19/80: current_loss=8.56429 | best_loss=8.50308
Epoch 20/80: current_loss=8.52893 | best_loss=8.50308
Epoch 21/80: current_loss=8.60809 | best_loss=8.50308
Epoch 22/80: current_loss=8.50741 | best_loss=8.50308
Early Stopping at epoch 22
      explained_var=0.00057 | mse_loss=8.57235

----------------------------------------------
Params for Trial 68
{'learning_rate': 0.01, 'weight_decay': 0.0012498643522016212, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=12.67105 | best_loss=12.67105
Epoch 1/80: current_loss=9.95302 | best_loss=9.95302
Epoch 2/80: current_loss=12.41239 | best_loss=9.95302
Epoch 3/80: current_loss=10.85892 | best_loss=9.95302
Epoch 4/80: current_loss=10.25772 | best_loss=9.95302
Epoch 5/80: current_loss=9.98728 | best_loss=9.95302
Epoch 6/80: current_loss=10.98527 | best_loss=9.95302
Epoch 7/80: current_loss=10.03887 | best_loss=9.95302
Epoch 8/80: current_loss=9.90198 | best_loss=9.90198
Epoch 9/80: current_loss=9.91012 | best_loss=9.90198
Epoch 10/80: current_loss=10.01930 | best_loss=9.90198
Epoch 11/80: current_loss=9.93519 | best_loss=9.90198
Epoch 12/80: current_loss=9.91384 | best_loss=9.90198
Epoch 13/80: current_loss=9.94349 | best_loss=9.90198
Epoch 14/80: current_loss=12.27688 | best_loss=9.90198
Epoch 15/80: current_loss=9.92996 | best_loss=9.90198
Epoch 16/80: current_loss=10.09794 | best_loss=9.90198
Epoch 17/80: current_loss=9.90919 | best_loss=9.90198
Epoch 18/80: current_loss=10.29374 | best_loss=9.90198
Epoch 19/80: current_loss=10.58398 | best_loss=9.90198
Epoch 20/80: current_loss=10.22832 | best_loss=9.90198
Epoch 21/80: current_loss=11.35344 | best_loss=9.90198
Epoch 22/80: current_loss=10.20374 | best_loss=9.90198
Epoch 23/80: current_loss=11.29563 | best_loss=9.90198
Epoch 24/80: current_loss=10.21328 | best_loss=9.90198
Epoch 25/80: current_loss=12.64258 | best_loss=9.90198
Epoch 26/80: current_loss=10.15356 | best_loss=9.90198
Epoch 27/80: current_loss=10.13273 | best_loss=9.90198
Epoch 28/80: current_loss=10.93289 | best_loss=9.90198
Early Stopping at epoch 28
      explained_var=0.00063 | mse_loss=9.57336

----------------------------------------------
Params for Trial 69
{'learning_rate': 0.0001, 'weight_decay': 0.0024103586278821135, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=40.91893 | best_loss=40.91893
Epoch 1/80: current_loss=22.15035 | best_loss=22.15035
Epoch 2/80: current_loss=16.77267 | best_loss=16.77267
Epoch 3/80: current_loss=14.09834 | best_loss=14.09834
Epoch 4/80: current_loss=12.55441 | best_loss=12.55441
Epoch 5/80: current_loss=11.63006 | best_loss=11.63006
Epoch 6/80: current_loss=11.09075 | best_loss=11.09075
Epoch 7/80: current_loss=10.73730 | best_loss=10.73730
Epoch 8/80: current_loss=10.51349 | best_loss=10.51349
Epoch 9/80: current_loss=10.37636 | best_loss=10.37636
Epoch 10/80: current_loss=10.29170 | best_loss=10.29170
Epoch 11/80: current_loss=10.21051 | best_loss=10.21051
Epoch 12/80: current_loss=10.14940 | best_loss=10.14940
Epoch 13/80: current_loss=10.10901 | best_loss=10.10901
Epoch 14/80: current_loss=10.07876 | best_loss=10.07876
Epoch 15/80: current_loss=10.04799 | best_loss=10.04799
Epoch 16/80: current_loss=10.02300 | best_loss=10.02300
Epoch 17/80: current_loss=10.00368 | best_loss=10.00368
Epoch 18/80: current_loss=9.98331 | best_loss=9.98331
Epoch 19/80: current_loss=9.96786 | best_loss=9.96786
Epoch 20/80: current_loss=9.95404 | best_loss=9.95404
Epoch 21/80: current_loss=9.94203 | best_loss=9.94203
Epoch 22/80: current_loss=9.93892 | best_loss=9.93892
Epoch 23/80: current_loss=9.92742 | best_loss=9.92742
Epoch 24/80: current_loss=9.92561 | best_loss=9.92561
Epoch 25/80: current_loss=9.91945 | best_loss=9.91945
Epoch 26/80: current_loss=9.92349 | best_loss=9.91945
Epoch 27/80: current_loss=9.92201 | best_loss=9.91945
Epoch 28/80: current_loss=9.91632 | best_loss=9.91632
Epoch 29/80: current_loss=9.91481 | best_loss=9.91481
Epoch 30/80: current_loss=9.90829 | best_loss=9.90829
Epoch 31/80: current_loss=9.90525 | best_loss=9.90525
Epoch 32/80: current_loss=9.90823 | best_loss=9.90525
Epoch 33/80: current_loss=9.90428 | best_loss=9.90428
Epoch 34/80: current_loss=9.91012 | best_loss=9.90428
Epoch 35/80: current_loss=9.90642 | best_loss=9.90428
Epoch 36/80: current_loss=9.90770 | best_loss=9.90428
Epoch 37/80: current_loss=9.90782 | best_loss=9.90428
Epoch 38/80: current_loss=9.90800 | best_loss=9.90428
Epoch 39/80: current_loss=9.90517 | best_loss=9.90428
Epoch 40/80: current_loss=9.90479 | best_loss=9.90428
Epoch 41/80: current_loss=9.90741 | best_loss=9.90428
Epoch 42/80: current_loss=9.90844 | best_loss=9.90428
Epoch 43/80: current_loss=9.90525 | best_loss=9.90428
Epoch 44/80: current_loss=9.90335 | best_loss=9.90335
Epoch 45/80: current_loss=9.90174 | best_loss=9.90174
Epoch 46/80: current_loss=9.90836 | best_loss=9.90174
Epoch 47/80: current_loss=9.90563 | best_loss=9.90174
Epoch 48/80: current_loss=9.90653 | best_loss=9.90174
Epoch 49/80: current_loss=9.91292 | best_loss=9.90174
Epoch 50/80: current_loss=9.89921 | best_loss=9.89921
Epoch 51/80: current_loss=9.91809 | best_loss=9.89921
Epoch 52/80: current_loss=9.91127 | best_loss=9.89921
Epoch 53/80: current_loss=9.90791 | best_loss=9.89921
Epoch 54/80: current_loss=9.91083 | best_loss=9.89921
Epoch 55/80: current_loss=9.90859 | best_loss=9.89921
Epoch 56/80: current_loss=9.90959 | best_loss=9.89921
Epoch 57/80: current_loss=9.90503 | best_loss=9.89921
Epoch 58/80: current_loss=9.90384 | best_loss=9.89921
Epoch 59/80: current_loss=9.91375 | best_loss=9.89921
Epoch 60/80: current_loss=9.90847 | best_loss=9.89921
Epoch 61/80: current_loss=9.91448 | best_loss=9.89921
Epoch 62/80: current_loss=9.90708 | best_loss=9.89921
Epoch 63/80: current_loss=9.90654 | best_loss=9.89921
Epoch 64/80: current_loss=9.90591 | best_loss=9.89921
Epoch 65/80: current_loss=9.89948 | best_loss=9.89921
Epoch 66/80: current_loss=9.90754 | best_loss=9.89921
Epoch 67/80: current_loss=9.91020 | best_loss=9.89921
Epoch 68/80: current_loss=9.91693 | best_loss=9.89921
Epoch 69/80: current_loss=9.90825 | best_loss=9.89921
Epoch 70/80: current_loss=9.90439 | best_loss=9.89921
Early Stopping at epoch 70
      explained_var=0.00229 | mse_loss=9.57628

----------------------------------------------
Params for Trial 70
{'learning_rate': 0.01, 'weight_decay': 0.0006073727406846086, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.93699 | best_loss=9.93699
Epoch 1/80: current_loss=9.91542 | best_loss=9.91542
Epoch 2/80: current_loss=10.05017 | best_loss=9.91542
Epoch 3/80: current_loss=9.89182 | best_loss=9.89182
Epoch 4/80: current_loss=9.89448 | best_loss=9.89182
Epoch 5/80: current_loss=9.90179 | best_loss=9.89182
Epoch 6/80: current_loss=10.03670 | best_loss=9.89182
Epoch 7/80: current_loss=9.88910 | best_loss=9.88910
Epoch 8/80: current_loss=10.09243 | best_loss=9.88910
Epoch 9/80: current_loss=9.91711 | best_loss=9.88910
Epoch 10/80: current_loss=10.23006 | best_loss=9.88910
Epoch 11/80: current_loss=10.06350 | best_loss=9.88910
Epoch 12/80: current_loss=10.00806 | best_loss=9.88910
Epoch 13/80: current_loss=9.90259 | best_loss=9.88910
Epoch 14/80: current_loss=9.95916 | best_loss=9.88910
Epoch 15/80: current_loss=10.36945 | best_loss=9.88910
Epoch 16/80: current_loss=10.01656 | best_loss=9.88910
Epoch 17/80: current_loss=9.90179 | best_loss=9.88910
Epoch 18/80: current_loss=9.92309 | best_loss=9.88910
Epoch 19/80: current_loss=9.91090 | best_loss=9.88910
Epoch 20/80: current_loss=10.05916 | best_loss=9.88910
Epoch 21/80: current_loss=9.93221 | best_loss=9.88910
Epoch 22/80: current_loss=9.90294 | best_loss=9.88910
Epoch 23/80: current_loss=9.92280 | best_loss=9.88910
Epoch 24/80: current_loss=9.96065 | best_loss=9.88910
Epoch 25/80: current_loss=9.99560 | best_loss=9.88910
Epoch 26/80: current_loss=10.02931 | best_loss=9.88910
Epoch 27/80: current_loss=9.91080 | best_loss=9.88910
Early Stopping at epoch 27
      explained_var=0.00233 | mse_loss=9.56003
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.47742 | best_loss=8.47742
Epoch 1/80: current_loss=8.48817 | best_loss=8.47742
Epoch 2/80: current_loss=8.47416 | best_loss=8.47416
Epoch 3/80: current_loss=8.53977 | best_loss=8.47416
Epoch 4/80: current_loss=8.58207 | best_loss=8.47416
Epoch 5/80: current_loss=8.51133 | best_loss=8.47416
Epoch 6/80: current_loss=9.70894 | best_loss=8.47416
Epoch 7/80: current_loss=9.06671 | best_loss=8.47416
Epoch 8/80: current_loss=8.52419 | best_loss=8.47416
Epoch 9/80: current_loss=8.52806 | best_loss=8.47416
Epoch 10/80: current_loss=8.58117 | best_loss=8.47416
Epoch 11/80: current_loss=8.51291 | best_loss=8.47416
Epoch 12/80: current_loss=9.33480 | best_loss=8.47416
Epoch 13/80: current_loss=8.70219 | best_loss=8.47416
Epoch 14/80: current_loss=8.53206 | best_loss=8.47416
Epoch 15/80: current_loss=8.50917 | best_loss=8.47416
Epoch 16/80: current_loss=8.54666 | best_loss=8.47416
Epoch 17/80: current_loss=9.07106 | best_loss=8.47416
Epoch 18/80: current_loss=8.46384 | best_loss=8.46384
Epoch 19/80: current_loss=8.66532 | best_loss=8.46384
Epoch 20/80: current_loss=8.64124 | best_loss=8.46384
Epoch 21/80: current_loss=8.71958 | best_loss=8.46384
Epoch 22/80: current_loss=8.48255 | best_loss=8.46384
Epoch 23/80: current_loss=8.52137 | best_loss=8.46384
Epoch 24/80: current_loss=8.68255 | best_loss=8.46384
Epoch 25/80: current_loss=8.59724 | best_loss=8.46384
Epoch 26/80: current_loss=8.70327 | best_loss=8.46384
Epoch 27/80: current_loss=8.53427 | best_loss=8.46384
Epoch 28/80: current_loss=8.61884 | best_loss=8.46384
Epoch 29/80: current_loss=8.65435 | best_loss=8.46384
Epoch 30/80: current_loss=8.50096 | best_loss=8.46384
Epoch 31/80: current_loss=8.75961 | best_loss=8.46384
Epoch 32/80: current_loss=8.56678 | best_loss=8.46384
Epoch 33/80: current_loss=8.69189 | best_loss=8.46384
Epoch 34/80: current_loss=8.53227 | best_loss=8.46384
Epoch 35/80: current_loss=8.60612 | best_loss=8.46384
Epoch 36/80: current_loss=8.77709 | best_loss=8.46384
Epoch 37/80: current_loss=8.51518 | best_loss=8.46384
Epoch 38/80: current_loss=8.51482 | best_loss=8.46384
Early Stopping at epoch 38
      explained_var=0.00187 | mse_loss=8.56047
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.54558 | best_loss=9.54558
Epoch 1/80: current_loss=10.27781 | best_loss=9.54558
Epoch 2/80: current_loss=9.29794 | best_loss=9.29794
Epoch 3/80: current_loss=9.83490 | best_loss=9.29794
Epoch 4/80: current_loss=10.21954 | best_loss=9.29794
Epoch 5/80: current_loss=9.63668 | best_loss=9.29794
Epoch 6/80: current_loss=9.52368 | best_loss=9.29794
Epoch 7/80: current_loss=9.43367 | best_loss=9.29794
Epoch 8/80: current_loss=9.30894 | best_loss=9.29794
Epoch 9/80: current_loss=9.33823 | best_loss=9.29794
Epoch 10/80: current_loss=9.60093 | best_loss=9.29794
Epoch 11/80: current_loss=9.94195 | best_loss=9.29794
Epoch 12/80: current_loss=9.27281 | best_loss=9.27281
Epoch 13/80: current_loss=9.34409 | best_loss=9.27281
Epoch 14/80: current_loss=10.53432 | best_loss=9.27281
Epoch 15/80: current_loss=9.77242 | best_loss=9.27281
Epoch 16/80: current_loss=9.39694 | best_loss=9.27281
Epoch 17/80: current_loss=9.26915 | best_loss=9.26915
Epoch 18/80: current_loss=9.40822 | best_loss=9.26915
Epoch 19/80: current_loss=9.26158 | best_loss=9.26158
Epoch 20/80: current_loss=9.67220 | best_loss=9.26158
Epoch 21/80: current_loss=9.66196 | best_loss=9.26158
Epoch 22/80: current_loss=9.42752 | best_loss=9.26158
Epoch 23/80: current_loss=9.68960 | best_loss=9.26158
Epoch 24/80: current_loss=9.35463 | best_loss=9.26158
Epoch 25/80: current_loss=9.59962 | best_loss=9.26158
Epoch 26/80: current_loss=9.33422 | best_loss=9.26158
Epoch 27/80: current_loss=9.81404 | best_loss=9.26158
Epoch 28/80: current_loss=10.76440 | best_loss=9.26158
Epoch 29/80: current_loss=9.77516 | best_loss=9.26158
Epoch 30/80: current_loss=9.31819 | best_loss=9.26158
Epoch 31/80: current_loss=9.40358 | best_loss=9.26158
Epoch 32/80: current_loss=9.26328 | best_loss=9.26158
Epoch 33/80: current_loss=9.30339 | best_loss=9.26158
Epoch 34/80: current_loss=9.27557 | best_loss=9.26158
Epoch 35/80: current_loss=9.26644 | best_loss=9.26158
Epoch 36/80: current_loss=9.64141 | best_loss=9.26158
Epoch 37/80: current_loss=9.45872 | best_loss=9.26158
Epoch 38/80: current_loss=9.58720 | best_loss=9.26158
Epoch 39/80: current_loss=9.33002 | best_loss=9.26158
Early Stopping at epoch 39
      explained_var=-0.00134 | mse_loss=9.39129
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.92183 | best_loss=8.92183
Epoch 1/80: current_loss=9.07087 | best_loss=8.92183
Epoch 2/80: current_loss=9.46217 | best_loss=8.92183
Epoch 3/80: current_loss=9.06933 | best_loss=8.92183
Epoch 4/80: current_loss=9.08753 | best_loss=8.92183
Epoch 5/80: current_loss=9.31005 | best_loss=8.92183
Epoch 6/80: current_loss=9.01184 | best_loss=8.92183
Epoch 7/80: current_loss=8.93841 | best_loss=8.92183
Epoch 8/80: current_loss=9.56346 | best_loss=8.92183
Epoch 9/80: current_loss=9.00006 | best_loss=8.92183
Epoch 10/80: current_loss=8.92870 | best_loss=8.92183
Epoch 11/80: current_loss=9.03469 | best_loss=8.92183
Epoch 12/80: current_loss=8.98702 | best_loss=8.92183
Epoch 13/80: current_loss=9.00253 | best_loss=8.92183
Epoch 14/80: current_loss=8.93296 | best_loss=8.92183
Epoch 15/80: current_loss=9.29675 | best_loss=8.92183
Epoch 16/80: current_loss=8.97543 | best_loss=8.92183
Epoch 17/80: current_loss=9.02785 | best_loss=8.92183
Epoch 18/80: current_loss=9.04597 | best_loss=8.92183
Epoch 19/80: current_loss=9.22252 | best_loss=8.92183
Epoch 20/80: current_loss=8.99202 | best_loss=8.92183
Early Stopping at epoch 20
      explained_var=0.00408 | mse_loss=8.34677
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.64753 | best_loss=8.64753
Epoch 1/80: current_loss=8.62667 | best_loss=8.62667
Epoch 2/80: current_loss=8.56645 | best_loss=8.56645
Epoch 3/80: current_loss=8.48831 | best_loss=8.48831
Epoch 4/80: current_loss=8.50944 | best_loss=8.48831
Epoch 5/80: current_loss=8.51866 | best_loss=8.48831
Epoch 6/80: current_loss=8.50731 | best_loss=8.48831
Epoch 7/80: current_loss=8.49783 | best_loss=8.48831
Epoch 8/80: current_loss=8.52702 | best_loss=8.48831
Epoch 9/80: current_loss=8.50652 | best_loss=8.48831
Epoch 10/80: current_loss=8.79215 | best_loss=8.48831
Epoch 11/80: current_loss=8.72228 | best_loss=8.48831
Epoch 12/80: current_loss=8.49830 | best_loss=8.48831
Epoch 13/80: current_loss=8.57814 | best_loss=8.48831
Epoch 14/80: current_loss=8.49055 | best_loss=8.48831
Epoch 15/80: current_loss=8.53847 | best_loss=8.48831
Epoch 16/80: current_loss=8.51323 | best_loss=8.48831
Epoch 17/80: current_loss=8.72976 | best_loss=8.48831
Epoch 18/80: current_loss=8.56837 | best_loss=8.48831
Epoch 19/80: current_loss=8.79074 | best_loss=8.48831
Epoch 20/80: current_loss=8.49054 | best_loss=8.48831
Epoch 21/80: current_loss=8.54549 | best_loss=8.48831
Epoch 22/80: current_loss=8.56335 | best_loss=8.48831
Epoch 23/80: current_loss=8.77710 | best_loss=8.48831
Early Stopping at epoch 23
      explained_var=0.00047 | mse_loss=8.20158
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.00148| avg_loss=8.81203
----------------------------------------------


----------------------------------------------
Params for Trial 71
{'learning_rate': 0.01, 'weight_decay': 0.0029626069760214156, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91331 | best_loss=9.91331
Epoch 1/80: current_loss=10.06405 | best_loss=9.91331
Epoch 2/80: current_loss=9.91385 | best_loss=9.91331
Epoch 3/80: current_loss=9.95867 | best_loss=9.91331
Epoch 4/80: current_loss=9.91714 | best_loss=9.91331
Epoch 5/80: current_loss=10.22153 | best_loss=9.91331
Epoch 6/80: current_loss=9.91973 | best_loss=9.91331
Epoch 7/80: current_loss=10.66391 | best_loss=9.91331
Epoch 8/80: current_loss=10.06927 | best_loss=9.91331
Epoch 9/80: current_loss=10.28647 | best_loss=9.91331
Epoch 10/80: current_loss=10.39041 | best_loss=9.91331
Epoch 11/80: current_loss=9.98964 | best_loss=9.91331
Epoch 12/80: current_loss=9.90271 | best_loss=9.90271
Epoch 13/80: current_loss=10.06750 | best_loss=9.90271
Epoch 14/80: current_loss=10.42367 | best_loss=9.90271
Epoch 15/80: current_loss=9.92166 | best_loss=9.90271
Epoch 16/80: current_loss=9.90174 | best_loss=9.90174
Epoch 17/80: current_loss=10.12526 | best_loss=9.90174
Epoch 18/80: current_loss=9.92334 | best_loss=9.90174
Epoch 19/80: current_loss=9.91857 | best_loss=9.90174
Epoch 20/80: current_loss=10.12249 | best_loss=9.90174
Epoch 21/80: current_loss=10.12853 | best_loss=9.90174
Epoch 22/80: current_loss=9.93116 | best_loss=9.90174
Epoch 23/80: current_loss=9.99117 | best_loss=9.90174
Epoch 24/80: current_loss=10.31237 | best_loss=9.90174
Epoch 25/80: current_loss=9.91554 | best_loss=9.90174
Epoch 26/80: current_loss=10.43449 | best_loss=9.90174
Epoch 27/80: current_loss=10.95041 | best_loss=9.90174
Epoch 28/80: current_loss=9.89986 | best_loss=9.89986
Epoch 29/80: current_loss=10.16546 | best_loss=9.89986
Epoch 30/80: current_loss=9.95092 | best_loss=9.89986
Epoch 31/80: current_loss=11.51684 | best_loss=9.89986
Epoch 32/80: current_loss=11.51153 | best_loss=9.89986
Epoch 33/80: current_loss=10.10337 | best_loss=9.89986
Epoch 34/80: current_loss=9.90416 | best_loss=9.89986
Epoch 35/80: current_loss=10.29374 | best_loss=9.89986
Epoch 36/80: current_loss=10.03730 | best_loss=9.89986
Epoch 37/80: current_loss=10.71544 | best_loss=9.89986
Epoch 38/80: current_loss=10.20010 | best_loss=9.89986
Epoch 39/80: current_loss=9.97726 | best_loss=9.89986
Epoch 40/80: current_loss=9.91560 | best_loss=9.89986
Epoch 41/80: current_loss=9.91106 | best_loss=9.89986
Epoch 42/80: current_loss=9.93233 | best_loss=9.89986
Epoch 43/80: current_loss=10.82632 | best_loss=9.89986
Epoch 44/80: current_loss=10.40464 | best_loss=9.89986
Epoch 45/80: current_loss=9.90607 | best_loss=9.89986
Epoch 46/80: current_loss=10.34470 | best_loss=9.89986
Epoch 47/80: current_loss=9.92006 | best_loss=9.89986
Epoch 48/80: current_loss=10.40110 | best_loss=9.89986
Early Stopping at epoch 48
      explained_var=0.00231 | mse_loss=9.57810

----------------------------------------------
Params for Trial 72
{'learning_rate': 0.01, 'weight_decay': 0.003860159229452212, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.93115 | best_loss=9.93115
Epoch 1/80: current_loss=10.02200 | best_loss=9.93115
Epoch 2/80: current_loss=9.91237 | best_loss=9.91237
Epoch 3/80: current_loss=10.17129 | best_loss=9.91237
Epoch 4/80: current_loss=10.01821 | best_loss=9.91237
Epoch 5/80: current_loss=9.94656 | best_loss=9.91237
Epoch 6/80: current_loss=9.95452 | best_loss=9.91237
Epoch 7/80: current_loss=9.89939 | best_loss=9.89939
Epoch 8/80: current_loss=9.94967 | best_loss=9.89939
Epoch 9/80: current_loss=9.92765 | best_loss=9.89939
Epoch 10/80: current_loss=10.18614 | best_loss=9.89939
Epoch 11/80: current_loss=10.59355 | best_loss=9.89939
Epoch 12/80: current_loss=9.91103 | best_loss=9.89939
Epoch 13/80: current_loss=9.90666 | best_loss=9.89939
Epoch 14/80: current_loss=9.89800 | best_loss=9.89800
Epoch 15/80: current_loss=9.90525 | best_loss=9.89800
Epoch 16/80: current_loss=9.99238 | best_loss=9.89800
Epoch 17/80: current_loss=9.89655 | best_loss=9.89655
Epoch 18/80: current_loss=9.91245 | best_loss=9.89655
Epoch 19/80: current_loss=10.37531 | best_loss=9.89655
Epoch 20/80: current_loss=9.95733 | best_loss=9.89655
Epoch 21/80: current_loss=9.95417 | best_loss=9.89655
Epoch 22/80: current_loss=10.65101 | best_loss=9.89655
Epoch 23/80: current_loss=10.14205 | best_loss=9.89655
Epoch 24/80: current_loss=10.25344 | best_loss=9.89655
Epoch 25/80: current_loss=10.52884 | best_loss=9.89655
Epoch 26/80: current_loss=9.93108 | best_loss=9.89655
Epoch 27/80: current_loss=10.34795 | best_loss=9.89655
Epoch 28/80: current_loss=11.94020 | best_loss=9.89655
Epoch 29/80: current_loss=10.03841 | best_loss=9.89655
Epoch 30/80: current_loss=10.01088 | best_loss=9.89655
Epoch 31/80: current_loss=9.97467 | best_loss=9.89655
Epoch 32/80: current_loss=10.09104 | best_loss=9.89655
Epoch 33/80: current_loss=9.90843 | best_loss=9.89655
Epoch 34/80: current_loss=9.91899 | best_loss=9.89655
Epoch 35/80: current_loss=9.90846 | best_loss=9.89655
Epoch 36/80: current_loss=10.11247 | best_loss=9.89655
Epoch 37/80: current_loss=10.16999 | best_loss=9.89655
Early Stopping at epoch 37
      explained_var=0.00139 | mse_loss=9.56345
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.82782 | best_loss=8.82782
Epoch 1/80: current_loss=8.71844 | best_loss=8.71844
Epoch 2/80: current_loss=8.51042 | best_loss=8.51042
Epoch 3/80: current_loss=8.57439 | best_loss=8.51042
Epoch 4/80: current_loss=8.51688 | best_loss=8.51042
Epoch 5/80: current_loss=9.35498 | best_loss=8.51042
Epoch 6/80: current_loss=8.62883 | best_loss=8.51042
Epoch 7/80: current_loss=8.53034 | best_loss=8.51042
Epoch 8/80: current_loss=8.65543 | best_loss=8.51042
Epoch 9/80: current_loss=8.56837 | best_loss=8.51042
Epoch 10/80: current_loss=8.71272 | best_loss=8.51042
Epoch 11/80: current_loss=8.55466 | best_loss=8.51042
Epoch 12/80: current_loss=8.52859 | best_loss=8.51042
Epoch 13/80: current_loss=9.31706 | best_loss=8.51042
Epoch 14/80: current_loss=8.69778 | best_loss=8.51042
Epoch 15/80: current_loss=8.57435 | best_loss=8.51042
Epoch 16/80: current_loss=8.53553 | best_loss=8.51042
Epoch 17/80: current_loss=8.50716 | best_loss=8.50716
Epoch 18/80: current_loss=8.69033 | best_loss=8.50716
Epoch 19/80: current_loss=8.88307 | best_loss=8.50716
Epoch 20/80: current_loss=8.79658 | best_loss=8.50716
Epoch 21/80: current_loss=8.78048 | best_loss=8.50716
Epoch 22/80: current_loss=8.61457 | best_loss=8.50716
Epoch 23/80: current_loss=8.68736 | best_loss=8.50716
Epoch 24/80: current_loss=8.58906 | best_loss=8.50716
Epoch 25/80: current_loss=8.78249 | best_loss=8.50716
Epoch 26/80: current_loss=9.52446 | best_loss=8.50716
Epoch 27/80: current_loss=8.49104 | best_loss=8.49104
Epoch 28/80: current_loss=8.52172 | best_loss=8.49104
Epoch 29/80: current_loss=8.57200 | best_loss=8.49104
Epoch 30/80: current_loss=8.54172 | best_loss=8.49104
Epoch 31/80: current_loss=9.03380 | best_loss=8.49104
Epoch 32/80: current_loss=8.45937 | best_loss=8.45937
Epoch 33/80: current_loss=9.09136 | best_loss=8.45937
Epoch 34/80: current_loss=9.13569 | best_loss=8.45937
Epoch 35/80: current_loss=8.87822 | best_loss=8.45937
Epoch 36/80: current_loss=8.50536 | best_loss=8.45937
Epoch 37/80: current_loss=9.02502 | best_loss=8.45937
Epoch 38/80: current_loss=8.55515 | best_loss=8.45937
Epoch 39/80: current_loss=8.39208 | best_loss=8.39208
Epoch 40/80: current_loss=8.56895 | best_loss=8.39208
Epoch 41/80: current_loss=8.89764 | best_loss=8.39208
Epoch 42/80: current_loss=8.62787 | best_loss=8.39208
Epoch 43/80: current_loss=8.72420 | best_loss=8.39208
Epoch 44/80: current_loss=8.60030 | best_loss=8.39208
Epoch 45/80: current_loss=8.61679 | best_loss=8.39208
Epoch 46/80: current_loss=8.71236 | best_loss=8.39208
Epoch 47/80: current_loss=8.52061 | best_loss=8.39208
Epoch 48/80: current_loss=8.58390 | best_loss=8.39208
Epoch 49/80: current_loss=8.63641 | best_loss=8.39208
Epoch 50/80: current_loss=8.82361 | best_loss=8.39208
Epoch 51/80: current_loss=8.49963 | best_loss=8.39208
Epoch 52/80: current_loss=8.60758 | best_loss=8.39208
Epoch 53/80: current_loss=8.51959 | best_loss=8.39208
Epoch 54/80: current_loss=8.73046 | best_loss=8.39208
Epoch 55/80: current_loss=8.53151 | best_loss=8.39208
Epoch 56/80: current_loss=11.29708 | best_loss=8.39208
Epoch 57/80: current_loss=8.61655 | best_loss=8.39208
Epoch 58/80: current_loss=8.65726 | best_loss=8.39208
Epoch 59/80: current_loss=9.02463 | best_loss=8.39208
Early Stopping at epoch 59
      explained_var=0.00490 | mse_loss=8.57069

----------------------------------------------
Params for Trial 73
{'learning_rate': 0.01, 'weight_decay': 0.0021016370138087887, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.99639 | best_loss=9.99639
Epoch 1/80: current_loss=9.92848 | best_loss=9.92848
Epoch 2/80: current_loss=10.39100 | best_loss=9.92848
Epoch 3/80: current_loss=9.91371 | best_loss=9.91371
Epoch 4/80: current_loss=10.13347 | best_loss=9.91371
Epoch 5/80: current_loss=9.92806 | best_loss=9.91371
Epoch 6/80: current_loss=9.92070 | best_loss=9.91371
Epoch 7/80: current_loss=10.11941 | best_loss=9.91371
Epoch 8/80: current_loss=10.00417 | best_loss=9.91371
Epoch 9/80: current_loss=9.92513 | best_loss=9.91371
Epoch 10/80: current_loss=10.02166 | best_loss=9.91371
Epoch 11/80: current_loss=10.96946 | best_loss=9.91371
Epoch 12/80: current_loss=10.41424 | best_loss=9.91371
Epoch 13/80: current_loss=9.95143 | best_loss=9.91371
Epoch 14/80: current_loss=10.20877 | best_loss=9.91371
Epoch 15/80: current_loss=10.44333 | best_loss=9.91371
Epoch 16/80: current_loss=9.90823 | best_loss=9.90823
Epoch 17/80: current_loss=10.44094 | best_loss=9.90823
Epoch 18/80: current_loss=9.89771 | best_loss=9.89771
Epoch 19/80: current_loss=9.98109 | best_loss=9.89771
Epoch 20/80: current_loss=9.92470 | best_loss=9.89771
Epoch 21/80: current_loss=10.20698 | best_loss=9.89771
Epoch 22/80: current_loss=10.30883 | best_loss=9.89771
Epoch 23/80: current_loss=9.94159 | best_loss=9.89771
Epoch 24/80: current_loss=10.08944 | best_loss=9.89771
Epoch 25/80: current_loss=10.14841 | best_loss=9.89771
Epoch 26/80: current_loss=9.92474 | best_loss=9.89771
Epoch 27/80: current_loss=9.90792 | best_loss=9.89771
Epoch 28/80: current_loss=9.91703 | best_loss=9.89771
Epoch 29/80: current_loss=9.91589 | best_loss=9.89771
Epoch 30/80: current_loss=10.08553 | best_loss=9.89771
Epoch 31/80: current_loss=11.34436 | best_loss=9.89771
Epoch 32/80: current_loss=10.78192 | best_loss=9.89771
Epoch 33/80: current_loss=10.69586 | best_loss=9.89771
Epoch 34/80: current_loss=9.95673 | best_loss=9.89771
Epoch 35/80: current_loss=10.11661 | best_loss=9.89771
Epoch 36/80: current_loss=9.93309 | best_loss=9.89771
Epoch 37/80: current_loss=10.11825 | best_loss=9.89771
Epoch 38/80: current_loss=9.91890 | best_loss=9.89771
Early Stopping at epoch 38
      explained_var=0.00193 | mse_loss=9.57099
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.95775 | best_loss=8.95775
Epoch 1/80: current_loss=8.48793 | best_loss=8.48793
Epoch 2/80: current_loss=8.79635 | best_loss=8.48793
Epoch 3/80: current_loss=8.53055 | best_loss=8.48793
Epoch 4/80: current_loss=9.45486 | best_loss=8.48793
Epoch 5/80: current_loss=8.56826 | best_loss=8.48793
Epoch 6/80: current_loss=8.50194 | best_loss=8.48793
Epoch 7/80: current_loss=8.62641 | best_loss=8.48793
Epoch 8/80: current_loss=8.50233 | best_loss=8.48793
Epoch 9/80: current_loss=8.66084 | best_loss=8.48793
Epoch 10/80: current_loss=8.90471 | best_loss=8.48793
Epoch 11/80: current_loss=8.79542 | best_loss=8.48793
Epoch 12/80: current_loss=8.52412 | best_loss=8.48793
Epoch 13/80: current_loss=8.89428 | best_loss=8.48793
Epoch 14/80: current_loss=8.64927 | best_loss=8.48793
Epoch 15/80: current_loss=8.58236 | best_loss=8.48793
Epoch 16/80: current_loss=8.54231 | best_loss=8.48793
Epoch 17/80: current_loss=8.54147 | best_loss=8.48793
Epoch 18/80: current_loss=8.67279 | best_loss=8.48793
Epoch 19/80: current_loss=8.52446 | best_loss=8.48793
Epoch 20/80: current_loss=8.91756 | best_loss=8.48793
Epoch 21/80: current_loss=8.53208 | best_loss=8.48793
Early Stopping at epoch 21
      explained_var=0.00095 | mse_loss=8.56829

----------------------------------------------
Params for Trial 74
{'learning_rate': 0.01, 'weight_decay': 0.0014484023816759754, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.47314 | best_loss=10.47314
Epoch 1/80: current_loss=10.53152 | best_loss=10.47314
Epoch 2/80: current_loss=10.05798 | best_loss=10.05798
Epoch 3/80: current_loss=10.04797 | best_loss=10.04797
Epoch 4/80: current_loss=10.01735 | best_loss=10.01735
Epoch 5/80: current_loss=9.91599 | best_loss=9.91599
Epoch 6/80: current_loss=10.19010 | best_loss=9.91599
Epoch 7/80: current_loss=9.95723 | best_loss=9.91599
Epoch 8/80: current_loss=10.22347 | best_loss=9.91599
Epoch 9/80: current_loss=9.91494 | best_loss=9.91494
Epoch 10/80: current_loss=9.99331 | best_loss=9.91494
Epoch 11/80: current_loss=10.02348 | best_loss=9.91494
Epoch 12/80: current_loss=10.09080 | best_loss=9.91494
Epoch 13/80: current_loss=10.44593 | best_loss=9.91494
Epoch 14/80: current_loss=9.90265 | best_loss=9.90265
Epoch 15/80: current_loss=9.92695 | best_loss=9.90265
Epoch 16/80: current_loss=10.18123 | best_loss=9.90265
Epoch 17/80: current_loss=9.89201 | best_loss=9.89201
Epoch 18/80: current_loss=9.92581 | best_loss=9.89201
Epoch 19/80: current_loss=9.93895 | best_loss=9.89201
Epoch 20/80: current_loss=10.96131 | best_loss=9.89201
Epoch 21/80: current_loss=11.28770 | best_loss=9.89201
Epoch 22/80: current_loss=10.00237 | best_loss=9.89201
Epoch 23/80: current_loss=11.63381 | best_loss=9.89201
Epoch 24/80: current_loss=10.09335 | best_loss=9.89201
Epoch 25/80: current_loss=10.18666 | best_loss=9.89201
Epoch 26/80: current_loss=10.07993 | best_loss=9.89201
Epoch 27/80: current_loss=9.89251 | best_loss=9.89201
Epoch 28/80: current_loss=9.93286 | best_loss=9.89201
Epoch 29/80: current_loss=10.99416 | best_loss=9.89201
Epoch 30/80: current_loss=9.91140 | best_loss=9.89201
Epoch 31/80: current_loss=10.80565 | best_loss=9.89201
Epoch 32/80: current_loss=9.92177 | best_loss=9.89201
Epoch 33/80: current_loss=9.92630 | best_loss=9.89201
Epoch 34/80: current_loss=10.54698 | best_loss=9.89201
Epoch 35/80: current_loss=9.95429 | best_loss=9.89201
Epoch 36/80: current_loss=10.00434 | best_loss=9.89201
Epoch 37/80: current_loss=9.91363 | best_loss=9.89201
Early Stopping at epoch 37
      explained_var=0.00272 | mse_loss=9.56283
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.63871 | best_loss=8.63871
Epoch 1/80: current_loss=8.56351 | best_loss=8.56351
Epoch 2/80: current_loss=8.74911 | best_loss=8.56351
Epoch 3/80: current_loss=8.79205 | best_loss=8.56351
Epoch 4/80: current_loss=8.55714 | best_loss=8.55714
Epoch 5/80: current_loss=8.70015 | best_loss=8.55714
Epoch 6/80: current_loss=8.84857 | best_loss=8.55714
Epoch 7/80: current_loss=8.61101 | best_loss=8.55714
Epoch 8/80: current_loss=8.52206 | best_loss=8.52206
Epoch 9/80: current_loss=8.70862 | best_loss=8.52206
Epoch 10/80: current_loss=8.54768 | best_loss=8.52206
Epoch 11/80: current_loss=8.51574 | best_loss=8.51574
Epoch 12/80: current_loss=8.57231 | best_loss=8.51574
Epoch 13/80: current_loss=8.66947 | best_loss=8.51574
Epoch 14/80: current_loss=8.88034 | best_loss=8.51574
Epoch 15/80: current_loss=8.66198 | best_loss=8.51574
Epoch 16/80: current_loss=8.63653 | best_loss=8.51574
Epoch 17/80: current_loss=8.79295 | best_loss=8.51574
Epoch 18/80: current_loss=9.20718 | best_loss=8.51574
Epoch 19/80: current_loss=8.48448 | best_loss=8.48448
Epoch 20/80: current_loss=8.75363 | best_loss=8.48448
Epoch 21/80: current_loss=9.35392 | best_loss=8.48448
Epoch 22/80: current_loss=8.49770 | best_loss=8.48448
Epoch 23/80: current_loss=9.28174 | best_loss=8.48448
Epoch 24/80: current_loss=8.74976 | best_loss=8.48448
Epoch 25/80: current_loss=8.54441 | best_loss=8.48448
Epoch 26/80: current_loss=9.53085 | best_loss=8.48448
Epoch 27/80: current_loss=8.50450 | best_loss=8.48448
Epoch 28/80: current_loss=8.36471 | best_loss=8.36471
Epoch 29/80: current_loss=8.85427 | best_loss=8.36471
Epoch 30/80: current_loss=8.47292 | best_loss=8.36471
Epoch 31/80: current_loss=8.93519 | best_loss=8.36471
Epoch 32/80: current_loss=8.76120 | best_loss=8.36471
Epoch 33/80: current_loss=8.72424 | best_loss=8.36471
Epoch 34/80: current_loss=9.14217 | best_loss=8.36471
Epoch 35/80: current_loss=8.38415 | best_loss=8.36471
Epoch 36/80: current_loss=9.72528 | best_loss=8.36471
Epoch 37/80: current_loss=8.91492 | best_loss=8.36471
Epoch 38/80: current_loss=9.21999 | best_loss=8.36471
Epoch 39/80: current_loss=8.68328 | best_loss=8.36471
Epoch 40/80: current_loss=9.45134 | best_loss=8.36471
Epoch 41/80: current_loss=10.46279 | best_loss=8.36471
Epoch 42/80: current_loss=10.59363 | best_loss=8.36471
Epoch 43/80: current_loss=8.87433 | best_loss=8.36471
Epoch 44/80: current_loss=8.61005 | best_loss=8.36471
Epoch 45/80: current_loss=8.81758 | best_loss=8.36471
Epoch 46/80: current_loss=8.94268 | best_loss=8.36471
Epoch 47/80: current_loss=9.11764 | best_loss=8.36471
Epoch 48/80: current_loss=8.80701 | best_loss=8.36471
Early Stopping at epoch 48
      explained_var=0.00988 | mse_loss=8.50164
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.40233 | best_loss=10.40233
Epoch 1/80: current_loss=10.15971 | best_loss=10.15971
Epoch 2/80: current_loss=10.83682 | best_loss=10.15971
Epoch 3/80: current_loss=9.80046 | best_loss=9.80046
Epoch 4/80: current_loss=9.96008 | best_loss=9.80046
Epoch 5/80: current_loss=9.40297 | best_loss=9.40297
Epoch 6/80: current_loss=9.87816 | best_loss=9.40297
Epoch 7/80: current_loss=10.41108 | best_loss=9.40297
Epoch 8/80: current_loss=9.67804 | best_loss=9.40297
Epoch 9/80: current_loss=10.30877 | best_loss=9.40297
Epoch 10/80: current_loss=9.66810 | best_loss=9.40297
Epoch 11/80: current_loss=9.54231 | best_loss=9.40297
Epoch 12/80: current_loss=9.79333 | best_loss=9.40297
Epoch 13/80: current_loss=10.57556 | best_loss=9.40297
Epoch 14/80: current_loss=10.03035 | best_loss=9.40297
Epoch 15/80: current_loss=9.74576 | best_loss=9.40297
Epoch 16/80: current_loss=9.67257 | best_loss=9.40297
Epoch 17/80: current_loss=9.48688 | best_loss=9.40297
Epoch 18/80: current_loss=9.88831 | best_loss=9.40297
Epoch 19/80: current_loss=9.46172 | best_loss=9.40297
Epoch 20/80: current_loss=9.74784 | best_loss=9.40297
Epoch 21/80: current_loss=9.73822 | best_loss=9.40297
Epoch 22/80: current_loss=9.88553 | best_loss=9.40297
Epoch 23/80: current_loss=9.43048 | best_loss=9.40297
Epoch 24/80: current_loss=9.36127 | best_loss=9.36127
Epoch 25/80: current_loss=9.52262 | best_loss=9.36127
Epoch 26/80: current_loss=9.40003 | best_loss=9.36127
Epoch 27/80: current_loss=9.46711 | best_loss=9.36127
Epoch 28/80: current_loss=9.60335 | best_loss=9.36127
Epoch 29/80: current_loss=9.78587 | best_loss=9.36127
Epoch 30/80: current_loss=9.26049 | best_loss=9.26049
Epoch 31/80: current_loss=10.06094 | best_loss=9.26049
Epoch 32/80: current_loss=9.21796 | best_loss=9.21796
Epoch 33/80: current_loss=9.63548 | best_loss=9.21796
Epoch 34/80: current_loss=9.62720 | best_loss=9.21796
Epoch 35/80: current_loss=9.57383 | best_loss=9.21796
Epoch 36/80: current_loss=9.52752 | best_loss=9.21796
Epoch 37/80: current_loss=9.57526 | best_loss=9.21796
Epoch 38/80: current_loss=9.51491 | best_loss=9.21796
Epoch 39/80: current_loss=9.61592 | best_loss=9.21796
Epoch 40/80: current_loss=9.48935 | best_loss=9.21796
Epoch 41/80: current_loss=9.43382 | best_loss=9.21796
Epoch 42/80: current_loss=9.40279 | best_loss=9.21796
Epoch 43/80: current_loss=9.38590 | best_loss=9.21796
Epoch 44/80: current_loss=9.44372 | best_loss=9.21796
Epoch 45/80: current_loss=9.35430 | best_loss=9.21796
Epoch 46/80: current_loss=9.49381 | best_loss=9.21796
Epoch 47/80: current_loss=9.41455 | best_loss=9.21796
Epoch 48/80: current_loss=9.35727 | best_loss=9.21796
Epoch 49/80: current_loss=9.60856 | best_loss=9.21796
Epoch 50/80: current_loss=10.02721 | best_loss=9.21796
Epoch 51/80: current_loss=9.34066 | best_loss=9.21796
Epoch 52/80: current_loss=9.58242 | best_loss=9.21796
Early Stopping at epoch 52
      explained_var=0.00284 | mse_loss=9.36562
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.90620 | best_loss=8.90620
Epoch 1/80: current_loss=9.31613 | best_loss=8.90620
Epoch 2/80: current_loss=9.31956 | best_loss=8.90620
Epoch 3/80: current_loss=9.05649 | best_loss=8.90620
Epoch 4/80: current_loss=9.05813 | best_loss=8.90620
Epoch 5/80: current_loss=9.16763 | best_loss=8.90620
Epoch 6/80: current_loss=8.89726 | best_loss=8.89726
Epoch 7/80: current_loss=9.63633 | best_loss=8.89726
Epoch 8/80: current_loss=9.70941 | best_loss=8.89726
Epoch 9/80: current_loss=10.24500 | best_loss=8.89726
Epoch 10/80: current_loss=9.52267 | best_loss=8.89726
Epoch 11/80: current_loss=9.12112 | best_loss=8.89726
Epoch 12/80: current_loss=8.95062 | best_loss=8.89726
Epoch 13/80: current_loss=8.98945 | best_loss=8.89726
Epoch 14/80: current_loss=9.48723 | best_loss=8.89726
Epoch 15/80: current_loss=9.09667 | best_loss=8.89726
Epoch 16/80: current_loss=8.96118 | best_loss=8.89726
Epoch 17/80: current_loss=9.14400 | best_loss=8.89726
Epoch 18/80: current_loss=9.71481 | best_loss=8.89726
Epoch 19/80: current_loss=9.97583 | best_loss=8.89726
Epoch 20/80: current_loss=9.47383 | best_loss=8.89726
Epoch 21/80: current_loss=9.00527 | best_loss=8.89726
Epoch 22/80: current_loss=8.98513 | best_loss=8.89726
Epoch 23/80: current_loss=9.30102 | best_loss=8.89726
Epoch 24/80: current_loss=9.02646 | best_loss=8.89726
Epoch 25/80: current_loss=9.89495 | best_loss=8.89726
Epoch 26/80: current_loss=9.21232 | best_loss=8.89726
Early Stopping at epoch 26
      explained_var=0.00553 | mse_loss=8.31563
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.49274 | best_loss=8.49274
Epoch 1/80: current_loss=8.49893 | best_loss=8.49274
Epoch 2/80: current_loss=8.56692 | best_loss=8.49274
Epoch 3/80: current_loss=9.54307 | best_loss=8.49274
Epoch 4/80: current_loss=8.65979 | best_loss=8.49274
Epoch 5/80: current_loss=8.59495 | best_loss=8.49274
Epoch 6/80: current_loss=8.61764 | best_loss=8.49274
Epoch 7/80: current_loss=8.55989 | best_loss=8.49274
Epoch 8/80: current_loss=8.65346 | best_loss=8.49274
Epoch 9/80: current_loss=8.95275 | best_loss=8.49274
Epoch 10/80: current_loss=8.50473 | best_loss=8.49274
Epoch 11/80: current_loss=8.83635 | best_loss=8.49274
Epoch 12/80: current_loss=8.51698 | best_loss=8.49274
Epoch 13/80: current_loss=8.52196 | best_loss=8.49274
Epoch 14/80: current_loss=8.65303 | best_loss=8.49274
Epoch 15/80: current_loss=8.55512 | best_loss=8.49274
Epoch 16/80: current_loss=8.88399 | best_loss=8.49274
Epoch 17/80: current_loss=8.54673 | best_loss=8.49274
Epoch 18/80: current_loss=8.56745 | best_loss=8.49274
Epoch 19/80: current_loss=8.57146 | best_loss=8.49274
Epoch 20/80: current_loss=8.85381 | best_loss=8.49274
Early Stopping at epoch 20
      explained_var=0.00029 | mse_loss=8.19847
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.00425| avg_loss=8.78884
----------------------------------------------


----------------------------------------------
Params for Trial 75
{'learning_rate': 0.01, 'weight_decay': 0.0010667523266996543, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.21321 | best_loss=10.21321
Epoch 1/80: current_loss=10.06470 | best_loss=10.06470
Epoch 2/80: current_loss=9.91654 | best_loss=9.91654
Epoch 3/80: current_loss=10.23562 | best_loss=9.91654
Epoch 4/80: current_loss=10.35255 | best_loss=9.91654
Epoch 5/80: current_loss=11.35938 | best_loss=9.91654
Epoch 6/80: current_loss=9.99913 | best_loss=9.91654
Epoch 7/80: current_loss=10.06795 | best_loss=9.91654
Epoch 8/80: current_loss=9.92975 | best_loss=9.91654
Epoch 9/80: current_loss=10.76549 | best_loss=9.91654
Epoch 10/80: current_loss=10.55876 | best_loss=9.91654
Epoch 11/80: current_loss=10.13092 | best_loss=9.91654
Epoch 12/80: current_loss=9.91828 | best_loss=9.91654
Epoch 13/80: current_loss=11.34205 | best_loss=9.91654
Epoch 14/80: current_loss=10.79519 | best_loss=9.91654
Epoch 15/80: current_loss=10.65008 | best_loss=9.91654
Epoch 16/80: current_loss=9.93444 | best_loss=9.91654
Epoch 17/80: current_loss=11.32198 | best_loss=9.91654
Epoch 18/80: current_loss=10.03465 | best_loss=9.91654
Epoch 19/80: current_loss=10.01225 | best_loss=9.91654
Epoch 20/80: current_loss=9.92188 | best_loss=9.91654
Epoch 21/80: current_loss=9.90989 | best_loss=9.90989
Epoch 22/80: current_loss=11.02534 | best_loss=9.90989
Epoch 23/80: current_loss=9.90641 | best_loss=9.90641
Epoch 24/80: current_loss=9.91180 | best_loss=9.90641
Epoch 25/80: current_loss=9.90184 | best_loss=9.90184
Epoch 26/80: current_loss=11.55304 | best_loss=9.90184
Epoch 27/80: current_loss=9.97499 | best_loss=9.90184
Epoch 28/80: current_loss=9.93109 | best_loss=9.90184
Epoch 29/80: current_loss=9.96512 | best_loss=9.90184
Epoch 30/80: current_loss=10.56744 | best_loss=9.90184
Epoch 31/80: current_loss=9.93912 | best_loss=9.90184
Epoch 32/80: current_loss=10.40550 | best_loss=9.90184
Epoch 33/80: current_loss=9.94742 | best_loss=9.90184
Epoch 34/80: current_loss=9.90470 | best_loss=9.90184
Epoch 35/80: current_loss=10.24064 | best_loss=9.90184
Epoch 36/80: current_loss=9.91769 | best_loss=9.90184
Epoch 37/80: current_loss=9.89355 | best_loss=9.89355
Epoch 38/80: current_loss=10.35846 | best_loss=9.89355
Epoch 39/80: current_loss=10.15873 | best_loss=9.89355
Epoch 40/80: current_loss=10.10960 | best_loss=9.89355
Epoch 41/80: current_loss=10.03112 | best_loss=9.89355
Epoch 42/80: current_loss=10.11683 | best_loss=9.89355
Epoch 43/80: current_loss=9.91088 | best_loss=9.89355
Epoch 44/80: current_loss=10.42007 | best_loss=9.89355
Epoch 45/80: current_loss=10.37300 | best_loss=9.89355
Epoch 46/80: current_loss=10.44215 | best_loss=9.89355
Epoch 47/80: current_loss=9.93268 | best_loss=9.89355
Epoch 48/80: current_loss=9.91413 | best_loss=9.89355
Epoch 49/80: current_loss=9.94868 | best_loss=9.89355
Epoch 50/80: current_loss=9.99516 | best_loss=9.89355
Epoch 51/80: current_loss=9.91447 | best_loss=9.89355
Epoch 52/80: current_loss=9.95767 | best_loss=9.89355
Epoch 53/80: current_loss=10.04696 | best_loss=9.89355
Epoch 54/80: current_loss=10.79642 | best_loss=9.89355
Epoch 55/80: current_loss=10.46601 | best_loss=9.89355
Epoch 56/80: current_loss=9.95346 | best_loss=9.89355
Epoch 57/80: current_loss=11.06997 | best_loss=9.89355
Early Stopping at epoch 57
      explained_var=0.00162 | mse_loss=9.56228
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.82553 | best_loss=8.82553
Epoch 1/80: current_loss=8.73207 | best_loss=8.73207
Epoch 2/80: current_loss=8.47654 | best_loss=8.47654
Epoch 3/80: current_loss=9.08253 | best_loss=8.47654
Epoch 4/80: current_loss=8.52805 | best_loss=8.47654
Epoch 5/80: current_loss=9.88506 | best_loss=8.47654
Epoch 6/80: current_loss=8.51785 | best_loss=8.47654
Epoch 7/80: current_loss=8.50444 | best_loss=8.47654
Epoch 8/80: current_loss=8.59070 | best_loss=8.47654
Epoch 9/80: current_loss=8.82317 | best_loss=8.47654
Epoch 10/80: current_loss=8.50500 | best_loss=8.47654
Epoch 11/80: current_loss=8.49951 | best_loss=8.47654
Epoch 12/80: current_loss=8.49802 | best_loss=8.47654
Epoch 13/80: current_loss=8.55523 | best_loss=8.47654
Epoch 14/80: current_loss=8.98111 | best_loss=8.47654
Epoch 15/80: current_loss=8.50645 | best_loss=8.47654
Epoch 16/80: current_loss=9.25873 | best_loss=8.47654
Epoch 17/80: current_loss=8.53058 | best_loss=8.47654
Epoch 18/80: current_loss=9.42474 | best_loss=8.47654
Epoch 19/80: current_loss=8.89329 | best_loss=8.47654
Epoch 20/80: current_loss=8.54189 | best_loss=8.47654
Epoch 21/80: current_loss=8.74772 | best_loss=8.47654
Epoch 22/80: current_loss=9.00211 | best_loss=8.47654
Early Stopping at epoch 22
      explained_var=0.00065 | mse_loss=8.58406

----------------------------------------------
Params for Trial 76
{'learning_rate': 1e-05, 'weight_decay': 0.0014005336154984615, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=73.14789 | best_loss=73.14789
Epoch 1/80: current_loss=70.90527 | best_loss=70.90527
Epoch 2/80: current_loss=67.85202 | best_loss=67.85202
Epoch 3/80: current_loss=63.32033 | best_loss=63.32033
Epoch 4/80: current_loss=56.71482 | best_loss=56.71482
Epoch 5/80: current_loss=48.79150 | best_loss=48.79150
Epoch 6/80: current_loss=41.45078 | best_loss=41.45078
Epoch 7/80: current_loss=35.54564 | best_loss=35.54564
Epoch 8/80: current_loss=31.09537 | best_loss=31.09537
Epoch 9/80: current_loss=27.75870 | best_loss=27.75870
Epoch 10/80: current_loss=25.16903 | best_loss=25.16903
Epoch 11/80: current_loss=23.12231 | best_loss=23.12231
Epoch 12/80: current_loss=21.47623 | best_loss=21.47623
Epoch 13/80: current_loss=20.14047 | best_loss=20.14047
Epoch 14/80: current_loss=19.04633 | best_loss=19.04633
Epoch 15/80: current_loss=18.09923 | best_loss=18.09923
Epoch 16/80: current_loss=17.30023 | best_loss=17.30023
Epoch 17/80: current_loss=16.61906 | best_loss=16.61906
Epoch 18/80: current_loss=16.02940 | best_loss=16.02940
Epoch 19/80: current_loss=15.50293 | best_loss=15.50293
Epoch 20/80: current_loss=15.04515 | best_loss=15.04515
Epoch 21/80: current_loss=14.64878 | best_loss=14.64878
Epoch 22/80: current_loss=14.28570 | best_loss=14.28570
Epoch 23/80: current_loss=13.97702 | best_loss=13.97702
Epoch 24/80: current_loss=13.68850 | best_loss=13.68850
Epoch 25/80: current_loss=13.43507 | best_loss=13.43507
Epoch 26/80: current_loss=13.20140 | best_loss=13.20140
Epoch 27/80: current_loss=12.99121 | best_loss=12.99121
Epoch 28/80: current_loss=12.80433 | best_loss=12.80433
Epoch 29/80: current_loss=12.63601 | best_loss=12.63601
Epoch 30/80: current_loss=12.47724 | best_loss=12.47724
Epoch 31/80: current_loss=12.33199 | best_loss=12.33199
Epoch 32/80: current_loss=12.19559 | best_loss=12.19559
Epoch 33/80: current_loss=12.07356 | best_loss=12.07356
Epoch 34/80: current_loss=11.96211 | best_loss=11.96211
Epoch 35/80: current_loss=11.85641 | best_loss=11.85641
Epoch 36/80: current_loss=11.76517 | best_loss=11.76517
Epoch 37/80: current_loss=11.67728 | best_loss=11.67728
Epoch 38/80: current_loss=11.59680 | best_loss=11.59680
Epoch 39/80: current_loss=11.52094 | best_loss=11.52094
Epoch 40/80: current_loss=11.45423 | best_loss=11.45423
Epoch 41/80: current_loss=11.39348 | best_loss=11.39348
Epoch 42/80: current_loss=11.33274 | best_loss=11.33274
Epoch 43/80: current_loss=11.27786 | best_loss=11.27786
Epoch 44/80: current_loss=11.22675 | best_loss=11.22675
Epoch 45/80: current_loss=11.17738 | best_loss=11.17738
Epoch 46/80: current_loss=11.13070 | best_loss=11.13070
Epoch 47/80: current_loss=11.08809 | best_loss=11.08809
Epoch 48/80: current_loss=11.04629 | best_loss=11.04629
Epoch 49/80: current_loss=11.01373 | best_loss=11.01373
Epoch 50/80: current_loss=10.98318 | best_loss=10.98318
Epoch 51/80: current_loss=10.95087 | best_loss=10.95087
Epoch 52/80: current_loss=10.91960 | best_loss=10.91960
Epoch 53/80: current_loss=10.89078 | best_loss=10.89078
Epoch 54/80: current_loss=10.86316 | best_loss=10.86316
Epoch 55/80: current_loss=10.83849 | best_loss=10.83849
Epoch 56/80: current_loss=10.81606 | best_loss=10.81606
Epoch 57/80: current_loss=10.79291 | best_loss=10.79291
Epoch 58/80: current_loss=10.77129 | best_loss=10.77129
Epoch 59/80: current_loss=10.75125 | best_loss=10.75125
Epoch 60/80: current_loss=10.73487 | best_loss=10.73487
Epoch 61/80: current_loss=10.71637 | best_loss=10.71637
Epoch 62/80: current_loss=10.70027 | best_loss=10.70027
Epoch 63/80: current_loss=10.68384 | best_loss=10.68384
Epoch 64/80: current_loss=10.66594 | best_loss=10.66594
Epoch 65/80: current_loss=10.65090 | best_loss=10.65090
Epoch 66/80: current_loss=10.63391 | best_loss=10.63391
Epoch 67/80: current_loss=10.61915 | best_loss=10.61915
Epoch 68/80: current_loss=10.60526 | best_loss=10.60526
Epoch 69/80: current_loss=10.59114 | best_loss=10.59114
Epoch 70/80: current_loss=10.57640 | best_loss=10.57640
Epoch 71/80: current_loss=10.56219 | best_loss=10.56219
Epoch 72/80: current_loss=10.54938 | best_loss=10.54938
Epoch 73/80: current_loss=10.53537 | best_loss=10.53537
Epoch 74/80: current_loss=10.52229 | best_loss=10.52229
Epoch 75/80: current_loss=10.50886 | best_loss=10.50886
Epoch 76/80: current_loss=10.49641 | best_loss=10.49641
Epoch 77/80: current_loss=10.48373 | best_loss=10.48373
Epoch 78/80: current_loss=10.47037 | best_loss=10.47037
Epoch 79/80: current_loss=10.45673 | best_loss=10.45673
      explained_var=-0.04413 | mse_loss=10.00509

----------------------------------------------
Params for Trial 77
{'learning_rate': 0.01, 'weight_decay': 0.0018792746780009644, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.92895 | best_loss=10.92895
Epoch 1/80: current_loss=10.21960 | best_loss=10.21960
Epoch 2/80: current_loss=10.48422 | best_loss=10.21960
Epoch 3/80: current_loss=10.11801 | best_loss=10.11801
Epoch 4/80: current_loss=9.99077 | best_loss=9.99077
Epoch 5/80: current_loss=10.67699 | best_loss=9.99077
Epoch 6/80: current_loss=10.09620 | best_loss=9.99077
Epoch 7/80: current_loss=9.94688 | best_loss=9.94688
Epoch 8/80: current_loss=9.98798 | best_loss=9.94688
Epoch 9/80: current_loss=9.95109 | best_loss=9.94688
Epoch 10/80: current_loss=10.03726 | best_loss=9.94688
Epoch 11/80: current_loss=9.92075 | best_loss=9.92075
Epoch 12/80: current_loss=9.89637 | best_loss=9.89637
Epoch 13/80: current_loss=9.89898 | best_loss=9.89637
Epoch 14/80: current_loss=9.97591 | best_loss=9.89637
Epoch 15/80: current_loss=10.05517 | best_loss=9.89637
Epoch 16/80: current_loss=10.40823 | best_loss=9.89637
Epoch 17/80: current_loss=10.45993 | best_loss=9.89637
Epoch 18/80: current_loss=9.92397 | best_loss=9.89637
Epoch 19/80: current_loss=9.92082 | best_loss=9.89637
Epoch 20/80: current_loss=9.98851 | best_loss=9.89637
Epoch 21/80: current_loss=9.89714 | best_loss=9.89637
Epoch 22/80: current_loss=9.92536 | best_loss=9.89637
Epoch 23/80: current_loss=10.02869 | best_loss=9.89637
Epoch 24/80: current_loss=10.18259 | best_loss=9.89637
Epoch 25/80: current_loss=10.10816 | best_loss=9.89637
Epoch 26/80: current_loss=10.00343 | best_loss=9.89637
Epoch 27/80: current_loss=9.91515 | best_loss=9.89637
Epoch 28/80: current_loss=10.49663 | best_loss=9.89637
Epoch 29/80: current_loss=10.19909 | best_loss=9.89637
Epoch 30/80: current_loss=10.08505 | best_loss=9.89637
Epoch 31/80: current_loss=9.90306 | best_loss=9.89637
Epoch 32/80: current_loss=10.09731 | best_loss=9.89637
Early Stopping at epoch 32
      explained_var=0.00215 | mse_loss=9.55621
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.83725 | best_loss=8.83725
Epoch 1/80: current_loss=8.63620 | best_loss=8.63620
Epoch 2/80: current_loss=8.44752 | best_loss=8.44752
Epoch 3/80: current_loss=8.46127 | best_loss=8.44752
Epoch 4/80: current_loss=8.89612 | best_loss=8.44752
Epoch 5/80: current_loss=8.77936 | best_loss=8.44752
Epoch 6/80: current_loss=9.31312 | best_loss=8.44752
Epoch 7/80: current_loss=8.90229 | best_loss=8.44752
Epoch 8/80: current_loss=8.47220 | best_loss=8.44752
Epoch 9/80: current_loss=8.46796 | best_loss=8.44752
Epoch 10/80: current_loss=8.52218 | best_loss=8.44752
Epoch 11/80: current_loss=8.46730 | best_loss=8.44752
Epoch 12/80: current_loss=8.60065 | best_loss=8.44752
Epoch 13/80: current_loss=8.46293 | best_loss=8.44752
Epoch 14/80: current_loss=8.51056 | best_loss=8.44752
Epoch 15/80: current_loss=8.76149 | best_loss=8.44752
Epoch 16/80: current_loss=8.87507 | best_loss=8.44752
Epoch 17/80: current_loss=8.53289 | best_loss=8.44752
Epoch 18/80: current_loss=8.87542 | best_loss=8.44752
Epoch 19/80: current_loss=8.53294 | best_loss=8.44752
Epoch 20/80: current_loss=8.64164 | best_loss=8.44752
Epoch 21/80: current_loss=8.53619 | best_loss=8.44752
Epoch 22/80: current_loss=8.65112 | best_loss=8.44752
Early Stopping at epoch 22
      explained_var=0.00164 | mse_loss=8.56245
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.57844 | best_loss=9.57844
Epoch 1/80: current_loss=9.65689 | best_loss=9.57844
Epoch 2/80: current_loss=10.05668 | best_loss=9.57844
Epoch 3/80: current_loss=9.49875 | best_loss=9.49875
Epoch 4/80: current_loss=9.88375 | best_loss=9.49875
Epoch 5/80: current_loss=10.10823 | best_loss=9.49875
Epoch 6/80: current_loss=10.76166 | best_loss=9.49875
Epoch 7/80: current_loss=9.70346 | best_loss=9.49875
Epoch 8/80: current_loss=9.72554 | best_loss=9.49875
Epoch 9/80: current_loss=10.28476 | best_loss=9.49875
Epoch 10/80: current_loss=10.11161 | best_loss=9.49875
Epoch 11/80: current_loss=9.42419 | best_loss=9.42419
Epoch 12/80: current_loss=9.83201 | best_loss=9.42419
Epoch 13/80: current_loss=9.44608 | best_loss=9.42419
Epoch 14/80: current_loss=9.75989 | best_loss=9.42419
Epoch 15/80: current_loss=10.01867 | best_loss=9.42419
Epoch 16/80: current_loss=9.46153 | best_loss=9.42419
Epoch 17/80: current_loss=10.54992 | best_loss=9.42419
Epoch 18/80: current_loss=9.54582 | best_loss=9.42419
Epoch 19/80: current_loss=9.63103 | best_loss=9.42419
Epoch 20/80: current_loss=9.80135 | best_loss=9.42419
Epoch 21/80: current_loss=9.39665 | best_loss=9.39665
Epoch 22/80: current_loss=10.01601 | best_loss=9.39665
Epoch 23/80: current_loss=9.92739 | best_loss=9.39665
Epoch 24/80: current_loss=10.37652 | best_loss=9.39665
Epoch 25/80: current_loss=10.06680 | best_loss=9.39665
Epoch 26/80: current_loss=9.46565 | best_loss=9.39665
Epoch 27/80: current_loss=9.39081 | best_loss=9.39081
Epoch 28/80: current_loss=10.14411 | best_loss=9.39081
Epoch 29/80: current_loss=9.31789 | best_loss=9.31789
Epoch 30/80: current_loss=9.95937 | best_loss=9.31789
Epoch 31/80: current_loss=9.37107 | best_loss=9.31789
Epoch 32/80: current_loss=10.40481 | best_loss=9.31789
Epoch 33/80: current_loss=9.72682 | best_loss=9.31789
Epoch 34/80: current_loss=9.42885 | best_loss=9.31789
Epoch 35/80: current_loss=9.38142 | best_loss=9.31789
Epoch 36/80: current_loss=9.94204 | best_loss=9.31789
Epoch 37/80: current_loss=9.61324 | best_loss=9.31789
Epoch 38/80: current_loss=9.86414 | best_loss=9.31789
Epoch 39/80: current_loss=9.55673 | best_loss=9.31789
Epoch 40/80: current_loss=9.36173 | best_loss=9.31789
Epoch 41/80: current_loss=9.48465 | best_loss=9.31789
Epoch 42/80: current_loss=9.52958 | best_loss=9.31789
Epoch 43/80: current_loss=9.41460 | best_loss=9.31789
Epoch 44/80: current_loss=9.36363 | best_loss=9.31789
Epoch 45/80: current_loss=9.48372 | best_loss=9.31789
Epoch 46/80: current_loss=9.52137 | best_loss=9.31789
Epoch 47/80: current_loss=9.35104 | best_loss=9.31789
Epoch 48/80: current_loss=9.47432 | best_loss=9.31789
Epoch 49/80: current_loss=9.61873 | best_loss=9.31789
Early Stopping at epoch 49
      explained_var=-0.00913 | mse_loss=9.47278
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.98482 | best_loss=8.98482
Epoch 1/80: current_loss=8.97063 | best_loss=8.97063
Epoch 2/80: current_loss=9.06315 | best_loss=8.97063
Epoch 3/80: current_loss=9.34657 | best_loss=8.97063
Epoch 4/80: current_loss=9.30368 | best_loss=8.97063
Epoch 5/80: current_loss=8.92820 | best_loss=8.92820
Epoch 6/80: current_loss=11.27915 | best_loss=8.92820
Epoch 7/80: current_loss=9.57368 | best_loss=8.92820
Epoch 8/80: current_loss=9.36174 | best_loss=8.92820
Epoch 9/80: current_loss=8.97502 | best_loss=8.92820
Epoch 10/80: current_loss=9.86547 | best_loss=8.92820
Epoch 11/80: current_loss=8.89815 | best_loss=8.89815
Epoch 12/80: current_loss=9.11948 | best_loss=8.89815
Epoch 13/80: current_loss=8.93060 | best_loss=8.89815
Epoch 14/80: current_loss=9.49303 | best_loss=8.89815
Epoch 15/80: current_loss=9.10269 | best_loss=8.89815
Epoch 16/80: current_loss=9.06572 | best_loss=8.89815
Epoch 17/80: current_loss=8.93822 | best_loss=8.89815
Epoch 18/80: current_loss=8.91945 | best_loss=8.89815
Epoch 19/80: current_loss=8.97038 | best_loss=8.89815
Epoch 20/80: current_loss=8.92535 | best_loss=8.89815
Epoch 21/80: current_loss=9.02381 | best_loss=8.89815
Epoch 22/80: current_loss=9.06571 | best_loss=8.89815
Epoch 23/80: current_loss=9.16249 | best_loss=8.89815
Epoch 24/80: current_loss=9.02508 | best_loss=8.89815
Epoch 25/80: current_loss=9.13426 | best_loss=8.89815
Epoch 26/80: current_loss=9.02850 | best_loss=8.89815
Epoch 27/80: current_loss=9.09400 | best_loss=8.89815
Epoch 28/80: current_loss=9.37956 | best_loss=8.89815
Epoch 29/80: current_loss=8.90409 | best_loss=8.89815
Epoch 30/80: current_loss=9.69721 | best_loss=8.89815
Epoch 31/80: current_loss=9.54685 | best_loss=8.89815
Early Stopping at epoch 31
      explained_var=0.00003 | mse_loss=8.36296

----------------------------------------------
Params for Trial 78
{'learning_rate': 0.01, 'weight_decay': 0.0007501706541449324, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.33094 | best_loss=10.33094
Epoch 1/80: current_loss=10.11310 | best_loss=10.11310
Epoch 2/80: current_loss=9.90783 | best_loss=9.90783
Epoch 3/80: current_loss=10.02538 | best_loss=9.90783
Epoch 4/80: current_loss=9.97718 | best_loss=9.90783
Epoch 5/80: current_loss=10.11623 | best_loss=9.90783
Epoch 6/80: current_loss=9.91059 | best_loss=9.90783
Epoch 7/80: current_loss=10.05269 | best_loss=9.90783
Epoch 8/80: current_loss=9.91322 | best_loss=9.90783
Epoch 9/80: current_loss=10.78277 | best_loss=9.90783
Epoch 10/80: current_loss=10.51859 | best_loss=9.90783
Epoch 11/80: current_loss=9.90205 | best_loss=9.90205
Epoch 12/80: current_loss=10.22656 | best_loss=9.90205
Epoch 13/80: current_loss=9.88927 | best_loss=9.88927
Epoch 14/80: current_loss=10.03247 | best_loss=9.88927
Epoch 15/80: current_loss=9.89416 | best_loss=9.88927
Epoch 16/80: current_loss=9.89315 | best_loss=9.88927
Epoch 17/80: current_loss=9.90253 | best_loss=9.88927
Epoch 18/80: current_loss=9.94618 | best_loss=9.88927
Epoch 19/80: current_loss=9.90695 | best_loss=9.88927
Epoch 20/80: current_loss=10.37536 | best_loss=9.88927
Epoch 21/80: current_loss=9.97611 | best_loss=9.88927
Epoch 22/80: current_loss=9.97411 | best_loss=9.88927
Epoch 23/80: current_loss=9.94331 | best_loss=9.88927
Epoch 24/80: current_loss=11.19705 | best_loss=9.88927
Epoch 25/80: current_loss=10.05655 | best_loss=9.88927
Epoch 26/80: current_loss=9.95097 | best_loss=9.88927
Epoch 27/80: current_loss=9.92036 | best_loss=9.88927
Epoch 28/80: current_loss=9.95182 | best_loss=9.88927
Epoch 29/80: current_loss=10.22555 | best_loss=9.88927
Epoch 30/80: current_loss=9.96474 | best_loss=9.88927
Epoch 31/80: current_loss=10.16795 | best_loss=9.88927
Epoch 32/80: current_loss=10.71265 | best_loss=9.88927
Epoch 33/80: current_loss=10.60848 | best_loss=9.88927
Early Stopping at epoch 33
      explained_var=0.00215 | mse_loss=9.55792
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.54605 | best_loss=8.54605
Epoch 1/80: current_loss=8.51594 | best_loss=8.51594
Epoch 2/80: current_loss=8.63608 | best_loss=8.51594
Epoch 3/80: current_loss=8.61547 | best_loss=8.51594
Epoch 4/80: current_loss=8.50572 | best_loss=8.50572
Epoch 5/80: current_loss=8.80344 | best_loss=8.50572
Epoch 6/80: current_loss=8.49563 | best_loss=8.49563
Epoch 7/80: current_loss=8.75204 | best_loss=8.49563
Epoch 8/80: current_loss=8.87044 | best_loss=8.49563
Epoch 9/80: current_loss=8.53397 | best_loss=8.49563
Epoch 10/80: current_loss=8.48928 | best_loss=8.48928
Epoch 11/80: current_loss=8.57721 | best_loss=8.48928
Epoch 12/80: current_loss=8.53098 | best_loss=8.48928
Epoch 13/80: current_loss=8.58681 | best_loss=8.48928
Epoch 14/80: current_loss=9.28859 | best_loss=8.48928
Epoch 15/80: current_loss=9.07653 | best_loss=8.48928
Epoch 16/80: current_loss=8.70535 | best_loss=8.48928
Epoch 17/80: current_loss=8.53470 | best_loss=8.48928
Epoch 18/80: current_loss=8.62932 | best_loss=8.48928
Epoch 19/80: current_loss=8.89352 | best_loss=8.48928
Epoch 20/80: current_loss=9.20089 | best_loss=8.48928
Epoch 21/80: current_loss=8.52190 | best_loss=8.48928
Epoch 22/80: current_loss=8.48399 | best_loss=8.48399
Epoch 23/80: current_loss=8.82179 | best_loss=8.48399
Epoch 24/80: current_loss=8.51457 | best_loss=8.48399
Epoch 25/80: current_loss=8.78422 | best_loss=8.48399
Epoch 26/80: current_loss=8.97164 | best_loss=8.48399
Epoch 27/80: current_loss=8.53781 | best_loss=8.48399
Epoch 28/80: current_loss=9.37301 | best_loss=8.48399
Epoch 29/80: current_loss=9.24759 | best_loss=8.48399
Epoch 30/80: current_loss=8.47121 | best_loss=8.47121
Epoch 31/80: current_loss=9.26222 | best_loss=8.47121
Epoch 32/80: current_loss=8.50397 | best_loss=8.47121
Epoch 33/80: current_loss=8.70754 | best_loss=8.47121
Epoch 34/80: current_loss=8.62546 | best_loss=8.47121
Epoch 35/80: current_loss=8.55892 | best_loss=8.47121
Epoch 36/80: current_loss=9.22594 | best_loss=8.47121
Epoch 37/80: current_loss=8.57698 | best_loss=8.47121
Epoch 38/80: current_loss=8.51146 | best_loss=8.47121
Epoch 39/80: current_loss=8.60216 | best_loss=8.47121
Epoch 40/80: current_loss=8.52537 | best_loss=8.47121
Epoch 41/80: current_loss=8.49789 | best_loss=8.47121
Epoch 42/80: current_loss=8.88198 | best_loss=8.47121
Epoch 43/80: current_loss=8.63845 | best_loss=8.47121
Epoch 44/80: current_loss=8.60827 | best_loss=8.47121
Epoch 45/80: current_loss=8.58458 | best_loss=8.47121
Epoch 46/80: current_loss=8.58669 | best_loss=8.47121
Epoch 47/80: current_loss=10.08215 | best_loss=8.47121
Epoch 48/80: current_loss=9.24732 | best_loss=8.47121
Epoch 49/80: current_loss=8.56152 | best_loss=8.47121
Epoch 50/80: current_loss=8.49010 | best_loss=8.47121
Early Stopping at epoch 50
      explained_var=0.00258 | mse_loss=8.55509
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.63442 | best_loss=9.63442
Epoch 1/80: current_loss=9.91919 | best_loss=9.63442
Epoch 2/80: current_loss=9.40179 | best_loss=9.40179
Epoch 3/80: current_loss=9.49539 | best_loss=9.40179
Epoch 4/80: current_loss=10.29405 | best_loss=9.40179
Epoch 5/80: current_loss=10.37589 | best_loss=9.40179
Epoch 6/80: current_loss=9.56734 | best_loss=9.40179
Epoch 7/80: current_loss=9.35296 | best_loss=9.35296
Epoch 8/80: current_loss=9.37740 | best_loss=9.35296
Epoch 9/80: current_loss=9.80812 | best_loss=9.35296
Epoch 10/80: current_loss=10.14412 | best_loss=9.35296
Epoch 11/80: current_loss=10.08208 | best_loss=9.35296
Epoch 12/80: current_loss=9.94444 | best_loss=9.35296
Epoch 13/80: current_loss=9.59313 | best_loss=9.35296
Epoch 14/80: current_loss=9.71900 | best_loss=9.35296
Epoch 15/80: current_loss=9.67233 | best_loss=9.35296
Epoch 16/80: current_loss=9.56475 | best_loss=9.35296
Epoch 17/80: current_loss=10.31027 | best_loss=9.35296
Epoch 18/80: current_loss=10.18804 | best_loss=9.35296
Epoch 19/80: current_loss=11.21553 | best_loss=9.35296
Epoch 20/80: current_loss=13.29638 | best_loss=9.35296
Epoch 21/80: current_loss=10.13981 | best_loss=9.35296
Epoch 22/80: current_loss=9.56003 | best_loss=9.35296
Epoch 23/80: current_loss=9.78596 | best_loss=9.35296
Epoch 24/80: current_loss=9.85926 | best_loss=9.35296
Epoch 25/80: current_loss=9.62985 | best_loss=9.35296
Epoch 26/80: current_loss=9.59542 | best_loss=9.35296
Epoch 27/80: current_loss=9.81756 | best_loss=9.35296
Early Stopping at epoch 27
      explained_var=-0.01308 | mse_loss=9.51047
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.91454 | best_loss=8.91454
Epoch 1/80: current_loss=9.71892 | best_loss=8.91454
Epoch 2/80: current_loss=9.08779 | best_loss=8.91454
Epoch 3/80: current_loss=9.04624 | best_loss=8.91454
Epoch 4/80: current_loss=9.04106 | best_loss=8.91454
Epoch 5/80: current_loss=8.93104 | best_loss=8.91454
Epoch 6/80: current_loss=9.29021 | best_loss=8.91454
Epoch 7/80: current_loss=8.93859 | best_loss=8.91454
Epoch 8/80: current_loss=9.66848 | best_loss=8.91454
Epoch 9/80: current_loss=9.20430 | best_loss=8.91454
Epoch 10/80: current_loss=8.94479 | best_loss=8.91454
Epoch 11/80: current_loss=9.16564 | best_loss=8.91454
Epoch 12/80: current_loss=8.92394 | best_loss=8.91454
Epoch 13/80: current_loss=8.96708 | best_loss=8.91454
Epoch 14/80: current_loss=8.99056 | best_loss=8.91454
Epoch 15/80: current_loss=9.42796 | best_loss=8.91454
Epoch 16/80: current_loss=8.93449 | best_loss=8.91454
Epoch 17/80: current_loss=8.92996 | best_loss=8.91454
Epoch 18/80: current_loss=9.94166 | best_loss=8.91454
Epoch 19/80: current_loss=9.39703 | best_loss=8.91454
Epoch 20/80: current_loss=9.01954 | best_loss=8.91454
Early Stopping at epoch 20
      explained_var=0.00083 | mse_loss=8.35674
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.86578 | best_loss=8.86578
Epoch 1/80: current_loss=8.64971 | best_loss=8.64971
Epoch 2/80: current_loss=8.51916 | best_loss=8.51916
Epoch 3/80: current_loss=8.94379 | best_loss=8.51916
Epoch 4/80: current_loss=9.44446 | best_loss=8.51916
Epoch 5/80: current_loss=9.16412 | best_loss=8.51916
Epoch 6/80: current_loss=8.50297 | best_loss=8.50297
Epoch 7/80: current_loss=8.58275 | best_loss=8.50297
Epoch 8/80: current_loss=8.94699 | best_loss=8.50297
Epoch 9/80: current_loss=8.81175 | best_loss=8.50297
Epoch 10/80: current_loss=8.52863 | best_loss=8.50297
Epoch 11/80: current_loss=8.96106 | best_loss=8.50297
Epoch 12/80: current_loss=8.57337 | best_loss=8.50297
Epoch 13/80: current_loss=8.54240 | best_loss=8.50297
Epoch 14/80: current_loss=8.57990 | best_loss=8.50297
Epoch 15/80: current_loss=9.22508 | best_loss=8.50297
Epoch 16/80: current_loss=10.74886 | best_loss=8.50297
Epoch 17/80: current_loss=8.56157 | best_loss=8.50297
Epoch 18/80: current_loss=8.50879 | best_loss=8.50297
Epoch 19/80: current_loss=9.23379 | best_loss=8.50297
Epoch 20/80: current_loss=8.53522 | best_loss=8.50297
Epoch 21/80: current_loss=8.72574 | best_loss=8.50297
Epoch 22/80: current_loss=8.48159 | best_loss=8.48159
Epoch 23/80: current_loss=8.59215 | best_loss=8.48159
Epoch 24/80: current_loss=8.47728 | best_loss=8.47728
Epoch 25/80: current_loss=9.04491 | best_loss=8.47728
Epoch 26/80: current_loss=8.58216 | best_loss=8.47728
Epoch 27/80: current_loss=8.48231 | best_loss=8.47728
Epoch 28/80: current_loss=8.53359 | best_loss=8.47728
Epoch 29/80: current_loss=8.71289 | best_loss=8.47728
Epoch 30/80: current_loss=8.52032 | best_loss=8.47728
Epoch 31/80: current_loss=8.57883 | best_loss=8.47728
Epoch 32/80: current_loss=8.60624 | best_loss=8.47728
Epoch 33/80: current_loss=8.66999 | best_loss=8.47728
Epoch 34/80: current_loss=8.76933 | best_loss=8.47728
Epoch 35/80: current_loss=9.12966 | best_loss=8.47728
Epoch 36/80: current_loss=8.35378 | best_loss=8.35378
Epoch 37/80: current_loss=9.09527 | best_loss=8.35378
Epoch 38/80: current_loss=8.49472 | best_loss=8.35378
Epoch 39/80: current_loss=9.40105 | best_loss=8.35378
Epoch 40/80: current_loss=8.51319 | best_loss=8.35378
Epoch 41/80: current_loss=8.41234 | best_loss=8.35378
Epoch 42/80: current_loss=8.71038 | best_loss=8.35378
Epoch 43/80: current_loss=8.59500 | best_loss=8.35378
Epoch 44/80: current_loss=8.86390 | best_loss=8.35378
Epoch 45/80: current_loss=8.65032 | best_loss=8.35378
Epoch 46/80: current_loss=8.54014 | best_loss=8.35378
Epoch 47/80: current_loss=8.45003 | best_loss=8.35378
Epoch 48/80: current_loss=8.82507 | best_loss=8.35378
Epoch 49/80: current_loss=8.40146 | best_loss=8.35378
Epoch 50/80: current_loss=8.91042 | best_loss=8.35378
Epoch 51/80: current_loss=8.52490 | best_loss=8.35378
Epoch 52/80: current_loss=8.50654 | best_loss=8.35378
Epoch 53/80: current_loss=8.66117 | best_loss=8.35378
Epoch 54/80: current_loss=9.12333 | best_loss=8.35378
Epoch 55/80: current_loss=8.43623 | best_loss=8.35378
Epoch 56/80: current_loss=8.43775 | best_loss=8.35378
Early Stopping at epoch 56
      explained_var=0.02188 | mse_loss=8.02101
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.00287| avg_loss=8.80025
----------------------------------------------


----------------------------------------------
Params for Trial 79
{'learning_rate': 0.01, 'weight_decay': 0.004935628009738306, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.09450 | best_loss=10.09450
Epoch 1/80: current_loss=9.93587 | best_loss=9.93587
Epoch 2/80: current_loss=9.92703 | best_loss=9.92703
Epoch 3/80: current_loss=9.93039 | best_loss=9.92703
Epoch 4/80: current_loss=9.92464 | best_loss=9.92464
Epoch 5/80: current_loss=9.94002 | best_loss=9.92464
Epoch 6/80: current_loss=9.89287 | best_loss=9.89287
Epoch 7/80: current_loss=9.91942 | best_loss=9.89287
Epoch 8/80: current_loss=9.89517 | best_loss=9.89287
Epoch 9/80: current_loss=9.95444 | best_loss=9.89287
Epoch 10/80: current_loss=9.89863 | best_loss=9.89287
Epoch 11/80: current_loss=9.89992 | best_loss=9.89287
Epoch 12/80: current_loss=9.94918 | best_loss=9.89287
Epoch 13/80: current_loss=9.90085 | best_loss=9.89287
Epoch 14/80: current_loss=9.96322 | best_loss=9.89287
Epoch 15/80: current_loss=10.09895 | best_loss=9.89287
Epoch 16/80: current_loss=9.95347 | best_loss=9.89287
Epoch 17/80: current_loss=9.92155 | best_loss=9.89287
Epoch 18/80: current_loss=9.92207 | best_loss=9.89287
Epoch 19/80: current_loss=10.34962 | best_loss=9.89287
Epoch 20/80: current_loss=9.90186 | best_loss=9.89287
Epoch 21/80: current_loss=9.93418 | best_loss=9.89287
Epoch 22/80: current_loss=9.95863 | best_loss=9.89287
Epoch 23/80: current_loss=10.37286 | best_loss=9.89287
Epoch 24/80: current_loss=9.91039 | best_loss=9.89287
Epoch 25/80: current_loss=9.95809 | best_loss=9.89287
Epoch 26/80: current_loss=10.11003 | best_loss=9.89287
Early Stopping at epoch 26
      explained_var=0.00253 | mse_loss=9.56811
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.55554 | best_loss=8.55554
Epoch 1/80: current_loss=8.51568 | best_loss=8.51568
Epoch 2/80: current_loss=8.65350 | best_loss=8.51568
Epoch 3/80: current_loss=8.58904 | best_loss=8.51568
Epoch 4/80: current_loss=8.52881 | best_loss=8.51568
Epoch 5/80: current_loss=8.53135 | best_loss=8.51568
Epoch 6/80: current_loss=8.56256 | best_loss=8.51568
Epoch 7/80: current_loss=8.49508 | best_loss=8.49508
Epoch 8/80: current_loss=8.49218 | best_loss=8.49218
Epoch 9/80: current_loss=8.52716 | best_loss=8.49218
Epoch 10/80: current_loss=8.60429 | best_loss=8.49218
Epoch 11/80: current_loss=8.68927 | best_loss=8.49218
Epoch 12/80: current_loss=8.76263 | best_loss=8.49218
Epoch 13/80: current_loss=8.90006 | best_loss=8.49218
Epoch 14/80: current_loss=8.49013 | best_loss=8.49013
Epoch 15/80: current_loss=8.47668 | best_loss=8.47668
Epoch 16/80: current_loss=9.06650 | best_loss=8.47668
Epoch 17/80: current_loss=8.49203 | best_loss=8.47668
Epoch 18/80: current_loss=8.50425 | best_loss=8.47668
Epoch 19/80: current_loss=8.58242 | best_loss=8.47668
Epoch 20/80: current_loss=8.51273 | best_loss=8.47668
Epoch 21/80: current_loss=8.51690 | best_loss=8.47668
Epoch 22/80: current_loss=8.53133 | best_loss=8.47668
Epoch 23/80: current_loss=8.49975 | best_loss=8.47668
Epoch 24/80: current_loss=8.50244 | best_loss=8.47668
Epoch 25/80: current_loss=8.65197 | best_loss=8.47668
Epoch 26/80: current_loss=8.48143 | best_loss=8.47668
Epoch 27/80: current_loss=8.52271 | best_loss=8.47668
Epoch 28/80: current_loss=8.53722 | best_loss=8.47668
Epoch 29/80: current_loss=8.68907 | best_loss=8.47668
Epoch 30/80: current_loss=8.55922 | best_loss=8.47668
Epoch 31/80: current_loss=8.67319 | best_loss=8.47668
Epoch 32/80: current_loss=8.48217 | best_loss=8.47668
Epoch 33/80: current_loss=8.62248 | best_loss=8.47668
Epoch 34/80: current_loss=8.53004 | best_loss=8.47668
Epoch 35/80: current_loss=8.51121 | best_loss=8.47668
Early Stopping at epoch 35
      explained_var=0.00190 | mse_loss=8.55995
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.67434 | best_loss=9.67434
Epoch 1/80: current_loss=9.27775 | best_loss=9.27775
Epoch 2/80: current_loss=9.31150 | best_loss=9.27775
Epoch 3/80: current_loss=9.31160 | best_loss=9.27775
Epoch 4/80: current_loss=9.40839 | best_loss=9.27775
Epoch 5/80: current_loss=9.79678 | best_loss=9.27775
Epoch 6/80: current_loss=9.43595 | best_loss=9.27775
Epoch 7/80: current_loss=9.48132 | best_loss=9.27775
Epoch 8/80: current_loss=9.44070 | best_loss=9.27775
Epoch 9/80: current_loss=9.35238 | best_loss=9.27775
Epoch 10/80: current_loss=9.38696 | best_loss=9.27775
Epoch 11/80: current_loss=9.27504 | best_loss=9.27504
Epoch 12/80: current_loss=9.33765 | best_loss=9.27504
Epoch 13/80: current_loss=9.32041 | best_loss=9.27504
Epoch 14/80: current_loss=9.83606 | best_loss=9.27504
Epoch 15/80: current_loss=9.91492 | best_loss=9.27504
Epoch 16/80: current_loss=9.34759 | best_loss=9.27504
Epoch 17/80: current_loss=9.37375 | best_loss=9.27504
Epoch 18/80: current_loss=9.31218 | best_loss=9.27504
Epoch 19/80: current_loss=10.08924 | best_loss=9.27504
Epoch 20/80: current_loss=9.31741 | best_loss=9.27504
Epoch 21/80: current_loss=9.31740 | best_loss=9.27504
Epoch 22/80: current_loss=9.33205 | best_loss=9.27504
Epoch 23/80: current_loss=9.41274 | best_loss=9.27504
Epoch 24/80: current_loss=9.45555 | best_loss=9.27504
Epoch 25/80: current_loss=9.36147 | best_loss=9.27504
Epoch 26/80: current_loss=9.49768 | best_loss=9.27504
Epoch 27/80: current_loss=9.32313 | best_loss=9.27504
Epoch 28/80: current_loss=9.60015 | best_loss=9.27504
Epoch 29/80: current_loss=9.59707 | best_loss=9.27504
Epoch 30/80: current_loss=9.54119 | best_loss=9.27504
Epoch 31/80: current_loss=9.34188 | best_loss=9.27504
Early Stopping at epoch 31
      explained_var=-0.00384 | mse_loss=9.41463
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.92755 | best_loss=8.92755
Epoch 1/80: current_loss=9.04592 | best_loss=8.92755
Epoch 2/80: current_loss=8.94278 | best_loss=8.92755
Epoch 3/80: current_loss=8.95131 | best_loss=8.92755
Epoch 4/80: current_loss=9.38707 | best_loss=8.92755
Epoch 5/80: current_loss=8.98204 | best_loss=8.92755
Epoch 6/80: current_loss=8.97655 | best_loss=8.92755
Epoch 7/80: current_loss=8.94326 | best_loss=8.92755
Epoch 8/80: current_loss=8.96157 | best_loss=8.92755
Epoch 9/80: current_loss=8.97053 | best_loss=8.92755
Epoch 10/80: current_loss=8.96598 | best_loss=8.92755
Epoch 11/80: current_loss=8.95269 | best_loss=8.92755
Epoch 12/80: current_loss=8.99462 | best_loss=8.92755
Epoch 13/80: current_loss=9.02307 | best_loss=8.92755
Epoch 14/80: current_loss=8.95907 | best_loss=8.92755
Epoch 15/80: current_loss=9.05580 | best_loss=8.92755
Epoch 16/80: current_loss=8.99443 | best_loss=8.92755
Epoch 17/80: current_loss=8.93233 | best_loss=8.92755
Epoch 18/80: current_loss=8.94169 | best_loss=8.92755
Epoch 19/80: current_loss=9.05640 | best_loss=8.92755
Epoch 20/80: current_loss=8.99854 | best_loss=8.92755
Early Stopping at epoch 20
      explained_var=0.00050 | mse_loss=8.38799

----------------------------------------------
Params for Trial 80
{'learning_rate': 0.01, 'weight_decay': 0.0003322913750856953, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.40251 | best_loss=10.40251
Epoch 1/80: current_loss=10.01948 | best_loss=10.01948
Epoch 2/80: current_loss=9.91304 | best_loss=9.91304
Epoch 3/80: current_loss=10.13367 | best_loss=9.91304
Epoch 4/80: current_loss=9.94673 | best_loss=9.91304
Epoch 5/80: current_loss=9.91041 | best_loss=9.91041
Epoch 6/80: current_loss=9.93619 | best_loss=9.91041
Epoch 7/80: current_loss=9.98540 | best_loss=9.91041
Epoch 8/80: current_loss=9.90729 | best_loss=9.90729
Epoch 9/80: current_loss=11.04378 | best_loss=9.90729
Epoch 10/80: current_loss=9.90850 | best_loss=9.90729
Epoch 11/80: current_loss=10.09809 | best_loss=9.90729
Epoch 12/80: current_loss=10.11298 | best_loss=9.90729
Epoch 13/80: current_loss=10.00559 | best_loss=9.90729
Epoch 14/80: current_loss=10.04758 | best_loss=9.90729
Epoch 15/80: current_loss=10.00504 | best_loss=9.90729
Epoch 16/80: current_loss=10.09907 | best_loss=9.90729
Epoch 17/80: current_loss=9.90921 | best_loss=9.90729
Epoch 18/80: current_loss=10.19538 | best_loss=9.90729
Epoch 19/80: current_loss=10.87130 | best_loss=9.90729
Epoch 20/80: current_loss=10.33999 | best_loss=9.90729
Epoch 21/80: current_loss=10.43027 | best_loss=9.90729
Epoch 22/80: current_loss=9.90754 | best_loss=9.90729
Epoch 23/80: current_loss=10.56533 | best_loss=9.90729
Epoch 24/80: current_loss=10.67833 | best_loss=9.90729
Epoch 25/80: current_loss=9.95095 | best_loss=9.90729
Epoch 26/80: current_loss=9.91385 | best_loss=9.90729
Epoch 27/80: current_loss=9.96318 | best_loss=9.90729
Epoch 28/80: current_loss=10.67810 | best_loss=9.90729
Early Stopping at epoch 28
      explained_var=0.00004 | mse_loss=9.57730

----------------------------------------------
Params for Trial 81
{'learning_rate': 0.01, 'weight_decay': 0.002801919826550663, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.00978 | best_loss=10.00978
Epoch 1/80: current_loss=9.97655 | best_loss=9.97655
Epoch 2/80: current_loss=10.24296 | best_loss=9.97655
Epoch 3/80: current_loss=10.14808 | best_loss=9.97655
Epoch 4/80: current_loss=9.91514 | best_loss=9.91514
Epoch 5/80: current_loss=10.01678 | best_loss=9.91514
Epoch 6/80: current_loss=10.00799 | best_loss=9.91514
Epoch 7/80: current_loss=10.28955 | best_loss=9.91514
Epoch 8/80: current_loss=9.94493 | best_loss=9.91514
Epoch 9/80: current_loss=10.03979 | best_loss=9.91514
Epoch 10/80: current_loss=10.31157 | best_loss=9.91514
Epoch 11/80: current_loss=9.99342 | best_loss=9.91514
Epoch 12/80: current_loss=9.97507 | best_loss=9.91514
Epoch 13/80: current_loss=9.92506 | best_loss=9.91514
Epoch 14/80: current_loss=9.96784 | best_loss=9.91514
Epoch 15/80: current_loss=10.34751 | best_loss=9.91514
Epoch 16/80: current_loss=10.64698 | best_loss=9.91514
Epoch 17/80: current_loss=10.16032 | best_loss=9.91514
Epoch 18/80: current_loss=10.01190 | best_loss=9.91514
Epoch 19/80: current_loss=10.46379 | best_loss=9.91514
Epoch 20/80: current_loss=9.94668 | best_loss=9.91514
Epoch 21/80: current_loss=10.02684 | best_loss=9.91514
Epoch 22/80: current_loss=10.25488 | best_loss=9.91514
Epoch 23/80: current_loss=9.95189 | best_loss=9.91514
Epoch 24/80: current_loss=9.91296 | best_loss=9.91296
Epoch 25/80: current_loss=9.96669 | best_loss=9.91296
Epoch 26/80: current_loss=10.79561 | best_loss=9.91296
Epoch 27/80: current_loss=9.99296 | best_loss=9.91296
Epoch 28/80: current_loss=9.90372 | best_loss=9.90372
Epoch 29/80: current_loss=9.92741 | best_loss=9.90372
Epoch 30/80: current_loss=9.90888 | best_loss=9.90372
Epoch 31/80: current_loss=10.19856 | best_loss=9.90372
Epoch 32/80: current_loss=10.60239 | best_loss=9.90372
Epoch 33/80: current_loss=9.97230 | best_loss=9.90372
Epoch 34/80: current_loss=10.35681 | best_loss=9.90372
Epoch 35/80: current_loss=9.90806 | best_loss=9.90372
Epoch 36/80: current_loss=10.62072 | best_loss=9.90372
Epoch 37/80: current_loss=10.20163 | best_loss=9.90372
Epoch 38/80: current_loss=9.88876 | best_loss=9.88876
Epoch 39/80: current_loss=9.91828 | best_loss=9.88876
Epoch 40/80: current_loss=9.99811 | best_loss=9.88876
Epoch 41/80: current_loss=10.32334 | best_loss=9.88876
Epoch 42/80: current_loss=9.91181 | best_loss=9.88876
Epoch 43/80: current_loss=10.01439 | best_loss=9.88876
Epoch 44/80: current_loss=9.96346 | best_loss=9.88876
Epoch 45/80: current_loss=9.95109 | best_loss=9.88876
Epoch 46/80: current_loss=9.90454 | best_loss=9.88876
Epoch 47/80: current_loss=9.90525 | best_loss=9.88876
Epoch 48/80: current_loss=10.07780 | best_loss=9.88876
Epoch 49/80: current_loss=10.04393 | best_loss=9.88876
Epoch 50/80: current_loss=9.94775 | best_loss=9.88876
Epoch 51/80: current_loss=10.23446 | best_loss=9.88876
Epoch 52/80: current_loss=9.94617 | best_loss=9.88876
Epoch 53/80: current_loss=10.53591 | best_loss=9.88876
Epoch 54/80: current_loss=10.00558 | best_loss=9.88876
Epoch 55/80: current_loss=9.89930 | best_loss=9.88876
Epoch 56/80: current_loss=10.13053 | best_loss=9.88876
Epoch 57/80: current_loss=10.16718 | best_loss=9.88876
Epoch 58/80: current_loss=10.06162 | best_loss=9.88876
Early Stopping at epoch 58
      explained_var=0.00235 | mse_loss=9.55486
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.10967 | best_loss=9.10967
Epoch 1/80: current_loss=8.48680 | best_loss=8.48680
Epoch 2/80: current_loss=8.52342 | best_loss=8.48680
Epoch 3/80: current_loss=8.48569 | best_loss=8.48569
Epoch 4/80: current_loss=8.45893 | best_loss=8.45893
Epoch 5/80: current_loss=8.70476 | best_loss=8.45893
Epoch 6/80: current_loss=8.53199 | best_loss=8.45893
Epoch 7/80: current_loss=8.57439 | best_loss=8.45893
Epoch 8/80: current_loss=9.17944 | best_loss=8.45893
Epoch 9/80: current_loss=8.97229 | best_loss=8.45893
Epoch 10/80: current_loss=8.58889 | best_loss=8.45893
Epoch 11/80: current_loss=8.59731 | best_loss=8.45893
Epoch 12/80: current_loss=8.57092 | best_loss=8.45893
Epoch 13/80: current_loss=9.39159 | best_loss=8.45893
Epoch 14/80: current_loss=9.20862 | best_loss=8.45893
Epoch 15/80: current_loss=8.59908 | best_loss=8.45893
Epoch 16/80: current_loss=8.48824 | best_loss=8.45893
Epoch 17/80: current_loss=8.77216 | best_loss=8.45893
Epoch 18/80: current_loss=8.51536 | best_loss=8.45893
Epoch 19/80: current_loss=8.57506 | best_loss=8.45893
Epoch 20/80: current_loss=8.51705 | best_loss=8.45893
Epoch 21/80: current_loss=8.73551 | best_loss=8.45893
Epoch 22/80: current_loss=9.17374 | best_loss=8.45893
Epoch 23/80: current_loss=8.57531 | best_loss=8.45893
Epoch 24/80: current_loss=8.51355 | best_loss=8.45893
Early Stopping at epoch 24
      explained_var=0.00743 | mse_loss=8.54440
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91987 | best_loss=9.91987
Epoch 1/80: current_loss=9.27338 | best_loss=9.27338
Epoch 2/80: current_loss=9.30757 | best_loss=9.27338
Epoch 3/80: current_loss=9.41819 | best_loss=9.27338
Epoch 4/80: current_loss=9.64617 | best_loss=9.27338
Epoch 5/80: current_loss=10.03570 | best_loss=9.27338
Epoch 6/80: current_loss=9.33585 | best_loss=9.27338
Epoch 7/80: current_loss=9.26444 | best_loss=9.26444
Epoch 8/80: current_loss=9.35960 | best_loss=9.26444
Epoch 9/80: current_loss=9.35929 | best_loss=9.26444
Epoch 10/80: current_loss=9.74410 | best_loss=9.26444
Epoch 11/80: current_loss=9.27297 | best_loss=9.26444
Epoch 12/80: current_loss=9.58516 | best_loss=9.26444
Epoch 13/80: current_loss=9.43043 | best_loss=9.26444
Epoch 14/80: current_loss=9.80808 | best_loss=9.26444
Epoch 15/80: current_loss=10.00834 | best_loss=9.26444
Epoch 16/80: current_loss=9.34484 | best_loss=9.26444
Epoch 17/80: current_loss=9.59190 | best_loss=9.26444
Epoch 18/80: current_loss=10.55097 | best_loss=9.26444
Epoch 19/80: current_loss=10.22401 | best_loss=9.26444
Epoch 20/80: current_loss=9.65930 | best_loss=9.26444
Epoch 21/80: current_loss=9.87364 | best_loss=9.26444
Epoch 22/80: current_loss=9.44576 | best_loss=9.26444
Epoch 23/80: current_loss=9.28855 | best_loss=9.26444
Epoch 24/80: current_loss=9.33465 | best_loss=9.26444
Epoch 25/80: current_loss=9.34513 | best_loss=9.26444
Epoch 26/80: current_loss=9.38701 | best_loss=9.26444
Epoch 27/80: current_loss=9.35102 | best_loss=9.26444
Early Stopping at epoch 27
      explained_var=-0.00081 | mse_loss=9.39281
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.25203 | best_loss=9.25203
Epoch 1/80: current_loss=8.90903 | best_loss=8.90903
Epoch 2/80: current_loss=8.93302 | best_loss=8.90903
Epoch 3/80: current_loss=9.24540 | best_loss=8.90903
Epoch 4/80: current_loss=9.10143 | best_loss=8.90903
Epoch 5/80: current_loss=9.27282 | best_loss=8.90903
Epoch 6/80: current_loss=9.03241 | best_loss=8.90903
Epoch 7/80: current_loss=9.17993 | best_loss=8.90903
Epoch 8/80: current_loss=9.01383 | best_loss=8.90903
Epoch 9/80: current_loss=9.24080 | best_loss=8.90903
Epoch 10/80: current_loss=9.11897 | best_loss=8.90903
Epoch 11/80: current_loss=9.18690 | best_loss=8.90903
Epoch 12/80: current_loss=9.25295 | best_loss=8.90903
Epoch 13/80: current_loss=9.12870 | best_loss=8.90903
Epoch 14/80: current_loss=9.24734 | best_loss=8.90903
Epoch 15/80: current_loss=8.92435 | best_loss=8.90903
Epoch 16/80: current_loss=10.01040 | best_loss=8.90903
Epoch 17/80: current_loss=9.37640 | best_loss=8.90903
Epoch 18/80: current_loss=9.25246 | best_loss=8.90903
Epoch 19/80: current_loss=10.42180 | best_loss=8.90903
Epoch 20/80: current_loss=8.90633 | best_loss=8.90633
Epoch 21/80: current_loss=8.99215 | best_loss=8.90633
Epoch 22/80: current_loss=9.04578 | best_loss=8.90633
Epoch 23/80: current_loss=9.18073 | best_loss=8.90633
Epoch 24/80: current_loss=9.17755 | best_loss=8.90633
Epoch 25/80: current_loss=9.22279 | best_loss=8.90633
Epoch 26/80: current_loss=9.90593 | best_loss=8.90633
Epoch 27/80: current_loss=9.37596 | best_loss=8.90633
Epoch 28/80: current_loss=9.21243 | best_loss=8.90633
Epoch 29/80: current_loss=9.80238 | best_loss=8.90633
Epoch 30/80: current_loss=9.36019 | best_loss=8.90633
Epoch 31/80: current_loss=9.38718 | best_loss=8.90633
Epoch 32/80: current_loss=9.29082 | best_loss=8.90633
Epoch 33/80: current_loss=9.01464 | best_loss=8.90633
Epoch 34/80: current_loss=9.43843 | best_loss=8.90633
Epoch 35/80: current_loss=9.14209 | best_loss=8.90633
Epoch 36/80: current_loss=9.44005 | best_loss=8.90633
Epoch 37/80: current_loss=9.00294 | best_loss=8.90633
Epoch 38/80: current_loss=9.20446 | best_loss=8.90633
Epoch 39/80: current_loss=9.71053 | best_loss=8.90633
Epoch 40/80: current_loss=9.00879 | best_loss=8.90633
Early Stopping at epoch 40
      explained_var=0.00486 | mse_loss=8.33609
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.87256 | best_loss=8.87256
Epoch 1/80: current_loss=8.54241 | best_loss=8.54241
Epoch 2/80: current_loss=8.80636 | best_loss=8.54241
Epoch 3/80: current_loss=9.24588 | best_loss=8.54241
Epoch 4/80: current_loss=8.57555 | best_loss=8.54241
Epoch 5/80: current_loss=8.56558 | best_loss=8.54241
Epoch 6/80: current_loss=8.53973 | best_loss=8.53973
Epoch 7/80: current_loss=8.82838 | best_loss=8.53973
Epoch 8/80: current_loss=8.51270 | best_loss=8.51270
Epoch 9/80: current_loss=8.50507 | best_loss=8.50507
Epoch 10/80: current_loss=8.48596 | best_loss=8.48596
Epoch 11/80: current_loss=8.48820 | best_loss=8.48596
Epoch 12/80: current_loss=8.76628 | best_loss=8.48596
Epoch 13/80: current_loss=9.76019 | best_loss=8.48596
Epoch 14/80: current_loss=8.96830 | best_loss=8.48596
Epoch 15/80: current_loss=8.92636 | best_loss=8.48596
Epoch 16/80: current_loss=8.55455 | best_loss=8.48596
Epoch 17/80: current_loss=8.67475 | best_loss=8.48596
Epoch 18/80: current_loss=8.96849 | best_loss=8.48596
Epoch 19/80: current_loss=8.78906 | best_loss=8.48596
Epoch 20/80: current_loss=8.65485 | best_loss=8.48596
Epoch 21/80: current_loss=8.48342 | best_loss=8.48342
Epoch 22/80: current_loss=8.49169 | best_loss=8.48342
Epoch 23/80: current_loss=8.65996 | best_loss=8.48342
Epoch 24/80: current_loss=8.76197 | best_loss=8.48342
Epoch 25/80: current_loss=8.49934 | best_loss=8.48342
Epoch 26/80: current_loss=8.48612 | best_loss=8.48342
Epoch 27/80: current_loss=8.60312 | best_loss=8.48342
Epoch 28/80: current_loss=8.66628 | best_loss=8.48342
Epoch 29/80: current_loss=8.52488 | best_loss=8.48342
Epoch 30/80: current_loss=8.59814 | best_loss=8.48342
Epoch 31/80: current_loss=8.64654 | best_loss=8.48342
Epoch 32/80: current_loss=8.49753 | best_loss=8.48342
Epoch 33/80: current_loss=9.04088 | best_loss=8.48342
Epoch 34/80: current_loss=8.76586 | best_loss=8.48342
Epoch 35/80: current_loss=8.75704 | best_loss=8.48342
Epoch 36/80: current_loss=8.48860 | best_loss=8.48342
Epoch 37/80: current_loss=8.53910 | best_loss=8.48342
Epoch 38/80: current_loss=8.57986 | best_loss=8.48342
Epoch 39/80: current_loss=8.55285 | best_loss=8.48342
Epoch 40/80: current_loss=8.58746 | best_loss=8.48342
Epoch 41/80: current_loss=8.57888 | best_loss=8.48342
Early Stopping at epoch 41
      explained_var=0.00039 | mse_loss=8.19955
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00284| avg_loss=8.80554
----------------------------------------------


----------------------------------------------
Params for Trial 82
{'learning_rate': 0.01, 'weight_decay': 0.0032449390034749955, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.99780 | best_loss=9.99780
Epoch 1/80: current_loss=10.19125 | best_loss=9.99780
Epoch 2/80: current_loss=10.00164 | best_loss=9.99780
Epoch 3/80: current_loss=9.91435 | best_loss=9.91435
Epoch 4/80: current_loss=10.20360 | best_loss=9.91435
Epoch 5/80: current_loss=10.14924 | best_loss=9.91435
Epoch 6/80: current_loss=9.94126 | best_loss=9.91435
Epoch 7/80: current_loss=10.28108 | best_loss=9.91435
Epoch 8/80: current_loss=10.35357 | best_loss=9.91435
Epoch 9/80: current_loss=9.96254 | best_loss=9.91435
Epoch 10/80: current_loss=9.97825 | best_loss=9.91435
Epoch 11/80: current_loss=9.93084 | best_loss=9.91435
Epoch 12/80: current_loss=10.19289 | best_loss=9.91435
Epoch 13/80: current_loss=9.90271 | best_loss=9.90271
Epoch 14/80: current_loss=9.90854 | best_loss=9.90271
Epoch 15/80: current_loss=9.93824 | best_loss=9.90271
Epoch 16/80: current_loss=10.14691 | best_loss=9.90271
Epoch 17/80: current_loss=10.27869 | best_loss=9.90271
Epoch 18/80: current_loss=10.25665 | best_loss=9.90271
Epoch 19/80: current_loss=10.02196 | best_loss=9.90271
Epoch 20/80: current_loss=10.00542 | best_loss=9.90271
Epoch 21/80: current_loss=10.75727 | best_loss=9.90271
Epoch 22/80: current_loss=11.85702 | best_loss=9.90271
Epoch 23/80: current_loss=9.98948 | best_loss=9.90271
Epoch 24/80: current_loss=9.94925 | best_loss=9.90271
Epoch 25/80: current_loss=9.91728 | best_loss=9.90271
Epoch 26/80: current_loss=10.25362 | best_loss=9.90271
Epoch 27/80: current_loss=9.98847 | best_loss=9.90271
Epoch 28/80: current_loss=10.37446 | best_loss=9.90271
Epoch 29/80: current_loss=9.94721 | best_loss=9.90271
Epoch 30/80: current_loss=10.05916 | best_loss=9.90271
Epoch 31/80: current_loss=10.20840 | best_loss=9.90271
Epoch 32/80: current_loss=10.58013 | best_loss=9.90271
Epoch 33/80: current_loss=10.60590 | best_loss=9.90271
Early Stopping at epoch 33
      explained_var=0.00088 | mse_loss=9.56834
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.51361 | best_loss=9.51361
Epoch 1/80: current_loss=8.53859 | best_loss=8.53859
Epoch 2/80: current_loss=8.63923 | best_loss=8.53859
Epoch 3/80: current_loss=8.48401 | best_loss=8.48401
Epoch 4/80: current_loss=8.54798 | best_loss=8.48401
Epoch 5/80: current_loss=8.53025 | best_loss=8.48401
Epoch 6/80: current_loss=9.19440 | best_loss=8.48401
Epoch 7/80: current_loss=9.27702 | best_loss=8.48401
Epoch 8/80: current_loss=8.66754 | best_loss=8.48401
Epoch 9/80: current_loss=8.68516 | best_loss=8.48401
Epoch 10/80: current_loss=9.02994 | best_loss=8.48401
Epoch 11/80: current_loss=9.45869 | best_loss=8.48401
Epoch 12/80: current_loss=8.59065 | best_loss=8.48401
Epoch 13/80: current_loss=8.47926 | best_loss=8.47926
Epoch 14/80: current_loss=8.53801 | best_loss=8.47926
Epoch 15/80: current_loss=9.69968 | best_loss=8.47926
Epoch 16/80: current_loss=9.43862 | best_loss=8.47926
Epoch 17/80: current_loss=8.67619 | best_loss=8.47926
Epoch 18/80: current_loss=8.52670 | best_loss=8.47926
Epoch 19/80: current_loss=9.40654 | best_loss=8.47926
Epoch 20/80: current_loss=8.55494 | best_loss=8.47926
Epoch 21/80: current_loss=8.50599 | best_loss=8.47926
Epoch 22/80: current_loss=8.83597 | best_loss=8.47926
Epoch 23/80: current_loss=8.82218 | best_loss=8.47926
Epoch 24/80: current_loss=8.63265 | best_loss=8.47926
Epoch 25/80: current_loss=8.67363 | best_loss=8.47926
Epoch 26/80: current_loss=8.53167 | best_loss=8.47926
Epoch 27/80: current_loss=8.69991 | best_loss=8.47926
Epoch 28/80: current_loss=8.55677 | best_loss=8.47926
Epoch 29/80: current_loss=8.80939 | best_loss=8.47926
Epoch 30/80: current_loss=9.73889 | best_loss=8.47926
Epoch 31/80: current_loss=8.49099 | best_loss=8.47926
Epoch 32/80: current_loss=8.79343 | best_loss=8.47926
Epoch 33/80: current_loss=9.74817 | best_loss=8.47926
Early Stopping at epoch 33
      explained_var=0.00132 | mse_loss=8.59768

----------------------------------------------
Params for Trial 83
{'learning_rate': 0.01, 'weight_decay': 0.0017690104655055, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.00528 | best_loss=10.00528
Epoch 1/80: current_loss=10.17679 | best_loss=10.00528
Epoch 2/80: current_loss=10.20168 | best_loss=10.00528
Epoch 3/80: current_loss=10.16174 | best_loss=10.00528
Epoch 4/80: current_loss=9.89443 | best_loss=9.89443
Epoch 5/80: current_loss=9.93684 | best_loss=9.89443
Epoch 6/80: current_loss=10.44535 | best_loss=9.89443
Epoch 7/80: current_loss=10.07750 | best_loss=9.89443
Epoch 8/80: current_loss=10.41554 | best_loss=9.89443
Epoch 9/80: current_loss=9.96019 | best_loss=9.89443
Epoch 10/80: current_loss=9.95051 | best_loss=9.89443
Epoch 11/80: current_loss=9.89479 | best_loss=9.89443
Epoch 12/80: current_loss=9.90661 | best_loss=9.89443
Epoch 13/80: current_loss=9.92865 | best_loss=9.89443
Epoch 14/80: current_loss=10.20764 | best_loss=9.89443
Epoch 15/80: current_loss=10.64106 | best_loss=9.89443
Epoch 16/80: current_loss=10.10502 | best_loss=9.89443
Epoch 17/80: current_loss=9.94528 | best_loss=9.89443
Epoch 18/80: current_loss=10.75316 | best_loss=9.89443
Epoch 19/80: current_loss=9.95971 | best_loss=9.89443
Epoch 20/80: current_loss=9.90989 | best_loss=9.89443
Epoch 21/80: current_loss=9.91852 | best_loss=9.89443
Epoch 22/80: current_loss=9.93767 | best_loss=9.89443
Epoch 23/80: current_loss=9.94729 | best_loss=9.89443
Epoch 24/80: current_loss=10.08209 | best_loss=9.89443
Early Stopping at epoch 24
      explained_var=0.00158 | mse_loss=9.56650
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.62063 | best_loss=8.62063
Epoch 1/80: current_loss=8.76712 | best_loss=8.62063
Epoch 2/80: current_loss=9.10143 | best_loss=8.62063
Epoch 3/80: current_loss=8.67631 | best_loss=8.62063
Epoch 4/80: current_loss=8.68955 | best_loss=8.62063
Epoch 5/80: current_loss=9.40260 | best_loss=8.62063
Epoch 6/80: current_loss=8.78267 | best_loss=8.62063
Epoch 7/80: current_loss=8.58876 | best_loss=8.58876
Epoch 8/80: current_loss=8.48445 | best_loss=8.48445
Epoch 9/80: current_loss=8.37489 | best_loss=8.37489
Epoch 10/80: current_loss=8.60342 | best_loss=8.37489
Epoch 11/80: current_loss=8.45196 | best_loss=8.37489
Epoch 12/80: current_loss=8.59490 | best_loss=8.37489
Epoch 13/80: current_loss=8.62554 | best_loss=8.37489
Epoch 14/80: current_loss=8.55689 | best_loss=8.37489
Epoch 15/80: current_loss=8.49620 | best_loss=8.37489
Epoch 16/80: current_loss=8.54656 | best_loss=8.37489
Epoch 17/80: current_loss=8.48785 | best_loss=8.37489
Epoch 18/80: current_loss=8.84451 | best_loss=8.37489
Epoch 19/80: current_loss=8.62823 | best_loss=8.37489
Epoch 20/80: current_loss=8.57100 | best_loss=8.37489
Epoch 21/80: current_loss=8.86543 | best_loss=8.37489
Epoch 22/80: current_loss=8.58022 | best_loss=8.37489
Epoch 23/80: current_loss=8.73266 | best_loss=8.37489
Epoch 24/80: current_loss=8.59987 | best_loss=8.37489
Epoch 25/80: current_loss=9.71857 | best_loss=8.37489
Epoch 26/80: current_loss=8.49457 | best_loss=8.37489
Epoch 27/80: current_loss=9.21085 | best_loss=8.37489
Epoch 28/80: current_loss=8.55662 | best_loss=8.37489
Epoch 29/80: current_loss=8.69653 | best_loss=8.37489
Early Stopping at epoch 29
      explained_var=0.01012 | mse_loss=8.48967
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.47087 | best_loss=9.47087
Epoch 1/80: current_loss=9.26403 | best_loss=9.26403
Epoch 2/80: current_loss=10.00138 | best_loss=9.26403
Epoch 3/80: current_loss=10.98200 | best_loss=9.26403
Epoch 4/80: current_loss=10.29916 | best_loss=9.26403
Epoch 5/80: current_loss=9.77972 | best_loss=9.26403
Epoch 6/80: current_loss=9.42069 | best_loss=9.26403
Epoch 7/80: current_loss=9.40451 | best_loss=9.26403
Epoch 8/80: current_loss=9.29959 | best_loss=9.26403
Epoch 9/80: current_loss=10.54910 | best_loss=9.26403
Epoch 10/80: current_loss=10.58097 | best_loss=9.26403
Epoch 11/80: current_loss=9.70624 | best_loss=9.26403
Epoch 12/80: current_loss=9.87853 | best_loss=9.26403
Epoch 13/80: current_loss=9.36051 | best_loss=9.26403
Epoch 14/80: current_loss=9.29674 | best_loss=9.26403
Epoch 15/80: current_loss=9.29132 | best_loss=9.26403
Epoch 16/80: current_loss=15.16285 | best_loss=9.26403
Epoch 17/80: current_loss=9.83674 | best_loss=9.26403
Epoch 18/80: current_loss=9.34024 | best_loss=9.26403
Epoch 19/80: current_loss=9.28379 | best_loss=9.26403
Epoch 20/80: current_loss=9.31101 | best_loss=9.26403
Epoch 21/80: current_loss=9.68443 | best_loss=9.26403
Early Stopping at epoch 21
      explained_var=-0.00263 | mse_loss=9.41333
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.04535 | best_loss=9.04535
Epoch 1/80: current_loss=8.96233 | best_loss=8.96233
Epoch 2/80: current_loss=10.24470 | best_loss=8.96233
Epoch 3/80: current_loss=9.14001 | best_loss=8.96233
Epoch 4/80: current_loss=9.18336 | best_loss=8.96233
Epoch 5/80: current_loss=9.02202 | best_loss=8.96233
Epoch 6/80: current_loss=8.95458 | best_loss=8.95458
Epoch 7/80: current_loss=8.94490 | best_loss=8.94490
Epoch 8/80: current_loss=9.75600 | best_loss=8.94490
Epoch 9/80: current_loss=9.33810 | best_loss=8.94490
Epoch 10/80: current_loss=8.92244 | best_loss=8.92244
Epoch 11/80: current_loss=9.52369 | best_loss=8.92244
Epoch 12/80: current_loss=9.06366 | best_loss=8.92244
Epoch 13/80: current_loss=10.16666 | best_loss=8.92244
Epoch 14/80: current_loss=9.18743 | best_loss=8.92244
Epoch 15/80: current_loss=8.94477 | best_loss=8.92244
Epoch 16/80: current_loss=8.96196 | best_loss=8.92244
Epoch 17/80: current_loss=9.03742 | best_loss=8.92244
Epoch 18/80: current_loss=9.30766 | best_loss=8.92244
Epoch 19/80: current_loss=9.05998 | best_loss=8.92244
Epoch 20/80: current_loss=9.07335 | best_loss=8.92244
Epoch 21/80: current_loss=9.07584 | best_loss=8.92244
Epoch 22/80: current_loss=8.93444 | best_loss=8.92244
Epoch 23/80: current_loss=8.96180 | best_loss=8.92244
Epoch 24/80: current_loss=9.03512 | best_loss=8.92244
Epoch 25/80: current_loss=9.00837 | best_loss=8.92244
Epoch 26/80: current_loss=9.28140 | best_loss=8.92244
Epoch 27/80: current_loss=8.98082 | best_loss=8.92244
Epoch 28/80: current_loss=9.23250 | best_loss=8.92244
Epoch 29/80: current_loss=9.08558 | best_loss=8.92244
Epoch 30/80: current_loss=8.99760 | best_loss=8.92244
Early Stopping at epoch 30
      explained_var=0.00058 | mse_loss=8.36570

----------------------------------------------
Params for Trial 84
{'learning_rate': 0.1, 'weight_decay': 0.0023162430492689235, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.26266 | best_loss=10.26266
Epoch 1/80: current_loss=11.59213 | best_loss=10.26266
Epoch 2/80: current_loss=10.51968 | best_loss=10.26266
Epoch 3/80: current_loss=9.89923 | best_loss=9.89923
Epoch 4/80: current_loss=12.01637 | best_loss=9.89923
Epoch 5/80: current_loss=12.68899 | best_loss=9.89923
Epoch 6/80: current_loss=10.73645 | best_loss=9.89923
Epoch 7/80: current_loss=11.69646 | best_loss=9.89923
Epoch 8/80: current_loss=12.88027 | best_loss=9.89923
Epoch 9/80: current_loss=12.96511 | best_loss=9.89923
Epoch 10/80: current_loss=10.18866 | best_loss=9.89923
Epoch 11/80: current_loss=13.15884 | best_loss=9.89923
Epoch 12/80: current_loss=11.05859 | best_loss=9.89923
Epoch 13/80: current_loss=11.80185 | best_loss=9.89923
Epoch 14/80: current_loss=13.93615 | best_loss=9.89923
Epoch 15/80: current_loss=10.04767 | best_loss=9.89923
Epoch 16/80: current_loss=12.28698 | best_loss=9.89923
Epoch 17/80: current_loss=13.15673 | best_loss=9.89923
Epoch 18/80: current_loss=10.37502 | best_loss=9.89923
Epoch 19/80: current_loss=11.40687 | best_loss=9.89923
Epoch 20/80: current_loss=11.74934 | best_loss=9.89923
Epoch 21/80: current_loss=10.68123 | best_loss=9.89923
Epoch 22/80: current_loss=12.08691 | best_loss=9.89923
Epoch 23/80: current_loss=10.11271 | best_loss=9.89923
Early Stopping at epoch 23
      explained_var=0.00230 | mse_loss=9.56157
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=13.71737 | best_loss=13.71737
Epoch 1/80: current_loss=16.40292 | best_loss=13.71737
Epoch 2/80: current_loss=9.17442 | best_loss=9.17442
Epoch 3/80: current_loss=9.36643 | best_loss=9.17442
Epoch 4/80: current_loss=18.90448 | best_loss=9.17442
Epoch 5/80: current_loss=8.94464 | best_loss=8.94464
Epoch 6/80: current_loss=9.44742 | best_loss=8.94464
Epoch 7/80: current_loss=14.24361 | best_loss=8.94464
Epoch 8/80: current_loss=11.72285 | best_loss=8.94464
Epoch 9/80: current_loss=8.99552 | best_loss=8.94464
Epoch 10/80: current_loss=9.19169 | best_loss=8.94464
Epoch 11/80: current_loss=9.83565 | best_loss=8.94464
Epoch 12/80: current_loss=9.88781 | best_loss=8.94464
Epoch 13/80: current_loss=13.30608 | best_loss=8.94464
Epoch 14/80: current_loss=10.94504 | best_loss=8.94464
Epoch 15/80: current_loss=10.42731 | best_loss=8.94464
Epoch 16/80: current_loss=9.44034 | best_loss=8.94464
Epoch 17/80: current_loss=9.24234 | best_loss=8.94464
Epoch 18/80: current_loss=8.67351 | best_loss=8.67351
Epoch 19/80: current_loss=9.14189 | best_loss=8.67351
Epoch 20/80: current_loss=9.01288 | best_loss=8.67351
Epoch 21/80: current_loss=9.50523 | best_loss=8.67351
Epoch 22/80: current_loss=11.90668 | best_loss=8.67351
Epoch 23/80: current_loss=11.86509 | best_loss=8.67351
Epoch 24/80: current_loss=9.16732 | best_loss=8.67351
Epoch 25/80: current_loss=9.39005 | best_loss=8.67351
Epoch 26/80: current_loss=8.84375 | best_loss=8.67351
Epoch 27/80: current_loss=9.81826 | best_loss=8.67351
Epoch 28/80: current_loss=8.74548 | best_loss=8.67351
Epoch 29/80: current_loss=9.37890 | best_loss=8.67351
Epoch 30/80: current_loss=9.43510 | best_loss=8.67351
Epoch 31/80: current_loss=9.67201 | best_loss=8.67351
Epoch 32/80: current_loss=16.15667 | best_loss=8.67351
Epoch 33/80: current_loss=8.91958 | best_loss=8.67351
Epoch 34/80: current_loss=9.02836 | best_loss=8.67351
Epoch 35/80: current_loss=10.55423 | best_loss=8.67351
Epoch 36/80: current_loss=9.66537 | best_loss=8.67351
Epoch 37/80: current_loss=9.54110 | best_loss=8.67351
Epoch 38/80: current_loss=9.04564 | best_loss=8.67351
Early Stopping at epoch 38
      explained_var=-0.01428 | mse_loss=8.96630

----------------------------------------------
Params for Trial 85
{'learning_rate': 0.01, 'weight_decay': 0.00089931251399769, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.96356 | best_loss=9.96356
Epoch 1/80: current_loss=9.95166 | best_loss=9.95166
Epoch 2/80: current_loss=10.27247 | best_loss=9.95166
Epoch 3/80: current_loss=10.59135 | best_loss=9.95166
Epoch 4/80: current_loss=10.34492 | best_loss=9.95166
Epoch 5/80: current_loss=9.91558 | best_loss=9.91558
Epoch 6/80: current_loss=10.21985 | best_loss=9.91558
Epoch 7/80: current_loss=10.38818 | best_loss=9.91558
Epoch 8/80: current_loss=10.46732 | best_loss=9.91558
Epoch 9/80: current_loss=10.54261 | best_loss=9.91558
Epoch 10/80: current_loss=10.32400 | best_loss=9.91558
Epoch 11/80: current_loss=10.05539 | best_loss=9.91558
Epoch 12/80: current_loss=9.95751 | best_loss=9.91558
Epoch 13/80: current_loss=9.91142 | best_loss=9.91142
Epoch 14/80: current_loss=10.24738 | best_loss=9.91142
Epoch 15/80: current_loss=10.27648 | best_loss=9.91142
Epoch 16/80: current_loss=10.34737 | best_loss=9.91142
Epoch 17/80: current_loss=10.27440 | best_loss=9.91142
Epoch 18/80: current_loss=9.89743 | best_loss=9.89743
Epoch 19/80: current_loss=9.90323 | best_loss=9.89743
Epoch 20/80: current_loss=9.91251 | best_loss=9.89743
Epoch 21/80: current_loss=9.93703 | best_loss=9.89743
Epoch 22/80: current_loss=9.92415 | best_loss=9.89743
Epoch 23/80: current_loss=9.91515 | best_loss=9.89743
Epoch 24/80: current_loss=10.06053 | best_loss=9.89743
Epoch 25/80: current_loss=10.14504 | best_loss=9.89743
Epoch 26/80: current_loss=10.58258 | best_loss=9.89743
Epoch 27/80: current_loss=9.90589 | best_loss=9.89743
Epoch 28/80: current_loss=9.91314 | best_loss=9.89743
Epoch 29/80: current_loss=11.06991 | best_loss=9.89743
Epoch 30/80: current_loss=10.23449 | best_loss=9.89743
Epoch 31/80: current_loss=10.07579 | best_loss=9.89743
Epoch 32/80: current_loss=9.94923 | best_loss=9.89743
Epoch 33/80: current_loss=9.93241 | best_loss=9.89743
Epoch 34/80: current_loss=9.96513 | best_loss=9.89743
Epoch 35/80: current_loss=9.93125 | best_loss=9.89743
Epoch 36/80: current_loss=10.51633 | best_loss=9.89743
Epoch 37/80: current_loss=10.09803 | best_loss=9.89743
Epoch 38/80: current_loss=9.92847 | best_loss=9.89743
Early Stopping at epoch 38
      explained_var=0.00115 | mse_loss=9.56815
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.24490 | best_loss=9.24490
Epoch 1/80: current_loss=8.63967 | best_loss=8.63967
Epoch 2/80: current_loss=8.71195 | best_loss=8.63967
Epoch 3/80: current_loss=8.81602 | best_loss=8.63967
Epoch 4/80: current_loss=8.60279 | best_loss=8.60279
Epoch 5/80: current_loss=8.66943 | best_loss=8.60279
Epoch 6/80: current_loss=8.50513 | best_loss=8.50513
Epoch 7/80: current_loss=8.87425 | best_loss=8.50513
Epoch 8/80: current_loss=8.66267 | best_loss=8.50513
Epoch 9/80: current_loss=9.19943 | best_loss=8.50513
Epoch 10/80: current_loss=9.06184 | best_loss=8.50513
Epoch 11/80: current_loss=8.77157 | best_loss=8.50513
Epoch 12/80: current_loss=8.45982 | best_loss=8.45982
Epoch 13/80: current_loss=8.56340 | best_loss=8.45982
Epoch 14/80: current_loss=8.86101 | best_loss=8.45982
Epoch 15/80: current_loss=8.44307 | best_loss=8.44307
Epoch 16/80: current_loss=8.57029 | best_loss=8.44307
Epoch 17/80: current_loss=8.84354 | best_loss=8.44307
Epoch 18/80: current_loss=8.88403 | best_loss=8.44307
Epoch 19/80: current_loss=8.61579 | best_loss=8.44307
Epoch 20/80: current_loss=8.57672 | best_loss=8.44307
Epoch 21/80: current_loss=8.59699 | best_loss=8.44307
Epoch 22/80: current_loss=8.76651 | best_loss=8.44307
Epoch 23/80: current_loss=8.88132 | best_loss=8.44307
Epoch 24/80: current_loss=8.59699 | best_loss=8.44307
Epoch 25/80: current_loss=9.30291 | best_loss=8.44307
Epoch 26/80: current_loss=8.76283 | best_loss=8.44307
Epoch 27/80: current_loss=8.76030 | best_loss=8.44307
Epoch 28/80: current_loss=9.42981 | best_loss=8.44307
Epoch 29/80: current_loss=9.46027 | best_loss=8.44307
Epoch 30/80: current_loss=8.39917 | best_loss=8.39917
Epoch 31/80: current_loss=8.78658 | best_loss=8.39917
Epoch 32/80: current_loss=8.59296 | best_loss=8.39917
Epoch 33/80: current_loss=9.11312 | best_loss=8.39917
Epoch 34/80: current_loss=8.51398 | best_loss=8.39917
Epoch 35/80: current_loss=8.42268 | best_loss=8.39917
Epoch 36/80: current_loss=8.66569 | best_loss=8.39917
Epoch 37/80: current_loss=9.62767 | best_loss=8.39917
Epoch 38/80: current_loss=8.51008 | best_loss=8.39917
Epoch 39/80: current_loss=8.52689 | best_loss=8.39917
Epoch 40/80: current_loss=8.48956 | best_loss=8.39917
Epoch 41/80: current_loss=8.63025 | best_loss=8.39917
Epoch 42/80: current_loss=8.51086 | best_loss=8.39917
Epoch 43/80: current_loss=8.54633 | best_loss=8.39917
Epoch 44/80: current_loss=8.61208 | best_loss=8.39917
Epoch 45/80: current_loss=9.10296 | best_loss=8.39917
Epoch 46/80: current_loss=9.88359 | best_loss=8.39917
Epoch 47/80: current_loss=8.55967 | best_loss=8.39917
Epoch 48/80: current_loss=8.60731 | best_loss=8.39917
Epoch 49/80: current_loss=8.56917 | best_loss=8.39917
Epoch 50/80: current_loss=8.78030 | best_loss=8.39917
Early Stopping at epoch 50
      explained_var=0.00874 | mse_loss=8.50601
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.94687 | best_loss=9.94687
Epoch 1/80: current_loss=9.32745 | best_loss=9.32745
Epoch 2/80: current_loss=11.22885 | best_loss=9.32745
Epoch 3/80: current_loss=9.59857 | best_loss=9.32745
Epoch 4/80: current_loss=9.49070 | best_loss=9.32745
Epoch 5/80: current_loss=9.71724 | best_loss=9.32745
Epoch 6/80: current_loss=9.36330 | best_loss=9.32745
Epoch 7/80: current_loss=9.25528 | best_loss=9.25528
Epoch 8/80: current_loss=11.62087 | best_loss=9.25528
Epoch 9/80: current_loss=9.32534 | best_loss=9.25528
Epoch 10/80: current_loss=9.31601 | best_loss=9.25528
Epoch 11/80: current_loss=9.62902 | best_loss=9.25528
Epoch 12/80: current_loss=9.33073 | best_loss=9.25528
Epoch 13/80: current_loss=9.29123 | best_loss=9.25528
Epoch 14/80: current_loss=9.30241 | best_loss=9.25528
Epoch 15/80: current_loss=10.37044 | best_loss=9.25528
Epoch 16/80: current_loss=9.36869 | best_loss=9.25528
Epoch 17/80: current_loss=9.70370 | best_loss=9.25528
Epoch 18/80: current_loss=9.25741 | best_loss=9.25528
Epoch 19/80: current_loss=9.92468 | best_loss=9.25528
Epoch 20/80: current_loss=9.59012 | best_loss=9.25528
Epoch 21/80: current_loss=9.39379 | best_loss=9.25528
Epoch 22/80: current_loss=9.39442 | best_loss=9.25528
Epoch 23/80: current_loss=9.34088 | best_loss=9.25528
Epoch 24/80: current_loss=9.49540 | best_loss=9.25528
Epoch 25/80: current_loss=10.05063 | best_loss=9.25528
Epoch 26/80: current_loss=9.66711 | best_loss=9.25528
Epoch 27/80: current_loss=9.29067 | best_loss=9.25528
Early Stopping at epoch 27
      explained_var=-0.00034 | mse_loss=9.38207
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.08359 | best_loss=9.08359
Epoch 1/80: current_loss=9.55544 | best_loss=9.08359
Epoch 2/80: current_loss=9.94356 | best_loss=9.08359
Epoch 3/80: current_loss=8.97425 | best_loss=8.97425
Epoch 4/80: current_loss=8.98494 | best_loss=8.97425
Epoch 5/80: current_loss=9.55141 | best_loss=8.97425
Epoch 6/80: current_loss=9.05203 | best_loss=8.97425
Epoch 7/80: current_loss=9.06603 | best_loss=8.97425
Epoch 8/80: current_loss=9.54690 | best_loss=8.97425
Epoch 9/80: current_loss=8.99161 | best_loss=8.97425
Epoch 10/80: current_loss=9.01836 | best_loss=8.97425
Epoch 11/80: current_loss=8.93750 | best_loss=8.93750
Epoch 12/80: current_loss=9.24603 | best_loss=8.93750
Epoch 13/80: current_loss=8.99219 | best_loss=8.93750
Epoch 14/80: current_loss=9.10914 | best_loss=8.93750
Epoch 15/80: current_loss=8.96674 | best_loss=8.93750
Epoch 16/80: current_loss=9.95876 | best_loss=8.93750
Epoch 17/80: current_loss=8.99650 | best_loss=8.93750
Epoch 18/80: current_loss=8.97189 | best_loss=8.93750
Epoch 19/80: current_loss=9.19393 | best_loss=8.93750
Epoch 20/80: current_loss=8.95844 | best_loss=8.93750
Epoch 21/80: current_loss=9.56169 | best_loss=8.93750
Epoch 22/80: current_loss=9.09110 | best_loss=8.93750
Epoch 23/80: current_loss=8.97396 | best_loss=8.93750
Epoch 24/80: current_loss=10.97025 | best_loss=8.93750
Epoch 25/80: current_loss=9.04529 | best_loss=8.93750
Epoch 26/80: current_loss=9.05886 | best_loss=8.93750
Epoch 27/80: current_loss=9.00231 | best_loss=8.93750
Epoch 28/80: current_loss=9.02069 | best_loss=8.93750
Epoch 29/80: current_loss=9.01649 | best_loss=8.93750
Epoch 30/80: current_loss=9.69485 | best_loss=8.93750
Epoch 31/80: current_loss=9.01509 | best_loss=8.93750
Early Stopping at epoch 31
      explained_var=0.00098 | mse_loss=8.35381
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.49219 | best_loss=8.49219
Epoch 1/80: current_loss=8.62791 | best_loss=8.49219
Epoch 2/80: current_loss=9.15730 | best_loss=8.49219
Epoch 3/80: current_loss=8.51390 | best_loss=8.49219
Epoch 4/80: current_loss=8.51531 | best_loss=8.49219
Epoch 5/80: current_loss=8.50471 | best_loss=8.49219
Epoch 6/80: current_loss=8.49591 | best_loss=8.49219
Epoch 7/80: current_loss=8.67352 | best_loss=8.49219
Epoch 8/80: current_loss=8.54822 | best_loss=8.49219
Epoch 9/80: current_loss=8.80079 | best_loss=8.49219
Epoch 10/80: current_loss=8.59821 | best_loss=8.49219
Epoch 11/80: current_loss=10.13316 | best_loss=8.49219
Epoch 12/80: current_loss=8.58302 | best_loss=8.49219
Epoch 13/80: current_loss=8.88760 | best_loss=8.49219
Epoch 14/80: current_loss=8.51695 | best_loss=8.49219
Epoch 15/80: current_loss=8.79329 | best_loss=8.49219
Epoch 16/80: current_loss=9.29681 | best_loss=8.49219
Epoch 17/80: current_loss=8.49324 | best_loss=8.49219
Epoch 18/80: current_loss=8.53574 | best_loss=8.49219
Epoch 19/80: current_loss=8.87986 | best_loss=8.49219
Epoch 20/80: current_loss=8.50367 | best_loss=8.49219
Early Stopping at epoch 20
      explained_var=0.00027 | mse_loss=8.19822
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.00216| avg_loss=8.80165
----------------------------------------------


----------------------------------------------
Params for Trial 86
{'learning_rate': 0.0001, 'weight_decay': 0.0014997012380659216, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=12.14414 | best_loss=12.14414
Epoch 1/80: current_loss=11.19929 | best_loss=11.19929
Epoch 2/80: current_loss=10.84641 | best_loss=10.84641
Epoch 3/80: current_loss=10.59806 | best_loss=10.59806
Epoch 4/80: current_loss=10.32837 | best_loss=10.32837
Epoch 5/80: current_loss=10.30691 | best_loss=10.30691
Epoch 6/80: current_loss=10.03888 | best_loss=10.03888
Epoch 7/80: current_loss=9.97277 | best_loss=9.97277
Epoch 8/80: current_loss=9.96892 | best_loss=9.96892
Epoch 9/80: current_loss=9.92303 | best_loss=9.92303
Epoch 10/80: current_loss=9.92278 | best_loss=9.92278
Epoch 11/80: current_loss=9.97530 | best_loss=9.92278
Epoch 12/80: current_loss=10.01773 | best_loss=9.92278
Epoch 13/80: current_loss=9.92133 | best_loss=9.92133
Epoch 14/80: current_loss=9.94297 | best_loss=9.92133
Epoch 15/80: current_loss=9.97852 | best_loss=9.92133
Epoch 16/80: current_loss=9.96273 | best_loss=9.92133
Epoch 17/80: current_loss=9.93919 | best_loss=9.92133
Epoch 18/80: current_loss=10.00382 | best_loss=9.92133
Epoch 19/80: current_loss=9.96363 | best_loss=9.92133
Epoch 20/80: current_loss=9.96428 | best_loss=9.92133
Epoch 21/80: current_loss=9.99908 | best_loss=9.92133
Epoch 22/80: current_loss=9.95160 | best_loss=9.92133
Epoch 23/80: current_loss=9.97193 | best_loss=9.92133
Epoch 24/80: current_loss=9.96475 | best_loss=9.92133
Epoch 25/80: current_loss=10.01990 | best_loss=9.92133
Epoch 26/80: current_loss=9.94682 | best_loss=9.92133
Epoch 27/80: current_loss=9.98944 | best_loss=9.92133
Epoch 28/80: current_loss=9.92664 | best_loss=9.92133
Epoch 29/80: current_loss=9.94571 | best_loss=9.92133
Epoch 30/80: current_loss=9.92464 | best_loss=9.92133
Epoch 31/80: current_loss=9.91453 | best_loss=9.91453
Epoch 32/80: current_loss=9.95013 | best_loss=9.91453
Epoch 33/80: current_loss=9.93110 | best_loss=9.91453
Epoch 34/80: current_loss=9.93652 | best_loss=9.91453
Epoch 35/80: current_loss=9.94526 | best_loss=9.91453
Epoch 36/80: current_loss=9.93083 | best_loss=9.91453
Epoch 37/80: current_loss=9.91882 | best_loss=9.91453
Epoch 38/80: current_loss=9.92408 | best_loss=9.91453
Epoch 39/80: current_loss=9.95414 | best_loss=9.91453
Epoch 40/80: current_loss=9.96970 | best_loss=9.91453
Epoch 41/80: current_loss=9.91424 | best_loss=9.91424
Epoch 42/80: current_loss=9.92455 | best_loss=9.91424
Epoch 43/80: current_loss=9.95063 | best_loss=9.91424
Epoch 44/80: current_loss=9.92453 | best_loss=9.91424
Epoch 45/80: current_loss=9.92959 | best_loss=9.91424
Epoch 46/80: current_loss=9.98878 | best_loss=9.91424
Epoch 47/80: current_loss=9.94450 | best_loss=9.91424
Epoch 48/80: current_loss=9.94480 | best_loss=9.91424
Epoch 49/80: current_loss=9.95180 | best_loss=9.91424
Epoch 50/80: current_loss=9.92853 | best_loss=9.91424
Epoch 51/80: current_loss=9.91850 | best_loss=9.91424
Epoch 52/80: current_loss=9.96208 | best_loss=9.91424
Epoch 53/80: current_loss=9.94598 | best_loss=9.91424
Epoch 54/80: current_loss=9.94087 | best_loss=9.91424
Epoch 55/80: current_loss=9.94346 | best_loss=9.91424
Epoch 56/80: current_loss=9.93327 | best_loss=9.91424
Epoch 57/80: current_loss=9.93743 | best_loss=9.91424
Epoch 58/80: current_loss=9.90143 | best_loss=9.90143
Epoch 59/80: current_loss=9.96866 | best_loss=9.90143
Epoch 60/80: current_loss=9.93300 | best_loss=9.90143
Epoch 61/80: current_loss=9.98485 | best_loss=9.90143
Epoch 62/80: current_loss=9.92306 | best_loss=9.90143
Epoch 63/80: current_loss=9.91472 | best_loss=9.90143
Epoch 64/80: current_loss=9.94787 | best_loss=9.90143
Epoch 65/80: current_loss=9.93117 | best_loss=9.90143
Epoch 66/80: current_loss=9.96798 | best_loss=9.90143
Epoch 67/80: current_loss=9.90986 | best_loss=9.90143
Epoch 68/80: current_loss=9.92155 | best_loss=9.90143
Epoch 69/80: current_loss=9.91026 | best_loss=9.90143
Epoch 70/80: current_loss=9.95844 | best_loss=9.90143
Epoch 71/80: current_loss=9.95054 | best_loss=9.90143
Epoch 72/80: current_loss=9.91372 | best_loss=9.90143
Epoch 73/80: current_loss=10.01381 | best_loss=9.90143
Epoch 74/80: current_loss=9.90908 | best_loss=9.90143
Epoch 75/80: current_loss=9.95167 | best_loss=9.90143
Epoch 76/80: current_loss=9.92366 | best_loss=9.90143
Epoch 77/80: current_loss=9.96281 | best_loss=9.90143
Epoch 78/80: current_loss=9.94663 | best_loss=9.90143
Early Stopping at epoch 78
      explained_var=0.00073 | mse_loss=9.57335

----------------------------------------------
Params for Trial 87
{'learning_rate': 0.001, 'weight_decay': 0.0036925448836130957, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.10678 | best_loss=10.10678
Epoch 1/80: current_loss=9.95205 | best_loss=9.95205
Epoch 2/80: current_loss=9.90800 | best_loss=9.90800
Epoch 3/80: current_loss=9.95207 | best_loss=9.90800
Epoch 4/80: current_loss=9.89343 | best_loss=9.89343
Epoch 5/80: current_loss=9.95047 | best_loss=9.89343
Epoch 6/80: current_loss=9.89624 | best_loss=9.89343
Epoch 7/80: current_loss=9.91545 | best_loss=9.89343
Epoch 8/80: current_loss=9.90851 | best_loss=9.89343
Epoch 9/80: current_loss=9.93496 | best_loss=9.89343
Epoch 10/80: current_loss=9.91186 | best_loss=9.89343
Epoch 11/80: current_loss=9.90838 | best_loss=9.89343
Epoch 12/80: current_loss=9.92245 | best_loss=9.89343
Epoch 13/80: current_loss=10.02582 | best_loss=9.89343
Epoch 14/80: current_loss=9.89573 | best_loss=9.89343
Epoch 15/80: current_loss=9.89803 | best_loss=9.89343
Epoch 16/80: current_loss=9.92940 | best_loss=9.89343
Epoch 17/80: current_loss=9.91580 | best_loss=9.89343
Epoch 18/80: current_loss=9.89497 | best_loss=9.89343
Epoch 19/80: current_loss=9.89697 | best_loss=9.89343
Epoch 20/80: current_loss=9.91352 | best_loss=9.89343
Epoch 21/80: current_loss=9.93411 | best_loss=9.89343
Epoch 22/80: current_loss=10.07155 | best_loss=9.89343
Epoch 23/80: current_loss=9.93113 | best_loss=9.89343
Epoch 24/80: current_loss=9.92570 | best_loss=9.89343
Early Stopping at epoch 24
      explained_var=0.00277 | mse_loss=9.56837
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.57010 | best_loss=8.57010
Epoch 1/80: current_loss=8.52469 | best_loss=8.52469
Epoch 2/80: current_loss=8.49976 | best_loss=8.49976
Epoch 3/80: current_loss=8.52924 | best_loss=8.49976
Epoch 4/80: current_loss=8.48121 | best_loss=8.48121
Epoch 5/80: current_loss=8.48160 | best_loss=8.48121
Epoch 6/80: current_loss=8.51911 | best_loss=8.48121
Epoch 7/80: current_loss=8.48697 | best_loss=8.48121
Epoch 8/80: current_loss=8.48296 | best_loss=8.48121
Epoch 9/80: current_loss=8.61801 | best_loss=8.48121
Epoch 10/80: current_loss=8.51692 | best_loss=8.48121
Epoch 11/80: current_loss=8.50818 | best_loss=8.48121
Epoch 12/80: current_loss=8.50260 | best_loss=8.48121
Epoch 13/80: current_loss=8.54437 | best_loss=8.48121
Epoch 14/80: current_loss=8.52680 | best_loss=8.48121
Epoch 15/80: current_loss=8.51512 | best_loss=8.48121
Epoch 16/80: current_loss=8.49810 | best_loss=8.48121
Epoch 17/80: current_loss=8.53783 | best_loss=8.48121
Epoch 18/80: current_loss=8.50803 | best_loss=8.48121
Epoch 19/80: current_loss=8.53755 | best_loss=8.48121
Epoch 20/80: current_loss=8.52459 | best_loss=8.48121
Epoch 21/80: current_loss=8.49313 | best_loss=8.48121
Epoch 22/80: current_loss=8.53682 | best_loss=8.48121
Epoch 23/80: current_loss=8.48477 | best_loss=8.48121
Epoch 24/80: current_loss=8.53064 | best_loss=8.48121
Early Stopping at epoch 24
      explained_var=0.00143 | mse_loss=8.56907

----------------------------------------------
Params for Trial 88
{'learning_rate': 0.01, 'weight_decay': 0.0011360581648137958, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.01976 | best_loss=10.01976
Epoch 1/80: current_loss=10.20913 | best_loss=10.01976
Epoch 2/80: current_loss=9.92900 | best_loss=9.92900
Epoch 3/80: current_loss=10.62216 | best_loss=9.92900
Epoch 4/80: current_loss=9.93415 | best_loss=9.92900
Epoch 5/80: current_loss=10.00338 | best_loss=9.92900
Epoch 6/80: current_loss=9.98762 | best_loss=9.92900
Epoch 7/80: current_loss=9.91291 | best_loss=9.91291
Epoch 8/80: current_loss=9.90987 | best_loss=9.90987
Epoch 9/80: current_loss=9.96877 | best_loss=9.90987
Epoch 10/80: current_loss=10.14763 | best_loss=9.90987
Epoch 11/80: current_loss=9.94634 | best_loss=9.90987
Epoch 12/80: current_loss=10.33709 | best_loss=9.90987
Epoch 13/80: current_loss=10.27166 | best_loss=9.90987
Epoch 14/80: current_loss=10.00222 | best_loss=9.90987
Epoch 15/80: current_loss=9.94589 | best_loss=9.90987
Epoch 16/80: current_loss=9.97501 | best_loss=9.90987
Epoch 17/80: current_loss=9.91509 | best_loss=9.90987
Epoch 18/80: current_loss=9.91495 | best_loss=9.90987
Epoch 19/80: current_loss=10.04263 | best_loss=9.90987
Epoch 20/80: current_loss=9.94064 | best_loss=9.90987
Epoch 21/80: current_loss=9.99849 | best_loss=9.90987
Epoch 22/80: current_loss=9.89263 | best_loss=9.89263
Epoch 23/80: current_loss=10.01730 | best_loss=9.89263
Epoch 24/80: current_loss=10.09896 | best_loss=9.89263
Epoch 25/80: current_loss=9.90455 | best_loss=9.89263
Epoch 26/80: current_loss=9.98943 | best_loss=9.89263
Epoch 27/80: current_loss=10.69670 | best_loss=9.89263
Epoch 28/80: current_loss=9.90481 | best_loss=9.89263
Epoch 29/80: current_loss=9.91021 | best_loss=9.89263
Epoch 30/80: current_loss=10.12603 | best_loss=9.89263
Epoch 31/80: current_loss=9.91553 | best_loss=9.89263
Epoch 32/80: current_loss=9.95698 | best_loss=9.89263
Epoch 33/80: current_loss=9.96014 | best_loss=9.89263
Epoch 34/80: current_loss=9.94875 | best_loss=9.89263
Epoch 35/80: current_loss=10.10883 | best_loss=9.89263
Epoch 36/80: current_loss=10.01726 | best_loss=9.89263
Epoch 37/80: current_loss=9.99593 | best_loss=9.89263
Epoch 38/80: current_loss=9.92922 | best_loss=9.89263
Epoch 39/80: current_loss=10.04802 | best_loss=9.89263
Epoch 40/80: current_loss=10.44536 | best_loss=9.89263
Epoch 41/80: current_loss=10.00584 | best_loss=9.89263
Epoch 42/80: current_loss=10.04084 | best_loss=9.89263
Early Stopping at epoch 42
      explained_var=0.00178 | mse_loss=9.56007
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.47346 | best_loss=8.47346
Epoch 1/80: current_loss=8.53943 | best_loss=8.47346
Epoch 2/80: current_loss=8.61966 | best_loss=8.47346
Epoch 3/80: current_loss=8.50923 | best_loss=8.47346
Epoch 4/80: current_loss=8.61919 | best_loss=8.47346
Epoch 5/80: current_loss=8.52596 | best_loss=8.47346
Epoch 6/80: current_loss=8.51591 | best_loss=8.47346
Epoch 7/80: current_loss=8.49610 | best_loss=8.47346
Epoch 8/80: current_loss=8.48531 | best_loss=8.47346
Epoch 9/80: current_loss=8.54194 | best_loss=8.47346
Epoch 10/80: current_loss=8.54672 | best_loss=8.47346
Epoch 11/80: current_loss=8.65387 | best_loss=8.47346
Epoch 12/80: current_loss=8.56253 | best_loss=8.47346
Epoch 13/80: current_loss=8.52356 | best_loss=8.47346
Epoch 14/80: current_loss=8.69143 | best_loss=8.47346
Epoch 15/80: current_loss=9.58323 | best_loss=8.47346
Epoch 16/80: current_loss=8.56921 | best_loss=8.47346
Epoch 17/80: current_loss=8.50776 | best_loss=8.47346
Epoch 18/80: current_loss=8.87640 | best_loss=8.47346
Epoch 19/80: current_loss=8.59375 | best_loss=8.47346
Epoch 20/80: current_loss=8.76226 | best_loss=8.47346
Early Stopping at epoch 20
      explained_var=0.00251 | mse_loss=8.55807
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.27150 | best_loss=9.27150
Epoch 1/80: current_loss=9.31222 | best_loss=9.27150
Epoch 2/80: current_loss=9.34477 | best_loss=9.27150
Epoch 3/80: current_loss=9.49032 | best_loss=9.27150
Epoch 4/80: current_loss=9.43594 | best_loss=9.27150
Epoch 5/80: current_loss=9.54417 | best_loss=9.27150
Epoch 6/80: current_loss=9.33873 | best_loss=9.27150
Epoch 7/80: current_loss=9.42513 | best_loss=9.27150
Epoch 8/80: current_loss=9.39687 | best_loss=9.27150
Epoch 9/80: current_loss=9.46527 | best_loss=9.27150
Epoch 10/80: current_loss=9.46515 | best_loss=9.27150
Epoch 11/80: current_loss=9.52353 | best_loss=9.27150
Epoch 12/80: current_loss=9.32950 | best_loss=9.27150
Epoch 13/80: current_loss=9.27923 | best_loss=9.27150
Epoch 14/80: current_loss=10.34615 | best_loss=9.27150
Epoch 15/80: current_loss=9.50483 | best_loss=9.27150
Epoch 16/80: current_loss=9.64183 | best_loss=9.27150
Epoch 17/80: current_loss=9.32087 | best_loss=9.27150
Epoch 18/80: current_loss=9.88447 | best_loss=9.27150
Epoch 19/80: current_loss=10.17857 | best_loss=9.27150
Epoch 20/80: current_loss=9.28324 | best_loss=9.27150
Early Stopping at epoch 20
      explained_var=-0.00124 | mse_loss=9.40139
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.10586 | best_loss=9.10586
Epoch 1/80: current_loss=9.02012 | best_loss=9.02012
Epoch 2/80: current_loss=8.99948 | best_loss=8.99948
Epoch 3/80: current_loss=8.97539 | best_loss=8.97539
Epoch 4/80: current_loss=8.94434 | best_loss=8.94434
Epoch 5/80: current_loss=9.21736 | best_loss=8.94434
Epoch 6/80: current_loss=8.99014 | best_loss=8.94434
Epoch 7/80: current_loss=8.97053 | best_loss=8.94434
Epoch 8/80: current_loss=9.43353 | best_loss=8.94434
Epoch 9/80: current_loss=8.94213 | best_loss=8.94213
Epoch 10/80: current_loss=9.09177 | best_loss=8.94213
Epoch 11/80: current_loss=9.26082 | best_loss=8.94213
Epoch 12/80: current_loss=9.27279 | best_loss=8.94213
Epoch 13/80: current_loss=9.44422 | best_loss=8.94213
Epoch 14/80: current_loss=8.93507 | best_loss=8.93507
Epoch 15/80: current_loss=9.12180 | best_loss=8.93507
Epoch 16/80: current_loss=9.34541 | best_loss=8.93507
Epoch 17/80: current_loss=8.99441 | best_loss=8.93507
Epoch 18/80: current_loss=8.98635 | best_loss=8.93507
Epoch 19/80: current_loss=8.93160 | best_loss=8.93160
Epoch 20/80: current_loss=9.80816 | best_loss=8.93160
Epoch 21/80: current_loss=8.94330 | best_loss=8.93160
Epoch 22/80: current_loss=8.97289 | best_loss=8.93160
Epoch 23/80: current_loss=9.10954 | best_loss=8.93160
Epoch 24/80: current_loss=8.99122 | best_loss=8.93160
Epoch 25/80: current_loss=8.96576 | best_loss=8.93160
Epoch 26/80: current_loss=9.01676 | best_loss=8.93160
Epoch 27/80: current_loss=9.09080 | best_loss=8.93160
Epoch 28/80: current_loss=8.95714 | best_loss=8.93160
Epoch 29/80: current_loss=9.10757 | best_loss=8.93160
Epoch 30/80: current_loss=8.93665 | best_loss=8.93160
Epoch 31/80: current_loss=8.93347 | best_loss=8.93160
Epoch 32/80: current_loss=8.98261 | best_loss=8.93160
Epoch 33/80: current_loss=9.46282 | best_loss=8.93160
Epoch 34/80: current_loss=8.93145 | best_loss=8.93145
Epoch 35/80: current_loss=9.05553 | best_loss=8.93145
Epoch 36/80: current_loss=9.11350 | best_loss=8.93145
Epoch 37/80: current_loss=9.05867 | best_loss=8.93145
Epoch 38/80: current_loss=9.18275 | best_loss=8.93145
Epoch 39/80: current_loss=9.21205 | best_loss=8.93145
Epoch 40/80: current_loss=9.31089 | best_loss=8.93145
Epoch 41/80: current_loss=8.94539 | best_loss=8.93145
Epoch 42/80: current_loss=9.48068 | best_loss=8.93145
Epoch 43/80: current_loss=8.96174 | best_loss=8.93145
Epoch 44/80: current_loss=8.94447 | best_loss=8.93145
Epoch 45/80: current_loss=9.10300 | best_loss=8.93145
Epoch 46/80: current_loss=9.39619 | best_loss=8.93145
Epoch 47/80: current_loss=9.22001 | best_loss=8.93145
Epoch 48/80: current_loss=8.91576 | best_loss=8.91576
Epoch 49/80: current_loss=8.96443 | best_loss=8.91576
Epoch 50/80: current_loss=8.88703 | best_loss=8.88703
Epoch 51/80: current_loss=8.95474 | best_loss=8.88703
Epoch 52/80: current_loss=8.97945 | best_loss=8.88703
Epoch 53/80: current_loss=9.04727 | best_loss=8.88703
Epoch 54/80: current_loss=9.31707 | best_loss=8.88703
Epoch 55/80: current_loss=8.92382 | best_loss=8.88703
Epoch 56/80: current_loss=8.92102 | best_loss=8.88703
Epoch 57/80: current_loss=9.03160 | best_loss=8.88703
Epoch 58/80: current_loss=9.23935 | best_loss=8.88703
Epoch 59/80: current_loss=9.49863 | best_loss=8.88703
Epoch 60/80: current_loss=8.99077 | best_loss=8.88703
Epoch 61/80: current_loss=9.00926 | best_loss=8.88703
Epoch 62/80: current_loss=9.12751 | best_loss=8.88703
Epoch 63/80: current_loss=9.21701 | best_loss=8.88703
Epoch 64/80: current_loss=9.00975 | best_loss=8.88703
Epoch 65/80: current_loss=9.00173 | best_loss=8.88703
Epoch 66/80: current_loss=8.96862 | best_loss=8.88703
Epoch 67/80: current_loss=8.97396 | best_loss=8.88703
Epoch 68/80: current_loss=9.02730 | best_loss=8.88703
Epoch 69/80: current_loss=8.96021 | best_loss=8.88703
Epoch 70/80: current_loss=8.99492 | best_loss=8.88703
Early Stopping at epoch 70
      explained_var=0.00361 | mse_loss=8.33311
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.48648 | best_loss=8.48648
Epoch 1/80: current_loss=8.77891 | best_loss=8.48648
Epoch 2/80: current_loss=8.61515 | best_loss=8.48648
Epoch 3/80: current_loss=8.71891 | best_loss=8.48648
Epoch 4/80: current_loss=8.48687 | best_loss=8.48648
Epoch 5/80: current_loss=8.60797 | best_loss=8.48648
Epoch 6/80: current_loss=8.49575 | best_loss=8.48648
Epoch 7/80: current_loss=8.59736 | best_loss=8.48648
Epoch 8/80: current_loss=8.69549 | best_loss=8.48648
Epoch 9/80: current_loss=8.55681 | best_loss=8.48648
Epoch 10/80: current_loss=8.67069 | best_loss=8.48648
Epoch 11/80: current_loss=8.48165 | best_loss=8.48165
Epoch 12/80: current_loss=8.66587 | best_loss=8.48165
Epoch 13/80: current_loss=8.48586 | best_loss=8.48165
Epoch 14/80: current_loss=8.51000 | best_loss=8.48165
Epoch 15/80: current_loss=8.73318 | best_loss=8.48165
Epoch 16/80: current_loss=8.50894 | best_loss=8.48165
Epoch 17/80: current_loss=8.69026 | best_loss=8.48165
Epoch 18/80: current_loss=8.69606 | best_loss=8.48165
Epoch 19/80: current_loss=8.57619 | best_loss=8.48165
Epoch 20/80: current_loss=8.56602 | best_loss=8.48165
Epoch 21/80: current_loss=8.57741 | best_loss=8.48165
Epoch 22/80: current_loss=8.52199 | best_loss=8.48165
Epoch 23/80: current_loss=8.66178 | best_loss=8.48165
Epoch 24/80: current_loss=8.66396 | best_loss=8.48165
Epoch 25/80: current_loss=8.73828 | best_loss=8.48165
Epoch 26/80: current_loss=8.91072 | best_loss=8.48165
Epoch 27/80: current_loss=8.50533 | best_loss=8.48165
Epoch 28/80: current_loss=8.54474 | best_loss=8.48165
Epoch 29/80: current_loss=9.08143 | best_loss=8.48165
Epoch 30/80: current_loss=8.47712 | best_loss=8.47712
Epoch 31/80: current_loss=8.41559 | best_loss=8.41559
Epoch 32/80: current_loss=8.50167 | best_loss=8.41559
Epoch 33/80: current_loss=8.56798 | best_loss=8.41559
Epoch 34/80: current_loss=8.53561 | best_loss=8.41559
Epoch 35/80: current_loss=8.59267 | best_loss=8.41559
Epoch 36/80: current_loss=8.68699 | best_loss=8.41559
Epoch 37/80: current_loss=8.81741 | best_loss=8.41559
Epoch 38/80: current_loss=8.53133 | best_loss=8.41559
Epoch 39/80: current_loss=8.88988 | best_loss=8.41559
Epoch 40/80: current_loss=8.50103 | best_loss=8.41559
Epoch 41/80: current_loss=8.49943 | best_loss=8.41559
Epoch 42/80: current_loss=8.51895 | best_loss=8.41559
Epoch 43/80: current_loss=8.54862 | best_loss=8.41559
Epoch 44/80: current_loss=8.54569 | best_loss=8.41559
Epoch 45/80: current_loss=8.51636 | best_loss=8.41559
Epoch 46/80: current_loss=8.58856 | best_loss=8.41559
Epoch 47/80: current_loss=8.62960 | best_loss=8.41559
Epoch 48/80: current_loss=8.53903 | best_loss=8.41559
Epoch 49/80: current_loss=8.50475 | best_loss=8.41559
Epoch 50/80: current_loss=8.49453 | best_loss=8.41559
Epoch 51/80: current_loss=8.64635 | best_loss=8.41559
Early Stopping at epoch 51
      explained_var=0.00244 | mse_loss=8.19793
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00182| avg_loss=8.81012
----------------------------------------------


----------------------------------------------
Params for Trial 89
{'learning_rate': 0.01, 'weight_decay': 0.005689163299418702, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.13897 | best_loss=10.13897
Epoch 1/80: current_loss=10.57847 | best_loss=10.13897
Epoch 2/80: current_loss=9.99853 | best_loss=9.99853
Epoch 3/80: current_loss=9.99770 | best_loss=9.99770
Epoch 4/80: current_loss=9.91045 | best_loss=9.91045
Epoch 5/80: current_loss=9.89092 | best_loss=9.89092
Epoch 6/80: current_loss=9.93305 | best_loss=9.89092
Epoch 7/80: current_loss=9.98516 | best_loss=9.89092
Epoch 8/80: current_loss=10.34773 | best_loss=9.89092
Epoch 9/80: current_loss=10.55392 | best_loss=9.89092
Epoch 10/80: current_loss=10.15267 | best_loss=9.89092
Epoch 11/80: current_loss=9.88833 | best_loss=9.88833
Epoch 12/80: current_loss=10.03747 | best_loss=9.88833
Epoch 13/80: current_loss=9.92915 | best_loss=9.88833
Epoch 14/80: current_loss=9.89939 | best_loss=9.88833
Epoch 15/80: current_loss=9.91176 | best_loss=9.88833
Epoch 16/80: current_loss=9.95154 | best_loss=9.88833
Epoch 17/80: current_loss=9.96405 | best_loss=9.88833
Epoch 18/80: current_loss=10.09375 | best_loss=9.88833
Epoch 19/80: current_loss=9.90303 | best_loss=9.88833
Epoch 20/80: current_loss=9.91251 | best_loss=9.88833
Epoch 21/80: current_loss=10.30841 | best_loss=9.88833
Epoch 22/80: current_loss=10.39898 | best_loss=9.88833
Epoch 23/80: current_loss=9.99067 | best_loss=9.88833
Epoch 24/80: current_loss=10.12526 | best_loss=9.88833
Epoch 25/80: current_loss=10.20490 | best_loss=9.88833
Epoch 26/80: current_loss=9.89608 | best_loss=9.88833
Epoch 27/80: current_loss=9.93709 | best_loss=9.88833
Epoch 28/80: current_loss=9.93213 | best_loss=9.88833
Epoch 29/80: current_loss=10.01113 | best_loss=9.88833
Epoch 30/80: current_loss=10.04138 | best_loss=9.88833
Epoch 31/80: current_loss=10.37114 | best_loss=9.88833
Early Stopping at epoch 31
      explained_var=0.00246 | mse_loss=9.55920
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.53068 | best_loss=8.53068
Epoch 1/80: current_loss=8.55602 | best_loss=8.53068
Epoch 2/80: current_loss=8.50493 | best_loss=8.50493
Epoch 3/80: current_loss=8.53804 | best_loss=8.50493
Epoch 4/80: current_loss=9.28073 | best_loss=8.50493
Epoch 5/80: current_loss=8.52336 | best_loss=8.50493
Epoch 6/80: current_loss=8.49100 | best_loss=8.49100
Epoch 7/80: current_loss=8.60117 | best_loss=8.49100
Epoch 8/80: current_loss=8.46093 | best_loss=8.46093
Epoch 9/80: current_loss=8.78672 | best_loss=8.46093
Epoch 10/80: current_loss=8.75951 | best_loss=8.46093
Epoch 11/80: current_loss=8.54870 | best_loss=8.46093
Epoch 12/80: current_loss=8.60787 | best_loss=8.46093
Epoch 13/80: current_loss=8.61518 | best_loss=8.46093
Epoch 14/80: current_loss=8.66796 | best_loss=8.46093
Epoch 15/80: current_loss=8.70136 | best_loss=8.46093
Epoch 16/80: current_loss=8.55211 | best_loss=8.46093
Epoch 17/80: current_loss=8.60894 | best_loss=8.46093
Epoch 18/80: current_loss=9.92967 | best_loss=8.46093
Epoch 19/80: current_loss=8.68827 | best_loss=8.46093
Epoch 20/80: current_loss=8.68511 | best_loss=8.46093
Epoch 21/80: current_loss=9.69055 | best_loss=8.46093
Epoch 22/80: current_loss=8.47118 | best_loss=8.46093
Epoch 23/80: current_loss=8.51990 | best_loss=8.46093
Epoch 24/80: current_loss=8.60671 | best_loss=8.46093
Epoch 25/80: current_loss=9.07370 | best_loss=8.46093
Epoch 26/80: current_loss=8.48032 | best_loss=8.46093
Epoch 27/80: current_loss=8.94254 | best_loss=8.46093
Epoch 28/80: current_loss=8.52709 | best_loss=8.46093
Early Stopping at epoch 28
      explained_var=0.00258 | mse_loss=8.55630
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.28318 | best_loss=9.28318
Epoch 1/80: current_loss=9.29545 | best_loss=9.28318
Epoch 2/80: current_loss=9.27555 | best_loss=9.27555
Epoch 3/80: current_loss=9.87117 | best_loss=9.27555
Epoch 4/80: current_loss=9.80725 | best_loss=9.27555
Epoch 5/80: current_loss=9.31695 | best_loss=9.27555
Epoch 6/80: current_loss=11.36223 | best_loss=9.27555
Epoch 7/80: current_loss=9.33784 | best_loss=9.27555
Epoch 8/80: current_loss=9.34940 | best_loss=9.27555
Epoch 9/80: current_loss=10.07373 | best_loss=9.27555
Epoch 10/80: current_loss=9.34970 | best_loss=9.27555
Epoch 11/80: current_loss=9.96736 | best_loss=9.27555
Epoch 12/80: current_loss=9.78395 | best_loss=9.27555
Epoch 13/80: current_loss=9.32414 | best_loss=9.27555
Epoch 14/80: current_loss=9.46894 | best_loss=9.27555
Epoch 15/80: current_loss=9.46072 | best_loss=9.27555
Epoch 16/80: current_loss=9.92594 | best_loss=9.27555
Epoch 17/80: current_loss=9.42360 | best_loss=9.27555
Epoch 18/80: current_loss=9.41415 | best_loss=9.27555
Epoch 19/80: current_loss=9.69674 | best_loss=9.27555
Epoch 20/80: current_loss=9.56371 | best_loss=9.27555
Epoch 21/80: current_loss=9.49849 | best_loss=9.27555
Epoch 22/80: current_loss=10.72653 | best_loss=9.27555
Early Stopping at epoch 22
      explained_var=-0.00251 | mse_loss=9.40095
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.01522 | best_loss=9.01522
Epoch 1/80: current_loss=9.16673 | best_loss=9.01522
Epoch 2/80: current_loss=9.04543 | best_loss=9.01522
Epoch 3/80: current_loss=9.28552 | best_loss=9.01522
Epoch 4/80: current_loss=8.93478 | best_loss=8.93478
Epoch 5/80: current_loss=9.01804 | best_loss=8.93478
Epoch 6/80: current_loss=8.95655 | best_loss=8.93478
Epoch 7/80: current_loss=8.98003 | best_loss=8.93478
Epoch 8/80: current_loss=10.57065 | best_loss=8.93478
Epoch 9/80: current_loss=9.33129 | best_loss=8.93478
Epoch 10/80: current_loss=9.60445 | best_loss=8.93478
Epoch 11/80: current_loss=9.66106 | best_loss=8.93478
Epoch 12/80: current_loss=9.10190 | best_loss=8.93478
Epoch 13/80: current_loss=10.06069 | best_loss=8.93478
Epoch 14/80: current_loss=9.36025 | best_loss=8.93478
Epoch 15/80: current_loss=9.26257 | best_loss=8.93478
Epoch 16/80: current_loss=9.20979 | best_loss=8.93478
Epoch 17/80: current_loss=9.09127 | best_loss=8.93478
Epoch 18/80: current_loss=8.99075 | best_loss=8.93478
Epoch 19/80: current_loss=9.42925 | best_loss=8.93478
Epoch 20/80: current_loss=9.49891 | best_loss=8.93478
Epoch 21/80: current_loss=9.08528 | best_loss=8.93478
Epoch 22/80: current_loss=8.97790 | best_loss=8.93478
Epoch 23/80: current_loss=9.14346 | best_loss=8.93478
Epoch 24/80: current_loss=9.19494 | best_loss=8.93478
Early Stopping at epoch 24
      explained_var=0.00113 | mse_loss=8.35657
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.53815 | best_loss=8.53815
Epoch 1/80: current_loss=8.89166 | best_loss=8.53815
Epoch 2/80: current_loss=8.70135 | best_loss=8.53815
Epoch 3/80: current_loss=8.49526 | best_loss=8.49526
Epoch 4/80: current_loss=8.79349 | best_loss=8.49526
Epoch 5/80: current_loss=8.50773 | best_loss=8.49526
Epoch 6/80: current_loss=8.65698 | best_loss=8.49526
Epoch 7/80: current_loss=8.61078 | best_loss=8.49526
Epoch 8/80: current_loss=8.78011 | best_loss=8.49526
Epoch 9/80: current_loss=8.83354 | best_loss=8.49526
Epoch 10/80: current_loss=8.46546 | best_loss=8.46546
Epoch 11/80: current_loss=8.53516 | best_loss=8.46546
Epoch 12/80: current_loss=8.55478 | best_loss=8.46546
Epoch 13/80: current_loss=8.87956 | best_loss=8.46546
Epoch 14/80: current_loss=8.60308 | best_loss=8.46546
Epoch 15/80: current_loss=8.82552 | best_loss=8.46546
Epoch 16/80: current_loss=8.56594 | best_loss=8.46546
Epoch 17/80: current_loss=8.54149 | best_loss=8.46546
Epoch 18/80: current_loss=8.49087 | best_loss=8.46546
Epoch 19/80: current_loss=8.52609 | best_loss=8.46546
Epoch 20/80: current_loss=8.78230 | best_loss=8.46546
Epoch 21/80: current_loss=8.82583 | best_loss=8.46546
Epoch 22/80: current_loss=8.89351 | best_loss=8.46546
Epoch 23/80: current_loss=8.93581 | best_loss=8.46546
Epoch 24/80: current_loss=8.57227 | best_loss=8.46546
Epoch 25/80: current_loss=10.77177 | best_loss=8.46546
Epoch 26/80: current_loss=8.50063 | best_loss=8.46546
Epoch 27/80: current_loss=8.69472 | best_loss=8.46546
Epoch 28/80: current_loss=8.78340 | best_loss=8.46546
Epoch 29/80: current_loss=8.49800 | best_loss=8.46546
Epoch 30/80: current_loss=8.68674 | best_loss=8.46546
Early Stopping at epoch 30
      explained_var=0.00010 | mse_loss=8.19965
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=0.00075| avg_loss=8.81453
----------------------------------------------


----------------------------------------------
Params for Trial 90
{'learning_rate': 0.01, 'weight_decay': 0.0021209309251191827, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.00570 | best_loss=10.00570
Epoch 1/80: current_loss=9.97413 | best_loss=9.97413
Epoch 2/80: current_loss=10.01422 | best_loss=9.97413
Epoch 3/80: current_loss=10.07353 | best_loss=9.97413
Epoch 4/80: current_loss=10.09681 | best_loss=9.97413
Epoch 5/80: current_loss=9.94488 | best_loss=9.94488
Epoch 6/80: current_loss=9.91344 | best_loss=9.91344
Epoch 7/80: current_loss=11.00640 | best_loss=9.91344
Epoch 8/80: current_loss=10.11540 | best_loss=9.91344
Epoch 9/80: current_loss=10.06519 | best_loss=9.91344
Epoch 10/80: current_loss=10.00490 | best_loss=9.91344
Epoch 11/80: current_loss=10.29289 | best_loss=9.91344
Epoch 12/80: current_loss=10.48659 | best_loss=9.91344
Epoch 13/80: current_loss=10.57284 | best_loss=9.91344
Epoch 14/80: current_loss=9.91206 | best_loss=9.91206
Epoch 15/80: current_loss=9.91613 | best_loss=9.91206
Epoch 16/80: current_loss=11.01862 | best_loss=9.91206
Epoch 17/80: current_loss=10.44834 | best_loss=9.91206
Epoch 18/80: current_loss=10.32551 | best_loss=9.91206
Epoch 19/80: current_loss=10.21984 | best_loss=9.91206
Epoch 20/80: current_loss=9.89414 | best_loss=9.89414
Epoch 21/80: current_loss=9.98835 | best_loss=9.89414
Epoch 22/80: current_loss=10.59541 | best_loss=9.89414
Epoch 23/80: current_loss=9.90938 | best_loss=9.89414
Epoch 24/80: current_loss=9.89411 | best_loss=9.89411
Epoch 25/80: current_loss=9.94497 | best_loss=9.89411
Epoch 26/80: current_loss=11.25425 | best_loss=9.89411
Epoch 27/80: current_loss=10.01075 | best_loss=9.89411
Epoch 28/80: current_loss=9.90264 | best_loss=9.89411
Epoch 29/80: current_loss=10.97107 | best_loss=9.89411
Epoch 30/80: current_loss=9.90559 | best_loss=9.89411
Epoch 31/80: current_loss=10.19404 | best_loss=9.89411
Epoch 32/80: current_loss=10.04817 | best_loss=9.89411
Epoch 33/80: current_loss=10.15135 | best_loss=9.89411
Epoch 34/80: current_loss=9.94494 | best_loss=9.89411
Epoch 35/80: current_loss=10.03069 | best_loss=9.89411
Epoch 36/80: current_loss=10.57797 | best_loss=9.89411
Epoch 37/80: current_loss=10.36014 | best_loss=9.89411
Epoch 38/80: current_loss=9.89985 | best_loss=9.89411
Epoch 39/80: current_loss=10.76980 | best_loss=9.89411
Epoch 40/80: current_loss=9.92087 | best_loss=9.89411
Epoch 41/80: current_loss=10.05772 | best_loss=9.89411
Epoch 42/80: current_loss=10.27808 | best_loss=9.89411
Epoch 43/80: current_loss=10.00746 | best_loss=9.89411
Epoch 44/80: current_loss=9.98089 | best_loss=9.89411
Early Stopping at epoch 44
      explained_var=0.00256 | mse_loss=9.55352
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.61029 | best_loss=8.61029
Epoch 1/80: current_loss=9.00774 | best_loss=8.61029
Epoch 2/80: current_loss=8.53123 | best_loss=8.53123
Epoch 3/80: current_loss=8.50908 | best_loss=8.50908
Epoch 4/80: current_loss=9.20409 | best_loss=8.50908
Epoch 5/80: current_loss=8.83190 | best_loss=8.50908
Epoch 6/80: current_loss=8.78913 | best_loss=8.50908
Epoch 7/80: current_loss=8.94698 | best_loss=8.50908
Epoch 8/80: current_loss=8.63245 | best_loss=8.50908
Epoch 9/80: current_loss=8.81227 | best_loss=8.50908
Epoch 10/80: current_loss=8.59515 | best_loss=8.50908
Epoch 11/80: current_loss=8.63581 | best_loss=8.50908
Epoch 12/80: current_loss=8.55985 | best_loss=8.50908
Epoch 13/80: current_loss=8.50281 | best_loss=8.50281
Epoch 14/80: current_loss=8.49956 | best_loss=8.49956
Epoch 15/80: current_loss=8.47611 | best_loss=8.47611
Epoch 16/80: current_loss=9.03618 | best_loss=8.47611
Epoch 17/80: current_loss=8.59692 | best_loss=8.47611
Epoch 18/80: current_loss=9.32877 | best_loss=8.47611
Epoch 19/80: current_loss=8.89294 | best_loss=8.47611
Epoch 20/80: current_loss=8.48460 | best_loss=8.47611
Epoch 21/80: current_loss=9.32799 | best_loss=8.47611
Epoch 22/80: current_loss=8.86295 | best_loss=8.47611
Epoch 23/80: current_loss=8.85240 | best_loss=8.47611
Epoch 24/80: current_loss=9.63544 | best_loss=8.47611
Epoch 25/80: current_loss=8.57945 | best_loss=8.47611
Epoch 26/80: current_loss=8.91067 | best_loss=8.47611
Epoch 27/80: current_loss=9.06386 | best_loss=8.47611
Epoch 28/80: current_loss=8.54818 | best_loss=8.47611
Epoch 29/80: current_loss=8.49209 | best_loss=8.47611
Epoch 30/80: current_loss=8.82316 | best_loss=8.47611
Epoch 31/80: current_loss=8.67288 | best_loss=8.47611
Epoch 32/80: current_loss=8.95438 | best_loss=8.47611
Epoch 33/80: current_loss=8.70028 | best_loss=8.47611
Epoch 34/80: current_loss=8.54849 | best_loss=8.47611
Epoch 35/80: current_loss=8.88919 | best_loss=8.47611
Early Stopping at epoch 35
      explained_var=0.00370 | mse_loss=8.54541
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.24832 | best_loss=9.24832
Epoch 1/80: current_loss=9.85970 | best_loss=9.24832
Epoch 2/80: current_loss=9.87515 | best_loss=9.24832
Epoch 3/80: current_loss=9.45315 | best_loss=9.24832
Epoch 4/80: current_loss=9.38592 | best_loss=9.24832
Epoch 5/80: current_loss=9.70991 | best_loss=9.24832
Epoch 6/80: current_loss=9.63023 | best_loss=9.24832
Epoch 7/80: current_loss=9.21247 | best_loss=9.21247
Epoch 8/80: current_loss=9.43449 | best_loss=9.21247
Epoch 9/80: current_loss=9.27389 | best_loss=9.21247
Epoch 10/80: current_loss=9.47952 | best_loss=9.21247
Epoch 11/80: current_loss=9.90100 | best_loss=9.21247
Epoch 12/80: current_loss=9.47253 | best_loss=9.21247
Epoch 13/80: current_loss=9.56096 | best_loss=9.21247
Epoch 14/80: current_loss=9.76834 | best_loss=9.21247
Epoch 15/80: current_loss=10.11731 | best_loss=9.21247
Epoch 16/80: current_loss=9.37306 | best_loss=9.21247
Epoch 17/80: current_loss=9.46003 | best_loss=9.21247
Epoch 18/80: current_loss=9.59077 | best_loss=9.21247
Epoch 19/80: current_loss=9.74631 | best_loss=9.21247
Epoch 20/80: current_loss=9.23615 | best_loss=9.21247
Epoch 21/80: current_loss=10.49850 | best_loss=9.21247
Epoch 22/80: current_loss=10.77519 | best_loss=9.21247
Epoch 23/80: current_loss=9.82435 | best_loss=9.21247
Epoch 24/80: current_loss=9.35138 | best_loss=9.21247
Epoch 25/80: current_loss=9.32624 | best_loss=9.21247
Epoch 26/80: current_loss=9.41491 | best_loss=9.21247
Epoch 27/80: current_loss=9.38557 | best_loss=9.21247
Early Stopping at epoch 27
      explained_var=0.01252 | mse_loss=9.30460
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.97659 | best_loss=8.97659
Epoch 1/80: current_loss=8.84728 | best_loss=8.84728
Epoch 2/80: current_loss=9.04901 | best_loss=8.84728
Epoch 3/80: current_loss=8.99228 | best_loss=8.84728
Epoch 4/80: current_loss=9.05765 | best_loss=8.84728
Epoch 5/80: current_loss=9.26761 | best_loss=8.84728
Epoch 6/80: current_loss=9.13007 | best_loss=8.84728
Epoch 7/80: current_loss=8.98357 | best_loss=8.84728
Epoch 8/80: current_loss=8.98894 | best_loss=8.84728
Epoch 9/80: current_loss=8.95827 | best_loss=8.84728
Epoch 10/80: current_loss=8.97954 | best_loss=8.84728
Epoch 11/80: current_loss=9.21596 | best_loss=8.84728
Epoch 12/80: current_loss=8.90747 | best_loss=8.84728
Epoch 13/80: current_loss=8.99269 | best_loss=8.84728
Epoch 14/80: current_loss=9.75640 | best_loss=8.84728
Epoch 15/80: current_loss=8.93067 | best_loss=8.84728
Epoch 16/80: current_loss=9.00451 | best_loss=8.84728
Epoch 17/80: current_loss=8.93235 | best_loss=8.84728
Epoch 18/80: current_loss=9.74980 | best_loss=8.84728
Epoch 19/80: current_loss=9.21369 | best_loss=8.84728
Epoch 20/80: current_loss=9.20723 | best_loss=8.84728
Epoch 21/80: current_loss=9.18617 | best_loss=8.84728
Early Stopping at epoch 21
      explained_var=0.00594 | mse_loss=8.31331
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.15357 | best_loss=9.15357
Epoch 1/80: current_loss=9.10743 | best_loss=9.10743
Epoch 2/80: current_loss=8.80296 | best_loss=8.80296
Epoch 3/80: current_loss=8.66888 | best_loss=8.66888
Epoch 4/80: current_loss=8.86156 | best_loss=8.66888
Epoch 5/80: current_loss=8.83997 | best_loss=8.66888
Epoch 6/80: current_loss=8.60645 | best_loss=8.60645
Epoch 7/80: current_loss=8.75642 | best_loss=8.60645
Epoch 8/80: current_loss=8.66805 | best_loss=8.60645
Epoch 9/80: current_loss=8.64554 | best_loss=8.60645
Epoch 10/80: current_loss=8.59876 | best_loss=8.59876
Epoch 11/80: current_loss=8.54907 | best_loss=8.54907
Epoch 12/80: current_loss=8.60648 | best_loss=8.54907
Epoch 13/80: current_loss=8.66450 | best_loss=8.54907
Epoch 14/80: current_loss=8.48559 | best_loss=8.48559
Epoch 15/80: current_loss=8.53140 | best_loss=8.48559
Epoch 16/80: current_loss=8.79875 | best_loss=8.48559
Epoch 17/80: current_loss=8.53911 | best_loss=8.48559
Epoch 18/80: current_loss=9.58566 | best_loss=8.48559
Epoch 19/80: current_loss=8.59536 | best_loss=8.48559
Epoch 20/80: current_loss=8.52712 | best_loss=8.48559
Epoch 21/80: current_loss=8.59208 | best_loss=8.48559
Epoch 22/80: current_loss=8.86597 | best_loss=8.48559
Epoch 23/80: current_loss=8.90309 | best_loss=8.48559
Epoch 24/80: current_loss=9.76147 | best_loss=8.48559
Epoch 25/80: current_loss=8.58301 | best_loss=8.48559
Epoch 26/80: current_loss=8.82765 | best_loss=8.48559
Epoch 27/80: current_loss=8.52175 | best_loss=8.48559
Epoch 28/80: current_loss=9.31675 | best_loss=8.48559
Epoch 29/80: current_loss=8.63460 | best_loss=8.48559
Epoch 30/80: current_loss=8.65203 | best_loss=8.48559
Epoch 31/80: current_loss=8.59312 | best_loss=8.48559
Epoch 32/80: current_loss=8.55902 | best_loss=8.48559
Epoch 33/80: current_loss=8.66644 | best_loss=8.48559
Epoch 34/80: current_loss=8.55195 | best_loss=8.48559
Early Stopping at epoch 34
      explained_var=0.00137 | mse_loss=8.19828
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=0.00522| avg_loss=8.78302
----------------------------------------------


----------------------------------------------
Params for Trial 91
{'learning_rate': 0.01, 'weight_decay': 0.0021470861716088715, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.40168 | best_loss=10.40168
Epoch 1/80: current_loss=9.91523 | best_loss=9.91523
Epoch 2/80: current_loss=10.09244 | best_loss=9.91523
Epoch 3/80: current_loss=10.01903 | best_loss=9.91523
Epoch 4/80: current_loss=9.97468 | best_loss=9.91523
Epoch 5/80: current_loss=10.07542 | best_loss=9.91523
Epoch 6/80: current_loss=9.92503 | best_loss=9.91523
Epoch 7/80: current_loss=9.92543 | best_loss=9.91523
Epoch 8/80: current_loss=10.16127 | best_loss=9.91523
Epoch 9/80: current_loss=9.90748 | best_loss=9.90748
Epoch 10/80: current_loss=10.13967 | best_loss=9.90748
Epoch 11/80: current_loss=9.93171 | best_loss=9.90748
Epoch 12/80: current_loss=9.90894 | best_loss=9.90748
Epoch 13/80: current_loss=10.55995 | best_loss=9.90748
Epoch 14/80: current_loss=10.15044 | best_loss=9.90748
Epoch 15/80: current_loss=9.89720 | best_loss=9.89720
Epoch 16/80: current_loss=9.96386 | best_loss=9.89720
Epoch 17/80: current_loss=10.57909 | best_loss=9.89720
Epoch 18/80: current_loss=10.02966 | best_loss=9.89720
Epoch 19/80: current_loss=9.90569 | best_loss=9.89720
Epoch 20/80: current_loss=10.56537 | best_loss=9.89720
Epoch 21/80: current_loss=9.96190 | best_loss=9.89720
Epoch 22/80: current_loss=9.90243 | best_loss=9.89720
Epoch 23/80: current_loss=10.28717 | best_loss=9.89720
Epoch 24/80: current_loss=10.05087 | best_loss=9.89720
Epoch 25/80: current_loss=10.37784 | best_loss=9.89720
Epoch 26/80: current_loss=10.64587 | best_loss=9.89720
Epoch 27/80: current_loss=10.47302 | best_loss=9.89720
Epoch 28/80: current_loss=9.89531 | best_loss=9.89531
Epoch 29/80: current_loss=10.31319 | best_loss=9.89531
Epoch 30/80: current_loss=9.94386 | best_loss=9.89531
Epoch 31/80: current_loss=9.91686 | best_loss=9.89531
Epoch 32/80: current_loss=10.04417 | best_loss=9.89531
Epoch 33/80: current_loss=10.13284 | best_loss=9.89531
Epoch 34/80: current_loss=9.92855 | best_loss=9.89531
Epoch 35/80: current_loss=10.07710 | best_loss=9.89531
Epoch 36/80: current_loss=9.95005 | best_loss=9.89531
Epoch 37/80: current_loss=9.96755 | best_loss=9.89531
Epoch 38/80: current_loss=10.22493 | best_loss=9.89531
Epoch 39/80: current_loss=9.95187 | best_loss=9.89531
Epoch 40/80: current_loss=9.89984 | best_loss=9.89531
Epoch 41/80: current_loss=10.83149 | best_loss=9.89531
Epoch 42/80: current_loss=9.90664 | best_loss=9.89531
Epoch 43/80: current_loss=10.19308 | best_loss=9.89531
Epoch 44/80: current_loss=9.96008 | best_loss=9.89531
Epoch 45/80: current_loss=10.18754 | best_loss=9.89531
Epoch 46/80: current_loss=9.92092 | best_loss=9.89531
Epoch 47/80: current_loss=9.95991 | best_loss=9.89531
Epoch 48/80: current_loss=10.53255 | best_loss=9.89531
Early Stopping at epoch 48
      explained_var=0.00167 | mse_loss=9.56908

----------------------------------------------
Params for Trial 92
{'learning_rate': 0.01, 'weight_decay': 0.0026650773342301727, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.96183 | best_loss=9.96183
Epoch 1/80: current_loss=9.95236 | best_loss=9.95236
Epoch 2/80: current_loss=10.25105 | best_loss=9.95236
Epoch 3/80: current_loss=9.93646 | best_loss=9.93646
Epoch 4/80: current_loss=9.93314 | best_loss=9.93314
Epoch 5/80: current_loss=9.98081 | best_loss=9.93314
Epoch 6/80: current_loss=10.01789 | best_loss=9.93314
Epoch 7/80: current_loss=9.91682 | best_loss=9.91682
Epoch 8/80: current_loss=10.26336 | best_loss=9.91682
Epoch 9/80: current_loss=10.02867 | best_loss=9.91682
Epoch 10/80: current_loss=9.97347 | best_loss=9.91682
Epoch 11/80: current_loss=10.88713 | best_loss=9.91682
Epoch 12/80: current_loss=9.95667 | best_loss=9.91682
Epoch 13/80: current_loss=9.90652 | best_loss=9.90652
Epoch 14/80: current_loss=9.90503 | best_loss=9.90503
Epoch 15/80: current_loss=10.09480 | best_loss=9.90503
Epoch 16/80: current_loss=10.74355 | best_loss=9.90503
Epoch 17/80: current_loss=10.09067 | best_loss=9.90503
Epoch 18/80: current_loss=10.10766 | best_loss=9.90503
Epoch 19/80: current_loss=10.05063 | best_loss=9.90503
Epoch 20/80: current_loss=9.92199 | best_loss=9.90503
Epoch 21/80: current_loss=10.04129 | best_loss=9.90503
Epoch 22/80: current_loss=10.03108 | best_loss=9.90503
Epoch 23/80: current_loss=9.89206 | best_loss=9.89206
Epoch 24/80: current_loss=10.03253 | best_loss=9.89206
Epoch 25/80: current_loss=9.92669 | best_loss=9.89206
Epoch 26/80: current_loss=10.90912 | best_loss=9.89206
Epoch 27/80: current_loss=10.01809 | best_loss=9.89206
Epoch 28/80: current_loss=10.23821 | best_loss=9.89206
Epoch 29/80: current_loss=9.90033 | best_loss=9.89206
Epoch 30/80: current_loss=10.19486 | best_loss=9.89206
Epoch 31/80: current_loss=9.94130 | best_loss=9.89206
Epoch 32/80: current_loss=10.29455 | best_loss=9.89206
Epoch 33/80: current_loss=10.22308 | best_loss=9.89206
Epoch 34/80: current_loss=9.94918 | best_loss=9.89206
Epoch 35/80: current_loss=10.04402 | best_loss=9.89206
Epoch 36/80: current_loss=10.04104 | best_loss=9.89206
Epoch 37/80: current_loss=10.42258 | best_loss=9.89206
Epoch 38/80: current_loss=10.88531 | best_loss=9.89206
Epoch 39/80: current_loss=10.10282 | best_loss=9.89206
Epoch 40/80: current_loss=9.91170 | best_loss=9.89206
Epoch 41/80: current_loss=10.17029 | best_loss=9.89206
Epoch 42/80: current_loss=9.91943 | best_loss=9.89206
Epoch 43/80: current_loss=10.50209 | best_loss=9.89206
Early Stopping at epoch 43
      explained_var=0.00190 | mse_loss=9.56401
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.59792 | best_loss=8.59792
Epoch 1/80: current_loss=9.19361 | best_loss=8.59792
Epoch 2/80: current_loss=9.07911 | best_loss=8.59792
Epoch 3/80: current_loss=8.56296 | best_loss=8.56296
Epoch 4/80: current_loss=8.73584 | best_loss=8.56296
Epoch 5/80: current_loss=8.67676 | best_loss=8.56296
Epoch 6/80: current_loss=8.65632 | best_loss=8.56296
Epoch 7/80: current_loss=8.73344 | best_loss=8.56296
Epoch 8/80: current_loss=8.60776 | best_loss=8.56296
Epoch 9/80: current_loss=8.58541 | best_loss=8.56296
Epoch 10/80: current_loss=8.58287 | best_loss=8.56296
Epoch 11/80: current_loss=8.50810 | best_loss=8.50810
Epoch 12/80: current_loss=8.76468 | best_loss=8.50810
Epoch 13/80: current_loss=8.60292 | best_loss=8.50810
Epoch 14/80: current_loss=8.59513 | best_loss=8.50810
Epoch 15/80: current_loss=8.50403 | best_loss=8.50403
Epoch 16/80: current_loss=8.60849 | best_loss=8.50403
Epoch 17/80: current_loss=8.61830 | best_loss=8.50403
Epoch 18/80: current_loss=8.65059 | best_loss=8.50403
Epoch 19/80: current_loss=8.45066 | best_loss=8.45066
Epoch 20/80: current_loss=8.53037 | best_loss=8.45066
Epoch 21/80: current_loss=8.37130 | best_loss=8.37130
Epoch 22/80: current_loss=8.52922 | best_loss=8.37130
Epoch 23/80: current_loss=8.64253 | best_loss=8.37130
Epoch 24/80: current_loss=8.66681 | best_loss=8.37130
Epoch 25/80: current_loss=8.69975 | best_loss=8.37130
Epoch 26/80: current_loss=8.73904 | best_loss=8.37130
Epoch 27/80: current_loss=8.60517 | best_loss=8.37130
Epoch 28/80: current_loss=8.87490 | best_loss=8.37130
Epoch 29/80: current_loss=8.62204 | best_loss=8.37130
Epoch 30/80: current_loss=8.53132 | best_loss=8.37130
Epoch 31/80: current_loss=8.53109 | best_loss=8.37130
Epoch 32/80: current_loss=8.74174 | best_loss=8.37130
Epoch 33/80: current_loss=8.77175 | best_loss=8.37130
Epoch 34/80: current_loss=8.50035 | best_loss=8.37130
Epoch 35/80: current_loss=8.52466 | best_loss=8.37130
Epoch 36/80: current_loss=8.55987 | best_loss=8.37130
Epoch 37/80: current_loss=8.55602 | best_loss=8.37130
Epoch 38/80: current_loss=9.01085 | best_loss=8.37130
Epoch 39/80: current_loss=8.57583 | best_loss=8.37130
Epoch 40/80: current_loss=8.51174 | best_loss=8.37130
Epoch 41/80: current_loss=8.58537 | best_loss=8.37130
Early Stopping at epoch 41
      explained_var=0.00926 | mse_loss=8.49874
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.25914 | best_loss=9.25914
Epoch 1/80: current_loss=9.84014 | best_loss=9.25914
Epoch 2/80: current_loss=9.40809 | best_loss=9.25914
Epoch 3/80: current_loss=9.20941 | best_loss=9.20941
Epoch 4/80: current_loss=9.39931 | best_loss=9.20941
Epoch 5/80: current_loss=10.30735 | best_loss=9.20941
Epoch 6/80: current_loss=9.29953 | best_loss=9.20941
Epoch 7/80: current_loss=9.52747 | best_loss=9.20941
Epoch 8/80: current_loss=9.63897 | best_loss=9.20941
Epoch 9/80: current_loss=9.27519 | best_loss=9.20941
Epoch 10/80: current_loss=9.32188 | best_loss=9.20941
Epoch 11/80: current_loss=9.28367 | best_loss=9.20941
Epoch 12/80: current_loss=9.50957 | best_loss=9.20941
Epoch 13/80: current_loss=9.35778 | best_loss=9.20941
Epoch 14/80: current_loss=9.36313 | best_loss=9.20941
Epoch 15/80: current_loss=9.82587 | best_loss=9.20941
Epoch 16/80: current_loss=9.34699 | best_loss=9.20941
Epoch 17/80: current_loss=9.37093 | best_loss=9.20941
Epoch 18/80: current_loss=9.55921 | best_loss=9.20941
Epoch 19/80: current_loss=9.44348 | best_loss=9.20941
Epoch 20/80: current_loss=9.27426 | best_loss=9.20941
Epoch 21/80: current_loss=9.61135 | best_loss=9.20941
Epoch 22/80: current_loss=9.28678 | best_loss=9.20941
Epoch 23/80: current_loss=9.31578 | best_loss=9.20941
Early Stopping at epoch 23
      explained_var=0.01332 | mse_loss=9.26388
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.93302 | best_loss=8.93302
Epoch 1/80: current_loss=8.97577 | best_loss=8.93302
Epoch 2/80: current_loss=8.91432 | best_loss=8.91432
Epoch 3/80: current_loss=9.15200 | best_loss=8.91432
Epoch 4/80: current_loss=10.67486 | best_loss=8.91432
Epoch 5/80: current_loss=9.07196 | best_loss=8.91432
Epoch 6/80: current_loss=9.28577 | best_loss=8.91432
Epoch 7/80: current_loss=8.96014 | best_loss=8.91432
Epoch 8/80: current_loss=8.92965 | best_loss=8.91432
Epoch 9/80: current_loss=8.95942 | best_loss=8.91432
Epoch 10/80: current_loss=8.93597 | best_loss=8.91432
Epoch 11/80: current_loss=9.36878 | best_loss=8.91432
Epoch 12/80: current_loss=9.10743 | best_loss=8.91432
Epoch 13/80: current_loss=8.98693 | best_loss=8.91432
Epoch 14/80: current_loss=9.11989 | best_loss=8.91432
Epoch 15/80: current_loss=9.03321 | best_loss=8.91432
Epoch 16/80: current_loss=8.99217 | best_loss=8.91432
Epoch 17/80: current_loss=9.15506 | best_loss=8.91432
Epoch 18/80: current_loss=9.05741 | best_loss=8.91432
Epoch 19/80: current_loss=8.98469 | best_loss=8.91432
Epoch 20/80: current_loss=9.03636 | best_loss=8.91432
Epoch 21/80: current_loss=8.99767 | best_loss=8.91432
Epoch 22/80: current_loss=9.49497 | best_loss=8.91432
Early Stopping at epoch 22
      explained_var=0.00089 | mse_loss=8.35500
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50201 | best_loss=8.50201
Epoch 1/80: current_loss=8.70938 | best_loss=8.50201
Epoch 2/80: current_loss=8.48284 | best_loss=8.48284
Epoch 3/80: current_loss=8.70734 | best_loss=8.48284
Epoch 4/80: current_loss=8.51717 | best_loss=8.48284
Epoch 5/80: current_loss=8.67572 | best_loss=8.48284
Epoch 6/80: current_loss=8.69018 | best_loss=8.48284
Epoch 7/80: current_loss=9.00130 | best_loss=8.48284
Epoch 8/80: current_loss=9.40427 | best_loss=8.48284
Epoch 9/80: current_loss=8.73631 | best_loss=8.48284
Epoch 10/80: current_loss=8.62533 | best_loss=8.48284
Epoch 11/80: current_loss=8.77152 | best_loss=8.48284
Epoch 12/80: current_loss=8.52151 | best_loss=8.48284
Epoch 13/80: current_loss=8.80190 | best_loss=8.48284
Epoch 14/80: current_loss=8.55208 | best_loss=8.48284
Epoch 15/80: current_loss=8.57430 | best_loss=8.48284
Epoch 16/80: current_loss=8.60067 | best_loss=8.48284
Epoch 17/80: current_loss=9.28128 | best_loss=8.48284
Epoch 18/80: current_loss=8.55731 | best_loss=8.48284
Epoch 19/80: current_loss=8.65309 | best_loss=8.48284
Epoch 20/80: current_loss=9.31598 | best_loss=8.48284
Epoch 21/80: current_loss=9.10887 | best_loss=8.48284
Epoch 22/80: current_loss=8.63426 | best_loss=8.48284
Early Stopping at epoch 22
      explained_var=-0.00013 | mse_loss=8.20172

----------------------------------------------
Params for Trial 93
{'learning_rate': 0.01, 'weight_decay': 0.0015176388386038634, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91306 | best_loss=9.91306
Epoch 1/80: current_loss=10.01372 | best_loss=9.91306
Epoch 2/80: current_loss=10.30001 | best_loss=9.91306
Epoch 3/80: current_loss=9.97071 | best_loss=9.91306
Epoch 4/80: current_loss=9.95814 | best_loss=9.91306
Epoch 5/80: current_loss=9.99790 | best_loss=9.91306
Epoch 6/80: current_loss=10.04118 | best_loss=9.91306
Epoch 7/80: current_loss=9.93570 | best_loss=9.91306
Epoch 8/80: current_loss=11.11828 | best_loss=9.91306
Epoch 9/80: current_loss=10.21020 | best_loss=9.91306
Epoch 10/80: current_loss=10.21333 | best_loss=9.91306
Epoch 11/80: current_loss=10.09656 | best_loss=9.91306
Epoch 12/80: current_loss=9.90817 | best_loss=9.90817
Epoch 13/80: current_loss=9.90915 | best_loss=9.90817
Epoch 14/80: current_loss=11.29525 | best_loss=9.90817
Epoch 15/80: current_loss=9.93662 | best_loss=9.90817
Epoch 16/80: current_loss=9.99452 | best_loss=9.90817
Epoch 17/80: current_loss=9.92572 | best_loss=9.90817
Epoch 18/80: current_loss=9.91185 | best_loss=9.90817
Epoch 19/80: current_loss=10.74036 | best_loss=9.90817
Epoch 20/80: current_loss=10.60282 | best_loss=9.90817
Epoch 21/80: current_loss=10.09080 | best_loss=9.90817
Epoch 22/80: current_loss=9.91273 | best_loss=9.90817
Epoch 23/80: current_loss=10.03855 | best_loss=9.90817
Epoch 24/80: current_loss=10.11919 | best_loss=9.90817
Epoch 25/80: current_loss=10.89961 | best_loss=9.90817
Epoch 26/80: current_loss=10.35210 | best_loss=9.90817
Epoch 27/80: current_loss=9.96945 | best_loss=9.90817
Epoch 28/80: current_loss=10.26962 | best_loss=9.90817
Epoch 29/80: current_loss=9.91349 | best_loss=9.90817
Epoch 30/80: current_loss=10.02826 | best_loss=9.90817
Epoch 31/80: current_loss=10.26038 | best_loss=9.90817
Epoch 32/80: current_loss=10.26737 | best_loss=9.90817
Early Stopping at epoch 32
      explained_var=0.00057 | mse_loss=9.57226

----------------------------------------------
Params for Trial 94
{'learning_rate': 0.01, 'weight_decay': 0.0018388801809352633, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.99254 | best_loss=9.99254
Epoch 1/80: current_loss=9.98582 | best_loss=9.98582
Epoch 2/80: current_loss=10.41319 | best_loss=9.98582
Epoch 3/80: current_loss=9.95794 | best_loss=9.95794
Epoch 4/80: current_loss=9.94524 | best_loss=9.94524
Epoch 5/80: current_loss=10.33885 | best_loss=9.94524
Epoch 6/80: current_loss=10.16355 | best_loss=9.94524
Epoch 7/80: current_loss=9.92249 | best_loss=9.92249
Epoch 8/80: current_loss=10.49468 | best_loss=9.92249
Epoch 9/80: current_loss=10.12215 | best_loss=9.92249
Epoch 10/80: current_loss=9.93264 | best_loss=9.92249
Epoch 11/80: current_loss=10.22938 | best_loss=9.92249
Epoch 12/80: current_loss=9.93742 | best_loss=9.92249
Epoch 13/80: current_loss=9.98279 | best_loss=9.92249
Epoch 14/80: current_loss=10.17473 | best_loss=9.92249
Epoch 15/80: current_loss=9.96665 | best_loss=9.92249
Epoch 16/80: current_loss=9.91165 | best_loss=9.91165
Epoch 17/80: current_loss=10.06294 | best_loss=9.91165
Epoch 18/80: current_loss=9.90822 | best_loss=9.90822
Epoch 19/80: current_loss=10.12731 | best_loss=9.90822
Epoch 20/80: current_loss=9.91152 | best_loss=9.90822
Epoch 21/80: current_loss=9.91774 | best_loss=9.90822
Epoch 22/80: current_loss=9.95646 | best_loss=9.90822
Epoch 23/80: current_loss=9.99976 | best_loss=9.90822
Epoch 24/80: current_loss=9.94080 | best_loss=9.90822
Epoch 25/80: current_loss=10.07180 | best_loss=9.90822
Epoch 26/80: current_loss=11.06359 | best_loss=9.90822
Epoch 27/80: current_loss=9.96817 | best_loss=9.90822
Epoch 28/80: current_loss=10.09019 | best_loss=9.90822
Epoch 29/80: current_loss=10.29431 | best_loss=9.90822
Epoch 30/80: current_loss=9.90467 | best_loss=9.90467
Epoch 31/80: current_loss=11.08247 | best_loss=9.90467
Epoch 32/80: current_loss=9.92492 | best_loss=9.90467
Epoch 33/80: current_loss=10.22815 | best_loss=9.90467
Epoch 34/80: current_loss=9.90809 | best_loss=9.90467
Epoch 35/80: current_loss=9.93087 | best_loss=9.90467
Epoch 36/80: current_loss=10.04905 | best_loss=9.90467
Epoch 37/80: current_loss=9.90957 | best_loss=9.90467
Epoch 38/80: current_loss=9.90038 | best_loss=9.90038
Epoch 39/80: current_loss=9.94279 | best_loss=9.90038
Epoch 40/80: current_loss=10.40718 | best_loss=9.90038
Epoch 41/80: current_loss=9.96807 | best_loss=9.90038
Epoch 42/80: current_loss=10.05445 | best_loss=9.90038
Epoch 43/80: current_loss=11.64728 | best_loss=9.90038
Epoch 44/80: current_loss=10.29614 | best_loss=9.90038
Epoch 45/80: current_loss=10.14713 | best_loss=9.90038
Epoch 46/80: current_loss=9.93287 | best_loss=9.90038
Epoch 47/80: current_loss=9.93297 | best_loss=9.90038
Epoch 48/80: current_loss=10.04863 | best_loss=9.90038
Epoch 49/80: current_loss=10.11729 | best_loss=9.90038
Epoch 50/80: current_loss=9.93706 | best_loss=9.90038
Epoch 51/80: current_loss=11.16158 | best_loss=9.90038
Epoch 52/80: current_loss=10.04944 | best_loss=9.90038
Epoch 53/80: current_loss=9.96948 | best_loss=9.90038
Epoch 54/80: current_loss=9.90598 | best_loss=9.90038
Epoch 55/80: current_loss=9.95549 | best_loss=9.90038
Epoch 56/80: current_loss=10.72855 | best_loss=9.90038
Epoch 57/80: current_loss=9.91526 | best_loss=9.90038
Epoch 58/80: current_loss=10.07616 | best_loss=9.90038
Early Stopping at epoch 58
      explained_var=0.00081 | mse_loss=9.56998

----------------------------------------------
Params for Trial 95
{'learning_rate': 1e-05, 'weight_decay': 0.0029040791322175715, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=76.53933 | best_loss=76.53933
Epoch 1/80: current_loss=74.12034 | best_loss=74.12034
Epoch 2/80: current_loss=70.79059 | best_loss=70.79059
Epoch 3/80: current_loss=65.80386 | best_loss=65.80386
Epoch 4/80: current_loss=58.87166 | best_loss=58.87166
Epoch 5/80: current_loss=51.25047 | best_loss=51.25047
Epoch 6/80: current_loss=44.56629 | best_loss=44.56629
Epoch 7/80: current_loss=39.48246 | best_loss=39.48246
Epoch 8/80: current_loss=35.61433 | best_loss=35.61433
Epoch 9/80: current_loss=32.61488 | best_loss=32.61488
Epoch 10/80: current_loss=30.24072 | best_loss=30.24072
Epoch 11/80: current_loss=28.30505 | best_loss=28.30505
Epoch 12/80: current_loss=26.73634 | best_loss=26.73634
Epoch 13/80: current_loss=25.38377 | best_loss=25.38377
Epoch 14/80: current_loss=24.24321 | best_loss=24.24321
Epoch 15/80: current_loss=23.26963 | best_loss=23.26963
Epoch 16/80: current_loss=22.41255 | best_loss=22.41255
Epoch 17/80: current_loss=21.65427 | best_loss=21.65427
Epoch 18/80: current_loss=20.96675 | best_loss=20.96675
Epoch 19/80: current_loss=20.33417 | best_loss=20.33417
Epoch 20/80: current_loss=19.77213 | best_loss=19.77213
Epoch 21/80: current_loss=19.25894 | best_loss=19.25894
Epoch 22/80: current_loss=18.77935 | best_loss=18.77935
Epoch 23/80: current_loss=18.33046 | best_loss=18.33046
Epoch 24/80: current_loss=17.90797 | best_loss=17.90797
Epoch 25/80: current_loss=17.52126 | best_loss=17.52126
Epoch 26/80: current_loss=17.14805 | best_loss=17.14805
Epoch 27/80: current_loss=16.80266 | best_loss=16.80266
Epoch 28/80: current_loss=16.47935 | best_loss=16.47935
Epoch 29/80: current_loss=16.17246 | best_loss=16.17246
Epoch 30/80: current_loss=15.88157 | best_loss=15.88157
Epoch 31/80: current_loss=15.61259 | best_loss=15.61259
Epoch 32/80: current_loss=15.35165 | best_loss=15.35165
Epoch 33/80: current_loss=15.10479 | best_loss=15.10479
Epoch 34/80: current_loss=14.86922 | best_loss=14.86922
Epoch 35/80: current_loss=14.64139 | best_loss=14.64139
Epoch 36/80: current_loss=14.42670 | best_loss=14.42670
Epoch 37/80: current_loss=14.22274 | best_loss=14.22274
Epoch 38/80: current_loss=14.02290 | best_loss=14.02290
Epoch 39/80: current_loss=13.83815 | best_loss=13.83815
Epoch 40/80: current_loss=13.65542 | best_loss=13.65542
Epoch 41/80: current_loss=13.48166 | best_loss=13.48166
Epoch 42/80: current_loss=13.31791 | best_loss=13.31791
Epoch 43/80: current_loss=13.16118 | best_loss=13.16118
Epoch 44/80: current_loss=13.01105 | best_loss=13.01105
Epoch 45/80: current_loss=12.86408 | best_loss=12.86408
Epoch 46/80: current_loss=12.73498 | best_loss=12.73498
Epoch 47/80: current_loss=12.60931 | best_loss=12.60931
Epoch 48/80: current_loss=12.48341 | best_loss=12.48341
Epoch 49/80: current_loss=12.36242 | best_loss=12.36242
Epoch 50/80: current_loss=12.25213 | best_loss=12.25213
Epoch 51/80: current_loss=12.15025 | best_loss=12.15025
Epoch 52/80: current_loss=12.04166 | best_loss=12.04166
Epoch 53/80: current_loss=11.94525 | best_loss=11.94525
Epoch 54/80: current_loss=11.85258 | best_loss=11.85258
Epoch 55/80: current_loss=11.76821 | best_loss=11.76821
Epoch 56/80: current_loss=11.68346 | best_loss=11.68346
Epoch 57/80: current_loss=11.60234 | best_loss=11.60234
Epoch 58/80: current_loss=11.52558 | best_loss=11.52558
Epoch 59/80: current_loss=11.45441 | best_loss=11.45441
Epoch 60/80: current_loss=11.38412 | best_loss=11.38412
Epoch 61/80: current_loss=11.31747 | best_loss=11.31747
Epoch 62/80: current_loss=11.25656 | best_loss=11.25656
Epoch 63/80: current_loss=11.19393 | best_loss=11.19393
Epoch 64/80: current_loss=11.13879 | best_loss=11.13879
Epoch 65/80: current_loss=11.08569 | best_loss=11.08569
Epoch 66/80: current_loss=11.03403 | best_loss=11.03403
Epoch 67/80: current_loss=10.98555 | best_loss=10.98555
Epoch 68/80: current_loss=10.93673 | best_loss=10.93673
Epoch 69/80: current_loss=10.89147 | best_loss=10.89147
Epoch 70/80: current_loss=10.84922 | best_loss=10.84922
Epoch 71/80: current_loss=10.80854 | best_loss=10.80854
Epoch 72/80: current_loss=10.76723 | best_loss=10.76723
Epoch 73/80: current_loss=10.73168 | best_loss=10.73168
Epoch 74/80: current_loss=10.69751 | best_loss=10.69751
Epoch 75/80: current_loss=10.66539 | best_loss=10.66539
Epoch 76/80: current_loss=10.63237 | best_loss=10.63237
Epoch 77/80: current_loss=10.60418 | best_loss=10.60418
Epoch 78/80: current_loss=10.57415 | best_loss=10.57415
Epoch 79/80: current_loss=10.54689 | best_loss=10.54689
      explained_var=-0.03694 | mse_loss=10.05132

----------------------------------------------
Params for Trial 96
{'learning_rate': 0.01, 'weight_decay': 0.0009540954229792735, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.91743 | best_loss=9.91743
Epoch 1/80: current_loss=9.91084 | best_loss=9.91084
Epoch 2/80: current_loss=10.10540 | best_loss=9.91084
Epoch 3/80: current_loss=9.91833 | best_loss=9.91084
Epoch 4/80: current_loss=10.20944 | best_loss=9.91084
Epoch 5/80: current_loss=10.03127 | best_loss=9.91084
Epoch 6/80: current_loss=10.26708 | best_loss=9.91084
Epoch 7/80: current_loss=11.18863 | best_loss=9.91084
Epoch 8/80: current_loss=9.97099 | best_loss=9.91084
Epoch 9/80: current_loss=9.90471 | best_loss=9.90471
Epoch 10/80: current_loss=9.90375 | best_loss=9.90375
Epoch 11/80: current_loss=9.91549 | best_loss=9.90375
Epoch 12/80: current_loss=10.17617 | best_loss=9.90375
Epoch 13/80: current_loss=9.90964 | best_loss=9.90375
Epoch 14/80: current_loss=9.97206 | best_loss=9.90375
Epoch 15/80: current_loss=9.90387 | best_loss=9.90375
Epoch 16/80: current_loss=9.99234 | best_loss=9.90375
Epoch 17/80: current_loss=9.89991 | best_loss=9.89991
Epoch 18/80: current_loss=10.99781 | best_loss=9.89991
Epoch 19/80: current_loss=9.90148 | best_loss=9.89991
Epoch 20/80: current_loss=10.29403 | best_loss=9.89991
Epoch 21/80: current_loss=10.00354 | best_loss=9.89991
Epoch 22/80: current_loss=10.39915 | best_loss=9.89991
Epoch 23/80: current_loss=11.46421 | best_loss=9.89991
Epoch 24/80: current_loss=10.06017 | best_loss=9.89991
Epoch 25/80: current_loss=9.90515 | best_loss=9.89991
Epoch 26/80: current_loss=9.91380 | best_loss=9.89991
Epoch 27/80: current_loss=10.95778 | best_loss=9.89991
Epoch 28/80: current_loss=9.94096 | best_loss=9.89991
Epoch 29/80: current_loss=10.26245 | best_loss=9.89991
Epoch 30/80: current_loss=10.50886 | best_loss=9.89991
Epoch 31/80: current_loss=10.47823 | best_loss=9.89991
Epoch 32/80: current_loss=10.01325 | best_loss=9.89991
Epoch 33/80: current_loss=9.91139 | best_loss=9.89991
Epoch 34/80: current_loss=10.00877 | best_loss=9.89991
Epoch 35/80: current_loss=9.91598 | best_loss=9.89991
Epoch 36/80: current_loss=10.29422 | best_loss=9.89991
Epoch 37/80: current_loss=9.90152 | best_loss=9.89991
Early Stopping at epoch 37
      explained_var=0.00106 | mse_loss=9.56655
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.59999 | best_loss=8.59999
Epoch 1/80: current_loss=9.03608 | best_loss=8.59999
Epoch 2/80: current_loss=10.09056 | best_loss=8.59999
Epoch 3/80: current_loss=9.70905 | best_loss=8.59999
Epoch 4/80: current_loss=8.61788 | best_loss=8.59999
Epoch 5/80: current_loss=9.46131 | best_loss=8.59999
Epoch 6/80: current_loss=8.51859 | best_loss=8.51859
Epoch 7/80: current_loss=8.90631 | best_loss=8.51859
Epoch 8/80: current_loss=8.62310 | best_loss=8.51859
Epoch 9/80: current_loss=8.85536 | best_loss=8.51859
Epoch 10/80: current_loss=8.98074 | best_loss=8.51859
Epoch 11/80: current_loss=8.69310 | best_loss=8.51859
Epoch 12/80: current_loss=8.52140 | best_loss=8.51859
Epoch 13/80: current_loss=8.50253 | best_loss=8.50253
Epoch 14/80: current_loss=8.60356 | best_loss=8.50253
Epoch 15/80: current_loss=9.07755 | best_loss=8.50253
Epoch 16/80: current_loss=9.10910 | best_loss=8.50253
Epoch 17/80: current_loss=8.65143 | best_loss=8.50253
Epoch 18/80: current_loss=8.59019 | best_loss=8.50253
Epoch 19/80: current_loss=8.55954 | best_loss=8.50253
Epoch 20/80: current_loss=8.76471 | best_loss=8.50253
Epoch 21/80: current_loss=8.52319 | best_loss=8.50253
Epoch 22/80: current_loss=8.57120 | best_loss=8.50253
Epoch 23/80: current_loss=8.51912 | best_loss=8.50253
Epoch 24/80: current_loss=9.15238 | best_loss=8.50253
Epoch 25/80: current_loss=8.51122 | best_loss=8.50253
Epoch 26/80: current_loss=8.49356 | best_loss=8.49356
Epoch 27/80: current_loss=9.01000 | best_loss=8.49356
Epoch 28/80: current_loss=8.80301 | best_loss=8.49356
Epoch 29/80: current_loss=8.56004 | best_loss=8.49356
Epoch 30/80: current_loss=8.50218 | best_loss=8.49356
Epoch 31/80: current_loss=8.51937 | best_loss=8.49356
Epoch 32/80: current_loss=8.47504 | best_loss=8.47504
Epoch 33/80: current_loss=8.67827 | best_loss=8.47504
Epoch 34/80: current_loss=8.49724 | best_loss=8.47504
Epoch 35/80: current_loss=8.60236 | best_loss=8.47504
Epoch 36/80: current_loss=9.84899 | best_loss=8.47504
Epoch 37/80: current_loss=8.70093 | best_loss=8.47504
Epoch 38/80: current_loss=9.34904 | best_loss=8.47504
Epoch 39/80: current_loss=8.64252 | best_loss=8.47504
Epoch 40/80: current_loss=8.77833 | best_loss=8.47504
Epoch 41/80: current_loss=8.52181 | best_loss=8.47504
Epoch 42/80: current_loss=8.54382 | best_loss=8.47504
Epoch 43/80: current_loss=8.58411 | best_loss=8.47504
Epoch 44/80: current_loss=9.17683 | best_loss=8.47504
Epoch 45/80: current_loss=8.72164 | best_loss=8.47504
Epoch 46/80: current_loss=8.57565 | best_loss=8.47504
Epoch 47/80: current_loss=8.57582 | best_loss=8.47504
Epoch 48/80: current_loss=8.49738 | best_loss=8.47504
Epoch 49/80: current_loss=8.74371 | best_loss=8.47504
Epoch 50/80: current_loss=8.79300 | best_loss=8.47504
Epoch 51/80: current_loss=8.50722 | best_loss=8.47504
Epoch 52/80: current_loss=8.56197 | best_loss=8.47504
Early Stopping at epoch 52
      explained_var=0.00191 | mse_loss=8.56172
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.39816 | best_loss=9.39816
Epoch 1/80: current_loss=10.40194 | best_loss=9.39816
Epoch 2/80: current_loss=9.24813 | best_loss=9.24813
Epoch 3/80: current_loss=10.10413 | best_loss=9.24813
Epoch 4/80: current_loss=9.50944 | best_loss=9.24813
Epoch 5/80: current_loss=10.82619 | best_loss=9.24813
Epoch 6/80: current_loss=9.34372 | best_loss=9.24813
Epoch 7/80: current_loss=9.57691 | best_loss=9.24813
Epoch 8/80: current_loss=10.92198 | best_loss=9.24813
Epoch 9/80: current_loss=10.03348 | best_loss=9.24813
Epoch 10/80: current_loss=9.28685 | best_loss=9.24813
Epoch 11/80: current_loss=9.50144 | best_loss=9.24813
Epoch 12/80: current_loss=9.67968 | best_loss=9.24813
Epoch 13/80: current_loss=9.32557 | best_loss=9.24813
Epoch 14/80: current_loss=10.47467 | best_loss=9.24813
Epoch 15/80: current_loss=9.86669 | best_loss=9.24813
Epoch 16/80: current_loss=9.39638 | best_loss=9.24813
Epoch 17/80: current_loss=9.59340 | best_loss=9.24813
Epoch 18/80: current_loss=9.48826 | best_loss=9.24813
Epoch 19/80: current_loss=9.62775 | best_loss=9.24813
Epoch 20/80: current_loss=10.28750 | best_loss=9.24813
Epoch 21/80: current_loss=9.18610 | best_loss=9.18610
Epoch 22/80: current_loss=10.56446 | best_loss=9.18610
Epoch 23/80: current_loss=9.35807 | best_loss=9.18610
Epoch 24/80: current_loss=9.42340 | best_loss=9.18610
Epoch 25/80: current_loss=9.49429 | best_loss=9.18610
Epoch 26/80: current_loss=9.27413 | best_loss=9.18610
Epoch 27/80: current_loss=9.25151 | best_loss=9.18610
Epoch 28/80: current_loss=9.41971 | best_loss=9.18610
Epoch 29/80: current_loss=10.04870 | best_loss=9.18610
Epoch 30/80: current_loss=10.21875 | best_loss=9.18610
Epoch 31/80: current_loss=10.09411 | best_loss=9.18610
Epoch 32/80: current_loss=10.08005 | best_loss=9.18610
Epoch 33/80: current_loss=9.78254 | best_loss=9.18610
Epoch 34/80: current_loss=9.20072 | best_loss=9.18610
Epoch 35/80: current_loss=10.18011 | best_loss=9.18610
Epoch 36/80: current_loss=9.69547 | best_loss=9.18610
Epoch 37/80: current_loss=10.06989 | best_loss=9.18610
Epoch 38/80: current_loss=9.30412 | best_loss=9.18610
Epoch 39/80: current_loss=9.70692 | best_loss=9.18610
Epoch 40/80: current_loss=9.72487 | best_loss=9.18610
Epoch 41/80: current_loss=9.73002 | best_loss=9.18610
Early Stopping at epoch 41
      explained_var=0.00917 | mse_loss=9.29828
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.18393 | best_loss=9.18393
Epoch 1/80: current_loss=9.01051 | best_loss=9.01051
Epoch 2/80: current_loss=9.29202 | best_loss=9.01051
Epoch 3/80: current_loss=9.27803 | best_loss=9.01051
Epoch 4/80: current_loss=9.09408 | best_loss=9.01051
Epoch 5/80: current_loss=9.88930 | best_loss=9.01051
Epoch 6/80: current_loss=9.68609 | best_loss=9.01051
Epoch 7/80: current_loss=9.01433 | best_loss=9.01051
Epoch 8/80: current_loss=9.11127 | best_loss=9.01051
Epoch 9/80: current_loss=8.88892 | best_loss=8.88892
Epoch 10/80: current_loss=9.14399 | best_loss=8.88892
Epoch 11/80: current_loss=8.86562 | best_loss=8.86562
Epoch 12/80: current_loss=9.28271 | best_loss=8.86562
Epoch 13/80: current_loss=8.93356 | best_loss=8.86562
Epoch 14/80: current_loss=8.98373 | best_loss=8.86562
Epoch 15/80: current_loss=8.94232 | best_loss=8.86562
Epoch 16/80: current_loss=9.22206 | best_loss=8.86562
Epoch 17/80: current_loss=8.89450 | best_loss=8.86562
Epoch 18/80: current_loss=9.26117 | best_loss=8.86562
Epoch 19/80: current_loss=9.31825 | best_loss=8.86562
Epoch 20/80: current_loss=9.20546 | best_loss=8.86562
Epoch 21/80: current_loss=9.27165 | best_loss=8.86562
Epoch 22/80: current_loss=9.06778 | best_loss=8.86562
Epoch 23/80: current_loss=9.11188 | best_loss=8.86562
Epoch 24/80: current_loss=8.96201 | best_loss=8.86562
Epoch 25/80: current_loss=9.31784 | best_loss=8.86562
Epoch 26/80: current_loss=9.59229 | best_loss=8.86562
Epoch 27/80: current_loss=8.99475 | best_loss=8.86562
Epoch 28/80: current_loss=9.84976 | best_loss=8.86562
Epoch 29/80: current_loss=9.27200 | best_loss=8.86562
Epoch 30/80: current_loss=10.65681 | best_loss=8.86562
Epoch 31/80: current_loss=9.23040 | best_loss=8.86562
Early Stopping at epoch 31
      explained_var=0.00359 | mse_loss=8.33233
Fold 4: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.38776 | best_loss=8.38776
Epoch 1/80: current_loss=8.37830 | best_loss=8.37830
Epoch 2/80: current_loss=8.29504 | best_loss=8.29504
Epoch 3/80: current_loss=8.60288 | best_loss=8.29504
Epoch 4/80: current_loss=8.37161 | best_loss=8.29504
Epoch 5/80: current_loss=8.57477 | best_loss=8.29504
Epoch 6/80: current_loss=8.56443 | best_loss=8.29504
Epoch 7/80: current_loss=8.59795 | best_loss=8.29504
Epoch 8/80: current_loss=9.51401 | best_loss=8.29504
Epoch 9/80: current_loss=8.37003 | best_loss=8.29504
Epoch 10/80: current_loss=8.30980 | best_loss=8.29504
Epoch 11/80: current_loss=8.50476 | best_loss=8.29504
Epoch 12/80: current_loss=8.36251 | best_loss=8.29504
Epoch 13/80: current_loss=8.93676 | best_loss=8.29504
Epoch 14/80: current_loss=9.27590 | best_loss=8.29504
Epoch 15/80: current_loss=8.68273 | best_loss=8.29504
Epoch 16/80: current_loss=8.41608 | best_loss=8.29504
Epoch 17/80: current_loss=8.27764 | best_loss=8.27764
Epoch 18/80: current_loss=8.46870 | best_loss=8.27764
Epoch 19/80: current_loss=8.52261 | best_loss=8.27764
Epoch 20/80: current_loss=8.78939 | best_loss=8.27764
Epoch 21/80: current_loss=9.25881 | best_loss=8.27764
Epoch 22/80: current_loss=8.72799 | best_loss=8.27764
Epoch 23/80: current_loss=8.42984 | best_loss=8.27764
Epoch 24/80: current_loss=8.41651 | best_loss=8.27764
Epoch 25/80: current_loss=9.01300 | best_loss=8.27764
Epoch 26/80: current_loss=8.63907 | best_loss=8.27764
Epoch 27/80: current_loss=8.51859 | best_loss=8.27764
Epoch 28/80: current_loss=8.76978 | best_loss=8.27764
Epoch 29/80: current_loss=8.49392 | best_loss=8.27764
Epoch 30/80: current_loss=8.89517 | best_loss=8.27764
Epoch 31/80: current_loss=8.71012 | best_loss=8.27764
Epoch 32/80: current_loss=8.91696 | best_loss=8.27764
Epoch 33/80: current_loss=8.73491 | best_loss=8.27764
Epoch 34/80: current_loss=8.57261 | best_loss=8.27764
Epoch 35/80: current_loss=10.04268 | best_loss=8.27764
Epoch 36/80: current_loss=9.08293 | best_loss=8.27764
Epoch 37/80: current_loss=9.22670 | best_loss=8.27764
Early Stopping at epoch 37
      explained_var=0.02201 | mse_loss=8.02227
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00755| avg_loss=8.75623
----------------------------------------------


----------------------------------------------
Params for Trial 97
{'learning_rate': 0.01, 'weight_decay': 0.0009121939225994889, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.67057 | best_loss=10.67057
Epoch 1/80: current_loss=9.93809 | best_loss=9.93809
Epoch 2/80: current_loss=10.23108 | best_loss=9.93809
Epoch 3/80: current_loss=10.21463 | best_loss=9.93809
Epoch 4/80: current_loss=9.99152 | best_loss=9.93809
Epoch 5/80: current_loss=10.13407 | best_loss=9.93809
Epoch 6/80: current_loss=9.90930 | best_loss=9.90930
Epoch 7/80: current_loss=10.24084 | best_loss=9.90930
Epoch 8/80: current_loss=10.47944 | best_loss=9.90930
Epoch 9/80: current_loss=10.08921 | best_loss=9.90930
Epoch 10/80: current_loss=10.19850 | best_loss=9.90930
Epoch 11/80: current_loss=10.40558 | best_loss=9.90930
Epoch 12/80: current_loss=9.90586 | best_loss=9.90586
Epoch 13/80: current_loss=9.96074 | best_loss=9.90586
Epoch 14/80: current_loss=9.90634 | best_loss=9.90586
Epoch 15/80: current_loss=10.02998 | best_loss=9.90586
Epoch 16/80: current_loss=9.89395 | best_loss=9.89395
Epoch 17/80: current_loss=10.74006 | best_loss=9.89395
Epoch 18/80: current_loss=9.90375 | best_loss=9.89395
Epoch 19/80: current_loss=10.10152 | best_loss=9.89395
Epoch 20/80: current_loss=10.00993 | best_loss=9.89395
Epoch 21/80: current_loss=9.91609 | best_loss=9.89395
Epoch 22/80: current_loss=9.99515 | best_loss=9.89395
Epoch 23/80: current_loss=9.97044 | best_loss=9.89395
Epoch 24/80: current_loss=9.92340 | best_loss=9.89395
Epoch 25/80: current_loss=10.35788 | best_loss=9.89395
Epoch 26/80: current_loss=11.60617 | best_loss=9.89395
Epoch 27/80: current_loss=9.92729 | best_loss=9.89395
Epoch 28/80: current_loss=10.72561 | best_loss=9.89395
Epoch 29/80: current_loss=9.92230 | best_loss=9.89395
Epoch 30/80: current_loss=10.07947 | best_loss=9.89395
Epoch 31/80: current_loss=9.95967 | best_loss=9.89395
Epoch 32/80: current_loss=11.01198 | best_loss=9.89395
Epoch 33/80: current_loss=10.64689 | best_loss=9.89395
Epoch 34/80: current_loss=10.53616 | best_loss=9.89395
Epoch 35/80: current_loss=10.46400 | best_loss=9.89395
Epoch 36/80: current_loss=10.96703 | best_loss=9.89395
Early Stopping at epoch 36
      explained_var=0.00158 | mse_loss=9.56460
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.50459 | best_loss=8.50459
Epoch 1/80: current_loss=8.53298 | best_loss=8.50459
Epoch 2/80: current_loss=8.57365 | best_loss=8.50459
Epoch 3/80: current_loss=8.49365 | best_loss=8.49365
Epoch 4/80: current_loss=8.59320 | best_loss=8.49365
Epoch 5/80: current_loss=8.54230 | best_loss=8.49365
Epoch 6/80: current_loss=9.22483 | best_loss=8.49365
Epoch 7/80: current_loss=8.64051 | best_loss=8.49365
Epoch 8/80: current_loss=8.57548 | best_loss=8.49365
Epoch 9/80: current_loss=9.84419 | best_loss=8.49365
Epoch 10/80: current_loss=8.69362 | best_loss=8.49365
Epoch 11/80: current_loss=8.71810 | best_loss=8.49365
Epoch 12/80: current_loss=9.08125 | best_loss=8.49365
Epoch 13/80: current_loss=8.52672 | best_loss=8.49365
Epoch 14/80: current_loss=8.66585 | best_loss=8.49365
Epoch 15/80: current_loss=8.62683 | best_loss=8.49365
Epoch 16/80: current_loss=8.87225 | best_loss=8.49365
Epoch 17/80: current_loss=8.52085 | best_loss=8.49365
Epoch 18/80: current_loss=8.51376 | best_loss=8.49365
Epoch 19/80: current_loss=8.76085 | best_loss=8.49365
Epoch 20/80: current_loss=8.51383 | best_loss=8.49365
Epoch 21/80: current_loss=8.58029 | best_loss=8.49365
Epoch 22/80: current_loss=8.83556 | best_loss=8.49365
Epoch 23/80: current_loss=8.51931 | best_loss=8.49365
Early Stopping at epoch 23
      explained_var=0.00118 | mse_loss=8.56613

----------------------------------------------
Params for Trial 98
{'learning_rate': 0.01, 'weight_decay': 0.0005070079609566415, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.94107 | best_loss=9.94107
Epoch 1/80: current_loss=10.04187 | best_loss=9.94107
Epoch 2/80: current_loss=9.93074 | best_loss=9.93074
Epoch 3/80: current_loss=10.19806 | best_loss=9.93074
Epoch 4/80: current_loss=10.55283 | best_loss=9.93074
Epoch 5/80: current_loss=9.90935 | best_loss=9.90935
Epoch 6/80: current_loss=9.94157 | best_loss=9.90935
Epoch 7/80: current_loss=9.90987 | best_loss=9.90935
Epoch 8/80: current_loss=10.07222 | best_loss=9.90935
Epoch 9/80: current_loss=9.91949 | best_loss=9.90935
Epoch 10/80: current_loss=9.90677 | best_loss=9.90677
Epoch 11/80: current_loss=9.98912 | best_loss=9.90677
Epoch 12/80: current_loss=9.91164 | best_loss=9.90677
Epoch 13/80: current_loss=10.14113 | best_loss=9.90677
Epoch 14/80: current_loss=10.09700 | best_loss=9.90677
Epoch 15/80: current_loss=10.17665 | best_loss=9.90677
Epoch 16/80: current_loss=10.17268 | best_loss=9.90677
Epoch 17/80: current_loss=10.17568 | best_loss=9.90677
Epoch 18/80: current_loss=9.90873 | best_loss=9.90677
Epoch 19/80: current_loss=9.90932 | best_loss=9.90677
Epoch 20/80: current_loss=9.90215 | best_loss=9.90215
Epoch 21/80: current_loss=10.33498 | best_loss=9.90215
Epoch 22/80: current_loss=10.31708 | best_loss=9.90215
Epoch 23/80: current_loss=9.93734 | best_loss=9.90215
Epoch 24/80: current_loss=10.49713 | best_loss=9.90215
Epoch 25/80: current_loss=10.96872 | best_loss=9.90215
Epoch 26/80: current_loss=10.29564 | best_loss=9.90215
Epoch 27/80: current_loss=10.15832 | best_loss=9.90215
Epoch 28/80: current_loss=9.93300 | best_loss=9.90215
Epoch 29/80: current_loss=9.92280 | best_loss=9.90215
Epoch 30/80: current_loss=9.92852 | best_loss=9.90215
Epoch 31/80: current_loss=10.03366 | best_loss=9.90215
Epoch 32/80: current_loss=9.94229 | best_loss=9.90215
Epoch 33/80: current_loss=9.91239 | best_loss=9.90215
Epoch 34/80: current_loss=9.97909 | best_loss=9.90215
Epoch 35/80: current_loss=10.08382 | best_loss=9.90215
Epoch 36/80: current_loss=10.52121 | best_loss=9.90215
Epoch 37/80: current_loss=9.88681 | best_loss=9.88681
Epoch 38/80: current_loss=9.95228 | best_loss=9.88681
Epoch 39/80: current_loss=9.91322 | best_loss=9.88681
Epoch 40/80: current_loss=10.78512 | best_loss=9.88681
Epoch 41/80: current_loss=9.90488 | best_loss=9.88681
Epoch 42/80: current_loss=10.76144 | best_loss=9.88681
Epoch 43/80: current_loss=9.90419 | best_loss=9.88681
Epoch 44/80: current_loss=9.90602 | best_loss=9.88681
Epoch 45/80: current_loss=9.90869 | best_loss=9.88681
Epoch 46/80: current_loss=10.02840 | best_loss=9.88681
Epoch 47/80: current_loss=10.16496 | best_loss=9.88681
Epoch 48/80: current_loss=10.24814 | best_loss=9.88681
Epoch 49/80: current_loss=10.51857 | best_loss=9.88681
Epoch 50/80: current_loss=10.04740 | best_loss=9.88681
Epoch 51/80: current_loss=9.97378 | best_loss=9.88681
Epoch 52/80: current_loss=9.91033 | best_loss=9.88681
Epoch 53/80: current_loss=11.32587 | best_loss=9.88681
Epoch 54/80: current_loss=11.73345 | best_loss=9.88681
Epoch 55/80: current_loss=10.01240 | best_loss=9.88681
Epoch 56/80: current_loss=10.14255 | best_loss=9.88681
Epoch 57/80: current_loss=10.17008 | best_loss=9.88681
Early Stopping at epoch 57
      explained_var=0.00250 | mse_loss=9.55461
Fold 1: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.94769 | best_loss=8.94769
Epoch 1/80: current_loss=8.46870 | best_loss=8.46870
Epoch 2/80: current_loss=8.63264 | best_loss=8.46870
Epoch 3/80: current_loss=8.55072 | best_loss=8.46870
Epoch 4/80: current_loss=9.34702 | best_loss=8.46870
Epoch 5/80: current_loss=8.50145 | best_loss=8.46870
Epoch 6/80: current_loss=8.85755 | best_loss=8.46870
Epoch 7/80: current_loss=8.53144 | best_loss=8.46870
Epoch 8/80: current_loss=8.53086 | best_loss=8.46870
Epoch 9/80: current_loss=8.54210 | best_loss=8.46870
Epoch 10/80: current_loss=8.52211 | best_loss=8.46870
Epoch 11/80: current_loss=8.63378 | best_loss=8.46870
Epoch 12/80: current_loss=9.00594 | best_loss=8.46870
Epoch 13/80: current_loss=8.49588 | best_loss=8.46870
Epoch 14/80: current_loss=8.91396 | best_loss=8.46870
Epoch 15/80: current_loss=8.83795 | best_loss=8.46870
Epoch 16/80: current_loss=8.55269 | best_loss=8.46870
Epoch 17/80: current_loss=9.87620 | best_loss=8.46870
Epoch 18/80: current_loss=10.56587 | best_loss=8.46870
Epoch 19/80: current_loss=8.52522 | best_loss=8.46870
Epoch 20/80: current_loss=8.57403 | best_loss=8.46870
Epoch 21/80: current_loss=8.50579 | best_loss=8.46870
Early Stopping at epoch 21
      explained_var=0.00501 | mse_loss=8.54662
Fold 2: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=9.30427 | best_loss=9.30427
Epoch 1/80: current_loss=9.37846 | best_loss=9.30427
Epoch 2/80: current_loss=10.33195 | best_loss=9.30427
Epoch 3/80: current_loss=9.38660 | best_loss=9.30427
Epoch 4/80: current_loss=9.40562 | best_loss=9.30427
Epoch 5/80: current_loss=10.43129 | best_loss=9.30427
Epoch 6/80: current_loss=9.26687 | best_loss=9.26687
Epoch 7/80: current_loss=9.29367 | best_loss=9.26687
Epoch 8/80: current_loss=9.96696 | best_loss=9.26687
Epoch 9/80: current_loss=9.45823 | best_loss=9.26687
Epoch 10/80: current_loss=9.26614 | best_loss=9.26614
Epoch 11/80: current_loss=9.39680 | best_loss=9.26614
Epoch 12/80: current_loss=10.17138 | best_loss=9.26614
Epoch 13/80: current_loss=11.59101 | best_loss=9.26614
Epoch 14/80: current_loss=9.56136 | best_loss=9.26614
Epoch 15/80: current_loss=9.48254 | best_loss=9.26614
Epoch 16/80: current_loss=9.52257 | best_loss=9.26614
Epoch 17/80: current_loss=9.65728 | best_loss=9.26614
Epoch 18/80: current_loss=9.77168 | best_loss=9.26614
Epoch 19/80: current_loss=11.11693 | best_loss=9.26614
Epoch 20/80: current_loss=10.51247 | best_loss=9.26614
Epoch 21/80: current_loss=9.32337 | best_loss=9.26614
Epoch 22/80: current_loss=10.24103 | best_loss=9.26614
Epoch 23/80: current_loss=9.45754 | best_loss=9.26614
Epoch 24/80: current_loss=11.35680 | best_loss=9.26614
Epoch 25/80: current_loss=9.67828 | best_loss=9.26614
Epoch 26/80: current_loss=9.81116 | best_loss=9.26614
Epoch 27/80: current_loss=9.86654 | best_loss=9.26614
Epoch 28/80: current_loss=10.39493 | best_loss=9.26614
Epoch 29/80: current_loss=9.47413 | best_loss=9.26614
Epoch 30/80: current_loss=9.75987 | best_loss=9.26614
Early Stopping at epoch 30
      explained_var=-0.00229 | mse_loss=9.40492
Fold 3: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=8.98246 | best_loss=8.98246
Epoch 1/80: current_loss=9.07111 | best_loss=8.98246
Epoch 2/80: current_loss=9.47670 | best_loss=8.98246
Epoch 3/80: current_loss=9.71132 | best_loss=8.98246
Epoch 4/80: current_loss=9.24282 | best_loss=8.98246
Epoch 5/80: current_loss=8.96483 | best_loss=8.96483
Epoch 6/80: current_loss=9.26260 | best_loss=8.96483
Epoch 7/80: current_loss=9.42036 | best_loss=8.96483
Epoch 8/80: current_loss=9.35331 | best_loss=8.96483
Epoch 9/80: current_loss=8.90421 | best_loss=8.90421
Epoch 10/80: current_loss=8.97479 | best_loss=8.90421
Epoch 11/80: current_loss=9.15121 | best_loss=8.90421
Epoch 12/80: current_loss=9.53684 | best_loss=8.90421
Epoch 13/80: current_loss=9.31011 | best_loss=8.90421
Epoch 14/80: current_loss=10.15558 | best_loss=8.90421
Epoch 15/80: current_loss=8.84626 | best_loss=8.84626
Epoch 16/80: current_loss=9.27354 | best_loss=8.84626
Epoch 17/80: current_loss=9.24482 | best_loss=8.84626
Epoch 18/80: current_loss=9.04013 | best_loss=8.84626
Epoch 19/80: current_loss=9.05602 | best_loss=8.84626
Epoch 20/80: current_loss=10.30773 | best_loss=8.84626
Epoch 21/80: current_loss=9.70948 | best_loss=8.84626
Epoch 22/80: current_loss=9.01713 | best_loss=8.84626
Epoch 23/80: current_loss=9.50951 | best_loss=8.84626
Epoch 24/80: current_loss=10.84618 | best_loss=8.84626
Epoch 25/80: current_loss=8.91754 | best_loss=8.84626
Epoch 26/80: current_loss=9.11576 | best_loss=8.84626
Epoch 27/80: current_loss=9.41168 | best_loss=8.84626
Epoch 28/80: current_loss=9.63718 | best_loss=8.84626
Epoch 29/80: current_loss=9.79503 | best_loss=8.84626
Epoch 30/80: current_loss=9.01211 | best_loss=8.84626
Epoch 31/80: current_loss=9.37435 | best_loss=8.84626
Epoch 32/80: current_loss=9.13012 | best_loss=8.84626
Epoch 33/80: current_loss=8.90678 | best_loss=8.84626
Epoch 34/80: current_loss=8.96526 | best_loss=8.84626
Epoch 35/80: current_loss=9.09179 | best_loss=8.84626
Early Stopping at epoch 35
      explained_var=-0.00594 | mse_loss=8.42192

----------------------------------------------
Params for Trial 99
{'learning_rate': 0.01, 'weight_decay': 0.0011408868852904716, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=1440, num_val_ids=360
Epoch 0/80: current_loss=10.09590 | best_loss=10.09590
Epoch 1/80: current_loss=9.98634 | best_loss=9.98634
Epoch 2/80: current_loss=9.93762 | best_loss=9.93762
Epoch 3/80: current_loss=9.91774 | best_loss=9.91774
Epoch 4/80: current_loss=9.92471 | best_loss=9.91774
Epoch 5/80: current_loss=10.00870 | best_loss=9.91774
Epoch 6/80: current_loss=10.00223 | best_loss=9.91774
Epoch 7/80: current_loss=10.00605 | best_loss=9.91774
Epoch 8/80: current_loss=9.91378 | best_loss=9.91378
Epoch 9/80: current_loss=9.90870 | best_loss=9.90870
Epoch 10/80: current_loss=9.98072 | best_loss=9.90870
Epoch 11/80: current_loss=9.90644 | best_loss=9.90644
Epoch 12/80: current_loss=9.90230 | best_loss=9.90230
Epoch 13/80: current_loss=9.92661 | best_loss=9.90230
Epoch 14/80: current_loss=9.90124 | best_loss=9.90124
Epoch 15/80: current_loss=9.97599 | best_loss=9.90124
Epoch 16/80: current_loss=9.90580 | best_loss=9.90124
Epoch 17/80: current_loss=9.92153 | best_loss=9.90124
Epoch 18/80: current_loss=10.01203 | best_loss=9.90124
Epoch 19/80: current_loss=9.89412 | best_loss=9.89412
Epoch 20/80: current_loss=9.91796 | best_loss=9.89412
Epoch 21/80: current_loss=9.92277 | best_loss=9.89412
Epoch 22/80: current_loss=9.93181 | best_loss=9.89412
Epoch 23/80: current_loss=10.10001 | best_loss=9.89412
Epoch 24/80: current_loss=10.25687 | best_loss=9.89412
Epoch 25/80: current_loss=9.93797 | best_loss=9.89412
Epoch 26/80: current_loss=9.97275 | best_loss=9.89412
Epoch 27/80: current_loss=10.05026 | best_loss=9.89412
Epoch 28/80: current_loss=10.04223 | best_loss=9.89412
Epoch 29/80: current_loss=9.90998 | best_loss=9.89412
Epoch 30/80: current_loss=9.91732 | best_loss=9.89412
Epoch 31/80: current_loss=9.90763 | best_loss=9.89412
Epoch 32/80: current_loss=9.98612 | best_loss=9.89412
Epoch 33/80: current_loss=9.90603 | best_loss=9.89412
Epoch 34/80: current_loss=9.90199 | best_loss=9.89412
Epoch 35/80: current_loss=10.07804 | best_loss=9.89412
Epoch 36/80: current_loss=9.91220 | best_loss=9.89412
Epoch 37/80: current_loss=10.30453 | best_loss=9.89412
Epoch 38/80: current_loss=9.90546 | best_loss=9.89412
Epoch 39/80: current_loss=9.91923 | best_loss=9.89412
Early Stopping at epoch 39
      explained_var=0.00240 | mse_loss=9.56937
Optuna study finished, study statistics:
  Finished trials:  100
  Pruned trials:  61
  Completed trials:  39
  Best Trial:  96
  Value:  8.756229876085499
  AVG stopping:  19
  Params: 
    learning_rate: 0.01
    weight_decay: 0.0009540954229792735
    n_layers: 3
    hidden_size: 128
    dropout: 0.25
----------------------------------------------

Check best params: {'learning_rate': 0.01, 'weight_decay': 0.0009540954229792735, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25, 'avg_epochs': 19}
--------------------------------------------------------------
Test CNN results: avg_loss=10.4251, avg_expvar=0.0013, avg_r2score=-0.0385, avg_mae=2.4713
--------------------------------------------------------------
