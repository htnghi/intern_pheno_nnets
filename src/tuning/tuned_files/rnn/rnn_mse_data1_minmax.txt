[I 2024-01-05 21:56:09,246] A new study created in memory with name: cnn_mseloss_data1_minmax
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:56:22,108] Trial 0 finished with value: 0.03074202258151923 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 0.03074202258151923.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:57:08,404] Trial 1 finished with value: 0.033842194927561395 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}. Best is trial 0 with value: 0.03074202258151923.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:58:11,416] Trial 2 finished with value: 0.02870829836788428 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:58:20,114] Trial 3 finished with value: 0.028803207580099667 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:58:41,718] Trial 4 finished with value: 0.028816046595728523 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:58:54,839] Trial 5 finished with value: 0.028730086838897478 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 21:59:21,597] Trial 6 finished with value: 0.02901650366828149 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:00:00,194] Trial 7 finished with value: 0.04654412941989522 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:00:18,172] Trial 8 finished with value: 0.032642641073929994 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:01:57,535] Trial 9 finished with value: 0.055633415204287384 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:04:37,517] Trial 10 finished with value: 0.03143345770479753 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0030741327184301173, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:05:45,093] Trial 11 finished with value: 0.028796342309351575 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.002554434662107642, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 2 with value: 0.02870829836788428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:06:32,568] Trial 12 finished with value: 0.028354878210632428 and parameters: {'learning_rate': 0.001, 'weight_decay': 8.686045909808823e-05, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 12 with value: 0.028354878210632428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:07:25,511] Trial 13 finished with value: 0.02872678743058299 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.00010813129554321693, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 12 with value: 0.028354878210632428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:09:12,055] Trial 14 finished with value: 0.028364875426062197 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0024080794509283073, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.4}. Best is trial 12 with value: 0.028354878210632428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:10:02,956] Trial 15 finished with value: 0.02868549184760219 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.002974208225832657, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 12 with value: 0.028354878210632428.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:11:32,103] Trial 16 finished with value: 0.028199001954340525 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0019441972553043121, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:12:11,921] Trial 17 finished with value: 0.028607611970494155 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.001476922228135342, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:14:16,581] Trial 18 finished with value: 0.02849209077066094 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.003634430838471082, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:15:09,528] Trial 19 finished with value: 0.028570085274053116 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0019321913361300055, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:15:29,869] Trial 20 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:17:23,814] Trial 21 finished with value: 0.02865392368591091 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0037804125099170248, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:19:11,873] Trial 22 finished with value: 0.028628102801016936 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0022456672580756015, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:20:30,793] Trial 23 finished with value: 0.02851516848122626 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0012424860992348644, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:21:25,392] Trial 24 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:22:13,396] Trial 25 finished with value: 0.028676196562236578 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0037026164096195507, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:22:28,770] Trial 26 finished with value: 0.02872574658458237 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.006814869993263544, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.25}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:23:09,607] Trial 27 finished with value: 0.028413733179225232 and parameters: {'learning_rate': 0.001, 'weight_decay': 3.347012310558863e-05, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:25:44,035] Trial 28 finished with value: 0.028480471633641685 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.002048741649980118, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:26:01,695] Trial 29 finished with value: 0.02883234579300007 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0013876994804289905, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:26:14,128] Trial 30 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:26:39,643] Trial 31 finished with value: 0.028508940584279458 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00017358413332184582, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:27:09,373] Trial 32 finished with value: 0.028616595542662427 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0009595826622126988, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:27:24,783] Trial 33 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:27:49,002] Trial 34 finished with value: 0.02858471769541897 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.001704506745174117, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.35}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:28:14,373] Trial 35 finished with value: 0.029017872783665195 and parameters: {'learning_rate': 0.0001, 'weight_decay': 4.488923017717564e-05, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.30000000000000004}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:28:40,286] Trial 36 finished with value: 0.02878551091915531 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0006800873349411869, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.45000000000000007}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:29:44,661] Trial 37 finished with value: 0.02836822249327072 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0010571773165327205, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:30:01,975] Trial 38 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:31:02,643] Trial 39 finished with value: 0.028729384053728012 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0015857409629578727, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}. Best is trial 16 with value: 0.028199001954340525.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:31:35,724] Trial 40 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:32:26,866] Trial 41 finished with value: 0.028111877193901225 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0007129580989387352, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:33:31,780] Trial 42 finished with value: 0.028181193138551486 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0007690493383900132, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:34:04,952] Trial 43 finished with value: 0.02845277320210407 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0007045610084180968, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:34:57,691] Trial 44 finished with value: 0.028622624824852422 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005873343529976683, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:36:00,880] Trial 45 finished with value: 0.028680196101152404 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0006729936129443094, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:37:01,789] Trial 46 finished with value: 0.028526006334439514 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.003120338740701159, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:37:09,938] Trial 47 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:37:34,644] Trial 48 finished with value: 0.028393796321863146 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0005013526701965179, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:38:51,591] Trial 49 finished with value: 0.028759835677654694 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0026073982463440206, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:38:56,342] Trial 50 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:40:11,091] Trial 51 finished with value: 0.028383163926000606 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.001082991294582433, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:41:22,846] Trial 52 finished with value: 0.028355134852205595 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0004905844663295239, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:42:23,721] Trial 53 finished with value: 0.02812405632002569 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00041237920372180406, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:43:37,375] Trial 54 finished with value: 0.028134517224858702 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00044089725406154756, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:44:51,294] Trial 55 finished with value: 0.02850787014910356 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0004704269646001223, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.15000000000000002}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:45:22,788] Trial 56 finished with value: 0.028374948865684792 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0014551910699131326, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.2}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:45:29,598] Trial 57 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:45:45,860] Trial 58 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:46:59,868] Trial 59 finished with value: 0.028290014789421798 and parameters: {'learning_rate': 0.001, 'weight_decay': 3.372977579609894e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:47:22,191] Trial 60 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:48:28,960] Trial 61 finished with value: 0.028167368290479965 and parameters: {'learning_rate': 0.001, 'weight_decay': 2.088822658393222e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:49:50,719] Trial 62 finished with value: 0.028305595107322067 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0007563500998507909, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:51:21,887] Trial 63 finished with value: 0.028310120642083287 and parameters: {'learning_rate': 0.001, 'weight_decay': 6.845269894026622e-06, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:52:34,828] Trial 64 finished with value: 0.028522402361864825 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0013421585019515697, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:53:56,592] Trial 65 finished with value: 0.02830537888267017 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0003253300984183563, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:55:13,143] Trial 66 finished with value: 0.02850611380724417 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0019918366562063205, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:56:19,873] Trial 67 finished with value: 0.028545690744785933 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004728153681329948, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 41 with value: 0.028111877193901225.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:56:39,564] Trial 68 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:58:12,515] Trial 69 finished with value: 0.028070672944264474 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00039103807841240886, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 22:59:21,281] Trial 70 finished with value: 0.028434337610415205 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0011353189314725343, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:00:27,326] Trial 71 finished with value: 0.02816440477405339 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00032736291366959906, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:01:41,496] Trial 72 finished with value: 0.028090553127342897 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00040236505180497943, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:03:07,888] Trial 73 finished with value: 0.028319737946597773 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00041034917527452386, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:04:14,541] Trial 74 finished with value: 0.028224837625782373 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0005978900717198284, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:05:31,646] Trial 75 finished with value: 0.028449062120460437 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.00022729235465922035, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:05:50,380] Trial 76 finished with value: 0.028776555613982797 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0016006985460057176, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.30000000000000004}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:06:57,831] Trial 77 finished with value: 0.028287629992885144 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0009452833854882376, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:07:31,156] Trial 78 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:09:03,469] Trial 79 finished with value: 0.02826463820136658 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0007059421684398006, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 69 with value: 0.028070672944264474.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:10:08,513] Trial 80 finished with value: 0.027975764131082714 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0003637460901328516, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 80 with value: 0.027975764131082714.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:11:25,845] Trial 81 finished with value: 0.02798074572615013 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00028140477332968844, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 80 with value: 0.027975764131082714.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:12:45,305] Trial 82 finished with value: 0.02838627817494315 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0002579656617129846, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 80 with value: 0.027975764131082714.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:13:04,272] Trial 83 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:14:11,612] Trial 84 finished with value: 0.028062931316244794 and parameters: {'learning_rate': 0.001, 'weight_decay': 1.6097203234986964e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 80 with value: 0.027975764131082714.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:14:26,763] Trial 85 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:15:52,182] Trial 86 finished with value: 0.028510074592832557 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0010953000114728565, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 80 with value: 0.027975764131082714.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:17:32,996] Trial 87 finished with value: 0.028416091982949386 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0008461979432719264, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 80 with value: 0.027975764131082714.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:17:35,423] Trial 88 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:18:29,880] Trial 89 finished with value: 0.028210294440357397 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00024750486992092626, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 80 with value: 0.027975764131082714.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:18:44,171] Trial 90 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:19:51,256] Trial 91 finished with value: 0.02787834571331548 and parameters: {'learning_rate': 0.001, 'weight_decay': 1.030629741702722e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 91 with value: 0.02787834571331548.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:21:12,341] Trial 92 finished with value: 0.02836190357997262 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00026400873997665854, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.1}. Best is trial 91 with value: 0.02787834571331548.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:21:18,240] Trial 93 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:22:25,585] Trial 94 finished with value: 0.02870651214002885 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0007601221454717703, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 91 with value: 0.02787834571331548.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:22:44,171] Trial 95 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:23:56,009] Trial 96 finished with value: 0.02840627938468303 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0012460549947755136, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 91 with value: 0.02787834571331548.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:25:16,567] Trial 97 finished with value: 0.02839725128619137 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0017919604728389603, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 91 with value: 0.02787834571331548.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:26:29,800] Trial 98 finished with value: 0.02815459485418914 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0005310603270520943, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}. Best is trial 91 with value: 0.02787834571331548.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-05 23:27:03,464] Trial 99 pruned. 
GPU CUDA available, using GPU for training the models.
-----------------------------------------------
Input arguments: 
   + data_dir: /home/ra56kop/nnets_genomic_prediction/src
   + model: RNN
   + tuned: 1
   + minmax_scale: 1
   + standa_scale: 0
   + pca_fitting: 0
   + dataset: pheno_1
   + gpucuda: 2
   + data_variants: [1, 0, 0, 1]
-----------------------------------------------

---------------------------------------------------------
Tuning RNN with dataset pheno-1
---------------------------------------------------------


----------------------------------------------
Params for Trial 0
{'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.31124 | best_loss=0.31124
Epoch 1/80: current_loss=0.27633 | best_loss=0.27633
Epoch 2/80: current_loss=0.24334 | best_loss=0.24334
Epoch 3/80: current_loss=0.21209 | best_loss=0.21209
Epoch 4/80: current_loss=0.18208 | best_loss=0.18208
Epoch 5/80: current_loss=0.15315 | best_loss=0.15315
Epoch 6/80: current_loss=0.12625 | best_loss=0.12625
Epoch 7/80: current_loss=0.10272 | best_loss=0.10272
Epoch 8/80: current_loss=0.08244 | best_loss=0.08244
Epoch 9/80: current_loss=0.06559 | best_loss=0.06559
Epoch 10/80: current_loss=0.05312 | best_loss=0.05312
Epoch 11/80: current_loss=0.04550 | best_loss=0.04550
Epoch 12/80: current_loss=0.04140 | best_loss=0.04140
Epoch 13/80: current_loss=0.03991 | best_loss=0.03991
Epoch 14/80: current_loss=0.03957 | best_loss=0.03957
Epoch 15/80: current_loss=0.03935 | best_loss=0.03935
Epoch 16/80: current_loss=0.03925 | best_loss=0.03925
Epoch 17/80: current_loss=0.03917 | best_loss=0.03917
Epoch 18/80: current_loss=0.03907 | best_loss=0.03907
Epoch 19/80: current_loss=0.03900 | best_loss=0.03900
Epoch 20/80: current_loss=0.03882 | best_loss=0.03882
Epoch 21/80: current_loss=0.03871 | best_loss=0.03871
Epoch 22/80: current_loss=0.03863 | best_loss=0.03863
Epoch 23/80: current_loss=0.03855 | best_loss=0.03855
Epoch 24/80: current_loss=0.03842 | best_loss=0.03842
Epoch 25/80: current_loss=0.03829 | best_loss=0.03829
Epoch 26/80: current_loss=0.03822 | best_loss=0.03822
Epoch 27/80: current_loss=0.03800 | best_loss=0.03800
Epoch 28/80: current_loss=0.03804 | best_loss=0.03800
Epoch 29/80: current_loss=0.03781 | best_loss=0.03781
Epoch 30/80: current_loss=0.03747 | best_loss=0.03747
Epoch 31/80: current_loss=0.03734 | best_loss=0.03734
Epoch 32/80: current_loss=0.03724 | best_loss=0.03724
Epoch 33/80: current_loss=0.03722 | best_loss=0.03722
Epoch 34/80: current_loss=0.03712 | best_loss=0.03712
Epoch 35/80: current_loss=0.03702 | best_loss=0.03702
Epoch 36/80: current_loss=0.03680 | best_loss=0.03680
Epoch 37/80: current_loss=0.03661 | best_loss=0.03661
Epoch 38/80: current_loss=0.03650 | best_loss=0.03650
Epoch 39/80: current_loss=0.03638 | best_loss=0.03638
Epoch 40/80: current_loss=0.03626 | best_loss=0.03626
Epoch 41/80: current_loss=0.03622 | best_loss=0.03622
Epoch 42/80: current_loss=0.03629 | best_loss=0.03622
Epoch 43/80: current_loss=0.03612 | best_loss=0.03612
Epoch 44/80: current_loss=0.03588 | best_loss=0.03588
Epoch 45/80: current_loss=0.03573 | best_loss=0.03573
Epoch 46/80: current_loss=0.03561 | best_loss=0.03561
Epoch 47/80: current_loss=0.03550 | best_loss=0.03550
Epoch 48/80: current_loss=0.03536 | best_loss=0.03536
Epoch 49/80: current_loss=0.03523 | best_loss=0.03523
Epoch 50/80: current_loss=0.03518 | best_loss=0.03518
Epoch 51/80: current_loss=0.03511 | best_loss=0.03511
Epoch 52/80: current_loss=0.03511 | best_loss=0.03511
Epoch 53/80: current_loss=0.03498 | best_loss=0.03498
Epoch 54/80: current_loss=0.03466 | best_loss=0.03466
Epoch 55/80: current_loss=0.03453 | best_loss=0.03453
Epoch 56/80: current_loss=0.03443 | best_loss=0.03443
Epoch 57/80: current_loss=0.03442 | best_loss=0.03442
Epoch 58/80: current_loss=0.03440 | best_loss=0.03440
Epoch 59/80: current_loss=0.03426 | best_loss=0.03426
Epoch 60/80: current_loss=0.03420 | best_loss=0.03420
Epoch 61/80: current_loss=0.03396 | best_loss=0.03396
Epoch 62/80: current_loss=0.03379 | best_loss=0.03379
Epoch 63/80: current_loss=0.03367 | best_loss=0.03367
Epoch 64/80: current_loss=0.03359 | best_loss=0.03359
Epoch 65/80: current_loss=0.03366 | best_loss=0.03359
Epoch 66/80: current_loss=0.03357 | best_loss=0.03357
Epoch 67/80: current_loss=0.03350 | best_loss=0.03350
Epoch 68/80: current_loss=0.03342 | best_loss=0.03342
Epoch 69/80: current_loss=0.03328 | best_loss=0.03328
Epoch 70/80: current_loss=0.03319 | best_loss=0.03319
Epoch 71/80: current_loss=0.03312 | best_loss=0.03312
Epoch 72/80: current_loss=0.03302 | best_loss=0.03302
Epoch 73/80: current_loss=0.03294 | best_loss=0.03294
Epoch 74/80: current_loss=0.03291 | best_loss=0.03291
Epoch 75/80: current_loss=0.03265 | best_loss=0.03265
Epoch 76/80: current_loss=0.03249 | best_loss=0.03249
Epoch 77/80: current_loss=0.03231 | best_loss=0.03231
Epoch 78/80: current_loss=0.03222 | best_loss=0.03222
Epoch 79/80: current_loss=0.03216 | best_loss=0.03216
      explained_var=-0.21126 | mse_loss=0.03128
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03479 | best_loss=0.03479
Epoch 1/80: current_loss=0.03499 | best_loss=0.03479
Epoch 2/80: current_loss=0.03461 | best_loss=0.03461
Epoch 3/80: current_loss=0.03441 | best_loss=0.03441
Epoch 4/80: current_loss=0.03445 | best_loss=0.03441
Epoch 5/80: current_loss=0.03451 | best_loss=0.03441
Epoch 6/80: current_loss=0.03433 | best_loss=0.03433
Epoch 7/80: current_loss=0.03429 | best_loss=0.03429
Epoch 8/80: current_loss=0.03391 | best_loss=0.03391
Epoch 9/80: current_loss=0.03364 | best_loss=0.03364
Epoch 10/80: current_loss=0.03352 | best_loss=0.03352
Epoch 11/80: current_loss=0.03351 | best_loss=0.03351
Epoch 12/80: current_loss=0.03327 | best_loss=0.03327
Epoch 13/80: current_loss=0.03321 | best_loss=0.03321
Epoch 14/80: current_loss=0.03309 | best_loss=0.03309
Epoch 15/80: current_loss=0.03307 | best_loss=0.03307
Epoch 16/80: current_loss=0.03271 | best_loss=0.03271
Epoch 17/80: current_loss=0.03262 | best_loss=0.03262
Epoch 18/80: current_loss=0.03304 | best_loss=0.03262
Epoch 19/80: current_loss=0.03304 | best_loss=0.03262
Epoch 20/80: current_loss=0.03301 | best_loss=0.03262
Epoch 21/80: current_loss=0.03273 | best_loss=0.03262
Epoch 22/80: current_loss=0.03267 | best_loss=0.03262
Epoch 23/80: current_loss=0.03272 | best_loss=0.03262
Epoch 24/80: current_loss=0.03283 | best_loss=0.03262
Epoch 25/80: current_loss=0.03258 | best_loss=0.03258
Epoch 26/80: current_loss=0.03238 | best_loss=0.03238
Epoch 27/80: current_loss=0.03215 | best_loss=0.03215
Epoch 28/80: current_loss=0.03213 | best_loss=0.03213
Epoch 29/80: current_loss=0.03223 | best_loss=0.03213
Epoch 30/80: current_loss=0.03224 | best_loss=0.03213
Epoch 31/80: current_loss=0.03195 | best_loss=0.03195
Epoch 32/80: current_loss=0.03204 | best_loss=0.03195
Epoch 33/80: current_loss=0.03208 | best_loss=0.03195
Epoch 34/80: current_loss=0.03195 | best_loss=0.03195
Epoch 35/80: current_loss=0.03177 | best_loss=0.03177
Epoch 36/80: current_loss=0.03164 | best_loss=0.03164
Epoch 37/80: current_loss=0.03164 | best_loss=0.03164
Epoch 38/80: current_loss=0.03167 | best_loss=0.03164
Epoch 39/80: current_loss=0.03154 | best_loss=0.03154
Epoch 40/80: current_loss=0.03153 | best_loss=0.03153
Epoch 41/80: current_loss=0.03160 | best_loss=0.03153
Epoch 42/80: current_loss=0.03143 | best_loss=0.03143
Epoch 43/80: current_loss=0.03130 | best_loss=0.03130
Epoch 44/80: current_loss=0.03133 | best_loss=0.03130
Epoch 45/80: current_loss=0.03122 | best_loss=0.03122
Epoch 46/80: current_loss=0.03122 | best_loss=0.03122
Epoch 47/80: current_loss=0.03143 | best_loss=0.03122
Epoch 48/80: current_loss=0.03111 | best_loss=0.03111
Epoch 49/80: current_loss=0.03108 | best_loss=0.03108
Epoch 50/80: current_loss=0.03112 | best_loss=0.03108
Epoch 51/80: current_loss=0.03119 | best_loss=0.03108
Epoch 52/80: current_loss=0.03084 | best_loss=0.03084
Epoch 53/80: current_loss=0.03070 | best_loss=0.03070
Epoch 54/80: current_loss=0.03070 | best_loss=0.03070
Epoch 55/80: current_loss=0.03065 | best_loss=0.03065
Epoch 56/80: current_loss=0.03071 | best_loss=0.03065
Epoch 57/80: current_loss=0.03064 | best_loss=0.03064
Epoch 58/80: current_loss=0.03052 | best_loss=0.03052
Epoch 59/80: current_loss=0.03046 | best_loss=0.03046
Epoch 60/80: current_loss=0.03049 | best_loss=0.03046
Epoch 61/80: current_loss=0.03039 | best_loss=0.03039
Epoch 62/80: current_loss=0.03030 | best_loss=0.03030
Epoch 63/80: current_loss=0.03015 | best_loss=0.03015
Epoch 64/80: current_loss=0.03008 | best_loss=0.03008
Epoch 65/80: current_loss=0.03003 | best_loss=0.03003
Epoch 66/80: current_loss=0.03020 | best_loss=0.03003
Epoch 67/80: current_loss=0.03026 | best_loss=0.03003
Epoch 68/80: current_loss=0.03021 | best_loss=0.03003
Epoch 69/80: current_loss=0.03045 | best_loss=0.03003
Epoch 70/80: current_loss=0.03026 | best_loss=0.03003
Epoch 71/80: current_loss=0.03027 | best_loss=0.03003
Epoch 72/80: current_loss=0.03005 | best_loss=0.03003
Epoch 73/80: current_loss=0.03023 | best_loss=0.03003
Epoch 74/80: current_loss=0.03038 | best_loss=0.03003
Epoch 75/80: current_loss=0.03027 | best_loss=0.03003
Epoch 76/80: current_loss=0.03000 | best_loss=0.03000
Epoch 77/80: current_loss=0.02993 | best_loss=0.02993
Epoch 78/80: current_loss=0.02997 | best_loss=0.02993
Epoch 79/80: current_loss=0.02986 | best_loss=0.02986
      explained_var=-0.04778 | mse_loss=0.02949
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03271 | best_loss=0.03271
Epoch 1/80: current_loss=0.03367 | best_loss=0.03271
Epoch 2/80: current_loss=0.03356 | best_loss=0.03271
Epoch 3/80: current_loss=0.03285 | best_loss=0.03271
Epoch 4/80: current_loss=0.03295 | best_loss=0.03271
Epoch 5/80: current_loss=0.03293 | best_loss=0.03271
Epoch 6/80: current_loss=0.03307 | best_loss=0.03271
Epoch 7/80: current_loss=0.03380 | best_loss=0.03271
Epoch 8/80: current_loss=0.03339 | best_loss=0.03271
Epoch 9/80: current_loss=0.03297 | best_loss=0.03271
Epoch 10/80: current_loss=0.03278 | best_loss=0.03271
Epoch 11/80: current_loss=0.03283 | best_loss=0.03271
Epoch 12/80: current_loss=0.03311 | best_loss=0.03271
Epoch 13/80: current_loss=0.03296 | best_loss=0.03271
Epoch 14/80: current_loss=0.03249 | best_loss=0.03249
Epoch 15/80: current_loss=0.03265 | best_loss=0.03249
Epoch 16/80: current_loss=0.03278 | best_loss=0.03249
Epoch 17/80: current_loss=0.03280 | best_loss=0.03249
Epoch 18/80: current_loss=0.03295 | best_loss=0.03249
Epoch 19/80: current_loss=0.03337 | best_loss=0.03249
Epoch 20/80: current_loss=0.03298 | best_loss=0.03249
Epoch 21/80: current_loss=0.03259 | best_loss=0.03249
Epoch 22/80: current_loss=0.03230 | best_loss=0.03230
Epoch 23/80: current_loss=0.03240 | best_loss=0.03230
Epoch 24/80: current_loss=0.03310 | best_loss=0.03230
Epoch 25/80: current_loss=0.03276 | best_loss=0.03230
Epoch 26/80: current_loss=0.03246 | best_loss=0.03230
Epoch 27/80: current_loss=0.03273 | best_loss=0.03230
Epoch 28/80: current_loss=0.03288 | best_loss=0.03230
Epoch 29/80: current_loss=0.03255 | best_loss=0.03230
Epoch 30/80: current_loss=0.03244 | best_loss=0.03230
Epoch 31/80: current_loss=0.03334 | best_loss=0.03230
Epoch 32/80: current_loss=0.03271 | best_loss=0.03230
Epoch 33/80: current_loss=0.03307 | best_loss=0.03230
Epoch 34/80: current_loss=0.03308 | best_loss=0.03230
Epoch 35/80: current_loss=0.03262 | best_loss=0.03230
Epoch 36/80: current_loss=0.03221 | best_loss=0.03221
Epoch 37/80: current_loss=0.03244 | best_loss=0.03221
Epoch 38/80: current_loss=0.03241 | best_loss=0.03221
Epoch 39/80: current_loss=0.03267 | best_loss=0.03221
Epoch 40/80: current_loss=0.03246 | best_loss=0.03221
Epoch 41/80: current_loss=0.03209 | best_loss=0.03209
Epoch 42/80: current_loss=0.03241 | best_loss=0.03209
Epoch 43/80: current_loss=0.03256 | best_loss=0.03209
Epoch 44/80: current_loss=0.03232 | best_loss=0.03209
Epoch 45/80: current_loss=0.03227 | best_loss=0.03209
Epoch 46/80: current_loss=0.03284 | best_loss=0.03209
Epoch 47/80: current_loss=0.03263 | best_loss=0.03209
Epoch 48/80: current_loss=0.03243 | best_loss=0.03209
Epoch 49/80: current_loss=0.03258 | best_loss=0.03209
Epoch 50/80: current_loss=0.03253 | best_loss=0.03209
Epoch 51/80: current_loss=0.03261 | best_loss=0.03209
Epoch 52/80: current_loss=0.03235 | best_loss=0.03209
Epoch 53/80: current_loss=0.03263 | best_loss=0.03209
Epoch 54/80: current_loss=0.03254 | best_loss=0.03209
Epoch 55/80: current_loss=0.03226 | best_loss=0.03209
Epoch 56/80: current_loss=0.03181 | best_loss=0.03181
Epoch 57/80: current_loss=0.03190 | best_loss=0.03181
Epoch 58/80: current_loss=0.03190 | best_loss=0.03181
Epoch 59/80: current_loss=0.03245 | best_loss=0.03181
Epoch 60/80: current_loss=0.03264 | best_loss=0.03181
Epoch 61/80: current_loss=0.03221 | best_loss=0.03181
Epoch 62/80: current_loss=0.03206 | best_loss=0.03181
Epoch 63/80: current_loss=0.03189 | best_loss=0.03181
Epoch 64/80: current_loss=0.03198 | best_loss=0.03181
Epoch 65/80: current_loss=0.03189 | best_loss=0.03181
Epoch 66/80: current_loss=0.03183 | best_loss=0.03181
Epoch 67/80: current_loss=0.03221 | best_loss=0.03181
Epoch 68/80: current_loss=0.03223 | best_loss=0.03181
Epoch 69/80: current_loss=0.03202 | best_loss=0.03181
Epoch 70/80: current_loss=0.03274 | best_loss=0.03181
Epoch 71/80: current_loss=0.03252 | best_loss=0.03181
Epoch 72/80: current_loss=0.03196 | best_loss=0.03181
Epoch 73/80: current_loss=0.03181 | best_loss=0.03181
Epoch 74/80: current_loss=0.03155 | best_loss=0.03155
Epoch 75/80: current_loss=0.03178 | best_loss=0.03155
Epoch 76/80: current_loss=0.03212 | best_loss=0.03155
Epoch 77/80: current_loss=0.03192 | best_loss=0.03155
Epoch 78/80: current_loss=0.03202 | best_loss=0.03155
Epoch 79/80: current_loss=0.03236 | best_loss=0.03155
      explained_var=-0.00006 | mse_loss=0.03085
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02816 | best_loss=0.02816
Epoch 1/80: current_loss=0.02810 | best_loss=0.02810
Epoch 2/80: current_loss=0.02815 | best_loss=0.02810
Epoch 3/80: current_loss=0.02815 | best_loss=0.02810
Epoch 4/80: current_loss=0.02837 | best_loss=0.02810
Epoch 5/80: current_loss=0.02866 | best_loss=0.02810
Epoch 6/80: current_loss=0.02850 | best_loss=0.02810
Epoch 7/80: current_loss=0.02837 | best_loss=0.02810
Epoch 8/80: current_loss=0.02821 | best_loss=0.02810
Epoch 9/80: current_loss=0.02824 | best_loss=0.02810
Epoch 10/80: current_loss=0.02822 | best_loss=0.02810
Epoch 11/80: current_loss=0.02825 | best_loss=0.02810
Epoch 12/80: current_loss=0.02834 | best_loss=0.02810
Epoch 13/80: current_loss=0.02841 | best_loss=0.02810
Epoch 14/80: current_loss=0.02833 | best_loss=0.02810
Epoch 15/80: current_loss=0.02841 | best_loss=0.02810
Epoch 16/80: current_loss=0.02845 | best_loss=0.02810
Epoch 17/80: current_loss=0.02838 | best_loss=0.02810
Epoch 18/80: current_loss=0.02843 | best_loss=0.02810
Epoch 19/80: current_loss=0.02844 | best_loss=0.02810
Epoch 20/80: current_loss=0.02837 | best_loss=0.02810
Epoch 21/80: current_loss=0.02837 | best_loss=0.02810
Early Stopping at epoch 21
      explained_var=0.00087 | mse_loss=0.02838
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03301 | best_loss=0.03301
Epoch 1/80: current_loss=0.03293 | best_loss=0.03293
Epoch 2/80: current_loss=0.03297 | best_loss=0.03293
Epoch 3/80: current_loss=0.03310 | best_loss=0.03293
Epoch 4/80: current_loss=0.03309 | best_loss=0.03293
Epoch 5/80: current_loss=0.03304 | best_loss=0.03293
Epoch 6/80: current_loss=0.03317 | best_loss=0.03293
Epoch 7/80: current_loss=0.03324 | best_loss=0.03293
Epoch 8/80: current_loss=0.03323 | best_loss=0.03293
Epoch 9/80: current_loss=0.03320 | best_loss=0.03293
Epoch 10/80: current_loss=0.03331 | best_loss=0.03293
Epoch 11/80: current_loss=0.03325 | best_loss=0.03293
Epoch 12/80: current_loss=0.03329 | best_loss=0.03293
Epoch 13/80: current_loss=0.03335 | best_loss=0.03293
Epoch 14/80: current_loss=0.03348 | best_loss=0.03293
Epoch 15/80: current_loss=0.03333 | best_loss=0.03293
Epoch 16/80: current_loss=0.03328 | best_loss=0.03293
Epoch 17/80: current_loss=0.03325 | best_loss=0.03293
Epoch 18/80: current_loss=0.03324 | best_loss=0.03293
Epoch 19/80: current_loss=0.03332 | best_loss=0.03293
Epoch 20/80: current_loss=0.03328 | best_loss=0.03293
Epoch 21/80: current_loss=0.03332 | best_loss=0.03293
Early Stopping at epoch 21
      explained_var=-0.03634 | mse_loss=0.03372
----------------------------------------------
Average early_stopping_point: 48| avg_exp_var=-0.05891| avg_loss=0.03074
----------------------------------------------


----------------------------------------------
Params for Trial 1
{'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.18558 | best_loss=0.18558
Epoch 1/80: current_loss=0.17504 | best_loss=0.17504
Epoch 2/80: current_loss=0.16475 | best_loss=0.16475
Epoch 3/80: current_loss=0.15486 | best_loss=0.15486
Epoch 4/80: current_loss=0.14538 | best_loss=0.14538
Epoch 5/80: current_loss=0.13613 | best_loss=0.13613
Epoch 6/80: current_loss=0.12726 | best_loss=0.12726
Epoch 7/80: current_loss=0.11872 | best_loss=0.11872
Epoch 8/80: current_loss=0.11045 | best_loss=0.11045
Epoch 9/80: current_loss=0.10268 | best_loss=0.10268
Epoch 10/80: current_loss=0.09542 | best_loss=0.09542
Epoch 11/80: current_loss=0.08844 | best_loss=0.08844
Epoch 12/80: current_loss=0.08199 | best_loss=0.08199
Epoch 13/80: current_loss=0.07593 | best_loss=0.07593
Epoch 14/80: current_loss=0.07038 | best_loss=0.07038
Epoch 15/80: current_loss=0.06545 | best_loss=0.06545
Epoch 16/80: current_loss=0.06071 | best_loss=0.06071
Epoch 17/80: current_loss=0.05644 | best_loss=0.05644
Epoch 18/80: current_loss=0.05275 | best_loss=0.05275
Epoch 19/80: current_loss=0.04957 | best_loss=0.04957
Epoch 20/80: current_loss=0.04689 | best_loss=0.04689
Epoch 21/80: current_loss=0.04466 | best_loss=0.04466
Epoch 22/80: current_loss=0.04282 | best_loss=0.04282
Epoch 23/80: current_loss=0.04149 | best_loss=0.04149
Epoch 24/80: current_loss=0.04034 | best_loss=0.04034
Epoch 25/80: current_loss=0.03952 | best_loss=0.03952
Epoch 26/80: current_loss=0.03891 | best_loss=0.03891
Epoch 27/80: current_loss=0.03848 | best_loss=0.03848
Epoch 28/80: current_loss=0.03821 | best_loss=0.03821
Epoch 29/80: current_loss=0.03803 | best_loss=0.03803
Epoch 30/80: current_loss=0.03794 | best_loss=0.03794
Epoch 31/80: current_loss=0.03788 | best_loss=0.03788
Epoch 32/80: current_loss=0.03786 | best_loss=0.03786
Epoch 33/80: current_loss=0.03786 | best_loss=0.03786
Epoch 34/80: current_loss=0.03787 | best_loss=0.03786
Epoch 35/80: current_loss=0.03792 | best_loss=0.03786
Epoch 36/80: current_loss=0.03792 | best_loss=0.03786
Epoch 37/80: current_loss=0.03797 | best_loss=0.03786
Epoch 38/80: current_loss=0.03800 | best_loss=0.03786
Epoch 39/80: current_loss=0.03805 | best_loss=0.03786
Epoch 40/80: current_loss=0.03811 | best_loss=0.03786
Epoch 41/80: current_loss=0.03819 | best_loss=0.03786
Epoch 42/80: current_loss=0.03810 | best_loss=0.03786
Epoch 43/80: current_loss=0.03812 | best_loss=0.03786
Epoch 44/80: current_loss=0.03805 | best_loss=0.03786
Epoch 45/80: current_loss=0.03802 | best_loss=0.03786
Epoch 46/80: current_loss=0.03803 | best_loss=0.03786
Epoch 47/80: current_loss=0.03804 | best_loss=0.03786
Epoch 48/80: current_loss=0.03802 | best_loss=0.03786
Epoch 49/80: current_loss=0.03800 | best_loss=0.03786
Epoch 50/80: current_loss=0.03793 | best_loss=0.03786
Epoch 51/80: current_loss=0.03788 | best_loss=0.03786
Epoch 52/80: current_loss=0.03781 | best_loss=0.03781
Epoch 53/80: current_loss=0.03778 | best_loss=0.03778
Epoch 54/80: current_loss=0.03780 | best_loss=0.03778
Epoch 55/80: current_loss=0.03786 | best_loss=0.03778
Epoch 56/80: current_loss=0.03777 | best_loss=0.03777
Epoch 57/80: current_loss=0.03774 | best_loss=0.03774
Epoch 58/80: current_loss=0.03775 | best_loss=0.03774
Epoch 59/80: current_loss=0.03763 | best_loss=0.03763
Epoch 60/80: current_loss=0.03754 | best_loss=0.03754
Epoch 61/80: current_loss=0.03746 | best_loss=0.03746
Epoch 62/80: current_loss=0.03733 | best_loss=0.03733
Epoch 63/80: current_loss=0.03731 | best_loss=0.03731
Epoch 64/80: current_loss=0.03727 | best_loss=0.03727
Epoch 65/80: current_loss=0.03722 | best_loss=0.03722
Epoch 66/80: current_loss=0.03718 | best_loss=0.03718
Epoch 67/80: current_loss=0.03717 | best_loss=0.03717
Epoch 68/80: current_loss=0.03718 | best_loss=0.03717
Epoch 69/80: current_loss=0.03722 | best_loss=0.03717
Epoch 70/80: current_loss=0.03721 | best_loss=0.03717
Epoch 71/80: current_loss=0.03718 | best_loss=0.03717
Epoch 72/80: current_loss=0.03707 | best_loss=0.03707
Epoch 73/80: current_loss=0.03692 | best_loss=0.03692
Epoch 74/80: current_loss=0.03679 | best_loss=0.03679
Epoch 75/80: current_loss=0.03677 | best_loss=0.03677
Epoch 76/80: current_loss=0.03681 | best_loss=0.03677
Epoch 77/80: current_loss=0.03687 | best_loss=0.03677
Epoch 78/80: current_loss=0.03685 | best_loss=0.03677
Epoch 79/80: current_loss=0.03680 | best_loss=0.03677
      explained_var=-0.39416 | mse_loss=0.03619
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04029 | best_loss=0.04029
Epoch 1/80: current_loss=0.04020 | best_loss=0.04020
Epoch 2/80: current_loss=0.04012 | best_loss=0.04012
Epoch 3/80: current_loss=0.04003 | best_loss=0.04003
Epoch 4/80: current_loss=0.03993 | best_loss=0.03993
Epoch 5/80: current_loss=0.03983 | best_loss=0.03983
Epoch 6/80: current_loss=0.03974 | best_loss=0.03974
Epoch 7/80: current_loss=0.03964 | best_loss=0.03964
Epoch 8/80: current_loss=0.03955 | best_loss=0.03955
Epoch 9/80: current_loss=0.03946 | best_loss=0.03946
Epoch 10/80: current_loss=0.03937 | best_loss=0.03937
Epoch 11/80: current_loss=0.03928 | best_loss=0.03928
Epoch 12/80: current_loss=0.03921 | best_loss=0.03921
Epoch 13/80: current_loss=0.03923 | best_loss=0.03921
Epoch 14/80: current_loss=0.03908 | best_loss=0.03908
Epoch 15/80: current_loss=0.03900 | best_loss=0.03900
Epoch 16/80: current_loss=0.03893 | best_loss=0.03893
Epoch 17/80: current_loss=0.03886 | best_loss=0.03886
Epoch 18/80: current_loss=0.03880 | best_loss=0.03880
Epoch 19/80: current_loss=0.03874 | best_loss=0.03874
Epoch 20/80: current_loss=0.03868 | best_loss=0.03868
Epoch 21/80: current_loss=0.03863 | best_loss=0.03863
Epoch 22/80: current_loss=0.03856 | best_loss=0.03856
Epoch 23/80: current_loss=0.03848 | best_loss=0.03848
Epoch 24/80: current_loss=0.03842 | best_loss=0.03842
Epoch 25/80: current_loss=0.03837 | best_loss=0.03837
Epoch 26/80: current_loss=0.03831 | best_loss=0.03831
Epoch 27/80: current_loss=0.03821 | best_loss=0.03821
Epoch 28/80: current_loss=0.03815 | best_loss=0.03815
Epoch 29/80: current_loss=0.03810 | best_loss=0.03810
Epoch 30/80: current_loss=0.03804 | best_loss=0.03804
Epoch 31/80: current_loss=0.03796 | best_loss=0.03796
Epoch 32/80: current_loss=0.03788 | best_loss=0.03788
Epoch 33/80: current_loss=0.03782 | best_loss=0.03782
Epoch 34/80: current_loss=0.03779 | best_loss=0.03779
Epoch 35/80: current_loss=0.03770 | best_loss=0.03770
Epoch 36/80: current_loss=0.03766 | best_loss=0.03766
Epoch 37/80: current_loss=0.03762 | best_loss=0.03762
Epoch 38/80: current_loss=0.03753 | best_loss=0.03753
Epoch 39/80: current_loss=0.03746 | best_loss=0.03746
Epoch 40/80: current_loss=0.03738 | best_loss=0.03738
Epoch 41/80: current_loss=0.03733 | best_loss=0.03733
Epoch 42/80: current_loss=0.03728 | best_loss=0.03728
Epoch 43/80: current_loss=0.03723 | best_loss=0.03723
Epoch 44/80: current_loss=0.03718 | best_loss=0.03718
Epoch 45/80: current_loss=0.03713 | best_loss=0.03713
Epoch 46/80: current_loss=0.03707 | best_loss=0.03707
Epoch 47/80: current_loss=0.03702 | best_loss=0.03702
Epoch 48/80: current_loss=0.03697 | best_loss=0.03697
Epoch 49/80: current_loss=0.03690 | best_loss=0.03690
Epoch 50/80: current_loss=0.03684 | best_loss=0.03684
Epoch 51/80: current_loss=0.03678 | best_loss=0.03678
Epoch 52/80: current_loss=0.03674 | best_loss=0.03674
Epoch 53/80: current_loss=0.03669 | best_loss=0.03669
Epoch 54/80: current_loss=0.03664 | best_loss=0.03664
Epoch 55/80: current_loss=0.03659 | best_loss=0.03659
Epoch 56/80: current_loss=0.03655 | best_loss=0.03655
Epoch 57/80: current_loss=0.03649 | best_loss=0.03649
Epoch 58/80: current_loss=0.03644 | best_loss=0.03644
Epoch 59/80: current_loss=0.03640 | best_loss=0.03640
Epoch 60/80: current_loss=0.03635 | best_loss=0.03635
Epoch 61/80: current_loss=0.03629 | best_loss=0.03629
Epoch 62/80: current_loss=0.03625 | best_loss=0.03625
Epoch 63/80: current_loss=0.03621 | best_loss=0.03621
Epoch 64/80: current_loss=0.03618 | best_loss=0.03618
Epoch 65/80: current_loss=0.03613 | best_loss=0.03613
Epoch 66/80: current_loss=0.03609 | best_loss=0.03609
Epoch 67/80: current_loss=0.03604 | best_loss=0.03604
Epoch 68/80: current_loss=0.03599 | best_loss=0.03599
Epoch 69/80: current_loss=0.03594 | best_loss=0.03594
Epoch 70/80: current_loss=0.03589 | best_loss=0.03589
Epoch 71/80: current_loss=0.03585 | best_loss=0.03585
Epoch 72/80: current_loss=0.03580 | best_loss=0.03580
Epoch 73/80: current_loss=0.03578 | best_loss=0.03578
Epoch 74/80: current_loss=0.03572 | best_loss=0.03572
Epoch 75/80: current_loss=0.03568 | best_loss=0.03568
Epoch 76/80: current_loss=0.03564 | best_loss=0.03564
Epoch 77/80: current_loss=0.03560 | best_loss=0.03560
Epoch 78/80: current_loss=0.03553 | best_loss=0.03553
Epoch 79/80: current_loss=0.03548 | best_loss=0.03548
      explained_var=-0.25180 | mse_loss=0.03509
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03718 | best_loss=0.03718
Epoch 1/80: current_loss=0.03720 | best_loss=0.03718
Epoch 2/80: current_loss=0.03745 | best_loss=0.03718
Epoch 3/80: current_loss=0.03742 | best_loss=0.03718
Epoch 4/80: current_loss=0.03747 | best_loss=0.03718
Epoch 5/80: current_loss=0.03730 | best_loss=0.03718
Epoch 6/80: current_loss=0.03726 | best_loss=0.03718
Epoch 7/80: current_loss=0.03717 | best_loss=0.03717
Epoch 8/80: current_loss=0.03719 | best_loss=0.03717
Epoch 9/80: current_loss=0.03722 | best_loss=0.03717
Epoch 10/80: current_loss=0.03714 | best_loss=0.03714
Epoch 11/80: current_loss=0.03718 | best_loss=0.03714
Epoch 12/80: current_loss=0.03706 | best_loss=0.03706
Epoch 13/80: current_loss=0.03696 | best_loss=0.03696
Epoch 14/80: current_loss=0.03694 | best_loss=0.03694
Epoch 15/80: current_loss=0.03679 | best_loss=0.03679
Epoch 16/80: current_loss=0.03687 | best_loss=0.03679
Epoch 17/80: current_loss=0.03693 | best_loss=0.03679
Epoch 18/80: current_loss=0.03689 | best_loss=0.03679
Epoch 19/80: current_loss=0.03677 | best_loss=0.03677
Epoch 20/80: current_loss=0.03679 | best_loss=0.03677
Epoch 21/80: current_loss=0.03663 | best_loss=0.03663
Epoch 22/80: current_loss=0.03663 | best_loss=0.03663
Epoch 23/80: current_loss=0.03676 | best_loss=0.03663
Epoch 24/80: current_loss=0.03698 | best_loss=0.03663
Epoch 25/80: current_loss=0.03684 | best_loss=0.03663
Epoch 26/80: current_loss=0.03687 | best_loss=0.03663
Epoch 27/80: current_loss=0.03667 | best_loss=0.03663
Epoch 28/80: current_loss=0.03665 | best_loss=0.03663
Epoch 29/80: current_loss=0.03667 | best_loss=0.03663
Epoch 30/80: current_loss=0.03665 | best_loss=0.03663
Epoch 31/80: current_loss=0.03655 | best_loss=0.03655
Epoch 32/80: current_loss=0.03649 | best_loss=0.03649
Epoch 33/80: current_loss=0.03642 | best_loss=0.03642
Epoch 34/80: current_loss=0.03638 | best_loss=0.03638
Epoch 35/80: current_loss=0.03628 | best_loss=0.03628
Epoch 36/80: current_loss=0.03632 | best_loss=0.03628
Epoch 37/80: current_loss=0.03623 | best_loss=0.03623
Epoch 38/80: current_loss=0.03622 | best_loss=0.03622
Epoch 39/80: current_loss=0.03625 | best_loss=0.03622
Epoch 40/80: current_loss=0.03631 | best_loss=0.03622
Epoch 41/80: current_loss=0.03619 | best_loss=0.03619
Epoch 42/80: current_loss=0.03619 | best_loss=0.03619
Epoch 43/80: current_loss=0.03600 | best_loss=0.03600
Epoch 44/80: current_loss=0.03604 | best_loss=0.03600
Epoch 45/80: current_loss=0.03603 | best_loss=0.03600
Epoch 46/80: current_loss=0.03605 | best_loss=0.03600
Epoch 47/80: current_loss=0.03612 | best_loss=0.03600
Epoch 48/80: current_loss=0.03612 | best_loss=0.03600
Epoch 49/80: current_loss=0.03607 | best_loss=0.03600
Epoch 50/80: current_loss=0.03589 | best_loss=0.03589
Epoch 51/80: current_loss=0.03589 | best_loss=0.03589
Epoch 52/80: current_loss=0.03594 | best_loss=0.03589
Epoch 53/80: current_loss=0.03590 | best_loss=0.03589
Epoch 54/80: current_loss=0.03568 | best_loss=0.03568
Epoch 55/80: current_loss=0.03553 | best_loss=0.03553
Epoch 56/80: current_loss=0.03545 | best_loss=0.03545
Epoch 57/80: current_loss=0.03554 | best_loss=0.03545
Epoch 58/80: current_loss=0.03549 | best_loss=0.03545
Epoch 59/80: current_loss=0.03546 | best_loss=0.03545
Epoch 60/80: current_loss=0.03565 | best_loss=0.03545
Epoch 61/80: current_loss=0.03569 | best_loss=0.03545
Epoch 62/80: current_loss=0.03574 | best_loss=0.03545
Epoch 63/80: current_loss=0.03573 | best_loss=0.03545
Epoch 64/80: current_loss=0.03576 | best_loss=0.03545
Epoch 65/80: current_loss=0.03575 | best_loss=0.03545
Epoch 66/80: current_loss=0.03584 | best_loss=0.03545
Epoch 67/80: current_loss=0.03571 | best_loss=0.03545
Epoch 68/80: current_loss=0.03556 | best_loss=0.03545
Epoch 69/80: current_loss=0.03563 | best_loss=0.03545
Epoch 70/80: current_loss=0.03556 | best_loss=0.03545
Epoch 71/80: current_loss=0.03545 | best_loss=0.03545
Epoch 72/80: current_loss=0.03529 | best_loss=0.03529
Epoch 73/80: current_loss=0.03532 | best_loss=0.03529
Epoch 74/80: current_loss=0.03549 | best_loss=0.03529
Epoch 75/80: current_loss=0.03543 | best_loss=0.03529
Epoch 76/80: current_loss=0.03538 | best_loss=0.03529
Epoch 77/80: current_loss=0.03531 | best_loss=0.03529
Epoch 78/80: current_loss=0.03526 | best_loss=0.03526
Epoch 79/80: current_loss=0.03551 | best_loss=0.03526
      explained_var=-0.12514 | mse_loss=0.03474
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03007 | best_loss=0.03007
Epoch 1/80: current_loss=0.02998 | best_loss=0.02998
Epoch 2/80: current_loss=0.02992 | best_loss=0.02992
Epoch 3/80: current_loss=0.02992 | best_loss=0.02992
Epoch 4/80: current_loss=0.02992 | best_loss=0.02992
Epoch 5/80: current_loss=0.02991 | best_loss=0.02991
Epoch 6/80: current_loss=0.02990 | best_loss=0.02990
Epoch 7/80: current_loss=0.02989 | best_loss=0.02989
Epoch 8/80: current_loss=0.02991 | best_loss=0.02989
Epoch 9/80: current_loss=0.02992 | best_loss=0.02989
Epoch 10/80: current_loss=0.02990 | best_loss=0.02989
Epoch 11/80: current_loss=0.02987 | best_loss=0.02987
Epoch 12/80: current_loss=0.02983 | best_loss=0.02983
Epoch 13/80: current_loss=0.02983 | best_loss=0.02983
Epoch 14/80: current_loss=0.02987 | best_loss=0.02983
Epoch 15/80: current_loss=0.02986 | best_loss=0.02983
Epoch 16/80: current_loss=0.02983 | best_loss=0.02983
Epoch 17/80: current_loss=0.02981 | best_loss=0.02981
Epoch 18/80: current_loss=0.02981 | best_loss=0.02981
Epoch 19/80: current_loss=0.02982 | best_loss=0.02981
Epoch 20/80: current_loss=0.02985 | best_loss=0.02981
Epoch 21/80: current_loss=0.02988 | best_loss=0.02981
Epoch 22/80: current_loss=0.02984 | best_loss=0.02981
Epoch 23/80: current_loss=0.02983 | best_loss=0.02981
Epoch 24/80: current_loss=0.02974 | best_loss=0.02974
Epoch 25/80: current_loss=0.02972 | best_loss=0.02972
Epoch 26/80: current_loss=0.02974 | best_loss=0.02972
Epoch 27/80: current_loss=0.02972 | best_loss=0.02972
Epoch 28/80: current_loss=0.02972 | best_loss=0.02972
Epoch 29/80: current_loss=0.02969 | best_loss=0.02969
Epoch 30/80: current_loss=0.02967 | best_loss=0.02967
Epoch 31/80: current_loss=0.02964 | best_loss=0.02964
Epoch 32/80: current_loss=0.02963 | best_loss=0.02963
Epoch 33/80: current_loss=0.02960 | best_loss=0.02960
Epoch 34/80: current_loss=0.02957 | best_loss=0.02957
Epoch 35/80: current_loss=0.02958 | best_loss=0.02957
Epoch 36/80: current_loss=0.02956 | best_loss=0.02956
Epoch 37/80: current_loss=0.02955 | best_loss=0.02955
Epoch 38/80: current_loss=0.02954 | best_loss=0.02954
Epoch 39/80: current_loss=0.02953 | best_loss=0.02953
Epoch 40/80: current_loss=0.02953 | best_loss=0.02953
Epoch 41/80: current_loss=0.02952 | best_loss=0.02952
Epoch 42/80: current_loss=0.02950 | best_loss=0.02950
Epoch 43/80: current_loss=0.02949 | best_loss=0.02949
Epoch 44/80: current_loss=0.02950 | best_loss=0.02949
Epoch 45/80: current_loss=0.02950 | best_loss=0.02949
Epoch 46/80: current_loss=0.02950 | best_loss=0.02949
Epoch 47/80: current_loss=0.02949 | best_loss=0.02949
Epoch 48/80: current_loss=0.02949 | best_loss=0.02949
Epoch 49/80: current_loss=0.02947 | best_loss=0.02947
Epoch 50/80: current_loss=0.02945 | best_loss=0.02945
Epoch 51/80: current_loss=0.02943 | best_loss=0.02943
Epoch 52/80: current_loss=0.02944 | best_loss=0.02943
Epoch 53/80: current_loss=0.02944 | best_loss=0.02943
Epoch 54/80: current_loss=0.02941 | best_loss=0.02941
Epoch 55/80: current_loss=0.02939 | best_loss=0.02939
Epoch 56/80: current_loss=0.02939 | best_loss=0.02939
Epoch 57/80: current_loss=0.02939 | best_loss=0.02939
Epoch 58/80: current_loss=0.02940 | best_loss=0.02939
Epoch 59/80: current_loss=0.02938 | best_loss=0.02938
Epoch 60/80: current_loss=0.02939 | best_loss=0.02938
Epoch 61/80: current_loss=0.02940 | best_loss=0.02938
Epoch 62/80: current_loss=0.02938 | best_loss=0.02938
Epoch 63/80: current_loss=0.02935 | best_loss=0.02935
Epoch 64/80: current_loss=0.02935 | best_loss=0.02935
Epoch 65/80: current_loss=0.02933 | best_loss=0.02933
Epoch 66/80: current_loss=0.02932 | best_loss=0.02932
Epoch 67/80: current_loss=0.02932 | best_loss=0.02932
Epoch 68/80: current_loss=0.02934 | best_loss=0.02932
Epoch 69/80: current_loss=0.02935 | best_loss=0.02932
Epoch 70/80: current_loss=0.02935 | best_loss=0.02932
Epoch 71/80: current_loss=0.02934 | best_loss=0.02932
Epoch 72/80: current_loss=0.02932 | best_loss=0.02932
Epoch 73/80: current_loss=0.02932 | best_loss=0.02932
Epoch 74/80: current_loss=0.02930 | best_loss=0.02930
Epoch 75/80: current_loss=0.02931 | best_loss=0.02930
Epoch 76/80: current_loss=0.02926 | best_loss=0.02926
Epoch 77/80: current_loss=0.02924 | best_loss=0.02924
Epoch 78/80: current_loss=0.02924 | best_loss=0.02924
Epoch 79/80: current_loss=0.02926 | best_loss=0.02924
      explained_var=-0.03345 | mse_loss=0.02938
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03436 | best_loss=0.03436
Epoch 1/80: current_loss=0.03377 | best_loss=0.03377
Epoch 2/80: current_loss=0.03344 | best_loss=0.03344
Epoch 3/80: current_loss=0.03333 | best_loss=0.03333
Epoch 4/80: current_loss=0.03324 | best_loss=0.03324
Epoch 5/80: current_loss=0.03319 | best_loss=0.03319
Epoch 6/80: current_loss=0.03318 | best_loss=0.03318
Epoch 7/80: current_loss=0.03322 | best_loss=0.03318
Epoch 8/80: current_loss=0.03324 | best_loss=0.03318
Epoch 9/80: current_loss=0.03324 | best_loss=0.03318
Epoch 10/80: current_loss=0.03325 | best_loss=0.03318
Epoch 11/80: current_loss=0.03324 | best_loss=0.03318
Epoch 12/80: current_loss=0.03322 | best_loss=0.03318
Epoch 13/80: current_loss=0.03323 | best_loss=0.03318
Epoch 14/80: current_loss=0.03323 | best_loss=0.03318
Epoch 15/80: current_loss=0.03323 | best_loss=0.03318
Epoch 16/80: current_loss=0.03323 | best_loss=0.03318
Epoch 17/80: current_loss=0.03323 | best_loss=0.03318
Epoch 18/80: current_loss=0.03320 | best_loss=0.03318
Epoch 19/80: current_loss=0.03320 | best_loss=0.03318
Epoch 20/80: current_loss=0.03321 | best_loss=0.03318
Epoch 21/80: current_loss=0.03321 | best_loss=0.03318
Epoch 22/80: current_loss=0.03322 | best_loss=0.03318
Epoch 23/80: current_loss=0.03322 | best_loss=0.03318
Epoch 24/80: current_loss=0.03323 | best_loss=0.03318
Epoch 25/80: current_loss=0.03321 | best_loss=0.03318
Epoch 26/80: current_loss=0.03319 | best_loss=0.03318
Early Stopping at epoch 26
      explained_var=-0.03895 | mse_loss=0.03383
----------------------------------------------
Average early_stopping_point: 65| avg_exp_var=-0.16870| avg_loss=0.03384
----------------------------------------------


----------------------------------------------
Params for Trial 2
{'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03413 | best_loss=0.03413
Epoch 1/80: current_loss=0.02986 | best_loss=0.02986
Epoch 2/80: current_loss=0.03252 | best_loss=0.02986
Epoch 3/80: current_loss=0.02868 | best_loss=0.02868
Epoch 4/80: current_loss=0.03001 | best_loss=0.02868
Epoch 5/80: current_loss=0.02836 | best_loss=0.02836
Epoch 6/80: current_loss=0.02778 | best_loss=0.02778
Epoch 7/80: current_loss=0.02660 | best_loss=0.02660
Epoch 8/80: current_loss=0.02869 | best_loss=0.02660
Epoch 9/80: current_loss=0.02541 | best_loss=0.02541
Epoch 10/80: current_loss=0.02695 | best_loss=0.02541
Epoch 11/80: current_loss=0.02581 | best_loss=0.02541
Epoch 12/80: current_loss=0.02758 | best_loss=0.02541
Epoch 13/80: current_loss=0.02615 | best_loss=0.02541
Epoch 14/80: current_loss=0.02587 | best_loss=0.02541
Epoch 15/80: current_loss=0.02801 | best_loss=0.02541
Epoch 16/80: current_loss=0.02544 | best_loss=0.02541
Epoch 17/80: current_loss=0.02956 | best_loss=0.02541
Epoch 18/80: current_loss=0.02722 | best_loss=0.02541
Epoch 19/80: current_loss=0.02714 | best_loss=0.02541
Epoch 20/80: current_loss=0.02646 | best_loss=0.02541
Epoch 21/80: current_loss=0.02536 | best_loss=0.02536
Epoch 22/80: current_loss=0.02772 | best_loss=0.02536
Epoch 23/80: current_loss=0.02558 | best_loss=0.02536
Epoch 24/80: current_loss=0.02687 | best_loss=0.02536
Epoch 25/80: current_loss=0.02522 | best_loss=0.02522
Epoch 26/80: current_loss=0.02865 | best_loss=0.02522
Epoch 27/80: current_loss=0.02607 | best_loss=0.02522
Epoch 28/80: current_loss=0.02892 | best_loss=0.02522
Epoch 29/80: current_loss=0.02631 | best_loss=0.02522
Epoch 30/80: current_loss=0.02740 | best_loss=0.02522
Epoch 31/80: current_loss=0.02664 | best_loss=0.02522
Epoch 32/80: current_loss=0.02643 | best_loss=0.02522
Epoch 33/80: current_loss=0.02641 | best_loss=0.02522
Epoch 34/80: current_loss=0.02564 | best_loss=0.02522
Epoch 35/80: current_loss=0.02594 | best_loss=0.02522
Epoch 36/80: current_loss=0.02628 | best_loss=0.02522
Epoch 37/80: current_loss=0.02605 | best_loss=0.02522
Epoch 38/80: current_loss=0.02548 | best_loss=0.02522
Epoch 39/80: current_loss=0.02778 | best_loss=0.02522
Epoch 40/80: current_loss=0.02512 | best_loss=0.02512
Epoch 41/80: current_loss=0.02582 | best_loss=0.02512
Epoch 42/80: current_loss=0.02657 | best_loss=0.02512
Epoch 43/80: current_loss=0.02570 | best_loss=0.02512
Epoch 44/80: current_loss=0.02762 | best_loss=0.02512
Epoch 45/80: current_loss=0.02522 | best_loss=0.02512
Epoch 46/80: current_loss=0.02687 | best_loss=0.02512
Epoch 47/80: current_loss=0.02704 | best_loss=0.02512
Epoch 48/80: current_loss=0.02668 | best_loss=0.02512
Epoch 49/80: current_loss=0.02598 | best_loss=0.02512
Epoch 50/80: current_loss=0.02723 | best_loss=0.02512
Epoch 51/80: current_loss=0.02641 | best_loss=0.02512
Epoch 52/80: current_loss=0.02722 | best_loss=0.02512
Epoch 53/80: current_loss=0.02544 | best_loss=0.02512
Epoch 54/80: current_loss=0.02619 | best_loss=0.02512
Epoch 55/80: current_loss=0.02527 | best_loss=0.02512
Epoch 56/80: current_loss=0.02521 | best_loss=0.02512
Epoch 57/80: current_loss=0.02703 | best_loss=0.02512
Epoch 58/80: current_loss=0.02542 | best_loss=0.02512
Epoch 59/80: current_loss=0.02581 | best_loss=0.02512
Epoch 60/80: current_loss=0.02603 | best_loss=0.02512
Early Stopping at epoch 60
      explained_var=0.05720 | mse_loss=0.02471
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02791 | best_loss=0.02791
Epoch 1/80: current_loss=0.02799 | best_loss=0.02791
Epoch 2/80: current_loss=0.02789 | best_loss=0.02789
Epoch 3/80: current_loss=0.02847 | best_loss=0.02789
Epoch 4/80: current_loss=0.02871 | best_loss=0.02789
Epoch 5/80: current_loss=0.02839 | best_loss=0.02789
Epoch 6/80: current_loss=0.02813 | best_loss=0.02789
Epoch 7/80: current_loss=0.02810 | best_loss=0.02789
Epoch 8/80: current_loss=0.02842 | best_loss=0.02789
Epoch 9/80: current_loss=0.02832 | best_loss=0.02789
Epoch 10/80: current_loss=0.02863 | best_loss=0.02789
Epoch 11/80: current_loss=0.02847 | best_loss=0.02789
Epoch 12/80: current_loss=0.02841 | best_loss=0.02789
Epoch 13/80: current_loss=0.02906 | best_loss=0.02789
Epoch 14/80: current_loss=0.02894 | best_loss=0.02789
Epoch 15/80: current_loss=0.02874 | best_loss=0.02789
Epoch 16/80: current_loss=0.02828 | best_loss=0.02789
Epoch 17/80: current_loss=0.02818 | best_loss=0.02789
Epoch 18/80: current_loss=0.02816 | best_loss=0.02789
Epoch 19/80: current_loss=0.02842 | best_loss=0.02789
Epoch 20/80: current_loss=0.02957 | best_loss=0.02789
Epoch 21/80: current_loss=0.02902 | best_loss=0.02789
Epoch 22/80: current_loss=0.03001 | best_loss=0.02789
Early Stopping at epoch 22
      explained_var=0.01337 | mse_loss=0.02742
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02923 | best_loss=0.02923
Epoch 1/80: current_loss=0.03101 | best_loss=0.02923
Epoch 2/80: current_loss=0.02952 | best_loss=0.02923
Epoch 3/80: current_loss=0.03151 | best_loss=0.02923
Epoch 4/80: current_loss=0.02960 | best_loss=0.02923
Epoch 5/80: current_loss=0.03277 | best_loss=0.02923
Epoch 6/80: current_loss=0.02970 | best_loss=0.02923
Epoch 7/80: current_loss=0.03133 | best_loss=0.02923
Epoch 8/80: current_loss=0.03027 | best_loss=0.02923
Epoch 9/80: current_loss=0.03025 | best_loss=0.02923
Epoch 10/80: current_loss=0.03028 | best_loss=0.02923
Epoch 11/80: current_loss=0.02974 | best_loss=0.02923
Epoch 12/80: current_loss=0.03039 | best_loss=0.02923
Epoch 13/80: current_loss=0.03042 | best_loss=0.02923
Epoch 14/80: current_loss=0.03152 | best_loss=0.02923
Epoch 15/80: current_loss=0.02989 | best_loss=0.02923
Epoch 16/80: current_loss=0.02973 | best_loss=0.02923
Epoch 17/80: current_loss=0.03013 | best_loss=0.02923
Epoch 18/80: current_loss=0.03131 | best_loss=0.02923
Epoch 19/80: current_loss=0.02986 | best_loss=0.02923
Epoch 20/80: current_loss=0.03053 | best_loss=0.02923
Early Stopping at epoch 20
      explained_var=0.03853 | mse_loss=0.02848
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02915 | best_loss=0.02915
Epoch 1/80: current_loss=0.02912 | best_loss=0.02912
Epoch 2/80: current_loss=0.02970 | best_loss=0.02912
Epoch 3/80: current_loss=0.02882 | best_loss=0.02882
Epoch 4/80: current_loss=0.02962 | best_loss=0.02882
Epoch 5/80: current_loss=0.02872 | best_loss=0.02872
Epoch 6/80: current_loss=0.02828 | best_loss=0.02828
Epoch 7/80: current_loss=0.02817 | best_loss=0.02817
Epoch 8/80: current_loss=0.02824 | best_loss=0.02817
Epoch 9/80: current_loss=0.02879 | best_loss=0.02817
Epoch 10/80: current_loss=0.02842 | best_loss=0.02817
Epoch 11/80: current_loss=0.02861 | best_loss=0.02817
Epoch 12/80: current_loss=0.02903 | best_loss=0.02817
Epoch 13/80: current_loss=0.02931 | best_loss=0.02817
Epoch 14/80: current_loss=0.02889 | best_loss=0.02817
Epoch 15/80: current_loss=0.02886 | best_loss=0.02817
Epoch 16/80: current_loss=0.02920 | best_loss=0.02817
Epoch 17/80: current_loss=0.02944 | best_loss=0.02817
Epoch 18/80: current_loss=0.02869 | best_loss=0.02817
Epoch 19/80: current_loss=0.02820 | best_loss=0.02817
Epoch 20/80: current_loss=0.02826 | best_loss=0.02817
Epoch 21/80: current_loss=0.02894 | best_loss=0.02817
Epoch 22/80: current_loss=0.02903 | best_loss=0.02817
Epoch 23/80: current_loss=0.02921 | best_loss=0.02817
Epoch 24/80: current_loss=0.02919 | best_loss=0.02817
Epoch 25/80: current_loss=0.02935 | best_loss=0.02817
Epoch 26/80: current_loss=0.02964 | best_loss=0.02817
Epoch 27/80: current_loss=0.02946 | best_loss=0.02817
Early Stopping at epoch 27
      explained_var=-0.00624 | mse_loss=0.02854
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03363 | best_loss=0.03363
Epoch 1/80: current_loss=0.03401 | best_loss=0.03363
Epoch 2/80: current_loss=0.03392 | best_loss=0.03363
Epoch 3/80: current_loss=0.03390 | best_loss=0.03363
Epoch 4/80: current_loss=0.03376 | best_loss=0.03363
Epoch 5/80: current_loss=0.03370 | best_loss=0.03363
Epoch 6/80: current_loss=0.03402 | best_loss=0.03363
Epoch 7/80: current_loss=0.03419 | best_loss=0.03363
Epoch 8/80: current_loss=0.03415 | best_loss=0.03363
Epoch 9/80: current_loss=0.03406 | best_loss=0.03363
Epoch 10/80: current_loss=0.03408 | best_loss=0.03363
Epoch 11/80: current_loss=0.03434 | best_loss=0.03363
Epoch 12/80: current_loss=0.03412 | best_loss=0.03363
Epoch 13/80: current_loss=0.03432 | best_loss=0.03363
Epoch 14/80: current_loss=0.03473 | best_loss=0.03363
Epoch 15/80: current_loss=0.03408 | best_loss=0.03363
Epoch 16/80: current_loss=0.03396 | best_loss=0.03363
Epoch 17/80: current_loss=0.03397 | best_loss=0.03363
Epoch 18/80: current_loss=0.03391 | best_loss=0.03363
Epoch 19/80: current_loss=0.03368 | best_loss=0.03363
Epoch 20/80: current_loss=0.03426 | best_loss=0.03363
Early Stopping at epoch 20
      explained_var=-0.05860 | mse_loss=0.03439
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.00885| avg_loss=0.02871
----------------------------------------------


----------------------------------------------
Params for Trial 3
{'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.13974 | best_loss=0.13974
Epoch 1/80: current_loss=0.02845 | best_loss=0.02845
Epoch 2/80: current_loss=0.03787 | best_loss=0.02845
Epoch 3/80: current_loss=0.02722 | best_loss=0.02722
Epoch 4/80: current_loss=0.02693 | best_loss=0.02693
Epoch 5/80: current_loss=0.02705 | best_loss=0.02693
Epoch 6/80: current_loss=0.02675 | best_loss=0.02675
Epoch 7/80: current_loss=0.02673 | best_loss=0.02673
Epoch 8/80: current_loss=0.02651 | best_loss=0.02651
Epoch 9/80: current_loss=0.02600 | best_loss=0.02600
Epoch 10/80: current_loss=0.02854 | best_loss=0.02600
Epoch 11/80: current_loss=0.02589 | best_loss=0.02589
Epoch 12/80: current_loss=0.02705 | best_loss=0.02589
Epoch 13/80: current_loss=0.02638 | best_loss=0.02589
Epoch 14/80: current_loss=0.02581 | best_loss=0.02581
Epoch 15/80: current_loss=0.02639 | best_loss=0.02581
Epoch 16/80: current_loss=0.02596 | best_loss=0.02581
Epoch 17/80: current_loss=0.02570 | best_loss=0.02570
Epoch 18/80: current_loss=0.02758 | best_loss=0.02570
Epoch 19/80: current_loss=0.02599 | best_loss=0.02570
Epoch 20/80: current_loss=0.02560 | best_loss=0.02560
Epoch 21/80: current_loss=0.02714 | best_loss=0.02560
Epoch 22/80: current_loss=0.02569 | best_loss=0.02560
Epoch 23/80: current_loss=0.02645 | best_loss=0.02560
Epoch 24/80: current_loss=0.02594 | best_loss=0.02560
Epoch 25/80: current_loss=0.02627 | best_loss=0.02560
Epoch 26/80: current_loss=0.02571 | best_loss=0.02560
Epoch 27/80: current_loss=0.02521 | best_loss=0.02521
Epoch 28/80: current_loss=0.02659 | best_loss=0.02521
Epoch 29/80: current_loss=0.02540 | best_loss=0.02521
Epoch 30/80: current_loss=0.02616 | best_loss=0.02521
Epoch 31/80: current_loss=0.02606 | best_loss=0.02521
Epoch 32/80: current_loss=0.02592 | best_loss=0.02521
Epoch 33/80: current_loss=0.02651 | best_loss=0.02521
Epoch 34/80: current_loss=0.02550 | best_loss=0.02521
Epoch 35/80: current_loss=0.02602 | best_loss=0.02521
Epoch 36/80: current_loss=0.02591 | best_loss=0.02521
Epoch 37/80: current_loss=0.02633 | best_loss=0.02521
Epoch 38/80: current_loss=0.02605 | best_loss=0.02521
Epoch 39/80: current_loss=0.02611 | best_loss=0.02521
Epoch 40/80: current_loss=0.02637 | best_loss=0.02521
Epoch 41/80: current_loss=0.02631 | best_loss=0.02521
Epoch 42/80: current_loss=0.02604 | best_loss=0.02521
Epoch 43/80: current_loss=0.02682 | best_loss=0.02521
Epoch 44/80: current_loss=0.02575 | best_loss=0.02521
Epoch 45/80: current_loss=0.02639 | best_loss=0.02521
Epoch 46/80: current_loss=0.02673 | best_loss=0.02521
Epoch 47/80: current_loss=0.02626 | best_loss=0.02521
Early Stopping at epoch 47
      explained_var=0.04781 | mse_loss=0.02471
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02865 | best_loss=0.02865
Epoch 1/80: current_loss=0.02834 | best_loss=0.02834
Epoch 2/80: current_loss=0.02815 | best_loss=0.02815
Epoch 3/80: current_loss=0.02844 | best_loss=0.02815
Epoch 4/80: current_loss=0.02809 | best_loss=0.02809
Epoch 5/80: current_loss=0.02839 | best_loss=0.02809
Epoch 6/80: current_loss=0.02820 | best_loss=0.02809
Epoch 7/80: current_loss=0.02862 | best_loss=0.02809
Epoch 8/80: current_loss=0.02817 | best_loss=0.02809
Epoch 9/80: current_loss=0.02817 | best_loss=0.02809
Epoch 10/80: current_loss=0.02856 | best_loss=0.02809
Epoch 11/80: current_loss=0.02823 | best_loss=0.02809
Epoch 12/80: current_loss=0.02827 | best_loss=0.02809
Epoch 13/80: current_loss=0.02820 | best_loss=0.02809
Epoch 14/80: current_loss=0.02830 | best_loss=0.02809
Epoch 15/80: current_loss=0.02808 | best_loss=0.02808
Epoch 16/80: current_loss=0.02892 | best_loss=0.02808
Epoch 17/80: current_loss=0.02806 | best_loss=0.02806
Epoch 18/80: current_loss=0.02842 | best_loss=0.02806
Epoch 19/80: current_loss=0.02806 | best_loss=0.02806
Epoch 20/80: current_loss=0.02832 | best_loss=0.02806
Epoch 21/80: current_loss=0.02833 | best_loss=0.02806
Epoch 22/80: current_loss=0.02808 | best_loss=0.02806
Epoch 23/80: current_loss=0.02845 | best_loss=0.02806
Epoch 24/80: current_loss=0.02807 | best_loss=0.02806
Epoch 25/80: current_loss=0.02826 | best_loss=0.02806
Epoch 26/80: current_loss=0.02803 | best_loss=0.02803
Epoch 27/80: current_loss=0.02837 | best_loss=0.02803
Epoch 28/80: current_loss=0.02805 | best_loss=0.02803
Epoch 29/80: current_loss=0.02814 | best_loss=0.02803
Epoch 30/80: current_loss=0.02806 | best_loss=0.02803
Epoch 31/80: current_loss=0.02852 | best_loss=0.02803
Epoch 32/80: current_loss=0.02802 | best_loss=0.02802
Epoch 33/80: current_loss=0.02830 | best_loss=0.02802
Epoch 34/80: current_loss=0.02831 | best_loss=0.02802
Epoch 35/80: current_loss=0.02806 | best_loss=0.02802
Epoch 36/80: current_loss=0.02803 | best_loss=0.02802
Epoch 37/80: current_loss=0.02809 | best_loss=0.02802
Epoch 38/80: current_loss=0.02809 | best_loss=0.02802
Epoch 39/80: current_loss=0.02820 | best_loss=0.02802
Epoch 40/80: current_loss=0.02801 | best_loss=0.02801
Epoch 41/80: current_loss=0.02795 | best_loss=0.02795
Epoch 42/80: current_loss=0.02841 | best_loss=0.02795
Epoch 43/80: current_loss=0.02798 | best_loss=0.02795
Epoch 44/80: current_loss=0.02835 | best_loss=0.02795
Epoch 45/80: current_loss=0.02796 | best_loss=0.02795
Epoch 46/80: current_loss=0.02811 | best_loss=0.02795
Epoch 47/80: current_loss=0.02799 | best_loss=0.02795
Epoch 48/80: current_loss=0.02797 | best_loss=0.02795
Epoch 49/80: current_loss=0.02804 | best_loss=0.02795
Epoch 50/80: current_loss=0.02840 | best_loss=0.02795
Epoch 51/80: current_loss=0.02800 | best_loss=0.02795
Epoch 52/80: current_loss=0.02797 | best_loss=0.02795
Epoch 53/80: current_loss=0.02831 | best_loss=0.02795
Epoch 54/80: current_loss=0.02806 | best_loss=0.02795
Epoch 55/80: current_loss=0.02810 | best_loss=0.02795
Epoch 56/80: current_loss=0.02839 | best_loss=0.02795
Epoch 57/80: current_loss=0.02798 | best_loss=0.02795
Epoch 58/80: current_loss=0.02867 | best_loss=0.02795
Epoch 59/80: current_loss=0.02792 | best_loss=0.02792
Epoch 60/80: current_loss=0.02812 | best_loss=0.02792
Epoch 61/80: current_loss=0.02809 | best_loss=0.02792
Epoch 62/80: current_loss=0.02796 | best_loss=0.02792
Epoch 63/80: current_loss=0.02798 | best_loss=0.02792
Epoch 64/80: current_loss=0.02827 | best_loss=0.02792
Epoch 65/80: current_loss=0.02793 | best_loss=0.02792
Epoch 66/80: current_loss=0.02837 | best_loss=0.02792
Epoch 67/80: current_loss=0.02804 | best_loss=0.02792
Epoch 68/80: current_loss=0.02801 | best_loss=0.02792
Epoch 69/80: current_loss=0.02805 | best_loss=0.02792
Epoch 70/80: current_loss=0.02816 | best_loss=0.02792
Epoch 71/80: current_loss=0.02805 | best_loss=0.02792
Epoch 72/80: current_loss=0.02843 | best_loss=0.02792
Epoch 73/80: current_loss=0.02798 | best_loss=0.02792
Epoch 74/80: current_loss=0.02821 | best_loss=0.02792
Epoch 75/80: current_loss=0.02801 | best_loss=0.02792
Epoch 76/80: current_loss=0.02808 | best_loss=0.02792
Epoch 77/80: current_loss=0.02803 | best_loss=0.02792
Epoch 78/80: current_loss=0.02804 | best_loss=0.02792
Epoch 79/80: current_loss=0.02802 | best_loss=0.02792
Early Stopping at epoch 79
      explained_var=0.01369 | mse_loss=0.02742
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02974 | best_loss=0.02974
Epoch 1/80: current_loss=0.03095 | best_loss=0.02974
Epoch 2/80: current_loss=0.03118 | best_loss=0.02974
Epoch 3/80: current_loss=0.02964 | best_loss=0.02964
Epoch 4/80: current_loss=0.03145 | best_loss=0.02964
Epoch 5/80: current_loss=0.02980 | best_loss=0.02964
Epoch 6/80: current_loss=0.03103 | best_loss=0.02964
Epoch 7/80: current_loss=0.03013 | best_loss=0.02964
Epoch 8/80: current_loss=0.03042 | best_loss=0.02964
Epoch 9/80: current_loss=0.03063 | best_loss=0.02964
Epoch 10/80: current_loss=0.03048 | best_loss=0.02964
Epoch 11/80: current_loss=0.03039 | best_loss=0.02964
Epoch 12/80: current_loss=0.03073 | best_loss=0.02964
Epoch 13/80: current_loss=0.03065 | best_loss=0.02964
Epoch 14/80: current_loss=0.03077 | best_loss=0.02964
Epoch 15/80: current_loss=0.03113 | best_loss=0.02964
Epoch 16/80: current_loss=0.03080 | best_loss=0.02964
Epoch 17/80: current_loss=0.03074 | best_loss=0.02964
Epoch 18/80: current_loss=0.03060 | best_loss=0.02964
Epoch 19/80: current_loss=0.03051 | best_loss=0.02964
Epoch 20/80: current_loss=0.03034 | best_loss=0.02964
Epoch 21/80: current_loss=0.03088 | best_loss=0.02964
Epoch 22/80: current_loss=0.03101 | best_loss=0.02964
Epoch 23/80: current_loss=0.03092 | best_loss=0.02964
Early Stopping at epoch 23
      explained_var=0.03149 | mse_loss=0.02889
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02842 | best_loss=0.02842
Epoch 1/80: current_loss=0.02869 | best_loss=0.02842
Epoch 2/80: current_loss=0.02852 | best_loss=0.02842
Epoch 3/80: current_loss=0.02872 | best_loss=0.02842
Epoch 4/80: current_loss=0.02848 | best_loss=0.02842
Epoch 5/80: current_loss=0.02852 | best_loss=0.02842
Epoch 6/80: current_loss=0.02931 | best_loss=0.02842
Epoch 7/80: current_loss=0.02848 | best_loss=0.02842
Epoch 8/80: current_loss=0.02846 | best_loss=0.02842
Epoch 9/80: current_loss=0.02847 | best_loss=0.02842
Epoch 10/80: current_loss=0.02846 | best_loss=0.02842
Epoch 11/80: current_loss=0.02858 | best_loss=0.02842
Epoch 12/80: current_loss=0.02863 | best_loss=0.02842
Epoch 13/80: current_loss=0.02857 | best_loss=0.02842
Epoch 14/80: current_loss=0.02856 | best_loss=0.02842
Epoch 15/80: current_loss=0.02863 | best_loss=0.02842
Epoch 16/80: current_loss=0.02857 | best_loss=0.02842
Epoch 17/80: current_loss=0.02850 | best_loss=0.02842
Epoch 18/80: current_loss=0.02851 | best_loss=0.02842
Epoch 19/80: current_loss=0.02842 | best_loss=0.02842
Epoch 20/80: current_loss=0.02882 | best_loss=0.02842
Epoch 21/80: current_loss=0.02844 | best_loss=0.02842
Epoch 22/80: current_loss=0.02845 | best_loss=0.02842
Epoch 23/80: current_loss=0.02856 | best_loss=0.02842
Epoch 24/80: current_loss=0.02850 | best_loss=0.02842
Epoch 25/80: current_loss=0.02880 | best_loss=0.02842
Epoch 26/80: current_loss=0.02851 | best_loss=0.02842
Epoch 27/80: current_loss=0.02861 | best_loss=0.02842
Epoch 28/80: current_loss=0.02855 | best_loss=0.02842
Epoch 29/80: current_loss=0.02855 | best_loss=0.02842
Epoch 30/80: current_loss=0.02863 | best_loss=0.02842
Epoch 31/80: current_loss=0.02852 | best_loss=0.02842
Epoch 32/80: current_loss=0.02877 | best_loss=0.02842
Epoch 33/80: current_loss=0.02883 | best_loss=0.02842
Epoch 34/80: current_loss=0.02875 | best_loss=0.02842
Epoch 35/80: current_loss=0.02864 | best_loss=0.02842
Epoch 36/80: current_loss=0.02868 | best_loss=0.02842
Epoch 37/80: current_loss=0.02873 | best_loss=0.02842
Epoch 38/80: current_loss=0.02860 | best_loss=0.02842
Epoch 39/80: current_loss=0.02860 | best_loss=0.02842
Early Stopping at epoch 39
      explained_var=-0.01510 | mse_loss=0.02879
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03360 | best_loss=0.03360
Epoch 1/80: current_loss=0.03342 | best_loss=0.03342
Epoch 2/80: current_loss=0.03356 | best_loss=0.03342
Epoch 3/80: current_loss=0.03354 | best_loss=0.03342
Epoch 4/80: current_loss=0.03361 | best_loss=0.03342
Epoch 5/80: current_loss=0.03351 | best_loss=0.03342
Epoch 6/80: current_loss=0.03384 | best_loss=0.03342
Epoch 7/80: current_loss=0.03351 | best_loss=0.03342
Epoch 8/80: current_loss=0.03342 | best_loss=0.03342
Epoch 9/80: current_loss=0.03367 | best_loss=0.03342
Epoch 10/80: current_loss=0.03345 | best_loss=0.03342
Epoch 11/80: current_loss=0.03347 | best_loss=0.03342
Epoch 12/80: current_loss=0.03371 | best_loss=0.03342
Epoch 13/80: current_loss=0.03353 | best_loss=0.03342
Epoch 14/80: current_loss=0.03349 | best_loss=0.03342
Epoch 15/80: current_loss=0.03371 | best_loss=0.03342
Epoch 16/80: current_loss=0.03362 | best_loss=0.03342
Epoch 17/80: current_loss=0.03380 | best_loss=0.03342
Epoch 18/80: current_loss=0.03374 | best_loss=0.03342
Epoch 19/80: current_loss=0.03368 | best_loss=0.03342
Epoch 20/80: current_loss=0.03360 | best_loss=0.03342
Epoch 21/80: current_loss=0.03383 | best_loss=0.03342
Early Stopping at epoch 21
      explained_var=-0.05292 | mse_loss=0.03420
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.00499| avg_loss=0.02880
----------------------------------------------


----------------------------------------------
Params for Trial 4
{'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.05698 | best_loss=0.05698
Epoch 1/80: current_loss=0.03371 | best_loss=0.03371
Epoch 2/80: current_loss=0.03195 | best_loss=0.03195
Epoch 3/80: current_loss=0.03342 | best_loss=0.03195
Epoch 4/80: current_loss=0.03002 | best_loss=0.03002
Epoch 5/80: current_loss=0.03008 | best_loss=0.03002
Epoch 6/80: current_loss=0.02942 | best_loss=0.02942
Epoch 7/80: current_loss=0.02790 | best_loss=0.02790
Epoch 8/80: current_loss=0.02889 | best_loss=0.02790
Epoch 9/80: current_loss=0.02749 | best_loss=0.02749
Epoch 10/80: current_loss=0.02707 | best_loss=0.02707
Epoch 11/80: current_loss=0.02786 | best_loss=0.02707
Epoch 12/80: current_loss=0.02728 | best_loss=0.02707
Epoch 13/80: current_loss=0.02745 | best_loss=0.02707
Epoch 14/80: current_loss=0.02720 | best_loss=0.02707
Epoch 15/80: current_loss=0.02692 | best_loss=0.02692
Epoch 16/80: current_loss=0.02703 | best_loss=0.02692
Epoch 17/80: current_loss=0.02703 | best_loss=0.02692
Epoch 18/80: current_loss=0.02711 | best_loss=0.02692
Epoch 19/80: current_loss=0.02669 | best_loss=0.02669
Epoch 20/80: current_loss=0.02891 | best_loss=0.02669
Epoch 21/80: current_loss=0.02620 | best_loss=0.02620
Epoch 22/80: current_loss=0.02783 | best_loss=0.02620
Epoch 23/80: current_loss=0.02597 | best_loss=0.02597
Epoch 24/80: current_loss=0.02721 | best_loss=0.02597
Epoch 25/80: current_loss=0.02684 | best_loss=0.02597
Epoch 26/80: current_loss=0.02656 | best_loss=0.02597
Epoch 27/80: current_loss=0.02677 | best_loss=0.02597
Epoch 28/80: current_loss=0.02685 | best_loss=0.02597
Epoch 29/80: current_loss=0.02652 | best_loss=0.02597
Epoch 30/80: current_loss=0.02730 | best_loss=0.02597
Epoch 31/80: current_loss=0.02699 | best_loss=0.02597
Epoch 32/80: current_loss=0.02672 | best_loss=0.02597
Epoch 33/80: current_loss=0.02761 | best_loss=0.02597
Epoch 34/80: current_loss=0.02658 | best_loss=0.02597
Epoch 35/80: current_loss=0.02724 | best_loss=0.02597
Epoch 36/80: current_loss=0.02673 | best_loss=0.02597
Epoch 37/80: current_loss=0.02725 | best_loss=0.02597
Epoch 38/80: current_loss=0.02683 | best_loss=0.02597
Epoch 39/80: current_loss=0.02666 | best_loss=0.02597
Epoch 40/80: current_loss=0.02759 | best_loss=0.02597
Epoch 41/80: current_loss=0.02652 | best_loss=0.02597
Epoch 42/80: current_loss=0.02682 | best_loss=0.02597
Epoch 43/80: current_loss=0.02772 | best_loss=0.02597
Early Stopping at epoch 43
      explained_var=0.01905 | mse_loss=0.02546
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02875 | best_loss=0.02875
Epoch 1/80: current_loss=0.02907 | best_loss=0.02875
Epoch 2/80: current_loss=0.02852 | best_loss=0.02852
Epoch 3/80: current_loss=0.02902 | best_loss=0.02852
Epoch 4/80: current_loss=0.02848 | best_loss=0.02848
Epoch 5/80: current_loss=0.02882 | best_loss=0.02848
Epoch 6/80: current_loss=0.02846 | best_loss=0.02846
Epoch 7/80: current_loss=0.02862 | best_loss=0.02846
Epoch 8/80: current_loss=0.02845 | best_loss=0.02845
Epoch 9/80: current_loss=0.02864 | best_loss=0.02845
Epoch 10/80: current_loss=0.02848 | best_loss=0.02845
Epoch 11/80: current_loss=0.02859 | best_loss=0.02845
Epoch 12/80: current_loss=0.02850 | best_loss=0.02845
Epoch 13/80: current_loss=0.02946 | best_loss=0.02845
Epoch 14/80: current_loss=0.02846 | best_loss=0.02845
Epoch 15/80: current_loss=0.02883 | best_loss=0.02845
Epoch 16/80: current_loss=0.02847 | best_loss=0.02845
Epoch 17/80: current_loss=0.02878 | best_loss=0.02845
Epoch 18/80: current_loss=0.02848 | best_loss=0.02845
Epoch 19/80: current_loss=0.02904 | best_loss=0.02845
Epoch 20/80: current_loss=0.02832 | best_loss=0.02832
Epoch 21/80: current_loss=0.02844 | best_loss=0.02832
Epoch 22/80: current_loss=0.02852 | best_loss=0.02832
Epoch 23/80: current_loss=0.02830 | best_loss=0.02830
Epoch 24/80: current_loss=0.02854 | best_loss=0.02830
Epoch 25/80: current_loss=0.02840 | best_loss=0.02830
Epoch 26/80: current_loss=0.02840 | best_loss=0.02830
Epoch 27/80: current_loss=0.02900 | best_loss=0.02830
Epoch 28/80: current_loss=0.02834 | best_loss=0.02830
Epoch 29/80: current_loss=0.02848 | best_loss=0.02830
Epoch 30/80: current_loss=0.02849 | best_loss=0.02830
Epoch 31/80: current_loss=0.02850 | best_loss=0.02830
Epoch 32/80: current_loss=0.02838 | best_loss=0.02830
Epoch 33/80: current_loss=0.02847 | best_loss=0.02830
Epoch 34/80: current_loss=0.02837 | best_loss=0.02830
Epoch 35/80: current_loss=0.02842 | best_loss=0.02830
Epoch 36/80: current_loss=0.02844 | best_loss=0.02830
Epoch 37/80: current_loss=0.02846 | best_loss=0.02830
Epoch 38/80: current_loss=0.02842 | best_loss=0.02830
Epoch 39/80: current_loss=0.02862 | best_loss=0.02830
Epoch 40/80: current_loss=0.02841 | best_loss=0.02830
Epoch 41/80: current_loss=0.02842 | best_loss=0.02830
Epoch 42/80: current_loss=0.02838 | best_loss=0.02830
Epoch 43/80: current_loss=0.02885 | best_loss=0.02830
Early Stopping at epoch 43
      explained_var=-0.00161 | mse_loss=0.02783
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03135 | best_loss=0.03135
Epoch 1/80: current_loss=0.03142 | best_loss=0.03135
Epoch 2/80: current_loss=0.03138 | best_loss=0.03135
Epoch 3/80: current_loss=0.03179 | best_loss=0.03135
Epoch 4/80: current_loss=0.03104 | best_loss=0.03104
Epoch 5/80: current_loss=0.03248 | best_loss=0.03104
Epoch 6/80: current_loss=0.03083 | best_loss=0.03083
Epoch 7/80: current_loss=0.03148 | best_loss=0.03083
Epoch 8/80: current_loss=0.03158 | best_loss=0.03083
Epoch 9/80: current_loss=0.03118 | best_loss=0.03083
Epoch 10/80: current_loss=0.03163 | best_loss=0.03083
Epoch 11/80: current_loss=0.03169 | best_loss=0.03083
Epoch 12/80: current_loss=0.03134 | best_loss=0.03083
Epoch 13/80: current_loss=0.03110 | best_loss=0.03083
Epoch 14/80: current_loss=0.03168 | best_loss=0.03083
Epoch 15/80: current_loss=0.03146 | best_loss=0.03083
Epoch 16/80: current_loss=0.03155 | best_loss=0.03083
Epoch 17/80: current_loss=0.03203 | best_loss=0.03083
Epoch 18/80: current_loss=0.03146 | best_loss=0.03083
Epoch 19/80: current_loss=0.03074 | best_loss=0.03074
Epoch 20/80: current_loss=0.03173 | best_loss=0.03074
Epoch 21/80: current_loss=0.03157 | best_loss=0.03074
Epoch 22/80: current_loss=0.03310 | best_loss=0.03074
Epoch 23/80: current_loss=0.03154 | best_loss=0.03074
Epoch 24/80: current_loss=0.03133 | best_loss=0.03074
Epoch 25/80: current_loss=0.03156 | best_loss=0.03074
Epoch 26/80: current_loss=0.03136 | best_loss=0.03074
Epoch 27/80: current_loss=0.03214 | best_loss=0.03074
Epoch 28/80: current_loss=0.03172 | best_loss=0.03074
Epoch 29/80: current_loss=0.03263 | best_loss=0.03074
Epoch 30/80: current_loss=0.03137 | best_loss=0.03074
Epoch 31/80: current_loss=0.03115 | best_loss=0.03074
Epoch 32/80: current_loss=0.03283 | best_loss=0.03074
Epoch 33/80: current_loss=0.03094 | best_loss=0.03074
Epoch 34/80: current_loss=0.03213 | best_loss=0.03074
Epoch 35/80: current_loss=0.03108 | best_loss=0.03074
Epoch 36/80: current_loss=0.03223 | best_loss=0.03074
Epoch 37/80: current_loss=0.03094 | best_loss=0.03074
Epoch 38/80: current_loss=0.03138 | best_loss=0.03074
Epoch 39/80: current_loss=0.03208 | best_loss=0.03074
Early Stopping at epoch 39
      explained_var=-0.00753 | mse_loss=0.02998
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02805 | best_loss=0.02805
Epoch 1/80: current_loss=0.02818 | best_loss=0.02805
Epoch 2/80: current_loss=0.02813 | best_loss=0.02805
Epoch 3/80: current_loss=0.02800 | best_loss=0.02800
Epoch 4/80: current_loss=0.02800 | best_loss=0.02800
Epoch 5/80: current_loss=0.02862 | best_loss=0.02800
Epoch 6/80: current_loss=0.02799 | best_loss=0.02799
Epoch 7/80: current_loss=0.02822 | best_loss=0.02799
Epoch 8/80: current_loss=0.02808 | best_loss=0.02799
Epoch 9/80: current_loss=0.02827 | best_loss=0.02799
Epoch 10/80: current_loss=0.02805 | best_loss=0.02799
Epoch 11/80: current_loss=0.02799 | best_loss=0.02799
Epoch 12/80: current_loss=0.02808 | best_loss=0.02799
Epoch 13/80: current_loss=0.02808 | best_loss=0.02799
Epoch 14/80: current_loss=0.02802 | best_loss=0.02799
Epoch 15/80: current_loss=0.02826 | best_loss=0.02799
Epoch 16/80: current_loss=0.02799 | best_loss=0.02799
Epoch 17/80: current_loss=0.02800 | best_loss=0.02799
Epoch 18/80: current_loss=0.02814 | best_loss=0.02799
Epoch 19/80: current_loss=0.02798 | best_loss=0.02798
Epoch 20/80: current_loss=0.02837 | best_loss=0.02798
Epoch 21/80: current_loss=0.02800 | best_loss=0.02798
Epoch 22/80: current_loss=0.02801 | best_loss=0.02798
Epoch 23/80: current_loss=0.02833 | best_loss=0.02798
Epoch 24/80: current_loss=0.02798 | best_loss=0.02798
Epoch 25/80: current_loss=0.02805 | best_loss=0.02798
Epoch 26/80: current_loss=0.02808 | best_loss=0.02798
Epoch 27/80: current_loss=0.02803 | best_loss=0.02798
Epoch 28/80: current_loss=0.02809 | best_loss=0.02798
Epoch 29/80: current_loss=0.02798 | best_loss=0.02798
Epoch 30/80: current_loss=0.02806 | best_loss=0.02798
Epoch 31/80: current_loss=0.02799 | best_loss=0.02798
Epoch 32/80: current_loss=0.02815 | best_loss=0.02798
Epoch 33/80: current_loss=0.02808 | best_loss=0.02798
Epoch 34/80: current_loss=0.02800 | best_loss=0.02798
Epoch 35/80: current_loss=0.02804 | best_loss=0.02798
Epoch 36/80: current_loss=0.02821 | best_loss=0.02798
Epoch 37/80: current_loss=0.02801 | best_loss=0.02798
Epoch 38/80: current_loss=0.02799 | best_loss=0.02798
Epoch 39/80: current_loss=0.02813 | best_loss=0.02798
Epoch 40/80: current_loss=0.02799 | best_loss=0.02798
Epoch 41/80: current_loss=0.02799 | best_loss=0.02798
Epoch 42/80: current_loss=0.02798 | best_loss=0.02798
Epoch 43/80: current_loss=0.02801 | best_loss=0.02798
Epoch 44/80: current_loss=0.02832 | best_loss=0.02798
Early Stopping at epoch 44
      explained_var=0.00126 | mse_loss=0.02833
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03194 | best_loss=0.03194
Epoch 1/80: current_loss=0.03175 | best_loss=0.03175
Epoch 2/80: current_loss=0.03192 | best_loss=0.03175
Epoch 3/80: current_loss=0.03177 | best_loss=0.03175
Epoch 4/80: current_loss=0.03187 | best_loss=0.03175
Epoch 5/80: current_loss=0.03177 | best_loss=0.03175
Epoch 6/80: current_loss=0.03192 | best_loss=0.03175
Epoch 7/80: current_loss=0.03176 | best_loss=0.03175
Epoch 8/80: current_loss=0.03180 | best_loss=0.03175
Epoch 9/80: current_loss=0.03203 | best_loss=0.03175
Epoch 10/80: current_loss=0.03186 | best_loss=0.03175
Epoch 11/80: current_loss=0.03208 | best_loss=0.03175
Epoch 12/80: current_loss=0.03188 | best_loss=0.03175
Epoch 13/80: current_loss=0.03182 | best_loss=0.03175
Epoch 14/80: current_loss=0.03192 | best_loss=0.03175
Epoch 15/80: current_loss=0.03182 | best_loss=0.03175
Epoch 16/80: current_loss=0.03182 | best_loss=0.03175
Epoch 17/80: current_loss=0.03180 | best_loss=0.03175
Epoch 18/80: current_loss=0.03209 | best_loss=0.03175
Epoch 19/80: current_loss=0.03187 | best_loss=0.03175
Epoch 20/80: current_loss=0.03178 | best_loss=0.03175
Epoch 21/80: current_loss=0.03187 | best_loss=0.03175
Early Stopping at epoch 21
      explained_var=-0.00004 | mse_loss=0.03249
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.00222| avg_loss=0.02882
----------------------------------------------


----------------------------------------------
Params for Trial 5
{'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03676 | best_loss=0.03676
Epoch 1/80: current_loss=0.03041 | best_loss=0.03041
Epoch 2/80: current_loss=0.02972 | best_loss=0.02972
Epoch 3/80: current_loss=0.02806 | best_loss=0.02806
Epoch 4/80: current_loss=0.02789 | best_loss=0.02789
Epoch 5/80: current_loss=0.02746 | best_loss=0.02746
Epoch 6/80: current_loss=0.02763 | best_loss=0.02746
Epoch 7/80: current_loss=0.02639 | best_loss=0.02639
Epoch 8/80: current_loss=0.02808 | best_loss=0.02639
Epoch 9/80: current_loss=0.02687 | best_loss=0.02639
Epoch 10/80: current_loss=0.02718 | best_loss=0.02639
Epoch 11/80: current_loss=0.02673 | best_loss=0.02639
Epoch 12/80: current_loss=0.02567 | best_loss=0.02567
Epoch 13/80: current_loss=0.02728 | best_loss=0.02567
Epoch 14/80: current_loss=0.02548 | best_loss=0.02548
Epoch 15/80: current_loss=0.02799 | best_loss=0.02548
Epoch 16/80: current_loss=0.02650 | best_loss=0.02548
Epoch 17/80: current_loss=0.02724 | best_loss=0.02548
Epoch 18/80: current_loss=0.02771 | best_loss=0.02548
Epoch 19/80: current_loss=0.02804 | best_loss=0.02548
Epoch 20/80: current_loss=0.02766 | best_loss=0.02548
Epoch 21/80: current_loss=0.02765 | best_loss=0.02548
Epoch 22/80: current_loss=0.02719 | best_loss=0.02548
Epoch 23/80: current_loss=0.02618 | best_loss=0.02548
Epoch 24/80: current_loss=0.02724 | best_loss=0.02548
Epoch 25/80: current_loss=0.02800 | best_loss=0.02548
Epoch 26/80: current_loss=0.02787 | best_loss=0.02548
Epoch 27/80: current_loss=0.02577 | best_loss=0.02548
Epoch 28/80: current_loss=0.03023 | best_loss=0.02548
Epoch 29/80: current_loss=0.02555 | best_loss=0.02548
Epoch 30/80: current_loss=0.02698 | best_loss=0.02548
Epoch 31/80: current_loss=0.02607 | best_loss=0.02548
Epoch 32/80: current_loss=0.02626 | best_loss=0.02548
Epoch 33/80: current_loss=0.02623 | best_loss=0.02548
Epoch 34/80: current_loss=0.02887 | best_loss=0.02548
Early Stopping at epoch 34
      explained_var=0.03819 | mse_loss=0.02499
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02877 | best_loss=0.02877
Epoch 1/80: current_loss=0.02845 | best_loss=0.02845
Epoch 2/80: current_loss=0.02809 | best_loss=0.02809
Epoch 3/80: current_loss=0.02886 | best_loss=0.02809
Epoch 4/80: current_loss=0.02799 | best_loss=0.02799
Epoch 5/80: current_loss=0.02803 | best_loss=0.02799
Epoch 6/80: current_loss=0.02806 | best_loss=0.02799
Epoch 7/80: current_loss=0.02843 | best_loss=0.02799
Epoch 8/80: current_loss=0.02854 | best_loss=0.02799
Epoch 9/80: current_loss=0.02819 | best_loss=0.02799
Epoch 10/80: current_loss=0.02865 | best_loss=0.02799
Epoch 11/80: current_loss=0.02926 | best_loss=0.02799
Epoch 12/80: current_loss=0.02835 | best_loss=0.02799
Epoch 13/80: current_loss=0.02805 | best_loss=0.02799
Epoch 14/80: current_loss=0.02925 | best_loss=0.02799
Epoch 15/80: current_loss=0.03089 | best_loss=0.02799
Epoch 16/80: current_loss=0.02948 | best_loss=0.02799
Epoch 17/80: current_loss=0.02841 | best_loss=0.02799
Epoch 18/80: current_loss=0.02814 | best_loss=0.02799
Epoch 19/80: current_loss=0.02814 | best_loss=0.02799
Epoch 20/80: current_loss=0.02817 | best_loss=0.02799
Epoch 21/80: current_loss=0.02809 | best_loss=0.02799
Epoch 22/80: current_loss=0.02809 | best_loss=0.02799
Epoch 23/80: current_loss=0.02838 | best_loss=0.02799
Epoch 24/80: current_loss=0.02805 | best_loss=0.02799
Early Stopping at epoch 24
      explained_var=0.01410 | mse_loss=0.02751
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03101 | best_loss=0.03101
Epoch 1/80: current_loss=0.03016 | best_loss=0.03016
Epoch 2/80: current_loss=0.03035 | best_loss=0.03016
Epoch 3/80: current_loss=0.03299 | best_loss=0.03016
Epoch 4/80: current_loss=0.03447 | best_loss=0.03016
Epoch 5/80: current_loss=0.03040 | best_loss=0.03016
Epoch 6/80: current_loss=0.03020 | best_loss=0.03016
Epoch 7/80: current_loss=0.03087 | best_loss=0.03016
Epoch 8/80: current_loss=0.03141 | best_loss=0.03016
Epoch 9/80: current_loss=0.03060 | best_loss=0.03016
Epoch 10/80: current_loss=0.03212 | best_loss=0.03016
Epoch 11/80: current_loss=0.03125 | best_loss=0.03016
Epoch 12/80: current_loss=0.03258 | best_loss=0.03016
Epoch 13/80: current_loss=0.02987 | best_loss=0.02987
Epoch 14/80: current_loss=0.03217 | best_loss=0.02987
Epoch 15/80: current_loss=0.02991 | best_loss=0.02987
Epoch 16/80: current_loss=0.03162 | best_loss=0.02987
Epoch 17/80: current_loss=0.03246 | best_loss=0.02987
Epoch 18/80: current_loss=0.03037 | best_loss=0.02987
Epoch 19/80: current_loss=0.03244 | best_loss=0.02987
Epoch 20/80: current_loss=0.02976 | best_loss=0.02976
Epoch 21/80: current_loss=0.03042 | best_loss=0.02976
Epoch 22/80: current_loss=0.03230 | best_loss=0.02976
Epoch 23/80: current_loss=0.02934 | best_loss=0.02934
Epoch 24/80: current_loss=0.03001 | best_loss=0.02934
Epoch 25/80: current_loss=0.03194 | best_loss=0.02934
Epoch 26/80: current_loss=0.03091 | best_loss=0.02934
Epoch 27/80: current_loss=0.02963 | best_loss=0.02934
Epoch 28/80: current_loss=0.02979 | best_loss=0.02934
Epoch 29/80: current_loss=0.03255 | best_loss=0.02934
Epoch 30/80: current_loss=0.02985 | best_loss=0.02934
Epoch 31/80: current_loss=0.03086 | best_loss=0.02934
Epoch 32/80: current_loss=0.02992 | best_loss=0.02934
Epoch 33/80: current_loss=0.03163 | best_loss=0.02934
Epoch 34/80: current_loss=0.03101 | best_loss=0.02934
Epoch 35/80: current_loss=0.02979 | best_loss=0.02934
Epoch 36/80: current_loss=0.03118 | best_loss=0.02934
Epoch 37/80: current_loss=0.02990 | best_loss=0.02934
Epoch 38/80: current_loss=0.02973 | best_loss=0.02934
Epoch 39/80: current_loss=0.03119 | best_loss=0.02934
Epoch 40/80: current_loss=0.02983 | best_loss=0.02934
Epoch 41/80: current_loss=0.03212 | best_loss=0.02934
Epoch 42/80: current_loss=0.03204 | best_loss=0.02934
Epoch 43/80: current_loss=0.03103 | best_loss=0.02934
Early Stopping at epoch 43
      explained_var=0.03398 | mse_loss=0.02857
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02858 | best_loss=0.02858
Epoch 1/80: current_loss=0.02866 | best_loss=0.02858
Epoch 2/80: current_loss=0.02846 | best_loss=0.02846
Epoch 3/80: current_loss=0.02854 | best_loss=0.02846
Epoch 4/80: current_loss=0.02884 | best_loss=0.02846
Epoch 5/80: current_loss=0.02907 | best_loss=0.02846
Epoch 6/80: current_loss=0.02923 | best_loss=0.02846
Epoch 7/80: current_loss=0.02980 | best_loss=0.02846
Epoch 8/80: current_loss=0.02922 | best_loss=0.02846
Epoch 9/80: current_loss=0.02869 | best_loss=0.02846
Epoch 10/80: current_loss=0.02857 | best_loss=0.02846
Epoch 11/80: current_loss=0.02837 | best_loss=0.02837
Epoch 12/80: current_loss=0.02850 | best_loss=0.02837
Epoch 13/80: current_loss=0.02857 | best_loss=0.02837
Epoch 14/80: current_loss=0.03042 | best_loss=0.02837
Epoch 15/80: current_loss=0.02857 | best_loss=0.02837
Epoch 16/80: current_loss=0.02864 | best_loss=0.02837
Epoch 17/80: current_loss=0.02879 | best_loss=0.02837
Epoch 18/80: current_loss=0.02880 | best_loss=0.02837
Epoch 19/80: current_loss=0.02945 | best_loss=0.02837
Epoch 20/80: current_loss=0.02859 | best_loss=0.02837
Epoch 21/80: current_loss=0.02840 | best_loss=0.02837
Epoch 22/80: current_loss=0.02886 | best_loss=0.02837
Epoch 23/80: current_loss=0.02851 | best_loss=0.02837
Epoch 24/80: current_loss=0.03068 | best_loss=0.02837
Epoch 25/80: current_loss=0.02857 | best_loss=0.02837
Epoch 26/80: current_loss=0.02854 | best_loss=0.02837
Epoch 27/80: current_loss=0.02868 | best_loss=0.02837
Epoch 28/80: current_loss=0.02864 | best_loss=0.02837
Epoch 29/80: current_loss=0.02853 | best_loss=0.02837
Epoch 30/80: current_loss=0.02850 | best_loss=0.02837
Epoch 31/80: current_loss=0.02850 | best_loss=0.02837
Early Stopping at epoch 31
      explained_var=-0.01260 | mse_loss=0.02873
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03302 | best_loss=0.03302
Epoch 1/80: current_loss=0.03373 | best_loss=0.03302
Epoch 2/80: current_loss=0.03360 | best_loss=0.03302
Epoch 3/80: current_loss=0.03392 | best_loss=0.03302
Epoch 4/80: current_loss=0.03382 | best_loss=0.03302
Epoch 5/80: current_loss=0.03365 | best_loss=0.03302
Epoch 6/80: current_loss=0.03395 | best_loss=0.03302
Epoch 7/80: current_loss=0.03385 | best_loss=0.03302
Epoch 8/80: current_loss=0.03379 | best_loss=0.03302
Epoch 9/80: current_loss=0.03402 | best_loss=0.03302
Epoch 10/80: current_loss=0.03394 | best_loss=0.03302
Epoch 11/80: current_loss=0.03388 | best_loss=0.03302
Epoch 12/80: current_loss=0.03397 | best_loss=0.03302
Epoch 13/80: current_loss=0.03388 | best_loss=0.03302
Epoch 14/80: current_loss=0.03368 | best_loss=0.03302
Epoch 15/80: current_loss=0.03361 | best_loss=0.03302
Epoch 16/80: current_loss=0.03380 | best_loss=0.03302
Epoch 17/80: current_loss=0.03364 | best_loss=0.03302
Epoch 18/80: current_loss=0.03383 | best_loss=0.03302
Epoch 19/80: current_loss=0.03374 | best_loss=0.03302
Epoch 20/80: current_loss=0.03364 | best_loss=0.03302
Early Stopping at epoch 20
      explained_var=-0.03813 | mse_loss=0.03385
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.00711| avg_loss=0.02873
----------------------------------------------


----------------------------------------------
Params for Trial 6
{'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.05251 | best_loss=0.05251
Epoch 1/80: current_loss=0.02937 | best_loss=0.02937
Epoch 2/80: current_loss=0.03289 | best_loss=0.02937
Epoch 3/80: current_loss=0.03141 | best_loss=0.02937
Epoch 4/80: current_loss=0.02920 | best_loss=0.02920
Epoch 5/80: current_loss=0.02957 | best_loss=0.02920
Epoch 6/80: current_loss=0.02953 | best_loss=0.02920
Epoch 7/80: current_loss=0.02917 | best_loss=0.02917
Epoch 8/80: current_loss=0.02854 | best_loss=0.02854
Epoch 9/80: current_loss=0.02894 | best_loss=0.02854
Epoch 10/80: current_loss=0.02880 | best_loss=0.02854
Epoch 11/80: current_loss=0.02775 | best_loss=0.02775
Epoch 12/80: current_loss=0.02815 | best_loss=0.02775
Epoch 13/80: current_loss=0.02865 | best_loss=0.02775
Epoch 14/80: current_loss=0.02807 | best_loss=0.02775
Epoch 15/80: current_loss=0.02767 | best_loss=0.02767
Epoch 16/80: current_loss=0.02829 | best_loss=0.02767
Epoch 17/80: current_loss=0.02750 | best_loss=0.02750
Epoch 18/80: current_loss=0.02741 | best_loss=0.02741
Epoch 19/80: current_loss=0.02657 | best_loss=0.02657
Epoch 20/80: current_loss=0.02700 | best_loss=0.02657
Epoch 21/80: current_loss=0.02690 | best_loss=0.02657
Epoch 22/80: current_loss=0.02645 | best_loss=0.02645
Epoch 23/80: current_loss=0.02688 | best_loss=0.02645
Epoch 24/80: current_loss=0.02746 | best_loss=0.02645
Epoch 25/80: current_loss=0.02605 | best_loss=0.02605
Epoch 26/80: current_loss=0.02673 | best_loss=0.02605
Epoch 27/80: current_loss=0.02661 | best_loss=0.02605
Epoch 28/80: current_loss=0.02667 | best_loss=0.02605
Epoch 29/80: current_loss=0.02667 | best_loss=0.02605
Epoch 30/80: current_loss=0.02658 | best_loss=0.02605
Epoch 31/80: current_loss=0.02709 | best_loss=0.02605
Epoch 32/80: current_loss=0.02652 | best_loss=0.02605
Epoch 33/80: current_loss=0.02656 | best_loss=0.02605
Epoch 34/80: current_loss=0.02655 | best_loss=0.02605
Epoch 35/80: current_loss=0.02617 | best_loss=0.02605
Epoch 36/80: current_loss=0.02633 | best_loss=0.02605
Epoch 37/80: current_loss=0.02710 | best_loss=0.02605
Epoch 38/80: current_loss=0.02591 | best_loss=0.02591
Epoch 39/80: current_loss=0.02682 | best_loss=0.02591
Epoch 40/80: current_loss=0.02666 | best_loss=0.02591
Epoch 41/80: current_loss=0.02621 | best_loss=0.02591
Epoch 42/80: current_loss=0.02617 | best_loss=0.02591
Epoch 43/80: current_loss=0.02617 | best_loss=0.02591
Epoch 44/80: current_loss=0.02704 | best_loss=0.02591
Epoch 45/80: current_loss=0.02641 | best_loss=0.02591
Epoch 46/80: current_loss=0.02662 | best_loss=0.02591
Epoch 47/80: current_loss=0.02647 | best_loss=0.02591
Epoch 48/80: current_loss=0.02654 | best_loss=0.02591
Epoch 49/80: current_loss=0.02669 | best_loss=0.02591
Epoch 50/80: current_loss=0.02641 | best_loss=0.02591
Epoch 51/80: current_loss=0.02648 | best_loss=0.02591
Epoch 52/80: current_loss=0.02604 | best_loss=0.02591
Epoch 53/80: current_loss=0.02592 | best_loss=0.02591
Epoch 54/80: current_loss=0.02699 | best_loss=0.02591
Epoch 55/80: current_loss=0.02684 | best_loss=0.02591
Epoch 56/80: current_loss=0.02685 | best_loss=0.02591
Epoch 57/80: current_loss=0.02592 | best_loss=0.02591
Epoch 58/80: current_loss=0.02622 | best_loss=0.02591
Early Stopping at epoch 58
      explained_var=0.03307 | mse_loss=0.02549
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02805 | best_loss=0.02805
Epoch 1/80: current_loss=0.02814 | best_loss=0.02805
Epoch 2/80: current_loss=0.02816 | best_loss=0.02805
Epoch 3/80: current_loss=0.02811 | best_loss=0.02805
Epoch 4/80: current_loss=0.02814 | best_loss=0.02805
Epoch 5/80: current_loss=0.02821 | best_loss=0.02805
Epoch 6/80: current_loss=0.02827 | best_loss=0.02805
Epoch 7/80: current_loss=0.02825 | best_loss=0.02805
Epoch 8/80: current_loss=0.02824 | best_loss=0.02805
Epoch 9/80: current_loss=0.02833 | best_loss=0.02805
Epoch 10/80: current_loss=0.02839 | best_loss=0.02805
Epoch 11/80: current_loss=0.02836 | best_loss=0.02805
Epoch 12/80: current_loss=0.02829 | best_loss=0.02805
Epoch 13/80: current_loss=0.02842 | best_loss=0.02805
Epoch 14/80: current_loss=0.02836 | best_loss=0.02805
Epoch 15/80: current_loss=0.02838 | best_loss=0.02805
Epoch 16/80: current_loss=0.02830 | best_loss=0.02805
Epoch 17/80: current_loss=0.02834 | best_loss=0.02805
Epoch 18/80: current_loss=0.02827 | best_loss=0.02805
Epoch 19/80: current_loss=0.02818 | best_loss=0.02805
Epoch 20/80: current_loss=0.02836 | best_loss=0.02805
Early Stopping at epoch 20
      explained_var=0.00527 | mse_loss=0.02765
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03010 | best_loss=0.03010
Epoch 1/80: current_loss=0.03056 | best_loss=0.03010
Epoch 2/80: current_loss=0.03093 | best_loss=0.03010
Epoch 3/80: current_loss=0.03051 | best_loss=0.03010
Epoch 4/80: current_loss=0.03071 | best_loss=0.03010
Epoch 5/80: current_loss=0.03067 | best_loss=0.03010
Epoch 6/80: current_loss=0.03131 | best_loss=0.03010
Epoch 7/80: current_loss=0.03085 | best_loss=0.03010
Epoch 8/80: current_loss=0.03052 | best_loss=0.03010
Epoch 9/80: current_loss=0.03067 | best_loss=0.03010
Epoch 10/80: current_loss=0.03159 | best_loss=0.03010
Epoch 11/80: current_loss=0.03006 | best_loss=0.03006
Epoch 12/80: current_loss=0.03015 | best_loss=0.03006
Epoch 13/80: current_loss=0.03058 | best_loss=0.03006
Epoch 14/80: current_loss=0.03111 | best_loss=0.03006
Epoch 15/80: current_loss=0.03000 | best_loss=0.03000
Epoch 16/80: current_loss=0.03147 | best_loss=0.03000
Epoch 17/80: current_loss=0.03010 | best_loss=0.03000
Epoch 18/80: current_loss=0.03041 | best_loss=0.03000
Epoch 19/80: current_loss=0.03019 | best_loss=0.03000
Epoch 20/80: current_loss=0.03035 | best_loss=0.03000
Epoch 21/80: current_loss=0.03080 | best_loss=0.03000
Epoch 22/80: current_loss=0.03065 | best_loss=0.03000
Epoch 23/80: current_loss=0.03134 | best_loss=0.03000
Epoch 24/80: current_loss=0.02997 | best_loss=0.02997
Epoch 25/80: current_loss=0.03128 | best_loss=0.02997
Epoch 26/80: current_loss=0.03101 | best_loss=0.02997
Epoch 27/80: current_loss=0.03025 | best_loss=0.02997
Epoch 28/80: current_loss=0.03058 | best_loss=0.02997
Epoch 29/80: current_loss=0.03035 | best_loss=0.02997
Epoch 30/80: current_loss=0.03099 | best_loss=0.02997
Epoch 31/80: current_loss=0.03012 | best_loss=0.02997
Epoch 32/80: current_loss=0.03041 | best_loss=0.02997
Epoch 33/80: current_loss=0.03019 | best_loss=0.02997
Epoch 34/80: current_loss=0.03058 | best_loss=0.02997
Epoch 35/80: current_loss=0.03048 | best_loss=0.02997
Epoch 36/80: current_loss=0.03059 | best_loss=0.02997
Epoch 37/80: current_loss=0.03010 | best_loss=0.02997
Epoch 38/80: current_loss=0.03070 | best_loss=0.02997
Epoch 39/80: current_loss=0.03114 | best_loss=0.02997
Epoch 40/80: current_loss=0.02991 | best_loss=0.02991
Epoch 41/80: current_loss=0.03051 | best_loss=0.02991
Epoch 42/80: current_loss=0.03107 | best_loss=0.02991
Epoch 43/80: current_loss=0.03034 | best_loss=0.02991
Epoch 44/80: current_loss=0.03115 | best_loss=0.02991
Epoch 45/80: current_loss=0.03061 | best_loss=0.02991
Epoch 46/80: current_loss=0.03051 | best_loss=0.02991
Epoch 47/80: current_loss=0.03063 | best_loss=0.02991
Epoch 48/80: current_loss=0.03023 | best_loss=0.02991
Epoch 49/80: current_loss=0.03017 | best_loss=0.02991
Epoch 50/80: current_loss=0.03085 | best_loss=0.02991
Epoch 51/80: current_loss=0.03068 | best_loss=0.02991
Epoch 52/80: current_loss=0.02988 | best_loss=0.02988
Epoch 53/80: current_loss=0.03035 | best_loss=0.02988
Epoch 54/80: current_loss=0.03138 | best_loss=0.02988
Epoch 55/80: current_loss=0.03027 | best_loss=0.02988
Epoch 56/80: current_loss=0.02995 | best_loss=0.02988
Epoch 57/80: current_loss=0.03095 | best_loss=0.02988
Epoch 58/80: current_loss=0.03083 | best_loss=0.02988
Epoch 59/80: current_loss=0.03008 | best_loss=0.02988
Epoch 60/80: current_loss=0.03079 | best_loss=0.02988
Epoch 61/80: current_loss=0.03045 | best_loss=0.02988
Epoch 62/80: current_loss=0.03018 | best_loss=0.02988
Epoch 63/80: current_loss=0.03169 | best_loss=0.02988
Epoch 64/80: current_loss=0.03015 | best_loss=0.02988
Epoch 65/80: current_loss=0.03004 | best_loss=0.02988
Epoch 66/80: current_loss=0.03146 | best_loss=0.02988
Epoch 67/80: current_loss=0.03031 | best_loss=0.02988
Epoch 68/80: current_loss=0.03023 | best_loss=0.02988
Epoch 69/80: current_loss=0.03025 | best_loss=0.02988
Epoch 70/80: current_loss=0.02999 | best_loss=0.02988
Epoch 71/80: current_loss=0.03077 | best_loss=0.02988
Epoch 72/80: current_loss=0.03006 | best_loss=0.02988
Early Stopping at epoch 72
      explained_var=0.02726 | mse_loss=0.02912
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02837 | best_loss=0.02837
Epoch 1/80: current_loss=0.02829 | best_loss=0.02829
Epoch 2/80: current_loss=0.02822 | best_loss=0.02822
Epoch 3/80: current_loss=0.02823 | best_loss=0.02822
Epoch 4/80: current_loss=0.02844 | best_loss=0.02822
Epoch 5/80: current_loss=0.02828 | best_loss=0.02822
Epoch 6/80: current_loss=0.02849 | best_loss=0.02822
Epoch 7/80: current_loss=0.02857 | best_loss=0.02822
Epoch 8/80: current_loss=0.02861 | best_loss=0.02822
Epoch 9/80: current_loss=0.02895 | best_loss=0.02822
Epoch 10/80: current_loss=0.02866 | best_loss=0.02822
Epoch 11/80: current_loss=0.02868 | best_loss=0.02822
Epoch 12/80: current_loss=0.02878 | best_loss=0.02822
Epoch 13/80: current_loss=0.02873 | best_loss=0.02822
Epoch 14/80: current_loss=0.02871 | best_loss=0.02822
Epoch 15/80: current_loss=0.02875 | best_loss=0.02822
Epoch 16/80: current_loss=0.02882 | best_loss=0.02822
Epoch 17/80: current_loss=0.02870 | best_loss=0.02822
Epoch 18/80: current_loss=0.02878 | best_loss=0.02822
Epoch 19/80: current_loss=0.02888 | best_loss=0.02822
Epoch 20/80: current_loss=0.02881 | best_loss=0.02822
Epoch 21/80: current_loss=0.02861 | best_loss=0.02822
Epoch 22/80: current_loss=0.02872 | best_loss=0.02822
Early Stopping at epoch 22
      explained_var=-0.00736 | mse_loss=0.02858
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03349 | best_loss=0.03349
Epoch 1/80: current_loss=0.03365 | best_loss=0.03349
Epoch 2/80: current_loss=0.03356 | best_loss=0.03349
Epoch 3/80: current_loss=0.03348 | best_loss=0.03348
Epoch 4/80: current_loss=0.03364 | best_loss=0.03348
Epoch 5/80: current_loss=0.03360 | best_loss=0.03348
Epoch 6/80: current_loss=0.03368 | best_loss=0.03348
Epoch 7/80: current_loss=0.03374 | best_loss=0.03348
Epoch 8/80: current_loss=0.03368 | best_loss=0.03348
Epoch 9/80: current_loss=0.03383 | best_loss=0.03348
Epoch 10/80: current_loss=0.03372 | best_loss=0.03348
Epoch 11/80: current_loss=0.03388 | best_loss=0.03348
Epoch 12/80: current_loss=0.03400 | best_loss=0.03348
Epoch 13/80: current_loss=0.03410 | best_loss=0.03348
Epoch 14/80: current_loss=0.03423 | best_loss=0.03348
Epoch 15/80: current_loss=0.03411 | best_loss=0.03348
Epoch 16/80: current_loss=0.03413 | best_loss=0.03348
Epoch 17/80: current_loss=0.03421 | best_loss=0.03348
Epoch 18/80: current_loss=0.03416 | best_loss=0.03348
Epoch 19/80: current_loss=0.03414 | best_loss=0.03348
Epoch 20/80: current_loss=0.03417 | best_loss=0.03348
Epoch 21/80: current_loss=0.03418 | best_loss=0.03348
Epoch 22/80: current_loss=0.03410 | best_loss=0.03348
Epoch 23/80: current_loss=0.03419 | best_loss=0.03348
Early Stopping at epoch 23
      explained_var=-0.05436 | mse_loss=0.03425
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00078| avg_loss=0.02902
----------------------------------------------


----------------------------------------------
Params for Trial 7
{'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=61.16597 | best_loss=61.16597
Epoch 1/80: current_loss=29.19778 | best_loss=29.19778
Epoch 2/80: current_loss=6.66295 | best_loss=6.66295
Epoch 3/80: current_loss=0.80419 | best_loss=0.80419
Epoch 4/80: current_loss=0.38802 | best_loss=0.38802
Epoch 5/80: current_loss=0.77749 | best_loss=0.38802
Epoch 6/80: current_loss=2.21693 | best_loss=0.38802
Epoch 7/80: current_loss=0.70503 | best_loss=0.38802
Epoch 8/80: current_loss=0.18329 | best_loss=0.18329
Epoch 9/80: current_loss=0.30633 | best_loss=0.18329
Epoch 10/80: current_loss=0.70388 | best_loss=0.18329
Epoch 11/80: current_loss=0.51347 | best_loss=0.18329
Epoch 12/80: current_loss=0.41829 | best_loss=0.18329
Epoch 13/80: current_loss=0.50838 | best_loss=0.18329
Epoch 14/80: current_loss=0.25504 | best_loss=0.18329
Epoch 15/80: current_loss=0.41964 | best_loss=0.18329
Epoch 16/80: current_loss=1.05395 | best_loss=0.18329
Epoch 17/80: current_loss=0.03547 | best_loss=0.03547
Epoch 18/80: current_loss=0.08094 | best_loss=0.03547
Epoch 19/80: current_loss=0.21339 | best_loss=0.03547
Epoch 20/80: current_loss=0.04328 | best_loss=0.03547
Epoch 21/80: current_loss=0.03639 | best_loss=0.03547
Epoch 22/80: current_loss=0.19304 | best_loss=0.03547
Epoch 23/80: current_loss=0.04392 | best_loss=0.03547
Epoch 24/80: current_loss=0.17174 | best_loss=0.03547
Epoch 25/80: current_loss=0.36084 | best_loss=0.03547
Epoch 26/80: current_loss=0.02704 | best_loss=0.02704
Epoch 27/80: current_loss=0.03301 | best_loss=0.02704
Epoch 28/80: current_loss=0.39648 | best_loss=0.02704
Epoch 29/80: current_loss=0.21094 | best_loss=0.02704
Epoch 30/80: current_loss=0.06076 | best_loss=0.02704
Epoch 31/80: current_loss=0.03309 | best_loss=0.02704
Epoch 32/80: current_loss=0.02712 | best_loss=0.02704
Epoch 33/80: current_loss=0.08150 | best_loss=0.02704
Epoch 34/80: current_loss=0.02726 | best_loss=0.02704
Epoch 35/80: current_loss=0.03190 | best_loss=0.02704
Epoch 36/80: current_loss=0.10511 | best_loss=0.02704
Epoch 37/80: current_loss=0.06866 | best_loss=0.02704
Epoch 38/80: current_loss=0.08077 | best_loss=0.02704
Epoch 39/80: current_loss=0.02894 | best_loss=0.02704
Epoch 40/80: current_loss=0.02958 | best_loss=0.02704
Epoch 41/80: current_loss=0.06050 | best_loss=0.02704
Epoch 42/80: current_loss=0.02861 | best_loss=0.02704
Epoch 43/80: current_loss=0.06585 | best_loss=0.02704
Epoch 44/80: current_loss=0.07882 | best_loss=0.02704
Epoch 45/80: current_loss=0.02960 | best_loss=0.02704
Epoch 46/80: current_loss=0.06333 | best_loss=0.02704
Early Stopping at epoch 46
      explained_var=-0.00667 | mse_loss=0.02622
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=37.58913 | best_loss=37.58913
Epoch 1/80: current_loss=8.67286 | best_loss=8.67286
Epoch 2/80: current_loss=2.63889 | best_loss=2.63889
Epoch 3/80: current_loss=3.86731 | best_loss=2.63889
Epoch 4/80: current_loss=4.49173 | best_loss=2.63889
Epoch 5/80: current_loss=0.75048 | best_loss=0.75048
Epoch 6/80: current_loss=2.02790 | best_loss=0.75048
Epoch 7/80: current_loss=6.74007 | best_loss=0.75048
Epoch 8/80: current_loss=0.28596 | best_loss=0.28596
Epoch 9/80: current_loss=3.42767 | best_loss=0.28596
Epoch 10/80: current_loss=0.60520 | best_loss=0.28596
Epoch 11/80: current_loss=1.19305 | best_loss=0.28596
Epoch 12/80: current_loss=0.22107 | best_loss=0.22107
Epoch 13/80: current_loss=1.38274 | best_loss=0.22107
Epoch 14/80: current_loss=0.22526 | best_loss=0.22107
Epoch 15/80: current_loss=0.73499 | best_loss=0.22107
Epoch 16/80: current_loss=1.12157 | best_loss=0.22107
Epoch 17/80: current_loss=0.33348 | best_loss=0.22107
Epoch 18/80: current_loss=0.79470 | best_loss=0.22107
Epoch 19/80: current_loss=0.04545 | best_loss=0.04545
Epoch 20/80: current_loss=0.28367 | best_loss=0.04545
Epoch 21/80: current_loss=0.59577 | best_loss=0.04545
Epoch 22/80: current_loss=0.85443 | best_loss=0.04545
Epoch 23/80: current_loss=0.25947 | best_loss=0.04545
Epoch 24/80: current_loss=0.10471 | best_loss=0.04545
Epoch 25/80: current_loss=0.15054 | best_loss=0.04545
Epoch 26/80: current_loss=0.04824 | best_loss=0.04545
Epoch 27/80: current_loss=0.08872 | best_loss=0.04545
Epoch 28/80: current_loss=0.33899 | best_loss=0.04545
Epoch 29/80: current_loss=0.48500 | best_loss=0.04545
Epoch 30/80: current_loss=0.13816 | best_loss=0.04545
Epoch 31/80: current_loss=0.27104 | best_loss=0.04545
Epoch 32/80: current_loss=0.78903 | best_loss=0.04545
Epoch 33/80: current_loss=0.66135 | best_loss=0.04545
Epoch 34/80: current_loss=1.02152 | best_loss=0.04545
Epoch 35/80: current_loss=0.53997 | best_loss=0.04545
Epoch 36/80: current_loss=0.85866 | best_loss=0.04545
Epoch 37/80: current_loss=1.41390 | best_loss=0.04545
Epoch 38/80: current_loss=1.53608 | best_loss=0.04545
Epoch 39/80: current_loss=1.59177 | best_loss=0.04545
Early Stopping at epoch 39
      explained_var=-0.66802 | mse_loss=0.04642
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=22.52065 | best_loss=22.52065
Epoch 1/80: current_loss=7.48625 | best_loss=7.48625
Epoch 2/80: current_loss=8.63740 | best_loss=7.48625
Epoch 3/80: current_loss=3.16505 | best_loss=3.16505
Epoch 4/80: current_loss=0.65642 | best_loss=0.65642
Epoch 5/80: current_loss=3.36719 | best_loss=0.65642
Epoch 6/80: current_loss=3.88337 | best_loss=0.65642
Epoch 7/80: current_loss=1.18348 | best_loss=0.65642
Epoch 8/80: current_loss=1.64028 | best_loss=0.65642
Epoch 9/80: current_loss=1.04593 | best_loss=0.65642
Epoch 10/80: current_loss=0.75934 | best_loss=0.65642
Epoch 11/80: current_loss=0.08529 | best_loss=0.08529
Epoch 12/80: current_loss=0.25746 | best_loss=0.08529
Epoch 13/80: current_loss=0.46902 | best_loss=0.08529
Epoch 14/80: current_loss=0.19413 | best_loss=0.08529
Epoch 15/80: current_loss=0.06425 | best_loss=0.06425
Epoch 16/80: current_loss=0.06105 | best_loss=0.06105
Epoch 17/80: current_loss=0.04252 | best_loss=0.04252
Epoch 18/80: current_loss=0.05856 | best_loss=0.04252
Epoch 19/80: current_loss=0.41629 | best_loss=0.04252
Epoch 20/80: current_loss=0.42043 | best_loss=0.04252
Epoch 21/80: current_loss=0.52884 | best_loss=0.04252
Epoch 22/80: current_loss=0.38630 | best_loss=0.04252
Epoch 23/80: current_loss=0.60148 | best_loss=0.04252
Epoch 24/80: current_loss=0.39415 | best_loss=0.04252
Epoch 25/80: current_loss=0.58155 | best_loss=0.04252
Epoch 26/80: current_loss=0.19689 | best_loss=0.04252
Epoch 27/80: current_loss=0.45184 | best_loss=0.04252
Epoch 28/80: current_loss=0.14678 | best_loss=0.04252
Epoch 29/80: current_loss=0.15845 | best_loss=0.04252
Epoch 30/80: current_loss=0.12678 | best_loss=0.04252
Epoch 31/80: current_loss=0.04818 | best_loss=0.04252
Epoch 32/80: current_loss=0.40589 | best_loss=0.04252
Epoch 33/80: current_loss=0.64310 | best_loss=0.04252
Epoch 34/80: current_loss=0.77026 | best_loss=0.04252
Epoch 35/80: current_loss=0.15767 | best_loss=0.04252
Epoch 36/80: current_loss=0.51520 | best_loss=0.04252
Epoch 37/80: current_loss=0.49793 | best_loss=0.04252
Early Stopping at epoch 37
      explained_var=-0.41126 | mse_loss=0.04211
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=2.97701 | best_loss=2.97701
Epoch 1/80: current_loss=4.40186 | best_loss=2.97701
Epoch 2/80: current_loss=3.39496 | best_loss=2.97701
Epoch 3/80: current_loss=3.17277 | best_loss=2.97701
Epoch 4/80: current_loss=5.77841 | best_loss=2.97701
Epoch 5/80: current_loss=6.23229 | best_loss=2.97701
Epoch 6/80: current_loss=28.37118 | best_loss=2.97701
Epoch 7/80: current_loss=30.60843 | best_loss=2.97701
Epoch 8/80: current_loss=3.51382 | best_loss=2.97701
Epoch 9/80: current_loss=0.11091 | best_loss=0.11091
Epoch 10/80: current_loss=2.96362 | best_loss=0.11091
Epoch 11/80: current_loss=0.11231 | best_loss=0.11091
Epoch 12/80: current_loss=1.74422 | best_loss=0.11091
Epoch 13/80: current_loss=0.12305 | best_loss=0.11091
Epoch 14/80: current_loss=1.99106 | best_loss=0.11091
Epoch 15/80: current_loss=1.90711 | best_loss=0.11091
Epoch 16/80: current_loss=0.14775 | best_loss=0.11091
Epoch 17/80: current_loss=0.27460 | best_loss=0.11091
Epoch 18/80: current_loss=0.18154 | best_loss=0.11091
Epoch 19/80: current_loss=0.08868 | best_loss=0.08868
Epoch 20/80: current_loss=0.42368 | best_loss=0.08868
Epoch 21/80: current_loss=0.24300 | best_loss=0.08868
Epoch 22/80: current_loss=0.47983 | best_loss=0.08868
Epoch 23/80: current_loss=0.08001 | best_loss=0.08001
Epoch 24/80: current_loss=1.57467 | best_loss=0.08001
Epoch 25/80: current_loss=5.47993 | best_loss=0.08001
Epoch 26/80: current_loss=1.32030 | best_loss=0.08001
Epoch 27/80: current_loss=1.38986 | best_loss=0.08001
Epoch 28/80: current_loss=0.76862 | best_loss=0.08001
Epoch 29/80: current_loss=3.58311 | best_loss=0.08001
Epoch 30/80: current_loss=2.77425 | best_loss=0.08001
Epoch 31/80: current_loss=2.24359 | best_loss=0.08001
Epoch 32/80: current_loss=0.61700 | best_loss=0.08001
Epoch 33/80: current_loss=2.64528 | best_loss=0.08001
Epoch 34/80: current_loss=0.39635 | best_loss=0.08001
Epoch 35/80: current_loss=10.70852 | best_loss=0.08001
Epoch 36/80: current_loss=25.13508 | best_loss=0.08001
Epoch 37/80: current_loss=6.01109 | best_loss=0.08001
Epoch 38/80: current_loss=7.30231 | best_loss=0.08001
Epoch 39/80: current_loss=1.58270 | best_loss=0.08001
Epoch 40/80: current_loss=0.48173 | best_loss=0.08001
Epoch 41/80: current_loss=10.84141 | best_loss=0.08001
Epoch 42/80: current_loss=2.01873 | best_loss=0.08001
Epoch 43/80: current_loss=6.93159 | best_loss=0.08001
Early Stopping at epoch 43
      explained_var=-1.74808 | mse_loss=0.08014
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=4.96008 | best_loss=4.96008
Epoch 1/80: current_loss=4.76557 | best_loss=4.76557
Epoch 2/80: current_loss=0.88248 | best_loss=0.88248
Epoch 3/80: current_loss=2.20269 | best_loss=0.88248
Epoch 4/80: current_loss=6.62827 | best_loss=0.88248
Epoch 5/80: current_loss=16.60894 | best_loss=0.88248
Epoch 6/80: current_loss=5.74149 | best_loss=0.88248
Epoch 7/80: current_loss=0.50066 | best_loss=0.50066
Epoch 8/80: current_loss=7.77242 | best_loss=0.50066
Epoch 9/80: current_loss=10.75692 | best_loss=0.50066
Epoch 10/80: current_loss=1.56747 | best_loss=0.50066
Epoch 11/80: current_loss=3.33359 | best_loss=0.50066
Epoch 12/80: current_loss=1.75408 | best_loss=0.50066
Epoch 13/80: current_loss=0.16236 | best_loss=0.16236
Epoch 14/80: current_loss=0.64104 | best_loss=0.16236
Epoch 15/80: current_loss=0.57861 | best_loss=0.16236
Epoch 16/80: current_loss=2.08977 | best_loss=0.16236
Epoch 17/80: current_loss=0.35796 | best_loss=0.16236
Epoch 18/80: current_loss=0.05092 | best_loss=0.05092
Epoch 19/80: current_loss=0.94185 | best_loss=0.05092
Epoch 20/80: current_loss=0.04410 | best_loss=0.04410
Epoch 21/80: current_loss=0.06924 | best_loss=0.04410
Epoch 22/80: current_loss=0.05542 | best_loss=0.04410
Epoch 23/80: current_loss=0.06623 | best_loss=0.04410
Epoch 24/80: current_loss=0.07980 | best_loss=0.04410
Epoch 25/80: current_loss=0.09928 | best_loss=0.04410
Epoch 26/80: current_loss=0.16743 | best_loss=0.04410
Epoch 27/80: current_loss=0.04615 | best_loss=0.04410
Epoch 28/80: current_loss=0.06729 | best_loss=0.04410
Epoch 29/80: current_loss=0.03967 | best_loss=0.03967
Epoch 30/80: current_loss=0.16091 | best_loss=0.03967
Epoch 31/80: current_loss=0.13007 | best_loss=0.03967
Epoch 32/80: current_loss=0.14556 | best_loss=0.03967
Epoch 33/80: current_loss=0.03729 | best_loss=0.03729
Epoch 34/80: current_loss=0.04002 | best_loss=0.03729
Epoch 35/80: current_loss=0.28981 | best_loss=0.03729
Epoch 36/80: current_loss=0.30050 | best_loss=0.03729
Epoch 37/80: current_loss=0.10359 | best_loss=0.03729
Epoch 38/80: current_loss=0.07228 | best_loss=0.03729
Epoch 39/80: current_loss=1.61869 | best_loss=0.03729
Epoch 40/80: current_loss=1.81854 | best_loss=0.03729
Epoch 41/80: current_loss=5.18803 | best_loss=0.03729
Epoch 42/80: current_loss=0.35889 | best_loss=0.03729
Epoch 43/80: current_loss=1.27618 | best_loss=0.03729
Epoch 44/80: current_loss=1.31226 | best_loss=0.03729
Epoch 45/80: current_loss=0.18855 | best_loss=0.03729
Epoch 46/80: current_loss=13.87235 | best_loss=0.03729
Epoch 47/80: current_loss=1.86129 | best_loss=0.03729
Epoch 48/80: current_loss=0.62362 | best_loss=0.03729
Epoch 49/80: current_loss=2.65003 | best_loss=0.03729
Epoch 50/80: current_loss=1.33060 | best_loss=0.03729
Epoch 51/80: current_loss=0.94154 | best_loss=0.03729
Epoch 52/80: current_loss=0.33059 | best_loss=0.03729
Epoch 53/80: current_loss=1.90906 | best_loss=0.03729
Early Stopping at epoch 53
      explained_var=0.00727 | mse_loss=0.03782
----------------------------------------------
Average early_stopping_point: 23| avg_exp_var=-0.56535| avg_loss=0.04654
----------------------------------------------


----------------------------------------------
Params for Trial 8
{'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.16258 | best_loss=0.16258
Epoch 1/80: current_loss=0.15771 | best_loss=0.15771
Epoch 2/80: current_loss=0.15307 | best_loss=0.15307
Epoch 3/80: current_loss=0.14855 | best_loss=0.14855
Epoch 4/80: current_loss=0.14421 | best_loss=0.14421
Epoch 5/80: current_loss=0.13992 | best_loss=0.13992
Epoch 6/80: current_loss=0.13570 | best_loss=0.13570
Epoch 7/80: current_loss=0.13163 | best_loss=0.13163
Epoch 8/80: current_loss=0.12771 | best_loss=0.12771
Epoch 9/80: current_loss=0.12392 | best_loss=0.12392
Epoch 10/80: current_loss=0.12025 | best_loss=0.12025
Epoch 11/80: current_loss=0.11657 | best_loss=0.11657
Epoch 12/80: current_loss=0.11299 | best_loss=0.11299
Epoch 13/80: current_loss=0.10950 | best_loss=0.10950
Epoch 14/80: current_loss=0.10615 | best_loss=0.10615
Epoch 15/80: current_loss=0.10295 | best_loss=0.10295
Epoch 16/80: current_loss=0.09972 | best_loss=0.09972
Epoch 17/80: current_loss=0.09662 | best_loss=0.09662
Epoch 18/80: current_loss=0.09362 | best_loss=0.09362
Epoch 19/80: current_loss=0.09069 | best_loss=0.09069
Epoch 20/80: current_loss=0.08790 | best_loss=0.08790
Epoch 21/80: current_loss=0.08516 | best_loss=0.08516
Epoch 22/80: current_loss=0.08247 | best_loss=0.08247
Epoch 23/80: current_loss=0.07985 | best_loss=0.07985
Epoch 24/80: current_loss=0.07737 | best_loss=0.07737
Epoch 25/80: current_loss=0.07494 | best_loss=0.07494
Epoch 26/80: current_loss=0.07259 | best_loss=0.07259
Epoch 27/80: current_loss=0.07033 | best_loss=0.07033
Epoch 28/80: current_loss=0.06814 | best_loss=0.06814
Epoch 29/80: current_loss=0.06603 | best_loss=0.06603
Epoch 30/80: current_loss=0.06399 | best_loss=0.06399
Epoch 31/80: current_loss=0.06204 | best_loss=0.06204
Epoch 32/80: current_loss=0.06016 | best_loss=0.06016
Epoch 33/80: current_loss=0.05830 | best_loss=0.05830
Epoch 34/80: current_loss=0.05649 | best_loss=0.05649
Epoch 35/80: current_loss=0.05479 | best_loss=0.05479
Epoch 36/80: current_loss=0.05321 | best_loss=0.05321
Epoch 37/80: current_loss=0.05165 | best_loss=0.05165
Epoch 38/80: current_loss=0.05013 | best_loss=0.05013
Epoch 39/80: current_loss=0.04872 | best_loss=0.04872
Epoch 40/80: current_loss=0.04742 | best_loss=0.04742
Epoch 41/80: current_loss=0.04619 | best_loss=0.04619
Epoch 42/80: current_loss=0.04505 | best_loss=0.04505
Epoch 43/80: current_loss=0.04394 | best_loss=0.04394
Epoch 44/80: current_loss=0.04285 | best_loss=0.04285
Epoch 45/80: current_loss=0.04190 | best_loss=0.04190
Epoch 46/80: current_loss=0.04101 | best_loss=0.04101
Epoch 47/80: current_loss=0.04016 | best_loss=0.04016
Epoch 48/80: current_loss=0.03935 | best_loss=0.03935
Epoch 49/80: current_loss=0.03862 | best_loss=0.03862
Epoch 50/80: current_loss=0.03793 | best_loss=0.03793
Epoch 51/80: current_loss=0.03732 | best_loss=0.03732
Epoch 52/80: current_loss=0.03677 | best_loss=0.03677
Epoch 53/80: current_loss=0.03623 | best_loss=0.03623
Epoch 54/80: current_loss=0.03574 | best_loss=0.03574
Epoch 55/80: current_loss=0.03530 | best_loss=0.03530
Epoch 56/80: current_loss=0.03493 | best_loss=0.03493
Epoch 57/80: current_loss=0.03463 | best_loss=0.03463
Epoch 58/80: current_loss=0.03436 | best_loss=0.03436
Epoch 59/80: current_loss=0.03410 | best_loss=0.03410
Epoch 60/80: current_loss=0.03387 | best_loss=0.03387
Epoch 61/80: current_loss=0.03368 | best_loss=0.03368
Epoch 62/80: current_loss=0.03350 | best_loss=0.03350
Epoch 63/80: current_loss=0.03337 | best_loss=0.03337
Epoch 64/80: current_loss=0.03326 | best_loss=0.03326
Epoch 65/80: current_loss=0.03316 | best_loss=0.03316
Epoch 66/80: current_loss=0.03309 | best_loss=0.03309
Epoch 67/80: current_loss=0.03302 | best_loss=0.03302
Epoch 68/80: current_loss=0.03297 | best_loss=0.03297
Epoch 69/80: current_loss=0.03292 | best_loss=0.03292
Epoch 70/80: current_loss=0.03289 | best_loss=0.03289
Epoch 71/80: current_loss=0.03286 | best_loss=0.03286
Epoch 72/80: current_loss=0.03284 | best_loss=0.03284
Epoch 73/80: current_loss=0.03283 | best_loss=0.03283
Epoch 74/80: current_loss=0.03282 | best_loss=0.03282
Epoch 75/80: current_loss=0.03281 | best_loss=0.03281
Epoch 76/80: current_loss=0.03281 | best_loss=0.03281
Epoch 77/80: current_loss=0.03281 | best_loss=0.03281
Epoch 78/80: current_loss=0.03281 | best_loss=0.03281
Epoch 79/80: current_loss=0.03281 | best_loss=0.03281
      explained_var=-0.23926 | mse_loss=0.03202
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03738 | best_loss=0.03738
Epoch 1/80: current_loss=0.03733 | best_loss=0.03733
Epoch 2/80: current_loss=0.03726 | best_loss=0.03726
Epoch 3/80: current_loss=0.03719 | best_loss=0.03719
Epoch 4/80: current_loss=0.03711 | best_loss=0.03711
Epoch 5/80: current_loss=0.03705 | best_loss=0.03705
Epoch 6/80: current_loss=0.03701 | best_loss=0.03701
Epoch 7/80: current_loss=0.03697 | best_loss=0.03697
Epoch 8/80: current_loss=0.03692 | best_loss=0.03692
Epoch 9/80: current_loss=0.03688 | best_loss=0.03688
Epoch 10/80: current_loss=0.03681 | best_loss=0.03681
Epoch 11/80: current_loss=0.03676 | best_loss=0.03676
Epoch 12/80: current_loss=0.03673 | best_loss=0.03673
Epoch 13/80: current_loss=0.03665 | best_loss=0.03665
Epoch 14/80: current_loss=0.03659 | best_loss=0.03659
Epoch 15/80: current_loss=0.03654 | best_loss=0.03654
Epoch 16/80: current_loss=0.03648 | best_loss=0.03648
Epoch 17/80: current_loss=0.03645 | best_loss=0.03645
Epoch 18/80: current_loss=0.03643 | best_loss=0.03643
Epoch 19/80: current_loss=0.03639 | best_loss=0.03639
Epoch 20/80: current_loss=0.03636 | best_loss=0.03636
Epoch 21/80: current_loss=0.03634 | best_loss=0.03634
Epoch 22/80: current_loss=0.03631 | best_loss=0.03631
Epoch 23/80: current_loss=0.03623 | best_loss=0.03623
Epoch 24/80: current_loss=0.03618 | best_loss=0.03618
Epoch 25/80: current_loss=0.03614 | best_loss=0.03614
Epoch 26/80: current_loss=0.03612 | best_loss=0.03612
Epoch 27/80: current_loss=0.03608 | best_loss=0.03608
Epoch 28/80: current_loss=0.03603 | best_loss=0.03603
Epoch 29/80: current_loss=0.03598 | best_loss=0.03598
Epoch 30/80: current_loss=0.03593 | best_loss=0.03593
Epoch 31/80: current_loss=0.03591 | best_loss=0.03591
Epoch 32/80: current_loss=0.03587 | best_loss=0.03587
Epoch 33/80: current_loss=0.03581 | best_loss=0.03581
Epoch 34/80: current_loss=0.03578 | best_loss=0.03578
Epoch 35/80: current_loss=0.03574 | best_loss=0.03574
Epoch 36/80: current_loss=0.03570 | best_loss=0.03570
Epoch 37/80: current_loss=0.03563 | best_loss=0.03563
Epoch 38/80: current_loss=0.03561 | best_loss=0.03561
Epoch 39/80: current_loss=0.03558 | best_loss=0.03558
Epoch 40/80: current_loss=0.03554 | best_loss=0.03554
Epoch 41/80: current_loss=0.03551 | best_loss=0.03551
Epoch 42/80: current_loss=0.03547 | best_loss=0.03547
Epoch 43/80: current_loss=0.03546 | best_loss=0.03546
Epoch 44/80: current_loss=0.03544 | best_loss=0.03544
Epoch 45/80: current_loss=0.03542 | best_loss=0.03542
Epoch 46/80: current_loss=0.03537 | best_loss=0.03537
Epoch 47/80: current_loss=0.03534 | best_loss=0.03534
Epoch 48/80: current_loss=0.03527 | best_loss=0.03527
Epoch 49/80: current_loss=0.03525 | best_loss=0.03525
Epoch 50/80: current_loss=0.03520 | best_loss=0.03520
Epoch 51/80: current_loss=0.03516 | best_loss=0.03516
Epoch 52/80: current_loss=0.03512 | best_loss=0.03512
Epoch 53/80: current_loss=0.03508 | best_loss=0.03508
Epoch 54/80: current_loss=0.03505 | best_loss=0.03505
Epoch 55/80: current_loss=0.03503 | best_loss=0.03503
Epoch 56/80: current_loss=0.03499 | best_loss=0.03499
Epoch 57/80: current_loss=0.03493 | best_loss=0.03493
Epoch 58/80: current_loss=0.03490 | best_loss=0.03490
Epoch 59/80: current_loss=0.03486 | best_loss=0.03486
Epoch 60/80: current_loss=0.03485 | best_loss=0.03485
Epoch 61/80: current_loss=0.03484 | best_loss=0.03484
Epoch 62/80: current_loss=0.03482 | best_loss=0.03482
Epoch 63/80: current_loss=0.03479 | best_loss=0.03479
Epoch 64/80: current_loss=0.03476 | best_loss=0.03476
Epoch 65/80: current_loss=0.03473 | best_loss=0.03473
Epoch 66/80: current_loss=0.03472 | best_loss=0.03472
Epoch 67/80: current_loss=0.03471 | best_loss=0.03471
Epoch 68/80: current_loss=0.03467 | best_loss=0.03467
Epoch 69/80: current_loss=0.03464 | best_loss=0.03464
Epoch 70/80: current_loss=0.03457 | best_loss=0.03457
Epoch 71/80: current_loss=0.03452 | best_loss=0.03452
Epoch 72/80: current_loss=0.03447 | best_loss=0.03447
Epoch 73/80: current_loss=0.03445 | best_loss=0.03445
Epoch 74/80: current_loss=0.03443 | best_loss=0.03443
Epoch 75/80: current_loss=0.03441 | best_loss=0.03441
Epoch 76/80: current_loss=0.03437 | best_loss=0.03437
Epoch 77/80: current_loss=0.03435 | best_loss=0.03435
Epoch 78/80: current_loss=0.03432 | best_loss=0.03432
Epoch 79/80: current_loss=0.03430 | best_loss=0.03430
      explained_var=-0.20086 | mse_loss=0.03387
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03603 | best_loss=0.03603
Epoch 1/80: current_loss=0.03605 | best_loss=0.03603
Epoch 2/80: current_loss=0.03596 | best_loss=0.03596
Epoch 3/80: current_loss=0.03585 | best_loss=0.03585
Epoch 4/80: current_loss=0.03590 | best_loss=0.03585
Epoch 5/80: current_loss=0.03587 | best_loss=0.03585
Epoch 6/80: current_loss=0.03582 | best_loss=0.03582
Epoch 7/80: current_loss=0.03574 | best_loss=0.03574
Epoch 8/80: current_loss=0.03573 | best_loss=0.03573
Epoch 9/80: current_loss=0.03569 | best_loss=0.03569
Epoch 10/80: current_loss=0.03562 | best_loss=0.03562
Epoch 11/80: current_loss=0.03560 | best_loss=0.03560
Epoch 12/80: current_loss=0.03552 | best_loss=0.03552
Epoch 13/80: current_loss=0.03544 | best_loss=0.03544
Epoch 14/80: current_loss=0.03542 | best_loss=0.03542
Epoch 15/80: current_loss=0.03548 | best_loss=0.03542
Epoch 16/80: current_loss=0.03546 | best_loss=0.03542
Epoch 17/80: current_loss=0.03538 | best_loss=0.03538
Epoch 18/80: current_loss=0.03534 | best_loss=0.03534
Epoch 19/80: current_loss=0.03534 | best_loss=0.03534
Epoch 20/80: current_loss=0.03535 | best_loss=0.03534
Epoch 21/80: current_loss=0.03534 | best_loss=0.03534
Epoch 22/80: current_loss=0.03531 | best_loss=0.03531
Epoch 23/80: current_loss=0.03533 | best_loss=0.03531
Epoch 24/80: current_loss=0.03531 | best_loss=0.03531
Epoch 25/80: current_loss=0.03533 | best_loss=0.03531
Epoch 26/80: current_loss=0.03528 | best_loss=0.03528
Epoch 27/80: current_loss=0.03527 | best_loss=0.03527
Epoch 28/80: current_loss=0.03526 | best_loss=0.03526
Epoch 29/80: current_loss=0.03519 | best_loss=0.03519
Epoch 30/80: current_loss=0.03517 | best_loss=0.03517
Epoch 31/80: current_loss=0.03523 | best_loss=0.03517
Epoch 32/80: current_loss=0.03523 | best_loss=0.03517
Epoch 33/80: current_loss=0.03522 | best_loss=0.03517
Epoch 34/80: current_loss=0.03522 | best_loss=0.03517
Epoch 35/80: current_loss=0.03516 | best_loss=0.03516
Epoch 36/80: current_loss=0.03513 | best_loss=0.03513
Epoch 37/80: current_loss=0.03512 | best_loss=0.03512
Epoch 38/80: current_loss=0.03508 | best_loss=0.03508
Epoch 39/80: current_loss=0.03513 | best_loss=0.03508
Epoch 40/80: current_loss=0.03515 | best_loss=0.03508
Epoch 41/80: current_loss=0.03508 | best_loss=0.03508
Epoch 42/80: current_loss=0.03507 | best_loss=0.03507
Epoch 43/80: current_loss=0.03505 | best_loss=0.03505
Epoch 44/80: current_loss=0.03498 | best_loss=0.03498
Epoch 45/80: current_loss=0.03495 | best_loss=0.03495
Epoch 46/80: current_loss=0.03492 | best_loss=0.03492
Epoch 47/80: current_loss=0.03491 | best_loss=0.03491
Epoch 48/80: current_loss=0.03492 | best_loss=0.03491
Epoch 49/80: current_loss=0.03491 | best_loss=0.03491
Epoch 50/80: current_loss=0.03493 | best_loss=0.03491
Epoch 51/80: current_loss=0.03491 | best_loss=0.03491
Epoch 52/80: current_loss=0.03491 | best_loss=0.03491
Epoch 53/80: current_loss=0.03496 | best_loss=0.03491
Epoch 54/80: current_loss=0.03493 | best_loss=0.03491
Epoch 55/80: current_loss=0.03488 | best_loss=0.03488
Epoch 56/80: current_loss=0.03484 | best_loss=0.03484
Epoch 57/80: current_loss=0.03480 | best_loss=0.03480
Epoch 58/80: current_loss=0.03482 | best_loss=0.03480
Epoch 59/80: current_loss=0.03479 | best_loss=0.03479
Epoch 60/80: current_loss=0.03470 | best_loss=0.03470
Epoch 61/80: current_loss=0.03464 | best_loss=0.03464
Epoch 62/80: current_loss=0.03462 | best_loss=0.03462
Epoch 63/80: current_loss=0.03463 | best_loss=0.03462
Epoch 64/80: current_loss=0.03463 | best_loss=0.03462
Epoch 65/80: current_loss=0.03469 | best_loss=0.03462
Epoch 66/80: current_loss=0.03476 | best_loss=0.03462
Epoch 67/80: current_loss=0.03477 | best_loss=0.03462
Epoch 68/80: current_loss=0.03471 | best_loss=0.03462
Epoch 69/80: current_loss=0.03468 | best_loss=0.03462
Epoch 70/80: current_loss=0.03463 | best_loss=0.03462
Epoch 71/80: current_loss=0.03463 | best_loss=0.03462
Epoch 72/80: current_loss=0.03463 | best_loss=0.03462
Epoch 73/80: current_loss=0.03459 | best_loss=0.03459
Epoch 74/80: current_loss=0.03452 | best_loss=0.03452
Epoch 75/80: current_loss=0.03456 | best_loss=0.03452
Epoch 76/80: current_loss=0.03456 | best_loss=0.03452
Epoch 77/80: current_loss=0.03455 | best_loss=0.03452
Epoch 78/80: current_loss=0.03451 | best_loss=0.03451
Epoch 79/80: current_loss=0.03453 | best_loss=0.03451
      explained_var=-0.09197 | mse_loss=0.03400
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02971 | best_loss=0.02971
Epoch 1/80: current_loss=0.02970 | best_loss=0.02970
Epoch 2/80: current_loss=0.02972 | best_loss=0.02970
Epoch 3/80: current_loss=0.02973 | best_loss=0.02970
Epoch 4/80: current_loss=0.02969 | best_loss=0.02969
Epoch 5/80: current_loss=0.02967 | best_loss=0.02967
Epoch 6/80: current_loss=0.02967 | best_loss=0.02967
Epoch 7/80: current_loss=0.02965 | best_loss=0.02965
Epoch 8/80: current_loss=0.02963 | best_loss=0.02963
Epoch 9/80: current_loss=0.02964 | best_loss=0.02963
Epoch 10/80: current_loss=0.02962 | best_loss=0.02962
Epoch 11/80: current_loss=0.02962 | best_loss=0.02962
Epoch 12/80: current_loss=0.02964 | best_loss=0.02962
Epoch 13/80: current_loss=0.02961 | best_loss=0.02961
Epoch 14/80: current_loss=0.02960 | best_loss=0.02960
Epoch 15/80: current_loss=0.02958 | best_loss=0.02958
Epoch 16/80: current_loss=0.02958 | best_loss=0.02958
Epoch 17/80: current_loss=0.02956 | best_loss=0.02956
Epoch 18/80: current_loss=0.02952 | best_loss=0.02952
Epoch 19/80: current_loss=0.02953 | best_loss=0.02952
Epoch 20/80: current_loss=0.02952 | best_loss=0.02952
Epoch 21/80: current_loss=0.02951 | best_loss=0.02951
Epoch 22/80: current_loss=0.02948 | best_loss=0.02948
Epoch 23/80: current_loss=0.02947 | best_loss=0.02947
Epoch 24/80: current_loss=0.02945 | best_loss=0.02945
Epoch 25/80: current_loss=0.02944 | best_loss=0.02944
Epoch 26/80: current_loss=0.02943 | best_loss=0.02943
Epoch 27/80: current_loss=0.02943 | best_loss=0.02943
Epoch 28/80: current_loss=0.02942 | best_loss=0.02942
Epoch 29/80: current_loss=0.02943 | best_loss=0.02942
Epoch 30/80: current_loss=0.02942 | best_loss=0.02942
Epoch 31/80: current_loss=0.02941 | best_loss=0.02941
Epoch 32/80: current_loss=0.02942 | best_loss=0.02941
Epoch 33/80: current_loss=0.02944 | best_loss=0.02941
Epoch 34/80: current_loss=0.02944 | best_loss=0.02941
Epoch 35/80: current_loss=0.02942 | best_loss=0.02941
Epoch 36/80: current_loss=0.02942 | best_loss=0.02941
Epoch 37/80: current_loss=0.02941 | best_loss=0.02941
Epoch 38/80: current_loss=0.02940 | best_loss=0.02940
Epoch 39/80: current_loss=0.02940 | best_loss=0.02940
Epoch 40/80: current_loss=0.02939 | best_loss=0.02939
Epoch 41/80: current_loss=0.02940 | best_loss=0.02939
Epoch 42/80: current_loss=0.02943 | best_loss=0.02939
Epoch 43/80: current_loss=0.02942 | best_loss=0.02939
Epoch 44/80: current_loss=0.02941 | best_loss=0.02939
Epoch 45/80: current_loss=0.02938 | best_loss=0.02938
Epoch 46/80: current_loss=0.02936 | best_loss=0.02936
Epoch 47/80: current_loss=0.02935 | best_loss=0.02935
Epoch 48/80: current_loss=0.02933 | best_loss=0.02933
Epoch 49/80: current_loss=0.02933 | best_loss=0.02933
Epoch 50/80: current_loss=0.02932 | best_loss=0.02932
Epoch 51/80: current_loss=0.02931 | best_loss=0.02931
Epoch 52/80: current_loss=0.02931 | best_loss=0.02931
Epoch 53/80: current_loss=0.02930 | best_loss=0.02930
Epoch 54/80: current_loss=0.02929 | best_loss=0.02929
Epoch 55/80: current_loss=0.02927 | best_loss=0.02927
Epoch 56/80: current_loss=0.02927 | best_loss=0.02927
Epoch 57/80: current_loss=0.02929 | best_loss=0.02927
Epoch 58/80: current_loss=0.02929 | best_loss=0.02927
Epoch 59/80: current_loss=0.02927 | best_loss=0.02927
Epoch 60/80: current_loss=0.02927 | best_loss=0.02927
Epoch 61/80: current_loss=0.02926 | best_loss=0.02926
Epoch 62/80: current_loss=0.02926 | best_loss=0.02926
Epoch 63/80: current_loss=0.02924 | best_loss=0.02924
Epoch 64/80: current_loss=0.02922 | best_loss=0.02922
Epoch 65/80: current_loss=0.02923 | best_loss=0.02922
Epoch 66/80: current_loss=0.02922 | best_loss=0.02922
Epoch 67/80: current_loss=0.02922 | best_loss=0.02922
Epoch 68/80: current_loss=0.02921 | best_loss=0.02921
Epoch 69/80: current_loss=0.02920 | best_loss=0.02920
Epoch 70/80: current_loss=0.02921 | best_loss=0.02920
Epoch 71/80: current_loss=0.02921 | best_loss=0.02920
Epoch 72/80: current_loss=0.02921 | best_loss=0.02920
Epoch 73/80: current_loss=0.02920 | best_loss=0.02920
Epoch 74/80: current_loss=0.02919 | best_loss=0.02919
Epoch 75/80: current_loss=0.02918 | best_loss=0.02918
Epoch 76/80: current_loss=0.02918 | best_loss=0.02918
Epoch 77/80: current_loss=0.02917 | best_loss=0.02917
Epoch 78/80: current_loss=0.02916 | best_loss=0.02916
Epoch 79/80: current_loss=0.02916 | best_loss=0.02916
      explained_var=-0.02675 | mse_loss=0.02932
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03488 | best_loss=0.03488
Epoch 1/80: current_loss=0.03452 | best_loss=0.03452
Epoch 2/80: current_loss=0.03422 | best_loss=0.03422
Epoch 3/80: current_loss=0.03401 | best_loss=0.03401
Epoch 4/80: current_loss=0.03384 | best_loss=0.03384
Epoch 5/80: current_loss=0.03374 | best_loss=0.03374
Epoch 6/80: current_loss=0.03366 | best_loss=0.03366
Epoch 7/80: current_loss=0.03359 | best_loss=0.03359
Epoch 8/80: current_loss=0.03354 | best_loss=0.03354
Epoch 9/80: current_loss=0.03349 | best_loss=0.03349
Epoch 10/80: current_loss=0.03344 | best_loss=0.03344
Epoch 11/80: current_loss=0.03339 | best_loss=0.03339
Epoch 12/80: current_loss=0.03338 | best_loss=0.03338
Epoch 13/80: current_loss=0.03336 | best_loss=0.03336
Epoch 14/80: current_loss=0.03334 | best_loss=0.03334
Epoch 15/80: current_loss=0.03333 | best_loss=0.03333
Epoch 16/80: current_loss=0.03333 | best_loss=0.03333
Epoch 17/80: current_loss=0.03333 | best_loss=0.03333
Epoch 18/80: current_loss=0.03333 | best_loss=0.03333
Epoch 19/80: current_loss=0.03333 | best_loss=0.03333
Epoch 20/80: current_loss=0.03334 | best_loss=0.03333
Epoch 21/80: current_loss=0.03335 | best_loss=0.03333
Epoch 22/80: current_loss=0.03335 | best_loss=0.03333
Epoch 23/80: current_loss=0.03336 | best_loss=0.03333
Epoch 24/80: current_loss=0.03337 | best_loss=0.03333
Epoch 25/80: current_loss=0.03337 | best_loss=0.03333
Epoch 26/80: current_loss=0.03337 | best_loss=0.03333
Epoch 27/80: current_loss=0.03337 | best_loss=0.03333
Epoch 28/80: current_loss=0.03337 | best_loss=0.03333
Epoch 29/80: current_loss=0.03338 | best_loss=0.03333
Epoch 30/80: current_loss=0.03338 | best_loss=0.03333
Epoch 31/80: current_loss=0.03339 | best_loss=0.03333
Epoch 32/80: current_loss=0.03339 | best_loss=0.03333
Epoch 33/80: current_loss=0.03340 | best_loss=0.03333
Epoch 34/80: current_loss=0.03340 | best_loss=0.03333
Epoch 35/80: current_loss=0.03339 | best_loss=0.03333
Epoch 36/80: current_loss=0.03338 | best_loss=0.03333
Epoch 37/80: current_loss=0.03338 | best_loss=0.03333
Early Stopping at epoch 37
      explained_var=-0.04569 | mse_loss=0.03401
----------------------------------------------
Average early_stopping_point: 67| avg_exp_var=-0.12091| avg_loss=0.03264
----------------------------------------------


----------------------------------------------
Params for Trial 9
{'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=58.28735 | best_loss=58.28735
Epoch 1/80: current_loss=6.81428 | best_loss=6.81428
Epoch 2/80: current_loss=2.70449 | best_loss=2.70449
Epoch 3/80: current_loss=1.45346 | best_loss=1.45346
Epoch 4/80: current_loss=0.37534 | best_loss=0.37534
Epoch 5/80: current_loss=0.99036 | best_loss=0.37534
Epoch 6/80: current_loss=2.82987 | best_loss=0.37534
Epoch 7/80: current_loss=0.26478 | best_loss=0.26478
Epoch 8/80: current_loss=2.17991 | best_loss=0.26478
Epoch 9/80: current_loss=0.15912 | best_loss=0.15912
Epoch 10/80: current_loss=0.41903 | best_loss=0.15912
Epoch 11/80: current_loss=0.21170 | best_loss=0.15912
Epoch 12/80: current_loss=0.39122 | best_loss=0.15912
Epoch 13/80: current_loss=0.59837 | best_loss=0.15912
Epoch 14/80: current_loss=0.15517 | best_loss=0.15517
Epoch 15/80: current_loss=0.08037 | best_loss=0.08037
Epoch 16/80: current_loss=0.28735 | best_loss=0.08037
Epoch 17/80: current_loss=0.67091 | best_loss=0.08037
Epoch 18/80: current_loss=0.06358 | best_loss=0.06358
Epoch 19/80: current_loss=0.04656 | best_loss=0.04656
Epoch 20/80: current_loss=0.14988 | best_loss=0.04656
Epoch 21/80: current_loss=0.06007 | best_loss=0.04656
Epoch 22/80: current_loss=0.06249 | best_loss=0.04656
Epoch 23/80: current_loss=0.08196 | best_loss=0.04656
Epoch 24/80: current_loss=0.11640 | best_loss=0.04656
Epoch 25/80: current_loss=0.03347 | best_loss=0.03347
Epoch 26/80: current_loss=0.07981 | best_loss=0.03347
Epoch 27/80: current_loss=0.04343 | best_loss=0.03347
Epoch 28/80: current_loss=0.03645 | best_loss=0.03347
Epoch 29/80: current_loss=0.10198 | best_loss=0.03347
Epoch 30/80: current_loss=0.05602 | best_loss=0.03347
Epoch 31/80: current_loss=0.03109 | best_loss=0.03109
Epoch 32/80: current_loss=0.05394 | best_loss=0.03109
Epoch 33/80: current_loss=0.04642 | best_loss=0.03109
Epoch 34/80: current_loss=0.03368 | best_loss=0.03109
Epoch 35/80: current_loss=0.02876 | best_loss=0.02876
Epoch 36/80: current_loss=0.03343 | best_loss=0.02876
Epoch 37/80: current_loss=0.02798 | best_loss=0.02798
Epoch 38/80: current_loss=0.03776 | best_loss=0.02798
Epoch 39/80: current_loss=0.02854 | best_loss=0.02798
Epoch 40/80: current_loss=0.03406 | best_loss=0.02798
Epoch 41/80: current_loss=0.02781 | best_loss=0.02781
Epoch 42/80: current_loss=0.03141 | best_loss=0.02781
Epoch 43/80: current_loss=0.02741 | best_loss=0.02741
Epoch 44/80: current_loss=0.02910 | best_loss=0.02741
Epoch 45/80: current_loss=0.02895 | best_loss=0.02741
Epoch 46/80: current_loss=0.02918 | best_loss=0.02741
Epoch 47/80: current_loss=0.03172 | best_loss=0.02741
Epoch 48/80: current_loss=0.03638 | best_loss=0.02741
Epoch 49/80: current_loss=0.03597 | best_loss=0.02741
Epoch 50/80: current_loss=0.02864 | best_loss=0.02741
Epoch 51/80: current_loss=0.03589 | best_loss=0.02741
Epoch 52/80: current_loss=0.03217 | best_loss=0.02741
Epoch 53/80: current_loss=0.02707 | best_loss=0.02707
Epoch 54/80: current_loss=0.03006 | best_loss=0.02707
Epoch 55/80: current_loss=0.02736 | best_loss=0.02707
Epoch 56/80: current_loss=0.03132 | best_loss=0.02707
Epoch 57/80: current_loss=0.03632 | best_loss=0.02707
Epoch 58/80: current_loss=0.02982 | best_loss=0.02707
Epoch 59/80: current_loss=0.03478 | best_loss=0.02707
Epoch 60/80: current_loss=0.02640 | best_loss=0.02640
Epoch 61/80: current_loss=0.03779 | best_loss=0.02640
Epoch 62/80: current_loss=0.03675 | best_loss=0.02640
Epoch 63/80: current_loss=0.03693 | best_loss=0.02640
Epoch 64/80: current_loss=0.02948 | best_loss=0.02640
Epoch 65/80: current_loss=0.03007 | best_loss=0.02640
Epoch 66/80: current_loss=0.02955 | best_loss=0.02640
Epoch 67/80: current_loss=0.03343 | best_loss=0.02640
Epoch 68/80: current_loss=0.03676 | best_loss=0.02640
Epoch 69/80: current_loss=0.03929 | best_loss=0.02640
Epoch 70/80: current_loss=0.03950 | best_loss=0.02640
Epoch 71/80: current_loss=0.03349 | best_loss=0.02640
Epoch 72/80: current_loss=0.03606 | best_loss=0.02640
Epoch 73/80: current_loss=0.03267 | best_loss=0.02640
Epoch 74/80: current_loss=0.03476 | best_loss=0.02640
Epoch 75/80: current_loss=0.02995 | best_loss=0.02640
Epoch 76/80: current_loss=0.02656 | best_loss=0.02640
Epoch 77/80: current_loss=0.02932 | best_loss=0.02640
Epoch 78/80: current_loss=0.03726 | best_loss=0.02640
Epoch 79/80: current_loss=0.04265 | best_loss=0.02640
      explained_var=0.00152 | mse_loss=0.02594
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=18.45583 | best_loss=18.45583
Epoch 1/80: current_loss=0.39832 | best_loss=0.39832
Epoch 2/80: current_loss=16.04409 | best_loss=0.39832
Epoch 3/80: current_loss=7.68297 | best_loss=0.39832
Epoch 4/80: current_loss=0.31616 | best_loss=0.31616
Epoch 5/80: current_loss=1.49433 | best_loss=0.31616
Epoch 6/80: current_loss=2.48057 | best_loss=0.31616
Epoch 7/80: current_loss=1.80248 | best_loss=0.31616
Epoch 8/80: current_loss=0.18988 | best_loss=0.18988
Epoch 9/80: current_loss=0.27429 | best_loss=0.18988
Epoch 10/80: current_loss=0.60589 | best_loss=0.18988
Epoch 11/80: current_loss=0.13461 | best_loss=0.13461
Epoch 12/80: current_loss=0.31525 | best_loss=0.13461
Epoch 13/80: current_loss=0.15439 | best_loss=0.13461
Epoch 14/80: current_loss=0.14312 | best_loss=0.13461
Epoch 15/80: current_loss=0.12949 | best_loss=0.12949
Epoch 16/80: current_loss=0.09049 | best_loss=0.09049
Epoch 17/80: current_loss=0.07288 | best_loss=0.07288
Epoch 18/80: current_loss=0.08958 | best_loss=0.07288
Epoch 19/80: current_loss=0.96257 | best_loss=0.07288
Epoch 20/80: current_loss=0.51885 | best_loss=0.07288
Epoch 21/80: current_loss=2.27058 | best_loss=0.07288
Epoch 22/80: current_loss=0.19864 | best_loss=0.07288
Epoch 23/80: current_loss=0.39063 | best_loss=0.07288
Epoch 24/80: current_loss=0.14697 | best_loss=0.07288
Epoch 25/80: current_loss=2.80181 | best_loss=0.07288
Epoch 26/80: current_loss=1.76912 | best_loss=0.07288
Epoch 27/80: current_loss=0.14368 | best_loss=0.07288
Epoch 28/80: current_loss=1.67827 | best_loss=0.07288
Epoch 29/80: current_loss=0.08312 | best_loss=0.07288
Epoch 30/80: current_loss=1.00059 | best_loss=0.07288
Epoch 31/80: current_loss=2.87300 | best_loss=0.07288
Epoch 32/80: current_loss=2.72287 | best_loss=0.07288
Epoch 33/80: current_loss=9.74444 | best_loss=0.07288
Epoch 34/80: current_loss=2.13116 | best_loss=0.07288
Epoch 35/80: current_loss=4.60646 | best_loss=0.07288
Epoch 36/80: current_loss=25.42624 | best_loss=0.07288
Epoch 37/80: current_loss=0.63389 | best_loss=0.07288
Early Stopping at epoch 37
      explained_var=-1.64546 | mse_loss=0.07367
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=27.96260 | best_loss=27.96260
Epoch 1/80: current_loss=14.62166 | best_loss=14.62166
Epoch 2/80: current_loss=1.85713 | best_loss=1.85713
Epoch 3/80: current_loss=2.22002 | best_loss=1.85713
Epoch 4/80: current_loss=0.70477 | best_loss=0.70477
Epoch 5/80: current_loss=3.22520 | best_loss=0.70477
Epoch 6/80: current_loss=2.55544 | best_loss=0.70477
Epoch 7/80: current_loss=2.30644 | best_loss=0.70477
Epoch 8/80: current_loss=1.06840 | best_loss=0.70477
Epoch 9/80: current_loss=0.62160 | best_loss=0.62160
Epoch 10/80: current_loss=0.98992 | best_loss=0.62160
Epoch 11/80: current_loss=0.21660 | best_loss=0.21660
Epoch 12/80: current_loss=0.90017 | best_loss=0.21660
Epoch 13/80: current_loss=0.55193 | best_loss=0.21660
Epoch 14/80: current_loss=0.81487 | best_loss=0.21660
Epoch 15/80: current_loss=0.28711 | best_loss=0.21660
Epoch 16/80: current_loss=0.07026 | best_loss=0.07026
Epoch 17/80: current_loss=0.11240 | best_loss=0.07026
Epoch 18/80: current_loss=0.07704 | best_loss=0.07026
Epoch 19/80: current_loss=0.22233 | best_loss=0.07026
Epoch 20/80: current_loss=0.13641 | best_loss=0.07026
Epoch 21/80: current_loss=0.15666 | best_loss=0.07026
Epoch 22/80: current_loss=0.03666 | best_loss=0.03666
Epoch 23/80: current_loss=0.16795 | best_loss=0.03666
Epoch 24/80: current_loss=0.16895 | best_loss=0.03666
Epoch 25/80: current_loss=0.11442 | best_loss=0.03666
Epoch 26/80: current_loss=0.16218 | best_loss=0.03666
Epoch 27/80: current_loss=0.08863 | best_loss=0.03666
Epoch 28/80: current_loss=0.16491 | best_loss=0.03666
Epoch 29/80: current_loss=0.04453 | best_loss=0.03666
Epoch 30/80: current_loss=0.11764 | best_loss=0.03666
Epoch 31/80: current_loss=0.36692 | best_loss=0.03666
Epoch 32/80: current_loss=0.13361 | best_loss=0.03666
Epoch 33/80: current_loss=0.21880 | best_loss=0.03666
Epoch 34/80: current_loss=0.39936 | best_loss=0.03666
Epoch 35/80: current_loss=0.35485 | best_loss=0.03666
Epoch 36/80: current_loss=0.10427 | best_loss=0.03666
Epoch 37/80: current_loss=0.05049 | best_loss=0.03666
Epoch 38/80: current_loss=0.23300 | best_loss=0.03666
Epoch 39/80: current_loss=0.24319 | best_loss=0.03666
Epoch 40/80: current_loss=0.09928 | best_loss=0.03666
Epoch 41/80: current_loss=0.58887 | best_loss=0.03666
Epoch 42/80: current_loss=0.54047 | best_loss=0.03666
Early Stopping at epoch 42
      explained_var=-0.19939 | mse_loss=0.03596
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=21.30894 | best_loss=21.30894
Epoch 1/80: current_loss=0.66270 | best_loss=0.66270
Epoch 2/80: current_loss=4.79911 | best_loss=0.66270
Epoch 3/80: current_loss=1.19968 | best_loss=0.66270
Epoch 4/80: current_loss=7.36538 | best_loss=0.66270
Epoch 5/80: current_loss=1.12361 | best_loss=0.66270
Epoch 6/80: current_loss=2.40628 | best_loss=0.66270
Epoch 7/80: current_loss=1.04989 | best_loss=0.66270
Epoch 8/80: current_loss=0.53622 | best_loss=0.53622
Epoch 9/80: current_loss=3.32706 | best_loss=0.53622
Epoch 10/80: current_loss=0.73300 | best_loss=0.53622
Epoch 11/80: current_loss=0.85718 | best_loss=0.53622
Epoch 12/80: current_loss=0.26887 | best_loss=0.26887
Epoch 13/80: current_loss=0.45808 | best_loss=0.26887
Epoch 14/80: current_loss=0.64075 | best_loss=0.26887
Epoch 15/80: current_loss=0.10659 | best_loss=0.10659
Epoch 16/80: current_loss=0.20857 | best_loss=0.10659
Epoch 17/80: current_loss=12.54655 | best_loss=0.10659
Epoch 18/80: current_loss=2.40927 | best_loss=0.10659
Epoch 19/80: current_loss=0.80276 | best_loss=0.10659
Epoch 20/80: current_loss=0.88180 | best_loss=0.10659
Epoch 21/80: current_loss=1.37646 | best_loss=0.10659
Epoch 22/80: current_loss=7.66774 | best_loss=0.10659
Epoch 23/80: current_loss=0.98861 | best_loss=0.10659
Epoch 24/80: current_loss=6.87553 | best_loss=0.10659
Epoch 25/80: current_loss=2.01557 | best_loss=0.10659
Epoch 26/80: current_loss=2.27289 | best_loss=0.10659
Epoch 27/80: current_loss=0.24274 | best_loss=0.10659
Epoch 28/80: current_loss=0.68730 | best_loss=0.10659
Epoch 29/80: current_loss=43.40798 | best_loss=0.10659
Epoch 30/80: current_loss=8.67035 | best_loss=0.10659
Epoch 31/80: current_loss=23.04042 | best_loss=0.10659
Epoch 32/80: current_loss=1.41758 | best_loss=0.10659
Epoch 33/80: current_loss=14.40871 | best_loss=0.10659
Epoch 34/80: current_loss=2.58051 | best_loss=0.10659
Epoch 35/80: current_loss=2.07364 | best_loss=0.10659
Early Stopping at epoch 35
      explained_var=-2.76258 | mse_loss=0.10673
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=9.08480 | best_loss=9.08480
Epoch 1/80: current_loss=4.28148 | best_loss=4.28148
Epoch 2/80: current_loss=4.51927 | best_loss=4.28148
Epoch 3/80: current_loss=1.41968 | best_loss=1.41968
Epoch 4/80: current_loss=0.80855 | best_loss=0.80855
Epoch 5/80: current_loss=1.53725 | best_loss=0.80855
Epoch 6/80: current_loss=6.67589 | best_loss=0.80855
Epoch 7/80: current_loss=0.68392 | best_loss=0.68392
Epoch 8/80: current_loss=1.96556 | best_loss=0.68392
Epoch 9/80: current_loss=0.24553 | best_loss=0.24553
Epoch 10/80: current_loss=0.16457 | best_loss=0.16457
Epoch 11/80: current_loss=0.26323 | best_loss=0.16457
Epoch 12/80: current_loss=0.44946 | best_loss=0.16457
Epoch 13/80: current_loss=0.22654 | best_loss=0.16457
Epoch 14/80: current_loss=0.15338 | best_loss=0.15338
Epoch 15/80: current_loss=0.17995 | best_loss=0.15338
Epoch 16/80: current_loss=0.10301 | best_loss=0.10301
Epoch 17/80: current_loss=0.18232 | best_loss=0.10301
Epoch 18/80: current_loss=0.12834 | best_loss=0.10301
Epoch 19/80: current_loss=0.14626 | best_loss=0.10301
Epoch 20/80: current_loss=0.05451 | best_loss=0.05451
Epoch 21/80: current_loss=0.06325 | best_loss=0.05451
Epoch 22/80: current_loss=0.24839 | best_loss=0.05451
Epoch 23/80: current_loss=0.08414 | best_loss=0.05451
Epoch 24/80: current_loss=0.10294 | best_loss=0.05451
Epoch 25/80: current_loss=0.15021 | best_loss=0.05451
Epoch 26/80: current_loss=0.06410 | best_loss=0.05451
Epoch 27/80: current_loss=0.03523 | best_loss=0.03523
Epoch 28/80: current_loss=0.04181 | best_loss=0.03523
Epoch 29/80: current_loss=0.04813 | best_loss=0.03523
Epoch 30/80: current_loss=0.05103 | best_loss=0.03523
Epoch 31/80: current_loss=0.08691 | best_loss=0.03523
Epoch 32/80: current_loss=0.08268 | best_loss=0.03523
Epoch 33/80: current_loss=0.03575 | best_loss=0.03523
Epoch 34/80: current_loss=0.05020 | best_loss=0.03523
Epoch 35/80: current_loss=0.07402 | best_loss=0.03523
Epoch 36/80: current_loss=0.11862 | best_loss=0.03523
Epoch 37/80: current_loss=0.04216 | best_loss=0.03523
Epoch 38/80: current_loss=0.07115 | best_loss=0.03523
Epoch 39/80: current_loss=0.07082 | best_loss=0.03523
Epoch 40/80: current_loss=0.05734 | best_loss=0.03523
Epoch 41/80: current_loss=0.05363 | best_loss=0.03523
Epoch 42/80: current_loss=0.04543 | best_loss=0.03523
Epoch 43/80: current_loss=0.05669 | best_loss=0.03523
Epoch 44/80: current_loss=0.04197 | best_loss=0.03523
Epoch 45/80: current_loss=0.04872 | best_loss=0.03523
Epoch 46/80: current_loss=0.07729 | best_loss=0.03523
Epoch 47/80: current_loss=0.06853 | best_loss=0.03523
Early Stopping at epoch 47
      explained_var=-0.10162 | mse_loss=0.03586
----------------------------------------------
Average early_stopping_point: 32| avg_exp_var=-0.94150| avg_loss=0.05563
----------------------------------------------


----------------------------------------------
Params for Trial 10
{'learning_rate': 0.01, 'weight_decay': 0.0030741327184301173, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=1.09405 | best_loss=1.09405
Epoch 1/80: current_loss=0.63230 | best_loss=0.63230
Epoch 2/80: current_loss=0.28480 | best_loss=0.28480
Epoch 3/80: current_loss=0.22987 | best_loss=0.22987
Epoch 4/80: current_loss=0.20173 | best_loss=0.20173
Epoch 5/80: current_loss=0.11157 | best_loss=0.11157
Epoch 6/80: current_loss=0.04702 | best_loss=0.04702
Epoch 7/80: current_loss=0.04380 | best_loss=0.04380
Epoch 8/80: current_loss=0.03805 | best_loss=0.03805
Epoch 9/80: current_loss=0.03841 | best_loss=0.03805
Epoch 10/80: current_loss=0.04273 | best_loss=0.03805
Epoch 11/80: current_loss=0.05389 | best_loss=0.03805
Epoch 12/80: current_loss=0.03560 | best_loss=0.03560
Epoch 13/80: current_loss=0.05063 | best_loss=0.03560
Epoch 14/80: current_loss=0.02942 | best_loss=0.02942
Epoch 15/80: current_loss=0.05756 | best_loss=0.02942
Epoch 16/80: current_loss=0.02842 | best_loss=0.02842
Epoch 17/80: current_loss=0.03244 | best_loss=0.02842
Epoch 18/80: current_loss=0.06291 | best_loss=0.02842
Epoch 19/80: current_loss=0.03092 | best_loss=0.02842
Epoch 20/80: current_loss=0.04123 | best_loss=0.02842
Epoch 21/80: current_loss=0.04111 | best_loss=0.02842
Epoch 22/80: current_loss=0.03761 | best_loss=0.02842
Epoch 23/80: current_loss=0.02804 | best_loss=0.02804
Epoch 24/80: current_loss=0.06001 | best_loss=0.02804
Epoch 25/80: current_loss=0.03020 | best_loss=0.02804
Epoch 26/80: current_loss=0.06345 | best_loss=0.02804
Epoch 27/80: current_loss=0.03657 | best_loss=0.02804
Epoch 28/80: current_loss=0.09206 | best_loss=0.02804
Epoch 29/80: current_loss=0.05255 | best_loss=0.02804
Epoch 30/80: current_loss=0.04415 | best_loss=0.02804
Epoch 31/80: current_loss=0.06220 | best_loss=0.02804
Epoch 32/80: current_loss=0.04084 | best_loss=0.02804
Epoch 33/80: current_loss=0.03885 | best_loss=0.02804
Epoch 34/80: current_loss=0.03593 | best_loss=0.02804
Epoch 35/80: current_loss=0.07371 | best_loss=0.02804
Epoch 36/80: current_loss=0.03604 | best_loss=0.02804
Epoch 37/80: current_loss=0.04155 | best_loss=0.02804
Epoch 38/80: current_loss=0.05289 | best_loss=0.02804
Epoch 39/80: current_loss=0.03277 | best_loss=0.02804
Epoch 40/80: current_loss=0.02926 | best_loss=0.02804
Epoch 41/80: current_loss=0.06793 | best_loss=0.02804
Epoch 42/80: current_loss=0.03467 | best_loss=0.02804
Epoch 43/80: current_loss=0.03817 | best_loss=0.02804
Early Stopping at epoch 43
      explained_var=-0.02681 | mse_loss=0.02752
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.17813 | best_loss=0.17813
Epoch 1/80: current_loss=0.08178 | best_loss=0.08178
Epoch 2/80: current_loss=0.09244 | best_loss=0.08178
Epoch 3/80: current_loss=0.17216 | best_loss=0.08178
Epoch 4/80: current_loss=0.04696 | best_loss=0.04696
Epoch 5/80: current_loss=0.05442 | best_loss=0.04696
Epoch 6/80: current_loss=0.06742 | best_loss=0.04696
Epoch 7/80: current_loss=0.17286 | best_loss=0.04696
Epoch 8/80: current_loss=0.12457 | best_loss=0.04696
Epoch 9/80: current_loss=0.03403 | best_loss=0.03403
Epoch 10/80: current_loss=0.05219 | best_loss=0.03403
Epoch 11/80: current_loss=0.09287 | best_loss=0.03403
Epoch 12/80: current_loss=0.03281 | best_loss=0.03281
Epoch 13/80: current_loss=0.03066 | best_loss=0.03066
Epoch 14/80: current_loss=0.04085 | best_loss=0.03066
Epoch 15/80: current_loss=0.11474 | best_loss=0.03066
Epoch 16/80: current_loss=0.03839 | best_loss=0.03066
Epoch 17/80: current_loss=0.03554 | best_loss=0.03066
Epoch 18/80: current_loss=0.10198 | best_loss=0.03066
Epoch 19/80: current_loss=0.02934 | best_loss=0.02934
Epoch 20/80: current_loss=0.20989 | best_loss=0.02934
Epoch 21/80: current_loss=0.05480 | best_loss=0.02934
Epoch 22/80: current_loss=0.15808 | best_loss=0.02934
Epoch 23/80: current_loss=0.13601 | best_loss=0.02934
Epoch 24/80: current_loss=0.09345 | best_loss=0.02934
Epoch 25/80: current_loss=0.13886 | best_loss=0.02934
Epoch 26/80: current_loss=0.63629 | best_loss=0.02934
Epoch 27/80: current_loss=0.16705 | best_loss=0.02934
Epoch 28/80: current_loss=0.35417 | best_loss=0.02934
Epoch 29/80: current_loss=0.32774 | best_loss=0.02934
Epoch 30/80: current_loss=0.06106 | best_loss=0.02934
Epoch 31/80: current_loss=0.04847 | best_loss=0.02934
Epoch 32/80: current_loss=0.04292 | best_loss=0.02934
Epoch 33/80: current_loss=0.11600 | best_loss=0.02934
Epoch 34/80: current_loss=0.03283 | best_loss=0.02934
Epoch 35/80: current_loss=0.02866 | best_loss=0.02866
Epoch 36/80: current_loss=0.04772 | best_loss=0.02866
Epoch 37/80: current_loss=0.05784 | best_loss=0.02866
Epoch 38/80: current_loss=0.22069 | best_loss=0.02866
Epoch 39/80: current_loss=0.02827 | best_loss=0.02827
Epoch 40/80: current_loss=0.02882 | best_loss=0.02827
Epoch 41/80: current_loss=0.06524 | best_loss=0.02827
Epoch 42/80: current_loss=0.08755 | best_loss=0.02827
Epoch 43/80: current_loss=0.05000 | best_loss=0.02827
Epoch 44/80: current_loss=0.08009 | best_loss=0.02827
Epoch 45/80: current_loss=0.09011 | best_loss=0.02827
Epoch 46/80: current_loss=0.05628 | best_loss=0.02827
Epoch 47/80: current_loss=0.04159 | best_loss=0.02827
Epoch 48/80: current_loss=0.11527 | best_loss=0.02827
Epoch 49/80: current_loss=0.05622 | best_loss=0.02827
Epoch 50/80: current_loss=0.09909 | best_loss=0.02827
Epoch 51/80: current_loss=0.17606 | best_loss=0.02827
Epoch 52/80: current_loss=0.02805 | best_loss=0.02805
Epoch 53/80: current_loss=0.03571 | best_loss=0.02805
Epoch 54/80: current_loss=0.02831 | best_loss=0.02805
Epoch 55/80: current_loss=0.04030 | best_loss=0.02805
Epoch 56/80: current_loss=0.03202 | best_loss=0.02805
Epoch 57/80: current_loss=0.02916 | best_loss=0.02805
Epoch 58/80: current_loss=0.03026 | best_loss=0.02805
Epoch 59/80: current_loss=0.02906 | best_loss=0.02805
Epoch 60/80: current_loss=0.02845 | best_loss=0.02805
Epoch 61/80: current_loss=0.02957 | best_loss=0.02805
Epoch 62/80: current_loss=0.02843 | best_loss=0.02805
Epoch 63/80: current_loss=0.02851 | best_loss=0.02805
Epoch 64/80: current_loss=0.02845 | best_loss=0.02805
Epoch 65/80: current_loss=0.02862 | best_loss=0.02805
Epoch 66/80: current_loss=0.02851 | best_loss=0.02805
Epoch 67/80: current_loss=0.02836 | best_loss=0.02805
Epoch 68/80: current_loss=0.02831 | best_loss=0.02805
Epoch 69/80: current_loss=0.02839 | best_loss=0.02805
Epoch 70/80: current_loss=0.02838 | best_loss=0.02805
Epoch 71/80: current_loss=0.02829 | best_loss=0.02805
Epoch 72/80: current_loss=0.02843 | best_loss=0.02805
Early Stopping at epoch 72
      explained_var=0.00543 | mse_loss=0.02765
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.16838 | best_loss=0.16838
Epoch 1/80: current_loss=0.07833 | best_loss=0.07833
Epoch 2/80: current_loss=0.10825 | best_loss=0.07833
Epoch 3/80: current_loss=0.18114 | best_loss=0.07833
Epoch 4/80: current_loss=0.08805 | best_loss=0.07833
Epoch 5/80: current_loss=0.11959 | best_loss=0.07833
Epoch 6/80: current_loss=0.03944 | best_loss=0.03944
Epoch 7/80: current_loss=0.04246 | best_loss=0.03944
Epoch 8/80: current_loss=0.05324 | best_loss=0.03944
Epoch 9/80: current_loss=0.10363 | best_loss=0.03944
Epoch 10/80: current_loss=0.04416 | best_loss=0.03944
Epoch 11/80: current_loss=0.04396 | best_loss=0.03944
Epoch 12/80: current_loss=0.05183 | best_loss=0.03944
Epoch 13/80: current_loss=0.04583 | best_loss=0.03944
Epoch 14/80: current_loss=0.04026 | best_loss=0.03944
Epoch 15/80: current_loss=0.11049 | best_loss=0.03944
Epoch 16/80: current_loss=0.13646 | best_loss=0.03944
Epoch 17/80: current_loss=0.03355 | best_loss=0.03355
Epoch 18/80: current_loss=0.20977 | best_loss=0.03355
Epoch 19/80: current_loss=0.10665 | best_loss=0.03355
Epoch 20/80: current_loss=0.25235 | best_loss=0.03355
Epoch 21/80: current_loss=0.03623 | best_loss=0.03355
Epoch 22/80: current_loss=0.17647 | best_loss=0.03355
Epoch 23/80: current_loss=0.04047 | best_loss=0.03355
Epoch 24/80: current_loss=0.05408 | best_loss=0.03355
Epoch 25/80: current_loss=0.06329 | best_loss=0.03355
Epoch 26/80: current_loss=0.03045 | best_loss=0.03045
Epoch 27/80: current_loss=0.03619 | best_loss=0.03045
Epoch 28/80: current_loss=0.03132 | best_loss=0.03045
Epoch 29/80: current_loss=0.03139 | best_loss=0.03045
Epoch 30/80: current_loss=0.03050 | best_loss=0.03045
Epoch 31/80: current_loss=0.03114 | best_loss=0.03045
Epoch 32/80: current_loss=0.03129 | best_loss=0.03045
Epoch 33/80: current_loss=0.03058 | best_loss=0.03045
Epoch 34/80: current_loss=0.03162 | best_loss=0.03045
Epoch 35/80: current_loss=0.03157 | best_loss=0.03045
Epoch 36/80: current_loss=0.03064 | best_loss=0.03045
Epoch 37/80: current_loss=0.03144 | best_loss=0.03045
Epoch 38/80: current_loss=0.03112 | best_loss=0.03045
Epoch 39/80: current_loss=0.03100 | best_loss=0.03045
Epoch 40/80: current_loss=0.03191 | best_loss=0.03045
Epoch 41/80: current_loss=0.03120 | best_loss=0.03045
Epoch 42/80: current_loss=0.03076 | best_loss=0.03045
Epoch 43/80: current_loss=0.03194 | best_loss=0.03045
Epoch 44/80: current_loss=0.03090 | best_loss=0.03045
Epoch 45/80: current_loss=0.03142 | best_loss=0.03045
Epoch 46/80: current_loss=0.03116 | best_loss=0.03045
Early Stopping at epoch 46
      explained_var=-0.00465 | mse_loss=0.02971
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.62076 | best_loss=0.62076
Epoch 1/80: current_loss=0.27804 | best_loss=0.27804
Epoch 2/80: current_loss=0.09484 | best_loss=0.09484
Epoch 3/80: current_loss=0.05184 | best_loss=0.05184
Epoch 4/80: current_loss=0.30652 | best_loss=0.05184
Epoch 5/80: current_loss=0.08039 | best_loss=0.05184
Epoch 6/80: current_loss=0.03949 | best_loss=0.03949
Epoch 7/80: current_loss=0.03879 | best_loss=0.03879
Epoch 8/80: current_loss=0.04492 | best_loss=0.03879
Epoch 9/80: current_loss=0.04668 | best_loss=0.03879
Epoch 10/80: current_loss=0.09790 | best_loss=0.03879
Epoch 11/80: current_loss=0.04979 | best_loss=0.03879
Epoch 12/80: current_loss=0.05121 | best_loss=0.03879
Epoch 13/80: current_loss=0.04095 | best_loss=0.03879
Epoch 14/80: current_loss=0.07835 | best_loss=0.03879
Epoch 15/80: current_loss=0.06999 | best_loss=0.03879
Epoch 16/80: current_loss=0.07221 | best_loss=0.03879
Epoch 17/80: current_loss=0.11621 | best_loss=0.03879
Epoch 18/80: current_loss=0.11448 | best_loss=0.03879
Epoch 19/80: current_loss=0.03737 | best_loss=0.03737
Epoch 20/80: current_loss=0.07515 | best_loss=0.03737
Epoch 21/80: current_loss=0.06521 | best_loss=0.03737
Epoch 22/80: current_loss=0.15254 | best_loss=0.03737
Epoch 23/80: current_loss=0.06344 | best_loss=0.03737
Epoch 24/80: current_loss=0.06052 | best_loss=0.03737
Epoch 25/80: current_loss=0.31591 | best_loss=0.03737
Epoch 26/80: current_loss=1.03972 | best_loss=0.03737
Epoch 27/80: current_loss=0.09172 | best_loss=0.03737
Epoch 28/80: current_loss=0.04098 | best_loss=0.03737
Epoch 29/80: current_loss=0.12736 | best_loss=0.03737
Epoch 30/80: current_loss=0.03763 | best_loss=0.03737
Epoch 31/80: current_loss=0.10803 | best_loss=0.03737
Epoch 32/80: current_loss=0.32123 | best_loss=0.03737
Epoch 33/80: current_loss=0.14519 | best_loss=0.03737
Epoch 34/80: current_loss=0.20359 | best_loss=0.03737
Epoch 35/80: current_loss=0.15847 | best_loss=0.03737
Epoch 36/80: current_loss=0.11722 | best_loss=0.03737
Epoch 37/80: current_loss=0.33126 | best_loss=0.03737
Epoch 38/80: current_loss=0.11732 | best_loss=0.03737
Epoch 39/80: current_loss=0.05069 | best_loss=0.03737
Early Stopping at epoch 39
      explained_var=-0.28195 | mse_loss=0.03806
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.38223 | best_loss=0.38223
Epoch 1/80: current_loss=0.09046 | best_loss=0.09046
Epoch 2/80: current_loss=0.10228 | best_loss=0.09046
Epoch 3/80: current_loss=0.06586 | best_loss=0.06586
Epoch 4/80: current_loss=0.08194 | best_loss=0.06586
Epoch 5/80: current_loss=0.14389 | best_loss=0.06586
Epoch 6/80: current_loss=0.05324 | best_loss=0.05324
Epoch 7/80: current_loss=0.14900 | best_loss=0.05324
Epoch 8/80: current_loss=0.05727 | best_loss=0.05324
Epoch 9/80: current_loss=0.05915 | best_loss=0.05324
Epoch 10/80: current_loss=0.03604 | best_loss=0.03604
Epoch 11/80: current_loss=0.03399 | best_loss=0.03399
Epoch 12/80: current_loss=0.03372 | best_loss=0.03372
Epoch 13/80: current_loss=0.05121 | best_loss=0.03372
Epoch 14/80: current_loss=0.06560 | best_loss=0.03372
Epoch 15/80: current_loss=0.10656 | best_loss=0.03372
Epoch 16/80: current_loss=0.03754 | best_loss=0.03372
Epoch 17/80: current_loss=0.04654 | best_loss=0.03372
Epoch 18/80: current_loss=0.13916 | best_loss=0.03372
Epoch 19/80: current_loss=0.11046 | best_loss=0.03372
Epoch 20/80: current_loss=0.03904 | best_loss=0.03372
Epoch 21/80: current_loss=0.06777 | best_loss=0.03372
Epoch 22/80: current_loss=0.10240 | best_loss=0.03372
Epoch 23/80: current_loss=0.12604 | best_loss=0.03372
Epoch 24/80: current_loss=0.15425 | best_loss=0.03372
Epoch 25/80: current_loss=0.06904 | best_loss=0.03372
Epoch 26/80: current_loss=0.10720 | best_loss=0.03372
Epoch 27/80: current_loss=0.06484 | best_loss=0.03372
Epoch 28/80: current_loss=0.18644 | best_loss=0.03372
Epoch 29/80: current_loss=0.12492 | best_loss=0.03372
Epoch 30/80: current_loss=0.05945 | best_loss=0.03372
Epoch 31/80: current_loss=0.10840 | best_loss=0.03372
Epoch 32/80: current_loss=0.11623 | best_loss=0.03372
Early Stopping at epoch 32
      explained_var=-0.03688 | mse_loss=0.03422
----------------------------------------------
Average early_stopping_point: 26| avg_exp_var=-0.06897| avg_loss=0.03143
----------------------------------------------


----------------------------------------------
Params for Trial 11
{'learning_rate': 0.0001, 'weight_decay': 0.002554434662107642, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03471 | best_loss=0.03471
Epoch 1/80: current_loss=0.03578 | best_loss=0.03471
Epoch 2/80: current_loss=0.03027 | best_loss=0.03027
Epoch 3/80: current_loss=0.03027 | best_loss=0.03027
Epoch 4/80: current_loss=0.03042 | best_loss=0.03027
Epoch 5/80: current_loss=0.02956 | best_loss=0.02956
Epoch 6/80: current_loss=0.02921 | best_loss=0.02921
Epoch 7/80: current_loss=0.02905 | best_loss=0.02905
Epoch 8/80: current_loss=0.02841 | best_loss=0.02841
Epoch 9/80: current_loss=0.02837 | best_loss=0.02837
Epoch 10/80: current_loss=0.02894 | best_loss=0.02837
Epoch 11/80: current_loss=0.02810 | best_loss=0.02810
Epoch 12/80: current_loss=0.02768 | best_loss=0.02768
Epoch 13/80: current_loss=0.02826 | best_loss=0.02768
Epoch 14/80: current_loss=0.02773 | best_loss=0.02768
Epoch 15/80: current_loss=0.02766 | best_loss=0.02766
Epoch 16/80: current_loss=0.02795 | best_loss=0.02766
Epoch 17/80: current_loss=0.02778 | best_loss=0.02766
Epoch 18/80: current_loss=0.02774 | best_loss=0.02766
Epoch 19/80: current_loss=0.02696 | best_loss=0.02696
Epoch 20/80: current_loss=0.02729 | best_loss=0.02696
Epoch 21/80: current_loss=0.02754 | best_loss=0.02696
Epoch 22/80: current_loss=0.02667 | best_loss=0.02667
Epoch 23/80: current_loss=0.02738 | best_loss=0.02667
Epoch 24/80: current_loss=0.02689 | best_loss=0.02667
Epoch 25/80: current_loss=0.02682 | best_loss=0.02667
Epoch 26/80: current_loss=0.02713 | best_loss=0.02667
Epoch 27/80: current_loss=0.02686 | best_loss=0.02667
Epoch 28/80: current_loss=0.02707 | best_loss=0.02667
Epoch 29/80: current_loss=0.02667 | best_loss=0.02667
Epoch 30/80: current_loss=0.02743 | best_loss=0.02667
Epoch 31/80: current_loss=0.02663 | best_loss=0.02663
Epoch 32/80: current_loss=0.02755 | best_loss=0.02663
Epoch 33/80: current_loss=0.02689 | best_loss=0.02663
Epoch 34/80: current_loss=0.02729 | best_loss=0.02663
Epoch 35/80: current_loss=0.02648 | best_loss=0.02648
Epoch 36/80: current_loss=0.02721 | best_loss=0.02648
Epoch 37/80: current_loss=0.02669 | best_loss=0.02648
Epoch 38/80: current_loss=0.02781 | best_loss=0.02648
Epoch 39/80: current_loss=0.02722 | best_loss=0.02648
Epoch 40/80: current_loss=0.02698 | best_loss=0.02648
Epoch 41/80: current_loss=0.02700 | best_loss=0.02648
Epoch 42/80: current_loss=0.02683 | best_loss=0.02648
Epoch 43/80: current_loss=0.02642 | best_loss=0.02642
Epoch 44/80: current_loss=0.02687 | best_loss=0.02642
Epoch 45/80: current_loss=0.02704 | best_loss=0.02642
Epoch 46/80: current_loss=0.02627 | best_loss=0.02627
Epoch 47/80: current_loss=0.02722 | best_loss=0.02627
Epoch 48/80: current_loss=0.02621 | best_loss=0.02621
Epoch 49/80: current_loss=0.02676 | best_loss=0.02621
Epoch 50/80: current_loss=0.02684 | best_loss=0.02621
Epoch 51/80: current_loss=0.02608 | best_loss=0.02608
Epoch 52/80: current_loss=0.02695 | best_loss=0.02608
Epoch 53/80: current_loss=0.02632 | best_loss=0.02608
Epoch 54/80: current_loss=0.02626 | best_loss=0.02608
Epoch 55/80: current_loss=0.02751 | best_loss=0.02608
Epoch 56/80: current_loss=0.02672 | best_loss=0.02608
Epoch 57/80: current_loss=0.02674 | best_loss=0.02608
Epoch 58/80: current_loss=0.02634 | best_loss=0.02608
Epoch 59/80: current_loss=0.02715 | best_loss=0.02608
Epoch 60/80: current_loss=0.02641 | best_loss=0.02608
Epoch 61/80: current_loss=0.02636 | best_loss=0.02608
Epoch 62/80: current_loss=0.02707 | best_loss=0.02608
Epoch 63/80: current_loss=0.02625 | best_loss=0.02608
Epoch 64/80: current_loss=0.02685 | best_loss=0.02608
Epoch 65/80: current_loss=0.02640 | best_loss=0.02608
Epoch 66/80: current_loss=0.02628 | best_loss=0.02608
Epoch 67/80: current_loss=0.02733 | best_loss=0.02608
Epoch 68/80: current_loss=0.02604 | best_loss=0.02604
Epoch 69/80: current_loss=0.02711 | best_loss=0.02604
Epoch 70/80: current_loss=0.02593 | best_loss=0.02593
Epoch 71/80: current_loss=0.02665 | best_loss=0.02593
Epoch 72/80: current_loss=0.02609 | best_loss=0.02593
Epoch 73/80: current_loss=0.02696 | best_loss=0.02593
Epoch 74/80: current_loss=0.02600 | best_loss=0.02593
Epoch 75/80: current_loss=0.02623 | best_loss=0.02593
Epoch 76/80: current_loss=0.02665 | best_loss=0.02593
Epoch 77/80: current_loss=0.02680 | best_loss=0.02593
Epoch 78/80: current_loss=0.02628 | best_loss=0.02593
Epoch 79/80: current_loss=0.02676 | best_loss=0.02593
      explained_var=0.03132 | mse_loss=0.02549
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02905 | best_loss=0.02905
Epoch 1/80: current_loss=0.02853 | best_loss=0.02853
Epoch 2/80: current_loss=0.02849 | best_loss=0.02849
Epoch 3/80: current_loss=0.02853 | best_loss=0.02849
Epoch 4/80: current_loss=0.02843 | best_loss=0.02843
Epoch 5/80: current_loss=0.02852 | best_loss=0.02843
Epoch 6/80: current_loss=0.02861 | best_loss=0.02843
Epoch 7/80: current_loss=0.02845 | best_loss=0.02843
Epoch 8/80: current_loss=0.02839 | best_loss=0.02839
Epoch 9/80: current_loss=0.02831 | best_loss=0.02831
Epoch 10/80: current_loss=0.02825 | best_loss=0.02825
Epoch 11/80: current_loss=0.02845 | best_loss=0.02825
Epoch 12/80: current_loss=0.02830 | best_loss=0.02825
Epoch 13/80: current_loss=0.02826 | best_loss=0.02825
Epoch 14/80: current_loss=0.02833 | best_loss=0.02825
Epoch 15/80: current_loss=0.02818 | best_loss=0.02818
Epoch 16/80: current_loss=0.02825 | best_loss=0.02818
Epoch 17/80: current_loss=0.02835 | best_loss=0.02818
Epoch 18/80: current_loss=0.02812 | best_loss=0.02812
Epoch 19/80: current_loss=0.02816 | best_loss=0.02812
Epoch 20/80: current_loss=0.02812 | best_loss=0.02812
Epoch 21/80: current_loss=0.02823 | best_loss=0.02812
Epoch 22/80: current_loss=0.02813 | best_loss=0.02812
Epoch 23/80: current_loss=0.02811 | best_loss=0.02811
Epoch 24/80: current_loss=0.02812 | best_loss=0.02811
Epoch 25/80: current_loss=0.02824 | best_loss=0.02811
Epoch 26/80: current_loss=0.02824 | best_loss=0.02811
Epoch 27/80: current_loss=0.02843 | best_loss=0.02811
Epoch 28/80: current_loss=0.02826 | best_loss=0.02811
Epoch 29/80: current_loss=0.02819 | best_loss=0.02811
Epoch 30/80: current_loss=0.02836 | best_loss=0.02811
Epoch 31/80: current_loss=0.02825 | best_loss=0.02811
Epoch 32/80: current_loss=0.02816 | best_loss=0.02811
Epoch 33/80: current_loss=0.02809 | best_loss=0.02809
Epoch 34/80: current_loss=0.02835 | best_loss=0.02809
Epoch 35/80: current_loss=0.02808 | best_loss=0.02808
Epoch 36/80: current_loss=0.02819 | best_loss=0.02808
Epoch 37/80: current_loss=0.02808 | best_loss=0.02808
Epoch 38/80: current_loss=0.02828 | best_loss=0.02808
Epoch 39/80: current_loss=0.02807 | best_loss=0.02807
Epoch 40/80: current_loss=0.02807 | best_loss=0.02807
Epoch 41/80: current_loss=0.02813 | best_loss=0.02807
Epoch 42/80: current_loss=0.02811 | best_loss=0.02807
Epoch 43/80: current_loss=0.02808 | best_loss=0.02807
Epoch 44/80: current_loss=0.02817 | best_loss=0.02807
Epoch 45/80: current_loss=0.02812 | best_loss=0.02807
Epoch 46/80: current_loss=0.02806 | best_loss=0.02806
Epoch 47/80: current_loss=0.02827 | best_loss=0.02806
Epoch 48/80: current_loss=0.02806 | best_loss=0.02806
Epoch 49/80: current_loss=0.02829 | best_loss=0.02806
Epoch 50/80: current_loss=0.02814 | best_loss=0.02806
Epoch 51/80: current_loss=0.02805 | best_loss=0.02805
Epoch 52/80: current_loss=0.02836 | best_loss=0.02805
Epoch 53/80: current_loss=0.02805 | best_loss=0.02805
Epoch 54/80: current_loss=0.02849 | best_loss=0.02805
Epoch 55/80: current_loss=0.02806 | best_loss=0.02805
Epoch 56/80: current_loss=0.02814 | best_loss=0.02805
Epoch 57/80: current_loss=0.02838 | best_loss=0.02805
Epoch 58/80: current_loss=0.02812 | best_loss=0.02805
Epoch 59/80: current_loss=0.02811 | best_loss=0.02805
Epoch 60/80: current_loss=0.02813 | best_loss=0.02805
Epoch 61/80: current_loss=0.02806 | best_loss=0.02805
Epoch 62/80: current_loss=0.02819 | best_loss=0.02805
Epoch 63/80: current_loss=0.02808 | best_loss=0.02805
Epoch 64/80: current_loss=0.02819 | best_loss=0.02805
Epoch 65/80: current_loss=0.02840 | best_loss=0.02805
Epoch 66/80: current_loss=0.02813 | best_loss=0.02805
Epoch 67/80: current_loss=0.02838 | best_loss=0.02805
Epoch 68/80: current_loss=0.02816 | best_loss=0.02805
Epoch 69/80: current_loss=0.02817 | best_loss=0.02805
Epoch 70/80: current_loss=0.02855 | best_loss=0.02805
Epoch 71/80: current_loss=0.02811 | best_loss=0.02805
Epoch 72/80: current_loss=0.02817 | best_loss=0.02805
Epoch 73/80: current_loss=0.02813 | best_loss=0.02805
Early Stopping at epoch 73
      explained_var=0.00801 | mse_loss=0.02757
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03018 | best_loss=0.03018
Epoch 1/80: current_loss=0.03060 | best_loss=0.03018
Epoch 2/80: current_loss=0.03074 | best_loss=0.03018
Epoch 3/80: current_loss=0.03162 | best_loss=0.03018
Epoch 4/80: current_loss=0.03007 | best_loss=0.03007
Epoch 5/80: current_loss=0.03164 | best_loss=0.03007
Epoch 6/80: current_loss=0.03049 | best_loss=0.03007
Epoch 7/80: current_loss=0.03056 | best_loss=0.03007
Epoch 8/80: current_loss=0.03089 | best_loss=0.03007
Epoch 9/80: current_loss=0.03080 | best_loss=0.03007
Epoch 10/80: current_loss=0.03011 | best_loss=0.03007
Epoch 11/80: current_loss=0.03049 | best_loss=0.03007
Epoch 12/80: current_loss=0.03097 | best_loss=0.03007
Epoch 13/80: current_loss=0.03092 | best_loss=0.03007
Epoch 14/80: current_loss=0.03092 | best_loss=0.03007
Epoch 15/80: current_loss=0.03034 | best_loss=0.03007
Epoch 16/80: current_loss=0.03065 | best_loss=0.03007
Epoch 17/80: current_loss=0.03093 | best_loss=0.03007
Epoch 18/80: current_loss=0.03078 | best_loss=0.03007
Epoch 19/80: current_loss=0.03041 | best_loss=0.03007
Epoch 20/80: current_loss=0.03089 | best_loss=0.03007
Epoch 21/80: current_loss=0.03073 | best_loss=0.03007
Epoch 22/80: current_loss=0.03037 | best_loss=0.03007
Epoch 23/80: current_loss=0.03065 | best_loss=0.03007
Epoch 24/80: current_loss=0.03141 | best_loss=0.03007
Early Stopping at epoch 24
      explained_var=0.01647 | mse_loss=0.02932
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02808 | best_loss=0.02808
Epoch 1/80: current_loss=0.02808 | best_loss=0.02808
Epoch 2/80: current_loss=0.02813 | best_loss=0.02808
Epoch 3/80: current_loss=0.02813 | best_loss=0.02808
Epoch 4/80: current_loss=0.02816 | best_loss=0.02808
Epoch 5/80: current_loss=0.02815 | best_loss=0.02808
Epoch 6/80: current_loss=0.02814 | best_loss=0.02808
Epoch 7/80: current_loss=0.02818 | best_loss=0.02808
Epoch 8/80: current_loss=0.02823 | best_loss=0.02808
Epoch 9/80: current_loss=0.02819 | best_loss=0.02808
Epoch 10/80: current_loss=0.02817 | best_loss=0.02808
Epoch 11/80: current_loss=0.02855 | best_loss=0.02808
Epoch 12/80: current_loss=0.02831 | best_loss=0.02808
Epoch 13/80: current_loss=0.02822 | best_loss=0.02808
Epoch 14/80: current_loss=0.02844 | best_loss=0.02808
Epoch 15/80: current_loss=0.02828 | best_loss=0.02808
Epoch 16/80: current_loss=0.02827 | best_loss=0.02808
Epoch 17/80: current_loss=0.02837 | best_loss=0.02808
Epoch 18/80: current_loss=0.02822 | best_loss=0.02808
Epoch 19/80: current_loss=0.02823 | best_loss=0.02808
Epoch 20/80: current_loss=0.02849 | best_loss=0.02808
Epoch 21/80: current_loss=0.02834 | best_loss=0.02808
Early Stopping at epoch 21
      explained_var=-0.00201 | mse_loss=0.02842
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03294 | best_loss=0.03294
Epoch 1/80: current_loss=0.03242 | best_loss=0.03242
Epoch 2/80: current_loss=0.03242 | best_loss=0.03242
Epoch 3/80: current_loss=0.03250 | best_loss=0.03242
Epoch 4/80: current_loss=0.03253 | best_loss=0.03242
Epoch 5/80: current_loss=0.03251 | best_loss=0.03242
Epoch 6/80: current_loss=0.03252 | best_loss=0.03242
Epoch 7/80: current_loss=0.03251 | best_loss=0.03242
Epoch 8/80: current_loss=0.03254 | best_loss=0.03242
Epoch 9/80: current_loss=0.03263 | best_loss=0.03242
Epoch 10/80: current_loss=0.03261 | best_loss=0.03242
Epoch 11/80: current_loss=0.03265 | best_loss=0.03242
Epoch 12/80: current_loss=0.03278 | best_loss=0.03242
Epoch 13/80: current_loss=0.03273 | best_loss=0.03242
Epoch 14/80: current_loss=0.03273 | best_loss=0.03242
Epoch 15/80: current_loss=0.03271 | best_loss=0.03242
Epoch 16/80: current_loss=0.03291 | best_loss=0.03242
Epoch 17/80: current_loss=0.03275 | best_loss=0.03242
Epoch 18/80: current_loss=0.03279 | best_loss=0.03242
Epoch 19/80: current_loss=0.03278 | best_loss=0.03242
Epoch 20/80: current_loss=0.03288 | best_loss=0.03242
Epoch 21/80: current_loss=0.03280 | best_loss=0.03242
Epoch 22/80: current_loss=0.03295 | best_loss=0.03242
Early Stopping at epoch 22
      explained_var=-0.02139 | mse_loss=0.03319
----------------------------------------------
Average early_stopping_point: 28| avg_exp_var=0.00648| avg_loss=0.02880
----------------------------------------------


----------------------------------------------
Params for Trial 12
{'learning_rate': 0.001, 'weight_decay': 8.686045909808823e-05, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03200 | best_loss=0.03200
Epoch 1/80: current_loss=0.02748 | best_loss=0.02748
Epoch 2/80: current_loss=0.02809 | best_loss=0.02748
Epoch 3/80: current_loss=0.02587 | best_loss=0.02587
Epoch 4/80: current_loss=0.02578 | best_loss=0.02578
Epoch 5/80: current_loss=0.02538 | best_loss=0.02538
Epoch 6/80: current_loss=0.02614 | best_loss=0.02538
Epoch 7/80: current_loss=0.02544 | best_loss=0.02538
Epoch 8/80: current_loss=0.02574 | best_loss=0.02538
Epoch 9/80: current_loss=0.02575 | best_loss=0.02538
Epoch 10/80: current_loss=0.02535 | best_loss=0.02535
Epoch 11/80: current_loss=0.02909 | best_loss=0.02535
Epoch 12/80: current_loss=0.02753 | best_loss=0.02535
Epoch 13/80: current_loss=0.02760 | best_loss=0.02535
Epoch 14/80: current_loss=0.02678 | best_loss=0.02535
Epoch 15/80: current_loss=0.02643 | best_loss=0.02535
Epoch 16/80: current_loss=0.02984 | best_loss=0.02535
Epoch 17/80: current_loss=0.02592 | best_loss=0.02535
Epoch 18/80: current_loss=0.02683 | best_loss=0.02535
Epoch 19/80: current_loss=0.02599 | best_loss=0.02535
Epoch 20/80: current_loss=0.02950 | best_loss=0.02535
Epoch 21/80: current_loss=0.02624 | best_loss=0.02535
Epoch 22/80: current_loss=0.02607 | best_loss=0.02535
Epoch 23/80: current_loss=0.02949 | best_loss=0.02535
Epoch 24/80: current_loss=0.02604 | best_loss=0.02535
Epoch 25/80: current_loss=0.02870 | best_loss=0.02535
Epoch 26/80: current_loss=0.02644 | best_loss=0.02535
Epoch 27/80: current_loss=0.02936 | best_loss=0.02535
Epoch 28/80: current_loss=0.02790 | best_loss=0.02535
Epoch 29/80: current_loss=0.02564 | best_loss=0.02535
Epoch 30/80: current_loss=0.02650 | best_loss=0.02535
Early Stopping at epoch 30
      explained_var=0.04079 | mse_loss=0.02481
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02812 | best_loss=0.02812
Epoch 1/80: current_loss=0.02807 | best_loss=0.02807
Epoch 2/80: current_loss=0.02810 | best_loss=0.02807
Epoch 3/80: current_loss=0.02875 | best_loss=0.02807
Epoch 4/80: current_loss=0.03012 | best_loss=0.02807
Epoch 5/80: current_loss=0.02897 | best_loss=0.02807
Epoch 6/80: current_loss=0.02859 | best_loss=0.02807
Epoch 7/80: current_loss=0.02850 | best_loss=0.02807
Epoch 8/80: current_loss=0.02762 | best_loss=0.02762
Epoch 9/80: current_loss=0.02811 | best_loss=0.02762
Epoch 10/80: current_loss=0.02784 | best_loss=0.02762
Epoch 11/80: current_loss=0.02793 | best_loss=0.02762
Epoch 12/80: current_loss=0.02771 | best_loss=0.02762
Epoch 13/80: current_loss=0.02756 | best_loss=0.02756
Epoch 14/80: current_loss=0.02748 | best_loss=0.02748
Epoch 15/80: current_loss=0.02719 | best_loss=0.02719
Epoch 16/80: current_loss=0.02773 | best_loss=0.02719
Epoch 17/80: current_loss=0.02776 | best_loss=0.02719
Epoch 18/80: current_loss=0.02769 | best_loss=0.02719
Epoch 19/80: current_loss=0.02880 | best_loss=0.02719
Epoch 20/80: current_loss=0.02774 | best_loss=0.02719
Epoch 21/80: current_loss=0.02799 | best_loss=0.02719
Epoch 22/80: current_loss=0.02785 | best_loss=0.02719
Epoch 23/80: current_loss=0.02874 | best_loss=0.02719
Epoch 24/80: current_loss=0.02749 | best_loss=0.02719
Epoch 25/80: current_loss=0.02798 | best_loss=0.02719
Epoch 26/80: current_loss=0.02783 | best_loss=0.02719
Epoch 27/80: current_loss=0.02827 | best_loss=0.02719
Epoch 28/80: current_loss=0.02771 | best_loss=0.02719
Epoch 29/80: current_loss=0.02879 | best_loss=0.02719
Epoch 30/80: current_loss=0.02759 | best_loss=0.02719
Epoch 31/80: current_loss=0.02815 | best_loss=0.02719
Epoch 32/80: current_loss=0.02788 | best_loss=0.02719
Epoch 33/80: current_loss=0.02774 | best_loss=0.02719
Epoch 34/80: current_loss=0.02794 | best_loss=0.02719
Epoch 35/80: current_loss=0.02801 | best_loss=0.02719
Early Stopping at epoch 35
      explained_var=0.03784 | mse_loss=0.02673
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02942 | best_loss=0.02942
Epoch 1/80: current_loss=0.02998 | best_loss=0.02942
Epoch 2/80: current_loss=0.03270 | best_loss=0.02942
Epoch 3/80: current_loss=0.03153 | best_loss=0.02942
Epoch 4/80: current_loss=0.03050 | best_loss=0.02942
Epoch 5/80: current_loss=0.03088 | best_loss=0.02942
Epoch 6/80: current_loss=0.03427 | best_loss=0.02942
Epoch 7/80: current_loss=0.03068 | best_loss=0.02942
Epoch 8/80: current_loss=0.03027 | best_loss=0.02942
Epoch 9/80: current_loss=0.03210 | best_loss=0.02942
Epoch 10/80: current_loss=0.02924 | best_loss=0.02924
Epoch 11/80: current_loss=0.02933 | best_loss=0.02924
Epoch 12/80: current_loss=0.03581 | best_loss=0.02924
Epoch 13/80: current_loss=0.02963 | best_loss=0.02924
Epoch 14/80: current_loss=0.03136 | best_loss=0.02924
Epoch 15/80: current_loss=0.03053 | best_loss=0.02924
Epoch 16/80: current_loss=0.03003 | best_loss=0.02924
Epoch 17/80: current_loss=0.03408 | best_loss=0.02924
Epoch 18/80: current_loss=0.02966 | best_loss=0.02924
Epoch 19/80: current_loss=0.03141 | best_loss=0.02924
Epoch 20/80: current_loss=0.02969 | best_loss=0.02924
Epoch 21/80: current_loss=0.03205 | best_loss=0.02924
Epoch 22/80: current_loss=0.02951 | best_loss=0.02924
Epoch 23/80: current_loss=0.03378 | best_loss=0.02924
Epoch 24/80: current_loss=0.03001 | best_loss=0.02924
Epoch 25/80: current_loss=0.03258 | best_loss=0.02924
Epoch 26/80: current_loss=0.02995 | best_loss=0.02924
Epoch 27/80: current_loss=0.03189 | best_loss=0.02924
Epoch 28/80: current_loss=0.03129 | best_loss=0.02924
Epoch 29/80: current_loss=0.03080 | best_loss=0.02924
Epoch 30/80: current_loss=0.03336 | best_loss=0.02924
Early Stopping at epoch 30
      explained_var=0.04302 | mse_loss=0.02848
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02894 | best_loss=0.02894
Epoch 1/80: current_loss=0.02802 | best_loss=0.02802
Epoch 2/80: current_loss=0.03126 | best_loss=0.02802
Epoch 3/80: current_loss=0.03098 | best_loss=0.02802
Epoch 4/80: current_loss=0.02861 | best_loss=0.02802
Epoch 5/80: current_loss=0.02812 | best_loss=0.02802
Epoch 6/80: current_loss=0.02822 | best_loss=0.02802
Epoch 7/80: current_loss=0.02856 | best_loss=0.02802
Epoch 8/80: current_loss=0.02929 | best_loss=0.02802
Epoch 9/80: current_loss=0.02825 | best_loss=0.02802
Epoch 10/80: current_loss=0.02914 | best_loss=0.02802
Epoch 11/80: current_loss=0.02864 | best_loss=0.02802
Epoch 12/80: current_loss=0.02822 | best_loss=0.02802
Epoch 13/80: current_loss=0.02849 | best_loss=0.02802
Epoch 14/80: current_loss=0.02911 | best_loss=0.02802
Epoch 15/80: current_loss=0.02831 | best_loss=0.02802
Epoch 16/80: current_loss=0.02830 | best_loss=0.02802
Epoch 17/80: current_loss=0.02833 | best_loss=0.02802
Epoch 18/80: current_loss=0.02877 | best_loss=0.02802
Epoch 19/80: current_loss=0.02851 | best_loss=0.02802
Epoch 20/80: current_loss=0.02832 | best_loss=0.02802
Epoch 21/80: current_loss=0.02835 | best_loss=0.02802
Early Stopping at epoch 21
      explained_var=0.00095 | mse_loss=0.02839
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03282 | best_loss=0.03282
Epoch 1/80: current_loss=0.03307 | best_loss=0.03282
Epoch 2/80: current_loss=0.03302 | best_loss=0.03282
Epoch 3/80: current_loss=0.03373 | best_loss=0.03282
Epoch 4/80: current_loss=0.03322 | best_loss=0.03282
Epoch 5/80: current_loss=0.03315 | best_loss=0.03282
Epoch 6/80: current_loss=0.03411 | best_loss=0.03282
Epoch 7/80: current_loss=0.03319 | best_loss=0.03282
Epoch 8/80: current_loss=0.03318 | best_loss=0.03282
Epoch 9/80: current_loss=0.03344 | best_loss=0.03282
Epoch 10/80: current_loss=0.03367 | best_loss=0.03282
Epoch 11/80: current_loss=0.03343 | best_loss=0.03282
Epoch 12/80: current_loss=0.03359 | best_loss=0.03282
Epoch 13/80: current_loss=0.03383 | best_loss=0.03282
Epoch 14/80: current_loss=0.03434 | best_loss=0.03282
Epoch 15/80: current_loss=0.03308 | best_loss=0.03282
Epoch 16/80: current_loss=0.03256 | best_loss=0.03256
Epoch 17/80: current_loss=0.03269 | best_loss=0.03256
Epoch 18/80: current_loss=0.03293 | best_loss=0.03256
Epoch 19/80: current_loss=0.03378 | best_loss=0.03256
Epoch 20/80: current_loss=0.03373 | best_loss=0.03256
Epoch 21/80: current_loss=0.03346 | best_loss=0.03256
Epoch 22/80: current_loss=0.03326 | best_loss=0.03256
Epoch 23/80: current_loss=0.03335 | best_loss=0.03256
Epoch 24/80: current_loss=0.03347 | best_loss=0.03256
Epoch 25/80: current_loss=0.03466 | best_loss=0.03256
Epoch 26/80: current_loss=0.03478 | best_loss=0.03256
Epoch 27/80: current_loss=0.03397 | best_loss=0.03256
Epoch 28/80: current_loss=0.03345 | best_loss=0.03256
Epoch 29/80: current_loss=0.03432 | best_loss=0.03256
Epoch 30/80: current_loss=0.03384 | best_loss=0.03256
Epoch 31/80: current_loss=0.03333 | best_loss=0.03256
Epoch 32/80: current_loss=0.03365 | best_loss=0.03256
Epoch 33/80: current_loss=0.03359 | best_loss=0.03256
Epoch 34/80: current_loss=0.03388 | best_loss=0.03256
Epoch 35/80: current_loss=0.03386 | best_loss=0.03256
Epoch 36/80: current_loss=0.03437 | best_loss=0.03256
Early Stopping at epoch 36
      explained_var=-0.02636 | mse_loss=0.03336
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.01925| avg_loss=0.02835
----------------------------------------------


----------------------------------------------
Params for Trial 13
{'learning_rate': 0.01, 'weight_decay': 0.00010813129554321693, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.07658 | best_loss=0.07658
Epoch 1/80: current_loss=0.02795 | best_loss=0.02795
Epoch 2/80: current_loss=0.04915 | best_loss=0.02795
Epoch 3/80: current_loss=0.03189 | best_loss=0.02795
Epoch 4/80: current_loss=0.04012 | best_loss=0.02795
Epoch 5/80: current_loss=0.02965 | best_loss=0.02795
Epoch 6/80: current_loss=0.02898 | best_loss=0.02795
Epoch 7/80: current_loss=0.02652 | best_loss=0.02652
Epoch 8/80: current_loss=0.02781 | best_loss=0.02652
Epoch 9/80: current_loss=0.06173 | best_loss=0.02652
Epoch 10/80: current_loss=0.05539 | best_loss=0.02652
Epoch 11/80: current_loss=0.06678 | best_loss=0.02652
Epoch 12/80: current_loss=0.06213 | best_loss=0.02652
Epoch 13/80: current_loss=0.03409 | best_loss=0.02652
Epoch 14/80: current_loss=0.03077 | best_loss=0.02652
Epoch 15/80: current_loss=0.02661 | best_loss=0.02652
Epoch 16/80: current_loss=0.04687 | best_loss=0.02652
Epoch 17/80: current_loss=0.02956 | best_loss=0.02652
Epoch 18/80: current_loss=0.02822 | best_loss=0.02652
Epoch 19/80: current_loss=0.05661 | best_loss=0.02652
Epoch 20/80: current_loss=0.03688 | best_loss=0.02652
Epoch 21/80: current_loss=0.03502 | best_loss=0.02652
Epoch 22/80: current_loss=0.04428 | best_loss=0.02652
Epoch 23/80: current_loss=0.03243 | best_loss=0.02652
Epoch 24/80: current_loss=0.02804 | best_loss=0.02652
Epoch 25/80: current_loss=0.04646 | best_loss=0.02652
Epoch 26/80: current_loss=0.03150 | best_loss=0.02652
Epoch 27/80: current_loss=0.02748 | best_loss=0.02652
Early Stopping at epoch 27
      explained_var=0.00324 | mse_loss=0.02598
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03350 | best_loss=0.03350
Epoch 1/80: current_loss=0.03316 | best_loss=0.03316
Epoch 2/80: current_loss=0.03374 | best_loss=0.03316
Epoch 3/80: current_loss=0.04860 | best_loss=0.03316
Epoch 4/80: current_loss=0.03825 | best_loss=0.03316
Epoch 5/80: current_loss=0.03232 | best_loss=0.03232
Epoch 6/80: current_loss=0.03485 | best_loss=0.03232
Epoch 7/80: current_loss=0.05668 | best_loss=0.03232
Epoch 8/80: current_loss=0.05421 | best_loss=0.03232
Epoch 9/80: current_loss=0.04590 | best_loss=0.03232
Epoch 10/80: current_loss=0.06990 | best_loss=0.03232
Epoch 11/80: current_loss=0.03475 | best_loss=0.03232
Epoch 12/80: current_loss=0.07352 | best_loss=0.03232
Epoch 13/80: current_loss=0.04842 | best_loss=0.03232
Epoch 14/80: current_loss=0.05793 | best_loss=0.03232
Epoch 15/80: current_loss=0.03061 | best_loss=0.03061
Epoch 16/80: current_loss=0.03017 | best_loss=0.03017
Epoch 17/80: current_loss=0.03771 | best_loss=0.03017
Epoch 18/80: current_loss=0.04155 | best_loss=0.03017
Epoch 19/80: current_loss=0.06175 | best_loss=0.03017
Epoch 20/80: current_loss=0.03638 | best_loss=0.03017
Epoch 21/80: current_loss=0.05073 | best_loss=0.03017
Epoch 22/80: current_loss=0.03642 | best_loss=0.03017
Epoch 23/80: current_loss=0.02914 | best_loss=0.02914
Epoch 24/80: current_loss=0.04977 | best_loss=0.02914
Epoch 25/80: current_loss=0.03422 | best_loss=0.02914
Epoch 26/80: current_loss=0.03438 | best_loss=0.02914
Epoch 27/80: current_loss=0.05353 | best_loss=0.02914
Epoch 28/80: current_loss=0.05090 | best_loss=0.02914
Epoch 29/80: current_loss=0.03022 | best_loss=0.02914
Epoch 30/80: current_loss=0.05601 | best_loss=0.02914
Epoch 31/80: current_loss=0.04711 | best_loss=0.02914
Epoch 32/80: current_loss=0.03112 | best_loss=0.02914
Epoch 33/80: current_loss=0.04647 | best_loss=0.02914
Epoch 34/80: current_loss=0.10945 | best_loss=0.02914
Epoch 35/80: current_loss=0.03761 | best_loss=0.02914
Epoch 36/80: current_loss=0.04581 | best_loss=0.02914
Epoch 37/80: current_loss=0.02866 | best_loss=0.02866
Epoch 38/80: current_loss=0.03464 | best_loss=0.02866
Epoch 39/80: current_loss=0.04214 | best_loss=0.02866
Epoch 40/80: current_loss=0.04996 | best_loss=0.02866
Epoch 41/80: current_loss=0.03160 | best_loss=0.02866
Epoch 42/80: current_loss=0.03013 | best_loss=0.02866
Epoch 43/80: current_loss=0.03228 | best_loss=0.02866
Epoch 44/80: current_loss=0.05183 | best_loss=0.02866
Epoch 45/80: current_loss=0.03055 | best_loss=0.02866
Epoch 46/80: current_loss=0.02942 | best_loss=0.02866
Epoch 47/80: current_loss=0.05577 | best_loss=0.02866
Epoch 48/80: current_loss=0.03430 | best_loss=0.02866
Epoch 49/80: current_loss=0.02802 | best_loss=0.02802
Epoch 50/80: current_loss=0.03240 | best_loss=0.02802
Epoch 51/80: current_loss=0.03592 | best_loss=0.02802
Epoch 52/80: current_loss=0.03357 | best_loss=0.02802
Epoch 53/80: current_loss=0.05485 | best_loss=0.02802
Epoch 54/80: current_loss=0.08003 | best_loss=0.02802
Epoch 55/80: current_loss=0.03422 | best_loss=0.02802
Epoch 56/80: current_loss=0.03477 | best_loss=0.02802
Epoch 57/80: current_loss=0.03639 | best_loss=0.02802
Epoch 58/80: current_loss=0.03203 | best_loss=0.02802
Epoch 59/80: current_loss=0.04137 | best_loss=0.02802
Epoch 60/80: current_loss=0.03574 | best_loss=0.02802
Epoch 61/80: current_loss=0.03229 | best_loss=0.02802
Epoch 62/80: current_loss=0.03567 | best_loss=0.02802
Epoch 63/80: current_loss=0.02928 | best_loss=0.02802
Epoch 64/80: current_loss=0.04462 | best_loss=0.02802
Epoch 65/80: current_loss=0.04994 | best_loss=0.02802
Epoch 66/80: current_loss=0.03391 | best_loss=0.02802
Epoch 67/80: current_loss=0.06086 | best_loss=0.02802
Epoch 68/80: current_loss=0.04335 | best_loss=0.02802
Epoch 69/80: current_loss=0.03990 | best_loss=0.02802
Early Stopping at epoch 69
      explained_var=0.00532 | mse_loss=0.02770
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03363 | best_loss=0.03363
Epoch 1/80: current_loss=0.04136 | best_loss=0.03363
Epoch 2/80: current_loss=0.03097 | best_loss=0.03097
Epoch 3/80: current_loss=0.03057 | best_loss=0.03057
Epoch 4/80: current_loss=0.05671 | best_loss=0.03057
Epoch 5/80: current_loss=0.02999 | best_loss=0.02999
Epoch 6/80: current_loss=0.05271 | best_loss=0.02999
Epoch 7/80: current_loss=0.03086 | best_loss=0.02999
Epoch 8/80: current_loss=0.03466 | best_loss=0.02999
Epoch 9/80: current_loss=0.02998 | best_loss=0.02998
Epoch 10/80: current_loss=0.04259 | best_loss=0.02998
Epoch 11/80: current_loss=0.05565 | best_loss=0.02998
Epoch 12/80: current_loss=0.03055 | best_loss=0.02998
Epoch 13/80: current_loss=0.03088 | best_loss=0.02998
Epoch 14/80: current_loss=0.03086 | best_loss=0.02998
Epoch 15/80: current_loss=0.03553 | best_loss=0.02998
Epoch 16/80: current_loss=0.03043 | best_loss=0.02998
Epoch 17/80: current_loss=0.05099 | best_loss=0.02998
Epoch 18/80: current_loss=0.05408 | best_loss=0.02998
Epoch 19/80: current_loss=0.03477 | best_loss=0.02998
Epoch 20/80: current_loss=0.05577 | best_loss=0.02998
Epoch 21/80: current_loss=0.05285 | best_loss=0.02998
Epoch 22/80: current_loss=0.03210 | best_loss=0.02998
Epoch 23/80: current_loss=0.03511 | best_loss=0.02998
Epoch 24/80: current_loss=0.05576 | best_loss=0.02998
Epoch 25/80: current_loss=0.05425 | best_loss=0.02998
Epoch 26/80: current_loss=0.03179 | best_loss=0.02998
Epoch 27/80: current_loss=0.03046 | best_loss=0.02998
Epoch 28/80: current_loss=0.05185 | best_loss=0.02998
Epoch 29/80: current_loss=0.03145 | best_loss=0.02998
Early Stopping at epoch 29
      explained_var=0.01665 | mse_loss=0.02928
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.05578 | best_loss=0.05578
Epoch 1/80: current_loss=0.02797 | best_loss=0.02797
Epoch 2/80: current_loss=0.04731 | best_loss=0.02797
Epoch 3/80: current_loss=0.02809 | best_loss=0.02797
Epoch 4/80: current_loss=0.02931 | best_loss=0.02797
Epoch 5/80: current_loss=0.03009 | best_loss=0.02797
Epoch 6/80: current_loss=0.04009 | best_loss=0.02797
Epoch 7/80: current_loss=0.02827 | best_loss=0.02797
Epoch 8/80: current_loss=0.03684 | best_loss=0.02797
Epoch 9/80: current_loss=0.02871 | best_loss=0.02797
Epoch 10/80: current_loss=0.03536 | best_loss=0.02797
Epoch 11/80: current_loss=0.03558 | best_loss=0.02797
Epoch 12/80: current_loss=0.02965 | best_loss=0.02797
Epoch 13/80: current_loss=0.03520 | best_loss=0.02797
Epoch 14/80: current_loss=0.03046 | best_loss=0.02797
Epoch 15/80: current_loss=0.02827 | best_loss=0.02797
Epoch 16/80: current_loss=0.02816 | best_loss=0.02797
Epoch 17/80: current_loss=0.03285 | best_loss=0.02797
Epoch 18/80: current_loss=0.03949 | best_loss=0.02797
Epoch 19/80: current_loss=0.02901 | best_loss=0.02797
Epoch 20/80: current_loss=0.03177 | best_loss=0.02797
Epoch 21/80: current_loss=0.02853 | best_loss=0.02797
Early Stopping at epoch 21
      explained_var=0.00928 | mse_loss=0.02816
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.08627 | best_loss=0.08627
Epoch 1/80: current_loss=0.03806 | best_loss=0.03806
Epoch 2/80: current_loss=0.04228 | best_loss=0.03806
Epoch 3/80: current_loss=0.03173 | best_loss=0.03173
Epoch 4/80: current_loss=0.03988 | best_loss=0.03173
Epoch 5/80: current_loss=0.06255 | best_loss=0.03173
Epoch 6/80: current_loss=0.03871 | best_loss=0.03173
Epoch 7/80: current_loss=0.03771 | best_loss=0.03173
Epoch 8/80: current_loss=0.03892 | best_loss=0.03173
Epoch 9/80: current_loss=0.04426 | best_loss=0.03173
Epoch 10/80: current_loss=0.04218 | best_loss=0.03173
Epoch 11/80: current_loss=0.03602 | best_loss=0.03173
Epoch 12/80: current_loss=0.04453 | best_loss=0.03173
Epoch 13/80: current_loss=0.05787 | best_loss=0.03173
Epoch 14/80: current_loss=0.08786 | best_loss=0.03173
Epoch 15/80: current_loss=0.03854 | best_loss=0.03173
Epoch 16/80: current_loss=0.03556 | best_loss=0.03173
Epoch 17/80: current_loss=0.04342 | best_loss=0.03173
Epoch 18/80: current_loss=0.03258 | best_loss=0.03173
Epoch 19/80: current_loss=0.07383 | best_loss=0.03173
Epoch 20/80: current_loss=0.03218 | best_loss=0.03173
Epoch 21/80: current_loss=0.03451 | best_loss=0.03173
Epoch 22/80: current_loss=0.04137 | best_loss=0.03173
Epoch 23/80: current_loss=0.03670 | best_loss=0.03173
Early Stopping at epoch 23
      explained_var=0.00033 | mse_loss=0.03252
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.00696| avg_loss=0.02873
----------------------------------------------


----------------------------------------------
Params for Trial 14
{'learning_rate': 0.001, 'weight_decay': 0.0024080794509283073, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02791 | best_loss=0.02791
Epoch 1/80: current_loss=0.03277 | best_loss=0.02791
Epoch 2/80: current_loss=0.03969 | best_loss=0.02791
Epoch 3/80: current_loss=0.03559 | best_loss=0.02791
Epoch 4/80: current_loss=0.02641 | best_loss=0.02641
Epoch 5/80: current_loss=0.02886 | best_loss=0.02641
Epoch 6/80: current_loss=0.02483 | best_loss=0.02483
Epoch 7/80: current_loss=0.02547 | best_loss=0.02483
Epoch 8/80: current_loss=0.02761 | best_loss=0.02483
Epoch 9/80: current_loss=0.02665 | best_loss=0.02483
Epoch 10/80: current_loss=0.02724 | best_loss=0.02483
Epoch 11/80: current_loss=0.02655 | best_loss=0.02483
Epoch 12/80: current_loss=0.03181 | best_loss=0.02483
Epoch 13/80: current_loss=0.02680 | best_loss=0.02483
Epoch 14/80: current_loss=0.02642 | best_loss=0.02483
Epoch 15/80: current_loss=0.02584 | best_loss=0.02483
Epoch 16/80: current_loss=0.02658 | best_loss=0.02483
Epoch 17/80: current_loss=0.02663 | best_loss=0.02483
Epoch 18/80: current_loss=0.02508 | best_loss=0.02483
Epoch 19/80: current_loss=0.02717 | best_loss=0.02483
Epoch 20/80: current_loss=0.02614 | best_loss=0.02483
Epoch 21/80: current_loss=0.02857 | best_loss=0.02483
Epoch 22/80: current_loss=0.02639 | best_loss=0.02483
Epoch 23/80: current_loss=0.02842 | best_loss=0.02483
Epoch 24/80: current_loss=0.02571 | best_loss=0.02483
Epoch 25/80: current_loss=0.02736 | best_loss=0.02483
Epoch 26/80: current_loss=0.02576 | best_loss=0.02483
Early Stopping at epoch 26
      explained_var=0.07036 | mse_loss=0.02421
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03981 | best_loss=0.03981
Epoch 1/80: current_loss=0.03280 | best_loss=0.03280
Epoch 2/80: current_loss=0.03202 | best_loss=0.03202
Epoch 3/80: current_loss=0.02876 | best_loss=0.02876
Epoch 4/80: current_loss=0.03167 | best_loss=0.02876
Epoch 5/80: current_loss=0.02877 | best_loss=0.02876
Epoch 6/80: current_loss=0.02808 | best_loss=0.02808
Epoch 7/80: current_loss=0.02992 | best_loss=0.02808
Epoch 8/80: current_loss=0.03113 | best_loss=0.02808
Epoch 9/80: current_loss=0.02805 | best_loss=0.02805
Epoch 10/80: current_loss=0.02815 | best_loss=0.02805
Epoch 11/80: current_loss=0.03154 | best_loss=0.02805
Epoch 12/80: current_loss=0.02973 | best_loss=0.02805
Epoch 13/80: current_loss=0.03621 | best_loss=0.02805
Epoch 14/80: current_loss=0.03188 | best_loss=0.02805
Epoch 15/80: current_loss=0.02790 | best_loss=0.02790
Epoch 16/80: current_loss=0.03012 | best_loss=0.02790
Epoch 17/80: current_loss=0.02920 | best_loss=0.02790
Epoch 18/80: current_loss=0.02944 | best_loss=0.02790
Epoch 19/80: current_loss=0.03394 | best_loss=0.02790
Epoch 20/80: current_loss=0.03236 | best_loss=0.02790
Epoch 21/80: current_loss=0.03086 | best_loss=0.02790
Epoch 22/80: current_loss=0.02957 | best_loss=0.02790
Epoch 23/80: current_loss=0.02854 | best_loss=0.02790
Epoch 24/80: current_loss=0.02831 | best_loss=0.02790
Epoch 25/80: current_loss=0.03060 | best_loss=0.02790
Epoch 26/80: current_loss=0.02966 | best_loss=0.02790
Epoch 27/80: current_loss=0.02911 | best_loss=0.02790
Epoch 28/80: current_loss=0.02859 | best_loss=0.02790
Epoch 29/80: current_loss=0.03131 | best_loss=0.02790
Epoch 30/80: current_loss=0.02802 | best_loss=0.02790
Epoch 31/80: current_loss=0.02799 | best_loss=0.02790
Epoch 32/80: current_loss=0.02819 | best_loss=0.02790
Epoch 33/80: current_loss=0.02984 | best_loss=0.02790
Epoch 34/80: current_loss=0.02991 | best_loss=0.02790
Epoch 35/80: current_loss=0.02853 | best_loss=0.02790
Early Stopping at epoch 35
      explained_var=0.01614 | mse_loss=0.02740
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03121 | best_loss=0.03121
Epoch 1/80: current_loss=0.03557 | best_loss=0.03121
Epoch 2/80: current_loss=0.03241 | best_loss=0.03121
Epoch 3/80: current_loss=0.03944 | best_loss=0.03121
Epoch 4/80: current_loss=0.03400 | best_loss=0.03121
Epoch 5/80: current_loss=0.03126 | best_loss=0.03121
Epoch 6/80: current_loss=0.03198 | best_loss=0.03121
Epoch 7/80: current_loss=0.03241 | best_loss=0.03121
Epoch 8/80: current_loss=0.03251 | best_loss=0.03121
Epoch 9/80: current_loss=0.03120 | best_loss=0.03120
Epoch 10/80: current_loss=0.03071 | best_loss=0.03071
Epoch 11/80: current_loss=0.03029 | best_loss=0.03029
Epoch 12/80: current_loss=0.03096 | best_loss=0.03029
Epoch 13/80: current_loss=0.03097 | best_loss=0.03029
Epoch 14/80: current_loss=0.03030 | best_loss=0.03029
Epoch 15/80: current_loss=0.03024 | best_loss=0.03024
Epoch 16/80: current_loss=0.03035 | best_loss=0.03024
Epoch 17/80: current_loss=0.03305 | best_loss=0.03024
Epoch 18/80: current_loss=0.03533 | best_loss=0.03024
Epoch 19/80: current_loss=0.03090 | best_loss=0.03024
Epoch 20/80: current_loss=0.03149 | best_loss=0.03024
Epoch 21/80: current_loss=0.03125 | best_loss=0.03024
Epoch 22/80: current_loss=0.03229 | best_loss=0.03024
Epoch 23/80: current_loss=0.03012 | best_loss=0.03012
Epoch 24/80: current_loss=0.03077 | best_loss=0.03012
Epoch 25/80: current_loss=0.03093 | best_loss=0.03012
Epoch 26/80: current_loss=0.03034 | best_loss=0.03012
Epoch 27/80: current_loss=0.03313 | best_loss=0.03012
Epoch 28/80: current_loss=0.03309 | best_loss=0.03012
Epoch 29/80: current_loss=0.03845 | best_loss=0.03012
Epoch 30/80: current_loss=0.03158 | best_loss=0.03012
Epoch 31/80: current_loss=0.03048 | best_loss=0.03012
Epoch 32/80: current_loss=0.03062 | best_loss=0.03012
Epoch 33/80: current_loss=0.03141 | best_loss=0.03012
Epoch 34/80: current_loss=0.03074 | best_loss=0.03012
Epoch 35/80: current_loss=0.03297 | best_loss=0.03012
Epoch 36/80: current_loss=0.03037 | best_loss=0.03012
Epoch 37/80: current_loss=0.03042 | best_loss=0.03012
Epoch 38/80: current_loss=0.03027 | best_loss=0.03012
Epoch 39/80: current_loss=0.03090 | best_loss=0.03012
Epoch 40/80: current_loss=0.04020 | best_loss=0.03012
Epoch 41/80: current_loss=0.03437 | best_loss=0.03012
Epoch 42/80: current_loss=0.03181 | best_loss=0.03012
Epoch 43/80: current_loss=0.03159 | best_loss=0.03012
Early Stopping at epoch 43
      explained_var=0.01100 | mse_loss=0.02934
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02864 | best_loss=0.02864
Epoch 1/80: current_loss=0.02847 | best_loss=0.02847
Epoch 2/80: current_loss=0.02838 | best_loss=0.02838
Epoch 3/80: current_loss=0.03086 | best_loss=0.02838
Epoch 4/80: current_loss=0.02830 | best_loss=0.02830
Epoch 5/80: current_loss=0.03048 | best_loss=0.02830
Epoch 6/80: current_loss=0.02827 | best_loss=0.02827
Epoch 7/80: current_loss=0.02964 | best_loss=0.02827
Epoch 8/80: current_loss=0.02876 | best_loss=0.02827
Epoch 9/80: current_loss=0.02814 | best_loss=0.02814
Epoch 10/80: current_loss=0.02801 | best_loss=0.02801
Epoch 11/80: current_loss=0.02882 | best_loss=0.02801
Epoch 12/80: current_loss=0.03017 | best_loss=0.02801
Epoch 13/80: current_loss=0.02819 | best_loss=0.02801
Epoch 14/80: current_loss=0.02892 | best_loss=0.02801
Epoch 15/80: current_loss=0.02811 | best_loss=0.02801
Epoch 16/80: current_loss=0.03224 | best_loss=0.02801
Epoch 17/80: current_loss=0.02906 | best_loss=0.02801
Epoch 18/80: current_loss=0.02878 | best_loss=0.02801
Epoch 19/80: current_loss=0.02988 | best_loss=0.02801
Epoch 20/80: current_loss=0.02967 | best_loss=0.02801
Epoch 21/80: current_loss=0.02841 | best_loss=0.02801
Epoch 22/80: current_loss=0.03057 | best_loss=0.02801
Epoch 23/80: current_loss=0.02812 | best_loss=0.02801
Epoch 24/80: current_loss=0.02825 | best_loss=0.02801
Epoch 25/80: current_loss=0.02892 | best_loss=0.02801
Epoch 26/80: current_loss=0.02817 | best_loss=0.02801
Epoch 27/80: current_loss=0.02838 | best_loss=0.02801
Epoch 28/80: current_loss=0.02854 | best_loss=0.02801
Epoch 29/80: current_loss=0.02836 | best_loss=0.02801
Epoch 30/80: current_loss=0.02842 | best_loss=0.02801
Early Stopping at epoch 30
      explained_var=0.00013 | mse_loss=0.02836
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03180 | best_loss=0.03180
Epoch 1/80: current_loss=0.03194 | best_loss=0.03180
Epoch 2/80: current_loss=0.03255 | best_loss=0.03180
Epoch 3/80: current_loss=0.03282 | best_loss=0.03180
Epoch 4/80: current_loss=0.03216 | best_loss=0.03180
Epoch 5/80: current_loss=0.03274 | best_loss=0.03180
Epoch 6/80: current_loss=0.03330 | best_loss=0.03180
Epoch 7/80: current_loss=0.03263 | best_loss=0.03180
Epoch 8/80: current_loss=0.03595 | best_loss=0.03180
Epoch 9/80: current_loss=0.03412 | best_loss=0.03180
Epoch 10/80: current_loss=0.03314 | best_loss=0.03180
Epoch 11/80: current_loss=0.03266 | best_loss=0.03180
Epoch 12/80: current_loss=0.03306 | best_loss=0.03180
Epoch 13/80: current_loss=0.03248 | best_loss=0.03180
Epoch 14/80: current_loss=0.03339 | best_loss=0.03180
Epoch 15/80: current_loss=0.03263 | best_loss=0.03180
Epoch 16/80: current_loss=0.03283 | best_loss=0.03180
Epoch 17/80: current_loss=0.03273 | best_loss=0.03180
Epoch 18/80: current_loss=0.03284 | best_loss=0.03180
Epoch 19/80: current_loss=0.03375 | best_loss=0.03180
Epoch 20/80: current_loss=0.03402 | best_loss=0.03180
Early Stopping at epoch 20
      explained_var=0.00194 | mse_loss=0.03251
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.01991| avg_loss=0.02836
----------------------------------------------


----------------------------------------------
Params for Trial 15
{'learning_rate': 0.001, 'weight_decay': 0.002974208225832657, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03563 | best_loss=0.03563
Epoch 1/80: current_loss=0.02632 | best_loss=0.02632
Epoch 2/80: current_loss=0.02617 | best_loss=0.02617
Epoch 3/80: current_loss=0.02872 | best_loss=0.02617
Epoch 4/80: current_loss=0.02576 | best_loss=0.02576
Epoch 5/80: current_loss=0.02634 | best_loss=0.02576
Epoch 6/80: current_loss=0.03169 | best_loss=0.02576
Epoch 7/80: current_loss=0.02745 | best_loss=0.02576
Epoch 8/80: current_loss=0.02630 | best_loss=0.02576
Epoch 9/80: current_loss=0.02809 | best_loss=0.02576
Epoch 10/80: current_loss=0.02607 | best_loss=0.02576
Epoch 11/80: current_loss=0.02628 | best_loss=0.02576
Epoch 12/80: current_loss=0.02631 | best_loss=0.02576
Epoch 13/80: current_loss=0.02615 | best_loss=0.02576
Epoch 14/80: current_loss=0.02938 | best_loss=0.02576
Epoch 15/80: current_loss=0.02602 | best_loss=0.02576
Epoch 16/80: current_loss=0.02888 | best_loss=0.02576
Epoch 17/80: current_loss=0.02853 | best_loss=0.02576
Epoch 18/80: current_loss=0.02603 | best_loss=0.02576
Epoch 19/80: current_loss=0.02789 | best_loss=0.02576
Epoch 20/80: current_loss=0.02865 | best_loss=0.02576
Epoch 21/80: current_loss=0.02644 | best_loss=0.02576
Epoch 22/80: current_loss=0.02583 | best_loss=0.02576
Epoch 23/80: current_loss=0.02886 | best_loss=0.02576
Epoch 24/80: current_loss=0.02853 | best_loss=0.02576
Early Stopping at epoch 24
      explained_var=0.02890 | mse_loss=0.02538
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03009 | best_loss=0.03009
Epoch 1/80: current_loss=0.02925 | best_loss=0.02925
Epoch 2/80: current_loss=0.02888 | best_loss=0.02888
Epoch 3/80: current_loss=0.03282 | best_loss=0.02888
Epoch 4/80: current_loss=0.02912 | best_loss=0.02888
Epoch 5/80: current_loss=0.02965 | best_loss=0.02888
Epoch 6/80: current_loss=0.02815 | best_loss=0.02815
Epoch 7/80: current_loss=0.02814 | best_loss=0.02814
Epoch 8/80: current_loss=0.02916 | best_loss=0.02814
Epoch 9/80: current_loss=0.02851 | best_loss=0.02814
Epoch 10/80: current_loss=0.02810 | best_loss=0.02810
Epoch 11/80: current_loss=0.02894 | best_loss=0.02810
Epoch 12/80: current_loss=0.02813 | best_loss=0.02810
Epoch 13/80: current_loss=0.02842 | best_loss=0.02810
Epoch 14/80: current_loss=0.03009 | best_loss=0.02810
Epoch 15/80: current_loss=0.02899 | best_loss=0.02810
Epoch 16/80: current_loss=0.02836 | best_loss=0.02810
Epoch 17/80: current_loss=0.02823 | best_loss=0.02810
Epoch 18/80: current_loss=0.02848 | best_loss=0.02810
Epoch 19/80: current_loss=0.02822 | best_loss=0.02810
Epoch 20/80: current_loss=0.02823 | best_loss=0.02810
Epoch 21/80: current_loss=0.02874 | best_loss=0.02810
Epoch 22/80: current_loss=0.03074 | best_loss=0.02810
Epoch 23/80: current_loss=0.02884 | best_loss=0.02810
Epoch 24/80: current_loss=0.03048 | best_loss=0.02810
Epoch 25/80: current_loss=0.02823 | best_loss=0.02810
Epoch 26/80: current_loss=0.02825 | best_loss=0.02810
Epoch 27/80: current_loss=0.02833 | best_loss=0.02810
Epoch 28/80: current_loss=0.02883 | best_loss=0.02810
Epoch 29/80: current_loss=0.02824 | best_loss=0.02810
Epoch 30/80: current_loss=0.02866 | best_loss=0.02810
Early Stopping at epoch 30
      explained_var=0.00668 | mse_loss=0.02760
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03055 | best_loss=0.03055
Epoch 1/80: current_loss=0.03308 | best_loss=0.03055
Epoch 2/80: current_loss=0.03055 | best_loss=0.03055
Epoch 3/80: current_loss=0.03320 | best_loss=0.03055
Epoch 4/80: current_loss=0.03076 | best_loss=0.03055
Epoch 5/80: current_loss=0.03426 | best_loss=0.03055
Epoch 6/80: current_loss=0.03101 | best_loss=0.03055
Epoch 7/80: current_loss=0.03555 | best_loss=0.03055
Epoch 8/80: current_loss=0.03045 | best_loss=0.03045
Epoch 9/80: current_loss=0.03272 | best_loss=0.03045
Epoch 10/80: current_loss=0.03114 | best_loss=0.03045
Epoch 11/80: current_loss=0.03157 | best_loss=0.03045
Epoch 12/80: current_loss=0.03105 | best_loss=0.03045
Epoch 13/80: current_loss=0.03117 | best_loss=0.03045
Epoch 14/80: current_loss=0.03229 | best_loss=0.03045
Epoch 15/80: current_loss=0.03086 | best_loss=0.03045
Epoch 16/80: current_loss=0.03137 | best_loss=0.03045
Epoch 17/80: current_loss=0.03219 | best_loss=0.03045
Epoch 18/80: current_loss=0.03068 | best_loss=0.03045
Epoch 19/80: current_loss=0.03092 | best_loss=0.03045
Epoch 20/80: current_loss=0.03080 | best_loss=0.03045
Epoch 21/80: current_loss=0.03348 | best_loss=0.03045
Epoch 22/80: current_loss=0.03172 | best_loss=0.03045
Epoch 23/80: current_loss=0.03094 | best_loss=0.03045
Epoch 24/80: current_loss=0.03139 | best_loss=0.03045
Epoch 25/80: current_loss=0.03054 | best_loss=0.03045
Epoch 26/80: current_loss=0.03074 | best_loss=0.03045
Epoch 27/80: current_loss=0.03305 | best_loss=0.03045
Epoch 28/80: current_loss=0.03174 | best_loss=0.03045
Early Stopping at epoch 28
      explained_var=-0.00178 | mse_loss=0.02966
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02852 | best_loss=0.02852
Epoch 1/80: current_loss=0.02913 | best_loss=0.02852
Epoch 2/80: current_loss=0.02909 | best_loss=0.02852
Epoch 3/80: current_loss=0.02801 | best_loss=0.02801
Epoch 4/80: current_loss=0.02810 | best_loss=0.02801
Epoch 5/80: current_loss=0.02819 | best_loss=0.02801
Epoch 6/80: current_loss=0.02820 | best_loss=0.02801
Epoch 7/80: current_loss=0.02827 | best_loss=0.02801
Epoch 8/80: current_loss=0.02799 | best_loss=0.02799
Epoch 9/80: current_loss=0.02837 | best_loss=0.02799
Epoch 10/80: current_loss=0.02798 | best_loss=0.02798
Epoch 11/80: current_loss=0.02800 | best_loss=0.02798
Epoch 12/80: current_loss=0.02798 | best_loss=0.02798
Epoch 13/80: current_loss=0.02844 | best_loss=0.02798
Epoch 14/80: current_loss=0.02824 | best_loss=0.02798
Epoch 15/80: current_loss=0.02811 | best_loss=0.02798
Epoch 16/80: current_loss=0.02817 | best_loss=0.02798
Epoch 17/80: current_loss=0.02808 | best_loss=0.02798
Epoch 18/80: current_loss=0.02807 | best_loss=0.02798
Epoch 19/80: current_loss=0.02801 | best_loss=0.02798
Epoch 20/80: current_loss=0.02809 | best_loss=0.02798
Epoch 21/80: current_loss=0.02816 | best_loss=0.02798
Epoch 22/80: current_loss=0.02800 | best_loss=0.02798
Epoch 23/80: current_loss=0.02805 | best_loss=0.02798
Epoch 24/80: current_loss=0.02840 | best_loss=0.02798
Epoch 25/80: current_loss=0.02902 | best_loss=0.02798
Epoch 26/80: current_loss=0.02982 | best_loss=0.02798
Epoch 27/80: current_loss=0.02851 | best_loss=0.02798
Epoch 28/80: current_loss=0.02802 | best_loss=0.02798
Epoch 29/80: current_loss=0.02808 | best_loss=0.02798
Epoch 30/80: current_loss=0.02804 | best_loss=0.02798
Epoch 31/80: current_loss=0.02809 | best_loss=0.02798
Epoch 32/80: current_loss=0.02909 | best_loss=0.02798
Early Stopping at epoch 32
      explained_var=0.00144 | mse_loss=0.02833
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03187 | best_loss=0.03187
Epoch 1/80: current_loss=0.03187 | best_loss=0.03187
Epoch 2/80: current_loss=0.03178 | best_loss=0.03178
Epoch 3/80: current_loss=0.03182 | best_loss=0.03178
Epoch 4/80: current_loss=0.03174 | best_loss=0.03174
Epoch 5/80: current_loss=0.03175 | best_loss=0.03174
Epoch 6/80: current_loss=0.03175 | best_loss=0.03174
Epoch 7/80: current_loss=0.03193 | best_loss=0.03174
Epoch 8/80: current_loss=0.03175 | best_loss=0.03174
Epoch 9/80: current_loss=0.03179 | best_loss=0.03174
Epoch 10/80: current_loss=0.03205 | best_loss=0.03174
Epoch 11/80: current_loss=0.03176 | best_loss=0.03174
Epoch 12/80: current_loss=0.03175 | best_loss=0.03174
Epoch 13/80: current_loss=0.03175 | best_loss=0.03174
Epoch 14/80: current_loss=0.03225 | best_loss=0.03174
Epoch 15/80: current_loss=0.03172 | best_loss=0.03172
Epoch 16/80: current_loss=0.03172 | best_loss=0.03172
Epoch 17/80: current_loss=0.03174 | best_loss=0.03172
Epoch 18/80: current_loss=0.03172 | best_loss=0.03172
Epoch 19/80: current_loss=0.03199 | best_loss=0.03172
Epoch 20/80: current_loss=0.03182 | best_loss=0.03172
Epoch 21/80: current_loss=0.03177 | best_loss=0.03172
Epoch 22/80: current_loss=0.03173 | best_loss=0.03172
Epoch 23/80: current_loss=0.03179 | best_loss=0.03172
Epoch 24/80: current_loss=0.03175 | best_loss=0.03172
Epoch 25/80: current_loss=0.03175 | best_loss=0.03172
Epoch 26/80: current_loss=0.03207 | best_loss=0.03172
Epoch 27/80: current_loss=0.03174 | best_loss=0.03172
Epoch 28/80: current_loss=0.03185 | best_loss=0.03172
Epoch 29/80: current_loss=0.03176 | best_loss=0.03172
Epoch 30/80: current_loss=0.03172 | best_loss=0.03172
Epoch 31/80: current_loss=0.03177 | best_loss=0.03172
Epoch 32/80: current_loss=0.03185 | best_loss=0.03172
Epoch 33/80: current_loss=0.03177 | best_loss=0.03172
Epoch 34/80: current_loss=0.03183 | best_loss=0.03172
Epoch 35/80: current_loss=0.03173 | best_loss=0.03172
Epoch 36/80: current_loss=0.03185 | best_loss=0.03172
Epoch 37/80: current_loss=0.03177 | best_loss=0.03172
Epoch 38/80: current_loss=0.03177 | best_loss=0.03172
Epoch 39/80: current_loss=0.03181 | best_loss=0.03172
Epoch 40/80: current_loss=0.03177 | best_loss=0.03172
Epoch 41/80: current_loss=0.03179 | best_loss=0.03172
Epoch 42/80: current_loss=0.03188 | best_loss=0.03172
Epoch 43/80: current_loss=0.03178 | best_loss=0.03172
Epoch 44/80: current_loss=0.03177 | best_loss=0.03172
Epoch 45/80: current_loss=0.03190 | best_loss=0.03172
Epoch 46/80: current_loss=0.03175 | best_loss=0.03172
Epoch 47/80: current_loss=0.03175 | best_loss=0.03172
Epoch 48/80: current_loss=0.03227 | best_loss=0.03172
Epoch 49/80: current_loss=0.03180 | best_loss=0.03172
Epoch 50/80: current_loss=0.03173 | best_loss=0.03172
Early Stopping at epoch 50
      explained_var=0.00094 | mse_loss=0.03246
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=0.00724| avg_loss=0.02869
----------------------------------------------


----------------------------------------------
Params for Trial 16
{'learning_rate': 0.001, 'weight_decay': 0.0019441972553043121, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03881 | best_loss=0.03881
Epoch 1/80: current_loss=0.04285 | best_loss=0.03881
Epoch 2/80: current_loss=0.02627 | best_loss=0.02627
Epoch 3/80: current_loss=0.02988 | best_loss=0.02627
Epoch 4/80: current_loss=0.02644 | best_loss=0.02627
Epoch 5/80: current_loss=0.02455 | best_loss=0.02455
Epoch 6/80: current_loss=0.02638 | best_loss=0.02455
Epoch 7/80: current_loss=0.02660 | best_loss=0.02455
Epoch 8/80: current_loss=0.02739 | best_loss=0.02455
Epoch 9/80: current_loss=0.02552 | best_loss=0.02455
Epoch 10/80: current_loss=0.02576 | best_loss=0.02455
Epoch 11/80: current_loss=0.02598 | best_loss=0.02455
Epoch 12/80: current_loss=0.02569 | best_loss=0.02455
Epoch 13/80: current_loss=0.02669 | best_loss=0.02455
Epoch 14/80: current_loss=0.02707 | best_loss=0.02455
Epoch 15/80: current_loss=0.02709 | best_loss=0.02455
Epoch 16/80: current_loss=0.02568 | best_loss=0.02455
Epoch 17/80: current_loss=0.02663 | best_loss=0.02455
Epoch 18/80: current_loss=0.02695 | best_loss=0.02455
Epoch 19/80: current_loss=0.03215 | best_loss=0.02455
Epoch 20/80: current_loss=0.02884 | best_loss=0.02455
Epoch 21/80: current_loss=0.02550 | best_loss=0.02455
Epoch 22/80: current_loss=0.02588 | best_loss=0.02455
Epoch 23/80: current_loss=0.02839 | best_loss=0.02455
Epoch 24/80: current_loss=0.02541 | best_loss=0.02455
Epoch 25/80: current_loss=0.03035 | best_loss=0.02455
Early Stopping at epoch 25
      explained_var=0.07219 | mse_loss=0.02406
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03670 | best_loss=0.03670
Epoch 1/80: current_loss=0.03309 | best_loss=0.03309
Epoch 2/80: current_loss=0.02749 | best_loss=0.02749
Epoch 3/80: current_loss=0.03949 | best_loss=0.02749
Epoch 4/80: current_loss=0.02822 | best_loss=0.02749
Epoch 5/80: current_loss=0.02828 | best_loss=0.02749
Epoch 6/80: current_loss=0.02821 | best_loss=0.02749
Epoch 7/80: current_loss=0.02826 | best_loss=0.02749
Epoch 8/80: current_loss=0.03102 | best_loss=0.02749
Epoch 9/80: current_loss=0.03217 | best_loss=0.02749
Epoch 10/80: current_loss=0.02923 | best_loss=0.02749
Epoch 11/80: current_loss=0.02802 | best_loss=0.02749
Epoch 12/80: current_loss=0.03395 | best_loss=0.02749
Epoch 13/80: current_loss=0.02923 | best_loss=0.02749
Epoch 14/80: current_loss=0.02838 | best_loss=0.02749
Epoch 15/80: current_loss=0.02942 | best_loss=0.02749
Epoch 16/80: current_loss=0.02998 | best_loss=0.02749
Epoch 17/80: current_loss=0.02908 | best_loss=0.02749
Epoch 18/80: current_loss=0.02949 | best_loss=0.02749
Epoch 19/80: current_loss=0.03079 | best_loss=0.02749
Epoch 20/80: current_loss=0.02920 | best_loss=0.02749
Epoch 21/80: current_loss=0.02882 | best_loss=0.02749
Epoch 22/80: current_loss=0.02986 | best_loss=0.02749
Early Stopping at epoch 22
      explained_var=0.02929 | mse_loss=0.02699
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02969 | best_loss=0.02969
Epoch 1/80: current_loss=0.03031 | best_loss=0.02969
Epoch 2/80: current_loss=0.03006 | best_loss=0.02969
Epoch 3/80: current_loss=0.03032 | best_loss=0.02969
Epoch 4/80: current_loss=0.03027 | best_loss=0.02969
Epoch 5/80: current_loss=0.04140 | best_loss=0.02969
Epoch 6/80: current_loss=0.03035 | best_loss=0.02969
Epoch 7/80: current_loss=0.03112 | best_loss=0.02969
Epoch 8/80: current_loss=0.03815 | best_loss=0.02969
Epoch 9/80: current_loss=0.03163 | best_loss=0.02969
Epoch 10/80: current_loss=0.03293 | best_loss=0.02969
Epoch 11/80: current_loss=0.03185 | best_loss=0.02969
Epoch 12/80: current_loss=0.03896 | best_loss=0.02969
Epoch 13/80: current_loss=0.03260 | best_loss=0.02969
Epoch 14/80: current_loss=0.03036 | best_loss=0.02969
Epoch 15/80: current_loss=0.03096 | best_loss=0.02969
Epoch 16/80: current_loss=0.03065 | best_loss=0.02969
Epoch 17/80: current_loss=0.03224 | best_loss=0.02969
Epoch 18/80: current_loss=0.03313 | best_loss=0.02969
Epoch 19/80: current_loss=0.03097 | best_loss=0.02969
Epoch 20/80: current_loss=0.03164 | best_loss=0.02969
Early Stopping at epoch 20
      explained_var=0.02392 | mse_loss=0.02893
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02858 | best_loss=0.02858
Epoch 1/80: current_loss=0.02996 | best_loss=0.02858
Epoch 2/80: current_loss=0.02996 | best_loss=0.02858
Epoch 3/80: current_loss=0.02812 | best_loss=0.02812
Epoch 4/80: current_loss=0.02780 | best_loss=0.02780
Epoch 5/80: current_loss=0.02875 | best_loss=0.02780
Epoch 6/80: current_loss=0.02926 | best_loss=0.02780
Epoch 7/80: current_loss=0.02776 | best_loss=0.02776
Epoch 8/80: current_loss=0.02822 | best_loss=0.02776
Epoch 9/80: current_loss=0.02852 | best_loss=0.02776
Epoch 10/80: current_loss=0.02807 | best_loss=0.02776
Epoch 11/80: current_loss=0.02877 | best_loss=0.02776
Epoch 12/80: current_loss=0.02785 | best_loss=0.02776
Epoch 13/80: current_loss=0.02888 | best_loss=0.02776
Epoch 14/80: current_loss=0.02926 | best_loss=0.02776
Epoch 15/80: current_loss=0.02815 | best_loss=0.02776
Epoch 16/80: current_loss=0.02845 | best_loss=0.02776
Epoch 17/80: current_loss=0.02904 | best_loss=0.02776
Epoch 18/80: current_loss=0.02877 | best_loss=0.02776
Epoch 19/80: current_loss=0.02886 | best_loss=0.02776
Epoch 20/80: current_loss=0.02827 | best_loss=0.02776
Epoch 21/80: current_loss=0.04442 | best_loss=0.02776
Epoch 22/80: current_loss=0.10321 | best_loss=0.02776
Epoch 23/80: current_loss=0.08356 | best_loss=0.02776
Epoch 24/80: current_loss=0.03825 | best_loss=0.02776
Epoch 25/80: current_loss=0.02848 | best_loss=0.02776
Epoch 26/80: current_loss=0.02891 | best_loss=0.02776
Epoch 27/80: current_loss=0.02926 | best_loss=0.02776
Early Stopping at epoch 27
      explained_var=0.00792 | mse_loss=0.02814
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04027 | best_loss=0.04027
Epoch 1/80: current_loss=0.03585 | best_loss=0.03585
Epoch 2/80: current_loss=0.05766 | best_loss=0.03585
Epoch 3/80: current_loss=0.03387 | best_loss=0.03387
Epoch 4/80: current_loss=0.04480 | best_loss=0.03387
Epoch 5/80: current_loss=0.03264 | best_loss=0.03264
Epoch 6/80: current_loss=0.03229 | best_loss=0.03229
Epoch 7/80: current_loss=0.03324 | best_loss=0.03229
Epoch 8/80: current_loss=0.04263 | best_loss=0.03229
Epoch 9/80: current_loss=0.04514 | best_loss=0.03229
Epoch 10/80: current_loss=0.04512 | best_loss=0.03229
Epoch 11/80: current_loss=0.04814 | best_loss=0.03229
Epoch 12/80: current_loss=0.03494 | best_loss=0.03229
Epoch 13/80: current_loss=0.03220 | best_loss=0.03220
Epoch 14/80: current_loss=0.03257 | best_loss=0.03220
Epoch 15/80: current_loss=0.03353 | best_loss=0.03220
Epoch 16/80: current_loss=0.03342 | best_loss=0.03220
Epoch 17/80: current_loss=0.03232 | best_loss=0.03220
Epoch 18/80: current_loss=0.03233 | best_loss=0.03220
Epoch 19/80: current_loss=0.03269 | best_loss=0.03220
Epoch 20/80: current_loss=0.03270 | best_loss=0.03220
Epoch 21/80: current_loss=0.03236 | best_loss=0.03220
Epoch 22/80: current_loss=0.03240 | best_loss=0.03220
Epoch 23/80: current_loss=0.03583 | best_loss=0.03220
Epoch 24/80: current_loss=0.03258 | best_loss=0.03220
Epoch 25/80: current_loss=0.03279 | best_loss=0.03220
Epoch 26/80: current_loss=0.03233 | best_loss=0.03220
Epoch 27/80: current_loss=0.03312 | best_loss=0.03220
Epoch 28/80: current_loss=0.03233 | best_loss=0.03220
Epoch 29/80: current_loss=0.03293 | best_loss=0.03220
Epoch 30/80: current_loss=0.03470 | best_loss=0.03220
Epoch 31/80: current_loss=0.03287 | best_loss=0.03220
Epoch 32/80: current_loss=0.03245 | best_loss=0.03220
Epoch 33/80: current_loss=0.03230 | best_loss=0.03220
Early Stopping at epoch 33
      explained_var=-0.00954 | mse_loss=0.03288
----------------------------------------------
Average early_stopping_point: 5| avg_exp_var=0.02476| avg_loss=0.02820
----------------------------------------------


----------------------------------------------
Params for Trial 17
{'learning_rate': 0.001, 'weight_decay': 0.001476922228135342, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03665 | best_loss=0.03665
Epoch 1/80: current_loss=0.02749 | best_loss=0.02749
Epoch 2/80: current_loss=0.02817 | best_loss=0.02749
Epoch 3/80: current_loss=0.02865 | best_loss=0.02749
Epoch 4/80: current_loss=0.02594 | best_loss=0.02594
Epoch 5/80: current_loss=0.02662 | best_loss=0.02594
Epoch 6/80: current_loss=0.02645 | best_loss=0.02594
Epoch 7/80: current_loss=0.02570 | best_loss=0.02570
Epoch 8/80: current_loss=0.02625 | best_loss=0.02570
Epoch 9/80: current_loss=0.03064 | best_loss=0.02570
Epoch 10/80: current_loss=0.02572 | best_loss=0.02570
Epoch 11/80: current_loss=0.02578 | best_loss=0.02570
Epoch 12/80: current_loss=0.02529 | best_loss=0.02529
Epoch 13/80: current_loss=0.03532 | best_loss=0.02529
Epoch 14/80: current_loss=0.02595 | best_loss=0.02529
Epoch 15/80: current_loss=0.02751 | best_loss=0.02529
Epoch 16/80: current_loss=0.02695 | best_loss=0.02529
Epoch 17/80: current_loss=0.02573 | best_loss=0.02529
Epoch 18/80: current_loss=0.02649 | best_loss=0.02529
Epoch 19/80: current_loss=0.02579 | best_loss=0.02529
Epoch 20/80: current_loss=0.02799 | best_loss=0.02529
Epoch 21/80: current_loss=0.02772 | best_loss=0.02529
Epoch 22/80: current_loss=0.02631 | best_loss=0.02529
Epoch 23/80: current_loss=0.02573 | best_loss=0.02529
Epoch 24/80: current_loss=0.02642 | best_loss=0.02529
Epoch 25/80: current_loss=0.02741 | best_loss=0.02529
Epoch 26/80: current_loss=0.02742 | best_loss=0.02529
Epoch 27/80: current_loss=0.02682 | best_loss=0.02529
Epoch 28/80: current_loss=0.02596 | best_loss=0.02529
Epoch 29/80: current_loss=0.02758 | best_loss=0.02529
Epoch 30/80: current_loss=0.02815 | best_loss=0.02529
Epoch 31/80: current_loss=0.02588 | best_loss=0.02529
Epoch 32/80: current_loss=0.02592 | best_loss=0.02529
Early Stopping at epoch 32
      explained_var=0.04087 | mse_loss=0.02477
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02920 | best_loss=0.02920
Epoch 1/80: current_loss=0.02830 | best_loss=0.02830
Epoch 2/80: current_loss=0.02929 | best_loss=0.02830
Epoch 3/80: current_loss=0.02898 | best_loss=0.02830
Epoch 4/80: current_loss=0.02887 | best_loss=0.02830
Epoch 5/80: current_loss=0.02881 | best_loss=0.02830
Epoch 6/80: current_loss=0.02936 | best_loss=0.02830
Epoch 7/80: current_loss=0.02797 | best_loss=0.02797
Epoch 8/80: current_loss=0.02791 | best_loss=0.02791
Epoch 9/80: current_loss=0.03050 | best_loss=0.02791
Epoch 10/80: current_loss=0.02811 | best_loss=0.02791
Epoch 11/80: current_loss=0.02826 | best_loss=0.02791
Epoch 12/80: current_loss=0.02807 | best_loss=0.02791
Epoch 13/80: current_loss=0.02806 | best_loss=0.02791
Epoch 14/80: current_loss=0.02821 | best_loss=0.02791
Epoch 15/80: current_loss=0.02825 | best_loss=0.02791
Epoch 16/80: current_loss=0.03069 | best_loss=0.02791
Epoch 17/80: current_loss=0.02815 | best_loss=0.02791
Epoch 18/80: current_loss=0.02910 | best_loss=0.02791
Epoch 19/80: current_loss=0.02813 | best_loss=0.02791
Epoch 20/80: current_loss=0.02793 | best_loss=0.02791
Epoch 21/80: current_loss=0.02793 | best_loss=0.02791
Epoch 22/80: current_loss=0.02811 | best_loss=0.02791
Epoch 23/80: current_loss=0.02832 | best_loss=0.02791
Epoch 24/80: current_loss=0.02879 | best_loss=0.02791
Epoch 25/80: current_loss=0.02828 | best_loss=0.02791
Epoch 26/80: current_loss=0.02811 | best_loss=0.02791
Epoch 27/80: current_loss=0.02833 | best_loss=0.02791
Epoch 28/80: current_loss=0.02811 | best_loss=0.02791
Early Stopping at epoch 28
      explained_var=0.01365 | mse_loss=0.02744
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03033 | best_loss=0.03033
Epoch 1/80: current_loss=0.03218 | best_loss=0.03033
Epoch 2/80: current_loss=0.03052 | best_loss=0.03033
Epoch 3/80: current_loss=0.03018 | best_loss=0.03018
Epoch 4/80: current_loss=0.03522 | best_loss=0.03018
Epoch 5/80: current_loss=0.03093 | best_loss=0.03018
Epoch 6/80: current_loss=0.03134 | best_loss=0.03018
Epoch 7/80: current_loss=0.03027 | best_loss=0.03018
Epoch 8/80: current_loss=0.03546 | best_loss=0.03018
Epoch 9/80: current_loss=0.03110 | best_loss=0.03018
Epoch 10/80: current_loss=0.03037 | best_loss=0.03018
Epoch 11/80: current_loss=0.03376 | best_loss=0.03018
Epoch 12/80: current_loss=0.03081 | best_loss=0.03018
Epoch 13/80: current_loss=0.03266 | best_loss=0.03018
Epoch 14/80: current_loss=0.03093 | best_loss=0.03018
Epoch 15/80: current_loss=0.03052 | best_loss=0.03018
Epoch 16/80: current_loss=0.03171 | best_loss=0.03018
Epoch 17/80: current_loss=0.03054 | best_loss=0.03018
Epoch 18/80: current_loss=0.03205 | best_loss=0.03018
Epoch 19/80: current_loss=0.03099 | best_loss=0.03018
Epoch 20/80: current_loss=0.03223 | best_loss=0.03018
Epoch 21/80: current_loss=0.03025 | best_loss=0.03018
Epoch 22/80: current_loss=0.03068 | best_loss=0.03018
Epoch 23/80: current_loss=0.03338 | best_loss=0.03018
Early Stopping at epoch 23
      explained_var=0.01342 | mse_loss=0.02939
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02816 | best_loss=0.02816
Epoch 1/80: current_loss=0.02796 | best_loss=0.02796
Epoch 2/80: current_loss=0.02796 | best_loss=0.02796
Epoch 3/80: current_loss=0.02822 | best_loss=0.02796
Epoch 4/80: current_loss=0.02803 | best_loss=0.02796
Epoch 5/80: current_loss=0.02898 | best_loss=0.02796
Epoch 6/80: current_loss=0.02824 | best_loss=0.02796
Epoch 7/80: current_loss=0.02854 | best_loss=0.02796
Epoch 8/80: current_loss=0.03015 | best_loss=0.02796
Epoch 9/80: current_loss=0.02931 | best_loss=0.02796
Epoch 10/80: current_loss=0.02812 | best_loss=0.02796
Epoch 11/80: current_loss=0.02807 | best_loss=0.02796
Epoch 12/80: current_loss=0.02819 | best_loss=0.02796
Epoch 13/80: current_loss=0.02841 | best_loss=0.02796
Epoch 14/80: current_loss=0.02818 | best_loss=0.02796
Epoch 15/80: current_loss=0.02816 | best_loss=0.02796
Epoch 16/80: current_loss=0.02836 | best_loss=0.02796
Epoch 17/80: current_loss=0.02878 | best_loss=0.02796
Epoch 18/80: current_loss=0.02990 | best_loss=0.02796
Epoch 19/80: current_loss=0.02843 | best_loss=0.02796
Epoch 20/80: current_loss=0.02800 | best_loss=0.02796
Epoch 21/80: current_loss=0.02962 | best_loss=0.02796
Early Stopping at epoch 21
      explained_var=0.00190 | mse_loss=0.02833
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03333 | best_loss=0.03333
Epoch 1/80: current_loss=0.03245 | best_loss=0.03245
Epoch 2/80: current_loss=0.03328 | best_loss=0.03245
Epoch 3/80: current_loss=0.03233 | best_loss=0.03233
Epoch 4/80: current_loss=0.03292 | best_loss=0.03233
Epoch 5/80: current_loss=0.03295 | best_loss=0.03233
Epoch 6/80: current_loss=0.03313 | best_loss=0.03233
Epoch 7/80: current_loss=0.03301 | best_loss=0.03233
Epoch 8/80: current_loss=0.03258 | best_loss=0.03233
Epoch 9/80: current_loss=0.03324 | best_loss=0.03233
Epoch 10/80: current_loss=0.03316 | best_loss=0.03233
Epoch 11/80: current_loss=0.03407 | best_loss=0.03233
Epoch 12/80: current_loss=0.03289 | best_loss=0.03233
Epoch 13/80: current_loss=0.03304 | best_loss=0.03233
Epoch 14/80: current_loss=0.03375 | best_loss=0.03233
Epoch 15/80: current_loss=0.03304 | best_loss=0.03233
Epoch 16/80: current_loss=0.03368 | best_loss=0.03233
Epoch 17/80: current_loss=0.03285 | best_loss=0.03233
Epoch 18/80: current_loss=0.03376 | best_loss=0.03233
Epoch 19/80: current_loss=0.03271 | best_loss=0.03233
Epoch 20/80: current_loss=0.03333 | best_loss=0.03233
Epoch 21/80: current_loss=0.03375 | best_loss=0.03233
Epoch 22/80: current_loss=0.03319 | best_loss=0.03233
Epoch 23/80: current_loss=0.03316 | best_loss=0.03233
Early Stopping at epoch 23
      explained_var=-0.01896 | mse_loss=0.03310
----------------------------------------------
Average early_stopping_point: 5| avg_exp_var=0.01018| avg_loss=0.02861
----------------------------------------------


----------------------------------------------
Params for Trial 18
{'learning_rate': 0.001, 'weight_decay': 0.003634430838471082, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03821 | best_loss=0.03821
Epoch 1/80: current_loss=0.02867 | best_loss=0.02867
Epoch 2/80: current_loss=0.02771 | best_loss=0.02771
Epoch 3/80: current_loss=0.02628 | best_loss=0.02628
Epoch 4/80: current_loss=0.02947 | best_loss=0.02628
Epoch 5/80: current_loss=0.02672 | best_loss=0.02628
Epoch 6/80: current_loss=0.02544 | best_loss=0.02544
Epoch 7/80: current_loss=0.03040 | best_loss=0.02544
Epoch 8/80: current_loss=0.02606 | best_loss=0.02544
Epoch 9/80: current_loss=0.02683 | best_loss=0.02544
Epoch 10/80: current_loss=0.02678 | best_loss=0.02544
Epoch 11/80: current_loss=0.02788 | best_loss=0.02544
Epoch 12/80: current_loss=0.02979 | best_loss=0.02544
Epoch 13/80: current_loss=0.02784 | best_loss=0.02544
Epoch 14/80: current_loss=0.02612 | best_loss=0.02544
Epoch 15/80: current_loss=0.03579 | best_loss=0.02544
Epoch 16/80: current_loss=0.03544 | best_loss=0.02544
Epoch 17/80: current_loss=0.03052 | best_loss=0.02544
Epoch 18/80: current_loss=0.02628 | best_loss=0.02544
Epoch 19/80: current_loss=0.03152 | best_loss=0.02544
Epoch 20/80: current_loss=0.02912 | best_loss=0.02544
Epoch 21/80: current_loss=0.02618 | best_loss=0.02544
Epoch 22/80: current_loss=0.02627 | best_loss=0.02544
Epoch 23/80: current_loss=0.02628 | best_loss=0.02544
Epoch 24/80: current_loss=0.02755 | best_loss=0.02544
Epoch 25/80: current_loss=0.02713 | best_loss=0.02544
Epoch 26/80: current_loss=0.02732 | best_loss=0.02544
Early Stopping at epoch 26
      explained_var=0.03877 | mse_loss=0.02488
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02783 | best_loss=0.02783
Epoch 1/80: current_loss=0.03020 | best_loss=0.02783
Epoch 2/80: current_loss=0.02887 | best_loss=0.02783
Epoch 3/80: current_loss=0.02875 | best_loss=0.02783
Epoch 4/80: current_loss=0.02851 | best_loss=0.02783
Epoch 5/80: current_loss=0.03066 | best_loss=0.02783
Epoch 6/80: current_loss=0.03226 | best_loss=0.02783
Epoch 7/80: current_loss=0.03142 | best_loss=0.02783
Epoch 8/80: current_loss=0.02831 | best_loss=0.02783
Epoch 9/80: current_loss=0.02831 | best_loss=0.02783
Epoch 10/80: current_loss=0.02828 | best_loss=0.02783
Epoch 11/80: current_loss=0.02828 | best_loss=0.02783
Epoch 12/80: current_loss=0.02864 | best_loss=0.02783
Epoch 13/80: current_loss=0.03004 | best_loss=0.02783
Epoch 14/80: current_loss=0.03079 | best_loss=0.02783
Epoch 15/80: current_loss=0.02836 | best_loss=0.02783
Epoch 16/80: current_loss=0.02900 | best_loss=0.02783
Epoch 17/80: current_loss=0.03367 | best_loss=0.02783
Epoch 18/80: current_loss=0.02986 | best_loss=0.02783
Epoch 19/80: current_loss=0.03000 | best_loss=0.02783
Epoch 20/80: current_loss=0.02826 | best_loss=0.02783
Early Stopping at epoch 20
      explained_var=0.03889 | mse_loss=0.02723
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04030 | best_loss=0.04030
Epoch 1/80: current_loss=0.03222 | best_loss=0.03222
Epoch 2/80: current_loss=0.03064 | best_loss=0.03064
Epoch 3/80: current_loss=0.03306 | best_loss=0.03064
Epoch 4/80: current_loss=0.03671 | best_loss=0.03064
Epoch 5/80: current_loss=0.03443 | best_loss=0.03064
Epoch 6/80: current_loss=0.03233 | best_loss=0.03064
Epoch 7/80: current_loss=0.03049 | best_loss=0.03049
Epoch 8/80: current_loss=0.03169 | best_loss=0.03049
Epoch 9/80: current_loss=0.03123 | best_loss=0.03049
Epoch 10/80: current_loss=0.03253 | best_loss=0.03049
Epoch 11/80: current_loss=0.03362 | best_loss=0.03049
Epoch 12/80: current_loss=0.03252 | best_loss=0.03049
Epoch 13/80: current_loss=0.03381 | best_loss=0.03049
Epoch 14/80: current_loss=0.03565 | best_loss=0.03049
Epoch 15/80: current_loss=0.03233 | best_loss=0.03049
Epoch 16/80: current_loss=0.03192 | best_loss=0.03049
Epoch 17/80: current_loss=0.03070 | best_loss=0.03049
Epoch 18/80: current_loss=0.03099 | best_loss=0.03049
Epoch 19/80: current_loss=0.03049 | best_loss=0.03049
Epoch 20/80: current_loss=0.03311 | best_loss=0.03049
Epoch 21/80: current_loss=0.03333 | best_loss=0.03049
Epoch 22/80: current_loss=0.03079 | best_loss=0.03049
Epoch 23/80: current_loss=0.03237 | best_loss=0.03049
Epoch 24/80: current_loss=0.03084 | best_loss=0.03049
Epoch 25/80: current_loss=0.03076 | best_loss=0.03049
Epoch 26/80: current_loss=0.03069 | best_loss=0.03049
Epoch 27/80: current_loss=0.03056 | best_loss=0.03049
Epoch 28/80: current_loss=0.03213 | best_loss=0.03049
Epoch 29/80: current_loss=0.03064 | best_loss=0.03049
Epoch 30/80: current_loss=0.03234 | best_loss=0.03049
Epoch 31/80: current_loss=0.03035 | best_loss=0.03035
Epoch 32/80: current_loss=0.03059 | best_loss=0.03035
Epoch 33/80: current_loss=0.03075 | best_loss=0.03035
Epoch 34/80: current_loss=0.03111 | best_loss=0.03035
Epoch 35/80: current_loss=0.03105 | best_loss=0.03035
Epoch 36/80: current_loss=0.03191 | best_loss=0.03035
Epoch 37/80: current_loss=0.03120 | best_loss=0.03035
Epoch 38/80: current_loss=0.03323 | best_loss=0.03035
Epoch 39/80: current_loss=0.03090 | best_loss=0.03035
Epoch 40/80: current_loss=0.03150 | best_loss=0.03035
Epoch 41/80: current_loss=0.03322 | best_loss=0.03035
Epoch 42/80: current_loss=0.03095 | best_loss=0.03035
Epoch 43/80: current_loss=0.03082 | best_loss=0.03035
Epoch 44/80: current_loss=0.03053 | best_loss=0.03035
Epoch 45/80: current_loss=0.03370 | best_loss=0.03035
Epoch 46/80: current_loss=0.03201 | best_loss=0.03035
Epoch 47/80: current_loss=0.03064 | best_loss=0.03035
Epoch 48/80: current_loss=0.03126 | best_loss=0.03035
Epoch 49/80: current_loss=0.03414 | best_loss=0.03035
Epoch 50/80: current_loss=0.03365 | best_loss=0.03035
Epoch 51/80: current_loss=0.03116 | best_loss=0.03035
Early Stopping at epoch 51
      explained_var=0.00203 | mse_loss=0.02956
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02933 | best_loss=0.02933
Epoch 1/80: current_loss=0.02858 | best_loss=0.02858
Epoch 2/80: current_loss=0.02884 | best_loss=0.02858
Epoch 3/80: current_loss=0.02807 | best_loss=0.02807
Epoch 4/80: current_loss=0.02883 | best_loss=0.02807
Epoch 5/80: current_loss=0.02797 | best_loss=0.02797
Epoch 6/80: current_loss=0.02930 | best_loss=0.02797
Epoch 7/80: current_loss=0.02837 | best_loss=0.02797
Epoch 8/80: current_loss=0.02870 | best_loss=0.02797
Epoch 9/80: current_loss=0.02833 | best_loss=0.02797
Epoch 10/80: current_loss=0.02847 | best_loss=0.02797
Epoch 11/80: current_loss=0.02816 | best_loss=0.02797
Epoch 12/80: current_loss=0.02890 | best_loss=0.02797
Epoch 13/80: current_loss=0.02899 | best_loss=0.02797
Epoch 14/80: current_loss=0.02813 | best_loss=0.02797
Epoch 15/80: current_loss=0.02833 | best_loss=0.02797
Epoch 16/80: current_loss=0.03072 | best_loss=0.02797
Epoch 17/80: current_loss=0.02833 | best_loss=0.02797
Epoch 18/80: current_loss=0.02827 | best_loss=0.02797
Epoch 19/80: current_loss=0.02988 | best_loss=0.02797
Epoch 20/80: current_loss=0.02822 | best_loss=0.02797
Epoch 21/80: current_loss=0.02824 | best_loss=0.02797
Epoch 22/80: current_loss=0.02802 | best_loss=0.02797
Epoch 23/80: current_loss=0.02980 | best_loss=0.02797
Epoch 24/80: current_loss=0.02824 | best_loss=0.02797
Epoch 25/80: current_loss=0.02806 | best_loss=0.02797
Early Stopping at epoch 25
      explained_var=0.00215 | mse_loss=0.02831
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03196 | best_loss=0.03196
Epoch 1/80: current_loss=0.03227 | best_loss=0.03196
Epoch 2/80: current_loss=0.03248 | best_loss=0.03196
Epoch 3/80: current_loss=0.03326 | best_loss=0.03196
Epoch 4/80: current_loss=0.03200 | best_loss=0.03196
Epoch 5/80: current_loss=0.03218 | best_loss=0.03196
Epoch 6/80: current_loss=0.03375 | best_loss=0.03196
Epoch 7/80: current_loss=0.03192 | best_loss=0.03192
Epoch 8/80: current_loss=0.03197 | best_loss=0.03192
Epoch 9/80: current_loss=0.03196 | best_loss=0.03192
Epoch 10/80: current_loss=0.03204 | best_loss=0.03192
Epoch 11/80: current_loss=0.03183 | best_loss=0.03183
Epoch 12/80: current_loss=0.03238 | best_loss=0.03183
Epoch 13/80: current_loss=0.03176 | best_loss=0.03176
Epoch 14/80: current_loss=0.03174 | best_loss=0.03174
Epoch 15/80: current_loss=0.03185 | best_loss=0.03174
Epoch 16/80: current_loss=0.03180 | best_loss=0.03174
Epoch 17/80: current_loss=0.03173 | best_loss=0.03173
Epoch 18/80: current_loss=0.03273 | best_loss=0.03173
Epoch 19/80: current_loss=0.03425 | best_loss=0.03173
Epoch 20/80: current_loss=0.03188 | best_loss=0.03173
Epoch 21/80: current_loss=0.03211 | best_loss=0.03173
Epoch 22/80: current_loss=0.03226 | best_loss=0.03173
Epoch 23/80: current_loss=0.03317 | best_loss=0.03173
Epoch 24/80: current_loss=0.03182 | best_loss=0.03173
Epoch 25/80: current_loss=0.03264 | best_loss=0.03173
Epoch 26/80: current_loss=0.03185 | best_loss=0.03173
Epoch 27/80: current_loss=0.03180 | best_loss=0.03173
Epoch 28/80: current_loss=0.03183 | best_loss=0.03173
Epoch 29/80: current_loss=0.03281 | best_loss=0.03173
Epoch 30/80: current_loss=0.03186 | best_loss=0.03173
Epoch 31/80: current_loss=0.03180 | best_loss=0.03173
Epoch 32/80: current_loss=0.03178 | best_loss=0.03173
Epoch 33/80: current_loss=0.03180 | best_loss=0.03173
Epoch 34/80: current_loss=0.03177 | best_loss=0.03173
Epoch 35/80: current_loss=0.03219 | best_loss=0.03173
Epoch 36/80: current_loss=0.03209 | best_loss=0.03173
Epoch 37/80: current_loss=0.03172 | best_loss=0.03172
Epoch 38/80: current_loss=0.03181 | best_loss=0.03172
Epoch 39/80: current_loss=0.03215 | best_loss=0.03172
Epoch 40/80: current_loss=0.03248 | best_loss=0.03172
Epoch 41/80: current_loss=0.03201 | best_loss=0.03172
Epoch 42/80: current_loss=0.03243 | best_loss=0.03172
Epoch 43/80: current_loss=0.03245 | best_loss=0.03172
Epoch 44/80: current_loss=0.03238 | best_loss=0.03172
Epoch 45/80: current_loss=0.03173 | best_loss=0.03172
Epoch 46/80: current_loss=0.03251 | best_loss=0.03172
Epoch 47/80: current_loss=0.03217 | best_loss=0.03172
Epoch 48/80: current_loss=0.03188 | best_loss=0.03172
Epoch 49/80: current_loss=0.03179 | best_loss=0.03172
Epoch 50/80: current_loss=0.03179 | best_loss=0.03172
Epoch 51/80: current_loss=0.03183 | best_loss=0.03172
Epoch 52/80: current_loss=0.03194 | best_loss=0.03172
Epoch 53/80: current_loss=0.03173 | best_loss=0.03172
Epoch 54/80: current_loss=0.03180 | best_loss=0.03172
Epoch 55/80: current_loss=0.03176 | best_loss=0.03172
Epoch 56/80: current_loss=0.03214 | best_loss=0.03172
Epoch 57/80: current_loss=0.03201 | best_loss=0.03172
Early Stopping at epoch 57
      explained_var=0.00133 | mse_loss=0.03247
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.01663| avg_loss=0.02849
----------------------------------------------


----------------------------------------------
Params for Trial 19
{'learning_rate': 0.001, 'weight_decay': 0.0019321913361300055, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02919 | best_loss=0.02919
Epoch 1/80: current_loss=0.02680 | best_loss=0.02680
Epoch 2/80: current_loss=0.02906 | best_loss=0.02680
Epoch 3/80: current_loss=0.02614 | best_loss=0.02614
Epoch 4/80: current_loss=0.02675 | best_loss=0.02614
Epoch 5/80: current_loss=0.02730 | best_loss=0.02614
Epoch 6/80: current_loss=0.02580 | best_loss=0.02580
Epoch 7/80: current_loss=0.04119 | best_loss=0.02580
Epoch 8/80: current_loss=0.02629 | best_loss=0.02580
Epoch 9/80: current_loss=0.02647 | best_loss=0.02580
Epoch 10/80: current_loss=0.02713 | best_loss=0.02580
Epoch 11/80: current_loss=0.03152 | best_loss=0.02580
Epoch 12/80: current_loss=0.02592 | best_loss=0.02580
Epoch 13/80: current_loss=0.02567 | best_loss=0.02567
Epoch 14/80: current_loss=0.02607 | best_loss=0.02567
Epoch 15/80: current_loss=0.02782 | best_loss=0.02567
Epoch 16/80: current_loss=0.02843 | best_loss=0.02567
Epoch 17/80: current_loss=0.02834 | best_loss=0.02567
Epoch 18/80: current_loss=0.02803 | best_loss=0.02567
Epoch 19/80: current_loss=0.02733 | best_loss=0.02567
Epoch 20/80: current_loss=0.03151 | best_loss=0.02567
Epoch 21/80: current_loss=0.02593 | best_loss=0.02567
Epoch 22/80: current_loss=0.02857 | best_loss=0.02567
Epoch 23/80: current_loss=0.02611 | best_loss=0.02567
Epoch 24/80: current_loss=0.02778 | best_loss=0.02567
Epoch 25/80: current_loss=0.02591 | best_loss=0.02567
Epoch 26/80: current_loss=0.02605 | best_loss=0.02567
Epoch 27/80: current_loss=0.02571 | best_loss=0.02567
Epoch 28/80: current_loss=0.02891 | best_loss=0.02567
Epoch 29/80: current_loss=0.02810 | best_loss=0.02567
Epoch 30/80: current_loss=0.02707 | best_loss=0.02567
Epoch 31/80: current_loss=0.02577 | best_loss=0.02567
Epoch 32/80: current_loss=0.02620 | best_loss=0.02567
Epoch 33/80: current_loss=0.03272 | best_loss=0.02567
Early Stopping at epoch 33
      explained_var=0.03056 | mse_loss=0.02507
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02950 | best_loss=0.02950
Epoch 1/80: current_loss=0.02960 | best_loss=0.02950
Epoch 2/80: current_loss=0.02839 | best_loss=0.02839
Epoch 3/80: current_loss=0.03023 | best_loss=0.02839
Epoch 4/80: current_loss=0.02862 | best_loss=0.02839
Epoch 5/80: current_loss=0.03062 | best_loss=0.02839
Epoch 6/80: current_loss=0.02980 | best_loss=0.02839
Epoch 7/80: current_loss=0.02874 | best_loss=0.02839
Epoch 8/80: current_loss=0.02990 | best_loss=0.02839
Epoch 9/80: current_loss=0.02815 | best_loss=0.02815
Epoch 10/80: current_loss=0.02858 | best_loss=0.02815
Epoch 11/80: current_loss=0.02894 | best_loss=0.02815
Epoch 12/80: current_loss=0.02849 | best_loss=0.02815
Epoch 13/80: current_loss=0.02984 | best_loss=0.02815
Epoch 14/80: current_loss=0.02877 | best_loss=0.02815
Epoch 15/80: current_loss=0.02865 | best_loss=0.02815
Epoch 16/80: current_loss=0.02859 | best_loss=0.02815
Epoch 17/80: current_loss=0.02851 | best_loss=0.02815
Epoch 18/80: current_loss=0.02853 | best_loss=0.02815
Epoch 19/80: current_loss=0.02836 | best_loss=0.02815
Epoch 20/80: current_loss=0.03020 | best_loss=0.02815
Epoch 21/80: current_loss=0.02878 | best_loss=0.02815
Epoch 22/80: current_loss=0.02877 | best_loss=0.02815
Epoch 23/80: current_loss=0.02840 | best_loss=0.02815
Epoch 24/80: current_loss=0.02817 | best_loss=0.02815
Epoch 25/80: current_loss=0.02825 | best_loss=0.02815
Epoch 26/80: current_loss=0.02845 | best_loss=0.02815
Epoch 27/80: current_loss=0.02844 | best_loss=0.02815
Epoch 28/80: current_loss=0.02814 | best_loss=0.02814
Epoch 29/80: current_loss=0.02867 | best_loss=0.02814
Epoch 30/80: current_loss=0.02830 | best_loss=0.02814
Epoch 31/80: current_loss=0.02847 | best_loss=0.02814
Epoch 32/80: current_loss=0.02985 | best_loss=0.02814
Epoch 33/80: current_loss=0.02807 | best_loss=0.02807
Epoch 34/80: current_loss=0.02810 | best_loss=0.02807
Epoch 35/80: current_loss=0.02836 | best_loss=0.02807
Epoch 36/80: current_loss=0.02830 | best_loss=0.02807
Epoch 37/80: current_loss=0.02817 | best_loss=0.02807
Epoch 38/80: current_loss=0.02872 | best_loss=0.02807
Epoch 39/80: current_loss=0.03024 | best_loss=0.02807
Epoch 40/80: current_loss=0.02879 | best_loss=0.02807
Epoch 41/80: current_loss=0.02861 | best_loss=0.02807
Epoch 42/80: current_loss=0.02807 | best_loss=0.02807
Epoch 43/80: current_loss=0.02809 | best_loss=0.02807
Epoch 44/80: current_loss=0.02819 | best_loss=0.02807
Epoch 45/80: current_loss=0.02824 | best_loss=0.02807
Epoch 46/80: current_loss=0.02828 | best_loss=0.02807
Epoch 47/80: current_loss=0.02910 | best_loss=0.02807
Epoch 48/80: current_loss=0.02831 | best_loss=0.02807
Epoch 49/80: current_loss=0.02923 | best_loss=0.02807
Epoch 50/80: current_loss=0.02842 | best_loss=0.02807
Epoch 51/80: current_loss=0.02819 | best_loss=0.02807
Epoch 52/80: current_loss=0.02839 | best_loss=0.02807
Epoch 53/80: current_loss=0.02873 | best_loss=0.02807
Epoch 54/80: current_loss=0.02828 | best_loss=0.02807
Epoch 55/80: current_loss=0.02885 | best_loss=0.02807
Epoch 56/80: current_loss=0.02823 | best_loss=0.02807
Epoch 57/80: current_loss=0.02835 | best_loss=0.02807
Epoch 58/80: current_loss=0.03118 | best_loss=0.02807
Epoch 59/80: current_loss=0.02855 | best_loss=0.02807
Epoch 60/80: current_loss=0.02831 | best_loss=0.02807
Epoch 61/80: current_loss=0.02833 | best_loss=0.02807
Epoch 62/80: current_loss=0.02863 | best_loss=0.02807
Early Stopping at epoch 62
      explained_var=0.00875 | mse_loss=0.02755
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03111 | best_loss=0.03111
Epoch 1/80: current_loss=0.03123 | best_loss=0.03111
Epoch 2/80: current_loss=0.03048 | best_loss=0.03048
Epoch 3/80: current_loss=0.03297 | best_loss=0.03048
Epoch 4/80: current_loss=0.03137 | best_loss=0.03048
Epoch 5/80: current_loss=0.03052 | best_loss=0.03048
Epoch 6/80: current_loss=0.03442 | best_loss=0.03048
Epoch 7/80: current_loss=0.03038 | best_loss=0.03038
Epoch 8/80: current_loss=0.03231 | best_loss=0.03038
Epoch 9/80: current_loss=0.03382 | best_loss=0.03038
Epoch 10/80: current_loss=0.03038 | best_loss=0.03038
Epoch 11/80: current_loss=0.03080 | best_loss=0.03038
Epoch 12/80: current_loss=0.03587 | best_loss=0.03038
Epoch 13/80: current_loss=0.03026 | best_loss=0.03026
Epoch 14/80: current_loss=0.03206 | best_loss=0.03026
Epoch 15/80: current_loss=0.03044 | best_loss=0.03026
Epoch 16/80: current_loss=0.03203 | best_loss=0.03026
Epoch 17/80: current_loss=0.03197 | best_loss=0.03026
Epoch 18/80: current_loss=0.03134 | best_loss=0.03026
Epoch 19/80: current_loss=0.03079 | best_loss=0.03026
Epoch 20/80: current_loss=0.03109 | best_loss=0.03026
Epoch 21/80: current_loss=0.03058 | best_loss=0.03026
Epoch 22/80: current_loss=0.03168 | best_loss=0.03026
Epoch 23/80: current_loss=0.03171 | best_loss=0.03026
Epoch 24/80: current_loss=0.03179 | best_loss=0.03026
Epoch 25/80: current_loss=0.03073 | best_loss=0.03026
Epoch 26/80: current_loss=0.03144 | best_loss=0.03026
Epoch 27/80: current_loss=0.03080 | best_loss=0.03026
Epoch 28/80: current_loss=0.03287 | best_loss=0.03026
Epoch 29/80: current_loss=0.03114 | best_loss=0.03026
Epoch 30/80: current_loss=0.03078 | best_loss=0.03026
Epoch 31/80: current_loss=0.03177 | best_loss=0.03026
Epoch 32/80: current_loss=0.03127 | best_loss=0.03026
Epoch 33/80: current_loss=0.03108 | best_loss=0.03026
Early Stopping at epoch 33
      explained_var=0.00416 | mse_loss=0.02946
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02819 | best_loss=0.02819
Epoch 1/80: current_loss=0.02883 | best_loss=0.02819
Epoch 2/80: current_loss=0.02798 | best_loss=0.02798
Epoch 3/80: current_loss=0.02817 | best_loss=0.02798
Epoch 4/80: current_loss=0.02838 | best_loss=0.02798
Epoch 5/80: current_loss=0.02933 | best_loss=0.02798
Epoch 6/80: current_loss=0.02810 | best_loss=0.02798
Epoch 7/80: current_loss=0.02805 | best_loss=0.02798
Epoch 8/80: current_loss=0.02826 | best_loss=0.02798
Epoch 9/80: current_loss=0.02807 | best_loss=0.02798
Epoch 10/80: current_loss=0.02804 | best_loss=0.02798
Epoch 11/80: current_loss=0.02808 | best_loss=0.02798
Epoch 12/80: current_loss=0.02946 | best_loss=0.02798
Epoch 13/80: current_loss=0.02899 | best_loss=0.02798
Epoch 14/80: current_loss=0.02804 | best_loss=0.02798
Epoch 15/80: current_loss=0.02902 | best_loss=0.02798
Epoch 16/80: current_loss=0.02802 | best_loss=0.02798
Epoch 17/80: current_loss=0.02824 | best_loss=0.02798
Epoch 18/80: current_loss=0.02847 | best_loss=0.02798
Epoch 19/80: current_loss=0.02849 | best_loss=0.02798
Epoch 20/80: current_loss=0.02836 | best_loss=0.02798
Epoch 21/80: current_loss=0.02817 | best_loss=0.02798
Epoch 22/80: current_loss=0.02848 | best_loss=0.02798
Early Stopping at epoch 22
      explained_var=0.00071 | mse_loss=0.02836
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03171 | best_loss=0.03171
Epoch 1/80: current_loss=0.03166 | best_loss=0.03166
Epoch 2/80: current_loss=0.03191 | best_loss=0.03166
Epoch 3/80: current_loss=0.03223 | best_loss=0.03166
Epoch 4/80: current_loss=0.03181 | best_loss=0.03166
Epoch 5/80: current_loss=0.03291 | best_loss=0.03166
Epoch 6/80: current_loss=0.03173 | best_loss=0.03166
Epoch 7/80: current_loss=0.03184 | best_loss=0.03166
Epoch 8/80: current_loss=0.03195 | best_loss=0.03166
Epoch 9/80: current_loss=0.03175 | best_loss=0.03166
Epoch 10/80: current_loss=0.03177 | best_loss=0.03166
Epoch 11/80: current_loss=0.03249 | best_loss=0.03166
Epoch 12/80: current_loss=0.03200 | best_loss=0.03166
Epoch 13/80: current_loss=0.03182 | best_loss=0.03166
Epoch 14/80: current_loss=0.03192 | best_loss=0.03166
Epoch 15/80: current_loss=0.03206 | best_loss=0.03166
Epoch 16/80: current_loss=0.03241 | best_loss=0.03166
Epoch 17/80: current_loss=0.03206 | best_loss=0.03166
Epoch 18/80: current_loss=0.03219 | best_loss=0.03166
Epoch 19/80: current_loss=0.03186 | best_loss=0.03166
Epoch 20/80: current_loss=0.03229 | best_loss=0.03166
Epoch 21/80: current_loss=0.03183 | best_loss=0.03166
Early Stopping at epoch 21
      explained_var=0.00288 | mse_loss=0.03242
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.00941| avg_loss=0.02857
----------------------------------------------


----------------------------------------------
Params for Trial 20
{'learning_rate': 0.1, 'weight_decay': 0.0009418794372841358, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=2.13029 | best_loss=2.13029
Epoch 1/80: current_loss=7.06097 | best_loss=2.13029
Epoch 2/80: current_loss=6.32997 | best_loss=2.13029
Epoch 3/80: current_loss=22.21509 | best_loss=2.13029
Epoch 4/80: current_loss=0.76133 | best_loss=0.76133
Epoch 5/80: current_loss=0.37630 | best_loss=0.37630
Epoch 6/80: current_loss=7.43875 | best_loss=0.37630
Epoch 7/80: current_loss=0.73114 | best_loss=0.37630
Epoch 8/80: current_loss=2.84992 | best_loss=0.37630
Epoch 9/80: current_loss=0.10059 | best_loss=0.10059
Epoch 10/80: current_loss=0.05868 | best_loss=0.05868
Epoch 11/80: current_loss=1.39529 | best_loss=0.05868
Epoch 12/80: current_loss=1.08674 | best_loss=0.05868
Epoch 13/80: current_loss=1.78889 | best_loss=0.05868
Epoch 14/80: current_loss=0.81499 | best_loss=0.05868
Epoch 15/80: current_loss=0.15438 | best_loss=0.05868
Epoch 16/80: current_loss=0.29254 | best_loss=0.05868
Epoch 17/80: current_loss=0.44013 | best_loss=0.05868
Epoch 18/80: current_loss=0.58103 | best_loss=0.05868
Epoch 19/80: current_loss=1.94922 | best_loss=0.05868
Epoch 20/80: current_loss=4.67829 | best_loss=0.05868
Epoch 21/80: current_loss=1.79566 | best_loss=0.05868
Epoch 22/80: current_loss=2.66703 | best_loss=0.05868
Epoch 23/80: current_loss=9.48944 | best_loss=0.05868
Epoch 24/80: current_loss=0.10417 | best_loss=0.05868
Epoch 25/80: current_loss=1.16606 | best_loss=0.05868
Epoch 26/80: current_loss=6.44585 | best_loss=0.05868
Epoch 27/80: current_loss=0.13877 | best_loss=0.05868
Epoch 28/80: current_loss=10.06357 | best_loss=0.05868
Epoch 29/80: current_loss=6.34051 | best_loss=0.05868
Epoch 30/80: current_loss=1.72854 | best_loss=0.05868
Early Stopping at epoch 30
      explained_var=-0.88388 | mse_loss=0.05819

----------------------------------------------
Params for Trial 21
{'learning_rate': 0.001, 'weight_decay': 0.0037804125099170248, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03272 | best_loss=0.03272
Epoch 1/80: current_loss=0.04254 | best_loss=0.03272
Epoch 2/80: current_loss=0.02864 | best_loss=0.02864
Epoch 3/80: current_loss=0.02585 | best_loss=0.02585
Epoch 4/80: current_loss=0.02616 | best_loss=0.02585
Epoch 5/80: current_loss=0.02637 | best_loss=0.02585
Epoch 6/80: current_loss=0.02809 | best_loss=0.02585
Epoch 7/80: current_loss=0.02636 | best_loss=0.02585
Epoch 8/80: current_loss=0.02570 | best_loss=0.02570
Epoch 9/80: current_loss=0.02605 | best_loss=0.02570
Epoch 10/80: current_loss=0.02899 | best_loss=0.02570
Epoch 11/80: current_loss=0.02694 | best_loss=0.02570
Epoch 12/80: current_loss=0.02729 | best_loss=0.02570
Epoch 13/80: current_loss=0.02624 | best_loss=0.02570
Epoch 14/80: current_loss=0.02604 | best_loss=0.02570
Epoch 15/80: current_loss=0.02613 | best_loss=0.02570
Epoch 16/80: current_loss=0.02647 | best_loss=0.02570
Epoch 17/80: current_loss=0.02644 | best_loss=0.02570
Epoch 18/80: current_loss=0.02624 | best_loss=0.02570
Epoch 19/80: current_loss=0.02608 | best_loss=0.02570
Epoch 20/80: current_loss=0.02601 | best_loss=0.02570
Epoch 21/80: current_loss=0.02601 | best_loss=0.02570
Epoch 22/80: current_loss=0.02653 | best_loss=0.02570
Epoch 23/80: current_loss=0.02749 | best_loss=0.02570
Epoch 24/80: current_loss=0.02814 | best_loss=0.02570
Epoch 25/80: current_loss=0.02816 | best_loss=0.02570
Epoch 26/80: current_loss=0.03485 | best_loss=0.02570
Epoch 27/80: current_loss=0.04824 | best_loss=0.02570
Epoch 28/80: current_loss=0.03332 | best_loss=0.02570
Early Stopping at epoch 28
      explained_var=0.02810 | mse_loss=0.02510
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02798 | best_loss=0.02798
Epoch 1/80: current_loss=0.02819 | best_loss=0.02798
Epoch 2/80: current_loss=0.02818 | best_loss=0.02798
Epoch 3/80: current_loss=0.03440 | best_loss=0.02798
Epoch 4/80: current_loss=0.02937 | best_loss=0.02798
Epoch 5/80: current_loss=0.03413 | best_loss=0.02798
Epoch 6/80: current_loss=0.02835 | best_loss=0.02798
Epoch 7/80: current_loss=0.02833 | best_loss=0.02798
Epoch 8/80: current_loss=0.03539 | best_loss=0.02798
Epoch 9/80: current_loss=0.03137 | best_loss=0.02798
Epoch 10/80: current_loss=0.02829 | best_loss=0.02798
Epoch 11/80: current_loss=0.03565 | best_loss=0.02798
Epoch 12/80: current_loss=0.02888 | best_loss=0.02798
Epoch 13/80: current_loss=0.03112 | best_loss=0.02798
Epoch 14/80: current_loss=0.03002 | best_loss=0.02798
Epoch 15/80: current_loss=0.02826 | best_loss=0.02798
Epoch 16/80: current_loss=0.02838 | best_loss=0.02798
Epoch 17/80: current_loss=0.03056 | best_loss=0.02798
Epoch 18/80: current_loss=0.02830 | best_loss=0.02798
Epoch 19/80: current_loss=0.02961 | best_loss=0.02798
Epoch 20/80: current_loss=0.02843 | best_loss=0.02798
Early Stopping at epoch 20
      explained_var=0.00914 | mse_loss=0.02754
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03196 | best_loss=0.03196
Epoch 1/80: current_loss=0.03161 | best_loss=0.03161
Epoch 2/80: current_loss=0.03180 | best_loss=0.03161
Epoch 3/80: current_loss=0.03061 | best_loss=0.03061
Epoch 4/80: current_loss=0.03089 | best_loss=0.03061
Epoch 5/80: current_loss=0.03335 | best_loss=0.03061
Epoch 6/80: current_loss=0.03747 | best_loss=0.03061
Epoch 7/80: current_loss=0.03077 | best_loss=0.03061
Epoch 8/80: current_loss=0.03381 | best_loss=0.03061
Epoch 9/80: current_loss=0.03558 | best_loss=0.03061
Epoch 10/80: current_loss=0.03718 | best_loss=0.03061
Epoch 11/80: current_loss=0.03891 | best_loss=0.03061
Epoch 12/80: current_loss=0.04174 | best_loss=0.03061
Epoch 13/80: current_loss=0.03995 | best_loss=0.03061
Epoch 14/80: current_loss=0.03703 | best_loss=0.03061
Epoch 15/80: current_loss=0.03108 | best_loss=0.03061
Epoch 16/80: current_loss=0.03128 | best_loss=0.03061
Epoch 17/80: current_loss=0.03861 | best_loss=0.03061
Epoch 18/80: current_loss=0.03282 | best_loss=0.03061
Epoch 19/80: current_loss=0.03052 | best_loss=0.03052
Epoch 20/80: current_loss=0.03089 | best_loss=0.03052
Epoch 21/80: current_loss=0.03059 | best_loss=0.03052
Epoch 22/80: current_loss=0.03529 | best_loss=0.03052
Epoch 23/80: current_loss=0.03723 | best_loss=0.03052
Epoch 24/80: current_loss=0.03071 | best_loss=0.03052
Epoch 25/80: current_loss=0.03189 | best_loss=0.03052
Epoch 26/80: current_loss=0.03245 | best_loss=0.03052
Epoch 27/80: current_loss=0.03086 | best_loss=0.03052
Epoch 28/80: current_loss=0.03381 | best_loss=0.03052
Epoch 29/80: current_loss=0.03252 | best_loss=0.03052
Epoch 30/80: current_loss=0.03587 | best_loss=0.03052
Epoch 31/80: current_loss=0.03086 | best_loss=0.03052
Epoch 32/80: current_loss=0.03185 | best_loss=0.03052
Epoch 33/80: current_loss=0.03287 | best_loss=0.03052
Epoch 34/80: current_loss=0.03277 | best_loss=0.03052
Epoch 35/80: current_loss=0.03103 | best_loss=0.03052
Epoch 36/80: current_loss=0.03115 | best_loss=0.03052
Epoch 37/80: current_loss=0.03072 | best_loss=0.03052
Epoch 38/80: current_loss=0.03319 | best_loss=0.03052
Epoch 39/80: current_loss=0.03353 | best_loss=0.03052
Early Stopping at epoch 39
      explained_var=-0.00101 | mse_loss=0.02975
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02809 | best_loss=0.02809
Epoch 1/80: current_loss=0.02869 | best_loss=0.02809
Epoch 2/80: current_loss=0.02822 | best_loss=0.02809
Epoch 3/80: current_loss=0.02928 | best_loss=0.02809
Epoch 4/80: current_loss=0.02852 | best_loss=0.02809
Epoch 5/80: current_loss=0.02829 | best_loss=0.02809
Epoch 6/80: current_loss=0.02839 | best_loss=0.02809
Epoch 7/80: current_loss=0.02915 | best_loss=0.02809
Epoch 8/80: current_loss=0.02834 | best_loss=0.02809
Epoch 9/80: current_loss=0.03051 | best_loss=0.02809
Epoch 10/80: current_loss=0.03058 | best_loss=0.02809
Epoch 11/80: current_loss=0.02814 | best_loss=0.02809
Epoch 12/80: current_loss=0.02912 | best_loss=0.02809
Epoch 13/80: current_loss=0.02812 | best_loss=0.02809
Epoch 14/80: current_loss=0.02859 | best_loss=0.02809
Epoch 15/80: current_loss=0.03115 | best_loss=0.02809
Epoch 16/80: current_loss=0.03000 | best_loss=0.02809
Epoch 17/80: current_loss=0.02836 | best_loss=0.02809
Epoch 18/80: current_loss=0.02830 | best_loss=0.02809
Epoch 19/80: current_loss=0.02939 | best_loss=0.02809
Epoch 20/80: current_loss=0.02831 | best_loss=0.02809
Early Stopping at epoch 20
      explained_var=-0.00207 | mse_loss=0.02846
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03383 | best_loss=0.03383
Epoch 1/80: current_loss=0.03181 | best_loss=0.03181
Epoch 2/80: current_loss=0.03385 | best_loss=0.03181
Epoch 3/80: current_loss=0.03539 | best_loss=0.03181
Epoch 4/80: current_loss=0.03185 | best_loss=0.03181
Epoch 5/80: current_loss=0.03632 | best_loss=0.03181
Epoch 6/80: current_loss=0.03263 | best_loss=0.03181
Epoch 7/80: current_loss=0.03258 | best_loss=0.03181
Epoch 8/80: current_loss=0.03218 | best_loss=0.03181
Epoch 9/80: current_loss=0.03217 | best_loss=0.03181
Epoch 10/80: current_loss=0.03214 | best_loss=0.03181
Epoch 11/80: current_loss=0.03240 | best_loss=0.03181
Epoch 12/80: current_loss=0.03267 | best_loss=0.03181
Epoch 13/80: current_loss=0.03196 | best_loss=0.03181
Epoch 14/80: current_loss=0.03325 | best_loss=0.03181
Epoch 15/80: current_loss=0.03340 | best_loss=0.03181
Epoch 16/80: current_loss=0.03192 | best_loss=0.03181
Epoch 17/80: current_loss=0.03171 | best_loss=0.03171
Epoch 18/80: current_loss=0.03182 | best_loss=0.03171
Epoch 19/80: current_loss=0.03191 | best_loss=0.03171
Epoch 20/80: current_loss=0.03209 | best_loss=0.03171
Epoch 21/80: current_loss=0.03208 | best_loss=0.03171
Epoch 22/80: current_loss=0.03179 | best_loss=0.03171
Epoch 23/80: current_loss=0.03312 | best_loss=0.03171
Epoch 24/80: current_loss=0.03179 | best_loss=0.03171
Epoch 25/80: current_loss=0.03217 | best_loss=0.03171
Epoch 26/80: current_loss=0.03186 | best_loss=0.03171
Epoch 27/80: current_loss=0.03278 | best_loss=0.03171
Epoch 28/80: current_loss=0.03188 | best_loss=0.03171
Epoch 29/80: current_loss=0.03244 | best_loss=0.03171
Epoch 30/80: current_loss=0.03187 | best_loss=0.03171
Epoch 31/80: current_loss=0.03295 | best_loss=0.03171
Epoch 32/80: current_loss=0.03277 | best_loss=0.03171
Epoch 33/80: current_loss=0.03195 | best_loss=0.03171
Epoch 34/80: current_loss=0.03174 | best_loss=0.03171
Epoch 35/80: current_loss=0.03226 | best_loss=0.03171
Epoch 36/80: current_loss=0.03168 | best_loss=0.03168
Epoch 37/80: current_loss=0.03177 | best_loss=0.03168
Epoch 38/80: current_loss=0.03173 | best_loss=0.03168
Epoch 39/80: current_loss=0.03175 | best_loss=0.03168
Epoch 40/80: current_loss=0.03190 | best_loss=0.03168
Epoch 41/80: current_loss=0.03175 | best_loss=0.03168
Epoch 42/80: current_loss=0.03247 | best_loss=0.03168
Epoch 43/80: current_loss=0.03296 | best_loss=0.03168
Epoch 44/80: current_loss=0.03188 | best_loss=0.03168
Epoch 45/80: current_loss=0.03835 | best_loss=0.03168
Epoch 46/80: current_loss=0.03451 | best_loss=0.03168
Epoch 47/80: current_loss=0.03205 | best_loss=0.03168
Epoch 48/80: current_loss=0.03182 | best_loss=0.03168
Epoch 49/80: current_loss=0.03180 | best_loss=0.03168
Epoch 50/80: current_loss=0.03186 | best_loss=0.03168
Epoch 51/80: current_loss=0.03180 | best_loss=0.03168
Epoch 52/80: current_loss=0.03181 | best_loss=0.03168
Epoch 53/80: current_loss=0.03222 | best_loss=0.03168
Epoch 54/80: current_loss=0.03196 | best_loss=0.03168
Epoch 55/80: current_loss=0.03196 | best_loss=0.03168
Epoch 56/80: current_loss=0.03184 | best_loss=0.03168
Early Stopping at epoch 56
      explained_var=0.00188 | mse_loss=0.03243
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=0.00721| avg_loss=0.02865
----------------------------------------------


----------------------------------------------
Params for Trial 22
{'learning_rate': 0.001, 'weight_decay': 0.0022456672580756015, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03059 | best_loss=0.03059
Epoch 1/80: current_loss=0.03224 | best_loss=0.03059
Epoch 2/80: current_loss=0.02909 | best_loss=0.02909
Epoch 3/80: current_loss=0.02786 | best_loss=0.02786
Epoch 4/80: current_loss=0.02786 | best_loss=0.02786
Epoch 5/80: current_loss=0.02773 | best_loss=0.02773
Epoch 6/80: current_loss=0.02737 | best_loss=0.02737
Epoch 7/80: current_loss=0.02797 | best_loss=0.02737
Epoch 8/80: current_loss=0.02761 | best_loss=0.02737
Epoch 9/80: current_loss=0.03240 | best_loss=0.02737
Epoch 10/80: current_loss=0.03432 | best_loss=0.02737
Epoch 11/80: current_loss=0.03238 | best_loss=0.02737
Epoch 12/80: current_loss=0.02739 | best_loss=0.02737
Epoch 13/80: current_loss=0.02596 | best_loss=0.02596
Epoch 14/80: current_loss=0.02641 | best_loss=0.02596
Epoch 15/80: current_loss=0.02648 | best_loss=0.02596
Epoch 16/80: current_loss=0.02746 | best_loss=0.02596
Epoch 17/80: current_loss=0.02804 | best_loss=0.02596
Epoch 18/80: current_loss=0.03546 | best_loss=0.02596
Epoch 19/80: current_loss=0.02574 | best_loss=0.02574
Epoch 20/80: current_loss=0.02582 | best_loss=0.02574
Epoch 21/80: current_loss=0.02582 | best_loss=0.02574
Epoch 22/80: current_loss=0.02705 | best_loss=0.02574
Epoch 23/80: current_loss=0.03012 | best_loss=0.02574
Epoch 24/80: current_loss=0.02611 | best_loss=0.02574
Epoch 25/80: current_loss=0.02596 | best_loss=0.02574
Epoch 26/80: current_loss=0.02877 | best_loss=0.02574
Epoch 27/80: current_loss=0.02610 | best_loss=0.02574
Epoch 28/80: current_loss=0.03303 | best_loss=0.02574
Epoch 29/80: current_loss=0.02625 | best_loss=0.02574
Epoch 30/80: current_loss=0.02614 | best_loss=0.02574
Epoch 31/80: current_loss=0.02909 | best_loss=0.02574
Epoch 32/80: current_loss=0.02842 | best_loss=0.02574
Epoch 33/80: current_loss=0.02699 | best_loss=0.02574
Epoch 34/80: current_loss=0.02803 | best_loss=0.02574
Epoch 35/80: current_loss=0.02727 | best_loss=0.02574
Epoch 36/80: current_loss=0.02718 | best_loss=0.02574
Epoch 37/80: current_loss=0.02616 | best_loss=0.02574
Epoch 38/80: current_loss=0.02612 | best_loss=0.02574
Epoch 39/80: current_loss=0.03462 | best_loss=0.02574
Early Stopping at epoch 39
      explained_var=0.03310 | mse_loss=0.02526
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02910 | best_loss=0.02910
Epoch 1/80: current_loss=0.05345 | best_loss=0.02910
Epoch 2/80: current_loss=0.02796 | best_loss=0.02796
Epoch 3/80: current_loss=0.02812 | best_loss=0.02796
Epoch 4/80: current_loss=0.03368 | best_loss=0.02796
Epoch 5/80: current_loss=0.02867 | best_loss=0.02796
Epoch 6/80: current_loss=0.03058 | best_loss=0.02796
Epoch 7/80: current_loss=0.02842 | best_loss=0.02796
Epoch 8/80: current_loss=0.02923 | best_loss=0.02796
Epoch 9/80: current_loss=0.02816 | best_loss=0.02796
Epoch 10/80: current_loss=0.02886 | best_loss=0.02796
Epoch 11/80: current_loss=0.02925 | best_loss=0.02796
Epoch 12/80: current_loss=0.03026 | best_loss=0.02796
Epoch 13/80: current_loss=0.02809 | best_loss=0.02796
Epoch 14/80: current_loss=0.02871 | best_loss=0.02796
Epoch 15/80: current_loss=0.02891 | best_loss=0.02796
Epoch 16/80: current_loss=0.02927 | best_loss=0.02796
Epoch 17/80: current_loss=0.02899 | best_loss=0.02796
Epoch 18/80: current_loss=0.02951 | best_loss=0.02796
Epoch 19/80: current_loss=0.02871 | best_loss=0.02796
Epoch 20/80: current_loss=0.03067 | best_loss=0.02796
Epoch 21/80: current_loss=0.02950 | best_loss=0.02796
Epoch 22/80: current_loss=0.03146 | best_loss=0.02796
Early Stopping at epoch 22
      explained_var=0.01293 | mse_loss=0.02745
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03207 | best_loss=0.03207
Epoch 1/80: current_loss=0.03488 | best_loss=0.03207
Epoch 2/80: current_loss=0.03059 | best_loss=0.03059
Epoch 3/80: current_loss=0.03234 | best_loss=0.03059
Epoch 4/80: current_loss=0.03305 | best_loss=0.03059
Epoch 5/80: current_loss=0.03259 | best_loss=0.03059
Epoch 6/80: current_loss=0.03371 | best_loss=0.03059
Epoch 7/80: current_loss=0.03088 | best_loss=0.03059
Epoch 8/80: current_loss=0.03051 | best_loss=0.03051
Epoch 9/80: current_loss=0.03081 | best_loss=0.03051
Epoch 10/80: current_loss=0.03159 | best_loss=0.03051
Epoch 11/80: current_loss=0.03030 | best_loss=0.03030
Epoch 12/80: current_loss=0.03063 | best_loss=0.03030
Epoch 13/80: current_loss=0.03023 | best_loss=0.03023
Epoch 14/80: current_loss=0.03046 | best_loss=0.03023
Epoch 15/80: current_loss=0.03044 | best_loss=0.03023
Epoch 16/80: current_loss=0.03356 | best_loss=0.03023
Epoch 17/80: current_loss=0.03162 | best_loss=0.03023
Epoch 18/80: current_loss=0.03173 | best_loss=0.03023
Epoch 19/80: current_loss=0.03074 | best_loss=0.03023
Epoch 20/80: current_loss=0.03105 | best_loss=0.03023
Epoch 21/80: current_loss=0.03218 | best_loss=0.03023
Epoch 22/80: current_loss=0.03126 | best_loss=0.03023
Epoch 23/80: current_loss=0.03580 | best_loss=0.03023
Epoch 24/80: current_loss=0.03214 | best_loss=0.03023
Epoch 25/80: current_loss=0.03482 | best_loss=0.03023
Epoch 26/80: current_loss=0.03059 | best_loss=0.03023
Epoch 27/80: current_loss=0.03054 | best_loss=0.03023
Epoch 28/80: current_loss=0.03120 | best_loss=0.03023
Epoch 29/80: current_loss=0.03224 | best_loss=0.03023
Epoch 30/80: current_loss=0.03066 | best_loss=0.03023
Epoch 31/80: current_loss=0.03035 | best_loss=0.03023
Epoch 32/80: current_loss=0.03043 | best_loss=0.03023
Epoch 33/80: current_loss=0.03048 | best_loss=0.03023
Early Stopping at epoch 33
      explained_var=0.01021 | mse_loss=0.02943
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02860 | best_loss=0.02860
Epoch 1/80: current_loss=0.02870 | best_loss=0.02860
Epoch 2/80: current_loss=0.03087 | best_loss=0.02860
Epoch 3/80: current_loss=0.02935 | best_loss=0.02860
Epoch 4/80: current_loss=0.03184 | best_loss=0.02860
Epoch 5/80: current_loss=0.02832 | best_loss=0.02832
Epoch 6/80: current_loss=0.02831 | best_loss=0.02831
Epoch 7/80: current_loss=0.02868 | best_loss=0.02831
Epoch 8/80: current_loss=0.02804 | best_loss=0.02804
Epoch 9/80: current_loss=0.02917 | best_loss=0.02804
Epoch 10/80: current_loss=0.02887 | best_loss=0.02804
Epoch 11/80: current_loss=0.03118 | best_loss=0.02804
Epoch 12/80: current_loss=0.02832 | best_loss=0.02804
Epoch 13/80: current_loss=0.02886 | best_loss=0.02804
Epoch 14/80: current_loss=0.02921 | best_loss=0.02804
Epoch 15/80: current_loss=0.03022 | best_loss=0.02804
Epoch 16/80: current_loss=0.03451 | best_loss=0.02804
Epoch 17/80: current_loss=0.02816 | best_loss=0.02804
Epoch 18/80: current_loss=0.03016 | best_loss=0.02804
Epoch 19/80: current_loss=0.03273 | best_loss=0.02804
Epoch 20/80: current_loss=0.02774 | best_loss=0.02774
Epoch 21/80: current_loss=0.02961 | best_loss=0.02774
Epoch 22/80: current_loss=0.03101 | best_loss=0.02774
Epoch 23/80: current_loss=0.02837 | best_loss=0.02774
Epoch 24/80: current_loss=0.02820 | best_loss=0.02774
Epoch 25/80: current_loss=0.02781 | best_loss=0.02774
Epoch 26/80: current_loss=0.02868 | best_loss=0.02774
Epoch 27/80: current_loss=0.02915 | best_loss=0.02774
Epoch 28/80: current_loss=0.02931 | best_loss=0.02774
Epoch 29/80: current_loss=0.03055 | best_loss=0.02774
Epoch 30/80: current_loss=0.02797 | best_loss=0.02774
Epoch 31/80: current_loss=0.02884 | best_loss=0.02774
Epoch 32/80: current_loss=0.02807 | best_loss=0.02774
Epoch 33/80: current_loss=0.03013 | best_loss=0.02774
Epoch 34/80: current_loss=0.03272 | best_loss=0.02774
Epoch 35/80: current_loss=0.02830 | best_loss=0.02774
Epoch 36/80: current_loss=0.02853 | best_loss=0.02774
Epoch 37/80: current_loss=0.02818 | best_loss=0.02774
Epoch 38/80: current_loss=0.02840 | best_loss=0.02774
Epoch 39/80: current_loss=0.02804 | best_loss=0.02774
Epoch 40/80: current_loss=0.02845 | best_loss=0.02774
Early Stopping at epoch 40
      explained_var=0.00777 | mse_loss=0.02816
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03204 | best_loss=0.03204
Epoch 1/80: current_loss=0.03211 | best_loss=0.03204
Epoch 2/80: current_loss=0.03295 | best_loss=0.03204
Epoch 3/80: current_loss=0.03226 | best_loss=0.03204
Epoch 4/80: current_loss=0.03311 | best_loss=0.03204
Epoch 5/80: current_loss=0.03243 | best_loss=0.03204
Epoch 6/80: current_loss=0.03317 | best_loss=0.03204
Epoch 7/80: current_loss=0.03309 | best_loss=0.03204
Epoch 8/80: current_loss=0.03347 | best_loss=0.03204
Epoch 9/80: current_loss=0.03245 | best_loss=0.03204
Epoch 10/80: current_loss=0.03274 | best_loss=0.03204
Epoch 11/80: current_loss=0.03350 | best_loss=0.03204
Epoch 12/80: current_loss=0.03365 | best_loss=0.03204
Epoch 13/80: current_loss=0.03361 | best_loss=0.03204
Epoch 14/80: current_loss=0.03380 | best_loss=0.03204
Epoch 15/80: current_loss=0.03263 | best_loss=0.03204
Epoch 16/80: current_loss=0.03252 | best_loss=0.03204
Epoch 17/80: current_loss=0.03327 | best_loss=0.03204
Epoch 18/80: current_loss=0.03209 | best_loss=0.03204
Epoch 19/80: current_loss=0.03352 | best_loss=0.03204
Epoch 20/80: current_loss=0.03318 | best_loss=0.03204
Early Stopping at epoch 20
      explained_var=-0.00269 | mse_loss=0.03286
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.01227| avg_loss=0.02863
----------------------------------------------


----------------------------------------------
Params for Trial 23
{'learning_rate': 0.001, 'weight_decay': 0.0012424860992348644, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02772 | best_loss=0.02772
Epoch 1/80: current_loss=0.03582 | best_loss=0.02772
Epoch 2/80: current_loss=0.02812 | best_loss=0.02772
Epoch 3/80: current_loss=0.04135 | best_loss=0.02772
Epoch 4/80: current_loss=0.02720 | best_loss=0.02720
Epoch 5/80: current_loss=0.02762 | best_loss=0.02720
Epoch 6/80: current_loss=0.02619 | best_loss=0.02619
Epoch 7/80: current_loss=0.03042 | best_loss=0.02619
Epoch 8/80: current_loss=0.02797 | best_loss=0.02619
Epoch 9/80: current_loss=0.03029 | best_loss=0.02619
Epoch 10/80: current_loss=0.02772 | best_loss=0.02619
Epoch 11/80: current_loss=0.02753 | best_loss=0.02619
Epoch 12/80: current_loss=0.02699 | best_loss=0.02619
Epoch 13/80: current_loss=0.02995 | best_loss=0.02619
Epoch 14/80: current_loss=0.02918 | best_loss=0.02619
Epoch 15/80: current_loss=0.03021 | best_loss=0.02619
Epoch 16/80: current_loss=0.02678 | best_loss=0.02619
Epoch 17/80: current_loss=0.02506 | best_loss=0.02506
Epoch 18/80: current_loss=0.02626 | best_loss=0.02506
Epoch 19/80: current_loss=0.02688 | best_loss=0.02506
Epoch 20/80: current_loss=0.02713 | best_loss=0.02506
Epoch 21/80: current_loss=0.02670 | best_loss=0.02506
Epoch 22/80: current_loss=0.03467 | best_loss=0.02506
Epoch 23/80: current_loss=0.03032 | best_loss=0.02506
Epoch 24/80: current_loss=0.02708 | best_loss=0.02506
Epoch 25/80: current_loss=0.02691 | best_loss=0.02506
Epoch 26/80: current_loss=0.02904 | best_loss=0.02506
Epoch 27/80: current_loss=0.03001 | best_loss=0.02506
Epoch 28/80: current_loss=0.02627 | best_loss=0.02506
Epoch 29/80: current_loss=0.03271 | best_loss=0.02506
Epoch 30/80: current_loss=0.03355 | best_loss=0.02506
Epoch 31/80: current_loss=0.02653 | best_loss=0.02506
Epoch 32/80: current_loss=0.02649 | best_loss=0.02506
Epoch 33/80: current_loss=0.02596 | best_loss=0.02506
Epoch 34/80: current_loss=0.02545 | best_loss=0.02506
Epoch 35/80: current_loss=0.02612 | best_loss=0.02506
Epoch 36/80: current_loss=0.02627 | best_loss=0.02506
Epoch 37/80: current_loss=0.02601 | best_loss=0.02506
Early Stopping at epoch 37
      explained_var=0.05158 | mse_loss=0.02458
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02799 | best_loss=0.02799
Epoch 1/80: current_loss=0.03410 | best_loss=0.02799
Epoch 2/80: current_loss=0.03408 | best_loss=0.02799
Epoch 3/80: current_loss=0.02952 | best_loss=0.02799
Epoch 4/80: current_loss=0.03132 | best_loss=0.02799
Epoch 5/80: current_loss=0.03165 | best_loss=0.02799
Epoch 6/80: current_loss=0.02787 | best_loss=0.02787
Epoch 7/80: current_loss=0.02978 | best_loss=0.02787
Epoch 8/80: current_loss=0.03183 | best_loss=0.02787
Epoch 9/80: current_loss=0.02767 | best_loss=0.02767
Epoch 10/80: current_loss=0.02817 | best_loss=0.02767
Epoch 11/80: current_loss=0.03405 | best_loss=0.02767
Epoch 12/80: current_loss=0.02786 | best_loss=0.02767
Epoch 13/80: current_loss=0.02806 | best_loss=0.02767
Epoch 14/80: current_loss=0.02930 | best_loss=0.02767
Epoch 15/80: current_loss=0.02791 | best_loss=0.02767
Epoch 16/80: current_loss=0.02916 | best_loss=0.02767
Epoch 17/80: current_loss=0.03255 | best_loss=0.02767
Epoch 18/80: current_loss=0.03177 | best_loss=0.02767
Epoch 19/80: current_loss=0.02805 | best_loss=0.02767
Epoch 20/80: current_loss=0.02780 | best_loss=0.02767
Epoch 21/80: current_loss=0.02891 | best_loss=0.02767
Epoch 22/80: current_loss=0.02899 | best_loss=0.02767
Epoch 23/80: current_loss=0.02840 | best_loss=0.02767
Epoch 24/80: current_loss=0.02829 | best_loss=0.02767
Epoch 25/80: current_loss=0.02775 | best_loss=0.02767
Epoch 26/80: current_loss=0.02827 | best_loss=0.02767
Epoch 27/80: current_loss=0.03074 | best_loss=0.02767
Epoch 28/80: current_loss=0.02801 | best_loss=0.02767
Epoch 29/80: current_loss=0.02835 | best_loss=0.02767
Early Stopping at epoch 29
      explained_var=0.02483 | mse_loss=0.02716
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03253 | best_loss=0.03253
Epoch 1/80: current_loss=0.03209 | best_loss=0.03209
Epoch 2/80: current_loss=0.03405 | best_loss=0.03209
Epoch 3/80: current_loss=0.03012 | best_loss=0.03012
Epoch 4/80: current_loss=0.02931 | best_loss=0.02931
Epoch 5/80: current_loss=0.02953 | best_loss=0.02931
Epoch 6/80: current_loss=0.02939 | best_loss=0.02931
Epoch 7/80: current_loss=0.02993 | best_loss=0.02931
Epoch 8/80: current_loss=0.03019 | best_loss=0.02931
Epoch 9/80: current_loss=0.03329 | best_loss=0.02931
Epoch 10/80: current_loss=0.03148 | best_loss=0.02931
Epoch 11/80: current_loss=0.02989 | best_loss=0.02931
Epoch 12/80: current_loss=0.03766 | best_loss=0.02931
Epoch 13/80: current_loss=0.03186 | best_loss=0.02931
Epoch 14/80: current_loss=0.03033 | best_loss=0.02931
Epoch 15/80: current_loss=0.02967 | best_loss=0.02931
Epoch 16/80: current_loss=0.02988 | best_loss=0.02931
Epoch 17/80: current_loss=0.02986 | best_loss=0.02931
Epoch 18/80: current_loss=0.03233 | best_loss=0.02931
Epoch 19/80: current_loss=0.03082 | best_loss=0.02931
Epoch 20/80: current_loss=0.03013 | best_loss=0.02931
Epoch 21/80: current_loss=0.03699 | best_loss=0.02931
Epoch 22/80: current_loss=0.03092 | best_loss=0.02931
Epoch 23/80: current_loss=0.03059 | best_loss=0.02931
Epoch 24/80: current_loss=0.03082 | best_loss=0.02931
Early Stopping at epoch 24
      explained_var=0.03356 | mse_loss=0.02859
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02929 | best_loss=0.02929
Epoch 1/80: current_loss=0.02839 | best_loss=0.02839
Epoch 2/80: current_loss=0.03140 | best_loss=0.02839
Epoch 3/80: current_loss=0.02834 | best_loss=0.02834
Epoch 4/80: current_loss=0.02984 | best_loss=0.02834
Epoch 5/80: current_loss=0.02835 | best_loss=0.02834
Epoch 6/80: current_loss=0.02870 | best_loss=0.02834
Epoch 7/80: current_loss=0.03207 | best_loss=0.02834
Epoch 8/80: current_loss=0.02964 | best_loss=0.02834
Epoch 9/80: current_loss=0.02908 | best_loss=0.02834
Epoch 10/80: current_loss=0.02865 | best_loss=0.02834
Epoch 11/80: current_loss=0.02945 | best_loss=0.02834
Epoch 12/80: current_loss=0.02981 | best_loss=0.02834
Epoch 13/80: current_loss=0.03158 | best_loss=0.02834
Epoch 14/80: current_loss=0.02906 | best_loss=0.02834
Epoch 15/80: current_loss=0.02899 | best_loss=0.02834
Epoch 16/80: current_loss=0.02918 | best_loss=0.02834
Epoch 17/80: current_loss=0.02946 | best_loss=0.02834
Epoch 18/80: current_loss=0.02834 | best_loss=0.02834
Epoch 19/80: current_loss=0.03027 | best_loss=0.02834
Epoch 20/80: current_loss=0.02814 | best_loss=0.02814
Epoch 21/80: current_loss=0.02825 | best_loss=0.02814
Epoch 22/80: current_loss=0.02864 | best_loss=0.02814
Epoch 23/80: current_loss=0.02838 | best_loss=0.02814
Epoch 24/80: current_loss=0.02871 | best_loss=0.02814
Epoch 25/80: current_loss=0.02867 | best_loss=0.02814
Epoch 26/80: current_loss=0.02826 | best_loss=0.02814
Epoch 27/80: current_loss=0.03370 | best_loss=0.02814
Epoch 28/80: current_loss=0.02875 | best_loss=0.02814
Epoch 29/80: current_loss=0.02917 | best_loss=0.02814
Epoch 30/80: current_loss=0.03231 | best_loss=0.02814
Epoch 31/80: current_loss=0.02857 | best_loss=0.02814
Epoch 32/80: current_loss=0.02837 | best_loss=0.02814
Epoch 33/80: current_loss=0.02839 | best_loss=0.02814
Epoch 34/80: current_loss=0.02814 | best_loss=0.02814
Epoch 35/80: current_loss=0.02815 | best_loss=0.02814
Epoch 36/80: current_loss=0.03016 | best_loss=0.02814
Epoch 37/80: current_loss=0.02883 | best_loss=0.02814
Epoch 38/80: current_loss=0.03246 | best_loss=0.02814
Epoch 39/80: current_loss=0.02848 | best_loss=0.02814
Epoch 40/80: current_loss=0.02938 | best_loss=0.02814
Epoch 41/80: current_loss=0.02882 | best_loss=0.02814
Epoch 42/80: current_loss=0.02916 | best_loss=0.02814
Epoch 43/80: current_loss=0.02821 | best_loss=0.02814
Epoch 44/80: current_loss=0.02852 | best_loss=0.02814
Epoch 45/80: current_loss=0.02924 | best_loss=0.02814
Epoch 46/80: current_loss=0.02890 | best_loss=0.02814
Epoch 47/80: current_loss=0.02926 | best_loss=0.02814
Epoch 48/80: current_loss=0.02884 | best_loss=0.02814
Epoch 49/80: current_loss=0.03009 | best_loss=0.02814
Epoch 50/80: current_loss=0.02868 | best_loss=0.02814
Epoch 51/80: current_loss=0.02831 | best_loss=0.02814
Epoch 52/80: current_loss=0.02837 | best_loss=0.02814
Epoch 53/80: current_loss=0.02838 | best_loss=0.02814
Epoch 54/80: current_loss=0.02822 | best_loss=0.02814
Early Stopping at epoch 54
      explained_var=-0.00431 | mse_loss=0.02849
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03559 | best_loss=0.03559
Epoch 1/80: current_loss=0.03445 | best_loss=0.03445
Epoch 2/80: current_loss=0.03762 | best_loss=0.03445
Epoch 3/80: current_loss=0.03345 | best_loss=0.03345
Epoch 4/80: current_loss=0.03493 | best_loss=0.03345
Epoch 5/80: current_loss=0.03449 | best_loss=0.03345
Epoch 6/80: current_loss=0.03453 | best_loss=0.03345
Epoch 7/80: current_loss=0.03361 | best_loss=0.03345
Epoch 8/80: current_loss=0.03699 | best_loss=0.03345
Epoch 9/80: current_loss=0.03361 | best_loss=0.03345
Epoch 10/80: current_loss=0.03555 | best_loss=0.03345
Epoch 11/80: current_loss=0.03570 | best_loss=0.03345
Epoch 12/80: current_loss=0.03424 | best_loss=0.03345
Epoch 13/80: current_loss=0.03573 | best_loss=0.03345
Epoch 14/80: current_loss=0.03357 | best_loss=0.03345
Epoch 15/80: current_loss=0.03601 | best_loss=0.03345
Epoch 16/80: current_loss=0.03369 | best_loss=0.03345
Epoch 17/80: current_loss=0.03640 | best_loss=0.03345
Epoch 18/80: current_loss=0.03362 | best_loss=0.03345
Epoch 19/80: current_loss=0.03404 | best_loss=0.03345
Epoch 20/80: current_loss=0.03295 | best_loss=0.03295
Epoch 21/80: current_loss=0.03338 | best_loss=0.03295
Epoch 22/80: current_loss=0.03399 | best_loss=0.03295
Epoch 23/80: current_loss=0.03397 | best_loss=0.03295
Epoch 24/80: current_loss=0.03399 | best_loss=0.03295
Epoch 25/80: current_loss=0.03431 | best_loss=0.03295
Epoch 26/80: current_loss=0.03407 | best_loss=0.03295
Epoch 27/80: current_loss=0.03851 | best_loss=0.03295
Epoch 28/80: current_loss=0.03317 | best_loss=0.03295
Epoch 29/80: current_loss=0.03491 | best_loss=0.03295
Epoch 30/80: current_loss=0.03392 | best_loss=0.03295
Epoch 31/80: current_loss=0.03360 | best_loss=0.03295
Epoch 32/80: current_loss=0.03429 | best_loss=0.03295
Epoch 33/80: current_loss=0.03404 | best_loss=0.03295
Epoch 34/80: current_loss=0.03359 | best_loss=0.03295
Epoch 35/80: current_loss=0.03383 | best_loss=0.03295
Epoch 36/80: current_loss=0.03360 | best_loss=0.03295
Epoch 37/80: current_loss=0.03354 | best_loss=0.03295
Epoch 38/80: current_loss=0.03516 | best_loss=0.03295
Epoch 39/80: current_loss=0.03372 | best_loss=0.03295
Epoch 40/80: current_loss=0.03461 | best_loss=0.03295
Early Stopping at epoch 40
      explained_var=-0.03834 | mse_loss=0.03374
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.01346| avg_loss=0.02852
----------------------------------------------


----------------------------------------------
Params for Trial 24
{'learning_rate': 1e-05, 'weight_decay': 0.00033148048247052674, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.15601 | best_loss=0.15601
Epoch 1/80: current_loss=0.07933 | best_loss=0.07933
Epoch 2/80: current_loss=0.03902 | best_loss=0.03902
Epoch 3/80: current_loss=0.03741 | best_loss=0.03741
Epoch 4/80: current_loss=0.03855 | best_loss=0.03741
Epoch 5/80: current_loss=0.03498 | best_loss=0.03498
Epoch 6/80: current_loss=0.03376 | best_loss=0.03376
Epoch 7/80: current_loss=0.03354 | best_loss=0.03354
Epoch 8/80: current_loss=0.03383 | best_loss=0.03354
Epoch 9/80: current_loss=0.03310 | best_loss=0.03310
Epoch 10/80: current_loss=0.03264 | best_loss=0.03264
Epoch 11/80: current_loss=0.03212 | best_loss=0.03212
Epoch 12/80: current_loss=0.03248 | best_loss=0.03212
Epoch 13/80: current_loss=0.03142 | best_loss=0.03142
Epoch 14/80: current_loss=0.03128 | best_loss=0.03128
Epoch 15/80: current_loss=0.03110 | best_loss=0.03110
Epoch 16/80: current_loss=0.03185 | best_loss=0.03110
Epoch 17/80: current_loss=0.03087 | best_loss=0.03087
Epoch 18/80: current_loss=0.03022 | best_loss=0.03022
Epoch 19/80: current_loss=0.03053 | best_loss=0.03022
Epoch 20/80: current_loss=0.03058 | best_loss=0.03022
Epoch 21/80: current_loss=0.02993 | best_loss=0.02993
Epoch 22/80: current_loss=0.02967 | best_loss=0.02967
Epoch 23/80: current_loss=0.03002 | best_loss=0.02967
Epoch 24/80: current_loss=0.03012 | best_loss=0.02967
Epoch 25/80: current_loss=0.02977 | best_loss=0.02967
Epoch 26/80: current_loss=0.02953 | best_loss=0.02953
Epoch 27/80: current_loss=0.02952 | best_loss=0.02952
Epoch 28/80: current_loss=0.02983 | best_loss=0.02952
Epoch 29/80: current_loss=0.02920 | best_loss=0.02920
Epoch 30/80: current_loss=0.02909 | best_loss=0.02909
Epoch 31/80: current_loss=0.02884 | best_loss=0.02884
Epoch 32/80: current_loss=0.02922 | best_loss=0.02884
Epoch 33/80: current_loss=0.02960 | best_loss=0.02884
Epoch 34/80: current_loss=0.02888 | best_loss=0.02884
Epoch 35/80: current_loss=0.02904 | best_loss=0.02884
Epoch 36/80: current_loss=0.02886 | best_loss=0.02884
Epoch 37/80: current_loss=0.02813 | best_loss=0.02813
Epoch 38/80: current_loss=0.02840 | best_loss=0.02813
Epoch 39/80: current_loss=0.02860 | best_loss=0.02813
Epoch 40/80: current_loss=0.02864 | best_loss=0.02813
Epoch 41/80: current_loss=0.02756 | best_loss=0.02756
Epoch 42/80: current_loss=0.02807 | best_loss=0.02756
Epoch 43/80: current_loss=0.02901 | best_loss=0.02756
Epoch 44/80: current_loss=0.02828 | best_loss=0.02756
Epoch 45/80: current_loss=0.02772 | best_loss=0.02756
Epoch 46/80: current_loss=0.02828 | best_loss=0.02756
Epoch 47/80: current_loss=0.02891 | best_loss=0.02756
Epoch 48/80: current_loss=0.02776 | best_loss=0.02756
Epoch 49/80: current_loss=0.02824 | best_loss=0.02756
Epoch 50/80: current_loss=0.02825 | best_loss=0.02756
Epoch 51/80: current_loss=0.02733 | best_loss=0.02733
Epoch 52/80: current_loss=0.02859 | best_loss=0.02733
Epoch 53/80: current_loss=0.02815 | best_loss=0.02733
Epoch 54/80: current_loss=0.02817 | best_loss=0.02733
Epoch 55/80: current_loss=0.02784 | best_loss=0.02733
Epoch 56/80: current_loss=0.02771 | best_loss=0.02733
Epoch 57/80: current_loss=0.02737 | best_loss=0.02733
Epoch 58/80: current_loss=0.02704 | best_loss=0.02704
Epoch 59/80: current_loss=0.02734 | best_loss=0.02704
Epoch 60/80: current_loss=0.02788 | best_loss=0.02704
Epoch 61/80: current_loss=0.02761 | best_loss=0.02704
Epoch 62/80: current_loss=0.02738 | best_loss=0.02704
Epoch 63/80: current_loss=0.02719 | best_loss=0.02704
Epoch 64/80: current_loss=0.02746 | best_loss=0.02704
Epoch 65/80: current_loss=0.02758 | best_loss=0.02704
Epoch 66/80: current_loss=0.02708 | best_loss=0.02704
Epoch 67/80: current_loss=0.02729 | best_loss=0.02704
Epoch 68/80: current_loss=0.02808 | best_loss=0.02704
Epoch 69/80: current_loss=0.02713 | best_loss=0.02704
Epoch 70/80: current_loss=0.02632 | best_loss=0.02632
Epoch 71/80: current_loss=0.02712 | best_loss=0.02632
Epoch 72/80: current_loss=0.02765 | best_loss=0.02632
Epoch 73/80: current_loss=0.02681 | best_loss=0.02632
Epoch 74/80: current_loss=0.02639 | best_loss=0.02632
Epoch 75/80: current_loss=0.02664 | best_loss=0.02632
Epoch 76/80: current_loss=0.02695 | best_loss=0.02632
Epoch 77/80: current_loss=0.02710 | best_loss=0.02632
Epoch 78/80: current_loss=0.02616 | best_loss=0.02616
Epoch 79/80: current_loss=0.02642 | best_loss=0.02616
      explained_var=0.02503 | mse_loss=0.02570

----------------------------------------------
Params for Trial 25
{'learning_rate': 0.001, 'weight_decay': 0.0037026164096195507, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03091 | best_loss=0.03091
Epoch 1/80: current_loss=0.02901 | best_loss=0.02901
Epoch 2/80: current_loss=0.02868 | best_loss=0.02868
Epoch 3/80: current_loss=0.02640 | best_loss=0.02640
Epoch 4/80: current_loss=0.02604 | best_loss=0.02604
Epoch 5/80: current_loss=0.03028 | best_loss=0.02604
Epoch 6/80: current_loss=0.02622 | best_loss=0.02604
Epoch 7/80: current_loss=0.02577 | best_loss=0.02577
Epoch 8/80: current_loss=0.02644 | best_loss=0.02577
Epoch 9/80: current_loss=0.02624 | best_loss=0.02577
Epoch 10/80: current_loss=0.02610 | best_loss=0.02577
Epoch 11/80: current_loss=0.02762 | best_loss=0.02577
Epoch 12/80: current_loss=0.02636 | best_loss=0.02577
Epoch 13/80: current_loss=0.02565 | best_loss=0.02565
Epoch 14/80: current_loss=0.02587 | best_loss=0.02565
Epoch 15/80: current_loss=0.02730 | best_loss=0.02565
Epoch 16/80: current_loss=0.02668 | best_loss=0.02565
Epoch 17/80: current_loss=0.02595 | best_loss=0.02565
Epoch 18/80: current_loss=0.02644 | best_loss=0.02565
Epoch 19/80: current_loss=0.02585 | best_loss=0.02565
Epoch 20/80: current_loss=0.03081 | best_loss=0.02565
Epoch 21/80: current_loss=0.02608 | best_loss=0.02565
Epoch 22/80: current_loss=0.02669 | best_loss=0.02565
Epoch 23/80: current_loss=0.02961 | best_loss=0.02565
Epoch 24/80: current_loss=0.02621 | best_loss=0.02565
Epoch 25/80: current_loss=0.02736 | best_loss=0.02565
Epoch 26/80: current_loss=0.02898 | best_loss=0.02565
Epoch 27/80: current_loss=0.02687 | best_loss=0.02565
Epoch 28/80: current_loss=0.02635 | best_loss=0.02565
Epoch 29/80: current_loss=0.02676 | best_loss=0.02565
Epoch 30/80: current_loss=0.02651 | best_loss=0.02565
Epoch 31/80: current_loss=0.02636 | best_loss=0.02565
Epoch 32/80: current_loss=0.02786 | best_loss=0.02565
Epoch 33/80: current_loss=0.02770 | best_loss=0.02565
Early Stopping at epoch 33
      explained_var=0.02918 | mse_loss=0.02508
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02836 | best_loss=0.02836
Epoch 1/80: current_loss=0.02856 | best_loss=0.02836
Epoch 2/80: current_loss=0.02836 | best_loss=0.02836
Epoch 3/80: current_loss=0.02956 | best_loss=0.02836
Epoch 4/80: current_loss=0.03004 | best_loss=0.02836
Epoch 5/80: current_loss=0.02847 | best_loss=0.02836
Epoch 6/80: current_loss=0.02859 | best_loss=0.02836
Epoch 7/80: current_loss=0.02875 | best_loss=0.02836
Epoch 8/80: current_loss=0.02850 | best_loss=0.02836
Epoch 9/80: current_loss=0.02851 | best_loss=0.02836
Epoch 10/80: current_loss=0.02847 | best_loss=0.02836
Epoch 11/80: current_loss=0.02843 | best_loss=0.02836
Epoch 12/80: current_loss=0.02843 | best_loss=0.02836
Epoch 13/80: current_loss=0.02851 | best_loss=0.02836
Epoch 14/80: current_loss=0.02887 | best_loss=0.02836
Epoch 15/80: current_loss=0.02963 | best_loss=0.02836
Epoch 16/80: current_loss=0.02858 | best_loss=0.02836
Epoch 17/80: current_loss=0.02871 | best_loss=0.02836
Epoch 18/80: current_loss=0.02966 | best_loss=0.02836
Epoch 19/80: current_loss=0.02834 | best_loss=0.02834
Epoch 20/80: current_loss=0.02843 | best_loss=0.02834
Epoch 21/80: current_loss=0.02903 | best_loss=0.02834
Epoch 22/80: current_loss=0.02838 | best_loss=0.02834
Epoch 23/80: current_loss=0.03217 | best_loss=0.02834
Epoch 24/80: current_loss=0.02842 | best_loss=0.02834
Epoch 25/80: current_loss=0.02838 | best_loss=0.02834
Epoch 26/80: current_loss=0.02942 | best_loss=0.02834
Epoch 27/80: current_loss=0.02891 | best_loss=0.02834
Epoch 28/80: current_loss=0.02842 | best_loss=0.02834
Epoch 29/80: current_loss=0.02936 | best_loss=0.02834
Epoch 30/80: current_loss=0.02835 | best_loss=0.02834
Epoch 31/80: current_loss=0.02837 | best_loss=0.02834
Epoch 32/80: current_loss=0.02939 | best_loss=0.02834
Epoch 33/80: current_loss=0.02847 | best_loss=0.02834
Epoch 34/80: current_loss=0.02854 | best_loss=0.02834
Epoch 35/80: current_loss=0.02914 | best_loss=0.02834
Epoch 36/80: current_loss=0.02874 | best_loss=0.02834
Epoch 37/80: current_loss=0.02835 | best_loss=0.02834
Epoch 38/80: current_loss=0.02835 | best_loss=0.02834
Epoch 39/80: current_loss=0.02842 | best_loss=0.02834
Early Stopping at epoch 39
      explained_var=-0.00033 | mse_loss=0.02785
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03363 | best_loss=0.03363
Epoch 1/80: current_loss=0.03075 | best_loss=0.03075
Epoch 2/80: current_loss=0.03173 | best_loss=0.03075
Epoch 3/80: current_loss=0.03122 | best_loss=0.03075
Epoch 4/80: current_loss=0.03355 | best_loss=0.03075
Epoch 5/80: current_loss=0.03133 | best_loss=0.03075
Epoch 6/80: current_loss=0.03047 | best_loss=0.03047
Epoch 7/80: current_loss=0.03209 | best_loss=0.03047
Epoch 8/80: current_loss=0.03140 | best_loss=0.03047
Epoch 9/80: current_loss=0.03049 | best_loss=0.03047
Epoch 10/80: current_loss=0.03712 | best_loss=0.03047
Epoch 11/80: current_loss=0.03047 | best_loss=0.03047
Epoch 12/80: current_loss=0.03054 | best_loss=0.03047
Epoch 13/80: current_loss=0.03135 | best_loss=0.03047
Epoch 14/80: current_loss=0.03114 | best_loss=0.03047
Epoch 15/80: current_loss=0.03420 | best_loss=0.03047
Epoch 16/80: current_loss=0.03055 | best_loss=0.03047
Epoch 17/80: current_loss=0.03144 | best_loss=0.03047
Epoch 18/80: current_loss=0.03156 | best_loss=0.03047
Epoch 19/80: current_loss=0.03132 | best_loss=0.03047
Epoch 20/80: current_loss=0.03070 | best_loss=0.03047
Epoch 21/80: current_loss=0.03182 | best_loss=0.03047
Epoch 22/80: current_loss=0.03049 | best_loss=0.03047
Epoch 23/80: current_loss=0.03281 | best_loss=0.03047
Epoch 24/80: current_loss=0.03103 | best_loss=0.03047
Epoch 25/80: current_loss=0.03340 | best_loss=0.03047
Epoch 26/80: current_loss=0.03057 | best_loss=0.03047
Epoch 27/80: current_loss=0.03125 | best_loss=0.03047
Epoch 28/80: current_loss=0.03143 | best_loss=0.03047
Epoch 29/80: current_loss=0.03149 | best_loss=0.03047
Epoch 30/80: current_loss=0.03149 | best_loss=0.03047
Epoch 31/80: current_loss=0.03066 | best_loss=0.03047
Early Stopping at epoch 31
      explained_var=0.00061 | mse_loss=0.02969
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02830 | best_loss=0.02830
Epoch 1/80: current_loss=0.02890 | best_loss=0.02830
Epoch 2/80: current_loss=0.02823 | best_loss=0.02823
Epoch 3/80: current_loss=0.02799 | best_loss=0.02799
Epoch 4/80: current_loss=0.02830 | best_loss=0.02799
Epoch 5/80: current_loss=0.02797 | best_loss=0.02797
Epoch 6/80: current_loss=0.02827 | best_loss=0.02797
Epoch 7/80: current_loss=0.02815 | best_loss=0.02797
Epoch 8/80: current_loss=0.02810 | best_loss=0.02797
Epoch 9/80: current_loss=0.02830 | best_loss=0.02797
Epoch 10/80: current_loss=0.02813 | best_loss=0.02797
Epoch 11/80: current_loss=0.02884 | best_loss=0.02797
Epoch 12/80: current_loss=0.02814 | best_loss=0.02797
Epoch 13/80: current_loss=0.02804 | best_loss=0.02797
Epoch 14/80: current_loss=0.02829 | best_loss=0.02797
Epoch 15/80: current_loss=0.02854 | best_loss=0.02797
Epoch 16/80: current_loss=0.02809 | best_loss=0.02797
Epoch 17/80: current_loss=0.02822 | best_loss=0.02797
Epoch 18/80: current_loss=0.02819 | best_loss=0.02797
Epoch 19/80: current_loss=0.02801 | best_loss=0.02797
Epoch 20/80: current_loss=0.02838 | best_loss=0.02797
Epoch 21/80: current_loss=0.02807 | best_loss=0.02797
Epoch 22/80: current_loss=0.02823 | best_loss=0.02797
Epoch 23/80: current_loss=0.02810 | best_loss=0.02797
Epoch 24/80: current_loss=0.02803 | best_loss=0.02797
Epoch 25/80: current_loss=0.02812 | best_loss=0.02797
Early Stopping at epoch 25
      explained_var=0.00136 | mse_loss=0.02833
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03172 | best_loss=0.03172
Epoch 1/80: current_loss=0.03169 | best_loss=0.03169
Epoch 2/80: current_loss=0.03170 | best_loss=0.03169
Epoch 3/80: current_loss=0.03189 | best_loss=0.03169
Epoch 4/80: current_loss=0.03171 | best_loss=0.03169
Epoch 5/80: current_loss=0.03173 | best_loss=0.03169
Epoch 6/80: current_loss=0.03169 | best_loss=0.03169
Epoch 7/80: current_loss=0.03171 | best_loss=0.03169
Epoch 8/80: current_loss=0.03174 | best_loss=0.03169
Epoch 9/80: current_loss=0.03174 | best_loss=0.03169
Epoch 10/80: current_loss=0.03174 | best_loss=0.03169
Epoch 11/80: current_loss=0.03188 | best_loss=0.03169
Epoch 12/80: current_loss=0.03193 | best_loss=0.03169
Epoch 13/80: current_loss=0.03186 | best_loss=0.03169
Epoch 14/80: current_loss=0.03175 | best_loss=0.03169
Epoch 15/80: current_loss=0.03177 | best_loss=0.03169
Epoch 16/80: current_loss=0.03177 | best_loss=0.03169
Epoch 17/80: current_loss=0.03200 | best_loss=0.03169
Epoch 18/80: current_loss=0.03191 | best_loss=0.03169
Epoch 19/80: current_loss=0.03182 | best_loss=0.03169
Epoch 20/80: current_loss=0.03191 | best_loss=0.03169
Epoch 21/80: current_loss=0.03177 | best_loss=0.03169
Epoch 22/80: current_loss=0.03180 | best_loss=0.03169
Epoch 23/80: current_loss=0.03174 | best_loss=0.03169
Epoch 24/80: current_loss=0.03177 | best_loss=0.03169
Epoch 25/80: current_loss=0.03183 | best_loss=0.03169
Epoch 26/80: current_loss=0.03177 | best_loss=0.03169
Early Stopping at epoch 26
      explained_var=0.00166 | mse_loss=0.03244
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.00650| avg_loss=0.02868
----------------------------------------------


----------------------------------------------
Params for Trial 26
{'learning_rate': 0.01, 'weight_decay': 0.006814869993263544, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03507 | best_loss=0.03507
Epoch 1/80: current_loss=0.02705 | best_loss=0.02705
Epoch 2/80: current_loss=0.02951 | best_loss=0.02705
Epoch 3/80: current_loss=0.02634 | best_loss=0.02634
Epoch 4/80: current_loss=0.02588 | best_loss=0.02588
Epoch 5/80: current_loss=0.02620 | best_loss=0.02588
Epoch 6/80: current_loss=0.02804 | best_loss=0.02588
Epoch 7/80: current_loss=0.03172 | best_loss=0.02588
Epoch 8/80: current_loss=0.02717 | best_loss=0.02588
Epoch 9/80: current_loss=0.02847 | best_loss=0.02588
Epoch 10/80: current_loss=0.02691 | best_loss=0.02588
Epoch 11/80: current_loss=0.02641 | best_loss=0.02588
Epoch 12/80: current_loss=0.03073 | best_loss=0.02588
Epoch 13/80: current_loss=0.02646 | best_loss=0.02588
Epoch 14/80: current_loss=0.02694 | best_loss=0.02588
Epoch 15/80: current_loss=0.02746 | best_loss=0.02588
Epoch 16/80: current_loss=0.02811 | best_loss=0.02588
Epoch 17/80: current_loss=0.02777 | best_loss=0.02588
Epoch 18/80: current_loss=0.02645 | best_loss=0.02588
Epoch 19/80: current_loss=0.02963 | best_loss=0.02588
Epoch 20/80: current_loss=0.02695 | best_loss=0.02588
Epoch 21/80: current_loss=0.02721 | best_loss=0.02588
Epoch 22/80: current_loss=0.02842 | best_loss=0.02588
Epoch 23/80: current_loss=0.02661 | best_loss=0.02588
Epoch 24/80: current_loss=0.02766 | best_loss=0.02588
Early Stopping at epoch 24
      explained_var=0.02099 | mse_loss=0.02533
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02878 | best_loss=0.02878
Epoch 1/80: current_loss=0.02863 | best_loss=0.02863
Epoch 2/80: current_loss=0.02844 | best_loss=0.02844
Epoch 3/80: current_loss=0.02838 | best_loss=0.02838
Epoch 4/80: current_loss=0.02840 | best_loss=0.02838
Epoch 5/80: current_loss=0.02840 | best_loss=0.02838
Epoch 6/80: current_loss=0.02829 | best_loss=0.02829
Epoch 7/80: current_loss=0.02834 | best_loss=0.02829
Epoch 8/80: current_loss=0.02840 | best_loss=0.02829
Epoch 9/80: current_loss=0.02829 | best_loss=0.02829
Epoch 10/80: current_loss=0.02832 | best_loss=0.02829
Epoch 11/80: current_loss=0.02836 | best_loss=0.02829
Epoch 12/80: current_loss=0.02844 | best_loss=0.02829
Epoch 13/80: current_loss=0.02836 | best_loss=0.02829
Epoch 14/80: current_loss=0.02843 | best_loss=0.02829
Epoch 15/80: current_loss=0.02829 | best_loss=0.02829
Epoch 16/80: current_loss=0.02831 | best_loss=0.02829
Epoch 17/80: current_loss=0.02830 | best_loss=0.02829
Epoch 18/80: current_loss=0.02835 | best_loss=0.02829
Epoch 19/80: current_loss=0.02830 | best_loss=0.02829
Epoch 20/80: current_loss=0.02835 | best_loss=0.02829
Epoch 21/80: current_loss=0.02829 | best_loss=0.02829
Epoch 22/80: current_loss=0.02863 | best_loss=0.02829
Epoch 23/80: current_loss=0.02832 | best_loss=0.02829
Epoch 24/80: current_loss=0.02850 | best_loss=0.02829
Epoch 25/80: current_loss=0.02831 | best_loss=0.02829
Epoch 26/80: current_loss=0.02844 | best_loss=0.02829
Early Stopping at epoch 26
      explained_var=0.00002 | mse_loss=0.02779
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03047 | best_loss=0.03047
Epoch 1/80: current_loss=0.03176 | best_loss=0.03047
Epoch 2/80: current_loss=0.03150 | best_loss=0.03047
Epoch 3/80: current_loss=0.03156 | best_loss=0.03047
Epoch 4/80: current_loss=0.03126 | best_loss=0.03047
Epoch 5/80: current_loss=0.03127 | best_loss=0.03047
Epoch 6/80: current_loss=0.03185 | best_loss=0.03047
Epoch 7/80: current_loss=0.03147 | best_loss=0.03047
Epoch 8/80: current_loss=0.03077 | best_loss=0.03047
Epoch 9/80: current_loss=0.03158 | best_loss=0.03047
Epoch 10/80: current_loss=0.03110 | best_loss=0.03047
Epoch 11/80: current_loss=0.03086 | best_loss=0.03047
Epoch 12/80: current_loss=0.03199 | best_loss=0.03047
Epoch 13/80: current_loss=0.03057 | best_loss=0.03047
Epoch 14/80: current_loss=0.03175 | best_loss=0.03047
Epoch 15/80: current_loss=0.03092 | best_loss=0.03047
Epoch 16/80: current_loss=0.03224 | best_loss=0.03047
Epoch 17/80: current_loss=0.03063 | best_loss=0.03047
Epoch 18/80: current_loss=0.03137 | best_loss=0.03047
Epoch 19/80: current_loss=0.03149 | best_loss=0.03047
Epoch 20/80: current_loss=0.03140 | best_loss=0.03047
Early Stopping at epoch 20
      explained_var=-0.00073 | mse_loss=0.02968
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02823 | best_loss=0.02823
Epoch 1/80: current_loss=0.02822 | best_loss=0.02822
Epoch 2/80: current_loss=0.02804 | best_loss=0.02804
Epoch 3/80: current_loss=0.02805 | best_loss=0.02804
Epoch 4/80: current_loss=0.02800 | best_loss=0.02800
Epoch 5/80: current_loss=0.02801 | best_loss=0.02800
Epoch 6/80: current_loss=0.02801 | best_loss=0.02800
Epoch 7/80: current_loss=0.02803 | best_loss=0.02800
Epoch 8/80: current_loss=0.02800 | best_loss=0.02800
Epoch 9/80: current_loss=0.02807 | best_loss=0.02800
Epoch 10/80: current_loss=0.02814 | best_loss=0.02800
Epoch 11/80: current_loss=0.02812 | best_loss=0.02800
Epoch 12/80: current_loss=0.02800 | best_loss=0.02800
Epoch 13/80: current_loss=0.02807 | best_loss=0.02800
Epoch 14/80: current_loss=0.02806 | best_loss=0.02800
Epoch 15/80: current_loss=0.02801 | best_loss=0.02800
Epoch 16/80: current_loss=0.02799 | best_loss=0.02799
Epoch 17/80: current_loss=0.02802 | best_loss=0.02799
Epoch 18/80: current_loss=0.02822 | best_loss=0.02799
Epoch 19/80: current_loss=0.02813 | best_loss=0.02799
Epoch 20/80: current_loss=0.02801 | best_loss=0.02799
Epoch 21/80: current_loss=0.02850 | best_loss=0.02799
Epoch 22/80: current_loss=0.02801 | best_loss=0.02799
Epoch 23/80: current_loss=0.02810 | best_loss=0.02799
Epoch 24/80: current_loss=0.02800 | best_loss=0.02799
Epoch 25/80: current_loss=0.02801 | best_loss=0.02799
Epoch 26/80: current_loss=0.02800 | best_loss=0.02799
Epoch 27/80: current_loss=0.02801 | best_loss=0.02799
Epoch 28/80: current_loss=0.02800 | best_loss=0.02799
Epoch 29/80: current_loss=0.02825 | best_loss=0.02799
Epoch 30/80: current_loss=0.02804 | best_loss=0.02799
Epoch 31/80: current_loss=0.02805 | best_loss=0.02799
Epoch 32/80: current_loss=0.02810 | best_loss=0.02799
Epoch 33/80: current_loss=0.02803 | best_loss=0.02799
Epoch 34/80: current_loss=0.02802 | best_loss=0.02799
Epoch 35/80: current_loss=0.02810 | best_loss=0.02799
Epoch 36/80: current_loss=0.02801 | best_loss=0.02799
Early Stopping at epoch 36
      explained_var=0.00014 | mse_loss=0.02836
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03179 | best_loss=0.03179
Epoch 1/80: current_loss=0.03221 | best_loss=0.03179
Epoch 2/80: current_loss=0.03200 | best_loss=0.03179
Epoch 3/80: current_loss=0.03190 | best_loss=0.03179
Epoch 4/80: current_loss=0.03191 | best_loss=0.03179
Epoch 5/80: current_loss=0.03176 | best_loss=0.03176
Epoch 6/80: current_loss=0.03197 | best_loss=0.03176
Epoch 7/80: current_loss=0.03175 | best_loss=0.03175
Epoch 8/80: current_loss=0.03178 | best_loss=0.03175
Epoch 9/80: current_loss=0.03176 | best_loss=0.03175
Epoch 10/80: current_loss=0.03175 | best_loss=0.03175
Epoch 11/80: current_loss=0.03178 | best_loss=0.03175
Epoch 12/80: current_loss=0.03178 | best_loss=0.03175
Epoch 13/80: current_loss=0.03178 | best_loss=0.03175
Epoch 14/80: current_loss=0.03176 | best_loss=0.03175
Epoch 15/80: current_loss=0.03176 | best_loss=0.03175
Epoch 16/80: current_loss=0.03176 | best_loss=0.03175
Epoch 17/80: current_loss=0.03182 | best_loss=0.03175
Epoch 18/80: current_loss=0.03184 | best_loss=0.03175
Epoch 19/80: current_loss=0.03180 | best_loss=0.03175
Epoch 20/80: current_loss=0.03178 | best_loss=0.03175
Epoch 21/80: current_loss=0.03183 | best_loss=0.03175
Epoch 22/80: current_loss=0.03178 | best_loss=0.03175
Epoch 23/80: current_loss=0.03183 | best_loss=0.03175
Epoch 24/80: current_loss=0.03183 | best_loss=0.03175
Epoch 25/80: current_loss=0.03176 | best_loss=0.03175
Epoch 26/80: current_loss=0.03177 | best_loss=0.03175
Epoch 27/80: current_loss=0.03177 | best_loss=0.03175
Epoch 28/80: current_loss=0.03194 | best_loss=0.03175
Epoch 29/80: current_loss=0.03234 | best_loss=0.03175
Epoch 30/80: current_loss=0.03180 | best_loss=0.03175
Early Stopping at epoch 30
      explained_var=0.00002 | mse_loss=0.03248
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=0.00409| avg_loss=0.02873
----------------------------------------------


----------------------------------------------
Params for Trial 27
{'learning_rate': 0.001, 'weight_decay': 3.347012310558863e-05, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03313 | best_loss=0.03313
Epoch 1/80: current_loss=0.03141 | best_loss=0.03141
Epoch 2/80: current_loss=0.02772 | best_loss=0.02772
Epoch 3/80: current_loss=0.02927 | best_loss=0.02772
Epoch 4/80: current_loss=0.02664 | best_loss=0.02664
Epoch 5/80: current_loss=0.02924 | best_loss=0.02664
Epoch 6/80: current_loss=0.02662 | best_loss=0.02662
Epoch 7/80: current_loss=0.02944 | best_loss=0.02662
Epoch 8/80: current_loss=0.02573 | best_loss=0.02573
Epoch 9/80: current_loss=0.02614 | best_loss=0.02573
Epoch 10/80: current_loss=0.02660 | best_loss=0.02573
Epoch 11/80: current_loss=0.02713 | best_loss=0.02573
Epoch 12/80: current_loss=0.02560 | best_loss=0.02560
Epoch 13/80: current_loss=0.02517 | best_loss=0.02517
Epoch 14/80: current_loss=0.03231 | best_loss=0.02517
Epoch 15/80: current_loss=0.02563 | best_loss=0.02517
Epoch 16/80: current_loss=0.02484 | best_loss=0.02484
Epoch 17/80: current_loss=0.02852 | best_loss=0.02484
Epoch 18/80: current_loss=0.02518 | best_loss=0.02484
Epoch 19/80: current_loss=0.02544 | best_loss=0.02484
Epoch 20/80: current_loss=0.02478 | best_loss=0.02478
Epoch 21/80: current_loss=0.02692 | best_loss=0.02478
Epoch 22/80: current_loss=0.02490 | best_loss=0.02478
Epoch 23/80: current_loss=0.02506 | best_loss=0.02478
Epoch 24/80: current_loss=0.02593 | best_loss=0.02478
Epoch 25/80: current_loss=0.02498 | best_loss=0.02478
Epoch 26/80: current_loss=0.02656 | best_loss=0.02478
Epoch 27/80: current_loss=0.02527 | best_loss=0.02478
Epoch 28/80: current_loss=0.02512 | best_loss=0.02478
Epoch 29/80: current_loss=0.02604 | best_loss=0.02478
Epoch 30/80: current_loss=0.02999 | best_loss=0.02478
Epoch 31/80: current_loss=0.02602 | best_loss=0.02478
Epoch 32/80: current_loss=0.02511 | best_loss=0.02478
Epoch 33/80: current_loss=0.02678 | best_loss=0.02478
Epoch 34/80: current_loss=0.02586 | best_loss=0.02478
Epoch 35/80: current_loss=0.02661 | best_loss=0.02478
Epoch 36/80: current_loss=0.02699 | best_loss=0.02478
Epoch 37/80: current_loss=0.02857 | best_loss=0.02478
Epoch 38/80: current_loss=0.02566 | best_loss=0.02478
Epoch 39/80: current_loss=0.02837 | best_loss=0.02478
Epoch 40/80: current_loss=0.02545 | best_loss=0.02478
Early Stopping at epoch 40
      explained_var=0.05916 | mse_loss=0.02433
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02940 | best_loss=0.02940
Epoch 1/80: current_loss=0.02924 | best_loss=0.02924
Epoch 2/80: current_loss=0.02915 | best_loss=0.02915
Epoch 3/80: current_loss=0.03015 | best_loss=0.02915
Epoch 4/80: current_loss=0.02829 | best_loss=0.02829
Epoch 5/80: current_loss=0.02854 | best_loss=0.02829
Epoch 6/80: current_loss=0.02871 | best_loss=0.02829
Epoch 7/80: current_loss=0.02848 | best_loss=0.02829
Epoch 8/80: current_loss=0.02803 | best_loss=0.02803
Epoch 9/80: current_loss=0.02916 | best_loss=0.02803
Epoch 10/80: current_loss=0.02880 | best_loss=0.02803
Epoch 11/80: current_loss=0.02908 | best_loss=0.02803
Epoch 12/80: current_loss=0.02873 | best_loss=0.02803
Epoch 13/80: current_loss=0.02966 | best_loss=0.02803
Epoch 14/80: current_loss=0.02789 | best_loss=0.02789
Epoch 15/80: current_loss=0.02927 | best_loss=0.02789
Epoch 16/80: current_loss=0.02806 | best_loss=0.02789
Epoch 17/80: current_loss=0.02798 | best_loss=0.02789
Epoch 18/80: current_loss=0.02864 | best_loss=0.02789
Epoch 19/80: current_loss=0.02770 | best_loss=0.02770
Epoch 20/80: current_loss=0.02779 | best_loss=0.02770
Epoch 21/80: current_loss=0.02921 | best_loss=0.02770
Epoch 22/80: current_loss=0.02762 | best_loss=0.02762
Epoch 23/80: current_loss=0.02754 | best_loss=0.02754
Epoch 24/80: current_loss=0.02772 | best_loss=0.02754
Epoch 25/80: current_loss=0.02805 | best_loss=0.02754
Epoch 26/80: current_loss=0.02811 | best_loss=0.02754
Epoch 27/80: current_loss=0.02820 | best_loss=0.02754
Epoch 28/80: current_loss=0.02786 | best_loss=0.02754
Epoch 29/80: current_loss=0.02782 | best_loss=0.02754
Epoch 30/80: current_loss=0.02872 | best_loss=0.02754
Epoch 31/80: current_loss=0.02859 | best_loss=0.02754
Epoch 32/80: current_loss=0.02783 | best_loss=0.02754
Epoch 33/80: current_loss=0.02782 | best_loss=0.02754
Epoch 34/80: current_loss=0.02744 | best_loss=0.02744
Epoch 35/80: current_loss=0.02750 | best_loss=0.02744
Epoch 36/80: current_loss=0.02765 | best_loss=0.02744
Epoch 37/80: current_loss=0.02775 | best_loss=0.02744
Epoch 38/80: current_loss=0.02754 | best_loss=0.02744
Epoch 39/80: current_loss=0.02778 | best_loss=0.02744
Epoch 40/80: current_loss=0.02755 | best_loss=0.02744
Epoch 41/80: current_loss=0.02837 | best_loss=0.02744
Epoch 42/80: current_loss=0.02799 | best_loss=0.02744
Epoch 43/80: current_loss=0.02882 | best_loss=0.02744
Epoch 44/80: current_loss=0.02825 | best_loss=0.02744
Epoch 45/80: current_loss=0.02825 | best_loss=0.02744
Epoch 46/80: current_loss=0.02833 | best_loss=0.02744
Epoch 47/80: current_loss=0.02829 | best_loss=0.02744
Epoch 48/80: current_loss=0.03006 | best_loss=0.02744
Epoch 49/80: current_loss=0.02823 | best_loss=0.02744
Epoch 50/80: current_loss=0.02872 | best_loss=0.02744
Epoch 51/80: current_loss=0.02818 | best_loss=0.02744
Epoch 52/80: current_loss=0.02872 | best_loss=0.02744
Epoch 53/80: current_loss=0.02832 | best_loss=0.02744
Epoch 54/80: current_loss=0.02962 | best_loss=0.02744
Early Stopping at epoch 54
      explained_var=0.02890 | mse_loss=0.02698
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03158 | best_loss=0.03158
Epoch 1/80: current_loss=0.03092 | best_loss=0.03092
Epoch 2/80: current_loss=0.03037 | best_loss=0.03037
Epoch 3/80: current_loss=0.03104 | best_loss=0.03037
Epoch 4/80: current_loss=0.03112 | best_loss=0.03037
Epoch 5/80: current_loss=0.03084 | best_loss=0.03037
Epoch 6/80: current_loss=0.03061 | best_loss=0.03037
Epoch 7/80: current_loss=0.03158 | best_loss=0.03037
Epoch 8/80: current_loss=0.03072 | best_loss=0.03037
Epoch 9/80: current_loss=0.03051 | best_loss=0.03037
Epoch 10/80: current_loss=0.03130 | best_loss=0.03037
Epoch 11/80: current_loss=0.03173 | best_loss=0.03037
Epoch 12/80: current_loss=0.03054 | best_loss=0.03037
Epoch 13/80: current_loss=0.02968 | best_loss=0.02968
Epoch 14/80: current_loss=0.03327 | best_loss=0.02968
Epoch 15/80: current_loss=0.03082 | best_loss=0.02968
Epoch 16/80: current_loss=0.03079 | best_loss=0.02968
Epoch 17/80: current_loss=0.02965 | best_loss=0.02965
Epoch 18/80: current_loss=0.03098 | best_loss=0.02965
Epoch 19/80: current_loss=0.03200 | best_loss=0.02965
Epoch 20/80: current_loss=0.03082 | best_loss=0.02965
Epoch 21/80: current_loss=0.03024 | best_loss=0.02965
Epoch 22/80: current_loss=0.03097 | best_loss=0.02965
Epoch 23/80: current_loss=0.03086 | best_loss=0.02965
Epoch 24/80: current_loss=0.03166 | best_loss=0.02965
Epoch 25/80: current_loss=0.03024 | best_loss=0.02965
Epoch 26/80: current_loss=0.03198 | best_loss=0.02965
Epoch 27/80: current_loss=0.03013 | best_loss=0.02965
Epoch 28/80: current_loss=0.03015 | best_loss=0.02965
Epoch 29/80: current_loss=0.03126 | best_loss=0.02965
Epoch 30/80: current_loss=0.03030 | best_loss=0.02965
Epoch 31/80: current_loss=0.03063 | best_loss=0.02965
Epoch 32/80: current_loss=0.03068 | best_loss=0.02965
Epoch 33/80: current_loss=0.03026 | best_loss=0.02965
Epoch 34/80: current_loss=0.03178 | best_loss=0.02965
Epoch 35/80: current_loss=0.03030 | best_loss=0.02965
Epoch 36/80: current_loss=0.03012 | best_loss=0.02965
Epoch 37/80: current_loss=0.03012 | best_loss=0.02965
Early Stopping at epoch 37
      explained_var=0.02536 | mse_loss=0.02886
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02766 | best_loss=0.02766
Epoch 1/80: current_loss=0.02822 | best_loss=0.02766
Epoch 2/80: current_loss=0.02785 | best_loss=0.02766
Epoch 3/80: current_loss=0.02793 | best_loss=0.02766
Epoch 4/80: current_loss=0.02951 | best_loss=0.02766
Epoch 5/80: current_loss=0.02864 | best_loss=0.02766
Epoch 6/80: current_loss=0.02847 | best_loss=0.02766
Epoch 7/80: current_loss=0.02839 | best_loss=0.02766
Epoch 8/80: current_loss=0.02905 | best_loss=0.02766
Epoch 9/80: current_loss=0.02809 | best_loss=0.02766
Epoch 10/80: current_loss=0.02810 | best_loss=0.02766
Epoch 11/80: current_loss=0.02801 | best_loss=0.02766
Epoch 12/80: current_loss=0.02826 | best_loss=0.02766
Epoch 13/80: current_loss=0.02851 | best_loss=0.02766
Epoch 14/80: current_loss=0.02860 | best_loss=0.02766
Epoch 15/80: current_loss=0.02888 | best_loss=0.02766
Epoch 16/80: current_loss=0.02849 | best_loss=0.02766
Epoch 17/80: current_loss=0.02849 | best_loss=0.02766
Epoch 18/80: current_loss=0.02881 | best_loss=0.02766
Epoch 19/80: current_loss=0.02860 | best_loss=0.02766
Epoch 20/80: current_loss=0.02902 | best_loss=0.02766
Early Stopping at epoch 20
      explained_var=0.01262 | mse_loss=0.02801
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03464 | best_loss=0.03464
Epoch 1/80: current_loss=0.03326 | best_loss=0.03326
Epoch 2/80: current_loss=0.03329 | best_loss=0.03326
Epoch 3/80: current_loss=0.03333 | best_loss=0.03326
Epoch 4/80: current_loss=0.03384 | best_loss=0.03326
Epoch 5/80: current_loss=0.03364 | best_loss=0.03326
Epoch 6/80: current_loss=0.03357 | best_loss=0.03326
Epoch 7/80: current_loss=0.03336 | best_loss=0.03326
Epoch 8/80: current_loss=0.03354 | best_loss=0.03326
Epoch 9/80: current_loss=0.03341 | best_loss=0.03326
Epoch 10/80: current_loss=0.03357 | best_loss=0.03326
Epoch 11/80: current_loss=0.03450 | best_loss=0.03326
Epoch 12/80: current_loss=0.03384 | best_loss=0.03326
Epoch 13/80: current_loss=0.03377 | best_loss=0.03326
Epoch 14/80: current_loss=0.03382 | best_loss=0.03326
Epoch 15/80: current_loss=0.03327 | best_loss=0.03326
Epoch 16/80: current_loss=0.03346 | best_loss=0.03326
Epoch 17/80: current_loss=0.03311 | best_loss=0.03311
Epoch 18/80: current_loss=0.03325 | best_loss=0.03311
Epoch 19/80: current_loss=0.03395 | best_loss=0.03311
Epoch 20/80: current_loss=0.03387 | best_loss=0.03311
Epoch 21/80: current_loss=0.03337 | best_loss=0.03311
Epoch 22/80: current_loss=0.03316 | best_loss=0.03311
Epoch 23/80: current_loss=0.03381 | best_loss=0.03311
Epoch 24/80: current_loss=0.03391 | best_loss=0.03311
Epoch 25/80: current_loss=0.03361 | best_loss=0.03311
Epoch 26/80: current_loss=0.03372 | best_loss=0.03311
Epoch 27/80: current_loss=0.03386 | best_loss=0.03311
Epoch 28/80: current_loss=0.03395 | best_loss=0.03311
Epoch 29/80: current_loss=0.03400 | best_loss=0.03311
Epoch 30/80: current_loss=0.03344 | best_loss=0.03311
Epoch 31/80: current_loss=0.03354 | best_loss=0.03311
Epoch 32/80: current_loss=0.03398 | best_loss=0.03311
Epoch 33/80: current_loss=0.03458 | best_loss=0.03311
Epoch 34/80: current_loss=0.03429 | best_loss=0.03311
Epoch 35/80: current_loss=0.03361 | best_loss=0.03311
Epoch 36/80: current_loss=0.03310 | best_loss=0.03310
Epoch 37/80: current_loss=0.03327 | best_loss=0.03310
Epoch 38/80: current_loss=0.03359 | best_loss=0.03310
Epoch 39/80: current_loss=0.03338 | best_loss=0.03310
Epoch 40/80: current_loss=0.03330 | best_loss=0.03310
Epoch 41/80: current_loss=0.03326 | best_loss=0.03310
Epoch 42/80: current_loss=0.03376 | best_loss=0.03310
Epoch 43/80: current_loss=0.03400 | best_loss=0.03310
Epoch 44/80: current_loss=0.03425 | best_loss=0.03310
Epoch 45/80: current_loss=0.03407 | best_loss=0.03310
Epoch 46/80: current_loss=0.03384 | best_loss=0.03310
Epoch 47/80: current_loss=0.03361 | best_loss=0.03310
Epoch 48/80: current_loss=0.03353 | best_loss=0.03310
Epoch 49/80: current_loss=0.03365 | best_loss=0.03310
Epoch 50/80: current_loss=0.03372 | best_loss=0.03310
Epoch 51/80: current_loss=0.03369 | best_loss=0.03310
Epoch 52/80: current_loss=0.03387 | best_loss=0.03310
Epoch 53/80: current_loss=0.03370 | best_loss=0.03310
Epoch 54/80: current_loss=0.03389 | best_loss=0.03310
Epoch 55/80: current_loss=0.03356 | best_loss=0.03310
Epoch 56/80: current_loss=0.03343 | best_loss=0.03310
Early Stopping at epoch 56
      explained_var=-0.04008 | mse_loss=0.03388
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.01719| avg_loss=0.02841
----------------------------------------------


----------------------------------------------
Params for Trial 28
{'learning_rate': 0.001, 'weight_decay': 0.002048741649980118, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03974 | best_loss=0.03974
Epoch 1/80: current_loss=0.03594 | best_loss=0.03594
Epoch 2/80: current_loss=0.02822 | best_loss=0.02822
Epoch 3/80: current_loss=0.02553 | best_loss=0.02553
Epoch 4/80: current_loss=0.02544 | best_loss=0.02544
Epoch 5/80: current_loss=0.02526 | best_loss=0.02526
Epoch 6/80: current_loss=0.03480 | best_loss=0.02526
Epoch 7/80: current_loss=0.03076 | best_loss=0.02526
Epoch 8/80: current_loss=0.02710 | best_loss=0.02526
Epoch 9/80: current_loss=0.02797 | best_loss=0.02526
Epoch 10/80: current_loss=0.02836 | best_loss=0.02526
Epoch 11/80: current_loss=0.02802 | best_loss=0.02526
Epoch 12/80: current_loss=0.02881 | best_loss=0.02526
Epoch 13/80: current_loss=0.03078 | best_loss=0.02526
Epoch 14/80: current_loss=0.02630 | best_loss=0.02526
Epoch 15/80: current_loss=0.02662 | best_loss=0.02526
Epoch 16/80: current_loss=0.02608 | best_loss=0.02526
Epoch 17/80: current_loss=0.02658 | best_loss=0.02526
Epoch 18/80: current_loss=0.02669 | best_loss=0.02526
Epoch 19/80: current_loss=0.02668 | best_loss=0.02526
Epoch 20/80: current_loss=0.02694 | best_loss=0.02526
Epoch 21/80: current_loss=0.02675 | best_loss=0.02526
Epoch 22/80: current_loss=0.03040 | best_loss=0.02526
Epoch 23/80: current_loss=0.02615 | best_loss=0.02526
Epoch 24/80: current_loss=0.02807 | best_loss=0.02526
Epoch 25/80: current_loss=0.02627 | best_loss=0.02526
Early Stopping at epoch 25
      explained_var=0.06288 | mse_loss=0.02477
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03142 | best_loss=0.03142
Epoch 1/80: current_loss=0.02886 | best_loss=0.02886
Epoch 2/80: current_loss=0.02983 | best_loss=0.02886
Epoch 3/80: current_loss=0.02975 | best_loss=0.02886
Epoch 4/80: current_loss=0.02808 | best_loss=0.02808
Epoch 5/80: current_loss=0.02810 | best_loss=0.02808
Epoch 6/80: current_loss=0.03053 | best_loss=0.02808
Epoch 7/80: current_loss=0.02802 | best_loss=0.02802
Epoch 8/80: current_loss=0.02804 | best_loss=0.02802
Epoch 9/80: current_loss=0.02840 | best_loss=0.02802
Epoch 10/80: current_loss=0.02794 | best_loss=0.02794
Epoch 11/80: current_loss=0.02810 | best_loss=0.02794
Epoch 12/80: current_loss=0.03075 | best_loss=0.02794
Epoch 13/80: current_loss=0.03143 | best_loss=0.02794
Epoch 14/80: current_loss=0.02918 | best_loss=0.02794
Epoch 15/80: current_loss=0.03165 | best_loss=0.02794
Epoch 16/80: current_loss=0.02820 | best_loss=0.02794
Epoch 17/80: current_loss=0.02785 | best_loss=0.02785
Epoch 18/80: current_loss=0.02813 | best_loss=0.02785
Epoch 19/80: current_loss=0.03375 | best_loss=0.02785
Epoch 20/80: current_loss=0.02890 | best_loss=0.02785
Epoch 21/80: current_loss=0.02841 | best_loss=0.02785
Epoch 22/80: current_loss=0.03112 | best_loss=0.02785
Epoch 23/80: current_loss=0.03578 | best_loss=0.02785
Epoch 24/80: current_loss=0.02819 | best_loss=0.02785
Epoch 25/80: current_loss=0.02833 | best_loss=0.02785
Epoch 26/80: current_loss=0.02889 | best_loss=0.02785
Epoch 27/80: current_loss=0.02861 | best_loss=0.02785
Epoch 28/80: current_loss=0.03217 | best_loss=0.02785
Epoch 29/80: current_loss=0.02822 | best_loss=0.02785
Epoch 30/80: current_loss=0.02826 | best_loss=0.02785
Epoch 31/80: current_loss=0.02893 | best_loss=0.02785
Epoch 32/80: current_loss=0.02862 | best_loss=0.02785
Epoch 33/80: current_loss=0.02958 | best_loss=0.02785
Epoch 34/80: current_loss=0.02839 | best_loss=0.02785
Epoch 35/80: current_loss=0.02830 | best_loss=0.02785
Epoch 36/80: current_loss=0.02831 | best_loss=0.02785
Epoch 37/80: current_loss=0.02841 | best_loss=0.02785
Early Stopping at epoch 37
      explained_var=0.01779 | mse_loss=0.02733
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03927 | best_loss=0.03927
Epoch 1/80: current_loss=0.03342 | best_loss=0.03342
Epoch 2/80: current_loss=0.03150 | best_loss=0.03150
Epoch 3/80: current_loss=0.03191 | best_loss=0.03150
Epoch 4/80: current_loss=0.03238 | best_loss=0.03150
Epoch 5/80: current_loss=0.03020 | best_loss=0.03020
Epoch 6/80: current_loss=0.03056 | best_loss=0.03020
Epoch 7/80: current_loss=0.03217 | best_loss=0.03020
Epoch 8/80: current_loss=0.03324 | best_loss=0.03020
Epoch 9/80: current_loss=0.03116 | best_loss=0.03020
Epoch 10/80: current_loss=0.03045 | best_loss=0.03020
Epoch 11/80: current_loss=0.03138 | best_loss=0.03020
Epoch 12/80: current_loss=0.03039 | best_loss=0.03020
Epoch 13/80: current_loss=0.03393 | best_loss=0.03020
Epoch 14/80: current_loss=0.03327 | best_loss=0.03020
Epoch 15/80: current_loss=0.03077 | best_loss=0.03020
Epoch 16/80: current_loss=0.03059 | best_loss=0.03020
Epoch 17/80: current_loss=0.03202 | best_loss=0.03020
Epoch 18/80: current_loss=0.04004 | best_loss=0.03020
Epoch 19/80: current_loss=0.03405 | best_loss=0.03020
Epoch 20/80: current_loss=0.03130 | best_loss=0.03020
Epoch 21/80: current_loss=0.03056 | best_loss=0.03020
Epoch 22/80: current_loss=0.03071 | best_loss=0.03020
Epoch 23/80: current_loss=0.03055 | best_loss=0.03020
Epoch 24/80: current_loss=0.03048 | best_loss=0.03020
Epoch 25/80: current_loss=0.03258 | best_loss=0.03020
Early Stopping at epoch 25
      explained_var=0.00586 | mse_loss=0.02943
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03854 | best_loss=0.03854
Epoch 1/80: current_loss=0.02879 | best_loss=0.02879
Epoch 2/80: current_loss=0.03020 | best_loss=0.02879
Epoch 3/80: current_loss=0.03170 | best_loss=0.02879
Epoch 4/80: current_loss=0.02950 | best_loss=0.02879
Epoch 5/80: current_loss=0.02820 | best_loss=0.02820
Epoch 6/80: current_loss=0.02827 | best_loss=0.02820
Epoch 7/80: current_loss=0.02819 | best_loss=0.02819
Epoch 8/80: current_loss=0.02942 | best_loss=0.02819
Epoch 9/80: current_loss=0.02843 | best_loss=0.02819
Epoch 10/80: current_loss=0.02894 | best_loss=0.02819
Epoch 11/80: current_loss=0.02809 | best_loss=0.02809
Epoch 12/80: current_loss=0.02888 | best_loss=0.02809
Epoch 13/80: current_loss=0.02860 | best_loss=0.02809
Epoch 14/80: current_loss=0.03152 | best_loss=0.02809
Epoch 15/80: current_loss=0.02941 | best_loss=0.02809
Epoch 16/80: current_loss=0.03000 | best_loss=0.02809
Epoch 17/80: current_loss=0.02818 | best_loss=0.02809
Epoch 18/80: current_loss=0.02818 | best_loss=0.02809
Epoch 19/80: current_loss=0.02812 | best_loss=0.02809
Epoch 20/80: current_loss=0.02965 | best_loss=0.02809
Epoch 21/80: current_loss=0.02817 | best_loss=0.02809
Epoch 22/80: current_loss=0.02807 | best_loss=0.02807
Epoch 23/80: current_loss=0.02979 | best_loss=0.02807
Epoch 24/80: current_loss=0.02834 | best_loss=0.02807
Epoch 25/80: current_loss=0.03067 | best_loss=0.02807
Epoch 26/80: current_loss=0.02877 | best_loss=0.02807
Epoch 27/80: current_loss=0.03012 | best_loss=0.02807
Epoch 28/80: current_loss=0.02960 | best_loss=0.02807
Epoch 29/80: current_loss=0.03105 | best_loss=0.02807
Epoch 30/80: current_loss=0.02886 | best_loss=0.02807
Epoch 31/80: current_loss=0.02894 | best_loss=0.02807
Epoch 32/80: current_loss=0.02942 | best_loss=0.02807
Epoch 33/80: current_loss=0.02819 | best_loss=0.02807
Epoch 34/80: current_loss=0.02900 | best_loss=0.02807
Epoch 35/80: current_loss=0.02816 | best_loss=0.02807
Epoch 36/80: current_loss=0.02814 | best_loss=0.02807
Epoch 37/80: current_loss=0.02822 | best_loss=0.02807
Epoch 38/80: current_loss=0.02799 | best_loss=0.02799
Epoch 39/80: current_loss=0.02908 | best_loss=0.02799
Epoch 40/80: current_loss=0.02814 | best_loss=0.02799
Epoch 41/80: current_loss=0.03011 | best_loss=0.02799
Epoch 42/80: current_loss=0.03081 | best_loss=0.02799
Epoch 43/80: current_loss=0.02874 | best_loss=0.02799
Epoch 44/80: current_loss=0.02828 | best_loss=0.02799
Epoch 45/80: current_loss=0.02843 | best_loss=0.02799
Epoch 46/80: current_loss=0.03124 | best_loss=0.02799
Epoch 47/80: current_loss=0.02816 | best_loss=0.02799
Epoch 48/80: current_loss=0.02803 | best_loss=0.02799
Epoch 49/80: current_loss=0.02834 | best_loss=0.02799
Epoch 50/80: current_loss=0.02848 | best_loss=0.02799
Epoch 51/80: current_loss=0.02815 | best_loss=0.02799
Epoch 52/80: current_loss=0.02854 | best_loss=0.02799
Epoch 53/80: current_loss=0.02815 | best_loss=0.02799
Epoch 54/80: current_loss=0.02816 | best_loss=0.02799
Epoch 55/80: current_loss=0.68608 | best_loss=0.02799
Epoch 56/80: current_loss=0.19301 | best_loss=0.02799
Epoch 57/80: current_loss=0.22098 | best_loss=0.02799
Epoch 58/80: current_loss=0.12251 | best_loss=0.02799
Early Stopping at epoch 58
      explained_var=0.00016 | mse_loss=0.02837
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.08430 | best_loss=0.08430
Epoch 1/80: current_loss=0.04010 | best_loss=0.04010
Epoch 2/80: current_loss=0.03502 | best_loss=0.03502
Epoch 3/80: current_loss=0.05880 | best_loss=0.03502
Epoch 4/80: current_loss=0.04095 | best_loss=0.03502
Epoch 5/80: current_loss=0.03340 | best_loss=0.03340
Epoch 6/80: current_loss=0.05357 | best_loss=0.03340
Epoch 7/80: current_loss=0.03488 | best_loss=0.03340
Epoch 8/80: current_loss=0.03521 | best_loss=0.03340
Epoch 9/80: current_loss=0.03339 | best_loss=0.03339
Epoch 10/80: current_loss=0.03202 | best_loss=0.03202
Epoch 11/80: current_loss=0.03272 | best_loss=0.03202
Epoch 12/80: current_loss=0.03270 | best_loss=0.03202
Epoch 13/80: current_loss=0.03335 | best_loss=0.03202
Epoch 14/80: current_loss=0.03236 | best_loss=0.03202
Epoch 15/80: current_loss=0.03268 | best_loss=0.03202
Epoch 16/80: current_loss=0.03195 | best_loss=0.03195
Epoch 17/80: current_loss=0.03221 | best_loss=0.03195
Epoch 18/80: current_loss=0.03227 | best_loss=0.03195
Epoch 19/80: current_loss=0.03219 | best_loss=0.03195
Epoch 20/80: current_loss=0.03216 | best_loss=0.03195
Epoch 21/80: current_loss=0.03223 | best_loss=0.03195
Epoch 22/80: current_loss=0.03243 | best_loss=0.03195
Epoch 23/80: current_loss=0.03227 | best_loss=0.03195
Epoch 24/80: current_loss=0.03264 | best_loss=0.03195
Epoch 25/80: current_loss=0.03193 | best_loss=0.03193
Epoch 26/80: current_loss=0.03226 | best_loss=0.03193
Epoch 27/80: current_loss=0.03191 | best_loss=0.03191
Epoch 28/80: current_loss=0.03190 | best_loss=0.03190
Epoch 29/80: current_loss=0.03188 | best_loss=0.03188
Epoch 30/80: current_loss=0.03201 | best_loss=0.03188
Epoch 31/80: current_loss=0.03183 | best_loss=0.03183
Epoch 32/80: current_loss=0.03185 | best_loss=0.03183
Epoch 33/80: current_loss=0.03184 | best_loss=0.03183
Epoch 34/80: current_loss=0.03191 | best_loss=0.03183
Epoch 35/80: current_loss=0.03225 | best_loss=0.03183
Epoch 36/80: current_loss=0.03185 | best_loss=0.03183
Epoch 37/80: current_loss=0.03188 | best_loss=0.03183
Epoch 38/80: current_loss=0.03191 | best_loss=0.03183
Epoch 39/80: current_loss=0.03181 | best_loss=0.03181
Epoch 40/80: current_loss=0.03197 | best_loss=0.03181
Epoch 41/80: current_loss=0.03182 | best_loss=0.03181
Epoch 42/80: current_loss=0.03192 | best_loss=0.03181
Epoch 43/80: current_loss=0.03187 | best_loss=0.03181
Epoch 44/80: current_loss=0.03200 | best_loss=0.03181
Epoch 45/80: current_loss=0.03187 | best_loss=0.03181
Epoch 46/80: current_loss=0.03187 | best_loss=0.03181
Epoch 47/80: current_loss=0.03196 | best_loss=0.03181
Epoch 48/80: current_loss=0.03183 | best_loss=0.03181
Epoch 49/80: current_loss=0.03187 | best_loss=0.03181
Epoch 50/80: current_loss=0.03180 | best_loss=0.03180
Epoch 51/80: current_loss=0.03180 | best_loss=0.03180
Epoch 52/80: current_loss=0.03229 | best_loss=0.03180
Epoch 53/80: current_loss=0.03207 | best_loss=0.03180
Epoch 54/80: current_loss=0.03221 | best_loss=0.03180
Epoch 55/80: current_loss=0.03197 | best_loss=0.03180
Epoch 56/80: current_loss=0.03183 | best_loss=0.03180
Epoch 57/80: current_loss=0.03198 | best_loss=0.03180
Epoch 58/80: current_loss=0.03212 | best_loss=0.03180
Epoch 59/80: current_loss=0.03191 | best_loss=0.03180
Epoch 60/80: current_loss=0.03206 | best_loss=0.03180
Epoch 61/80: current_loss=0.03185 | best_loss=0.03180
Epoch 62/80: current_loss=0.03182 | best_loss=0.03180
Epoch 63/80: current_loss=0.03182 | best_loss=0.03180
Epoch 64/80: current_loss=0.03196 | best_loss=0.03180
Epoch 65/80: current_loss=0.03180 | best_loss=0.03180
Epoch 66/80: current_loss=0.03181 | best_loss=0.03180
Epoch 67/80: current_loss=0.03183 | best_loss=0.03180
Epoch 68/80: current_loss=0.03186 | best_loss=0.03180
Epoch 69/80: current_loss=0.03182 | best_loss=0.03180
Epoch 70/80: current_loss=0.03190 | best_loss=0.03180
Epoch 71/80: current_loss=0.03186 | best_loss=0.03180
Epoch 72/80: current_loss=0.03182 | best_loss=0.03180
Epoch 73/80: current_loss=0.03183 | best_loss=0.03180
Epoch 74/80: current_loss=0.03182 | best_loss=0.03180
Epoch 75/80: current_loss=0.03218 | best_loss=0.03180
Epoch 76/80: current_loss=0.03186 | best_loss=0.03180
Epoch 77/80: current_loss=0.03203 | best_loss=0.03180
Epoch 78/80: current_loss=0.03186 | best_loss=0.03180
Epoch 79/80: current_loss=0.03178 | best_loss=0.03178
      explained_var=-0.00060 | mse_loss=0.03250
----------------------------------------------
Average early_stopping_point: 29| avg_exp_var=0.01722| avg_loss=0.02848
----------------------------------------------


----------------------------------------------
Params for Trial 29
{'learning_rate': 0.001, 'weight_decay': 0.0013876994804289905, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03616 | best_loss=0.03616
Epoch 1/80: current_loss=0.03213 | best_loss=0.03213
Epoch 2/80: current_loss=0.03109 | best_loss=0.03109
Epoch 3/80: current_loss=0.02815 | best_loss=0.02815
Epoch 4/80: current_loss=0.02782 | best_loss=0.02782
Epoch 5/80: current_loss=0.02655 | best_loss=0.02655
Epoch 6/80: current_loss=0.02675 | best_loss=0.02655
Epoch 7/80: current_loss=0.02736 | best_loss=0.02655
Epoch 8/80: current_loss=0.02636 | best_loss=0.02636
Epoch 9/80: current_loss=0.02658 | best_loss=0.02636
Epoch 10/80: current_loss=0.02631 | best_loss=0.02631
Epoch 11/80: current_loss=0.02597 | best_loss=0.02597
Epoch 12/80: current_loss=0.02689 | best_loss=0.02597
Epoch 13/80: current_loss=0.02590 | best_loss=0.02590
Epoch 14/80: current_loss=0.02612 | best_loss=0.02590
Epoch 15/80: current_loss=0.02587 | best_loss=0.02587
Epoch 16/80: current_loss=0.02634 | best_loss=0.02587
Epoch 17/80: current_loss=0.02621 | best_loss=0.02587
Epoch 18/80: current_loss=0.02602 | best_loss=0.02587
Epoch 19/80: current_loss=0.02646 | best_loss=0.02587
Epoch 20/80: current_loss=0.02620 | best_loss=0.02587
Epoch 21/80: current_loss=0.02681 | best_loss=0.02587
Epoch 22/80: current_loss=0.02723 | best_loss=0.02587
Epoch 23/80: current_loss=0.02681 | best_loss=0.02587
Epoch 24/80: current_loss=0.02694 | best_loss=0.02587
Epoch 25/80: current_loss=0.02664 | best_loss=0.02587
Epoch 26/80: current_loss=0.02669 | best_loss=0.02587
Epoch 27/80: current_loss=0.02622 | best_loss=0.02587
Epoch 28/80: current_loss=0.02682 | best_loss=0.02587
Epoch 29/80: current_loss=0.02649 | best_loss=0.02587
Epoch 30/80: current_loss=0.02670 | best_loss=0.02587
Epoch 31/80: current_loss=0.02627 | best_loss=0.02587
Epoch 32/80: current_loss=0.02688 | best_loss=0.02587
Epoch 33/80: current_loss=0.02603 | best_loss=0.02587
Epoch 34/80: current_loss=0.02647 | best_loss=0.02587
Epoch 35/80: current_loss=0.02617 | best_loss=0.02587
Early Stopping at epoch 35
      explained_var=0.02568 | mse_loss=0.02543
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02829 | best_loss=0.02829
Epoch 1/80: current_loss=0.02823 | best_loss=0.02823
Epoch 2/80: current_loss=0.02841 | best_loss=0.02823
Epoch 3/80: current_loss=0.02816 | best_loss=0.02816
Epoch 4/80: current_loss=0.02831 | best_loss=0.02816
Epoch 5/80: current_loss=0.02842 | best_loss=0.02816
Epoch 6/80: current_loss=0.02820 | best_loss=0.02816
Epoch 7/80: current_loss=0.02805 | best_loss=0.02805
Epoch 8/80: current_loss=0.02842 | best_loss=0.02805
Epoch 9/80: current_loss=0.02833 | best_loss=0.02805
Epoch 10/80: current_loss=0.02819 | best_loss=0.02805
Epoch 11/80: current_loss=0.02807 | best_loss=0.02805
Epoch 12/80: current_loss=0.02882 | best_loss=0.02805
Epoch 13/80: current_loss=0.02809 | best_loss=0.02805
Epoch 14/80: current_loss=0.02813 | best_loss=0.02805
Epoch 15/80: current_loss=0.02846 | best_loss=0.02805
Epoch 16/80: current_loss=0.02810 | best_loss=0.02805
Epoch 17/80: current_loss=0.02812 | best_loss=0.02805
Epoch 18/80: current_loss=0.02802 | best_loss=0.02802
Epoch 19/80: current_loss=0.02807 | best_loss=0.02802
Epoch 20/80: current_loss=0.02793 | best_loss=0.02793
Epoch 21/80: current_loss=0.02798 | best_loss=0.02793
Epoch 22/80: current_loss=0.02808 | best_loss=0.02793
Epoch 23/80: current_loss=0.02800 | best_loss=0.02793
Epoch 24/80: current_loss=0.02803 | best_loss=0.02793
Epoch 25/80: current_loss=0.02790 | best_loss=0.02790
Epoch 26/80: current_loss=0.02802 | best_loss=0.02790
Epoch 27/80: current_loss=0.02799 | best_loss=0.02790
Epoch 28/80: current_loss=0.02790 | best_loss=0.02790
Epoch 29/80: current_loss=0.02792 | best_loss=0.02790
Epoch 30/80: current_loss=0.02813 | best_loss=0.02790
Epoch 31/80: current_loss=0.02817 | best_loss=0.02790
Epoch 32/80: current_loss=0.02798 | best_loss=0.02790
Epoch 33/80: current_loss=0.02794 | best_loss=0.02790
Epoch 34/80: current_loss=0.02816 | best_loss=0.02790
Epoch 35/80: current_loss=0.02796 | best_loss=0.02790
Epoch 36/80: current_loss=0.02794 | best_loss=0.02790
Epoch 37/80: current_loss=0.02795 | best_loss=0.02790
Epoch 38/80: current_loss=0.02794 | best_loss=0.02790
Epoch 39/80: current_loss=0.02798 | best_loss=0.02790
Epoch 40/80: current_loss=0.02799 | best_loss=0.02790
Epoch 41/80: current_loss=0.02809 | best_loss=0.02790
Epoch 42/80: current_loss=0.02843 | best_loss=0.02790
Epoch 43/80: current_loss=0.02797 | best_loss=0.02790
Epoch 44/80: current_loss=0.02795 | best_loss=0.02790
Epoch 45/80: current_loss=0.02803 | best_loss=0.02790
Epoch 46/80: current_loss=0.02792 | best_loss=0.02790
Epoch 47/80: current_loss=0.02808 | best_loss=0.02790
Epoch 48/80: current_loss=0.02799 | best_loss=0.02790
Early Stopping at epoch 48
      explained_var=0.01495 | mse_loss=0.02738
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03115 | best_loss=0.03115
Epoch 1/80: current_loss=0.03050 | best_loss=0.03050
Epoch 2/80: current_loss=0.03106 | best_loss=0.03050
Epoch 3/80: current_loss=0.03020 | best_loss=0.03020
Epoch 4/80: current_loss=0.03043 | best_loss=0.03020
Epoch 5/80: current_loss=0.03078 | best_loss=0.03020
Epoch 6/80: current_loss=0.03052 | best_loss=0.03020
Epoch 7/80: current_loss=0.03048 | best_loss=0.03020
Epoch 8/80: current_loss=0.03089 | best_loss=0.03020
Epoch 9/80: current_loss=0.03095 | best_loss=0.03020
Epoch 10/80: current_loss=0.03008 | best_loss=0.03008
Epoch 11/80: current_loss=0.03094 | best_loss=0.03008
Epoch 12/80: current_loss=0.03109 | best_loss=0.03008
Epoch 13/80: current_loss=0.03012 | best_loss=0.03008
Epoch 14/80: current_loss=0.03155 | best_loss=0.03008
Epoch 15/80: current_loss=0.03135 | best_loss=0.03008
Epoch 16/80: current_loss=0.03064 | best_loss=0.03008
Epoch 17/80: current_loss=0.03099 | best_loss=0.03008
Epoch 18/80: current_loss=0.03110 | best_loss=0.03008
Epoch 19/80: current_loss=0.03055 | best_loss=0.03008
Epoch 20/80: current_loss=0.03093 | best_loss=0.03008
Epoch 21/80: current_loss=0.03047 | best_loss=0.03008
Epoch 22/80: current_loss=0.03095 | best_loss=0.03008
Epoch 23/80: current_loss=0.03092 | best_loss=0.03008
Epoch 24/80: current_loss=0.03037 | best_loss=0.03008
Epoch 25/80: current_loss=0.03114 | best_loss=0.03008
Epoch 26/80: current_loss=0.03157 | best_loss=0.03008
Epoch 27/80: current_loss=0.03117 | best_loss=0.03008
Epoch 28/80: current_loss=0.03059 | best_loss=0.03008
Epoch 29/80: current_loss=0.03100 | best_loss=0.03008
Epoch 30/80: current_loss=0.03074 | best_loss=0.03008
Early Stopping at epoch 30
      explained_var=0.01869 | mse_loss=0.02931
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02819 | best_loss=0.02819
Epoch 1/80: current_loss=0.02829 | best_loss=0.02819
Epoch 2/80: current_loss=0.02815 | best_loss=0.02815
Epoch 3/80: current_loss=0.02819 | best_loss=0.02815
Epoch 4/80: current_loss=0.02820 | best_loss=0.02815
Epoch 5/80: current_loss=0.02822 | best_loss=0.02815
Epoch 6/80: current_loss=0.02826 | best_loss=0.02815
Epoch 7/80: current_loss=0.02852 | best_loss=0.02815
Epoch 8/80: current_loss=0.02820 | best_loss=0.02815
Epoch 9/80: current_loss=0.02824 | best_loss=0.02815
Epoch 10/80: current_loss=0.02835 | best_loss=0.02815
Epoch 11/80: current_loss=0.02834 | best_loss=0.02815
Epoch 12/80: current_loss=0.02834 | best_loss=0.02815
Epoch 13/80: current_loss=0.02830 | best_loss=0.02815
Epoch 14/80: current_loss=0.02833 | best_loss=0.02815
Epoch 15/80: current_loss=0.02843 | best_loss=0.02815
Epoch 16/80: current_loss=0.02840 | best_loss=0.02815
Epoch 17/80: current_loss=0.02842 | best_loss=0.02815
Epoch 18/80: current_loss=0.02845 | best_loss=0.02815
Epoch 19/80: current_loss=0.02845 | best_loss=0.02815
Epoch 20/80: current_loss=0.02885 | best_loss=0.02815
Epoch 21/80: current_loss=0.02847 | best_loss=0.02815
Epoch 22/80: current_loss=0.02833 | best_loss=0.02815
Early Stopping at epoch 22
      explained_var=-0.00433 | mse_loss=0.02852
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03274 | best_loss=0.03274
Epoch 1/80: current_loss=0.03301 | best_loss=0.03274
Epoch 2/80: current_loss=0.03303 | best_loss=0.03274
Epoch 3/80: current_loss=0.03308 | best_loss=0.03274
Epoch 4/80: current_loss=0.03304 | best_loss=0.03274
Epoch 5/80: current_loss=0.03319 | best_loss=0.03274
Epoch 6/80: current_loss=0.03319 | best_loss=0.03274
Epoch 7/80: current_loss=0.03311 | best_loss=0.03274
Epoch 8/80: current_loss=0.03316 | best_loss=0.03274
Epoch 9/80: current_loss=0.03303 | best_loss=0.03274
Epoch 10/80: current_loss=0.03309 | best_loss=0.03274
Epoch 11/80: current_loss=0.03309 | best_loss=0.03274
Epoch 12/80: current_loss=0.03293 | best_loss=0.03274
Epoch 13/80: current_loss=0.03297 | best_loss=0.03274
Epoch 14/80: current_loss=0.03309 | best_loss=0.03274
Epoch 15/80: current_loss=0.03321 | best_loss=0.03274
Epoch 16/80: current_loss=0.03344 | best_loss=0.03274
Epoch 17/80: current_loss=0.03333 | best_loss=0.03274
Epoch 18/80: current_loss=0.03320 | best_loss=0.03274
Epoch 19/80: current_loss=0.03314 | best_loss=0.03274
Epoch 20/80: current_loss=0.03322 | best_loss=0.03274
Early Stopping at epoch 20
      explained_var=-0.03115 | mse_loss=0.03352
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00477| avg_loss=0.02883
----------------------------------------------


----------------------------------------------
Params for Trial 30
{'learning_rate': 0.01, 'weight_decay': 0.0041442184503166345, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02735 | best_loss=0.02735
Epoch 1/80: current_loss=0.03353 | best_loss=0.02735
Epoch 2/80: current_loss=0.04215 | best_loss=0.02735
Epoch 3/80: current_loss=0.02780 | best_loss=0.02735
Epoch 4/80: current_loss=0.02986 | best_loss=0.02735
Epoch 5/80: current_loss=0.02866 | best_loss=0.02735
Epoch 6/80: current_loss=0.03015 | best_loss=0.02735
Epoch 7/80: current_loss=0.04390 | best_loss=0.02735
Epoch 8/80: current_loss=0.02983 | best_loss=0.02735
Epoch 9/80: current_loss=0.02821 | best_loss=0.02735
Epoch 10/80: current_loss=0.02902 | best_loss=0.02735
Epoch 11/80: current_loss=0.03740 | best_loss=0.02735
Epoch 12/80: current_loss=0.02716 | best_loss=0.02716
Epoch 13/80: current_loss=0.02710 | best_loss=0.02710
Epoch 14/80: current_loss=0.03131 | best_loss=0.02710
Epoch 15/80: current_loss=0.03375 | best_loss=0.02710
Epoch 16/80: current_loss=0.03200 | best_loss=0.02710
Epoch 17/80: current_loss=0.03167 | best_loss=0.02710
Epoch 18/80: current_loss=0.02783 | best_loss=0.02710
Epoch 19/80: current_loss=0.02685 | best_loss=0.02685
Epoch 20/80: current_loss=0.02632 | best_loss=0.02632
Epoch 21/80: current_loss=0.02757 | best_loss=0.02632
Epoch 22/80: current_loss=0.02964 | best_loss=0.02632
Epoch 23/80: current_loss=0.02786 | best_loss=0.02632
Epoch 24/80: current_loss=0.02791 | best_loss=0.02632
Epoch 25/80: current_loss=0.02644 | best_loss=0.02632
Epoch 26/80: current_loss=0.02656 | best_loss=0.02632
Epoch 27/80: current_loss=0.02896 | best_loss=0.02632
Epoch 28/80: current_loss=0.02959 | best_loss=0.02632
Epoch 29/80: current_loss=0.02667 | best_loss=0.02632
Epoch 30/80: current_loss=0.02992 | best_loss=0.02632
Epoch 31/80: current_loss=0.02774 | best_loss=0.02632
Epoch 32/80: current_loss=0.02684 | best_loss=0.02632
Epoch 33/80: current_loss=0.02787 | best_loss=0.02632
Epoch 34/80: current_loss=0.02703 | best_loss=0.02632
Epoch 35/80: current_loss=0.02769 | best_loss=0.02632
Epoch 36/80: current_loss=0.02820 | best_loss=0.02632
Epoch 37/80: current_loss=0.02673 | best_loss=0.02632
Epoch 38/80: current_loss=0.02917 | best_loss=0.02632
Epoch 39/80: current_loss=0.02692 | best_loss=0.02632
Epoch 40/80: current_loss=0.02703 | best_loss=0.02632
Early Stopping at epoch 40
      explained_var=0.00495 | mse_loss=0.02571

----------------------------------------------
Params for Trial 31
{'learning_rate': 0.001, 'weight_decay': 0.00017358413332184582, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02716 | best_loss=0.02716
Epoch 1/80: current_loss=0.02998 | best_loss=0.02716
Epoch 2/80: current_loss=0.02611 | best_loss=0.02611
Epoch 3/80: current_loss=0.02600 | best_loss=0.02600
Epoch 4/80: current_loss=0.02818 | best_loss=0.02600
Epoch 5/80: current_loss=0.02582 | best_loss=0.02582
Epoch 6/80: current_loss=0.02559 | best_loss=0.02559
Epoch 7/80: current_loss=0.02655 | best_loss=0.02559
Epoch 8/80: current_loss=0.02591 | best_loss=0.02559
Epoch 9/80: current_loss=0.02577 | best_loss=0.02559
Epoch 10/80: current_loss=0.02544 | best_loss=0.02544
Epoch 11/80: current_loss=0.02681 | best_loss=0.02544
Epoch 12/80: current_loss=0.02676 | best_loss=0.02544
Epoch 13/80: current_loss=0.02706 | best_loss=0.02544
Epoch 14/80: current_loss=0.02541 | best_loss=0.02541
Epoch 15/80: current_loss=0.03082 | best_loss=0.02541
Epoch 16/80: current_loss=0.02529 | best_loss=0.02529
Epoch 17/80: current_loss=0.02768 | best_loss=0.02529
Epoch 18/80: current_loss=0.02676 | best_loss=0.02529
Epoch 19/80: current_loss=0.02574 | best_loss=0.02529
Epoch 20/80: current_loss=0.03006 | best_loss=0.02529
Epoch 21/80: current_loss=0.02582 | best_loss=0.02529
Epoch 22/80: current_loss=0.02787 | best_loss=0.02529
Epoch 23/80: current_loss=0.02592 | best_loss=0.02529
Epoch 24/80: current_loss=0.02592 | best_loss=0.02529
Epoch 25/80: current_loss=0.02692 | best_loss=0.02529
Epoch 26/80: current_loss=0.02582 | best_loss=0.02529
Epoch 27/80: current_loss=0.02746 | best_loss=0.02529
Epoch 28/80: current_loss=0.02651 | best_loss=0.02529
Epoch 29/80: current_loss=0.02694 | best_loss=0.02529
Epoch 30/80: current_loss=0.02645 | best_loss=0.02529
Epoch 31/80: current_loss=0.02602 | best_loss=0.02529
Epoch 32/80: current_loss=0.02722 | best_loss=0.02529
Epoch 33/80: current_loss=0.02773 | best_loss=0.02529
Epoch 34/80: current_loss=0.02583 | best_loss=0.02529
Epoch 35/80: current_loss=0.02813 | best_loss=0.02529
Epoch 36/80: current_loss=0.02613 | best_loss=0.02529
Early Stopping at epoch 36
      explained_var=0.04349 | mse_loss=0.02475
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02763 | best_loss=0.02763
Epoch 1/80: current_loss=0.02779 | best_loss=0.02763
Epoch 2/80: current_loss=0.02850 | best_loss=0.02763
Epoch 3/80: current_loss=0.02872 | best_loss=0.02763
Epoch 4/80: current_loss=0.02802 | best_loss=0.02763
Epoch 5/80: current_loss=0.02862 | best_loss=0.02763
Epoch 6/80: current_loss=0.02822 | best_loss=0.02763
Epoch 7/80: current_loss=0.02805 | best_loss=0.02763
Epoch 8/80: current_loss=0.02955 | best_loss=0.02763
Epoch 9/80: current_loss=0.02797 | best_loss=0.02763
Epoch 10/80: current_loss=0.02789 | best_loss=0.02763
Epoch 11/80: current_loss=0.02792 | best_loss=0.02763
Epoch 12/80: current_loss=0.02903 | best_loss=0.02763
Epoch 13/80: current_loss=0.02780 | best_loss=0.02763
Epoch 14/80: current_loss=0.02863 | best_loss=0.02763
Epoch 15/80: current_loss=0.02821 | best_loss=0.02763
Epoch 16/80: current_loss=0.02826 | best_loss=0.02763
Epoch 17/80: current_loss=0.02903 | best_loss=0.02763
Epoch 18/80: current_loss=0.02829 | best_loss=0.02763
Epoch 19/80: current_loss=0.02795 | best_loss=0.02763
Epoch 20/80: current_loss=0.02804 | best_loss=0.02763
Early Stopping at epoch 20
      explained_var=0.02290 | mse_loss=0.02715
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03276 | best_loss=0.03276
Epoch 1/80: current_loss=0.03026 | best_loss=0.03026
Epoch 2/80: current_loss=0.03116 | best_loss=0.03026
Epoch 3/80: current_loss=0.02999 | best_loss=0.02999
Epoch 4/80: current_loss=0.03264 | best_loss=0.02999
Epoch 5/80: current_loss=0.03013 | best_loss=0.02999
Epoch 6/80: current_loss=0.03268 | best_loss=0.02999
Epoch 7/80: current_loss=0.02968 | best_loss=0.02968
Epoch 8/80: current_loss=0.03182 | best_loss=0.02968
Epoch 9/80: current_loss=0.03024 | best_loss=0.02968
Epoch 10/80: current_loss=0.03265 | best_loss=0.02968
Epoch 11/80: current_loss=0.03018 | best_loss=0.02968
Epoch 12/80: current_loss=0.03103 | best_loss=0.02968
Epoch 13/80: current_loss=0.02997 | best_loss=0.02968
Epoch 14/80: current_loss=0.03159 | best_loss=0.02968
Epoch 15/80: current_loss=0.03018 | best_loss=0.02968
Epoch 16/80: current_loss=0.03097 | best_loss=0.02968
Epoch 17/80: current_loss=0.03092 | best_loss=0.02968
Epoch 18/80: current_loss=0.02983 | best_loss=0.02968
Epoch 19/80: current_loss=0.03134 | best_loss=0.02968
Epoch 20/80: current_loss=0.03026 | best_loss=0.02968
Epoch 21/80: current_loss=0.03058 | best_loss=0.02968
Epoch 22/80: current_loss=0.02999 | best_loss=0.02968
Epoch 23/80: current_loss=0.03044 | best_loss=0.02968
Epoch 24/80: current_loss=0.03028 | best_loss=0.02968
Epoch 25/80: current_loss=0.03062 | best_loss=0.02968
Epoch 26/80: current_loss=0.03162 | best_loss=0.02968
Epoch 27/80: current_loss=0.03039 | best_loss=0.02968
Early Stopping at epoch 27
      explained_var=0.02354 | mse_loss=0.02893
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02829 | best_loss=0.02829
Epoch 1/80: current_loss=0.02863 | best_loss=0.02829
Epoch 2/80: current_loss=0.02790 | best_loss=0.02790
Epoch 3/80: current_loss=0.02787 | best_loss=0.02787
Epoch 4/80: current_loss=0.02797 | best_loss=0.02787
Epoch 5/80: current_loss=0.02841 | best_loss=0.02787
Epoch 6/80: current_loss=0.02846 | best_loss=0.02787
Epoch 7/80: current_loss=0.02841 | best_loss=0.02787
Epoch 8/80: current_loss=0.02813 | best_loss=0.02787
Epoch 9/80: current_loss=0.02819 | best_loss=0.02787
Epoch 10/80: current_loss=0.02818 | best_loss=0.02787
Epoch 11/80: current_loss=0.02830 | best_loss=0.02787
Epoch 12/80: current_loss=0.02831 | best_loss=0.02787
Epoch 13/80: current_loss=0.02817 | best_loss=0.02787
Epoch 14/80: current_loss=0.02817 | best_loss=0.02787
Epoch 15/80: current_loss=0.02803 | best_loss=0.02787
Epoch 16/80: current_loss=0.02843 | best_loss=0.02787
Epoch 17/80: current_loss=0.02814 | best_loss=0.02787
Epoch 18/80: current_loss=0.02886 | best_loss=0.02787
Epoch 19/80: current_loss=0.02790 | best_loss=0.02787
Epoch 20/80: current_loss=0.02809 | best_loss=0.02787
Epoch 21/80: current_loss=0.02817 | best_loss=0.02787
Epoch 22/80: current_loss=0.02825 | best_loss=0.02787
Epoch 23/80: current_loss=0.02830 | best_loss=0.02787
Early Stopping at epoch 23
      explained_var=0.00666 | mse_loss=0.02822
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03293 | best_loss=0.03293
Epoch 1/80: current_loss=0.03270 | best_loss=0.03270
Epoch 2/80: current_loss=0.03274 | best_loss=0.03270
Epoch 3/80: current_loss=0.03339 | best_loss=0.03270
Epoch 4/80: current_loss=0.03319 | best_loss=0.03270
Epoch 5/80: current_loss=0.03331 | best_loss=0.03270
Epoch 6/80: current_loss=0.03344 | best_loss=0.03270
Epoch 7/80: current_loss=0.03364 | best_loss=0.03270
Epoch 8/80: current_loss=0.03387 | best_loss=0.03270
Epoch 9/80: current_loss=0.03404 | best_loss=0.03270
Epoch 10/80: current_loss=0.03395 | best_loss=0.03270
Epoch 11/80: current_loss=0.03361 | best_loss=0.03270
Epoch 12/80: current_loss=0.03327 | best_loss=0.03270
Epoch 13/80: current_loss=0.03333 | best_loss=0.03270
Epoch 14/80: current_loss=0.03408 | best_loss=0.03270
Epoch 15/80: current_loss=0.03395 | best_loss=0.03270
Epoch 16/80: current_loss=0.03362 | best_loss=0.03270
Epoch 17/80: current_loss=0.03374 | best_loss=0.03270
Epoch 18/80: current_loss=0.03375 | best_loss=0.03270
Epoch 19/80: current_loss=0.03361 | best_loss=0.03270
Epoch 20/80: current_loss=0.03352 | best_loss=0.03270
Epoch 21/80: current_loss=0.03352 | best_loss=0.03270
Early Stopping at epoch 21
      explained_var=-0.02890 | mse_loss=0.03349
----------------------------------------------
Average early_stopping_point: 5| avg_exp_var=0.01354| avg_loss=0.02851
----------------------------------------------


----------------------------------------------
Params for Trial 32
{'learning_rate': 0.001, 'weight_decay': 0.0009595826622126988, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03629 | best_loss=0.03629
Epoch 1/80: current_loss=0.03000 | best_loss=0.03000
Epoch 2/80: current_loss=0.02723 | best_loss=0.02723
Epoch 3/80: current_loss=0.03073 | best_loss=0.02723
Epoch 4/80: current_loss=0.02671 | best_loss=0.02671
Epoch 5/80: current_loss=0.02636 | best_loss=0.02636
Epoch 6/80: current_loss=0.02860 | best_loss=0.02636
Epoch 7/80: current_loss=0.02561 | best_loss=0.02561
Epoch 8/80: current_loss=0.02567 | best_loss=0.02561
Epoch 9/80: current_loss=0.02666 | best_loss=0.02561
Epoch 10/80: current_loss=0.02547 | best_loss=0.02547
Epoch 11/80: current_loss=0.02595 | best_loss=0.02547
Epoch 12/80: current_loss=0.02666 | best_loss=0.02547
Epoch 13/80: current_loss=0.02706 | best_loss=0.02547
Epoch 14/80: current_loss=0.02701 | best_loss=0.02547
Epoch 15/80: current_loss=0.02602 | best_loss=0.02547
Epoch 16/80: current_loss=0.02721 | best_loss=0.02547
Epoch 17/80: current_loss=0.02558 | best_loss=0.02547
Epoch 18/80: current_loss=0.02589 | best_loss=0.02547
Epoch 19/80: current_loss=0.02553 | best_loss=0.02547
Epoch 20/80: current_loss=0.02893 | best_loss=0.02547
Epoch 21/80: current_loss=0.02640 | best_loss=0.02547
Epoch 22/80: current_loss=0.02799 | best_loss=0.02547
Epoch 23/80: current_loss=0.02597 | best_loss=0.02547
Epoch 24/80: current_loss=0.02799 | best_loss=0.02547
Epoch 25/80: current_loss=0.02645 | best_loss=0.02547
Epoch 26/80: current_loss=0.02603 | best_loss=0.02547
Epoch 27/80: current_loss=0.02921 | best_loss=0.02547
Epoch 28/80: current_loss=0.02608 | best_loss=0.02547
Epoch 29/80: current_loss=0.02975 | best_loss=0.02547
Epoch 30/80: current_loss=0.02600 | best_loss=0.02547
Early Stopping at epoch 30
      explained_var=0.04232 | mse_loss=0.02497
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02825 | best_loss=0.02825
Epoch 1/80: current_loss=0.02849 | best_loss=0.02825
Epoch 2/80: current_loss=0.02894 | best_loss=0.02825
Epoch 3/80: current_loss=0.02837 | best_loss=0.02825
Epoch 4/80: current_loss=0.02859 | best_loss=0.02825
Epoch 5/80: current_loss=0.02826 | best_loss=0.02825
Epoch 6/80: current_loss=0.02939 | best_loss=0.02825
Epoch 7/80: current_loss=0.02845 | best_loss=0.02825
Epoch 8/80: current_loss=0.02831 | best_loss=0.02825
Epoch 9/80: current_loss=0.02818 | best_loss=0.02818
Epoch 10/80: current_loss=0.02846 | best_loss=0.02818
Epoch 11/80: current_loss=0.02807 | best_loss=0.02807
Epoch 12/80: current_loss=0.02826 | best_loss=0.02807
Epoch 13/80: current_loss=0.02811 | best_loss=0.02807
Epoch 14/80: current_loss=0.02860 | best_loss=0.02807
Epoch 15/80: current_loss=0.02871 | best_loss=0.02807
Epoch 16/80: current_loss=0.02892 | best_loss=0.02807
Epoch 17/80: current_loss=0.02871 | best_loss=0.02807
Epoch 18/80: current_loss=0.02783 | best_loss=0.02783
Epoch 19/80: current_loss=0.02824 | best_loss=0.02783
Epoch 20/80: current_loss=0.02791 | best_loss=0.02783
Epoch 21/80: current_loss=0.02859 | best_loss=0.02783
Epoch 22/80: current_loss=0.02800 | best_loss=0.02783
Epoch 23/80: current_loss=0.02810 | best_loss=0.02783
Epoch 24/80: current_loss=0.02828 | best_loss=0.02783
Epoch 25/80: current_loss=0.02804 | best_loss=0.02783
Epoch 26/80: current_loss=0.02804 | best_loss=0.02783
Epoch 27/80: current_loss=0.02806 | best_loss=0.02783
Epoch 28/80: current_loss=0.02831 | best_loss=0.02783
Epoch 29/80: current_loss=0.02875 | best_loss=0.02783
Epoch 30/80: current_loss=0.02858 | best_loss=0.02783
Epoch 31/80: current_loss=0.02915 | best_loss=0.02783
Epoch 32/80: current_loss=0.02805 | best_loss=0.02783
Epoch 33/80: current_loss=0.02797 | best_loss=0.02783
Epoch 34/80: current_loss=0.02818 | best_loss=0.02783
Epoch 35/80: current_loss=0.02812 | best_loss=0.02783
Epoch 36/80: current_loss=0.02991 | best_loss=0.02783
Epoch 37/80: current_loss=0.02814 | best_loss=0.02783
Epoch 38/80: current_loss=0.02820 | best_loss=0.02783
Early Stopping at epoch 38
      explained_var=0.01819 | mse_loss=0.02733
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03308 | best_loss=0.03308
Epoch 1/80: current_loss=0.03218 | best_loss=0.03218
Epoch 2/80: current_loss=0.03066 | best_loss=0.03066
Epoch 3/80: current_loss=0.03162 | best_loss=0.03066
Epoch 4/80: current_loss=0.03162 | best_loss=0.03066
Epoch 5/80: current_loss=0.03006 | best_loss=0.03006
Epoch 6/80: current_loss=0.03144 | best_loss=0.03006
Epoch 7/80: current_loss=0.03005 | best_loss=0.03005
Epoch 8/80: current_loss=0.03191 | best_loss=0.03005
Epoch 9/80: current_loss=0.03142 | best_loss=0.03005
Epoch 10/80: current_loss=0.02999 | best_loss=0.02999
Epoch 11/80: current_loss=0.03376 | best_loss=0.02999
Epoch 12/80: current_loss=0.02993 | best_loss=0.02993
Epoch 13/80: current_loss=0.03155 | best_loss=0.02993
Epoch 14/80: current_loss=0.03326 | best_loss=0.02993
Epoch 15/80: current_loss=0.03008 | best_loss=0.02993
Epoch 16/80: current_loss=0.03047 | best_loss=0.02993
Epoch 17/80: current_loss=0.03180 | best_loss=0.02993
Epoch 18/80: current_loss=0.03017 | best_loss=0.02993
Epoch 19/80: current_loss=0.03185 | best_loss=0.02993
Epoch 20/80: current_loss=0.03047 | best_loss=0.02993
Epoch 21/80: current_loss=0.03098 | best_loss=0.02993
Epoch 22/80: current_loss=0.03102 | best_loss=0.02993
Epoch 23/80: current_loss=0.03040 | best_loss=0.02993
Epoch 24/80: current_loss=0.03223 | best_loss=0.02993
Epoch 25/80: current_loss=0.03091 | best_loss=0.02993
Epoch 26/80: current_loss=0.03045 | best_loss=0.02993
Epoch 27/80: current_loss=0.03066 | best_loss=0.02993
Epoch 28/80: current_loss=0.03110 | best_loss=0.02993
Epoch 29/80: current_loss=0.03092 | best_loss=0.02993
Epoch 30/80: current_loss=0.03000 | best_loss=0.02993
Epoch 31/80: current_loss=0.03175 | best_loss=0.02993
Epoch 32/80: current_loss=0.03056 | best_loss=0.02993
Early Stopping at epoch 32
      explained_var=0.01464 | mse_loss=0.02915
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02828 | best_loss=0.02828
Epoch 1/80: current_loss=0.02808 | best_loss=0.02808
Epoch 2/80: current_loss=0.02812 | best_loss=0.02808
Epoch 3/80: current_loss=0.02874 | best_loss=0.02808
Epoch 4/80: current_loss=0.02809 | best_loss=0.02808
Epoch 5/80: current_loss=0.02844 | best_loss=0.02808
Epoch 6/80: current_loss=0.02797 | best_loss=0.02797
Epoch 7/80: current_loss=0.02844 | best_loss=0.02797
Epoch 8/80: current_loss=0.02840 | best_loss=0.02797
Epoch 9/80: current_loss=0.02814 | best_loss=0.02797
Epoch 10/80: current_loss=0.02886 | best_loss=0.02797
Epoch 11/80: current_loss=0.02813 | best_loss=0.02797
Epoch 12/80: current_loss=0.02863 | best_loss=0.02797
Epoch 13/80: current_loss=0.02836 | best_loss=0.02797
Epoch 14/80: current_loss=0.02928 | best_loss=0.02797
Epoch 15/80: current_loss=0.02827 | best_loss=0.02797
Epoch 16/80: current_loss=0.02884 | best_loss=0.02797
Epoch 17/80: current_loss=0.02824 | best_loss=0.02797
Epoch 18/80: current_loss=0.02833 | best_loss=0.02797
Epoch 19/80: current_loss=0.02831 | best_loss=0.02797
Epoch 20/80: current_loss=0.02814 | best_loss=0.02797
Epoch 21/80: current_loss=0.02841 | best_loss=0.02797
Epoch 22/80: current_loss=0.02828 | best_loss=0.02797
Epoch 23/80: current_loss=0.02841 | best_loss=0.02797
Epoch 24/80: current_loss=0.02829 | best_loss=0.02797
Epoch 25/80: current_loss=0.02816 | best_loss=0.02797
Epoch 26/80: current_loss=0.02828 | best_loss=0.02797
Early Stopping at epoch 26
      explained_var=0.00120 | mse_loss=0.02833
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03307 | best_loss=0.03307
Epoch 1/80: current_loss=0.03294 | best_loss=0.03294
Epoch 2/80: current_loss=0.03279 | best_loss=0.03279
Epoch 3/80: current_loss=0.03252 | best_loss=0.03252
Epoch 4/80: current_loss=0.03294 | best_loss=0.03252
Epoch 5/80: current_loss=0.03277 | best_loss=0.03252
Epoch 6/80: current_loss=0.03329 | best_loss=0.03252
Epoch 7/80: current_loss=0.03345 | best_loss=0.03252
Epoch 8/80: current_loss=0.03358 | best_loss=0.03252
Epoch 9/80: current_loss=0.03310 | best_loss=0.03252
Epoch 10/80: current_loss=0.03336 | best_loss=0.03252
Epoch 11/80: current_loss=0.03309 | best_loss=0.03252
Epoch 12/80: current_loss=0.03292 | best_loss=0.03252
Epoch 13/80: current_loss=0.03288 | best_loss=0.03252
Epoch 14/80: current_loss=0.03291 | best_loss=0.03252
Epoch 15/80: current_loss=0.03284 | best_loss=0.03252
Epoch 16/80: current_loss=0.03292 | best_loss=0.03252
Epoch 17/80: current_loss=0.03316 | best_loss=0.03252
Epoch 18/80: current_loss=0.03308 | best_loss=0.03252
Epoch 19/80: current_loss=0.03303 | best_loss=0.03252
Epoch 20/80: current_loss=0.03360 | best_loss=0.03252
Epoch 21/80: current_loss=0.03342 | best_loss=0.03252
Epoch 22/80: current_loss=0.03334 | best_loss=0.03252
Epoch 23/80: current_loss=0.03320 | best_loss=0.03252
Early Stopping at epoch 23
      explained_var=-0.02465 | mse_loss=0.03329
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.01034| avg_loss=0.02862
----------------------------------------------


----------------------------------------------
Params for Trial 33
{'learning_rate': 1e-05, 'weight_decay': 0.0026902788124020883, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.20835 | best_loss=0.20835
Epoch 1/80: current_loss=0.19542 | best_loss=0.19542
Epoch 2/80: current_loss=0.18326 | best_loss=0.18326
Epoch 3/80: current_loss=0.17175 | best_loss=0.17175
Epoch 4/80: current_loss=0.16091 | best_loss=0.16091
Epoch 5/80: current_loss=0.15041 | best_loss=0.15041
Epoch 6/80: current_loss=0.14031 | best_loss=0.14031
Epoch 7/80: current_loss=0.13076 | best_loss=0.13076
Epoch 8/80: current_loss=0.12172 | best_loss=0.12172
Epoch 9/80: current_loss=0.11350 | best_loss=0.11350
Epoch 10/80: current_loss=0.10578 | best_loss=0.10578
Epoch 11/80: current_loss=0.09834 | best_loss=0.09834
Epoch 12/80: current_loss=0.09137 | best_loss=0.09137
Epoch 13/80: current_loss=0.08510 | best_loss=0.08510
Epoch 14/80: current_loss=0.07908 | best_loss=0.07908
Epoch 15/80: current_loss=0.07332 | best_loss=0.07332
Epoch 16/80: current_loss=0.06804 | best_loss=0.06804
Epoch 17/80: current_loss=0.06334 | best_loss=0.06334
Epoch 18/80: current_loss=0.05897 | best_loss=0.05897
Epoch 19/80: current_loss=0.05497 | best_loss=0.05497
Epoch 20/80: current_loss=0.05164 | best_loss=0.05164
Epoch 21/80: current_loss=0.04853 | best_loss=0.04853
Epoch 22/80: current_loss=0.04558 | best_loss=0.04558
Epoch 23/80: current_loss=0.04313 | best_loss=0.04313
Epoch 24/80: current_loss=0.04105 | best_loss=0.04105
Epoch 25/80: current_loss=0.03931 | best_loss=0.03931
Epoch 26/80: current_loss=0.03777 | best_loss=0.03777
Epoch 27/80: current_loss=0.03655 | best_loss=0.03655
Epoch 28/80: current_loss=0.03544 | best_loss=0.03544
Epoch 29/80: current_loss=0.03450 | best_loss=0.03450
Epoch 30/80: current_loss=0.03378 | best_loss=0.03378
Epoch 31/80: current_loss=0.03316 | best_loss=0.03316
Epoch 32/80: current_loss=0.03271 | best_loss=0.03271
Epoch 33/80: current_loss=0.03240 | best_loss=0.03240
Epoch 34/80: current_loss=0.03212 | best_loss=0.03212
Epoch 35/80: current_loss=0.03194 | best_loss=0.03194
Epoch 36/80: current_loss=0.03177 | best_loss=0.03177
Epoch 37/80: current_loss=0.03164 | best_loss=0.03164
Epoch 38/80: current_loss=0.03155 | best_loss=0.03155
Epoch 39/80: current_loss=0.03150 | best_loss=0.03150
Epoch 40/80: current_loss=0.03148 | best_loss=0.03148
Epoch 41/80: current_loss=0.03148 | best_loss=0.03148
Epoch 42/80: current_loss=0.03152 | best_loss=0.03148
Epoch 43/80: current_loss=0.03157 | best_loss=0.03148
Epoch 44/80: current_loss=0.03160 | best_loss=0.03148
Epoch 45/80: current_loss=0.03160 | best_loss=0.03148
Epoch 46/80: current_loss=0.03161 | best_loss=0.03148
Epoch 47/80: current_loss=0.03157 | best_loss=0.03148
Epoch 48/80: current_loss=0.03153 | best_loss=0.03148
Epoch 49/80: current_loss=0.03142 | best_loss=0.03142
Epoch 50/80: current_loss=0.03140 | best_loss=0.03140
Epoch 51/80: current_loss=0.03138 | best_loss=0.03138
Epoch 52/80: current_loss=0.03137 | best_loss=0.03137
Epoch 53/80: current_loss=0.03135 | best_loss=0.03135
Epoch 54/80: current_loss=0.03132 | best_loss=0.03132
Epoch 55/80: current_loss=0.03134 | best_loss=0.03132
Epoch 56/80: current_loss=0.03130 | best_loss=0.03130
Epoch 57/80: current_loss=0.03128 | best_loss=0.03128
Epoch 58/80: current_loss=0.03128 | best_loss=0.03128
Epoch 59/80: current_loss=0.03133 | best_loss=0.03128
Epoch 60/80: current_loss=0.03133 | best_loss=0.03128
Epoch 61/80: current_loss=0.03130 | best_loss=0.03128
Epoch 62/80: current_loss=0.03130 | best_loss=0.03128
Epoch 63/80: current_loss=0.03126 | best_loss=0.03126
Epoch 64/80: current_loss=0.03122 | best_loss=0.03122
Epoch 65/80: current_loss=0.03115 | best_loss=0.03115
Epoch 66/80: current_loss=0.03108 | best_loss=0.03108
Epoch 67/80: current_loss=0.03103 | best_loss=0.03103
Epoch 68/80: current_loss=0.03096 | best_loss=0.03096
Epoch 69/80: current_loss=0.03107 | best_loss=0.03096
Epoch 70/80: current_loss=0.03107 | best_loss=0.03096
Epoch 71/80: current_loss=0.03105 | best_loss=0.03096
Epoch 72/80: current_loss=0.03106 | best_loss=0.03096
Epoch 73/80: current_loss=0.03101 | best_loss=0.03096
Epoch 74/80: current_loss=0.03095 | best_loss=0.03095
Epoch 75/80: current_loss=0.03092 | best_loss=0.03092
Epoch 76/80: current_loss=0.03096 | best_loss=0.03092
Epoch 77/80: current_loss=0.03086 | best_loss=0.03086
Epoch 78/80: current_loss=0.03080 | best_loss=0.03080
Epoch 79/80: current_loss=0.03073 | best_loss=0.03073
      explained_var=-0.15858 | mse_loss=0.03016

----------------------------------------------
Params for Trial 34
{'learning_rate': 0.001, 'weight_decay': 0.001704506745174117, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02967 | best_loss=0.02967
Epoch 1/80: current_loss=0.02915 | best_loss=0.02915
Epoch 2/80: current_loss=0.02641 | best_loss=0.02641
Epoch 3/80: current_loss=0.02761 | best_loss=0.02641
Epoch 4/80: current_loss=0.02647 | best_loss=0.02641
Epoch 5/80: current_loss=0.02828 | best_loss=0.02641
Epoch 6/80: current_loss=0.02566 | best_loss=0.02566
Epoch 7/80: current_loss=0.02843 | best_loss=0.02566
Epoch 8/80: current_loss=0.02555 | best_loss=0.02555
Epoch 9/80: current_loss=0.03019 | best_loss=0.02555
Epoch 10/80: current_loss=0.02522 | best_loss=0.02522
Epoch 11/80: current_loss=0.02824 | best_loss=0.02522
Epoch 12/80: current_loss=0.02560 | best_loss=0.02522
Epoch 13/80: current_loss=0.02607 | best_loss=0.02522
Epoch 14/80: current_loss=0.02831 | best_loss=0.02522
Epoch 15/80: current_loss=0.02583 | best_loss=0.02522
Epoch 16/80: current_loss=0.02642 | best_loss=0.02522
Epoch 17/80: current_loss=0.02621 | best_loss=0.02522
Epoch 18/80: current_loss=0.02699 | best_loss=0.02522
Epoch 19/80: current_loss=0.02572 | best_loss=0.02522
Epoch 20/80: current_loss=0.02784 | best_loss=0.02522
Epoch 21/80: current_loss=0.02545 | best_loss=0.02522
Epoch 22/80: current_loss=0.02688 | best_loss=0.02522
Epoch 23/80: current_loss=0.02717 | best_loss=0.02522
Epoch 24/80: current_loss=0.02550 | best_loss=0.02522
Epoch 25/80: current_loss=0.02847 | best_loss=0.02522
Epoch 26/80: current_loss=0.02600 | best_loss=0.02522
Epoch 27/80: current_loss=0.02920 | best_loss=0.02522
Epoch 28/80: current_loss=0.02562 | best_loss=0.02522
Epoch 29/80: current_loss=0.02692 | best_loss=0.02522
Epoch 30/80: current_loss=0.02582 | best_loss=0.02522
Early Stopping at epoch 30
      explained_var=0.04478 | mse_loss=0.02474
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02799 | best_loss=0.02799
Epoch 1/80: current_loss=0.02797 | best_loss=0.02797
Epoch 2/80: current_loss=0.02796 | best_loss=0.02796
Epoch 3/80: current_loss=0.02802 | best_loss=0.02796
Epoch 4/80: current_loss=0.02905 | best_loss=0.02796
Epoch 5/80: current_loss=0.02801 | best_loss=0.02796
Epoch 6/80: current_loss=0.02802 | best_loss=0.02796
Epoch 7/80: current_loss=0.02810 | best_loss=0.02796
Epoch 8/80: current_loss=0.02811 | best_loss=0.02796
Epoch 9/80: current_loss=0.02815 | best_loss=0.02796
Epoch 10/80: current_loss=0.02823 | best_loss=0.02796
Epoch 11/80: current_loss=0.02915 | best_loss=0.02796
Epoch 12/80: current_loss=0.02832 | best_loss=0.02796
Epoch 13/80: current_loss=0.02866 | best_loss=0.02796
Epoch 14/80: current_loss=0.02803 | best_loss=0.02796
Epoch 15/80: current_loss=0.02835 | best_loss=0.02796
Epoch 16/80: current_loss=0.02837 | best_loss=0.02796
Epoch 17/80: current_loss=0.02842 | best_loss=0.02796
Epoch 18/80: current_loss=0.02838 | best_loss=0.02796
Epoch 19/80: current_loss=0.02958 | best_loss=0.02796
Epoch 20/80: current_loss=0.02805 | best_loss=0.02796
Epoch 21/80: current_loss=0.02818 | best_loss=0.02796
Epoch 22/80: current_loss=0.02797 | best_loss=0.02796
Early Stopping at epoch 22
      explained_var=0.01329 | mse_loss=0.02748
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03283 | best_loss=0.03283
Epoch 1/80: current_loss=0.03238 | best_loss=0.03238
Epoch 2/80: current_loss=0.03090 | best_loss=0.03090
Epoch 3/80: current_loss=0.03084 | best_loss=0.03084
Epoch 4/80: current_loss=0.03160 | best_loss=0.03084
Epoch 5/80: current_loss=0.03065 | best_loss=0.03065
Epoch 6/80: current_loss=0.03097 | best_loss=0.03065
Epoch 7/80: current_loss=0.03122 | best_loss=0.03065
Epoch 8/80: current_loss=0.03000 | best_loss=0.03000
Epoch 9/80: current_loss=0.03505 | best_loss=0.03000
Epoch 10/80: current_loss=0.03134 | best_loss=0.03000
Epoch 11/80: current_loss=0.03106 | best_loss=0.03000
Epoch 12/80: current_loss=0.03030 | best_loss=0.03000
Epoch 13/80: current_loss=0.03204 | best_loss=0.03000
Epoch 14/80: current_loss=0.03066 | best_loss=0.03000
Epoch 15/80: current_loss=0.03023 | best_loss=0.03000
Epoch 16/80: current_loss=0.03171 | best_loss=0.03000
Epoch 17/80: current_loss=0.03075 | best_loss=0.03000
Epoch 18/80: current_loss=0.03175 | best_loss=0.03000
Epoch 19/80: current_loss=0.03052 | best_loss=0.03000
Epoch 20/80: current_loss=0.03173 | best_loss=0.03000
Epoch 21/80: current_loss=0.03045 | best_loss=0.03000
Epoch 22/80: current_loss=0.03376 | best_loss=0.03000
Epoch 23/80: current_loss=0.03074 | best_loss=0.03000
Epoch 24/80: current_loss=0.03106 | best_loss=0.03000
Epoch 25/80: current_loss=0.03071 | best_loss=0.03000
Epoch 26/80: current_loss=0.03112 | best_loss=0.03000
Epoch 27/80: current_loss=0.03100 | best_loss=0.03000
Epoch 28/80: current_loss=0.03100 | best_loss=0.03000
Early Stopping at epoch 28
      explained_var=0.01358 | mse_loss=0.02920
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02798 | best_loss=0.02798
Epoch 1/80: current_loss=0.02799 | best_loss=0.02798
Epoch 2/80: current_loss=0.02804 | best_loss=0.02798
Epoch 3/80: current_loss=0.02869 | best_loss=0.02798
Epoch 4/80: current_loss=0.02801 | best_loss=0.02798
Epoch 5/80: current_loss=0.02804 | best_loss=0.02798
Epoch 6/80: current_loss=0.02809 | best_loss=0.02798
Epoch 7/80: current_loss=0.02807 | best_loss=0.02798
Epoch 8/80: current_loss=0.02826 | best_loss=0.02798
Epoch 9/80: current_loss=0.02809 | best_loss=0.02798
Epoch 10/80: current_loss=0.02804 | best_loss=0.02798
Epoch 11/80: current_loss=0.02804 | best_loss=0.02798
Epoch 12/80: current_loss=0.02803 | best_loss=0.02798
Epoch 13/80: current_loss=0.02806 | best_loss=0.02798
Epoch 14/80: current_loss=0.02864 | best_loss=0.02798
Epoch 15/80: current_loss=0.02805 | best_loss=0.02798
Epoch 16/80: current_loss=0.02807 | best_loss=0.02798
Epoch 17/80: current_loss=0.02830 | best_loss=0.02798
Epoch 18/80: current_loss=0.02812 | best_loss=0.02798
Epoch 19/80: current_loss=0.02808 | best_loss=0.02798
Epoch 20/80: current_loss=0.02818 | best_loss=0.02798
Early Stopping at epoch 20
      explained_var=0.00098 | mse_loss=0.02835
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03236 | best_loss=0.03236
Epoch 1/80: current_loss=0.03273 | best_loss=0.03236
Epoch 2/80: current_loss=0.03323 | best_loss=0.03236
Epoch 3/80: current_loss=0.03392 | best_loss=0.03236
Epoch 4/80: current_loss=0.03345 | best_loss=0.03236
Epoch 5/80: current_loss=0.03336 | best_loss=0.03236
Epoch 6/80: current_loss=0.03337 | best_loss=0.03236
Epoch 7/80: current_loss=0.03310 | best_loss=0.03236
Epoch 8/80: current_loss=0.03318 | best_loss=0.03236
Epoch 9/80: current_loss=0.03291 | best_loss=0.03236
Epoch 10/80: current_loss=0.03303 | best_loss=0.03236
Epoch 11/80: current_loss=0.03267 | best_loss=0.03236
Epoch 12/80: current_loss=0.03280 | best_loss=0.03236
Epoch 13/80: current_loss=0.03279 | best_loss=0.03236
Epoch 14/80: current_loss=0.03292 | best_loss=0.03236
Epoch 15/80: current_loss=0.03310 | best_loss=0.03236
Epoch 16/80: current_loss=0.03304 | best_loss=0.03236
Epoch 17/80: current_loss=0.03304 | best_loss=0.03236
Epoch 18/80: current_loss=0.03319 | best_loss=0.03236
Epoch 19/80: current_loss=0.03299 | best_loss=0.03236
Epoch 20/80: current_loss=0.03297 | best_loss=0.03236
Early Stopping at epoch 20
      explained_var=-0.02056 | mse_loss=0.03315
----------------------------------------------
Average early_stopping_point: 4| avg_exp_var=0.01041| avg_loss=0.02858
----------------------------------------------


----------------------------------------------
Params for Trial 35
{'learning_rate': 0.0001, 'weight_decay': 4.488923017717564e-05, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.18598 | best_loss=0.18598
Epoch 1/80: current_loss=0.11520 | best_loss=0.11520
Epoch 2/80: current_loss=0.06316 | best_loss=0.06316
Epoch 3/80: current_loss=0.03429 | best_loss=0.03429
Epoch 4/80: current_loss=0.02884 | best_loss=0.02884
Epoch 5/80: current_loss=0.03042 | best_loss=0.02884
Epoch 6/80: current_loss=0.03040 | best_loss=0.02884
Epoch 7/80: current_loss=0.02910 | best_loss=0.02884
Epoch 8/80: current_loss=0.02825 | best_loss=0.02825
Epoch 9/80: current_loss=0.02811 | best_loss=0.02811
Epoch 10/80: current_loss=0.02908 | best_loss=0.02811
Epoch 11/80: current_loss=0.02875 | best_loss=0.02811
Epoch 12/80: current_loss=0.02827 | best_loss=0.02811
Epoch 13/80: current_loss=0.02814 | best_loss=0.02811
Epoch 14/80: current_loss=0.02821 | best_loss=0.02811
Epoch 15/80: current_loss=0.02798 | best_loss=0.02798
Epoch 16/80: current_loss=0.02759 | best_loss=0.02759
Epoch 17/80: current_loss=0.02795 | best_loss=0.02759
Epoch 18/80: current_loss=0.02757 | best_loss=0.02757
Epoch 19/80: current_loss=0.02762 | best_loss=0.02757
Epoch 20/80: current_loss=0.02761 | best_loss=0.02757
Epoch 21/80: current_loss=0.02786 | best_loss=0.02757
Epoch 22/80: current_loss=0.02782 | best_loss=0.02757
Epoch 23/80: current_loss=0.02736 | best_loss=0.02736
Epoch 24/80: current_loss=0.02708 | best_loss=0.02708
Epoch 25/80: current_loss=0.02738 | best_loss=0.02708
Epoch 26/80: current_loss=0.02732 | best_loss=0.02708
Epoch 27/80: current_loss=0.02749 | best_loss=0.02708
Epoch 28/80: current_loss=0.02757 | best_loss=0.02708
Epoch 29/80: current_loss=0.02669 | best_loss=0.02669
Epoch 30/80: current_loss=0.02712 | best_loss=0.02669
Epoch 31/80: current_loss=0.02741 | best_loss=0.02669
Epoch 32/80: current_loss=0.02737 | best_loss=0.02669
Epoch 33/80: current_loss=0.02688 | best_loss=0.02669
Epoch 34/80: current_loss=0.02683 | best_loss=0.02669
Epoch 35/80: current_loss=0.02698 | best_loss=0.02669
Epoch 36/80: current_loss=0.02686 | best_loss=0.02669
Epoch 37/80: current_loss=0.02686 | best_loss=0.02669
Epoch 38/80: current_loss=0.02732 | best_loss=0.02669
Epoch 39/80: current_loss=0.02756 | best_loss=0.02669
Epoch 40/80: current_loss=0.02707 | best_loss=0.02669
Epoch 41/80: current_loss=0.02688 | best_loss=0.02669
Epoch 42/80: current_loss=0.02649 | best_loss=0.02649
Epoch 43/80: current_loss=0.02611 | best_loss=0.02611
Epoch 44/80: current_loss=0.02659 | best_loss=0.02611
Epoch 45/80: current_loss=0.02709 | best_loss=0.02611
Epoch 46/80: current_loss=0.02595 | best_loss=0.02595
Epoch 47/80: current_loss=0.02589 | best_loss=0.02589
Epoch 48/80: current_loss=0.02720 | best_loss=0.02589
Epoch 49/80: current_loss=0.02635 | best_loss=0.02589
Epoch 50/80: current_loss=0.02597 | best_loss=0.02589
Epoch 51/80: current_loss=0.02619 | best_loss=0.02589
Epoch 52/80: current_loss=0.02712 | best_loss=0.02589
Epoch 53/80: current_loss=0.02740 | best_loss=0.02589
Epoch 54/80: current_loss=0.02695 | best_loss=0.02589
Epoch 55/80: current_loss=0.02637 | best_loss=0.02589
Epoch 56/80: current_loss=0.02731 | best_loss=0.02589
Epoch 57/80: current_loss=0.02677 | best_loss=0.02589
Epoch 58/80: current_loss=0.02634 | best_loss=0.02589
Epoch 59/80: current_loss=0.02670 | best_loss=0.02589
Epoch 60/80: current_loss=0.02677 | best_loss=0.02589
Epoch 61/80: current_loss=0.02658 | best_loss=0.02589
Epoch 62/80: current_loss=0.02657 | best_loss=0.02589
Epoch 63/80: current_loss=0.02686 | best_loss=0.02589
Epoch 64/80: current_loss=0.02743 | best_loss=0.02589
Epoch 65/80: current_loss=0.02638 | best_loss=0.02589
Epoch 66/80: current_loss=0.02611 | best_loss=0.02589
Epoch 67/80: current_loss=0.02656 | best_loss=0.02589
Early Stopping at epoch 67
      explained_var=0.02256 | mse_loss=0.02537
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02843 | best_loss=0.02843
Epoch 1/80: current_loss=0.02830 | best_loss=0.02830
Epoch 2/80: current_loss=0.02843 | best_loss=0.02830
Epoch 3/80: current_loss=0.02824 | best_loss=0.02824
Epoch 4/80: current_loss=0.02821 | best_loss=0.02821
Epoch 5/80: current_loss=0.02825 | best_loss=0.02821
Epoch 6/80: current_loss=0.02873 | best_loss=0.02821
Epoch 7/80: current_loss=0.02862 | best_loss=0.02821
Epoch 8/80: current_loss=0.02845 | best_loss=0.02821
Epoch 9/80: current_loss=0.02835 | best_loss=0.02821
Epoch 10/80: current_loss=0.02834 | best_loss=0.02821
Epoch 11/80: current_loss=0.02833 | best_loss=0.02821
Epoch 12/80: current_loss=0.02841 | best_loss=0.02821
Epoch 13/80: current_loss=0.02835 | best_loss=0.02821
Epoch 14/80: current_loss=0.02825 | best_loss=0.02821
Epoch 15/80: current_loss=0.02831 | best_loss=0.02821
Epoch 16/80: current_loss=0.02835 | best_loss=0.02821
Epoch 17/80: current_loss=0.02855 | best_loss=0.02821
Epoch 18/80: current_loss=0.02842 | best_loss=0.02821
Epoch 19/80: current_loss=0.02852 | best_loss=0.02821
Epoch 20/80: current_loss=0.02857 | best_loss=0.02821
Epoch 21/80: current_loss=0.02860 | best_loss=0.02821
Epoch 22/80: current_loss=0.02864 | best_loss=0.02821
Epoch 23/80: current_loss=0.02873 | best_loss=0.02821
Epoch 24/80: current_loss=0.02858 | best_loss=0.02821
Early Stopping at epoch 24
      explained_var=-0.00230 | mse_loss=0.02785
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03042 | best_loss=0.03042
Epoch 1/80: current_loss=0.03113 | best_loss=0.03042
Epoch 2/80: current_loss=0.03165 | best_loss=0.03042
Epoch 3/80: current_loss=0.03078 | best_loss=0.03042
Epoch 4/80: current_loss=0.03057 | best_loss=0.03042
Epoch 5/80: current_loss=0.03146 | best_loss=0.03042
Epoch 6/80: current_loss=0.03079 | best_loss=0.03042
Epoch 7/80: current_loss=0.02994 | best_loss=0.02994
Epoch 8/80: current_loss=0.03095 | best_loss=0.02994
Epoch 9/80: current_loss=0.03061 | best_loss=0.02994
Epoch 10/80: current_loss=0.03058 | best_loss=0.02994
Epoch 11/80: current_loss=0.03053 | best_loss=0.02994
Epoch 12/80: current_loss=0.03103 | best_loss=0.02994
Epoch 13/80: current_loss=0.03070 | best_loss=0.02994
Epoch 14/80: current_loss=0.03008 | best_loss=0.02994
Epoch 15/80: current_loss=0.03099 | best_loss=0.02994
Epoch 16/80: current_loss=0.03021 | best_loss=0.02994
Epoch 17/80: current_loss=0.02986 | best_loss=0.02986
Epoch 18/80: current_loss=0.03065 | best_loss=0.02986
Epoch 19/80: current_loss=0.03071 | best_loss=0.02986
Epoch 20/80: current_loss=0.03032 | best_loss=0.02986
Epoch 21/80: current_loss=0.03051 | best_loss=0.02986
Epoch 22/80: current_loss=0.03069 | best_loss=0.02986
Epoch 23/80: current_loss=0.03080 | best_loss=0.02986
Epoch 24/80: current_loss=0.03133 | best_loss=0.02986
Epoch 25/80: current_loss=0.03053 | best_loss=0.02986
Epoch 26/80: current_loss=0.02996 | best_loss=0.02986
Epoch 27/80: current_loss=0.03097 | best_loss=0.02986
Epoch 28/80: current_loss=0.03092 | best_loss=0.02986
Epoch 29/80: current_loss=0.03083 | best_loss=0.02986
Epoch 30/80: current_loss=0.03045 | best_loss=0.02986
Epoch 31/80: current_loss=0.03025 | best_loss=0.02986
Epoch 32/80: current_loss=0.03091 | best_loss=0.02986
Epoch 33/80: current_loss=0.03069 | best_loss=0.02986
Epoch 34/80: current_loss=0.03020 | best_loss=0.02986
Epoch 35/80: current_loss=0.03073 | best_loss=0.02986
Epoch 36/80: current_loss=0.03015 | best_loss=0.02986
Epoch 37/80: current_loss=0.02970 | best_loss=0.02970
Epoch 38/80: current_loss=0.03125 | best_loss=0.02970
Epoch 39/80: current_loss=0.03002 | best_loss=0.02970
Epoch 40/80: current_loss=0.03045 | best_loss=0.02970
Epoch 41/80: current_loss=0.03063 | best_loss=0.02970
Epoch 42/80: current_loss=0.03133 | best_loss=0.02970
Epoch 43/80: current_loss=0.03077 | best_loss=0.02970
Epoch 44/80: current_loss=0.03046 | best_loss=0.02970
Epoch 45/80: current_loss=0.03108 | best_loss=0.02970
Epoch 46/80: current_loss=0.03000 | best_loss=0.02970
Epoch 47/80: current_loss=0.03019 | best_loss=0.02970
Epoch 48/80: current_loss=0.03077 | best_loss=0.02970
Epoch 49/80: current_loss=0.02979 | best_loss=0.02970
Epoch 50/80: current_loss=0.03070 | best_loss=0.02970
Epoch 51/80: current_loss=0.03067 | best_loss=0.02970
Epoch 52/80: current_loss=0.02979 | best_loss=0.02970
Epoch 53/80: current_loss=0.03106 | best_loss=0.02970
Epoch 54/80: current_loss=0.03079 | best_loss=0.02970
Epoch 55/80: current_loss=0.03015 | best_loss=0.02970
Epoch 56/80: current_loss=0.03071 | best_loss=0.02970
Epoch 57/80: current_loss=0.03078 | best_loss=0.02970
Early Stopping at epoch 57
      explained_var=0.02721 | mse_loss=0.02899
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02809 | best_loss=0.02809
Epoch 1/80: current_loss=0.02855 | best_loss=0.02809
Epoch 2/80: current_loss=0.02819 | best_loss=0.02809
Epoch 3/80: current_loss=0.02835 | best_loss=0.02809
Epoch 4/80: current_loss=0.02840 | best_loss=0.02809
Epoch 5/80: current_loss=0.02825 | best_loss=0.02809
Epoch 6/80: current_loss=0.02846 | best_loss=0.02809
Epoch 7/80: current_loss=0.02835 | best_loss=0.02809
Epoch 8/80: current_loss=0.02846 | best_loss=0.02809
Epoch 9/80: current_loss=0.02867 | best_loss=0.02809
Epoch 10/80: current_loss=0.02856 | best_loss=0.02809
Epoch 11/80: current_loss=0.02851 | best_loss=0.02809
Epoch 12/80: current_loss=0.02869 | best_loss=0.02809
Epoch 13/80: current_loss=0.02842 | best_loss=0.02809
Epoch 14/80: current_loss=0.02840 | best_loss=0.02809
Epoch 15/80: current_loss=0.02842 | best_loss=0.02809
Epoch 16/80: current_loss=0.02856 | best_loss=0.02809
Epoch 17/80: current_loss=0.02853 | best_loss=0.02809
Epoch 18/80: current_loss=0.02874 | best_loss=0.02809
Epoch 19/80: current_loss=0.02860 | best_loss=0.02809
Epoch 20/80: current_loss=0.02864 | best_loss=0.02809
Early Stopping at epoch 20
      explained_var=-0.00015 | mse_loss=0.02839
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03366 | best_loss=0.03366
Epoch 1/80: current_loss=0.03381 | best_loss=0.03366
Epoch 2/80: current_loss=0.03383 | best_loss=0.03366
Epoch 3/80: current_loss=0.03373 | best_loss=0.03366
Epoch 4/80: current_loss=0.03392 | best_loss=0.03366
Epoch 5/80: current_loss=0.03398 | best_loss=0.03366
Epoch 6/80: current_loss=0.03404 | best_loss=0.03366
Epoch 7/80: current_loss=0.03415 | best_loss=0.03366
Epoch 8/80: current_loss=0.03409 | best_loss=0.03366
Epoch 9/80: current_loss=0.03413 | best_loss=0.03366
Epoch 10/80: current_loss=0.03409 | best_loss=0.03366
Epoch 11/80: current_loss=0.03405 | best_loss=0.03366
Epoch 12/80: current_loss=0.03398 | best_loss=0.03366
Epoch 13/80: current_loss=0.03404 | best_loss=0.03366
Epoch 14/80: current_loss=0.03394 | best_loss=0.03366
Epoch 15/80: current_loss=0.03404 | best_loss=0.03366
Epoch 16/80: current_loss=0.03402 | best_loss=0.03366
Epoch 17/80: current_loss=0.03403 | best_loss=0.03366
Epoch 18/80: current_loss=0.03404 | best_loss=0.03366
Epoch 19/80: current_loss=0.03408 | best_loss=0.03366
Epoch 20/80: current_loss=0.03414 | best_loss=0.03366
Early Stopping at epoch 20
      explained_var=-0.05653 | mse_loss=0.03449
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=-0.00184| avg_loss=0.02902
----------------------------------------------


----------------------------------------------
Params for Trial 36
{'learning_rate': 0.001, 'weight_decay': 0.0006800873349411869, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04174 | best_loss=0.04174
Epoch 1/80: current_loss=0.02810 | best_loss=0.02810
Epoch 2/80: current_loss=0.02739 | best_loss=0.02739
Epoch 3/80: current_loss=0.02640 | best_loss=0.02640
Epoch 4/80: current_loss=0.02638 | best_loss=0.02638
Epoch 5/80: current_loss=0.02708 | best_loss=0.02638
Epoch 6/80: current_loss=0.02624 | best_loss=0.02624
Epoch 7/80: current_loss=0.02603 | best_loss=0.02603
Epoch 8/80: current_loss=0.02720 | best_loss=0.02603
Epoch 9/80: current_loss=0.02645 | best_loss=0.02603
Epoch 10/80: current_loss=0.02595 | best_loss=0.02595
Epoch 11/80: current_loss=0.02637 | best_loss=0.02595
Epoch 12/80: current_loss=0.02649 | best_loss=0.02595
Epoch 13/80: current_loss=0.02678 | best_loss=0.02595
Epoch 14/80: current_loss=0.02602 | best_loss=0.02595
Epoch 15/80: current_loss=0.02619 | best_loss=0.02595
Epoch 16/80: current_loss=0.02641 | best_loss=0.02595
Epoch 17/80: current_loss=0.02705 | best_loss=0.02595
Epoch 18/80: current_loss=0.02584 | best_loss=0.02584
Epoch 19/80: current_loss=0.02718 | best_loss=0.02584
Epoch 20/80: current_loss=0.02625 | best_loss=0.02584
Epoch 21/80: current_loss=0.02695 | best_loss=0.02584
Epoch 22/80: current_loss=0.02571 | best_loss=0.02571
Epoch 23/80: current_loss=0.02600 | best_loss=0.02571
Epoch 24/80: current_loss=0.02612 | best_loss=0.02571
Epoch 25/80: current_loss=0.02604 | best_loss=0.02571
Epoch 26/80: current_loss=0.02592 | best_loss=0.02571
Epoch 27/80: current_loss=0.02641 | best_loss=0.02571
Epoch 28/80: current_loss=0.02664 | best_loss=0.02571
Epoch 29/80: current_loss=0.02697 | best_loss=0.02571
Epoch 30/80: current_loss=0.02605 | best_loss=0.02571
Epoch 31/80: current_loss=0.02595 | best_loss=0.02571
Epoch 32/80: current_loss=0.02626 | best_loss=0.02571
Epoch 33/80: current_loss=0.02689 | best_loss=0.02571
Epoch 34/80: current_loss=0.02581 | best_loss=0.02571
Epoch 35/80: current_loss=0.02635 | best_loss=0.02571
Epoch 36/80: current_loss=0.02654 | best_loss=0.02571
Epoch 37/80: current_loss=0.02647 | best_loss=0.02571
Epoch 38/80: current_loss=0.02657 | best_loss=0.02571
Epoch 39/80: current_loss=0.02617 | best_loss=0.02571
Epoch 40/80: current_loss=0.02582 | best_loss=0.02571
Epoch 41/80: current_loss=0.02734 | best_loss=0.02571
Epoch 42/80: current_loss=0.02610 | best_loss=0.02571
Early Stopping at epoch 42
      explained_var=0.03203 | mse_loss=0.02519
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02855 | best_loss=0.02855
Epoch 1/80: current_loss=0.02825 | best_loss=0.02825
Epoch 2/80: current_loss=0.02816 | best_loss=0.02816
Epoch 3/80: current_loss=0.02802 | best_loss=0.02802
Epoch 4/80: current_loss=0.02812 | best_loss=0.02802
Epoch 5/80: current_loss=0.02841 | best_loss=0.02802
Epoch 6/80: current_loss=0.02826 | best_loss=0.02802
Epoch 7/80: current_loss=0.02904 | best_loss=0.02802
Epoch 8/80: current_loss=0.02824 | best_loss=0.02802
Epoch 9/80: current_loss=0.02818 | best_loss=0.02802
Epoch 10/80: current_loss=0.02825 | best_loss=0.02802
Epoch 11/80: current_loss=0.02842 | best_loss=0.02802
Epoch 12/80: current_loss=0.02831 | best_loss=0.02802
Epoch 13/80: current_loss=0.02821 | best_loss=0.02802
Epoch 14/80: current_loss=0.02824 | best_loss=0.02802
Epoch 15/80: current_loss=0.02827 | best_loss=0.02802
Epoch 16/80: current_loss=0.02828 | best_loss=0.02802
Epoch 17/80: current_loss=0.02811 | best_loss=0.02802
Epoch 18/80: current_loss=0.02817 | best_loss=0.02802
Epoch 19/80: current_loss=0.02816 | best_loss=0.02802
Epoch 20/80: current_loss=0.02809 | best_loss=0.02802
Epoch 21/80: current_loss=0.02837 | best_loss=0.02802
Epoch 22/80: current_loss=0.02812 | best_loss=0.02802
Epoch 23/80: current_loss=0.02816 | best_loss=0.02802
Early Stopping at epoch 23
      explained_var=0.00953 | mse_loss=0.02753
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03080 | best_loss=0.03080
Epoch 1/80: current_loss=0.03084 | best_loss=0.03080
Epoch 2/80: current_loss=0.03074 | best_loss=0.03074
Epoch 3/80: current_loss=0.03039 | best_loss=0.03039
Epoch 4/80: current_loss=0.03076 | best_loss=0.03039
Epoch 5/80: current_loss=0.03107 | best_loss=0.03039
Epoch 6/80: current_loss=0.03100 | best_loss=0.03039
Epoch 7/80: current_loss=0.03138 | best_loss=0.03039
Epoch 8/80: current_loss=0.03032 | best_loss=0.03032
Epoch 9/80: current_loss=0.03079 | best_loss=0.03032
Epoch 10/80: current_loss=0.03105 | best_loss=0.03032
Epoch 11/80: current_loss=0.03118 | best_loss=0.03032
Epoch 12/80: current_loss=0.03054 | best_loss=0.03032
Epoch 13/80: current_loss=0.03128 | best_loss=0.03032
Epoch 14/80: current_loss=0.03085 | best_loss=0.03032
Epoch 15/80: current_loss=0.03143 | best_loss=0.03032
Epoch 16/80: current_loss=0.03065 | best_loss=0.03032
Epoch 17/80: current_loss=0.03081 | best_loss=0.03032
Epoch 18/80: current_loss=0.03125 | best_loss=0.03032
Epoch 19/80: current_loss=0.03039 | best_loss=0.03032
Epoch 20/80: current_loss=0.03062 | best_loss=0.03032
Epoch 21/80: current_loss=0.03111 | best_loss=0.03032
Epoch 22/80: current_loss=0.03078 | best_loss=0.03032
Epoch 23/80: current_loss=0.03116 | best_loss=0.03032
Epoch 24/80: current_loss=0.03102 | best_loss=0.03032
Epoch 25/80: current_loss=0.03031 | best_loss=0.03031
Epoch 26/80: current_loss=0.03084 | best_loss=0.03031
Epoch 27/80: current_loss=0.03094 | best_loss=0.03031
Epoch 28/80: current_loss=0.03082 | best_loss=0.03031
Epoch 29/80: current_loss=0.03129 | best_loss=0.03031
Epoch 30/80: current_loss=0.03130 | best_loss=0.03031
Epoch 31/80: current_loss=0.03009 | best_loss=0.03009
Epoch 32/80: current_loss=0.03080 | best_loss=0.03009
Epoch 33/80: current_loss=0.03120 | best_loss=0.03009
Epoch 34/80: current_loss=0.03096 | best_loss=0.03009
Epoch 35/80: current_loss=0.03105 | best_loss=0.03009
Epoch 36/80: current_loss=0.03124 | best_loss=0.03009
Epoch 37/80: current_loss=0.03030 | best_loss=0.03009
Epoch 38/80: current_loss=0.03054 | best_loss=0.03009
Epoch 39/80: current_loss=0.03135 | best_loss=0.03009
Epoch 40/80: current_loss=0.03063 | best_loss=0.03009
Epoch 41/80: current_loss=0.03094 | best_loss=0.03009
Epoch 42/80: current_loss=0.03057 | best_loss=0.03009
Epoch 43/80: current_loss=0.03089 | best_loss=0.03009
Epoch 44/80: current_loss=0.03052 | best_loss=0.03009
Epoch 45/80: current_loss=0.03132 | best_loss=0.03009
Epoch 46/80: current_loss=0.03107 | best_loss=0.03009
Epoch 47/80: current_loss=0.03162 | best_loss=0.03009
Epoch 48/80: current_loss=0.03104 | best_loss=0.03009
Epoch 49/80: current_loss=0.03035 | best_loss=0.03009
Epoch 50/80: current_loss=0.03088 | best_loss=0.03009
Epoch 51/80: current_loss=0.03116 | best_loss=0.03009
Early Stopping at epoch 51
      explained_var=0.01628 | mse_loss=0.02931
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02809 | best_loss=0.02809
Epoch 1/80: current_loss=0.02813 | best_loss=0.02809
Epoch 2/80: current_loss=0.02828 | best_loss=0.02809
Epoch 3/80: current_loss=0.02817 | best_loss=0.02809
Epoch 4/80: current_loss=0.02825 | best_loss=0.02809
Epoch 5/80: current_loss=0.02826 | best_loss=0.02809
Epoch 6/80: current_loss=0.02850 | best_loss=0.02809
Epoch 7/80: current_loss=0.02866 | best_loss=0.02809
Epoch 8/80: current_loss=0.02854 | best_loss=0.02809
Epoch 9/80: current_loss=0.02846 | best_loss=0.02809
Epoch 10/80: current_loss=0.02825 | best_loss=0.02809
Epoch 11/80: current_loss=0.02848 | best_loss=0.02809
Epoch 12/80: current_loss=0.02821 | best_loss=0.02809
Epoch 13/80: current_loss=0.02840 | best_loss=0.02809
Epoch 14/80: current_loss=0.02817 | best_loss=0.02809
Epoch 15/80: current_loss=0.02817 | best_loss=0.02809
Epoch 16/80: current_loss=0.02830 | best_loss=0.02809
Epoch 17/80: current_loss=0.02827 | best_loss=0.02809
Epoch 18/80: current_loss=0.02815 | best_loss=0.02809
Epoch 19/80: current_loss=0.02853 | best_loss=0.02809
Epoch 20/80: current_loss=0.02825 | best_loss=0.02809
Early Stopping at epoch 20
      explained_var=-0.00338 | mse_loss=0.02846
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03330 | best_loss=0.03330
Epoch 1/80: current_loss=0.03279 | best_loss=0.03279
Epoch 2/80: current_loss=0.03276 | best_loss=0.03276
Epoch 3/80: current_loss=0.03267 | best_loss=0.03267
Epoch 4/80: current_loss=0.03279 | best_loss=0.03267
Epoch 5/80: current_loss=0.03274 | best_loss=0.03267
Epoch 6/80: current_loss=0.03280 | best_loss=0.03267
Epoch 7/80: current_loss=0.03278 | best_loss=0.03267
Epoch 8/80: current_loss=0.03287 | best_loss=0.03267
Epoch 9/80: current_loss=0.03274 | best_loss=0.03267
Epoch 10/80: current_loss=0.03286 | best_loss=0.03267
Epoch 11/80: current_loss=0.03295 | best_loss=0.03267
Epoch 12/80: current_loss=0.03313 | best_loss=0.03267
Epoch 13/80: current_loss=0.03337 | best_loss=0.03267
Epoch 14/80: current_loss=0.03356 | best_loss=0.03267
Epoch 15/80: current_loss=0.03355 | best_loss=0.03267
Epoch 16/80: current_loss=0.03340 | best_loss=0.03267
Epoch 17/80: current_loss=0.03352 | best_loss=0.03267
Epoch 18/80: current_loss=0.03355 | best_loss=0.03267
Epoch 19/80: current_loss=0.03361 | best_loss=0.03267
Epoch 20/80: current_loss=0.03366 | best_loss=0.03267
Epoch 21/80: current_loss=0.03373 | best_loss=0.03267
Epoch 22/80: current_loss=0.03297 | best_loss=0.03267
Epoch 23/80: current_loss=0.03305 | best_loss=0.03267
Early Stopping at epoch 23
      explained_var=-0.02951 | mse_loss=0.03344
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00499| avg_loss=0.02879
----------------------------------------------


----------------------------------------------
Params for Trial 37
{'learning_rate': 0.001, 'weight_decay': 0.0010571773165327205, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02857 | best_loss=0.02857
Epoch 1/80: current_loss=0.02619 | best_loss=0.02619
Epoch 2/80: current_loss=0.02859 | best_loss=0.02619
Epoch 3/80: current_loss=0.02808 | best_loss=0.02619
Epoch 4/80: current_loss=0.03439 | best_loss=0.02619
Epoch 5/80: current_loss=0.03534 | best_loss=0.02619
Epoch 6/80: current_loss=0.03011 | best_loss=0.02619
Epoch 7/80: current_loss=0.04093 | best_loss=0.02619
Epoch 8/80: current_loss=0.02786 | best_loss=0.02619
Epoch 9/80: current_loss=0.02882 | best_loss=0.02619
Epoch 10/80: current_loss=0.02763 | best_loss=0.02619
Epoch 11/80: current_loss=0.02676 | best_loss=0.02619
Epoch 12/80: current_loss=0.02566 | best_loss=0.02566
Epoch 13/80: current_loss=0.02567 | best_loss=0.02566
Epoch 14/80: current_loss=0.02502 | best_loss=0.02502
Epoch 15/80: current_loss=0.02494 | best_loss=0.02494
Epoch 16/80: current_loss=0.02760 | best_loss=0.02494
Epoch 17/80: current_loss=0.02587 | best_loss=0.02494
Epoch 18/80: current_loss=0.02958 | best_loss=0.02494
Epoch 19/80: current_loss=0.02666 | best_loss=0.02494
Epoch 20/80: current_loss=0.02510 | best_loss=0.02494
Epoch 21/80: current_loss=0.02711 | best_loss=0.02494
Epoch 22/80: current_loss=0.02597 | best_loss=0.02494
Epoch 23/80: current_loss=0.03273 | best_loss=0.02494
Epoch 24/80: current_loss=0.03727 | best_loss=0.02494
Epoch 25/80: current_loss=0.03447 | best_loss=0.02494
Epoch 26/80: current_loss=0.02623 | best_loss=0.02494
Epoch 27/80: current_loss=0.02677 | best_loss=0.02494
Epoch 28/80: current_loss=0.03083 | best_loss=0.02494
Epoch 29/80: current_loss=0.02529 | best_loss=0.02494
Epoch 30/80: current_loss=0.02573 | best_loss=0.02494
Epoch 31/80: current_loss=0.02773 | best_loss=0.02494
Epoch 32/80: current_loss=0.02568 | best_loss=0.02494
Epoch 33/80: current_loss=0.02620 | best_loss=0.02494
Epoch 34/80: current_loss=0.03792 | best_loss=0.02494
Epoch 35/80: current_loss=0.02639 | best_loss=0.02494
Early Stopping at epoch 35
      explained_var=0.05571 | mse_loss=0.02448
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02733 | best_loss=0.02733
Epoch 1/80: current_loss=0.03003 | best_loss=0.02733
Epoch 2/80: current_loss=0.02816 | best_loss=0.02733
Epoch 3/80: current_loss=0.02801 | best_loss=0.02733
Epoch 4/80: current_loss=0.03026 | best_loss=0.02733
Epoch 5/80: current_loss=0.02789 | best_loss=0.02733
Epoch 6/80: current_loss=0.02845 | best_loss=0.02733
Epoch 7/80: current_loss=0.04935 | best_loss=0.02733
Epoch 8/80: current_loss=0.03053 | best_loss=0.02733
Epoch 9/80: current_loss=0.02819 | best_loss=0.02733
Epoch 10/80: current_loss=0.04608 | best_loss=0.02733
Epoch 11/80: current_loss=0.09275 | best_loss=0.02733
Epoch 12/80: current_loss=0.13348 | best_loss=0.02733
Epoch 13/80: current_loss=0.06678 | best_loss=0.02733
Epoch 14/80: current_loss=0.03796 | best_loss=0.02733
Epoch 15/80: current_loss=0.03396 | best_loss=0.02733
Epoch 16/80: current_loss=0.02878 | best_loss=0.02733
Epoch 17/80: current_loss=0.03009 | best_loss=0.02733
Epoch 18/80: current_loss=0.02917 | best_loss=0.02733
Epoch 19/80: current_loss=0.03001 | best_loss=0.02733
Epoch 20/80: current_loss=0.02816 | best_loss=0.02733
Early Stopping at epoch 20
      explained_var=0.03363 | mse_loss=0.02686
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03159 | best_loss=0.03159
Epoch 1/80: current_loss=0.02918 | best_loss=0.02918
Epoch 2/80: current_loss=0.03315 | best_loss=0.02918
Epoch 3/80: current_loss=0.03012 | best_loss=0.02918
Epoch 4/80: current_loss=0.03162 | best_loss=0.02918
Epoch 5/80: current_loss=0.03195 | best_loss=0.02918
Epoch 6/80: current_loss=0.03444 | best_loss=0.02918
Epoch 7/80: current_loss=0.03240 | best_loss=0.02918
Epoch 8/80: current_loss=0.03051 | best_loss=0.02918
Epoch 9/80: current_loss=0.03048 | best_loss=0.02918
Epoch 10/80: current_loss=0.03071 | best_loss=0.02918
Epoch 11/80: current_loss=0.02959 | best_loss=0.02918
Epoch 12/80: current_loss=0.03063 | best_loss=0.02918
Epoch 13/80: current_loss=0.03268 | best_loss=0.02918
Epoch 14/80: current_loss=0.03125 | best_loss=0.02918
Epoch 15/80: current_loss=0.03124 | best_loss=0.02918
Epoch 16/80: current_loss=0.03063 | best_loss=0.02918
Epoch 17/80: current_loss=0.03367 | best_loss=0.02918
Epoch 18/80: current_loss=0.03665 | best_loss=0.02918
Epoch 19/80: current_loss=0.03590 | best_loss=0.02918
Epoch 20/80: current_loss=0.03204 | best_loss=0.02918
Epoch 21/80: current_loss=0.03355 | best_loss=0.02918
Early Stopping at epoch 21
      explained_var=0.03899 | mse_loss=0.02842
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03250 | best_loss=0.03250
Epoch 1/80: current_loss=0.03097 | best_loss=0.03097
Epoch 2/80: current_loss=0.02849 | best_loss=0.02849
Epoch 3/80: current_loss=0.02932 | best_loss=0.02849
Epoch 4/80: current_loss=0.03089 | best_loss=0.02849
Epoch 5/80: current_loss=0.03354 | best_loss=0.02849
Epoch 6/80: current_loss=0.02901 | best_loss=0.02849
Epoch 7/80: current_loss=0.02937 | best_loss=0.02849
Epoch 8/80: current_loss=0.03207 | best_loss=0.02849
Epoch 9/80: current_loss=0.03249 | best_loss=0.02849
Epoch 10/80: current_loss=0.02840 | best_loss=0.02840
Epoch 11/80: current_loss=0.02817 | best_loss=0.02817
Epoch 12/80: current_loss=0.02821 | best_loss=0.02817
Epoch 13/80: current_loss=0.02882 | best_loss=0.02817
Epoch 14/80: current_loss=0.02830 | best_loss=0.02817
Epoch 15/80: current_loss=0.02869 | best_loss=0.02817
Epoch 16/80: current_loss=0.02953 | best_loss=0.02817
Epoch 17/80: current_loss=0.02848 | best_loss=0.02817
Epoch 18/80: current_loss=0.02847 | best_loss=0.02817
Epoch 19/80: current_loss=0.02895 | best_loss=0.02817
Epoch 20/80: current_loss=0.02986 | best_loss=0.02817
Epoch 21/80: current_loss=0.02888 | best_loss=0.02817
Epoch 22/80: current_loss=0.02847 | best_loss=0.02817
Epoch 23/80: current_loss=0.02871 | best_loss=0.02817
Epoch 24/80: current_loss=0.02829 | best_loss=0.02817
Epoch 25/80: current_loss=0.02884 | best_loss=0.02817
Epoch 26/80: current_loss=0.02897 | best_loss=0.02817
Epoch 27/80: current_loss=0.02927 | best_loss=0.02817
Epoch 28/80: current_loss=0.02864 | best_loss=0.02817
Epoch 29/80: current_loss=0.02876 | best_loss=0.02817
Epoch 30/80: current_loss=0.03011 | best_loss=0.02817
Epoch 31/80: current_loss=0.02853 | best_loss=0.02817
Early Stopping at epoch 31
      explained_var=-0.00625 | mse_loss=0.02854
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03305 | best_loss=0.03305
Epoch 1/80: current_loss=0.03402 | best_loss=0.03305
Epoch 2/80: current_loss=0.03482 | best_loss=0.03305
Epoch 3/80: current_loss=0.03406 | best_loss=0.03305
Epoch 4/80: current_loss=0.03395 | best_loss=0.03305
Epoch 5/80: current_loss=0.03458 | best_loss=0.03305
Epoch 6/80: current_loss=0.03603 | best_loss=0.03305
Epoch 7/80: current_loss=0.03357 | best_loss=0.03305
Epoch 8/80: current_loss=0.03365 | best_loss=0.03305
Epoch 9/80: current_loss=0.03437 | best_loss=0.03305
Epoch 10/80: current_loss=0.03451 | best_loss=0.03305
Epoch 11/80: current_loss=0.03378 | best_loss=0.03305
Epoch 12/80: current_loss=0.03279 | best_loss=0.03279
Epoch 13/80: current_loss=0.03339 | best_loss=0.03279
Epoch 14/80: current_loss=0.03373 | best_loss=0.03279
Epoch 15/80: current_loss=0.03363 | best_loss=0.03279
Epoch 16/80: current_loss=0.03420 | best_loss=0.03279
Epoch 17/80: current_loss=0.03353 | best_loss=0.03279
Epoch 18/80: current_loss=0.03522 | best_loss=0.03279
Epoch 19/80: current_loss=0.03407 | best_loss=0.03279
Epoch 20/80: current_loss=0.03336 | best_loss=0.03279
Epoch 21/80: current_loss=0.03519 | best_loss=0.03279
Epoch 22/80: current_loss=0.03342 | best_loss=0.03279
Epoch 23/80: current_loss=0.03300 | best_loss=0.03279
Epoch 24/80: current_loss=0.03412 | best_loss=0.03279
Epoch 25/80: current_loss=0.03565 | best_loss=0.03279
Epoch 26/80: current_loss=0.03277 | best_loss=0.03277
Epoch 27/80: current_loss=0.03407 | best_loss=0.03277
Epoch 28/80: current_loss=0.03462 | best_loss=0.03277
Epoch 29/80: current_loss=0.03446 | best_loss=0.03277
Epoch 30/80: current_loss=0.03415 | best_loss=0.03277
Epoch 31/80: current_loss=0.03610 | best_loss=0.03277
Epoch 32/80: current_loss=0.03373 | best_loss=0.03277
Epoch 33/80: current_loss=0.03447 | best_loss=0.03277
Epoch 34/80: current_loss=0.03492 | best_loss=0.03277
Epoch 35/80: current_loss=0.03429 | best_loss=0.03277
Epoch 36/80: current_loss=0.03600 | best_loss=0.03277
Epoch 37/80: current_loss=0.03371 | best_loss=0.03277
Epoch 38/80: current_loss=0.03343 | best_loss=0.03277
Epoch 39/80: current_loss=0.03478 | best_loss=0.03277
Epoch 40/80: current_loss=0.03305 | best_loss=0.03277
Epoch 41/80: current_loss=0.03387 | best_loss=0.03277
Epoch 42/80: current_loss=0.03464 | best_loss=0.03277
Epoch 43/80: current_loss=0.03458 | best_loss=0.03277
Epoch 44/80: current_loss=0.03397 | best_loss=0.03277
Epoch 45/80: current_loss=0.03474 | best_loss=0.03277
Epoch 46/80: current_loss=0.03448 | best_loss=0.03277
Early Stopping at epoch 46
      explained_var=-0.03176 | mse_loss=0.03354
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.01806| avg_loss=0.02837
----------------------------------------------


----------------------------------------------
Params for Trial 38
{'learning_rate': 0.1, 'weight_decay': 0.002273137603089439, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=18.14264 | best_loss=18.14264
Epoch 1/80: current_loss=1.51571 | best_loss=1.51571
Epoch 2/80: current_loss=0.66907 | best_loss=0.66907
Epoch 3/80: current_loss=0.17529 | best_loss=0.17529
Epoch 4/80: current_loss=0.12001 | best_loss=0.12001
Epoch 5/80: current_loss=0.76483 | best_loss=0.12001
Epoch 6/80: current_loss=0.19186 | best_loss=0.12001
Epoch 7/80: current_loss=1.07679 | best_loss=0.12001
Epoch 8/80: current_loss=0.12305 | best_loss=0.12001
Epoch 9/80: current_loss=0.19037 | best_loss=0.12001
Epoch 10/80: current_loss=0.76624 | best_loss=0.12001
Epoch 11/80: current_loss=0.62420 | best_loss=0.12001
Epoch 12/80: current_loss=0.04010 | best_loss=0.04010
Epoch 13/80: current_loss=0.45373 | best_loss=0.04010
Epoch 14/80: current_loss=0.81483 | best_loss=0.04010
Epoch 15/80: current_loss=0.74617 | best_loss=0.04010
Epoch 16/80: current_loss=0.04407 | best_loss=0.04010
Epoch 17/80: current_loss=0.11707 | best_loss=0.04010
Epoch 18/80: current_loss=0.12656 | best_loss=0.04010
Epoch 19/80: current_loss=0.36988 | best_loss=0.04010
Epoch 20/80: current_loss=0.26887 | best_loss=0.04010
Epoch 21/80: current_loss=1.20100 | best_loss=0.04010
Epoch 22/80: current_loss=0.03746 | best_loss=0.03746
Epoch 23/80: current_loss=0.84607 | best_loss=0.03746
Epoch 24/80: current_loss=0.54703 | best_loss=0.03746
Epoch 25/80: current_loss=0.66176 | best_loss=0.03746
Epoch 26/80: current_loss=0.03850 | best_loss=0.03746
Epoch 27/80: current_loss=0.12080 | best_loss=0.03746
Epoch 28/80: current_loss=0.09162 | best_loss=0.03746
Epoch 29/80: current_loss=0.03958 | best_loss=0.03746
Epoch 30/80: current_loss=0.15901 | best_loss=0.03746
Epoch 31/80: current_loss=0.06621 | best_loss=0.03746
Epoch 32/80: current_loss=0.11979 | best_loss=0.03746
Epoch 33/80: current_loss=0.14105 | best_loss=0.03746
Epoch 34/80: current_loss=3.19939 | best_loss=0.03746
Epoch 35/80: current_loss=0.24059 | best_loss=0.03746
Epoch 36/80: current_loss=4.00996 | best_loss=0.03746
Epoch 37/80: current_loss=4.43271 | best_loss=0.03746
Epoch 38/80: current_loss=0.53750 | best_loss=0.03746
Epoch 39/80: current_loss=1.38797 | best_loss=0.03746
Epoch 40/80: current_loss=2.80406 | best_loss=0.03746
Epoch 41/80: current_loss=0.70090 | best_loss=0.03746
Epoch 42/80: current_loss=0.34266 | best_loss=0.03746
Early Stopping at epoch 42
      explained_var=-0.25869 | mse_loss=0.03652

----------------------------------------------
Params for Trial 39
{'learning_rate': 0.0001, 'weight_decay': 0.0015857409629578727, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03765 | best_loss=0.03765
Epoch 1/80: current_loss=0.02937 | best_loss=0.02937
Epoch 2/80: current_loss=0.03147 | best_loss=0.02937
Epoch 3/80: current_loss=0.02873 | best_loss=0.02873
Epoch 4/80: current_loss=0.03029 | best_loss=0.02873
Epoch 5/80: current_loss=0.02796 | best_loss=0.02796
Epoch 6/80: current_loss=0.02832 | best_loss=0.02796
Epoch 7/80: current_loss=0.02731 | best_loss=0.02731
Epoch 8/80: current_loss=0.02668 | best_loss=0.02668
Epoch 9/80: current_loss=0.02703 | best_loss=0.02668
Epoch 10/80: current_loss=0.02606 | best_loss=0.02606
Epoch 11/80: current_loss=0.02725 | best_loss=0.02606
Epoch 12/80: current_loss=0.02707 | best_loss=0.02606
Epoch 13/80: current_loss=0.02654 | best_loss=0.02606
Epoch 14/80: current_loss=0.02633 | best_loss=0.02606
Epoch 15/80: current_loss=0.02635 | best_loss=0.02606
Epoch 16/80: current_loss=0.02621 | best_loss=0.02606
Epoch 17/80: current_loss=0.02514 | best_loss=0.02514
Epoch 18/80: current_loss=0.02805 | best_loss=0.02514
Epoch 19/80: current_loss=0.02538 | best_loss=0.02514
Epoch 20/80: current_loss=0.02536 | best_loss=0.02514
Epoch 21/80: current_loss=0.02804 | best_loss=0.02514
Epoch 22/80: current_loss=0.02550 | best_loss=0.02514
Epoch 23/80: current_loss=0.02709 | best_loss=0.02514
Epoch 24/80: current_loss=0.02585 | best_loss=0.02514
Epoch 25/80: current_loss=0.02581 | best_loss=0.02514
Epoch 26/80: current_loss=0.02634 | best_loss=0.02514
Epoch 27/80: current_loss=0.02734 | best_loss=0.02514
Epoch 28/80: current_loss=0.02563 | best_loss=0.02514
Epoch 29/80: current_loss=0.02996 | best_loss=0.02514
Epoch 30/80: current_loss=0.02552 | best_loss=0.02514
Epoch 31/80: current_loss=0.02852 | best_loss=0.02514
Epoch 32/80: current_loss=0.02578 | best_loss=0.02514
Epoch 33/80: current_loss=0.02826 | best_loss=0.02514
Epoch 34/80: current_loss=0.02575 | best_loss=0.02514
Epoch 35/80: current_loss=0.02695 | best_loss=0.02514
Epoch 36/80: current_loss=0.02568 | best_loss=0.02514
Epoch 37/80: current_loss=0.02761 | best_loss=0.02514
Early Stopping at epoch 37
      explained_var=0.05668 | mse_loss=0.02475
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02805 | best_loss=0.02805
Epoch 1/80: current_loss=0.02844 | best_loss=0.02805
Epoch 2/80: current_loss=0.02821 | best_loss=0.02805
Epoch 3/80: current_loss=0.02845 | best_loss=0.02805
Epoch 4/80: current_loss=0.02859 | best_loss=0.02805
Epoch 5/80: current_loss=0.02811 | best_loss=0.02805
Epoch 6/80: current_loss=0.02815 | best_loss=0.02805
Epoch 7/80: current_loss=0.02926 | best_loss=0.02805
Epoch 8/80: current_loss=0.02813 | best_loss=0.02805
Epoch 9/80: current_loss=0.02791 | best_loss=0.02791
Epoch 10/80: current_loss=0.02783 | best_loss=0.02783
Epoch 11/80: current_loss=0.02783 | best_loss=0.02783
Epoch 12/80: current_loss=0.02808 | best_loss=0.02783
Epoch 13/80: current_loss=0.02870 | best_loss=0.02783
Epoch 14/80: current_loss=0.02887 | best_loss=0.02783
Epoch 15/80: current_loss=0.02969 | best_loss=0.02783
Epoch 16/80: current_loss=0.02842 | best_loss=0.02783
Epoch 17/80: current_loss=0.02811 | best_loss=0.02783
Epoch 18/80: current_loss=0.02821 | best_loss=0.02783
Epoch 19/80: current_loss=0.02812 | best_loss=0.02783
Epoch 20/80: current_loss=0.02826 | best_loss=0.02783
Epoch 21/80: current_loss=0.02819 | best_loss=0.02783
Epoch 22/80: current_loss=0.02844 | best_loss=0.02783
Epoch 23/80: current_loss=0.02827 | best_loss=0.02783
Epoch 24/80: current_loss=0.02815 | best_loss=0.02783
Epoch 25/80: current_loss=0.02797 | best_loss=0.02783
Epoch 26/80: current_loss=0.02841 | best_loss=0.02783
Epoch 27/80: current_loss=0.02851 | best_loss=0.02783
Epoch 28/80: current_loss=0.02884 | best_loss=0.02783
Epoch 29/80: current_loss=0.02820 | best_loss=0.02783
Epoch 30/80: current_loss=0.02809 | best_loss=0.02783
Epoch 31/80: current_loss=0.02818 | best_loss=0.02783
Early Stopping at epoch 31
      explained_var=0.01590 | mse_loss=0.02737
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03004 | best_loss=0.03004
Epoch 1/80: current_loss=0.03247 | best_loss=0.03004
Epoch 2/80: current_loss=0.03285 | best_loss=0.03004
Epoch 3/80: current_loss=0.03010 | best_loss=0.03004
Epoch 4/80: current_loss=0.03070 | best_loss=0.03004
Epoch 5/80: current_loss=0.03067 | best_loss=0.03004
Epoch 6/80: current_loss=0.03097 | best_loss=0.03004
Epoch 7/80: current_loss=0.02974 | best_loss=0.02974
Epoch 8/80: current_loss=0.03296 | best_loss=0.02974
Epoch 9/80: current_loss=0.02974 | best_loss=0.02974
Epoch 10/80: current_loss=0.03105 | best_loss=0.02974
Epoch 11/80: current_loss=0.03006 | best_loss=0.02974
Epoch 12/80: current_loss=0.03017 | best_loss=0.02974
Epoch 13/80: current_loss=0.02964 | best_loss=0.02964
Epoch 14/80: current_loss=0.03057 | best_loss=0.02964
Epoch 15/80: current_loss=0.03041 | best_loss=0.02964
Epoch 16/80: current_loss=0.03026 | best_loss=0.02964
Epoch 17/80: current_loss=0.02987 | best_loss=0.02964
Epoch 18/80: current_loss=0.03302 | best_loss=0.02964
Epoch 19/80: current_loss=0.03035 | best_loss=0.02964
Epoch 20/80: current_loss=0.02990 | best_loss=0.02964
Epoch 21/80: current_loss=0.03110 | best_loss=0.02964
Epoch 22/80: current_loss=0.03036 | best_loss=0.02964
Epoch 23/80: current_loss=0.03011 | best_loss=0.02964
Epoch 24/80: current_loss=0.03120 | best_loss=0.02964
Epoch 25/80: current_loss=0.02967 | best_loss=0.02964
Epoch 26/80: current_loss=0.03143 | best_loss=0.02964
Epoch 27/80: current_loss=0.03010 | best_loss=0.02964
Epoch 28/80: current_loss=0.03025 | best_loss=0.02964
Epoch 29/80: current_loss=0.03092 | best_loss=0.02964
Epoch 30/80: current_loss=0.02977 | best_loss=0.02964
Epoch 31/80: current_loss=0.03147 | best_loss=0.02964
Epoch 32/80: current_loss=0.03050 | best_loss=0.02964
Epoch 33/80: current_loss=0.03036 | best_loss=0.02964
Early Stopping at epoch 33
      explained_var=0.03118 | mse_loss=0.02888
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02862 | best_loss=0.02862
Epoch 1/80: current_loss=0.02815 | best_loss=0.02815
Epoch 2/80: current_loss=0.02870 | best_loss=0.02815
Epoch 3/80: current_loss=0.02871 | best_loss=0.02815
Epoch 4/80: current_loss=0.02864 | best_loss=0.02815
Epoch 5/80: current_loss=0.02838 | best_loss=0.02815
Epoch 6/80: current_loss=0.02825 | best_loss=0.02815
Epoch 7/80: current_loss=0.02859 | best_loss=0.02815
Epoch 8/80: current_loss=0.02846 | best_loss=0.02815
Epoch 9/80: current_loss=0.02841 | best_loss=0.02815
Epoch 10/80: current_loss=0.02862 | best_loss=0.02815
Epoch 11/80: current_loss=0.02853 | best_loss=0.02815
Epoch 12/80: current_loss=0.02856 | best_loss=0.02815
Epoch 13/80: current_loss=0.02882 | best_loss=0.02815
Epoch 14/80: current_loss=0.02898 | best_loss=0.02815
Epoch 15/80: current_loss=0.02877 | best_loss=0.02815
Epoch 16/80: current_loss=0.02873 | best_loss=0.02815
Epoch 17/80: current_loss=0.02905 | best_loss=0.02815
Epoch 18/80: current_loss=0.02865 | best_loss=0.02815
Epoch 19/80: current_loss=0.02904 | best_loss=0.02815
Epoch 20/80: current_loss=0.02867 | best_loss=0.02815
Epoch 21/80: current_loss=0.02853 | best_loss=0.02815
Early Stopping at epoch 21
      explained_var=-0.00424 | mse_loss=0.02848
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03337 | best_loss=0.03337
Epoch 1/80: current_loss=0.03341 | best_loss=0.03337
Epoch 2/80: current_loss=0.03373 | best_loss=0.03337
Epoch 3/80: current_loss=0.03381 | best_loss=0.03337
Epoch 4/80: current_loss=0.03448 | best_loss=0.03337
Epoch 5/80: current_loss=0.03410 | best_loss=0.03337
Epoch 6/80: current_loss=0.03407 | best_loss=0.03337
Epoch 7/80: current_loss=0.03416 | best_loss=0.03337
Epoch 8/80: current_loss=0.03385 | best_loss=0.03337
Epoch 9/80: current_loss=0.03403 | best_loss=0.03337
Epoch 10/80: current_loss=0.03413 | best_loss=0.03337
Epoch 11/80: current_loss=0.03418 | best_loss=0.03337
Epoch 12/80: current_loss=0.03403 | best_loss=0.03337
Epoch 13/80: current_loss=0.03379 | best_loss=0.03337
Epoch 14/80: current_loss=0.03384 | best_loss=0.03337
Epoch 15/80: current_loss=0.03379 | best_loss=0.03337
Epoch 16/80: current_loss=0.03375 | best_loss=0.03337
Epoch 17/80: current_loss=0.03372 | best_loss=0.03337
Epoch 18/80: current_loss=0.03371 | best_loss=0.03337
Epoch 19/80: current_loss=0.03378 | best_loss=0.03337
Epoch 20/80: current_loss=0.03421 | best_loss=0.03337
Early Stopping at epoch 20
      explained_var=-0.05135 | mse_loss=0.03417
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=0.00963| avg_loss=0.02873
----------------------------------------------


----------------------------------------------
Params for Trial 40
{'learning_rate': 1e-05, 'weight_decay': 0.009764968251243388, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.21118 | best_loss=0.21118
Epoch 1/80: current_loss=0.14438 | best_loss=0.14438
Epoch 2/80: current_loss=0.09247 | best_loss=0.09247
Epoch 3/80: current_loss=0.05615 | best_loss=0.05615
Epoch 4/80: current_loss=0.03921 | best_loss=0.03921
Epoch 5/80: current_loss=0.03775 | best_loss=0.03775
Epoch 6/80: current_loss=0.03858 | best_loss=0.03775
Epoch 7/80: current_loss=0.03744 | best_loss=0.03744
Epoch 8/80: current_loss=0.03693 | best_loss=0.03693
Epoch 9/80: current_loss=0.03633 | best_loss=0.03633
Epoch 10/80: current_loss=0.03635 | best_loss=0.03633
Epoch 11/80: current_loss=0.03602 | best_loss=0.03602
Epoch 12/80: current_loss=0.03576 | best_loss=0.03576
Epoch 13/80: current_loss=0.03574 | best_loss=0.03574
Epoch 14/80: current_loss=0.03559 | best_loss=0.03559
Epoch 15/80: current_loss=0.03520 | best_loss=0.03520
Epoch 16/80: current_loss=0.03561 | best_loss=0.03520
Epoch 17/80: current_loss=0.03516 | best_loss=0.03516
Epoch 18/80: current_loss=0.03468 | best_loss=0.03468
Epoch 19/80: current_loss=0.03476 | best_loss=0.03468
Epoch 20/80: current_loss=0.03481 | best_loss=0.03468
Epoch 21/80: current_loss=0.03413 | best_loss=0.03413
Epoch 22/80: current_loss=0.03416 | best_loss=0.03413
Epoch 23/80: current_loss=0.03393 | best_loss=0.03393
Epoch 24/80: current_loss=0.03377 | best_loss=0.03377
Epoch 25/80: current_loss=0.03381 | best_loss=0.03377
Epoch 26/80: current_loss=0.03331 | best_loss=0.03331
Epoch 27/80: current_loss=0.03325 | best_loss=0.03325
Epoch 28/80: current_loss=0.03336 | best_loss=0.03325
Epoch 29/80: current_loss=0.03334 | best_loss=0.03325
Epoch 30/80: current_loss=0.03279 | best_loss=0.03279
Epoch 31/80: current_loss=0.03272 | best_loss=0.03272
Epoch 32/80: current_loss=0.03286 | best_loss=0.03272
Epoch 33/80: current_loss=0.03246 | best_loss=0.03246
Epoch 34/80: current_loss=0.03268 | best_loss=0.03246
Epoch 35/80: current_loss=0.03255 | best_loss=0.03246
Epoch 36/80: current_loss=0.03242 | best_loss=0.03242
Epoch 37/80: current_loss=0.03216 | best_loss=0.03216
Epoch 38/80: current_loss=0.03200 | best_loss=0.03200
Epoch 39/80: current_loss=0.03197 | best_loss=0.03197
Epoch 40/80: current_loss=0.03192 | best_loss=0.03192
Epoch 41/80: current_loss=0.03204 | best_loss=0.03192
Epoch 42/80: current_loss=0.03190 | best_loss=0.03190
Epoch 43/80: current_loss=0.03174 | best_loss=0.03174
Epoch 44/80: current_loss=0.03191 | best_loss=0.03174
Epoch 45/80: current_loss=0.03133 | best_loss=0.03133
Epoch 46/80: current_loss=0.03161 | best_loss=0.03133
Epoch 47/80: current_loss=0.03177 | best_loss=0.03133
Epoch 48/80: current_loss=0.03140 | best_loss=0.03133
Epoch 49/80: current_loss=0.03083 | best_loss=0.03083
Epoch 50/80: current_loss=0.03075 | best_loss=0.03075
Epoch 51/80: current_loss=0.03059 | best_loss=0.03059
Epoch 52/80: current_loss=0.03100 | best_loss=0.03059
Epoch 53/80: current_loss=0.03067 | best_loss=0.03059
Epoch 54/80: current_loss=0.03077 | best_loss=0.03059
Epoch 55/80: current_loss=0.03040 | best_loss=0.03040
Epoch 56/80: current_loss=0.03042 | best_loss=0.03040
Epoch 57/80: current_loss=0.03044 | best_loss=0.03040
Epoch 58/80: current_loss=0.03044 | best_loss=0.03040
Epoch 59/80: current_loss=0.03034 | best_loss=0.03034
Epoch 60/80: current_loss=0.03008 | best_loss=0.03008
Epoch 61/80: current_loss=0.03013 | best_loss=0.03008
Epoch 62/80: current_loss=0.03021 | best_loss=0.03008
Epoch 63/80: current_loss=0.03013 | best_loss=0.03008
Epoch 64/80: current_loss=0.03007 | best_loss=0.03007
Epoch 65/80: current_loss=0.03061 | best_loss=0.03007
Epoch 66/80: current_loss=0.03078 | best_loss=0.03007
Epoch 67/80: current_loss=0.02988 | best_loss=0.02988
Epoch 68/80: current_loss=0.02947 | best_loss=0.02947
Epoch 69/80: current_loss=0.02995 | best_loss=0.02947
Epoch 70/80: current_loss=0.03005 | best_loss=0.02947
Epoch 71/80: current_loss=0.02965 | best_loss=0.02947
Epoch 72/80: current_loss=0.02944 | best_loss=0.02944
Epoch 73/80: current_loss=0.02977 | best_loss=0.02944
Epoch 74/80: current_loss=0.02940 | best_loss=0.02940
Epoch 75/80: current_loss=0.03017 | best_loss=0.02940
Epoch 76/80: current_loss=0.03013 | best_loss=0.02940
Epoch 77/80: current_loss=0.02974 | best_loss=0.02940
Epoch 78/80: current_loss=0.02949 | best_loss=0.02940
Epoch 79/80: current_loss=0.02953 | best_loss=0.02940
      explained_var=-0.10471 | mse_loss=0.02891

----------------------------------------------
Params for Trial 41
{'learning_rate': 0.001, 'weight_decay': 0.0007129580989387352, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03161 | best_loss=0.03161
Epoch 1/80: current_loss=0.02834 | best_loss=0.02834
Epoch 2/80: current_loss=0.02591 | best_loss=0.02591
Epoch 3/80: current_loss=0.02513 | best_loss=0.02513
Epoch 4/80: current_loss=0.03017 | best_loss=0.02513
Epoch 5/80: current_loss=0.02585 | best_loss=0.02513
Epoch 6/80: current_loss=0.02981 | best_loss=0.02513
Epoch 7/80: current_loss=0.02579 | best_loss=0.02513
Epoch 8/80: current_loss=0.02689 | best_loss=0.02513
Epoch 9/80: current_loss=0.02621 | best_loss=0.02513
Epoch 10/80: current_loss=0.02769 | best_loss=0.02513
Epoch 11/80: current_loss=0.02496 | best_loss=0.02496
Epoch 12/80: current_loss=0.03118 | best_loss=0.02496
Epoch 13/80: current_loss=0.02736 | best_loss=0.02496
Epoch 14/80: current_loss=0.02690 | best_loss=0.02496
Epoch 15/80: current_loss=0.02734 | best_loss=0.02496
Epoch 16/80: current_loss=0.02743 | best_loss=0.02496
Epoch 17/80: current_loss=0.02696 | best_loss=0.02496
Epoch 18/80: current_loss=0.02853 | best_loss=0.02496
Epoch 19/80: current_loss=0.03193 | best_loss=0.02496
Epoch 20/80: current_loss=0.02846 | best_loss=0.02496
Epoch 21/80: current_loss=0.02876 | best_loss=0.02496
Epoch 22/80: current_loss=0.02816 | best_loss=0.02496
Epoch 23/80: current_loss=0.02890 | best_loss=0.02496
Epoch 24/80: current_loss=0.02545 | best_loss=0.02496
Epoch 25/80: current_loss=0.02640 | best_loss=0.02496
Epoch 26/80: current_loss=0.02686 | best_loss=0.02496
Epoch 27/80: current_loss=0.02631 | best_loss=0.02496
Epoch 28/80: current_loss=0.02574 | best_loss=0.02496
Epoch 29/80: current_loss=0.02571 | best_loss=0.02496
Epoch 30/80: current_loss=0.02615 | best_loss=0.02496
Epoch 31/80: current_loss=0.02682 | best_loss=0.02496
Early Stopping at epoch 31
      explained_var=0.06493 | mse_loss=0.02459
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03298 | best_loss=0.03298
Epoch 1/80: current_loss=0.03161 | best_loss=0.03161
Epoch 2/80: current_loss=0.03230 | best_loss=0.03161
Epoch 3/80: current_loss=0.02890 | best_loss=0.02890
Epoch 4/80: current_loss=0.02734 | best_loss=0.02734
Epoch 5/80: current_loss=0.02863 | best_loss=0.02734
Epoch 6/80: current_loss=0.02880 | best_loss=0.02734
Epoch 7/80: current_loss=0.02861 | best_loss=0.02734
Epoch 8/80: current_loss=0.03139 | best_loss=0.02734
Epoch 9/80: current_loss=0.02758 | best_loss=0.02734
Epoch 10/80: current_loss=0.02791 | best_loss=0.02734
Epoch 11/80: current_loss=0.03278 | best_loss=0.02734
Epoch 12/80: current_loss=0.02820 | best_loss=0.02734
Epoch 13/80: current_loss=0.02824 | best_loss=0.02734
Epoch 14/80: current_loss=0.02747 | best_loss=0.02734
Epoch 15/80: current_loss=0.02777 | best_loss=0.02734
Epoch 16/80: current_loss=0.02906 | best_loss=0.02734
Epoch 17/80: current_loss=0.02827 | best_loss=0.02734
Epoch 18/80: current_loss=0.03022 | best_loss=0.02734
Epoch 19/80: current_loss=0.02799 | best_loss=0.02734
Epoch 20/80: current_loss=0.02993 | best_loss=0.02734
Epoch 21/80: current_loss=0.02767 | best_loss=0.02734
Epoch 22/80: current_loss=0.02786 | best_loss=0.02734
Epoch 23/80: current_loss=0.02771 | best_loss=0.02734
Epoch 24/80: current_loss=0.02774 | best_loss=0.02734
Early Stopping at epoch 24
      explained_var=0.03468 | mse_loss=0.02682
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03138 | best_loss=0.03138
Epoch 1/80: current_loss=0.03001 | best_loss=0.03001
Epoch 2/80: current_loss=0.02894 | best_loss=0.02894
Epoch 3/80: current_loss=0.02971 | best_loss=0.02894
Epoch 4/80: current_loss=0.02967 | best_loss=0.02894
Epoch 5/80: current_loss=0.03282 | best_loss=0.02894
Epoch 6/80: current_loss=0.03257 | best_loss=0.02894
Epoch 7/80: current_loss=0.03510 | best_loss=0.02894
Epoch 8/80: current_loss=0.03268 | best_loss=0.02894
Epoch 9/80: current_loss=0.03285 | best_loss=0.02894
Epoch 10/80: current_loss=0.03001 | best_loss=0.02894
Epoch 11/80: current_loss=0.03035 | best_loss=0.02894
Epoch 12/80: current_loss=0.02936 | best_loss=0.02894
Epoch 13/80: current_loss=0.03129 | best_loss=0.02894
Epoch 14/80: current_loss=0.03157 | best_loss=0.02894
Epoch 15/80: current_loss=0.03535 | best_loss=0.02894
Epoch 16/80: current_loss=0.03305 | best_loss=0.02894
Epoch 17/80: current_loss=0.03196 | best_loss=0.02894
Epoch 18/80: current_loss=0.03421 | best_loss=0.02894
Epoch 19/80: current_loss=0.03497 | best_loss=0.02894
Epoch 20/80: current_loss=0.03159 | best_loss=0.02894
Epoch 21/80: current_loss=0.03118 | best_loss=0.02894
Epoch 22/80: current_loss=0.02964 | best_loss=0.02894
Early Stopping at epoch 22
      explained_var=0.05255 | mse_loss=0.02824
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02779 | best_loss=0.02779
Epoch 1/80: current_loss=0.03007 | best_loss=0.02779
Epoch 2/80: current_loss=0.02866 | best_loss=0.02779
Epoch 3/80: current_loss=0.03181 | best_loss=0.02779
Epoch 4/80: current_loss=0.03679 | best_loss=0.02779
Epoch 5/80: current_loss=0.03034 | best_loss=0.02779
Epoch 6/80: current_loss=0.03194 | best_loss=0.02779
Epoch 7/80: current_loss=0.02990 | best_loss=0.02779
Epoch 8/80: current_loss=0.03027 | best_loss=0.02779
Epoch 9/80: current_loss=0.02938 | best_loss=0.02779
Epoch 10/80: current_loss=0.02955 | best_loss=0.02779
Epoch 11/80: current_loss=0.02867 | best_loss=0.02779
Epoch 12/80: current_loss=0.02909 | best_loss=0.02779
Epoch 13/80: current_loss=0.02864 | best_loss=0.02779
Epoch 14/80: current_loss=0.02862 | best_loss=0.02779
Epoch 15/80: current_loss=0.02870 | best_loss=0.02779
Epoch 16/80: current_loss=0.02844 | best_loss=0.02779
Epoch 17/80: current_loss=0.02910 | best_loss=0.02779
Epoch 18/80: current_loss=0.02938 | best_loss=0.02779
Epoch 19/80: current_loss=0.03013 | best_loss=0.02779
Epoch 20/80: current_loss=0.03118 | best_loss=0.02779
Early Stopping at epoch 20
      explained_var=0.00921 | mse_loss=0.02815
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03735 | best_loss=0.03735
Epoch 1/80: current_loss=0.03212 | best_loss=0.03212
Epoch 2/80: current_loss=0.03279 | best_loss=0.03212
Epoch 3/80: current_loss=0.03484 | best_loss=0.03212
Epoch 4/80: current_loss=0.03702 | best_loss=0.03212
Epoch 5/80: current_loss=0.03509 | best_loss=0.03212
Epoch 6/80: current_loss=0.03400 | best_loss=0.03212
Epoch 7/80: current_loss=0.03297 | best_loss=0.03212
Epoch 8/80: current_loss=0.03269 | best_loss=0.03212
Epoch 9/80: current_loss=0.03413 | best_loss=0.03212
Epoch 10/80: current_loss=0.03377 | best_loss=0.03212
Epoch 11/80: current_loss=0.03388 | best_loss=0.03212
Epoch 12/80: current_loss=0.03605 | best_loss=0.03212
Epoch 13/80: current_loss=0.03478 | best_loss=0.03212
Epoch 14/80: current_loss=0.03320 | best_loss=0.03212
Epoch 15/80: current_loss=0.03328 | best_loss=0.03212
Epoch 16/80: current_loss=0.03422 | best_loss=0.03212
Epoch 17/80: current_loss=0.03405 | best_loss=0.03212
Epoch 18/80: current_loss=0.03302 | best_loss=0.03212
Epoch 19/80: current_loss=0.03414 | best_loss=0.03212
Epoch 20/80: current_loss=0.03373 | best_loss=0.03212
Epoch 21/80: current_loss=0.03323 | best_loss=0.03212
Early Stopping at epoch 21
      explained_var=-0.00808 | mse_loss=0.03275
----------------------------------------------
Average early_stopping_point: 3| avg_exp_var=0.03066| avg_loss=0.02811
----------------------------------------------


----------------------------------------------
Params for Trial 42
{'learning_rate': 0.001, 'weight_decay': 0.0007690493383900132, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03025 | best_loss=0.03025
Epoch 1/80: current_loss=0.02649 | best_loss=0.02649
Epoch 2/80: current_loss=0.02774 | best_loss=0.02649
Epoch 3/80: current_loss=0.02897 | best_loss=0.02649
Epoch 4/80: current_loss=0.02781 | best_loss=0.02649
Epoch 5/80: current_loss=0.02897 | best_loss=0.02649
Epoch 6/80: current_loss=0.02938 | best_loss=0.02649
Epoch 7/80: current_loss=0.02586 | best_loss=0.02586
Epoch 8/80: current_loss=0.02672 | best_loss=0.02586
Epoch 9/80: current_loss=0.02656 | best_loss=0.02586
Epoch 10/80: current_loss=0.02780 | best_loss=0.02586
Epoch 11/80: current_loss=0.02709 | best_loss=0.02586
Epoch 12/80: current_loss=0.02763 | best_loss=0.02586
Epoch 13/80: current_loss=0.02819 | best_loss=0.02586
Epoch 14/80: current_loss=0.02670 | best_loss=0.02586
Epoch 15/80: current_loss=0.02619 | best_loss=0.02586
Epoch 16/80: current_loss=0.02735 | best_loss=0.02586
Epoch 17/80: current_loss=0.02871 | best_loss=0.02586
Epoch 18/80: current_loss=0.02674 | best_loss=0.02586
Epoch 19/80: current_loss=0.02554 | best_loss=0.02554
Epoch 20/80: current_loss=0.02637 | best_loss=0.02554
Epoch 21/80: current_loss=0.03740 | best_loss=0.02554
Epoch 22/80: current_loss=0.02716 | best_loss=0.02554
Epoch 23/80: current_loss=0.02508 | best_loss=0.02508
Epoch 24/80: current_loss=0.02632 | best_loss=0.02508
Epoch 25/80: current_loss=0.02733 | best_loss=0.02508
Epoch 26/80: current_loss=0.02624 | best_loss=0.02508
Epoch 27/80: current_loss=0.02899 | best_loss=0.02508
Epoch 28/80: current_loss=0.03010 | best_loss=0.02508
Epoch 29/80: current_loss=0.02649 | best_loss=0.02508
Epoch 30/80: current_loss=0.04403 | best_loss=0.02508
Epoch 31/80: current_loss=0.02601 | best_loss=0.02508
Epoch 32/80: current_loss=0.02537 | best_loss=0.02508
Epoch 33/80: current_loss=0.02895 | best_loss=0.02508
Epoch 34/80: current_loss=0.02854 | best_loss=0.02508
Epoch 35/80: current_loss=0.02585 | best_loss=0.02508
Epoch 36/80: current_loss=0.02580 | best_loss=0.02508
Epoch 37/80: current_loss=0.02645 | best_loss=0.02508
Epoch 38/80: current_loss=0.03090 | best_loss=0.02508
Epoch 39/80: current_loss=0.02564 | best_loss=0.02508
Epoch 40/80: current_loss=0.02643 | best_loss=0.02508
Epoch 41/80: current_loss=0.02608 | best_loss=0.02508
Epoch 42/80: current_loss=0.02530 | best_loss=0.02508
Epoch 43/80: current_loss=0.02810 | best_loss=0.02508
Early Stopping at epoch 43
      explained_var=0.05254 | mse_loss=0.02462
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03116 | best_loss=0.03116
Epoch 1/80: current_loss=0.02825 | best_loss=0.02825
Epoch 2/80: current_loss=0.02802 | best_loss=0.02802
Epoch 3/80: current_loss=0.03003 | best_loss=0.02802
Epoch 4/80: current_loss=0.03070 | best_loss=0.02802
Epoch 5/80: current_loss=0.02813 | best_loss=0.02802
Epoch 6/80: current_loss=0.02858 | best_loss=0.02802
Epoch 7/80: current_loss=0.02821 | best_loss=0.02802
Epoch 8/80: current_loss=0.02790 | best_loss=0.02790
Epoch 9/80: current_loss=0.03411 | best_loss=0.02790
Epoch 10/80: current_loss=0.02925 | best_loss=0.02790
Epoch 11/80: current_loss=0.02801 | best_loss=0.02790
Epoch 12/80: current_loss=0.02820 | best_loss=0.02790
Epoch 13/80: current_loss=0.03118 | best_loss=0.02790
Epoch 14/80: current_loss=0.02939 | best_loss=0.02790
Epoch 15/80: current_loss=0.03052 | best_loss=0.02790
Epoch 16/80: current_loss=0.02910 | best_loss=0.02790
Epoch 17/80: current_loss=0.02819 | best_loss=0.02790
Epoch 18/80: current_loss=0.02794 | best_loss=0.02790
Epoch 19/80: current_loss=0.02840 | best_loss=0.02790
Epoch 20/80: current_loss=0.02837 | best_loss=0.02790
Epoch 21/80: current_loss=0.02954 | best_loss=0.02790
Epoch 22/80: current_loss=0.03042 | best_loss=0.02790
Epoch 23/80: current_loss=0.02819 | best_loss=0.02790
Epoch 24/80: current_loss=0.02807 | best_loss=0.02790
Epoch 25/80: current_loss=0.02799 | best_loss=0.02790
Epoch 26/80: current_loss=0.02892 | best_loss=0.02790
Epoch 27/80: current_loss=0.02828 | best_loss=0.02790
Epoch 28/80: current_loss=0.02860 | best_loss=0.02790
Early Stopping at epoch 28
      explained_var=0.01353 | mse_loss=0.02744
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03262 | best_loss=0.03262
Epoch 1/80: current_loss=0.03129 | best_loss=0.03129
Epoch 2/80: current_loss=0.03058 | best_loss=0.03058
Epoch 3/80: current_loss=0.03501 | best_loss=0.03058
Epoch 4/80: current_loss=0.03440 | best_loss=0.03058
Epoch 5/80: current_loss=0.03164 | best_loss=0.03058
Epoch 6/80: current_loss=0.03154 | best_loss=0.03058
Epoch 7/80: current_loss=0.60886 | best_loss=0.03058
Epoch 8/80: current_loss=0.09186 | best_loss=0.03058
Epoch 9/80: current_loss=0.08612 | best_loss=0.03058
Epoch 10/80: current_loss=0.03904 | best_loss=0.03058
Epoch 11/80: current_loss=0.06345 | best_loss=0.03058
Epoch 12/80: current_loss=0.03868 | best_loss=0.03058
Epoch 13/80: current_loss=0.04510 | best_loss=0.03058
Epoch 14/80: current_loss=0.03375 | best_loss=0.03058
Epoch 15/80: current_loss=0.03031 | best_loss=0.03031
Epoch 16/80: current_loss=0.03495 | best_loss=0.03031
Epoch 17/80: current_loss=0.03568 | best_loss=0.03031
Epoch 18/80: current_loss=0.03769 | best_loss=0.03031
Epoch 19/80: current_loss=0.02864 | best_loss=0.02864
Epoch 20/80: current_loss=0.03492 | best_loss=0.02864
Epoch 21/80: current_loss=0.03176 | best_loss=0.02864
Epoch 22/80: current_loss=0.03417 | best_loss=0.02864
Epoch 23/80: current_loss=0.03203 | best_loss=0.02864
Epoch 24/80: current_loss=0.03270 | best_loss=0.02864
Epoch 25/80: current_loss=0.03108 | best_loss=0.02864
Epoch 26/80: current_loss=0.03339 | best_loss=0.02864
Epoch 27/80: current_loss=0.03062 | best_loss=0.02864
Epoch 28/80: current_loss=0.03157 | best_loss=0.02864
Epoch 29/80: current_loss=0.03085 | best_loss=0.02864
Epoch 30/80: current_loss=0.03049 | best_loss=0.02864
Epoch 31/80: current_loss=0.03143 | best_loss=0.02864
Epoch 32/80: current_loss=0.03040 | best_loss=0.02864
Epoch 33/80: current_loss=0.03096 | best_loss=0.02864
Epoch 34/80: current_loss=0.03056 | best_loss=0.02864
Epoch 35/80: current_loss=0.03070 | best_loss=0.02864
Epoch 36/80: current_loss=0.03083 | best_loss=0.02864
Epoch 37/80: current_loss=0.03043 | best_loss=0.02864
Epoch 38/80: current_loss=0.03042 | best_loss=0.02864
Epoch 39/80: current_loss=0.03089 | best_loss=0.02864
Early Stopping at epoch 39
      explained_var=0.04826 | mse_loss=0.02820
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04913 | best_loss=0.04913
Epoch 1/80: current_loss=0.02684 | best_loss=0.02684
Epoch 2/80: current_loss=0.03140 | best_loss=0.02684
Epoch 3/80: current_loss=0.03196 | best_loss=0.02684
Epoch 4/80: current_loss=0.03083 | best_loss=0.02684
Epoch 5/80: current_loss=0.02848 | best_loss=0.02684
Epoch 6/80: current_loss=0.02879 | best_loss=0.02684
Epoch 7/80: current_loss=0.02854 | best_loss=0.02684
Epoch 8/80: current_loss=0.02835 | best_loss=0.02684
Epoch 9/80: current_loss=0.02832 | best_loss=0.02684
Epoch 10/80: current_loss=0.02830 | best_loss=0.02684
Epoch 11/80: current_loss=0.02846 | best_loss=0.02684
Epoch 12/80: current_loss=0.02800 | best_loss=0.02684
Epoch 13/80: current_loss=0.02883 | best_loss=0.02684
Epoch 14/80: current_loss=0.03099 | best_loss=0.02684
Epoch 15/80: current_loss=0.02853 | best_loss=0.02684
Epoch 16/80: current_loss=0.02834 | best_loss=0.02684
Epoch 17/80: current_loss=0.02881 | best_loss=0.02684
Epoch 18/80: current_loss=0.02936 | best_loss=0.02684
Epoch 19/80: current_loss=0.02924 | best_loss=0.02684
Epoch 20/80: current_loss=0.03063 | best_loss=0.02684
Epoch 21/80: current_loss=0.03309 | best_loss=0.02684
Early Stopping at epoch 21
      explained_var=0.04669 | mse_loss=0.02710
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03277 | best_loss=0.03277
Epoch 1/80: current_loss=0.03461 | best_loss=0.03277
Epoch 2/80: current_loss=0.03464 | best_loss=0.03277
Epoch 3/80: current_loss=0.03305 | best_loss=0.03277
Epoch 4/80: current_loss=0.03343 | best_loss=0.03277
Epoch 5/80: current_loss=0.03486 | best_loss=0.03277
Epoch 6/80: current_loss=0.03358 | best_loss=0.03277
Epoch 7/80: current_loss=0.03489 | best_loss=0.03277
Epoch 8/80: current_loss=0.03462 | best_loss=0.03277
Epoch 9/80: current_loss=0.03453 | best_loss=0.03277
Epoch 10/80: current_loss=0.03598 | best_loss=0.03277
Epoch 11/80: current_loss=0.03617 | best_loss=0.03277
Epoch 12/80: current_loss=0.03412 | best_loss=0.03277
Epoch 13/80: current_loss=0.03386 | best_loss=0.03277
Epoch 14/80: current_loss=0.03393 | best_loss=0.03277
Epoch 15/80: current_loss=0.03483 | best_loss=0.03277
Epoch 16/80: current_loss=0.03485 | best_loss=0.03277
Epoch 17/80: current_loss=0.03373 | best_loss=0.03277
Epoch 18/80: current_loss=0.03291 | best_loss=0.03277
Epoch 19/80: current_loss=0.03375 | best_loss=0.03277
Epoch 20/80: current_loss=0.03411 | best_loss=0.03277
Early Stopping at epoch 20
      explained_var=-0.03258 | mse_loss=0.03355
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.02569| avg_loss=0.02818
----------------------------------------------


----------------------------------------------
Params for Trial 43
{'learning_rate': 0.001, 'weight_decay': 0.0007045610084180968, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.10914 | best_loss=0.10914
Epoch 1/80: current_loss=0.02871 | best_loss=0.02871
Epoch 2/80: current_loss=0.02830 | best_loss=0.02830
Epoch 3/80: current_loss=0.03272 | best_loss=0.02830
Epoch 4/80: current_loss=0.02717 | best_loss=0.02717
Epoch 5/80: current_loss=0.02698 | best_loss=0.02698
Epoch 6/80: current_loss=0.02833 | best_loss=0.02698
Epoch 7/80: current_loss=0.02541 | best_loss=0.02541
Epoch 8/80: current_loss=0.02767 | best_loss=0.02541
Epoch 9/80: current_loss=0.02643 | best_loss=0.02541
Epoch 10/80: current_loss=0.02553 | best_loss=0.02541
Epoch 11/80: current_loss=0.02878 | best_loss=0.02541
Epoch 12/80: current_loss=0.02566 | best_loss=0.02541
Epoch 13/80: current_loss=0.02814 | best_loss=0.02541
Epoch 14/80: current_loss=0.03173 | best_loss=0.02541
Epoch 15/80: current_loss=0.02612 | best_loss=0.02541
Epoch 16/80: current_loss=0.02604 | best_loss=0.02541
Epoch 17/80: current_loss=0.03236 | best_loss=0.02541
Epoch 18/80: current_loss=0.02625 | best_loss=0.02541
Epoch 19/80: current_loss=0.02619 | best_loss=0.02541
Epoch 20/80: current_loss=0.02525 | best_loss=0.02525
Epoch 21/80: current_loss=0.02859 | best_loss=0.02525
Epoch 22/80: current_loss=0.02613 | best_loss=0.02525
Epoch 23/80: current_loss=0.02927 | best_loss=0.02525
Epoch 24/80: current_loss=0.02555 | best_loss=0.02525
Epoch 25/80: current_loss=0.02495 | best_loss=0.02495
Epoch 26/80: current_loss=0.02649 | best_loss=0.02495
Epoch 27/80: current_loss=0.02517 | best_loss=0.02495
Epoch 28/80: current_loss=0.02875 | best_loss=0.02495
Epoch 29/80: current_loss=0.02817 | best_loss=0.02495
Epoch 30/80: current_loss=0.02577 | best_loss=0.02495
Epoch 31/80: current_loss=0.02587 | best_loss=0.02495
Epoch 32/80: current_loss=0.02728 | best_loss=0.02495
Epoch 33/80: current_loss=0.02987 | best_loss=0.02495
Epoch 34/80: current_loss=0.03072 | best_loss=0.02495
Epoch 35/80: current_loss=0.03293 | best_loss=0.02495
Epoch 36/80: current_loss=0.02516 | best_loss=0.02495
Epoch 37/80: current_loss=0.02483 | best_loss=0.02483
Epoch 38/80: current_loss=0.02561 | best_loss=0.02483
Epoch 39/80: current_loss=0.02604 | best_loss=0.02483
Epoch 40/80: current_loss=0.02699 | best_loss=0.02483
Epoch 41/80: current_loss=0.02554 | best_loss=0.02483
Epoch 42/80: current_loss=0.02630 | best_loss=0.02483
Epoch 43/80: current_loss=0.02568 | best_loss=0.02483
Epoch 44/80: current_loss=0.02607 | best_loss=0.02483
Epoch 45/80: current_loss=0.02493 | best_loss=0.02483
Epoch 46/80: current_loss=0.02670 | best_loss=0.02483
Epoch 47/80: current_loss=0.02965 | best_loss=0.02483
Epoch 48/80: current_loss=0.02719 | best_loss=0.02483
Epoch 49/80: current_loss=0.02677 | best_loss=0.02483
Epoch 50/80: current_loss=0.02521 | best_loss=0.02483
Epoch 51/80: current_loss=0.02657 | best_loss=0.02483
Epoch 52/80: current_loss=0.02809 | best_loss=0.02483
Epoch 53/80: current_loss=0.02917 | best_loss=0.02483
Epoch 54/80: current_loss=0.02905 | best_loss=0.02483
Epoch 55/80: current_loss=0.02558 | best_loss=0.02483
Epoch 56/80: current_loss=0.02627 | best_loss=0.02483
Epoch 57/80: current_loss=0.02875 | best_loss=0.02483
Early Stopping at epoch 57
      explained_var=0.05970 | mse_loss=0.02428
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03163 | best_loss=0.03163
Epoch 1/80: current_loss=0.03103 | best_loss=0.03103
Epoch 2/80: current_loss=0.02889 | best_loss=0.02889
Epoch 3/80: current_loss=0.02783 | best_loss=0.02783
Epoch 4/80: current_loss=0.02852 | best_loss=0.02783
Epoch 5/80: current_loss=0.03166 | best_loss=0.02783
Epoch 6/80: current_loss=0.03319 | best_loss=0.02783
Epoch 7/80: current_loss=0.02927 | best_loss=0.02783
Epoch 8/80: current_loss=0.03136 | best_loss=0.02783
Epoch 9/80: current_loss=0.02911 | best_loss=0.02783
Epoch 10/80: current_loss=0.03012 | best_loss=0.02783
Epoch 11/80: current_loss=0.02895 | best_loss=0.02783
Epoch 12/80: current_loss=0.02808 | best_loss=0.02783
Epoch 13/80: current_loss=0.02803 | best_loss=0.02783
Epoch 14/80: current_loss=0.03332 | best_loss=0.02783
Epoch 15/80: current_loss=0.02794 | best_loss=0.02783
Epoch 16/80: current_loss=0.02778 | best_loss=0.02778
Epoch 17/80: current_loss=0.02761 | best_loss=0.02761
Epoch 18/80: current_loss=0.02844 | best_loss=0.02761
Epoch 19/80: current_loss=0.02809 | best_loss=0.02761
Epoch 20/80: current_loss=0.02785 | best_loss=0.02761
Epoch 21/80: current_loss=0.02796 | best_loss=0.02761
Epoch 22/80: current_loss=0.02781 | best_loss=0.02761
Epoch 23/80: current_loss=0.02810 | best_loss=0.02761
Epoch 24/80: current_loss=0.02838 | best_loss=0.02761
Epoch 25/80: current_loss=0.02815 | best_loss=0.02761
Epoch 26/80: current_loss=0.02998 | best_loss=0.02761
Epoch 27/80: current_loss=0.02881 | best_loss=0.02761
Epoch 28/80: current_loss=0.02906 | best_loss=0.02761
Epoch 29/80: current_loss=0.03186 | best_loss=0.02761
Epoch 30/80: current_loss=0.02855 | best_loss=0.02761
Epoch 31/80: current_loss=0.02830 | best_loss=0.02761
Epoch 32/80: current_loss=0.02981 | best_loss=0.02761
Epoch 33/80: current_loss=0.02926 | best_loss=0.02761
Epoch 34/80: current_loss=0.02997 | best_loss=0.02761
Epoch 35/80: current_loss=0.02882 | best_loss=0.02761
Epoch 36/80: current_loss=0.02811 | best_loss=0.02761
Epoch 37/80: current_loss=0.02831 | best_loss=0.02761
Early Stopping at epoch 37
      explained_var=0.02661 | mse_loss=0.02714
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03472 | best_loss=0.03472
Epoch 1/80: current_loss=0.03569 | best_loss=0.03472
Epoch 2/80: current_loss=0.03237 | best_loss=0.03237
Epoch 3/80: current_loss=0.02974 | best_loss=0.02974
Epoch 4/80: current_loss=0.03152 | best_loss=0.02974
Epoch 5/80: current_loss=0.02959 | best_loss=0.02959
Epoch 6/80: current_loss=0.03313 | best_loss=0.02959
Epoch 7/80: current_loss=0.03000 | best_loss=0.02959
Epoch 8/80: current_loss=0.02967 | best_loss=0.02959
Epoch 9/80: current_loss=0.02935 | best_loss=0.02935
Epoch 10/80: current_loss=0.03030 | best_loss=0.02935
Epoch 11/80: current_loss=0.02975 | best_loss=0.02935
Epoch 12/80: current_loss=0.02932 | best_loss=0.02932
Epoch 13/80: current_loss=0.03006 | best_loss=0.02932
Epoch 14/80: current_loss=0.02938 | best_loss=0.02932
Epoch 15/80: current_loss=0.03041 | best_loss=0.02932
Epoch 16/80: current_loss=0.03059 | best_loss=0.02932
Epoch 17/80: current_loss=0.02938 | best_loss=0.02932
Epoch 18/80: current_loss=0.03190 | best_loss=0.02932
Epoch 19/80: current_loss=0.02930 | best_loss=0.02930
Epoch 20/80: current_loss=0.02924 | best_loss=0.02924
Epoch 21/80: current_loss=0.02985 | best_loss=0.02924
Epoch 22/80: current_loss=0.02961 | best_loss=0.02924
Epoch 23/80: current_loss=0.03065 | best_loss=0.02924
Epoch 24/80: current_loss=0.02934 | best_loss=0.02924
Epoch 25/80: current_loss=0.03376 | best_loss=0.02924
Epoch 26/80: current_loss=0.18498 | best_loss=0.02924
Epoch 27/80: current_loss=0.22995 | best_loss=0.02924
Epoch 28/80: current_loss=0.23358 | best_loss=0.02924
Epoch 29/80: current_loss=0.22495 | best_loss=0.02924
Epoch 30/80: current_loss=0.11345 | best_loss=0.02924
Epoch 31/80: current_loss=0.08861 | best_loss=0.02924
Epoch 32/80: current_loss=0.05398 | best_loss=0.02924
Epoch 33/80: current_loss=0.07149 | best_loss=0.02924
Epoch 34/80: current_loss=0.05797 | best_loss=0.02924
Epoch 35/80: current_loss=0.04251 | best_loss=0.02924
Epoch 36/80: current_loss=0.03698 | best_loss=0.02924
Epoch 37/80: current_loss=0.04493 | best_loss=0.02924
Epoch 38/80: current_loss=0.03411 | best_loss=0.02924
Epoch 39/80: current_loss=0.03023 | best_loss=0.02924
Epoch 40/80: current_loss=0.03457 | best_loss=0.02924
Early Stopping at epoch 40
      explained_var=0.03847 | mse_loss=0.02849
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03182 | best_loss=0.03182
Epoch 1/80: current_loss=0.02882 | best_loss=0.02882
Epoch 2/80: current_loss=0.02941 | best_loss=0.02882
Epoch 3/80: current_loss=0.02924 | best_loss=0.02882
Epoch 4/80: current_loss=0.02944 | best_loss=0.02882
Epoch 5/80: current_loss=0.03001 | best_loss=0.02882
Epoch 6/80: current_loss=0.03406 | best_loss=0.02882
Epoch 7/80: current_loss=0.02848 | best_loss=0.02848
Epoch 8/80: current_loss=0.02823 | best_loss=0.02823
Epoch 9/80: current_loss=0.02884 | best_loss=0.02823
Epoch 10/80: current_loss=0.02931 | best_loss=0.02823
Epoch 11/80: current_loss=0.02915 | best_loss=0.02823
Epoch 12/80: current_loss=0.03647 | best_loss=0.02823
Epoch 13/80: current_loss=0.03006 | best_loss=0.02823
Epoch 14/80: current_loss=0.03028 | best_loss=0.02823
Epoch 15/80: current_loss=0.02970 | best_loss=0.02823
Epoch 16/80: current_loss=0.02876 | best_loss=0.02823
Epoch 17/80: current_loss=0.02888 | best_loss=0.02823
Epoch 18/80: current_loss=0.02872 | best_loss=0.02823
Epoch 19/80: current_loss=0.02891 | best_loss=0.02823
Epoch 20/80: current_loss=0.02836 | best_loss=0.02823
Epoch 21/80: current_loss=0.02862 | best_loss=0.02823
Epoch 22/80: current_loss=0.02840 | best_loss=0.02823
Epoch 23/80: current_loss=0.02887 | best_loss=0.02823
Epoch 24/80: current_loss=0.02828 | best_loss=0.02823
Epoch 25/80: current_loss=0.02847 | best_loss=0.02823
Epoch 26/80: current_loss=0.02827 | best_loss=0.02823
Epoch 27/80: current_loss=0.02852 | best_loss=0.02823
Epoch 28/80: current_loss=0.02853 | best_loss=0.02823
Early Stopping at epoch 28
      explained_var=0.00111 | mse_loss=0.02846
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03308 | best_loss=0.03308
Epoch 1/80: current_loss=0.03393 | best_loss=0.03308
Epoch 2/80: current_loss=0.03344 | best_loss=0.03308
Epoch 3/80: current_loss=0.03441 | best_loss=0.03308
Epoch 4/80: current_loss=0.03596 | best_loss=0.03308
Epoch 5/80: current_loss=0.03419 | best_loss=0.03308
Epoch 6/80: current_loss=0.03442 | best_loss=0.03308
Epoch 7/80: current_loss=0.03407 | best_loss=0.03308
Epoch 8/80: current_loss=0.03379 | best_loss=0.03308
Epoch 9/80: current_loss=0.03411 | best_loss=0.03308
Epoch 10/80: current_loss=0.03444 | best_loss=0.03308
Epoch 11/80: current_loss=0.03378 | best_loss=0.03308
Epoch 12/80: current_loss=0.03371 | best_loss=0.03308
Epoch 13/80: current_loss=0.03418 | best_loss=0.03308
Epoch 14/80: current_loss=0.03389 | best_loss=0.03308
Epoch 15/80: current_loss=0.03453 | best_loss=0.03308
Epoch 16/80: current_loss=0.03400 | best_loss=0.03308
Epoch 17/80: current_loss=0.03429 | best_loss=0.03308
Epoch 18/80: current_loss=0.03399 | best_loss=0.03308
Epoch 19/80: current_loss=0.03464 | best_loss=0.03308
Epoch 20/80: current_loss=0.03325 | best_loss=0.03308
Early Stopping at epoch 20
      explained_var=-0.04124 | mse_loss=0.03389
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.01693| avg_loss=0.02845
----------------------------------------------


----------------------------------------------
Params for Trial 44
{'learning_rate': 0.001, 'weight_decay': 0.005873343529976683, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02896 | best_loss=0.02896
Epoch 1/80: current_loss=0.02701 | best_loss=0.02701
Epoch 2/80: current_loss=0.02720 | best_loss=0.02701
Epoch 3/80: current_loss=0.02547 | best_loss=0.02547
Epoch 4/80: current_loss=0.02578 | best_loss=0.02547
Epoch 5/80: current_loss=0.02799 | best_loss=0.02547
Epoch 6/80: current_loss=0.02643 | best_loss=0.02547
Epoch 7/80: current_loss=0.02607 | best_loss=0.02547
Epoch 8/80: current_loss=0.02636 | best_loss=0.02547
Epoch 9/80: current_loss=0.02596 | best_loss=0.02547
Epoch 10/80: current_loss=0.02522 | best_loss=0.02522
Epoch 11/80: current_loss=0.02608 | best_loss=0.02522
Epoch 12/80: current_loss=0.02588 | best_loss=0.02522
Epoch 13/80: current_loss=0.02607 | best_loss=0.02522
Epoch 14/80: current_loss=0.02963 | best_loss=0.02522
Epoch 15/80: current_loss=0.02838 | best_loss=0.02522
Epoch 16/80: current_loss=0.03048 | best_loss=0.02522
Epoch 17/80: current_loss=0.02708 | best_loss=0.02522
Epoch 18/80: current_loss=0.02626 | best_loss=0.02522
Epoch 19/80: current_loss=0.02624 | best_loss=0.02522
Epoch 20/80: current_loss=0.02789 | best_loss=0.02522
Epoch 21/80: current_loss=0.02610 | best_loss=0.02522
Epoch 22/80: current_loss=0.02682 | best_loss=0.02522
Epoch 23/80: current_loss=0.02624 | best_loss=0.02522
Epoch 24/80: current_loss=0.02782 | best_loss=0.02522
Epoch 25/80: current_loss=0.02815 | best_loss=0.02522
Epoch 26/80: current_loss=0.02846 | best_loss=0.02522
Epoch 27/80: current_loss=0.03390 | best_loss=0.02522
Epoch 28/80: current_loss=0.02765 | best_loss=0.02522
Epoch 29/80: current_loss=0.02677 | best_loss=0.02522
Epoch 30/80: current_loss=0.02677 | best_loss=0.02522
Early Stopping at epoch 30
      explained_var=0.04851 | mse_loss=0.02475
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03930 | best_loss=0.03930
Epoch 1/80: current_loss=0.03096 | best_loss=0.03096
Epoch 2/80: current_loss=0.02888 | best_loss=0.02888
Epoch 3/80: current_loss=0.02804 | best_loss=0.02804
Epoch 4/80: current_loss=0.02834 | best_loss=0.02804
Epoch 5/80: current_loss=0.02815 | best_loss=0.02804
Epoch 6/80: current_loss=0.02811 | best_loss=0.02804
Epoch 7/80: current_loss=0.02885 | best_loss=0.02804
Epoch 8/80: current_loss=0.03016 | best_loss=0.02804
Epoch 9/80: current_loss=0.02880 | best_loss=0.02804
Epoch 10/80: current_loss=0.02932 | best_loss=0.02804
Epoch 11/80: current_loss=0.02818 | best_loss=0.02804
Epoch 12/80: current_loss=0.02823 | best_loss=0.02804
Epoch 13/80: current_loss=0.02966 | best_loss=0.02804
Epoch 14/80: current_loss=0.02827 | best_loss=0.02804
Epoch 15/80: current_loss=0.02941 | best_loss=0.02804
Epoch 16/80: current_loss=0.02900 | best_loss=0.02804
Epoch 17/80: current_loss=0.02826 | best_loss=0.02804
Epoch 18/80: current_loss=0.03011 | best_loss=0.02804
Epoch 19/80: current_loss=0.02864 | best_loss=0.02804
Epoch 20/80: current_loss=0.02854 | best_loss=0.02804
Epoch 21/80: current_loss=0.02805 | best_loss=0.02804
Epoch 22/80: current_loss=0.02810 | best_loss=0.02804
Epoch 23/80: current_loss=0.02838 | best_loss=0.02804
Early Stopping at epoch 23
      explained_var=0.01056 | mse_loss=0.02752
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03194 | best_loss=0.03194
Epoch 1/80: current_loss=0.03028 | best_loss=0.03028
Epoch 2/80: current_loss=0.03010 | best_loss=0.03010
Epoch 3/80: current_loss=0.03020 | best_loss=0.03010
Epoch 4/80: current_loss=0.03040 | best_loss=0.03010
Epoch 5/80: current_loss=0.05953 | best_loss=0.03010
Epoch 6/80: current_loss=0.03366 | best_loss=0.03010
Epoch 7/80: current_loss=0.03059 | best_loss=0.03010
Epoch 8/80: current_loss=0.03165 | best_loss=0.03010
Epoch 9/80: current_loss=0.03091 | best_loss=0.03010
Epoch 10/80: current_loss=0.03498 | best_loss=0.03010
Epoch 11/80: current_loss=0.03186 | best_loss=0.03010
Epoch 12/80: current_loss=0.03101 | best_loss=0.03010
Epoch 13/80: current_loss=0.03025 | best_loss=0.03010
Epoch 14/80: current_loss=0.03050 | best_loss=0.03010
Epoch 15/80: current_loss=0.03037 | best_loss=0.03010
Epoch 16/80: current_loss=0.03039 | best_loss=0.03010
Epoch 17/80: current_loss=0.03052 | best_loss=0.03010
Epoch 18/80: current_loss=0.03068 | best_loss=0.03010
Epoch 19/80: current_loss=0.03078 | best_loss=0.03010
Epoch 20/80: current_loss=0.03165 | best_loss=0.03010
Epoch 21/80: current_loss=0.03164 | best_loss=0.03010
Epoch 22/80: current_loss=0.03117 | best_loss=0.03010
Early Stopping at epoch 22
      explained_var=0.00818 | mse_loss=0.02934
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02847 | best_loss=0.02847
Epoch 1/80: current_loss=0.02887 | best_loss=0.02847
Epoch 2/80: current_loss=0.03081 | best_loss=0.02847
Epoch 3/80: current_loss=0.03254 | best_loss=0.02847
Epoch 4/80: current_loss=0.03071 | best_loss=0.02847
Epoch 5/80: current_loss=0.02803 | best_loss=0.02803
Epoch 6/80: current_loss=0.02914 | best_loss=0.02803
Epoch 7/80: current_loss=0.02867 | best_loss=0.02803
Epoch 8/80: current_loss=0.02824 | best_loss=0.02803
Epoch 9/80: current_loss=0.02825 | best_loss=0.02803
Epoch 10/80: current_loss=0.02842 | best_loss=0.02803
Epoch 11/80: current_loss=0.02883 | best_loss=0.02803
Epoch 12/80: current_loss=0.02836 | best_loss=0.02803
Epoch 13/80: current_loss=0.02807 | best_loss=0.02803
Epoch 14/80: current_loss=0.02956 | best_loss=0.02803
Epoch 15/80: current_loss=0.03005 | best_loss=0.02803
Epoch 16/80: current_loss=0.02943 | best_loss=0.02803
Epoch 17/80: current_loss=0.02875 | best_loss=0.02803
Epoch 18/80: current_loss=0.02884 | best_loss=0.02803
Epoch 19/80: current_loss=0.02810 | best_loss=0.02803
Epoch 20/80: current_loss=0.02835 | best_loss=0.02803
Epoch 21/80: current_loss=0.02835 | best_loss=0.02803
Epoch 22/80: current_loss=0.02850 | best_loss=0.02803
Epoch 23/80: current_loss=0.02846 | best_loss=0.02803
Epoch 24/80: current_loss=0.02841 | best_loss=0.02803
Epoch 25/80: current_loss=0.02835 | best_loss=0.02803
Early Stopping at epoch 25
      explained_var=0.00065 | mse_loss=0.02838
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03232 | best_loss=0.03232
Epoch 1/80: current_loss=0.03258 | best_loss=0.03232
Epoch 2/80: current_loss=0.03237 | best_loss=0.03232
Epoch 3/80: current_loss=0.03236 | best_loss=0.03232
Epoch 4/80: current_loss=0.03327 | best_loss=0.03232
Epoch 5/80: current_loss=0.03256 | best_loss=0.03232
Epoch 6/80: current_loss=0.03264 | best_loss=0.03232
Epoch 7/80: current_loss=0.03253 | best_loss=0.03232
Epoch 8/80: current_loss=0.03277 | best_loss=0.03232
Epoch 9/80: current_loss=0.03317 | best_loss=0.03232
Epoch 10/80: current_loss=0.03502 | best_loss=0.03232
Epoch 11/80: current_loss=0.03259 | best_loss=0.03232
Epoch 12/80: current_loss=0.03279 | best_loss=0.03232
Epoch 13/80: current_loss=0.03277 | best_loss=0.03232
Epoch 14/80: current_loss=0.03379 | best_loss=0.03232
Epoch 15/80: current_loss=0.03373 | best_loss=0.03232
Epoch 16/80: current_loss=0.03347 | best_loss=0.03232
Epoch 17/80: current_loss=0.03252 | best_loss=0.03232
Epoch 18/80: current_loss=0.03307 | best_loss=0.03232
Epoch 19/80: current_loss=0.03268 | best_loss=0.03232
Epoch 20/80: current_loss=0.03299 | best_loss=0.03232
Early Stopping at epoch 20
      explained_var=-0.01482 | mse_loss=0.03312
----------------------------------------------
Average early_stopping_point: 4| avg_exp_var=0.01061| avg_loss=0.02862
----------------------------------------------


----------------------------------------------
Params for Trial 45
{'learning_rate': 0.001, 'weight_decay': 0.0006729936129443094, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03086 | best_loss=0.03086
Epoch 1/80: current_loss=0.03021 | best_loss=0.03021
Epoch 2/80: current_loss=0.02659 | best_loss=0.02659
Epoch 3/80: current_loss=0.03045 | best_loss=0.02659
Epoch 4/80: current_loss=0.04030 | best_loss=0.02659
Epoch 5/80: current_loss=0.03837 | best_loss=0.02659
Epoch 6/80: current_loss=0.02763 | best_loss=0.02659
Epoch 7/80: current_loss=0.02634 | best_loss=0.02634
Epoch 8/80: current_loss=0.03175 | best_loss=0.02634
Epoch 9/80: current_loss=0.02531 | best_loss=0.02531
Epoch 10/80: current_loss=0.04223 | best_loss=0.02531
Epoch 11/80: current_loss=0.03490 | best_loss=0.02531
Epoch 12/80: current_loss=0.03611 | best_loss=0.02531
Epoch 13/80: current_loss=0.02544 | best_loss=0.02531
Epoch 14/80: current_loss=0.03147 | best_loss=0.02531
Epoch 15/80: current_loss=0.02702 | best_loss=0.02531
Epoch 16/80: current_loss=0.02524 | best_loss=0.02524
Epoch 17/80: current_loss=0.03687 | best_loss=0.02524
Epoch 18/80: current_loss=0.03157 | best_loss=0.02524
Epoch 19/80: current_loss=0.02945 | best_loss=0.02524
Epoch 20/80: current_loss=0.03389 | best_loss=0.02524
Epoch 21/80: current_loss=0.05047 | best_loss=0.02524
Epoch 22/80: current_loss=0.02834 | best_loss=0.02524
Epoch 23/80: current_loss=0.04116 | best_loss=0.02524
Epoch 24/80: current_loss=0.03542 | best_loss=0.02524
Epoch 25/80: current_loss=0.03127 | best_loss=0.02524
Epoch 26/80: current_loss=0.02988 | best_loss=0.02524
Epoch 27/80: current_loss=0.03028 | best_loss=0.02524
Epoch 28/80: current_loss=0.02619 | best_loss=0.02524
Epoch 29/80: current_loss=0.02877 | best_loss=0.02524
Epoch 30/80: current_loss=0.02549 | best_loss=0.02524
Epoch 31/80: current_loss=0.02571 | best_loss=0.02524
Epoch 32/80: current_loss=0.02617 | best_loss=0.02524
Epoch 33/80: current_loss=0.02564 | best_loss=0.02524
Epoch 34/80: current_loss=0.02581 | best_loss=0.02524
Epoch 35/80: current_loss=0.02671 | best_loss=0.02524
Epoch 36/80: current_loss=0.02858 | best_loss=0.02524
Early Stopping at epoch 36
      explained_var=0.04373 | mse_loss=0.02470
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02953 | best_loss=0.02953
Epoch 1/80: current_loss=0.02876 | best_loss=0.02876
Epoch 2/80: current_loss=0.03178 | best_loss=0.02876
Epoch 3/80: current_loss=0.03316 | best_loss=0.02876
Epoch 4/80: current_loss=0.03058 | best_loss=0.02876
Epoch 5/80: current_loss=0.02878 | best_loss=0.02876
Epoch 6/80: current_loss=0.03112 | best_loss=0.02876
Epoch 7/80: current_loss=0.03228 | best_loss=0.02876
Epoch 8/80: current_loss=0.02785 | best_loss=0.02785
Epoch 9/80: current_loss=0.02780 | best_loss=0.02780
Epoch 10/80: current_loss=0.03198 | best_loss=0.02780
Epoch 11/80: current_loss=0.03080 | best_loss=0.02780
Epoch 12/80: current_loss=0.02906 | best_loss=0.02780
Epoch 13/80: current_loss=0.02812 | best_loss=0.02780
Epoch 14/80: current_loss=0.02917 | best_loss=0.02780
Epoch 15/80: current_loss=0.02796 | best_loss=0.02780
Epoch 16/80: current_loss=0.03008 | best_loss=0.02780
Epoch 17/80: current_loss=0.02844 | best_loss=0.02780
Epoch 18/80: current_loss=0.02940 | best_loss=0.02780
Epoch 19/80: current_loss=0.03098 | best_loss=0.02780
Epoch 20/80: current_loss=0.03364 | best_loss=0.02780
Epoch 21/80: current_loss=0.02865 | best_loss=0.02780
Epoch 22/80: current_loss=0.02790 | best_loss=0.02780
Epoch 23/80: current_loss=0.02876 | best_loss=0.02780
Epoch 24/80: current_loss=0.02863 | best_loss=0.02780
Epoch 25/80: current_loss=0.02962 | best_loss=0.02780
Epoch 26/80: current_loss=0.03055 | best_loss=0.02780
Epoch 27/80: current_loss=0.02784 | best_loss=0.02780
Epoch 28/80: current_loss=0.02798 | best_loss=0.02780
Epoch 29/80: current_loss=0.02905 | best_loss=0.02780
Early Stopping at epoch 29
      explained_var=0.01923 | mse_loss=0.02729
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03605 | best_loss=0.03605
Epoch 1/80: current_loss=0.03321 | best_loss=0.03321
Epoch 2/80: current_loss=0.03244 | best_loss=0.03244
Epoch 3/80: current_loss=0.03575 | best_loss=0.03244
Epoch 4/80: current_loss=0.03138 | best_loss=0.03138
Epoch 5/80: current_loss=0.03021 | best_loss=0.03021
Epoch 6/80: current_loss=0.03659 | best_loss=0.03021
Epoch 7/80: current_loss=0.02969 | best_loss=0.02969
Epoch 8/80: current_loss=0.03101 | best_loss=0.02969
Epoch 9/80: current_loss=0.03069 | best_loss=0.02969
Epoch 10/80: current_loss=0.03003 | best_loss=0.02969
Epoch 11/80: current_loss=0.03651 | best_loss=0.02969
Epoch 12/80: current_loss=0.03333 | best_loss=0.02969
Epoch 13/80: current_loss=0.03075 | best_loss=0.02969
Epoch 14/80: current_loss=0.03411 | best_loss=0.02969
Epoch 15/80: current_loss=0.04550 | best_loss=0.02969
Epoch 16/80: current_loss=0.42745 | best_loss=0.02969
Epoch 17/80: current_loss=0.14005 | best_loss=0.02969
Epoch 18/80: current_loss=0.05449 | best_loss=0.02969
Epoch 19/80: current_loss=0.03436 | best_loss=0.02969
Epoch 20/80: current_loss=0.05065 | best_loss=0.02969
Epoch 21/80: current_loss=0.03431 | best_loss=0.02969
Epoch 22/80: current_loss=0.03403 | best_loss=0.02969
Epoch 23/80: current_loss=0.03545 | best_loss=0.02969
Epoch 24/80: current_loss=0.03375 | best_loss=0.02969
Epoch 25/80: current_loss=0.03689 | best_loss=0.02969
Epoch 26/80: current_loss=0.03413 | best_loss=0.02969
Epoch 27/80: current_loss=0.03401 | best_loss=0.02969
Early Stopping at epoch 27
      explained_var=0.02406 | mse_loss=0.02897
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03439 | best_loss=0.03439
Epoch 1/80: current_loss=0.04184 | best_loss=0.03439
Epoch 2/80: current_loss=0.04615 | best_loss=0.03439
Epoch 3/80: current_loss=0.03051 | best_loss=0.03051
Epoch 4/80: current_loss=0.02925 | best_loss=0.02925
Epoch 5/80: current_loss=0.03472 | best_loss=0.02925
Epoch 6/80: current_loss=0.03465 | best_loss=0.02925
Epoch 7/80: current_loss=0.03025 | best_loss=0.02925
Epoch 8/80: current_loss=0.03183 | best_loss=0.02925
Epoch 9/80: current_loss=0.03044 | best_loss=0.02925
Epoch 10/80: current_loss=0.03091 | best_loss=0.02925
Epoch 11/80: current_loss=0.02854 | best_loss=0.02854
Epoch 12/80: current_loss=0.02838 | best_loss=0.02838
Epoch 13/80: current_loss=0.03128 | best_loss=0.02838
Epoch 14/80: current_loss=0.03486 | best_loss=0.02838
Epoch 15/80: current_loss=0.03110 | best_loss=0.02838
Epoch 16/80: current_loss=0.03290 | best_loss=0.02838
Epoch 17/80: current_loss=0.02928 | best_loss=0.02838
Epoch 18/80: current_loss=0.02912 | best_loss=0.02838
Epoch 19/80: current_loss=0.02896 | best_loss=0.02838
Epoch 20/80: current_loss=0.02919 | best_loss=0.02838
Epoch 21/80: current_loss=0.03222 | best_loss=0.02838
Epoch 22/80: current_loss=0.03170 | best_loss=0.02838
Epoch 23/80: current_loss=0.02973 | best_loss=0.02838
Epoch 24/80: current_loss=0.02905 | best_loss=0.02838
Epoch 25/80: current_loss=0.02917 | best_loss=0.02838
Epoch 26/80: current_loss=0.02906 | best_loss=0.02838
Epoch 27/80: current_loss=0.03064 | best_loss=0.02838
Epoch 28/80: current_loss=0.03558 | best_loss=0.02838
Epoch 29/80: current_loss=0.03167 | best_loss=0.02838
Epoch 30/80: current_loss=0.02881 | best_loss=0.02838
Epoch 31/80: current_loss=0.03099 | best_loss=0.02838
Epoch 32/80: current_loss=0.03503 | best_loss=0.02838
Early Stopping at epoch 32
      explained_var=-0.01074 | mse_loss=0.02868
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03476 | best_loss=0.03476
Epoch 1/80: current_loss=0.03312 | best_loss=0.03312
Epoch 2/80: current_loss=0.03389 | best_loss=0.03312
Epoch 3/80: current_loss=0.03325 | best_loss=0.03312
Epoch 4/80: current_loss=0.03369 | best_loss=0.03312
Epoch 5/80: current_loss=0.03365 | best_loss=0.03312
Epoch 6/80: current_loss=0.03377 | best_loss=0.03312
Epoch 7/80: current_loss=0.03383 | best_loss=0.03312
Epoch 8/80: current_loss=0.03384 | best_loss=0.03312
Epoch 9/80: current_loss=0.03439 | best_loss=0.03312
Epoch 10/80: current_loss=0.03454 | best_loss=0.03312
Epoch 11/80: current_loss=0.03446 | best_loss=0.03312
Epoch 12/80: current_loss=0.03615 | best_loss=0.03312
Epoch 13/80: current_loss=0.03379 | best_loss=0.03312
Epoch 14/80: current_loss=0.03356 | best_loss=0.03312
Epoch 15/80: current_loss=0.03369 | best_loss=0.03312
Epoch 16/80: current_loss=0.03400 | best_loss=0.03312
Epoch 17/80: current_loss=0.03545 | best_loss=0.03312
Epoch 18/80: current_loss=0.03425 | best_loss=0.03312
Epoch 19/80: current_loss=0.03351 | best_loss=0.03312
Epoch 20/80: current_loss=0.03493 | best_loss=0.03312
Epoch 21/80: current_loss=0.03514 | best_loss=0.03312
Early Stopping at epoch 21
      explained_var=-0.03011 | mse_loss=0.03375
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.00923| avg_loss=0.02868
----------------------------------------------


----------------------------------------------
Params for Trial 46
{'learning_rate': 0.001, 'weight_decay': 0.003120338740701159, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02982 | best_loss=0.02982
Epoch 1/80: current_loss=0.02863 | best_loss=0.02863
Epoch 2/80: current_loss=0.02524 | best_loss=0.02524
Epoch 3/80: current_loss=0.02716 | best_loss=0.02524
Epoch 4/80: current_loss=0.02859 | best_loss=0.02524
Epoch 5/80: current_loss=0.02656 | best_loss=0.02524
Epoch 6/80: current_loss=0.02572 | best_loss=0.02524
Epoch 7/80: current_loss=0.02497 | best_loss=0.02497
Epoch 8/80: current_loss=0.02646 | best_loss=0.02497
Epoch 9/80: current_loss=0.02649 | best_loss=0.02497
Epoch 10/80: current_loss=0.02499 | best_loss=0.02497
Epoch 11/80: current_loss=0.02759 | best_loss=0.02497
Epoch 12/80: current_loss=0.02686 | best_loss=0.02497
Epoch 13/80: current_loss=0.02754 | best_loss=0.02497
Epoch 14/80: current_loss=0.02696 | best_loss=0.02497
Epoch 15/80: current_loss=0.02551 | best_loss=0.02497
Epoch 16/80: current_loss=0.02575 | best_loss=0.02497
Epoch 17/80: current_loss=0.03128 | best_loss=0.02497
Epoch 18/80: current_loss=0.02861 | best_loss=0.02497
Epoch 19/80: current_loss=0.02769 | best_loss=0.02497
Epoch 20/80: current_loss=0.02625 | best_loss=0.02497
Epoch 21/80: current_loss=0.02887 | best_loss=0.02497
Epoch 22/80: current_loss=0.02659 | best_loss=0.02497
Epoch 23/80: current_loss=0.02984 | best_loss=0.02497
Epoch 24/80: current_loss=0.03233 | best_loss=0.02497
Epoch 25/80: current_loss=0.03368 | best_loss=0.02497
Epoch 26/80: current_loss=0.02920 | best_loss=0.02497
Epoch 27/80: current_loss=0.02538 | best_loss=0.02497
Early Stopping at epoch 27
      explained_var=0.05487 | mse_loss=0.02443
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02799 | best_loss=0.02799
Epoch 1/80: current_loss=0.02929 | best_loss=0.02799
Epoch 2/80: current_loss=0.02791 | best_loss=0.02791
Epoch 3/80: current_loss=0.03116 | best_loss=0.02791
Epoch 4/80: current_loss=0.02811 | best_loss=0.02791
Epoch 5/80: current_loss=0.02809 | best_loss=0.02791
Epoch 6/80: current_loss=0.02865 | best_loss=0.02791
Epoch 7/80: current_loss=0.02892 | best_loss=0.02791
Epoch 8/80: current_loss=0.02930 | best_loss=0.02791
Epoch 9/80: current_loss=0.02800 | best_loss=0.02791
Epoch 10/80: current_loss=0.02855 | best_loss=0.02791
Epoch 11/80: current_loss=0.02820 | best_loss=0.02791
Epoch 12/80: current_loss=0.03136 | best_loss=0.02791
Epoch 13/80: current_loss=0.02901 | best_loss=0.02791
Epoch 14/80: current_loss=0.02939 | best_loss=0.02791
Epoch 15/80: current_loss=0.03110 | best_loss=0.02791
Epoch 16/80: current_loss=0.02802 | best_loss=0.02791
Epoch 17/80: current_loss=0.02912 | best_loss=0.02791
Epoch 18/80: current_loss=0.02878 | best_loss=0.02791
Epoch 19/80: current_loss=0.02800 | best_loss=0.02791
Epoch 20/80: current_loss=0.02840 | best_loss=0.02791
Epoch 21/80: current_loss=0.02832 | best_loss=0.02791
Epoch 22/80: current_loss=0.02809 | best_loss=0.02791
Early Stopping at epoch 22
      explained_var=0.01472 | mse_loss=0.02744
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03327 | best_loss=0.03327
Epoch 1/80: current_loss=0.03236 | best_loss=0.03236
Epoch 2/80: current_loss=0.03463 | best_loss=0.03236
Epoch 3/80: current_loss=0.02991 | best_loss=0.02991
Epoch 4/80: current_loss=0.03149 | best_loss=0.02991
Epoch 5/80: current_loss=0.02987 | best_loss=0.02987
Epoch 6/80: current_loss=0.02963 | best_loss=0.02963
Epoch 7/80: current_loss=0.03014 | best_loss=0.02963
Epoch 8/80: current_loss=0.03080 | best_loss=0.02963
Epoch 9/80: current_loss=0.03075 | best_loss=0.02963
Epoch 10/80: current_loss=0.03008 | best_loss=0.02963
Epoch 11/80: current_loss=0.03132 | best_loss=0.02963
Epoch 12/80: current_loss=0.03320 | best_loss=0.02963
Epoch 13/80: current_loss=0.03096 | best_loss=0.02963
Epoch 14/80: current_loss=0.03016 | best_loss=0.02963
Epoch 15/80: current_loss=0.03109 | best_loss=0.02963
Epoch 16/80: current_loss=0.02991 | best_loss=0.02963
Epoch 17/80: current_loss=0.03368 | best_loss=0.02963
Epoch 18/80: current_loss=0.03021 | best_loss=0.02963
Epoch 19/80: current_loss=0.03002 | best_loss=0.02963
Epoch 20/80: current_loss=0.03170 | best_loss=0.02963
Epoch 21/80: current_loss=0.03062 | best_loss=0.02963
Epoch 22/80: current_loss=0.03473 | best_loss=0.02963
Epoch 23/80: current_loss=0.03518 | best_loss=0.02963
Epoch 24/80: current_loss=0.03299 | best_loss=0.02963
Epoch 25/80: current_loss=0.02977 | best_loss=0.02963
Epoch 26/80: current_loss=0.03002 | best_loss=0.02963
Early Stopping at epoch 26
      explained_var=0.02516 | mse_loss=0.02886
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03615 | best_loss=0.03615
Epoch 1/80: current_loss=0.02833 | best_loss=0.02833
Epoch 2/80: current_loss=0.02891 | best_loss=0.02833
Epoch 3/80: current_loss=0.02953 | best_loss=0.02833
Epoch 4/80: current_loss=0.02810 | best_loss=0.02810
Epoch 5/80: current_loss=0.02924 | best_loss=0.02810
Epoch 6/80: current_loss=0.02821 | best_loss=0.02810
Epoch 7/80: current_loss=0.02818 | best_loss=0.02810
Epoch 8/80: current_loss=0.02836 | best_loss=0.02810
Epoch 9/80: current_loss=0.02839 | best_loss=0.02810
Epoch 10/80: current_loss=0.02848 | best_loss=0.02810
Epoch 11/80: current_loss=0.02861 | best_loss=0.02810
Epoch 12/80: current_loss=0.02867 | best_loss=0.02810
Epoch 13/80: current_loss=0.02901 | best_loss=0.02810
Epoch 14/80: current_loss=0.02828 | best_loss=0.02810
Epoch 15/80: current_loss=0.02845 | best_loss=0.02810
Epoch 16/80: current_loss=0.02889 | best_loss=0.02810
Epoch 17/80: current_loss=0.02813 | best_loss=0.02810
Epoch 18/80: current_loss=0.02839 | best_loss=0.02810
Epoch 19/80: current_loss=0.02919 | best_loss=0.02810
Epoch 20/80: current_loss=0.02920 | best_loss=0.02810
Epoch 21/80: current_loss=0.02812 | best_loss=0.02810
Epoch 22/80: current_loss=0.02815 | best_loss=0.02810
Epoch 23/80: current_loss=0.02825 | best_loss=0.02810
Epoch 24/80: current_loss=0.02985 | best_loss=0.02810
Early Stopping at epoch 24
      explained_var=-0.00412 | mse_loss=0.02848
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03311 | best_loss=0.03311
Epoch 1/80: current_loss=0.03304 | best_loss=0.03304
Epoch 2/80: current_loss=0.03304 | best_loss=0.03304
Epoch 3/80: current_loss=0.03388 | best_loss=0.03304
Epoch 4/80: current_loss=0.03370 | best_loss=0.03304
Epoch 5/80: current_loss=0.03304 | best_loss=0.03304
Epoch 6/80: current_loss=0.04580 | best_loss=0.03304
Epoch 7/80: current_loss=0.03919 | best_loss=0.03304
Epoch 8/80: current_loss=0.03316 | best_loss=0.03304
Epoch 9/80: current_loss=0.03607 | best_loss=0.03304
Epoch 10/80: current_loss=0.03384 | best_loss=0.03304
Epoch 11/80: current_loss=0.03337 | best_loss=0.03304
Epoch 12/80: current_loss=0.03380 | best_loss=0.03304
Epoch 13/80: current_loss=0.03447 | best_loss=0.03304
Epoch 14/80: current_loss=0.03438 | best_loss=0.03304
Epoch 15/80: current_loss=0.03367 | best_loss=0.03304
Epoch 16/80: current_loss=0.03366 | best_loss=0.03304
Epoch 17/80: current_loss=0.03350 | best_loss=0.03304
Epoch 18/80: current_loss=0.03392 | best_loss=0.03304
Epoch 19/80: current_loss=0.03326 | best_loss=0.03304
Epoch 20/80: current_loss=0.03373 | best_loss=0.03304
Epoch 21/80: current_loss=0.03264 | best_loss=0.03264
Epoch 22/80: current_loss=0.03358 | best_loss=0.03264
Epoch 23/80: current_loss=0.03348 | best_loss=0.03264
Epoch 24/80: current_loss=0.03464 | best_loss=0.03264
Epoch 25/80: current_loss=0.03312 | best_loss=0.03264
Epoch 26/80: current_loss=0.03292 | best_loss=0.03264
Epoch 27/80: current_loss=0.03302 | best_loss=0.03264
Epoch 28/80: current_loss=0.03440 | best_loss=0.03264
Epoch 29/80: current_loss=0.03385 | best_loss=0.03264
Epoch 30/80: current_loss=0.03448 | best_loss=0.03264
Epoch 31/80: current_loss=0.03344 | best_loss=0.03264
Epoch 32/80: current_loss=0.03357 | best_loss=0.03264
Epoch 33/80: current_loss=0.03486 | best_loss=0.03264
Epoch 34/80: current_loss=0.03358 | best_loss=0.03264
Epoch 35/80: current_loss=0.03291 | best_loss=0.03264
Epoch 36/80: current_loss=0.03329 | best_loss=0.03264
Epoch 37/80: current_loss=0.03624 | best_loss=0.03264
Epoch 38/80: current_loss=0.03337 | best_loss=0.03264
Epoch 39/80: current_loss=0.03657 | best_loss=0.03264
Epoch 40/80: current_loss=0.03473 | best_loss=0.03264
Epoch 41/80: current_loss=0.03343 | best_loss=0.03264
Early Stopping at epoch 41
      explained_var=-0.02783 | mse_loss=0.03342
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=0.01256| avg_loss=0.02853
----------------------------------------------


----------------------------------------------
Params for Trial 47
{'learning_rate': 0.1, 'weight_decay': 0.0017676754278411777, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.18546 | best_loss=0.18546
Epoch 1/80: current_loss=0.37664 | best_loss=0.18546
Epoch 2/80: current_loss=0.22341 | best_loss=0.18546
Epoch 3/80: current_loss=0.05189 | best_loss=0.05189
Epoch 4/80: current_loss=0.05255 | best_loss=0.05189
Epoch 5/80: current_loss=0.03014 | best_loss=0.03014
Epoch 6/80: current_loss=0.02863 | best_loss=0.02863
Epoch 7/80: current_loss=0.05831 | best_loss=0.02863
Epoch 8/80: current_loss=0.03274 | best_loss=0.02863
Epoch 9/80: current_loss=0.06061 | best_loss=0.02863
Epoch 10/80: current_loss=0.03209 | best_loss=0.02863
Epoch 11/80: current_loss=0.09463 | best_loss=0.02863
Epoch 12/80: current_loss=0.06381 | best_loss=0.02863
Epoch 13/80: current_loss=0.04506 | best_loss=0.02863
Epoch 14/80: current_loss=0.05784 | best_loss=0.02863
Epoch 15/80: current_loss=0.13330 | best_loss=0.02863
Epoch 16/80: current_loss=0.03231 | best_loss=0.02863
Epoch 17/80: current_loss=0.02988 | best_loss=0.02863
Epoch 18/80: current_loss=0.07002 | best_loss=0.02863
Epoch 19/80: current_loss=0.02707 | best_loss=0.02707
Epoch 20/80: current_loss=0.13736 | best_loss=0.02707
Epoch 21/80: current_loss=0.06898 | best_loss=0.02707
Epoch 22/80: current_loss=0.08362 | best_loss=0.02707
Epoch 23/80: current_loss=0.09571 | best_loss=0.02707
Epoch 24/80: current_loss=0.13602 | best_loss=0.02707
Epoch 25/80: current_loss=0.19979 | best_loss=0.02707
Epoch 26/80: current_loss=0.06250 | best_loss=0.02707
Epoch 27/80: current_loss=0.52112 | best_loss=0.02707
Epoch 28/80: current_loss=0.70506 | best_loss=0.02707
Epoch 29/80: current_loss=1.69247 | best_loss=0.02707
Epoch 30/80: current_loss=0.54332 | best_loss=0.02707
Epoch 31/80: current_loss=0.08575 | best_loss=0.02707
Epoch 32/80: current_loss=2.00280 | best_loss=0.02707
Epoch 33/80: current_loss=0.82165 | best_loss=0.02707
Epoch 34/80: current_loss=0.61928 | best_loss=0.02707
Epoch 35/80: current_loss=0.04414 | best_loss=0.02707
Epoch 36/80: current_loss=0.49805 | best_loss=0.02707
Epoch 37/80: current_loss=0.05097 | best_loss=0.02707
Epoch 38/80: current_loss=0.07049 | best_loss=0.02707
Epoch 39/80: current_loss=0.29109 | best_loss=0.02707
Early Stopping at epoch 39
      explained_var=-0.02121 | mse_loss=0.02647

----------------------------------------------
Params for Trial 48
{'learning_rate': 0.001, 'weight_decay': 0.0005013526701965179, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04942 | best_loss=0.04942
Epoch 1/80: current_loss=0.03493 | best_loss=0.03493
Epoch 2/80: current_loss=0.03070 | best_loss=0.03070
Epoch 3/80: current_loss=0.02852 | best_loss=0.02852
Epoch 4/80: current_loss=0.02595 | best_loss=0.02595
Epoch 5/80: current_loss=0.02514 | best_loss=0.02514
Epoch 6/80: current_loss=0.02492 | best_loss=0.02492
Epoch 7/80: current_loss=0.03065 | best_loss=0.02492
Epoch 8/80: current_loss=0.02521 | best_loss=0.02492
Epoch 9/80: current_loss=0.02584 | best_loss=0.02492
Epoch 10/80: current_loss=0.02503 | best_loss=0.02492
Epoch 11/80: current_loss=0.02703 | best_loss=0.02492
Epoch 12/80: current_loss=0.03359 | best_loss=0.02492
Epoch 13/80: current_loss=0.02716 | best_loss=0.02492
Epoch 14/80: current_loss=0.02921 | best_loss=0.02492
Epoch 15/80: current_loss=0.02811 | best_loss=0.02492
Epoch 16/80: current_loss=0.02685 | best_loss=0.02492
Epoch 17/80: current_loss=0.02512 | best_loss=0.02492
Epoch 18/80: current_loss=0.03021 | best_loss=0.02492
Epoch 19/80: current_loss=0.02887 | best_loss=0.02492
Epoch 20/80: current_loss=0.03077 | best_loss=0.02492
Epoch 21/80: current_loss=0.02697 | best_loss=0.02492
Epoch 22/80: current_loss=0.02627 | best_loss=0.02492
Epoch 23/80: current_loss=0.02555 | best_loss=0.02492
Epoch 24/80: current_loss=0.02605 | best_loss=0.02492
Epoch 25/80: current_loss=0.03027 | best_loss=0.02492
Epoch 26/80: current_loss=0.03165 | best_loss=0.02492
Early Stopping at epoch 26
      explained_var=0.05948 | mse_loss=0.02449
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02799 | best_loss=0.02799
Epoch 1/80: current_loss=0.02754 | best_loss=0.02754
Epoch 2/80: current_loss=0.03151 | best_loss=0.02754
Epoch 3/80: current_loss=0.02723 | best_loss=0.02723
Epoch 4/80: current_loss=0.02933 | best_loss=0.02723
Epoch 5/80: current_loss=0.02726 | best_loss=0.02723
Epoch 6/80: current_loss=0.02832 | best_loss=0.02723
Epoch 7/80: current_loss=0.02809 | best_loss=0.02723
Epoch 8/80: current_loss=0.02829 | best_loss=0.02723
Epoch 9/80: current_loss=0.02871 | best_loss=0.02723
Epoch 10/80: current_loss=0.02953 | best_loss=0.02723
Epoch 11/80: current_loss=0.02782 | best_loss=0.02723
Epoch 12/80: current_loss=0.02745 | best_loss=0.02723
Epoch 13/80: current_loss=0.02959 | best_loss=0.02723
Epoch 14/80: current_loss=0.02917 | best_loss=0.02723
Epoch 15/80: current_loss=0.03871 | best_loss=0.02723
Epoch 16/80: current_loss=0.02932 | best_loss=0.02723
Epoch 17/80: current_loss=0.02775 | best_loss=0.02723
Epoch 18/80: current_loss=0.02733 | best_loss=0.02723
Epoch 19/80: current_loss=0.03303 | best_loss=0.02723
Epoch 20/80: current_loss=0.02753 | best_loss=0.02723
Epoch 21/80: current_loss=0.02813 | best_loss=0.02723
Epoch 22/80: current_loss=0.02967 | best_loss=0.02723
Epoch 23/80: current_loss=0.02913 | best_loss=0.02723
Early Stopping at epoch 23
      explained_var=0.03624 | mse_loss=0.02678
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02974 | best_loss=0.02974
Epoch 1/80: current_loss=0.02943 | best_loss=0.02943
Epoch 2/80: current_loss=0.03620 | best_loss=0.02943
Epoch 3/80: current_loss=0.03420 | best_loss=0.02943
Epoch 4/80: current_loss=0.03576 | best_loss=0.02943
Epoch 5/80: current_loss=0.03899 | best_loss=0.02943
Epoch 6/80: current_loss=0.03533 | best_loss=0.02943
Epoch 7/80: current_loss=0.03319 | best_loss=0.02943
Epoch 8/80: current_loss=0.03436 | best_loss=0.02943
Epoch 9/80: current_loss=0.03063 | best_loss=0.02943
Epoch 10/80: current_loss=0.03168 | best_loss=0.02943
Epoch 11/80: current_loss=0.02994 | best_loss=0.02943
Epoch 12/80: current_loss=0.03237 | best_loss=0.02943
Epoch 13/80: current_loss=0.03265 | best_loss=0.02943
Epoch 14/80: current_loss=0.03026 | best_loss=0.02943
Epoch 15/80: current_loss=0.03019 | best_loss=0.02943
Epoch 16/80: current_loss=0.02930 | best_loss=0.02930
Epoch 17/80: current_loss=0.02913 | best_loss=0.02913
Epoch 18/80: current_loss=0.03374 | best_loss=0.02913
Epoch 19/80: current_loss=0.03216 | best_loss=0.02913
Epoch 20/80: current_loss=0.03243 | best_loss=0.02913
Epoch 21/80: current_loss=0.03850 | best_loss=0.02913
Epoch 22/80: current_loss=0.03885 | best_loss=0.02913
Epoch 23/80: current_loss=0.03800 | best_loss=0.02913
Epoch 24/80: current_loss=0.03607 | best_loss=0.02913
Epoch 25/80: current_loss=0.33567 | best_loss=0.02913
Epoch 26/80: current_loss=0.19244 | best_loss=0.02913
Epoch 27/80: current_loss=0.04217 | best_loss=0.02913
Epoch 28/80: current_loss=0.04077 | best_loss=0.02913
Epoch 29/80: current_loss=0.09182 | best_loss=0.02913
Epoch 30/80: current_loss=0.03750 | best_loss=0.02913
Epoch 31/80: current_loss=0.03227 | best_loss=0.02913
Epoch 32/80: current_loss=0.03386 | best_loss=0.02913
Epoch 33/80: current_loss=0.05200 | best_loss=0.02913
Epoch 34/80: current_loss=0.03418 | best_loss=0.02913
Epoch 35/80: current_loss=0.03631 | best_loss=0.02913
Epoch 36/80: current_loss=0.05259 | best_loss=0.02913
Epoch 37/80: current_loss=0.03600 | best_loss=0.02913
Early Stopping at epoch 37
      explained_var=0.04807 | mse_loss=0.02833
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04541 | best_loss=0.04541
Epoch 1/80: current_loss=0.03397 | best_loss=0.03397
Epoch 2/80: current_loss=0.03371 | best_loss=0.03371
Epoch 3/80: current_loss=0.03080 | best_loss=0.03080
Epoch 4/80: current_loss=0.02926 | best_loss=0.02926
Epoch 5/80: current_loss=0.02996 | best_loss=0.02926
Epoch 6/80: current_loss=0.03180 | best_loss=0.02926
Epoch 7/80: current_loss=0.03053 | best_loss=0.02926
Epoch 8/80: current_loss=0.03142 | best_loss=0.02926
Epoch 9/80: current_loss=0.02831 | best_loss=0.02831
Epoch 10/80: current_loss=0.04154 | best_loss=0.02831
Epoch 11/80: current_loss=0.03063 | best_loss=0.02831
Epoch 12/80: current_loss=0.03125 | best_loss=0.02831
Epoch 13/80: current_loss=0.02948 | best_loss=0.02831
Epoch 14/80: current_loss=0.02939 | best_loss=0.02831
Epoch 15/80: current_loss=0.03145 | best_loss=0.02831
Epoch 16/80: current_loss=0.03584 | best_loss=0.02831
Epoch 17/80: current_loss=0.03406 | best_loss=0.02831
Epoch 18/80: current_loss=0.03760 | best_loss=0.02831
Epoch 19/80: current_loss=0.03077 | best_loss=0.02831
Epoch 20/80: current_loss=0.02916 | best_loss=0.02831
Epoch 21/80: current_loss=0.02878 | best_loss=0.02831
Epoch 22/80: current_loss=0.02888 | best_loss=0.02831
Epoch 23/80: current_loss=0.02936 | best_loss=0.02831
Epoch 24/80: current_loss=0.03049 | best_loss=0.02831
Epoch 25/80: current_loss=0.03021 | best_loss=0.02831
Epoch 26/80: current_loss=0.03190 | best_loss=0.02831
Epoch 27/80: current_loss=0.03037 | best_loss=0.02831
Epoch 28/80: current_loss=0.03538 | best_loss=0.02831
Epoch 29/80: current_loss=0.03222 | best_loss=0.02831
Early Stopping at epoch 29
      explained_var=-0.00272 | mse_loss=0.02858
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03307 | best_loss=0.03307
Epoch 1/80: current_loss=0.03305 | best_loss=0.03305
Epoch 2/80: current_loss=0.03514 | best_loss=0.03305
Epoch 3/80: current_loss=0.03308 | best_loss=0.03305
Epoch 4/80: current_loss=0.03322 | best_loss=0.03305
Epoch 5/80: current_loss=0.03339 | best_loss=0.03305
Epoch 6/80: current_loss=0.03378 | best_loss=0.03305
Epoch 7/80: current_loss=0.03365 | best_loss=0.03305
Epoch 8/80: current_loss=0.03542 | best_loss=0.03305
Epoch 9/80: current_loss=0.03442 | best_loss=0.03305
Epoch 10/80: current_loss=0.03426 | best_loss=0.03305
Epoch 11/80: current_loss=0.03382 | best_loss=0.03305
Epoch 12/80: current_loss=0.03426 | best_loss=0.03305
Epoch 13/80: current_loss=0.03378 | best_loss=0.03305
Epoch 14/80: current_loss=0.03467 | best_loss=0.03305
Epoch 15/80: current_loss=0.03547 | best_loss=0.03305
Epoch 16/80: current_loss=0.03480 | best_loss=0.03305
Epoch 17/80: current_loss=0.03396 | best_loss=0.03305
Epoch 18/80: current_loss=0.03378 | best_loss=0.03305
Epoch 19/80: current_loss=0.03627 | best_loss=0.03305
Epoch 20/80: current_loss=0.03435 | best_loss=0.03305
Epoch 21/80: current_loss=0.03416 | best_loss=0.03305
Early Stopping at epoch 21
      explained_var=-0.03859 | mse_loss=0.03379
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=0.02050| avg_loss=0.02839
----------------------------------------------


----------------------------------------------
Params for Trial 49
{'learning_rate': 0.0001, 'weight_decay': 0.0026073982463440206, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.05177 | best_loss=0.05177
Epoch 1/80: current_loss=0.03063 | best_loss=0.03063
Epoch 2/80: current_loss=0.03210 | best_loss=0.03063
Epoch 3/80: current_loss=0.02925 | best_loss=0.02925
Epoch 4/80: current_loss=0.02900 | best_loss=0.02900
Epoch 5/80: current_loss=0.02837 | best_loss=0.02837
Epoch 6/80: current_loss=0.02873 | best_loss=0.02837
Epoch 7/80: current_loss=0.02928 | best_loss=0.02837
Epoch 8/80: current_loss=0.02748 | best_loss=0.02748
Epoch 9/80: current_loss=0.02926 | best_loss=0.02748
Epoch 10/80: current_loss=0.02698 | best_loss=0.02698
Epoch 11/80: current_loss=0.02710 | best_loss=0.02698
Epoch 12/80: current_loss=0.02724 | best_loss=0.02698
Epoch 13/80: current_loss=0.02745 | best_loss=0.02698
Epoch 14/80: current_loss=0.02600 | best_loss=0.02600
Epoch 15/80: current_loss=0.02664 | best_loss=0.02600
Epoch 16/80: current_loss=0.02591 | best_loss=0.02591
Epoch 17/80: current_loss=0.02698 | best_loss=0.02591
Epoch 18/80: current_loss=0.02662 | best_loss=0.02591
Epoch 19/80: current_loss=0.02655 | best_loss=0.02591
Epoch 20/80: current_loss=0.02644 | best_loss=0.02591
Epoch 21/80: current_loss=0.02703 | best_loss=0.02591
Epoch 22/80: current_loss=0.02653 | best_loss=0.02591
Epoch 23/80: current_loss=0.02690 | best_loss=0.02591
Epoch 24/80: current_loss=0.02656 | best_loss=0.02591
Epoch 25/80: current_loss=0.02696 | best_loss=0.02591
Epoch 26/80: current_loss=0.02689 | best_loss=0.02591
Epoch 27/80: current_loss=0.02683 | best_loss=0.02591
Epoch 28/80: current_loss=0.02678 | best_loss=0.02591
Epoch 29/80: current_loss=0.02721 | best_loss=0.02591
Epoch 30/80: current_loss=0.02647 | best_loss=0.02591
Epoch 31/80: current_loss=0.02878 | best_loss=0.02591
Epoch 32/80: current_loss=0.02544 | best_loss=0.02544
Epoch 33/80: current_loss=0.02907 | best_loss=0.02544
Epoch 34/80: current_loss=0.02548 | best_loss=0.02544
Epoch 35/80: current_loss=0.02812 | best_loss=0.02544
Epoch 36/80: current_loss=0.02585 | best_loss=0.02544
Epoch 37/80: current_loss=0.02616 | best_loss=0.02544
Epoch 38/80: current_loss=0.02656 | best_loss=0.02544
Epoch 39/80: current_loss=0.02606 | best_loss=0.02544
Epoch 40/80: current_loss=0.02695 | best_loss=0.02544
Epoch 41/80: current_loss=0.02643 | best_loss=0.02544
Epoch 42/80: current_loss=0.02555 | best_loss=0.02544
Epoch 43/80: current_loss=0.02931 | best_loss=0.02544
Epoch 44/80: current_loss=0.02574 | best_loss=0.02544
Epoch 45/80: current_loss=0.02639 | best_loss=0.02544
Epoch 46/80: current_loss=0.02690 | best_loss=0.02544
Epoch 47/80: current_loss=0.02620 | best_loss=0.02544
Epoch 48/80: current_loss=0.02717 | best_loss=0.02544
Epoch 49/80: current_loss=0.02595 | best_loss=0.02544
Epoch 50/80: current_loss=0.02660 | best_loss=0.02544
Epoch 51/80: current_loss=0.02639 | best_loss=0.02544
Epoch 52/80: current_loss=0.02560 | best_loss=0.02544
Early Stopping at epoch 52
      explained_var=0.03737 | mse_loss=0.02492
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02951 | best_loss=0.02951
Epoch 1/80: current_loss=0.02906 | best_loss=0.02906
Epoch 2/80: current_loss=0.02875 | best_loss=0.02875
Epoch 3/80: current_loss=0.02837 | best_loss=0.02837
Epoch 4/80: current_loss=0.02864 | best_loss=0.02837
Epoch 5/80: current_loss=0.02826 | best_loss=0.02826
Epoch 6/80: current_loss=0.02818 | best_loss=0.02818
Epoch 7/80: current_loss=0.02827 | best_loss=0.02818
Epoch 8/80: current_loss=0.02810 | best_loss=0.02810
Epoch 9/80: current_loss=0.02821 | best_loss=0.02810
Epoch 10/80: current_loss=0.02837 | best_loss=0.02810
Epoch 11/80: current_loss=0.02816 | best_loss=0.02810
Epoch 12/80: current_loss=0.02859 | best_loss=0.02810
Epoch 13/80: current_loss=0.02859 | best_loss=0.02810
Epoch 14/80: current_loss=0.02998 | best_loss=0.02810
Epoch 15/80: current_loss=0.02800 | best_loss=0.02800
Epoch 16/80: current_loss=0.02877 | best_loss=0.02800
Epoch 17/80: current_loss=0.02819 | best_loss=0.02800
Epoch 18/80: current_loss=0.02819 | best_loss=0.02800
Epoch 19/80: current_loss=0.02852 | best_loss=0.02800
Epoch 20/80: current_loss=0.02820 | best_loss=0.02800
Epoch 21/80: current_loss=0.02810 | best_loss=0.02800
Epoch 22/80: current_loss=0.02853 | best_loss=0.02800
Epoch 23/80: current_loss=0.02790 | best_loss=0.02790
Epoch 24/80: current_loss=0.02808 | best_loss=0.02790
Epoch 25/80: current_loss=0.02827 | best_loss=0.02790
Epoch 26/80: current_loss=0.02826 | best_loss=0.02790
Epoch 27/80: current_loss=0.02851 | best_loss=0.02790
Epoch 28/80: current_loss=0.02843 | best_loss=0.02790
Epoch 29/80: current_loss=0.02869 | best_loss=0.02790
Epoch 30/80: current_loss=0.02899 | best_loss=0.02790
Epoch 31/80: current_loss=0.02819 | best_loss=0.02790
Epoch 32/80: current_loss=0.02865 | best_loss=0.02790
Epoch 33/80: current_loss=0.02849 | best_loss=0.02790
Epoch 34/80: current_loss=0.02977 | best_loss=0.02790
Epoch 35/80: current_loss=0.02836 | best_loss=0.02790
Epoch 36/80: current_loss=0.02802 | best_loss=0.02790
Epoch 37/80: current_loss=0.02987 | best_loss=0.02790
Epoch 38/80: current_loss=0.02932 | best_loss=0.02790
Epoch 39/80: current_loss=0.03024 | best_loss=0.02790
Epoch 40/80: current_loss=0.02812 | best_loss=0.02790
Epoch 41/80: current_loss=0.02871 | best_loss=0.02790
Epoch 42/80: current_loss=0.02840 | best_loss=0.02790
Epoch 43/80: current_loss=0.02883 | best_loss=0.02790
Early Stopping at epoch 43
      explained_var=0.01367 | mse_loss=0.02742
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03446 | best_loss=0.03446
Epoch 1/80: current_loss=0.02988 | best_loss=0.02988
Epoch 2/80: current_loss=0.03035 | best_loss=0.02988
Epoch 3/80: current_loss=0.03236 | best_loss=0.02988
Epoch 4/80: current_loss=0.03045 | best_loss=0.02988
Epoch 5/80: current_loss=0.03093 | best_loss=0.02988
Epoch 6/80: current_loss=0.03046 | best_loss=0.02988
Epoch 7/80: current_loss=0.03012 | best_loss=0.02988
Epoch 8/80: current_loss=0.03020 | best_loss=0.02988
Epoch 9/80: current_loss=0.03026 | best_loss=0.02988
Epoch 10/80: current_loss=0.03011 | best_loss=0.02988
Epoch 11/80: current_loss=0.03204 | best_loss=0.02988
Epoch 12/80: current_loss=0.03000 | best_loss=0.02988
Epoch 13/80: current_loss=0.03283 | best_loss=0.02988
Epoch 14/80: current_loss=0.03009 | best_loss=0.02988
Epoch 15/80: current_loss=0.03042 | best_loss=0.02988
Epoch 16/80: current_loss=0.03206 | best_loss=0.02988
Epoch 17/80: current_loss=0.03021 | best_loss=0.02988
Epoch 18/80: current_loss=0.03000 | best_loss=0.02988
Epoch 19/80: current_loss=0.03157 | best_loss=0.02988
Epoch 20/80: current_loss=0.03118 | best_loss=0.02988
Epoch 21/80: current_loss=0.03090 | best_loss=0.02988
Early Stopping at epoch 21
      explained_var=0.01889 | mse_loss=0.02914
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03185 | best_loss=0.03185
Epoch 1/80: current_loss=0.02852 | best_loss=0.02852
Epoch 2/80: current_loss=0.02833 | best_loss=0.02833
Epoch 3/80: current_loss=0.02844 | best_loss=0.02833
Epoch 4/80: current_loss=0.02859 | best_loss=0.02833
Epoch 5/80: current_loss=0.02862 | best_loss=0.02833
Epoch 6/80: current_loss=0.02886 | best_loss=0.02833
Epoch 7/80: current_loss=0.02876 | best_loss=0.02833
Epoch 8/80: current_loss=0.02880 | best_loss=0.02833
Epoch 9/80: current_loss=0.02854 | best_loss=0.02833
Epoch 10/80: current_loss=0.02924 | best_loss=0.02833
Epoch 11/80: current_loss=0.02905 | best_loss=0.02833
Epoch 12/80: current_loss=0.02848 | best_loss=0.02833
Epoch 13/80: current_loss=0.02858 | best_loss=0.02833
Epoch 14/80: current_loss=0.02852 | best_loss=0.02833
Epoch 15/80: current_loss=0.02963 | best_loss=0.02833
Epoch 16/80: current_loss=0.02883 | best_loss=0.02833
Epoch 17/80: current_loss=0.02838 | best_loss=0.02833
Epoch 18/80: current_loss=0.03025 | best_loss=0.02833
Epoch 19/80: current_loss=0.02910 | best_loss=0.02833
Epoch 20/80: current_loss=0.02882 | best_loss=0.02833
Epoch 21/80: current_loss=0.02831 | best_loss=0.02831
Epoch 22/80: current_loss=0.02825 | best_loss=0.02825
Epoch 23/80: current_loss=0.02823 | best_loss=0.02823
Epoch 24/80: current_loss=0.02909 | best_loss=0.02823
Epoch 25/80: current_loss=0.02851 | best_loss=0.02823
Epoch 26/80: current_loss=0.02889 | best_loss=0.02823
Epoch 27/80: current_loss=0.02980 | best_loss=0.02823
Epoch 28/80: current_loss=0.02850 | best_loss=0.02823
Epoch 29/80: current_loss=0.02836 | best_loss=0.02823
Epoch 30/80: current_loss=0.02842 | best_loss=0.02823
Epoch 31/80: current_loss=0.02836 | best_loss=0.02823
Epoch 32/80: current_loss=0.02833 | best_loss=0.02823
Epoch 33/80: current_loss=0.02876 | best_loss=0.02823
Epoch 34/80: current_loss=0.02839 | best_loss=0.02823
Epoch 35/80: current_loss=0.02865 | best_loss=0.02823
Epoch 36/80: current_loss=0.02838 | best_loss=0.02823
Epoch 37/80: current_loss=0.02845 | best_loss=0.02823
Epoch 38/80: current_loss=0.02854 | best_loss=0.02823
Epoch 39/80: current_loss=0.02864 | best_loss=0.02823
Epoch 40/80: current_loss=0.03000 | best_loss=0.02823
Epoch 41/80: current_loss=0.02841 | best_loss=0.02823
Epoch 42/80: current_loss=0.02833 | best_loss=0.02823
Epoch 43/80: current_loss=0.02833 | best_loss=0.02823
Early Stopping at epoch 43
      explained_var=-0.00546 | mse_loss=0.02854
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03298 | best_loss=0.03298
Epoch 1/80: current_loss=0.03313 | best_loss=0.03298
Epoch 2/80: current_loss=0.03326 | best_loss=0.03298
Epoch 3/80: current_loss=0.03333 | best_loss=0.03298
Epoch 4/80: current_loss=0.03339 | best_loss=0.03298
Epoch 5/80: current_loss=0.03348 | best_loss=0.03298
Epoch 6/80: current_loss=0.03374 | best_loss=0.03298
Epoch 7/80: current_loss=0.03350 | best_loss=0.03298
Epoch 8/80: current_loss=0.03363 | best_loss=0.03298
Epoch 9/80: current_loss=0.03359 | best_loss=0.03298
Epoch 10/80: current_loss=0.03364 | best_loss=0.03298
Epoch 11/80: current_loss=0.03354 | best_loss=0.03298
Epoch 12/80: current_loss=0.03375 | best_loss=0.03298
Epoch 13/80: current_loss=0.03369 | best_loss=0.03298
Epoch 14/80: current_loss=0.03374 | best_loss=0.03298
Epoch 15/80: current_loss=0.03374 | best_loss=0.03298
Epoch 16/80: current_loss=0.03378 | best_loss=0.03298
Epoch 17/80: current_loss=0.03379 | best_loss=0.03298
Epoch 18/80: current_loss=0.03373 | best_loss=0.03298
Epoch 19/80: current_loss=0.03366 | best_loss=0.03298
Epoch 20/80: current_loss=0.03371 | best_loss=0.03298
Early Stopping at epoch 20
      explained_var=-0.03781 | mse_loss=0.03378
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.00533| avg_loss=0.02876
----------------------------------------------


----------------------------------------------
Params for Trial 50
{'learning_rate': 0.01, 'weight_decay': 0.0011890368076176152, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03950 | best_loss=0.03950
Epoch 1/80: current_loss=0.03779 | best_loss=0.03779
Epoch 2/80: current_loss=0.02683 | best_loss=0.02683
Epoch 3/80: current_loss=0.03014 | best_loss=0.02683
Epoch 4/80: current_loss=0.02923 | best_loss=0.02683
Epoch 5/80: current_loss=0.03197 | best_loss=0.02683
Epoch 6/80: current_loss=0.02780 | best_loss=0.02683
Epoch 7/80: current_loss=0.02870 | best_loss=0.02683
Epoch 8/80: current_loss=0.03361 | best_loss=0.02683
Epoch 9/80: current_loss=0.02695 | best_loss=0.02683
Epoch 10/80: current_loss=0.03478 | best_loss=0.02683
Epoch 11/80: current_loss=0.02840 | best_loss=0.02683
Epoch 12/80: current_loss=0.02753 | best_loss=0.02683
Epoch 13/80: current_loss=0.02703 | best_loss=0.02683
Epoch 14/80: current_loss=0.08465 | best_loss=0.02683
Epoch 15/80: current_loss=0.02704 | best_loss=0.02683
Epoch 16/80: current_loss=0.03214 | best_loss=0.02683
Epoch 17/80: current_loss=0.02935 | best_loss=0.02683
Epoch 18/80: current_loss=0.03884 | best_loss=0.02683
Epoch 19/80: current_loss=0.02779 | best_loss=0.02683
Epoch 20/80: current_loss=0.03315 | best_loss=0.02683
Epoch 21/80: current_loss=0.03394 | best_loss=0.02683
Epoch 22/80: current_loss=0.02946 | best_loss=0.02683
Early Stopping at epoch 22
      explained_var=-0.01204 | mse_loss=0.02614

----------------------------------------------
Params for Trial 51
{'learning_rate': 0.001, 'weight_decay': 0.001082991294582433, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03223 | best_loss=0.03223
Epoch 1/80: current_loss=0.02901 | best_loss=0.02901
Epoch 2/80: current_loss=0.02692 | best_loss=0.02692
Epoch 3/80: current_loss=0.02923 | best_loss=0.02692
Epoch 4/80: current_loss=0.03762 | best_loss=0.02692
Epoch 5/80: current_loss=0.02515 | best_loss=0.02515
Epoch 6/80: current_loss=0.02776 | best_loss=0.02515
Epoch 7/80: current_loss=0.02704 | best_loss=0.02515
Epoch 8/80: current_loss=0.02655 | best_loss=0.02515
Epoch 9/80: current_loss=0.02678 | best_loss=0.02515
Epoch 10/80: current_loss=0.02609 | best_loss=0.02515
Epoch 11/80: current_loss=0.03411 | best_loss=0.02515
Epoch 12/80: current_loss=0.03038 | best_loss=0.02515
Epoch 13/80: current_loss=0.02580 | best_loss=0.02515
Epoch 14/80: current_loss=0.03023 | best_loss=0.02515
Epoch 15/80: current_loss=0.02802 | best_loss=0.02515
Epoch 16/80: current_loss=0.02651 | best_loss=0.02515
Epoch 17/80: current_loss=0.02537 | best_loss=0.02515
Epoch 18/80: current_loss=0.02579 | best_loss=0.02515
Epoch 19/80: current_loss=0.02643 | best_loss=0.02515
Epoch 20/80: current_loss=0.02640 | best_loss=0.02515
Epoch 21/80: current_loss=0.02713 | best_loss=0.02515
Epoch 22/80: current_loss=0.02647 | best_loss=0.02515
Epoch 23/80: current_loss=0.02517 | best_loss=0.02515
Epoch 24/80: current_loss=0.02626 | best_loss=0.02515
Epoch 25/80: current_loss=0.02651 | best_loss=0.02515
Early Stopping at epoch 25
      explained_var=0.04856 | mse_loss=0.02458
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.09905 | best_loss=0.09905
Epoch 1/80: current_loss=0.02788 | best_loss=0.02788
Epoch 2/80: current_loss=0.03497 | best_loss=0.02788
Epoch 3/80: current_loss=0.02786 | best_loss=0.02786
Epoch 4/80: current_loss=0.02822 | best_loss=0.02786
Epoch 5/80: current_loss=0.02823 | best_loss=0.02786
Epoch 6/80: current_loss=0.02851 | best_loss=0.02786
Epoch 7/80: current_loss=0.02814 | best_loss=0.02786
Epoch 8/80: current_loss=0.03058 | best_loss=0.02786
Epoch 9/80: current_loss=0.02831 | best_loss=0.02786
Epoch 10/80: current_loss=0.02991 | best_loss=0.02786
Epoch 11/80: current_loss=0.02791 | best_loss=0.02786
Epoch 12/80: current_loss=0.02830 | best_loss=0.02786
Epoch 13/80: current_loss=0.02809 | best_loss=0.02786
Epoch 14/80: current_loss=0.02843 | best_loss=0.02786
Epoch 15/80: current_loss=0.02894 | best_loss=0.02786
Epoch 16/80: current_loss=0.02788 | best_loss=0.02786
Epoch 17/80: current_loss=0.02779 | best_loss=0.02779
Epoch 18/80: current_loss=0.02746 | best_loss=0.02746
Epoch 19/80: current_loss=0.02883 | best_loss=0.02746
Epoch 20/80: current_loss=0.02832 | best_loss=0.02746
Epoch 21/80: current_loss=0.02817 | best_loss=0.02746
Epoch 22/80: current_loss=0.02996 | best_loss=0.02746
Epoch 23/80: current_loss=0.02798 | best_loss=0.02746
Epoch 24/80: current_loss=0.02804 | best_loss=0.02746
Epoch 25/80: current_loss=0.02798 | best_loss=0.02746
Epoch 26/80: current_loss=0.03027 | best_loss=0.02746
Epoch 27/80: current_loss=0.02894 | best_loss=0.02746
Epoch 28/80: current_loss=0.02798 | best_loss=0.02746
Epoch 29/80: current_loss=0.03097 | best_loss=0.02746
Epoch 30/80: current_loss=0.02805 | best_loss=0.02746
Epoch 31/80: current_loss=0.02819 | best_loss=0.02746
Epoch 32/80: current_loss=0.02806 | best_loss=0.02746
Epoch 33/80: current_loss=0.03017 | best_loss=0.02746
Epoch 34/80: current_loss=0.03013 | best_loss=0.02746
Epoch 35/80: current_loss=0.02894 | best_loss=0.02746
Epoch 36/80: current_loss=0.03118 | best_loss=0.02746
Epoch 37/80: current_loss=0.02784 | best_loss=0.02746
Epoch 38/80: current_loss=0.02804 | best_loss=0.02746
Early Stopping at epoch 38
      explained_var=0.02866 | mse_loss=0.02700
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03721 | best_loss=0.03721
Epoch 1/80: current_loss=0.03031 | best_loss=0.03031
Epoch 2/80: current_loss=0.03317 | best_loss=0.03031
Epoch 3/80: current_loss=0.03025 | best_loss=0.03025
Epoch 4/80: current_loss=0.03122 | best_loss=0.03025
Epoch 5/80: current_loss=0.03110 | best_loss=0.03025
Epoch 6/80: current_loss=0.03430 | best_loss=0.03025
Epoch 7/80: current_loss=0.03627 | best_loss=0.03025
Epoch 8/80: current_loss=0.03041 | best_loss=0.03025
Epoch 9/80: current_loss=0.03026 | best_loss=0.03025
Epoch 10/80: current_loss=0.03024 | best_loss=0.03024
Epoch 11/80: current_loss=0.03188 | best_loss=0.03024
Epoch 12/80: current_loss=0.03124 | best_loss=0.03024
Epoch 13/80: current_loss=0.02937 | best_loss=0.02937
Epoch 14/80: current_loss=0.02924 | best_loss=0.02924
Epoch 15/80: current_loss=0.03400 | best_loss=0.02924
Epoch 16/80: current_loss=0.03016 | best_loss=0.02924
Epoch 17/80: current_loss=0.02970 | best_loss=0.02924
Epoch 18/80: current_loss=0.03068 | best_loss=0.02924
Epoch 19/80: current_loss=0.03013 | best_loss=0.02924
Epoch 20/80: current_loss=0.02984 | best_loss=0.02924
Epoch 21/80: current_loss=0.03031 | best_loss=0.02924
Epoch 22/80: current_loss=0.02948 | best_loss=0.02924
Epoch 23/80: current_loss=0.02958 | best_loss=0.02924
Epoch 24/80: current_loss=0.03080 | best_loss=0.02924
Epoch 25/80: current_loss=0.02969 | best_loss=0.02924
Epoch 26/80: current_loss=0.03044 | best_loss=0.02924
Epoch 27/80: current_loss=0.03508 | best_loss=0.02924
Epoch 28/80: current_loss=0.03276 | best_loss=0.02924
Epoch 29/80: current_loss=0.03022 | best_loss=0.02924
Epoch 30/80: current_loss=0.03035 | best_loss=0.02924
Epoch 31/80: current_loss=0.03272 | best_loss=0.02924
Epoch 32/80: current_loss=0.02933 | best_loss=0.02924
Epoch 33/80: current_loss=0.03300 | best_loss=0.02924
Epoch 34/80: current_loss=0.03513 | best_loss=0.02924
Early Stopping at epoch 34
      explained_var=0.03670 | mse_loss=0.02849
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03207 | best_loss=0.03207
Epoch 1/80: current_loss=0.03530 | best_loss=0.03207
Epoch 2/80: current_loss=0.03341 | best_loss=0.03207
Epoch 3/80: current_loss=0.03376 | best_loss=0.03207
Epoch 4/80: current_loss=0.03022 | best_loss=0.03022
Epoch 5/80: current_loss=0.03190 | best_loss=0.03022
Epoch 6/80: current_loss=0.03066 | best_loss=0.03022
Epoch 7/80: current_loss=0.02951 | best_loss=0.02951
Epoch 8/80: current_loss=0.02876 | best_loss=0.02876
Epoch 9/80: current_loss=0.02961 | best_loss=0.02876
Epoch 10/80: current_loss=0.02889 | best_loss=0.02876
Epoch 11/80: current_loss=0.02835 | best_loss=0.02835
Epoch 12/80: current_loss=0.02832 | best_loss=0.02832
Epoch 13/80: current_loss=0.02991 | best_loss=0.02832
Epoch 14/80: current_loss=0.02930 | best_loss=0.02832
Epoch 15/80: current_loss=0.02905 | best_loss=0.02832
Epoch 16/80: current_loss=0.02873 | best_loss=0.02832
Epoch 17/80: current_loss=0.02850 | best_loss=0.02832
Epoch 18/80: current_loss=0.02857 | best_loss=0.02832
Epoch 19/80: current_loss=0.02897 | best_loss=0.02832
Epoch 20/80: current_loss=0.02865 | best_loss=0.02832
Epoch 21/80: current_loss=0.02803 | best_loss=0.02803
Epoch 22/80: current_loss=0.02891 | best_loss=0.02803
Epoch 23/80: current_loss=0.02864 | best_loss=0.02803
Epoch 24/80: current_loss=0.02861 | best_loss=0.02803
Epoch 25/80: current_loss=0.02942 | best_loss=0.02803
Epoch 26/80: current_loss=0.02902 | best_loss=0.02803
Epoch 27/80: current_loss=0.02847 | best_loss=0.02803
Epoch 28/80: current_loss=0.02869 | best_loss=0.02803
Epoch 29/80: current_loss=0.02904 | best_loss=0.02803
Epoch 30/80: current_loss=0.02878 | best_loss=0.02803
Epoch 31/80: current_loss=0.02827 | best_loss=0.02803
Epoch 32/80: current_loss=0.02834 | best_loss=0.02803
Epoch 33/80: current_loss=0.02819 | best_loss=0.02803
Epoch 34/80: current_loss=0.02831 | best_loss=0.02803
Epoch 35/80: current_loss=0.02817 | best_loss=0.02803
Epoch 36/80: current_loss=0.02824 | best_loss=0.02803
Epoch 37/80: current_loss=0.02848 | best_loss=0.02803
Epoch 38/80: current_loss=0.02871 | best_loss=0.02803
Epoch 39/80: current_loss=0.02889 | best_loss=0.02803
Epoch 40/80: current_loss=0.02852 | best_loss=0.02803
Epoch 41/80: current_loss=0.02898 | best_loss=0.02803
Early Stopping at epoch 41
      explained_var=0.00208 | mse_loss=0.02831
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03329 | best_loss=0.03329
Epoch 1/80: current_loss=0.03479 | best_loss=0.03329
Epoch 2/80: current_loss=0.03420 | best_loss=0.03329
Epoch 3/80: current_loss=0.03463 | best_loss=0.03329
Epoch 4/80: current_loss=0.03368 | best_loss=0.03329
Epoch 5/80: current_loss=0.03657 | best_loss=0.03329
Epoch 6/80: current_loss=0.03285 | best_loss=0.03285
Epoch 7/80: current_loss=0.03529 | best_loss=0.03285
Epoch 8/80: current_loss=0.03428 | best_loss=0.03285
Epoch 9/80: current_loss=0.03314 | best_loss=0.03285
Epoch 10/80: current_loss=0.03415 | best_loss=0.03285
Epoch 11/80: current_loss=0.03523 | best_loss=0.03285
Epoch 12/80: current_loss=0.03382 | best_loss=0.03285
Epoch 13/80: current_loss=0.03697 | best_loss=0.03285
Epoch 14/80: current_loss=0.03381 | best_loss=0.03285
Epoch 15/80: current_loss=0.03457 | best_loss=0.03285
Epoch 16/80: current_loss=0.03273 | best_loss=0.03273
Epoch 17/80: current_loss=0.03322 | best_loss=0.03273
Epoch 18/80: current_loss=0.03497 | best_loss=0.03273
Epoch 19/80: current_loss=0.03509 | best_loss=0.03273
Epoch 20/80: current_loss=0.03384 | best_loss=0.03273
Epoch 21/80: current_loss=0.03489 | best_loss=0.03273
Epoch 22/80: current_loss=0.03403 | best_loss=0.03273
Epoch 23/80: current_loss=0.03338 | best_loss=0.03273
Epoch 24/80: current_loss=0.03390 | best_loss=0.03273
Epoch 25/80: current_loss=0.03369 | best_loss=0.03273
Epoch 26/80: current_loss=0.03500 | best_loss=0.03273
Epoch 27/80: current_loss=0.03391 | best_loss=0.03273
Epoch 28/80: current_loss=0.03404 | best_loss=0.03273
Epoch 29/80: current_loss=0.03423 | best_loss=0.03273
Epoch 30/80: current_loss=0.03540 | best_loss=0.03273
Epoch 31/80: current_loss=0.03383 | best_loss=0.03273
Epoch 32/80: current_loss=0.03400 | best_loss=0.03273
Epoch 33/80: current_loss=0.03353 | best_loss=0.03273
Epoch 34/80: current_loss=0.03388 | best_loss=0.03273
Epoch 35/80: current_loss=0.03575 | best_loss=0.03273
Epoch 36/80: current_loss=0.03299 | best_loss=0.03273
Early Stopping at epoch 36
      explained_var=-0.02869 | mse_loss=0.03353
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.01746| avg_loss=0.02838
----------------------------------------------


----------------------------------------------
Params for Trial 52
{'learning_rate': 0.001, 'weight_decay': 0.0004905844663295239, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03280 | best_loss=0.03280
Epoch 1/80: current_loss=0.02728 | best_loss=0.02728
Epoch 2/80: current_loss=0.02905 | best_loss=0.02728
Epoch 3/80: current_loss=0.02713 | best_loss=0.02713
Epoch 4/80: current_loss=0.02745 | best_loss=0.02713
Epoch 5/80: current_loss=0.02661 | best_loss=0.02661
Epoch 6/80: current_loss=0.02873 | best_loss=0.02661
Epoch 7/80: current_loss=0.02645 | best_loss=0.02645
Epoch 8/80: current_loss=0.02896 | best_loss=0.02645
Epoch 9/80: current_loss=0.02999 | best_loss=0.02645
Epoch 10/80: current_loss=0.03907 | best_loss=0.02645
Epoch 11/80: current_loss=0.03139 | best_loss=0.02645
Epoch 12/80: current_loss=0.02589 | best_loss=0.02589
Epoch 13/80: current_loss=0.02596 | best_loss=0.02589
Epoch 14/80: current_loss=0.02641 | best_loss=0.02589
Epoch 15/80: current_loss=0.02848 | best_loss=0.02589
Epoch 16/80: current_loss=0.02843 | best_loss=0.02589
Epoch 17/80: current_loss=0.02617 | best_loss=0.02589
Epoch 18/80: current_loss=0.02592 | best_loss=0.02589
Epoch 19/80: current_loss=0.02578 | best_loss=0.02578
Epoch 20/80: current_loss=0.02777 | best_loss=0.02578
Epoch 21/80: current_loss=0.02750 | best_loss=0.02578
Epoch 22/80: current_loss=0.02730 | best_loss=0.02578
Epoch 23/80: current_loss=0.02806 | best_loss=0.02578
Epoch 24/80: current_loss=0.02482 | best_loss=0.02482
Epoch 25/80: current_loss=0.03739 | best_loss=0.02482
Epoch 26/80: current_loss=0.03216 | best_loss=0.02482
Epoch 27/80: current_loss=0.02659 | best_loss=0.02482
Epoch 28/80: current_loss=0.02567 | best_loss=0.02482
Epoch 29/80: current_loss=0.02948 | best_loss=0.02482
Epoch 30/80: current_loss=0.02688 | best_loss=0.02482
Epoch 31/80: current_loss=0.02573 | best_loss=0.02482
Epoch 32/80: current_loss=0.02575 | best_loss=0.02482
Epoch 33/80: current_loss=0.02737 | best_loss=0.02482
Epoch 34/80: current_loss=0.03033 | best_loss=0.02482
Epoch 35/80: current_loss=0.03503 | best_loss=0.02482
Epoch 36/80: current_loss=0.02564 | best_loss=0.02482
Epoch 37/80: current_loss=0.02614 | best_loss=0.02482
Epoch 38/80: current_loss=0.02544 | best_loss=0.02482
Epoch 39/80: current_loss=0.02813 | best_loss=0.02482
Epoch 40/80: current_loss=0.02991 | best_loss=0.02482
Epoch 41/80: current_loss=0.02733 | best_loss=0.02482
Epoch 42/80: current_loss=0.02637 | best_loss=0.02482
Epoch 43/80: current_loss=0.03195 | best_loss=0.02482
Epoch 44/80: current_loss=0.02698 | best_loss=0.02482
Early Stopping at epoch 44
      explained_var=0.06464 | mse_loss=0.02443
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03041 | best_loss=0.03041
Epoch 1/80: current_loss=0.05952 | best_loss=0.03041
Epoch 2/80: current_loss=0.04364 | best_loss=0.03041
Epoch 3/80: current_loss=0.03243 | best_loss=0.03041
Epoch 4/80: current_loss=0.03067 | best_loss=0.03041
Epoch 5/80: current_loss=0.03089 | best_loss=0.03041
Epoch 6/80: current_loss=0.03122 | best_loss=0.03041
Epoch 7/80: current_loss=0.02920 | best_loss=0.02920
Epoch 8/80: current_loss=0.02815 | best_loss=0.02815
Epoch 9/80: current_loss=0.02781 | best_loss=0.02781
Epoch 10/80: current_loss=0.02986 | best_loss=0.02781
Epoch 11/80: current_loss=0.03222 | best_loss=0.02781
Epoch 12/80: current_loss=0.03109 | best_loss=0.02781
Epoch 13/80: current_loss=0.03033 | best_loss=0.02781
Epoch 14/80: current_loss=0.02917 | best_loss=0.02781
Epoch 15/80: current_loss=0.02996 | best_loss=0.02781
Epoch 16/80: current_loss=0.02877 | best_loss=0.02781
Epoch 17/80: current_loss=0.02873 | best_loss=0.02781
Epoch 18/80: current_loss=0.02845 | best_loss=0.02781
Epoch 19/80: current_loss=0.02886 | best_loss=0.02781
Epoch 20/80: current_loss=0.02841 | best_loss=0.02781
Epoch 21/80: current_loss=0.02874 | best_loss=0.02781
Epoch 22/80: current_loss=0.02823 | best_loss=0.02781
Epoch 23/80: current_loss=0.02819 | best_loss=0.02781
Epoch 24/80: current_loss=0.02811 | best_loss=0.02781
Epoch 25/80: current_loss=0.02867 | best_loss=0.02781
Epoch 26/80: current_loss=0.02797 | best_loss=0.02781
Epoch 27/80: current_loss=0.02818 | best_loss=0.02781
Epoch 28/80: current_loss=0.02872 | best_loss=0.02781
Epoch 29/80: current_loss=0.02782 | best_loss=0.02781
Early Stopping at epoch 29
      explained_var=0.01741 | mse_loss=0.02734
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03147 | best_loss=0.03147
Epoch 1/80: current_loss=0.03080 | best_loss=0.03080
Epoch 2/80: current_loss=0.03679 | best_loss=0.03080
Epoch 3/80: current_loss=0.03372 | best_loss=0.03080
Epoch 4/80: current_loss=0.03398 | best_loss=0.03080
Epoch 5/80: current_loss=0.03335 | best_loss=0.03080
Epoch 6/80: current_loss=0.03047 | best_loss=0.03047
Epoch 7/80: current_loss=0.03031 | best_loss=0.03031
Epoch 8/80: current_loss=0.02997 | best_loss=0.02997
Epoch 9/80: current_loss=0.03189 | best_loss=0.02997
Epoch 10/80: current_loss=0.02998 | best_loss=0.02997
Epoch 11/80: current_loss=0.03308 | best_loss=0.02997
Epoch 12/80: current_loss=0.02926 | best_loss=0.02926
Epoch 13/80: current_loss=0.02914 | best_loss=0.02914
Epoch 14/80: current_loss=0.03668 | best_loss=0.02914
Epoch 15/80: current_loss=0.03658 | best_loss=0.02914
Epoch 16/80: current_loss=0.03402 | best_loss=0.02914
Epoch 17/80: current_loss=0.03467 | best_loss=0.02914
Epoch 18/80: current_loss=0.62719 | best_loss=0.02914
Epoch 19/80: current_loss=0.47334 | best_loss=0.02914
Epoch 20/80: current_loss=0.07622 | best_loss=0.02914
Epoch 21/80: current_loss=0.06482 | best_loss=0.02914
Epoch 22/80: current_loss=0.04686 | best_loss=0.02914
Epoch 23/80: current_loss=0.04051 | best_loss=0.02914
Epoch 24/80: current_loss=0.03232 | best_loss=0.02914
Epoch 25/80: current_loss=0.03460 | best_loss=0.02914
Epoch 26/80: current_loss=0.03639 | best_loss=0.02914
Epoch 27/80: current_loss=0.03906 | best_loss=0.02914
Epoch 28/80: current_loss=0.03765 | best_loss=0.02914
Epoch 29/80: current_loss=0.03213 | best_loss=0.02914
Epoch 30/80: current_loss=0.03143 | best_loss=0.02914
Epoch 31/80: current_loss=0.03279 | best_loss=0.02914
Epoch 32/80: current_loss=0.03674 | best_loss=0.02914
Epoch 33/80: current_loss=0.03590 | best_loss=0.02914
Early Stopping at epoch 33
      explained_var=0.04147 | mse_loss=0.02842
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03846 | best_loss=0.03846
Epoch 1/80: current_loss=0.02849 | best_loss=0.02849
Epoch 2/80: current_loss=0.02845 | best_loss=0.02845
Epoch 3/80: current_loss=0.03363 | best_loss=0.02845
Epoch 4/80: current_loss=0.03114 | best_loss=0.02845
Epoch 5/80: current_loss=0.04137 | best_loss=0.02845
Epoch 6/80: current_loss=0.02792 | best_loss=0.02792
Epoch 7/80: current_loss=0.02948 | best_loss=0.02792
Epoch 8/80: current_loss=0.03685 | best_loss=0.02792
Epoch 9/80: current_loss=0.03367 | best_loss=0.02792
Epoch 10/80: current_loss=0.05342 | best_loss=0.02792
Epoch 11/80: current_loss=0.03512 | best_loss=0.02792
Epoch 12/80: current_loss=0.02886 | best_loss=0.02792
Epoch 13/80: current_loss=0.04075 | best_loss=0.02792
Epoch 14/80: current_loss=0.02942 | best_loss=0.02792
Epoch 15/80: current_loss=0.03071 | best_loss=0.02792
Epoch 16/80: current_loss=0.02945 | best_loss=0.02792
Epoch 17/80: current_loss=0.02929 | best_loss=0.02792
Epoch 18/80: current_loss=0.02894 | best_loss=0.02792
Epoch 19/80: current_loss=0.02899 | best_loss=0.02792
Epoch 20/80: current_loss=0.02849 | best_loss=0.02792
Epoch 21/80: current_loss=0.02955 | best_loss=0.02792
Epoch 22/80: current_loss=0.02917 | best_loss=0.02792
Epoch 23/80: current_loss=0.02886 | best_loss=0.02792
Epoch 24/80: current_loss=0.02986 | best_loss=0.02792
Epoch 25/80: current_loss=0.02914 | best_loss=0.02792
Epoch 26/80: current_loss=0.02878 | best_loss=0.02792
Early Stopping at epoch 26
      explained_var=0.00804 | mse_loss=0.02818
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04345 | best_loss=0.04345
Epoch 1/80: current_loss=0.03974 | best_loss=0.03974
Epoch 2/80: current_loss=0.04024 | best_loss=0.03974
Epoch 3/80: current_loss=0.04290 | best_loss=0.03974
Epoch 4/80: current_loss=0.03285 | best_loss=0.03285
Epoch 5/80: current_loss=0.03414 | best_loss=0.03285
Epoch 6/80: current_loss=0.03749 | best_loss=0.03285
Epoch 7/80: current_loss=0.04164 | best_loss=0.03285
Epoch 8/80: current_loss=0.03489 | best_loss=0.03285
Epoch 9/80: current_loss=0.03466 | best_loss=0.03285
Epoch 10/80: current_loss=0.03368 | best_loss=0.03285
Epoch 11/80: current_loss=0.03352 | best_loss=0.03285
Epoch 12/80: current_loss=0.03359 | best_loss=0.03285
Epoch 13/80: current_loss=0.03372 | best_loss=0.03285
Epoch 14/80: current_loss=0.03270 | best_loss=0.03270
Epoch 15/80: current_loss=0.03280 | best_loss=0.03270
Epoch 16/80: current_loss=0.03288 | best_loss=0.03270
Epoch 17/80: current_loss=0.03303 | best_loss=0.03270
Epoch 18/80: current_loss=0.03413 | best_loss=0.03270
Epoch 19/80: current_loss=0.03593 | best_loss=0.03270
Epoch 20/80: current_loss=0.03347 | best_loss=0.03270
Epoch 21/80: current_loss=0.03348 | best_loss=0.03270
Epoch 22/80: current_loss=0.03522 | best_loss=0.03270
Epoch 23/80: current_loss=0.03400 | best_loss=0.03270
Epoch 24/80: current_loss=0.03319 | best_loss=0.03270
Epoch 25/80: current_loss=0.03355 | best_loss=0.03270
Epoch 26/80: current_loss=0.03335 | best_loss=0.03270
Epoch 27/80: current_loss=0.03310 | best_loss=0.03270
Epoch 28/80: current_loss=0.03339 | best_loss=0.03270
Epoch 29/80: current_loss=0.03374 | best_loss=0.03270
Epoch 30/80: current_loss=0.03343 | best_loss=0.03270
Epoch 31/80: current_loss=0.03320 | best_loss=0.03270
Epoch 32/80: current_loss=0.03420 | best_loss=0.03270
Epoch 33/80: current_loss=0.03366 | best_loss=0.03270
Epoch 34/80: current_loss=0.03379 | best_loss=0.03270
Early Stopping at epoch 34
      explained_var=-0.02809 | mse_loss=0.03340
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.02069| avg_loss=0.02836
----------------------------------------------


----------------------------------------------
Params for Trial 53
{'learning_rate': 0.001, 'weight_decay': 0.00041237920372180406, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03144 | best_loss=0.03144
Epoch 1/80: current_loss=0.02920 | best_loss=0.02920
Epoch 2/80: current_loss=0.02908 | best_loss=0.02908
Epoch 3/80: current_loss=0.02844 | best_loss=0.02844
Epoch 4/80: current_loss=0.02651 | best_loss=0.02651
Epoch 5/80: current_loss=0.02491 | best_loss=0.02491
Epoch 6/80: current_loss=0.02619 | best_loss=0.02491
Epoch 7/80: current_loss=0.04564 | best_loss=0.02491
Epoch 8/80: current_loss=0.02851 | best_loss=0.02491
Epoch 9/80: current_loss=0.02845 | best_loss=0.02491
Epoch 10/80: current_loss=0.02548 | best_loss=0.02491
Epoch 11/80: current_loss=0.02646 | best_loss=0.02491
Epoch 12/80: current_loss=0.02580 | best_loss=0.02491
Epoch 13/80: current_loss=0.02568 | best_loss=0.02491
Epoch 14/80: current_loss=0.02904 | best_loss=0.02491
Epoch 15/80: current_loss=0.03071 | best_loss=0.02491
Epoch 16/80: current_loss=0.02748 | best_loss=0.02491
Epoch 17/80: current_loss=0.02844 | best_loss=0.02491
Epoch 18/80: current_loss=0.02477 | best_loss=0.02477
Epoch 19/80: current_loss=0.03166 | best_loss=0.02477
Epoch 20/80: current_loss=0.02697 | best_loss=0.02477
Epoch 21/80: current_loss=0.03048 | best_loss=0.02477
Epoch 22/80: current_loss=0.03607 | best_loss=0.02477
Epoch 23/80: current_loss=0.02696 | best_loss=0.02477
Epoch 24/80: current_loss=0.02552 | best_loss=0.02477
Epoch 25/80: current_loss=0.02794 | best_loss=0.02477
Epoch 26/80: current_loss=0.02696 | best_loss=0.02477
Epoch 27/80: current_loss=0.02974 | best_loss=0.02477
Epoch 28/80: current_loss=0.02584 | best_loss=0.02477
Epoch 29/80: current_loss=0.02536 | best_loss=0.02477
Epoch 30/80: current_loss=0.02790 | best_loss=0.02477
Epoch 31/80: current_loss=0.02579 | best_loss=0.02477
Epoch 32/80: current_loss=0.02584 | best_loss=0.02477
Epoch 33/80: current_loss=0.02611 | best_loss=0.02477
Epoch 34/80: current_loss=0.02650 | best_loss=0.02477
Epoch 35/80: current_loss=0.02647 | best_loss=0.02477
Epoch 36/80: current_loss=0.02583 | best_loss=0.02477
Epoch 37/80: current_loss=0.02604 | best_loss=0.02477
Epoch 38/80: current_loss=0.02652 | best_loss=0.02477
Early Stopping at epoch 38
      explained_var=0.06212 | mse_loss=0.02426
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02866 | best_loss=0.02866
Epoch 1/80: current_loss=0.02803 | best_loss=0.02803
Epoch 2/80: current_loss=0.02771 | best_loss=0.02771
Epoch 3/80: current_loss=0.03002 | best_loss=0.02771
Epoch 4/80: current_loss=0.02785 | best_loss=0.02771
Epoch 5/80: current_loss=0.02795 | best_loss=0.02771
Epoch 6/80: current_loss=0.02778 | best_loss=0.02771
Epoch 7/80: current_loss=0.02809 | best_loss=0.02771
Epoch 8/80: current_loss=0.03028 | best_loss=0.02771
Epoch 9/80: current_loss=0.03063 | best_loss=0.02771
Epoch 10/80: current_loss=0.02817 | best_loss=0.02771
Epoch 11/80: current_loss=0.03147 | best_loss=0.02771
Epoch 12/80: current_loss=0.02832 | best_loss=0.02771
Epoch 13/80: current_loss=0.02841 | best_loss=0.02771
Epoch 14/80: current_loss=0.02942 | best_loss=0.02771
Epoch 15/80: current_loss=0.03020 | best_loss=0.02771
Epoch 16/80: current_loss=0.02789 | best_loss=0.02771
Epoch 17/80: current_loss=0.02797 | best_loss=0.02771
Epoch 18/80: current_loss=0.02858 | best_loss=0.02771
Epoch 19/80: current_loss=0.02833 | best_loss=0.02771
Epoch 20/80: current_loss=0.02834 | best_loss=0.02771
Epoch 21/80: current_loss=0.02940 | best_loss=0.02771
Epoch 22/80: current_loss=0.02821 | best_loss=0.02771
Early Stopping at epoch 22
      explained_var=0.02030 | mse_loss=0.02723
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03375 | best_loss=0.03375
Epoch 1/80: current_loss=0.03013 | best_loss=0.03013
Epoch 2/80: current_loss=0.03312 | best_loss=0.03013
Epoch 3/80: current_loss=0.03229 | best_loss=0.03013
Epoch 4/80: current_loss=0.02924 | best_loss=0.02924
Epoch 5/80: current_loss=0.03069 | best_loss=0.02924
Epoch 6/80: current_loss=0.03358 | best_loss=0.02924
Epoch 7/80: current_loss=0.03437 | best_loss=0.02924
Epoch 8/80: current_loss=0.03466 | best_loss=0.02924
Epoch 9/80: current_loss=0.03042 | best_loss=0.02924
Epoch 10/80: current_loss=0.03005 | best_loss=0.02924
Epoch 11/80: current_loss=0.02975 | best_loss=0.02924
Epoch 12/80: current_loss=0.02974 | best_loss=0.02924
Epoch 13/80: current_loss=0.02944 | best_loss=0.02924
Epoch 14/80: current_loss=0.03213 | best_loss=0.02924
Epoch 15/80: current_loss=0.03708 | best_loss=0.02924
Epoch 16/80: current_loss=0.03215 | best_loss=0.02924
Epoch 17/80: current_loss=0.03087 | best_loss=0.02924
Epoch 18/80: current_loss=0.02984 | best_loss=0.02924
Epoch 19/80: current_loss=0.02985 | best_loss=0.02924
Epoch 20/80: current_loss=0.03653 | best_loss=0.02924
Epoch 21/80: current_loss=0.03308 | best_loss=0.02924
Epoch 22/80: current_loss=0.03176 | best_loss=0.02924
Epoch 23/80: current_loss=0.03074 | best_loss=0.02924
Epoch 24/80: current_loss=0.03741 | best_loss=0.02924
Early Stopping at epoch 24
      explained_var=0.03827 | mse_loss=0.02850
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02846 | best_loss=0.02846
Epoch 1/80: current_loss=0.02951 | best_loss=0.02846
Epoch 2/80: current_loss=0.02839 | best_loss=0.02839
Epoch 3/80: current_loss=0.02839 | best_loss=0.02839
Epoch 4/80: current_loss=0.02951 | best_loss=0.02839
Epoch 5/80: current_loss=0.02787 | best_loss=0.02787
Epoch 6/80: current_loss=0.02975 | best_loss=0.02787
Epoch 7/80: current_loss=0.02914 | best_loss=0.02787
Epoch 8/80: current_loss=0.02918 | best_loss=0.02787
Epoch 9/80: current_loss=0.02786 | best_loss=0.02786
Epoch 10/80: current_loss=0.02932 | best_loss=0.02786
Epoch 11/80: current_loss=0.02866 | best_loss=0.02786
Epoch 12/80: current_loss=0.02899 | best_loss=0.02786
Epoch 13/80: current_loss=0.02903 | best_loss=0.02786
Epoch 14/80: current_loss=0.03056 | best_loss=0.02786
Epoch 15/80: current_loss=0.02843 | best_loss=0.02786
Epoch 16/80: current_loss=0.02879 | best_loss=0.02786
Epoch 17/80: current_loss=0.02824 | best_loss=0.02786
Epoch 18/80: current_loss=0.02885 | best_loss=0.02786
Epoch 19/80: current_loss=0.02834 | best_loss=0.02786
Epoch 20/80: current_loss=0.02886 | best_loss=0.02786
Epoch 21/80: current_loss=0.02882 | best_loss=0.02786
Epoch 22/80: current_loss=0.02854 | best_loss=0.02786
Epoch 23/80: current_loss=0.03281 | best_loss=0.02786
Epoch 24/80: current_loss=0.02894 | best_loss=0.02786
Epoch 25/80: current_loss=0.02861 | best_loss=0.02786
Epoch 26/80: current_loss=1.83511 | best_loss=0.02786
Epoch 27/80: current_loss=0.12401 | best_loss=0.02786
Epoch 28/80: current_loss=0.03376 | best_loss=0.02786
Epoch 29/80: current_loss=0.03504 | best_loss=0.02786
Early Stopping at epoch 29
      explained_var=0.00769 | mse_loss=0.02823
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03750 | best_loss=0.03750
Epoch 1/80: current_loss=0.03202 | best_loss=0.03202
Epoch 2/80: current_loss=0.03895 | best_loss=0.03202
Epoch 3/80: current_loss=0.03309 | best_loss=0.03202
Epoch 4/80: current_loss=0.03418 | best_loss=0.03202
Epoch 5/80: current_loss=0.03666 | best_loss=0.03202
Epoch 6/80: current_loss=0.04269 | best_loss=0.03202
Epoch 7/80: current_loss=0.03182 | best_loss=0.03182
Epoch 8/80: current_loss=0.05875 | best_loss=0.03182
Epoch 9/80: current_loss=0.03292 | best_loss=0.03182
Epoch 10/80: current_loss=0.03416 | best_loss=0.03182
Epoch 11/80: current_loss=0.04817 | best_loss=0.03182
Epoch 12/80: current_loss=0.05798 | best_loss=0.03182
Epoch 13/80: current_loss=0.03248 | best_loss=0.03182
Epoch 14/80: current_loss=0.03343 | best_loss=0.03182
Epoch 15/80: current_loss=0.03439 | best_loss=0.03182
Epoch 16/80: current_loss=0.03254 | best_loss=0.03182
Epoch 17/80: current_loss=0.03375 | best_loss=0.03182
Epoch 18/80: current_loss=0.03301 | best_loss=0.03182
Epoch 19/80: current_loss=0.03876 | best_loss=0.03182
Epoch 20/80: current_loss=0.03248 | best_loss=0.03182
Epoch 21/80: current_loss=0.03742 | best_loss=0.03182
Epoch 22/80: current_loss=0.03378 | best_loss=0.03182
Epoch 23/80: current_loss=0.03307 | best_loss=0.03182
Epoch 24/80: current_loss=0.04094 | best_loss=0.03182
Epoch 25/80: current_loss=0.03635 | best_loss=0.03182
Epoch 26/80: current_loss=0.03626 | best_loss=0.03182
Epoch 27/80: current_loss=0.04251 | best_loss=0.03182
Early Stopping at epoch 27
      explained_var=0.00290 | mse_loss=0.03239
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=0.02626| avg_loss=0.02812
----------------------------------------------


----------------------------------------------
Params for Trial 54
{'learning_rate': 0.001, 'weight_decay': 0.00044089725406154756, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03278 | best_loss=0.03278
Epoch 1/80: current_loss=0.03206 | best_loss=0.03206
Epoch 2/80: current_loss=0.02631 | best_loss=0.02631
Epoch 3/80: current_loss=0.03460 | best_loss=0.02631
Epoch 4/80: current_loss=0.02479 | best_loss=0.02479
Epoch 5/80: current_loss=0.02654 | best_loss=0.02479
Epoch 6/80: current_loss=0.02902 | best_loss=0.02479
Epoch 7/80: current_loss=0.02609 | best_loss=0.02479
Epoch 8/80: current_loss=0.02602 | best_loss=0.02479
Epoch 9/80: current_loss=0.02763 | best_loss=0.02479
Epoch 10/80: current_loss=0.02587 | best_loss=0.02479
Epoch 11/80: current_loss=0.02856 | best_loss=0.02479
Epoch 12/80: current_loss=0.02485 | best_loss=0.02479
Epoch 13/80: current_loss=0.02754 | best_loss=0.02479
Epoch 14/80: current_loss=0.03425 | best_loss=0.02479
Epoch 15/80: current_loss=0.02972 | best_loss=0.02479
Epoch 16/80: current_loss=0.03658 | best_loss=0.02479
Epoch 17/80: current_loss=0.03193 | best_loss=0.02479
Epoch 18/80: current_loss=0.02568 | best_loss=0.02479
Epoch 19/80: current_loss=0.03443 | best_loss=0.02479
Epoch 20/80: current_loss=0.03133 | best_loss=0.02479
Epoch 21/80: current_loss=0.02642 | best_loss=0.02479
Epoch 22/80: current_loss=0.02668 | best_loss=0.02479
Epoch 23/80: current_loss=0.02431 | best_loss=0.02431
Epoch 24/80: current_loss=0.02487 | best_loss=0.02431
Epoch 25/80: current_loss=0.02466 | best_loss=0.02431
Epoch 26/80: current_loss=0.02500 | best_loss=0.02431
Epoch 27/80: current_loss=0.02573 | best_loss=0.02431
Epoch 28/80: current_loss=0.02613 | best_loss=0.02431
Epoch 29/80: current_loss=0.02629 | best_loss=0.02431
Epoch 30/80: current_loss=0.02711 | best_loss=0.02431
Epoch 31/80: current_loss=0.02593 | best_loss=0.02431
Epoch 32/80: current_loss=0.02643 | best_loss=0.02431
Epoch 33/80: current_loss=0.02665 | best_loss=0.02431
Epoch 34/80: current_loss=0.02647 | best_loss=0.02431
Epoch 35/80: current_loss=0.02636 | best_loss=0.02431
Epoch 36/80: current_loss=0.02616 | best_loss=0.02431
Epoch 37/80: current_loss=0.02550 | best_loss=0.02431
Epoch 38/80: current_loss=0.02607 | best_loss=0.02431
Epoch 39/80: current_loss=0.02754 | best_loss=0.02431
Epoch 40/80: current_loss=0.02642 | best_loss=0.02431
Epoch 41/80: current_loss=0.03072 | best_loss=0.02431
Epoch 42/80: current_loss=0.03845 | best_loss=0.02431
Epoch 43/80: current_loss=0.02687 | best_loss=0.02431
Early Stopping at epoch 43
      explained_var=0.07850 | mse_loss=0.02383
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03337 | best_loss=0.03337
Epoch 1/80: current_loss=0.02774 | best_loss=0.02774
Epoch 2/80: current_loss=0.02734 | best_loss=0.02734
Epoch 3/80: current_loss=0.02750 | best_loss=0.02734
Epoch 4/80: current_loss=0.02929 | best_loss=0.02734
Epoch 5/80: current_loss=0.02750 | best_loss=0.02734
Epoch 6/80: current_loss=0.02888 | best_loss=0.02734
Epoch 7/80: current_loss=0.02945 | best_loss=0.02734
Epoch 8/80: current_loss=0.02757 | best_loss=0.02734
Epoch 9/80: current_loss=0.02799 | best_loss=0.02734
Epoch 10/80: current_loss=0.02821 | best_loss=0.02734
Epoch 11/80: current_loss=0.02878 | best_loss=0.02734
Epoch 12/80: current_loss=0.03478 | best_loss=0.02734
Epoch 13/80: current_loss=0.03242 | best_loss=0.02734
Epoch 14/80: current_loss=0.03144 | best_loss=0.02734
Epoch 15/80: current_loss=0.02832 | best_loss=0.02734
Epoch 16/80: current_loss=0.02840 | best_loss=0.02734
Epoch 17/80: current_loss=0.02769 | best_loss=0.02734
Epoch 18/80: current_loss=0.03173 | best_loss=0.02734
Epoch 19/80: current_loss=0.02778 | best_loss=0.02734
Epoch 20/80: current_loss=0.02827 | best_loss=0.02734
Epoch 21/80: current_loss=0.02868 | best_loss=0.02734
Epoch 22/80: current_loss=0.02871 | best_loss=0.02734
Early Stopping at epoch 22
      explained_var=0.04209 | mse_loss=0.02711
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04247 | best_loss=0.04247
Epoch 1/80: current_loss=0.03297 | best_loss=0.03297
Epoch 2/80: current_loss=0.03522 | best_loss=0.03297
Epoch 3/80: current_loss=0.03234 | best_loss=0.03234
Epoch 4/80: current_loss=0.02962 | best_loss=0.02962
Epoch 5/80: current_loss=0.03085 | best_loss=0.02962
Epoch 6/80: current_loss=0.03173 | best_loss=0.02962
Epoch 7/80: current_loss=0.03045 | best_loss=0.02962
Epoch 8/80: current_loss=0.03093 | best_loss=0.02962
Epoch 9/80: current_loss=0.03147 | best_loss=0.02962
Epoch 10/80: current_loss=0.03150 | best_loss=0.02962
Epoch 11/80: current_loss=0.03782 | best_loss=0.02962
Epoch 12/80: current_loss=0.02895 | best_loss=0.02895
Epoch 13/80: current_loss=0.02938 | best_loss=0.02895
Epoch 14/80: current_loss=0.03791 | best_loss=0.02895
Epoch 15/80: current_loss=0.02981 | best_loss=0.02895
Epoch 16/80: current_loss=0.03116 | best_loss=0.02895
Epoch 17/80: current_loss=0.03720 | best_loss=0.02895
Epoch 18/80: current_loss=0.03312 | best_loss=0.02895
Epoch 19/80: current_loss=0.03015 | best_loss=0.02895
Epoch 20/80: current_loss=0.03042 | best_loss=0.02895
Epoch 21/80: current_loss=0.03646 | best_loss=0.02895
Epoch 22/80: current_loss=0.03024 | best_loss=0.02895
Epoch 23/80: current_loss=0.03200 | best_loss=0.02895
Epoch 24/80: current_loss=0.03024 | best_loss=0.02895
Epoch 25/80: current_loss=0.03983 | best_loss=0.02895
Epoch 26/80: current_loss=0.24904 | best_loss=0.02895
Epoch 27/80: current_loss=0.13745 | best_loss=0.02895
Epoch 28/80: current_loss=0.04177 | best_loss=0.02895
Epoch 29/80: current_loss=0.03602 | best_loss=0.02895
Epoch 30/80: current_loss=0.03325 | best_loss=0.02895
Epoch 31/80: current_loss=0.02955 | best_loss=0.02895
Epoch 32/80: current_loss=0.03876 | best_loss=0.02895
Early Stopping at epoch 32
      explained_var=0.04613 | mse_loss=0.02823
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02979 | best_loss=0.02979
Epoch 1/80: current_loss=0.03394 | best_loss=0.02979
Epoch 2/80: current_loss=0.03071 | best_loss=0.02979
Epoch 3/80: current_loss=0.03441 | best_loss=0.02979
Epoch 4/80: current_loss=0.02885 | best_loss=0.02885
Epoch 5/80: current_loss=0.03374 | best_loss=0.02885
Epoch 6/80: current_loss=0.02935 | best_loss=0.02885
Epoch 7/80: current_loss=0.03047 | best_loss=0.02885
Epoch 8/80: current_loss=0.04683 | best_loss=0.02885
Epoch 9/80: current_loss=0.03758 | best_loss=0.02885
Epoch 10/80: current_loss=0.03144 | best_loss=0.02885
Epoch 11/80: current_loss=0.03207 | best_loss=0.02885
Epoch 12/80: current_loss=0.03169 | best_loss=0.02885
Epoch 13/80: current_loss=0.03352 | best_loss=0.02885
Epoch 14/80: current_loss=0.02860 | best_loss=0.02860
Epoch 15/80: current_loss=0.03886 | best_loss=0.02860
Epoch 16/80: current_loss=0.03452 | best_loss=0.02860
Epoch 17/80: current_loss=0.03199 | best_loss=0.02860
Epoch 18/80: current_loss=0.03382 | best_loss=0.02860
Epoch 19/80: current_loss=0.03999 | best_loss=0.02860
Epoch 20/80: current_loss=0.02885 | best_loss=0.02860
Epoch 21/80: current_loss=0.03512 | best_loss=0.02860
Epoch 22/80: current_loss=0.02933 | best_loss=0.02860
Epoch 23/80: current_loss=0.02859 | best_loss=0.02859
Epoch 24/80: current_loss=0.03020 | best_loss=0.02859
Epoch 25/80: current_loss=0.02931 | best_loss=0.02859
Epoch 26/80: current_loss=0.03141 | best_loss=0.02859
Epoch 27/80: current_loss=0.02851 | best_loss=0.02851
Epoch 28/80: current_loss=0.03101 | best_loss=0.02851
Epoch 29/80: current_loss=0.03969 | best_loss=0.02851
Epoch 30/80: current_loss=0.05886 | best_loss=0.02851
Epoch 31/80: current_loss=0.04472 | best_loss=0.02851
Epoch 32/80: current_loss=0.03021 | best_loss=0.02851
Epoch 33/80: current_loss=0.03152 | best_loss=0.02851
Epoch 34/80: current_loss=0.03087 | best_loss=0.02851
Epoch 35/80: current_loss=0.03211 | best_loss=0.02851
Epoch 36/80: current_loss=0.02943 | best_loss=0.02851
Epoch 37/80: current_loss=0.03687 | best_loss=0.02851
Epoch 38/80: current_loss=0.03068 | best_loss=0.02851
Epoch 39/80: current_loss=0.02974 | best_loss=0.02851
Epoch 40/80: current_loss=0.03809 | best_loss=0.02851
Epoch 41/80: current_loss=0.03971 | best_loss=0.02851
Epoch 42/80: current_loss=0.03143 | best_loss=0.02851
Epoch 43/80: current_loss=0.03354 | best_loss=0.02851
Epoch 44/80: current_loss=0.03140 | best_loss=0.02851
Epoch 45/80: current_loss=0.04355 | best_loss=0.02851
Epoch 46/80: current_loss=0.03629 | best_loss=0.02851
Epoch 47/80: current_loss=0.03415 | best_loss=0.02851
Early Stopping at epoch 47
      explained_var=-0.00609 | mse_loss=0.02881
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.05271 | best_loss=0.05271
Epoch 1/80: current_loss=0.05171 | best_loss=0.05171
Epoch 2/80: current_loss=0.03695 | best_loss=0.03695
Epoch 3/80: current_loss=0.03793 | best_loss=0.03695
Epoch 4/80: current_loss=0.03448 | best_loss=0.03448
Epoch 5/80: current_loss=0.03479 | best_loss=0.03448
Epoch 6/80: current_loss=0.03190 | best_loss=0.03190
Epoch 7/80: current_loss=0.06591 | best_loss=0.03190
Epoch 8/80: current_loss=0.03352 | best_loss=0.03190
Epoch 9/80: current_loss=0.03562 | best_loss=0.03190
Epoch 10/80: current_loss=0.03733 | best_loss=0.03190
Epoch 11/80: current_loss=0.04022 | best_loss=0.03190
Epoch 12/80: current_loss=0.03854 | best_loss=0.03190
Epoch 13/80: current_loss=0.04048 | best_loss=0.03190
Epoch 14/80: current_loss=0.03290 | best_loss=0.03190
Epoch 15/80: current_loss=0.04604 | best_loss=0.03190
Epoch 16/80: current_loss=0.04003 | best_loss=0.03190
Epoch 17/80: current_loss=0.03261 | best_loss=0.03190
Epoch 18/80: current_loss=0.03925 | best_loss=0.03190
Epoch 19/80: current_loss=0.03692 | best_loss=0.03190
Epoch 20/80: current_loss=0.03884 | best_loss=0.03190
Epoch 21/80: current_loss=0.03365 | best_loss=0.03190
Epoch 22/80: current_loss=0.03455 | best_loss=0.03190
Epoch 23/80: current_loss=0.04079 | best_loss=0.03190
Epoch 24/80: current_loss=0.03351 | best_loss=0.03190
Epoch 25/80: current_loss=0.03568 | best_loss=0.03190
Epoch 26/80: current_loss=0.03324 | best_loss=0.03190
Early Stopping at epoch 26
      explained_var=0.00161 | mse_loss=0.03270
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.03245| avg_loss=0.02813
----------------------------------------------


----------------------------------------------
Params for Trial 55
{'learning_rate': 0.001, 'weight_decay': 0.0004704269646001223, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03240 | best_loss=0.03240
Epoch 1/80: current_loss=0.02827 | best_loss=0.02827
Epoch 2/80: current_loss=0.02610 | best_loss=0.02610
Epoch 3/80: current_loss=0.02891 | best_loss=0.02610
Epoch 4/80: current_loss=0.02974 | best_loss=0.02610
Epoch 5/80: current_loss=0.03151 | best_loss=0.02610
Epoch 6/80: current_loss=0.02876 | best_loss=0.02610
Epoch 7/80: current_loss=0.03012 | best_loss=0.02610
Epoch 8/80: current_loss=0.02568 | best_loss=0.02568
Epoch 9/80: current_loss=0.02504 | best_loss=0.02504
Epoch 10/80: current_loss=0.02722 | best_loss=0.02504
Epoch 11/80: current_loss=0.02839 | best_loss=0.02504
Epoch 12/80: current_loss=0.02757 | best_loss=0.02504
Epoch 13/80: current_loss=0.02521 | best_loss=0.02504
Epoch 14/80: current_loss=0.02603 | best_loss=0.02504
Epoch 15/80: current_loss=0.02805 | best_loss=0.02504
Epoch 16/80: current_loss=0.02587 | best_loss=0.02504
Epoch 17/80: current_loss=0.02591 | best_loss=0.02504
Epoch 18/80: current_loss=0.02565 | best_loss=0.02504
Epoch 19/80: current_loss=0.03238 | best_loss=0.02504
Epoch 20/80: current_loss=0.02723 | best_loss=0.02504
Epoch 21/80: current_loss=0.02661 | best_loss=0.02504
Epoch 22/80: current_loss=0.03341 | best_loss=0.02504
Epoch 23/80: current_loss=0.03366 | best_loss=0.02504
Epoch 24/80: current_loss=0.02663 | best_loss=0.02504
Epoch 25/80: current_loss=0.03126 | best_loss=0.02504
Epoch 26/80: current_loss=0.02724 | best_loss=0.02504
Epoch 27/80: current_loss=0.02547 | best_loss=0.02504
Epoch 28/80: current_loss=0.02452 | best_loss=0.02452
Epoch 29/80: current_loss=0.02603 | best_loss=0.02452
Epoch 30/80: current_loss=0.02672 | best_loss=0.02452
Epoch 31/80: current_loss=0.03494 | best_loss=0.02452
Epoch 32/80: current_loss=0.03888 | best_loss=0.02452
Epoch 33/80: current_loss=0.02592 | best_loss=0.02452
Epoch 34/80: current_loss=0.02583 | best_loss=0.02452
Epoch 35/80: current_loss=0.02956 | best_loss=0.02452
Epoch 36/80: current_loss=0.02685 | best_loss=0.02452
Epoch 37/80: current_loss=0.02593 | best_loss=0.02452
Epoch 38/80: current_loss=0.03223 | best_loss=0.02452
Epoch 39/80: current_loss=0.02889 | best_loss=0.02452
Epoch 40/80: current_loss=0.02689 | best_loss=0.02452
Epoch 41/80: current_loss=0.02729 | best_loss=0.02452
Epoch 42/80: current_loss=0.02678 | best_loss=0.02452
Epoch 43/80: current_loss=0.02725 | best_loss=0.02452
Epoch 44/80: current_loss=0.02991 | best_loss=0.02452
Epoch 45/80: current_loss=0.02576 | best_loss=0.02452
Epoch 46/80: current_loss=0.02606 | best_loss=0.02452
Epoch 47/80: current_loss=0.02879 | best_loss=0.02452
Epoch 48/80: current_loss=0.03002 | best_loss=0.02452
Early Stopping at epoch 48
      explained_var=0.07446 | mse_loss=0.02405
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02906 | best_loss=0.02906
Epoch 1/80: current_loss=0.02857 | best_loss=0.02857
Epoch 2/80: current_loss=0.02763 | best_loss=0.02763
Epoch 3/80: current_loss=0.02851 | best_loss=0.02763
Epoch 4/80: current_loss=0.02851 | best_loss=0.02763
Epoch 5/80: current_loss=0.02751 | best_loss=0.02751
Epoch 6/80: current_loss=0.02811 | best_loss=0.02751
Epoch 7/80: current_loss=0.03271 | best_loss=0.02751
Epoch 8/80: current_loss=0.03106 | best_loss=0.02751
Epoch 9/80: current_loss=0.02767 | best_loss=0.02751
Epoch 10/80: current_loss=0.02842 | best_loss=0.02751
Epoch 11/80: current_loss=0.02922 | best_loss=0.02751
Epoch 12/80: current_loss=0.78900 | best_loss=0.02751
Epoch 13/80: current_loss=0.10862 | best_loss=0.02751
Epoch 14/80: current_loss=0.10768 | best_loss=0.02751
Epoch 15/80: current_loss=0.04680 | best_loss=0.02751
Epoch 16/80: current_loss=0.03557 | best_loss=0.02751
Epoch 17/80: current_loss=0.03725 | best_loss=0.02751
Epoch 18/80: current_loss=0.03238 | best_loss=0.02751
Epoch 19/80: current_loss=0.03292 | best_loss=0.02751
Epoch 20/80: current_loss=0.03190 | best_loss=0.02751
Epoch 21/80: current_loss=0.03146 | best_loss=0.02751
Epoch 22/80: current_loss=0.03263 | best_loss=0.02751
Epoch 23/80: current_loss=0.03072 | best_loss=0.02751
Epoch 24/80: current_loss=0.02926 | best_loss=0.02751
Epoch 25/80: current_loss=0.02994 | best_loss=0.02751
Early Stopping at epoch 25
      explained_var=0.02939 | mse_loss=0.02701
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03351 | best_loss=0.03351
Epoch 1/80: current_loss=0.04449 | best_loss=0.03351
Epoch 2/80: current_loss=0.04017 | best_loss=0.03351
Epoch 3/80: current_loss=0.03247 | best_loss=0.03247
Epoch 4/80: current_loss=0.03226 | best_loss=0.03226
Epoch 5/80: current_loss=0.03161 | best_loss=0.03161
Epoch 6/80: current_loss=0.04563 | best_loss=0.03161
Epoch 7/80: current_loss=0.03198 | best_loss=0.03161
Epoch 8/80: current_loss=0.03130 | best_loss=0.03130
Epoch 9/80: current_loss=0.04910 | best_loss=0.03130
Epoch 10/80: current_loss=0.03497 | best_loss=0.03130
Epoch 11/80: current_loss=0.03101 | best_loss=0.03101
Epoch 12/80: current_loss=0.03170 | best_loss=0.03101
Epoch 13/80: current_loss=0.04941 | best_loss=0.03101
Epoch 14/80: current_loss=0.03051 | best_loss=0.03051
Epoch 15/80: current_loss=0.03288 | best_loss=0.03051
Epoch 16/80: current_loss=0.03193 | best_loss=0.03051
Epoch 17/80: current_loss=0.04566 | best_loss=0.03051
Epoch 18/80: current_loss=0.03165 | best_loss=0.03051
Epoch 19/80: current_loss=0.06979 | best_loss=0.03051
Epoch 20/80: current_loss=0.03287 | best_loss=0.03051
Epoch 21/80: current_loss=0.06498 | best_loss=0.03051
Epoch 22/80: current_loss=0.06222 | best_loss=0.03051
Epoch 23/80: current_loss=0.03052 | best_loss=0.03051
Epoch 24/80: current_loss=0.03398 | best_loss=0.03051
Epoch 25/80: current_loss=0.03876 | best_loss=0.03051
Epoch 26/80: current_loss=0.04068 | best_loss=0.03051
Epoch 27/80: current_loss=0.03682 | best_loss=0.03051
Epoch 28/80: current_loss=0.03790 | best_loss=0.03051
Epoch 29/80: current_loss=0.03101 | best_loss=0.03051
Epoch 30/80: current_loss=0.03171 | best_loss=0.03051
Epoch 31/80: current_loss=0.03116 | best_loss=0.03051
Epoch 32/80: current_loss=0.03091 | best_loss=0.03051
Epoch 33/80: current_loss=0.03115 | best_loss=0.03051
Epoch 34/80: current_loss=0.04315 | best_loss=0.03051
Early Stopping at epoch 34
      explained_var=-0.00099 | mse_loss=0.02979
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03975 | best_loss=0.03975
Epoch 1/80: current_loss=0.02899 | best_loss=0.02899
Epoch 2/80: current_loss=0.03076 | best_loss=0.02899
Epoch 3/80: current_loss=0.02959 | best_loss=0.02899
Epoch 4/80: current_loss=0.02974 | best_loss=0.02899
Epoch 5/80: current_loss=0.02752 | best_loss=0.02752
Epoch 6/80: current_loss=0.03010 | best_loss=0.02752
Epoch 7/80: current_loss=0.02834 | best_loss=0.02752
Epoch 8/80: current_loss=0.02909 | best_loss=0.02752
Epoch 9/80: current_loss=0.03050 | best_loss=0.02752
Epoch 10/80: current_loss=0.02968 | best_loss=0.02752
Epoch 11/80: current_loss=0.03079 | best_loss=0.02752
Epoch 12/80: current_loss=0.02899 | best_loss=0.02752
Epoch 13/80: current_loss=0.03062 | best_loss=0.02752
Epoch 14/80: current_loss=0.03020 | best_loss=0.02752
Epoch 15/80: current_loss=0.02960 | best_loss=0.02752
Epoch 16/80: current_loss=0.02916 | best_loss=0.02752
Epoch 17/80: current_loss=0.03065 | best_loss=0.02752
Epoch 18/80: current_loss=0.03050 | best_loss=0.02752
Epoch 19/80: current_loss=0.02894 | best_loss=0.02752
Epoch 20/80: current_loss=0.02866 | best_loss=0.02752
Epoch 21/80: current_loss=0.02989 | best_loss=0.02752
Epoch 22/80: current_loss=0.02893 | best_loss=0.02752
Epoch 23/80: current_loss=0.02849 | best_loss=0.02752
Epoch 24/80: current_loss=0.03089 | best_loss=0.02752
Epoch 25/80: current_loss=0.02955 | best_loss=0.02752
Early Stopping at epoch 25
      explained_var=0.02054 | mse_loss=0.02784
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03391 | best_loss=0.03391
Epoch 1/80: current_loss=0.03589 | best_loss=0.03391
Epoch 2/80: current_loss=0.03430 | best_loss=0.03391
Epoch 3/80: current_loss=0.03542 | best_loss=0.03391
Epoch 4/80: current_loss=0.03333 | best_loss=0.03333
Epoch 5/80: current_loss=0.03539 | best_loss=0.03333
Epoch 6/80: current_loss=0.03531 | best_loss=0.03333
Epoch 7/80: current_loss=0.03387 | best_loss=0.03333
Epoch 8/80: current_loss=0.03445 | best_loss=0.03333
Epoch 9/80: current_loss=0.03361 | best_loss=0.03333
Epoch 10/80: current_loss=0.03458 | best_loss=0.03333
Epoch 11/80: current_loss=0.03366 | best_loss=0.03333
Epoch 12/80: current_loss=0.03390 | best_loss=0.03333
Epoch 13/80: current_loss=0.03381 | best_loss=0.03333
Epoch 14/80: current_loss=0.03522 | best_loss=0.03333
Epoch 15/80: current_loss=0.03336 | best_loss=0.03333
Epoch 16/80: current_loss=0.03412 | best_loss=0.03333
Epoch 17/80: current_loss=0.03405 | best_loss=0.03333
Epoch 18/80: current_loss=0.03400 | best_loss=0.03333
Epoch 19/80: current_loss=0.03438 | best_loss=0.03333
Epoch 20/80: current_loss=0.03306 | best_loss=0.03306
Epoch 21/80: current_loss=0.03344 | best_loss=0.03306
Epoch 22/80: current_loss=0.03347 | best_loss=0.03306
Epoch 23/80: current_loss=0.03356 | best_loss=0.03306
Epoch 24/80: current_loss=0.03617 | best_loss=0.03306
Epoch 25/80: current_loss=0.03358 | best_loss=0.03306
Epoch 26/80: current_loss=0.03338 | best_loss=0.03306
Epoch 27/80: current_loss=0.03313 | best_loss=0.03306
Epoch 28/80: current_loss=0.03338 | best_loss=0.03306
Epoch 29/80: current_loss=0.03359 | best_loss=0.03306
Epoch 30/80: current_loss=0.03376 | best_loss=0.03306
Epoch 31/80: current_loss=0.03404 | best_loss=0.03306
Epoch 32/80: current_loss=0.03368 | best_loss=0.03306
Epoch 33/80: current_loss=0.03359 | best_loss=0.03306
Epoch 34/80: current_loss=0.03353 | best_loss=0.03306
Epoch 35/80: current_loss=0.24387 | best_loss=0.03306
Epoch 36/80: current_loss=0.03372 | best_loss=0.03306
Epoch 37/80: current_loss=0.09774 | best_loss=0.03306
Epoch 38/80: current_loss=0.04304 | best_loss=0.03306
Epoch 39/80: current_loss=0.03427 | best_loss=0.03306
Epoch 40/80: current_loss=0.03911 | best_loss=0.03306
Early Stopping at epoch 40
      explained_var=-0.04189 | mse_loss=0.03385
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.01630| avg_loss=0.02851
----------------------------------------------


----------------------------------------------
Params for Trial 56
{'learning_rate': 0.001, 'weight_decay': 0.0014551910699131326, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04171 | best_loss=0.04171
Epoch 1/80: current_loss=0.04012 | best_loss=0.04012
Epoch 2/80: current_loss=0.02801 | best_loss=0.02801
Epoch 3/80: current_loss=0.03128 | best_loss=0.02801
Epoch 4/80: current_loss=0.02608 | best_loss=0.02608
Epoch 5/80: current_loss=0.02574 | best_loss=0.02574
Epoch 6/80: current_loss=0.02727 | best_loss=0.02574
Epoch 7/80: current_loss=0.02684 | best_loss=0.02574
Epoch 8/80: current_loss=0.02687 | best_loss=0.02574
Epoch 9/80: current_loss=0.05163 | best_loss=0.02574
Epoch 10/80: current_loss=0.02673 | best_loss=0.02574
Epoch 11/80: current_loss=0.02597 | best_loss=0.02574
Epoch 12/80: current_loss=0.02892 | best_loss=0.02574
Epoch 13/80: current_loss=0.02943 | best_loss=0.02574
Epoch 14/80: current_loss=0.02622 | best_loss=0.02574
Epoch 15/80: current_loss=0.02611 | best_loss=0.02574
Epoch 16/80: current_loss=0.02610 | best_loss=0.02574
Epoch 17/80: current_loss=0.02565 | best_loss=0.02565
Epoch 18/80: current_loss=0.02490 | best_loss=0.02490
Epoch 19/80: current_loss=0.02897 | best_loss=0.02490
Epoch 20/80: current_loss=0.02769 | best_loss=0.02490
Epoch 21/80: current_loss=0.02710 | best_loss=0.02490
Epoch 22/80: current_loss=0.02859 | best_loss=0.02490
Epoch 23/80: current_loss=0.02944 | best_loss=0.02490
Epoch 24/80: current_loss=0.02757 | best_loss=0.02490
Epoch 25/80: current_loss=0.02621 | best_loss=0.02490
Epoch 26/80: current_loss=0.02590 | best_loss=0.02490
Epoch 27/80: current_loss=0.02843 | best_loss=0.02490
Epoch 28/80: current_loss=0.02566 | best_loss=0.02490
Epoch 29/80: current_loss=0.02479 | best_loss=0.02479
Epoch 30/80: current_loss=0.02547 | best_loss=0.02479
Epoch 31/80: current_loss=0.02543 | best_loss=0.02479
Epoch 32/80: current_loss=0.02714 | best_loss=0.02479
Epoch 33/80: current_loss=0.03848 | best_loss=0.02479
Epoch 34/80: current_loss=0.02722 | best_loss=0.02479
Epoch 35/80: current_loss=0.02516 | best_loss=0.02479
Epoch 36/80: current_loss=0.02512 | best_loss=0.02479
Epoch 37/80: current_loss=0.02852 | best_loss=0.02479
Epoch 38/80: current_loss=0.02814 | best_loss=0.02479
Epoch 39/80: current_loss=0.02937 | best_loss=0.02479
Epoch 40/80: current_loss=0.03069 | best_loss=0.02479
Epoch 41/80: current_loss=0.02701 | best_loss=0.02479
Epoch 42/80: current_loss=0.02599 | best_loss=0.02479
Epoch 43/80: current_loss=0.03121 | best_loss=0.02479
Epoch 44/80: current_loss=0.02649 | best_loss=0.02479
Epoch 45/80: current_loss=0.02674 | best_loss=0.02479
Epoch 46/80: current_loss=0.03003 | best_loss=0.02479
Epoch 47/80: current_loss=0.02641 | best_loss=0.02479
Epoch 48/80: current_loss=0.02655 | best_loss=0.02479
Epoch 49/80: current_loss=0.02602 | best_loss=0.02479
Early Stopping at epoch 49
      explained_var=0.06070 | mse_loss=0.02433
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02964 | best_loss=0.02964
Epoch 1/80: current_loss=0.02916 | best_loss=0.02916
Epoch 2/80: current_loss=0.02869 | best_loss=0.02869
Epoch 3/80: current_loss=0.02814 | best_loss=0.02814
Epoch 4/80: current_loss=0.02770 | best_loss=0.02770
Epoch 5/80: current_loss=0.02796 | best_loss=0.02770
Epoch 6/80: current_loss=0.02790 | best_loss=0.02770
Epoch 7/80: current_loss=0.02783 | best_loss=0.02770
Epoch 8/80: current_loss=0.02829 | best_loss=0.02770
Epoch 9/80: current_loss=0.02841 | best_loss=0.02770
Epoch 10/80: current_loss=0.02804 | best_loss=0.02770
Epoch 11/80: current_loss=0.02954 | best_loss=0.02770
Epoch 12/80: current_loss=0.02896 | best_loss=0.02770
Epoch 13/80: current_loss=0.02897 | best_loss=0.02770
Epoch 14/80: current_loss=0.03067 | best_loss=0.02770
Epoch 15/80: current_loss=0.02826 | best_loss=0.02770
Epoch 16/80: current_loss=0.02819 | best_loss=0.02770
Epoch 17/80: current_loss=0.03168 | best_loss=0.02770
Epoch 18/80: current_loss=0.02802 | best_loss=0.02770
Epoch 19/80: current_loss=0.02783 | best_loss=0.02770
Epoch 20/80: current_loss=0.03096 | best_loss=0.02770
Epoch 21/80: current_loss=0.02787 | best_loss=0.02770
Epoch 22/80: current_loss=0.02816 | best_loss=0.02770
Epoch 23/80: current_loss=0.02891 | best_loss=0.02770
Epoch 24/80: current_loss=0.03518 | best_loss=0.02770
Early Stopping at epoch 24
      explained_var=0.03315 | mse_loss=0.02712
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02961 | best_loss=0.02961
Epoch 1/80: current_loss=0.03015 | best_loss=0.02961
Epoch 2/80: current_loss=0.03070 | best_loss=0.02961
Epoch 3/80: current_loss=0.03292 | best_loss=0.02961
Epoch 4/80: current_loss=0.03072 | best_loss=0.02961
Epoch 5/80: current_loss=0.03599 | best_loss=0.02961
Epoch 6/80: current_loss=0.03380 | best_loss=0.02961
Epoch 7/80: current_loss=0.03082 | best_loss=0.02961
Epoch 8/80: current_loss=0.02946 | best_loss=0.02946
Epoch 9/80: current_loss=0.03086 | best_loss=0.02946
Epoch 10/80: current_loss=0.03631 | best_loss=0.02946
Epoch 11/80: current_loss=0.03340 | best_loss=0.02946
Epoch 12/80: current_loss=0.03379 | best_loss=0.02946
Epoch 13/80: current_loss=0.03280 | best_loss=0.02946
Epoch 14/80: current_loss=0.03042 | best_loss=0.02946
Epoch 15/80: current_loss=0.02951 | best_loss=0.02946
Epoch 16/80: current_loss=0.03008 | best_loss=0.02946
Epoch 17/80: current_loss=0.03021 | best_loss=0.02946
Epoch 18/80: current_loss=0.03038 | best_loss=0.02946
Epoch 19/80: current_loss=0.03114 | best_loss=0.02946
Epoch 20/80: current_loss=0.02955 | best_loss=0.02946
Epoch 21/80: current_loss=0.02967 | best_loss=0.02946
Epoch 22/80: current_loss=0.03334 | best_loss=0.02946
Epoch 23/80: current_loss=0.03512 | best_loss=0.02946
Epoch 24/80: current_loss=0.03053 | best_loss=0.02946
Epoch 25/80: current_loss=0.02880 | best_loss=0.02880
Epoch 26/80: current_loss=0.02930 | best_loss=0.02880
Epoch 27/80: current_loss=0.35578 | best_loss=0.02880
Epoch 28/80: current_loss=0.15273 | best_loss=0.02880
Epoch 29/80: current_loss=0.04420 | best_loss=0.02880
Epoch 30/80: current_loss=0.03266 | best_loss=0.02880
Epoch 31/80: current_loss=0.03209 | best_loss=0.02880
Epoch 32/80: current_loss=0.03978 | best_loss=0.02880
Epoch 33/80: current_loss=0.04283 | best_loss=0.02880
Epoch 34/80: current_loss=0.06064 | best_loss=0.02880
Epoch 35/80: current_loss=0.04143 | best_loss=0.02880
Epoch 36/80: current_loss=0.05320 | best_loss=0.02880
Epoch 37/80: current_loss=0.04720 | best_loss=0.02880
Epoch 38/80: current_loss=0.03884 | best_loss=0.02880
Epoch 39/80: current_loss=0.03711 | best_loss=0.02880
Epoch 40/80: current_loss=0.04903 | best_loss=0.02880
Epoch 41/80: current_loss=0.03184 | best_loss=0.02880
Epoch 42/80: current_loss=0.03645 | best_loss=0.02880
Epoch 43/80: current_loss=0.03178 | best_loss=0.02880
Epoch 44/80: current_loss=0.03166 | best_loss=0.02880
Epoch 45/80: current_loss=0.03285 | best_loss=0.02880
Early Stopping at epoch 45
      explained_var=0.05295 | mse_loss=0.02805
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.07504 | best_loss=0.07504
Epoch 1/80: current_loss=0.03140 | best_loss=0.03140
Epoch 2/80: current_loss=0.02980 | best_loss=0.02980
Epoch 3/80: current_loss=0.03750 | best_loss=0.02980
Epoch 4/80: current_loss=0.05237 | best_loss=0.02980
Epoch 5/80: current_loss=0.03546 | best_loss=0.02980
Epoch 6/80: current_loss=0.03542 | best_loss=0.02980
Epoch 7/80: current_loss=0.02862 | best_loss=0.02862
Epoch 8/80: current_loss=0.02935 | best_loss=0.02862
Epoch 9/80: current_loss=0.02983 | best_loss=0.02862
Epoch 10/80: current_loss=0.03805 | best_loss=0.02862
Epoch 11/80: current_loss=0.02889 | best_loss=0.02862
Epoch 12/80: current_loss=0.02884 | best_loss=0.02862
Epoch 13/80: current_loss=0.02906 | best_loss=0.02862
Epoch 14/80: current_loss=0.03048 | best_loss=0.02862
Epoch 15/80: current_loss=0.02871 | best_loss=0.02862
Epoch 16/80: current_loss=0.02848 | best_loss=0.02848
Epoch 17/80: current_loss=0.02808 | best_loss=0.02808
Epoch 18/80: current_loss=0.02869 | best_loss=0.02808
Epoch 19/80: current_loss=0.02831 | best_loss=0.02808
Epoch 20/80: current_loss=0.02825 | best_loss=0.02808
Epoch 21/80: current_loss=0.02812 | best_loss=0.02808
Epoch 22/80: current_loss=0.02833 | best_loss=0.02808
Epoch 23/80: current_loss=0.02828 | best_loss=0.02808
Epoch 24/80: current_loss=0.02835 | best_loss=0.02808
Epoch 25/80: current_loss=0.02838 | best_loss=0.02808
Epoch 26/80: current_loss=0.02837 | best_loss=0.02808
Epoch 27/80: current_loss=0.02849 | best_loss=0.02808
Epoch 28/80: current_loss=0.02864 | best_loss=0.02808
Epoch 29/80: current_loss=0.02917 | best_loss=0.02808
Epoch 30/80: current_loss=0.02852 | best_loss=0.02808
Epoch 31/80: current_loss=0.02830 | best_loss=0.02808
Epoch 32/80: current_loss=0.02826 | best_loss=0.02808
Epoch 33/80: current_loss=0.02840 | best_loss=0.02808
Epoch 34/80: current_loss=0.02913 | best_loss=0.02808
Epoch 35/80: current_loss=0.02814 | best_loss=0.02808
Epoch 36/80: current_loss=0.02837 | best_loss=0.02808
Epoch 37/80: current_loss=0.02863 | best_loss=0.02808
Early Stopping at epoch 37
      explained_var=0.00172 | mse_loss=0.02842
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03320 | best_loss=0.03320
Epoch 1/80: current_loss=0.03360 | best_loss=0.03320
Epoch 2/80: current_loss=0.03334 | best_loss=0.03320
Epoch 3/80: current_loss=0.03445 | best_loss=0.03320
Epoch 4/80: current_loss=0.03394 | best_loss=0.03320
Epoch 5/80: current_loss=0.03542 | best_loss=0.03320
Epoch 6/80: current_loss=0.03397 | best_loss=0.03320
Epoch 7/80: current_loss=0.03426 | best_loss=0.03320
Epoch 8/80: current_loss=0.03386 | best_loss=0.03320
Epoch 9/80: current_loss=0.03378 | best_loss=0.03320
Epoch 10/80: current_loss=0.03354 | best_loss=0.03320
Epoch 11/80: current_loss=0.03397 | best_loss=0.03320
Epoch 12/80: current_loss=0.03364 | best_loss=0.03320
Epoch 13/80: current_loss=0.03395 | best_loss=0.03320
Epoch 14/80: current_loss=0.03421 | best_loss=0.03320
Epoch 15/80: current_loss=0.03436 | best_loss=0.03320
Epoch 16/80: current_loss=0.03436 | best_loss=0.03320
Epoch 17/80: current_loss=0.03487 | best_loss=0.03320
Epoch 18/80: current_loss=0.03393 | best_loss=0.03320
Epoch 19/80: current_loss=0.03415 | best_loss=0.03320
Epoch 20/80: current_loss=0.03402 | best_loss=0.03320
Early Stopping at epoch 20
      explained_var=-0.04025 | mse_loss=0.03394
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.02165| avg_loss=0.02837
----------------------------------------------


----------------------------------------------
Params for Trial 57
{'learning_rate': 0.001, 'weight_decay': 0.007506797681089372, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04410 | best_loss=0.04410
Epoch 1/80: current_loss=0.03438 | best_loss=0.03438
Epoch 2/80: current_loss=0.03063 | best_loss=0.03063
Epoch 3/80: current_loss=0.03153 | best_loss=0.03063
Epoch 4/80: current_loss=0.02860 | best_loss=0.02860
Epoch 5/80: current_loss=0.02820 | best_loss=0.02820
Epoch 6/80: current_loss=0.02886 | best_loss=0.02820
Epoch 7/80: current_loss=0.02788 | best_loss=0.02788
Epoch 8/80: current_loss=0.02806 | best_loss=0.02788
Epoch 9/80: current_loss=0.02782 | best_loss=0.02782
Epoch 10/80: current_loss=0.02888 | best_loss=0.02782
Epoch 11/80: current_loss=0.02710 | best_loss=0.02710
Epoch 12/80: current_loss=0.02785 | best_loss=0.02710
Epoch 13/80: current_loss=0.02713 | best_loss=0.02710
Epoch 14/80: current_loss=0.02765 | best_loss=0.02710
Epoch 15/80: current_loss=0.02722 | best_loss=0.02710
Epoch 16/80: current_loss=0.02688 | best_loss=0.02688
Epoch 17/80: current_loss=0.02681 | best_loss=0.02681
Epoch 18/80: current_loss=0.02717 | best_loss=0.02681
Epoch 19/80: current_loss=0.02679 | best_loss=0.02679
Epoch 20/80: current_loss=0.02784 | best_loss=0.02679
Epoch 21/80: current_loss=0.02774 | best_loss=0.02679
Epoch 22/80: current_loss=0.02655 | best_loss=0.02655
Epoch 23/80: current_loss=0.02773 | best_loss=0.02655
Epoch 24/80: current_loss=0.02674 | best_loss=0.02655
Epoch 25/80: current_loss=0.02646 | best_loss=0.02646
Epoch 26/80: current_loss=0.02769 | best_loss=0.02646
Epoch 27/80: current_loss=0.02645 | best_loss=0.02645
Epoch 28/80: current_loss=0.02666 | best_loss=0.02645
Epoch 29/80: current_loss=0.02752 | best_loss=0.02645
Epoch 30/80: current_loss=0.02623 | best_loss=0.02623
Epoch 31/80: current_loss=0.02757 | best_loss=0.02623
Epoch 32/80: current_loss=0.02627 | best_loss=0.02623
Epoch 33/80: current_loss=0.02710 | best_loss=0.02623
Epoch 34/80: current_loss=0.02624 | best_loss=0.02623
Epoch 35/80: current_loss=0.02736 | best_loss=0.02623
Epoch 36/80: current_loss=0.02654 | best_loss=0.02623
Epoch 37/80: current_loss=0.02730 | best_loss=0.02623
Epoch 38/80: current_loss=0.02661 | best_loss=0.02623
Epoch 39/80: current_loss=0.02679 | best_loss=0.02623
Epoch 40/80: current_loss=0.02715 | best_loss=0.02623
Epoch 41/80: current_loss=0.02616 | best_loss=0.02616
Epoch 42/80: current_loss=0.02696 | best_loss=0.02616
Epoch 43/80: current_loss=0.02681 | best_loss=0.02616
Epoch 44/80: current_loss=0.02649 | best_loss=0.02616
Epoch 45/80: current_loss=0.02658 | best_loss=0.02616
Epoch 46/80: current_loss=0.02687 | best_loss=0.02616
Epoch 47/80: current_loss=0.02677 | best_loss=0.02616
Epoch 48/80: current_loss=0.02635 | best_loss=0.02616
Epoch 49/80: current_loss=0.02672 | best_loss=0.02616
Epoch 50/80: current_loss=0.02662 | best_loss=0.02616
Epoch 51/80: current_loss=0.02724 | best_loss=0.02616
Epoch 52/80: current_loss=0.02697 | best_loss=0.02616
Epoch 53/80: current_loss=0.02755 | best_loss=0.02616
Epoch 54/80: current_loss=0.02814 | best_loss=0.02616
Epoch 55/80: current_loss=0.02657 | best_loss=0.02616
Epoch 56/80: current_loss=0.02685 | best_loss=0.02616
Epoch 57/80: current_loss=0.02734 | best_loss=0.02616
Epoch 58/80: current_loss=0.02712 | best_loss=0.02616
Epoch 59/80: current_loss=0.02673 | best_loss=0.02616
Epoch 60/80: current_loss=0.02697 | best_loss=0.02616
Epoch 61/80: current_loss=0.02645 | best_loss=0.02616
Early Stopping at epoch 61
      explained_var=0.02080 | mse_loss=0.02569

----------------------------------------------
Params for Trial 58
{'learning_rate': 1e-05, 'weight_decay': 0.0009067327769830616, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.24659 | best_loss=0.24659
Epoch 1/80: current_loss=0.22634 | best_loss=0.22634
Epoch 2/80: current_loss=0.20727 | best_loss=0.20727
Epoch 3/80: current_loss=0.18923 | best_loss=0.18923
Epoch 4/80: current_loss=0.17215 | best_loss=0.17215
Epoch 5/80: current_loss=0.15570 | best_loss=0.15570
Epoch 6/80: current_loss=0.14027 | best_loss=0.14027
Epoch 7/80: current_loss=0.12579 | best_loss=0.12579
Epoch 8/80: current_loss=0.11200 | best_loss=0.11200
Epoch 9/80: current_loss=0.09923 | best_loss=0.09923
Epoch 10/80: current_loss=0.08730 | best_loss=0.08730
Epoch 11/80: current_loss=0.07594 | best_loss=0.07594
Epoch 12/80: current_loss=0.06595 | best_loss=0.06595
Epoch 13/80: current_loss=0.05724 | best_loss=0.05724
Epoch 14/80: current_loss=0.04966 | best_loss=0.04966
Epoch 15/80: current_loss=0.04353 | best_loss=0.04353
Epoch 16/80: current_loss=0.03890 | best_loss=0.03890
Epoch 17/80: current_loss=0.03568 | best_loss=0.03568
Epoch 18/80: current_loss=0.03359 | best_loss=0.03359
Epoch 19/80: current_loss=0.03247 | best_loss=0.03247
Epoch 20/80: current_loss=0.03206 | best_loss=0.03206
Epoch 21/80: current_loss=0.03202 | best_loss=0.03202
Epoch 22/80: current_loss=0.03214 | best_loss=0.03202
Epoch 23/80: current_loss=0.03233 | best_loss=0.03202
Epoch 24/80: current_loss=0.03248 | best_loss=0.03202
Epoch 25/80: current_loss=0.03255 | best_loss=0.03202
Epoch 26/80: current_loss=0.03250 | best_loss=0.03202
Epoch 27/80: current_loss=0.03251 | best_loss=0.03202
Epoch 28/80: current_loss=0.03248 | best_loss=0.03202
Epoch 29/80: current_loss=0.03237 | best_loss=0.03202
Epoch 30/80: current_loss=0.03221 | best_loss=0.03202
Epoch 31/80: current_loss=0.03207 | best_loss=0.03202
Epoch 32/80: current_loss=0.03190 | best_loss=0.03190
Epoch 33/80: current_loss=0.03187 | best_loss=0.03187
Epoch 34/80: current_loss=0.03176 | best_loss=0.03176
Epoch 35/80: current_loss=0.03151 | best_loss=0.03151
Epoch 36/80: current_loss=0.03142 | best_loss=0.03142
Epoch 37/80: current_loss=0.03144 | best_loss=0.03142
Epoch 38/80: current_loss=0.03132 | best_loss=0.03132
Epoch 39/80: current_loss=0.03140 | best_loss=0.03132
Epoch 40/80: current_loss=0.03132 | best_loss=0.03132
Epoch 41/80: current_loss=0.03136 | best_loss=0.03132
Epoch 42/80: current_loss=0.03140 | best_loss=0.03132
Epoch 43/80: current_loss=0.03149 | best_loss=0.03132
Epoch 44/80: current_loss=0.03137 | best_loss=0.03132
Epoch 45/80: current_loss=0.03107 | best_loss=0.03107
Epoch 46/80: current_loss=0.03107 | best_loss=0.03107
Epoch 47/80: current_loss=0.03091 | best_loss=0.03091
Epoch 48/80: current_loss=0.03083 | best_loss=0.03083
Epoch 49/80: current_loss=0.03072 | best_loss=0.03072
Epoch 50/80: current_loss=0.03054 | best_loss=0.03054
Epoch 51/80: current_loss=0.03055 | best_loss=0.03054
Epoch 52/80: current_loss=0.03056 | best_loss=0.03054
Epoch 53/80: current_loss=0.03071 | best_loss=0.03054
Epoch 54/80: current_loss=0.03064 | best_loss=0.03054
Epoch 55/80: current_loss=0.03043 | best_loss=0.03043
Epoch 56/80: current_loss=0.03016 | best_loss=0.03016
Epoch 57/80: current_loss=0.03016 | best_loss=0.03016
Epoch 58/80: current_loss=0.03013 | best_loss=0.03013
Epoch 59/80: current_loss=0.02996 | best_loss=0.02996
Epoch 60/80: current_loss=0.02995 | best_loss=0.02995
Epoch 61/80: current_loss=0.02987 | best_loss=0.02987
Epoch 62/80: current_loss=0.03002 | best_loss=0.02987
Epoch 63/80: current_loss=0.02989 | best_loss=0.02987
Epoch 64/80: current_loss=0.02966 | best_loss=0.02966
Epoch 65/80: current_loss=0.02962 | best_loss=0.02962
Epoch 66/80: current_loss=0.02973 | best_loss=0.02962
Epoch 67/80: current_loss=0.02968 | best_loss=0.02962
Epoch 68/80: current_loss=0.02949 | best_loss=0.02949
Epoch 69/80: current_loss=0.02935 | best_loss=0.02935
Epoch 70/80: current_loss=0.02940 | best_loss=0.02935
Epoch 71/80: current_loss=0.02934 | best_loss=0.02934
Epoch 72/80: current_loss=0.02931 | best_loss=0.02931
Epoch 73/80: current_loss=0.02915 | best_loss=0.02915
Epoch 74/80: current_loss=0.02916 | best_loss=0.02915
Epoch 75/80: current_loss=0.02914 | best_loss=0.02914
Epoch 76/80: current_loss=0.02907 | best_loss=0.02907
Epoch 77/80: current_loss=0.02900 | best_loss=0.02900
Epoch 78/80: current_loss=0.02897 | best_loss=0.02897
Epoch 79/80: current_loss=0.02911 | best_loss=0.02897
      explained_var=-0.09337 | mse_loss=0.02871

----------------------------------------------
Params for Trial 59
{'learning_rate': 0.001, 'weight_decay': 3.372977579609894e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03065 | best_loss=0.03065
Epoch 1/80: current_loss=0.02722 | best_loss=0.02722
Epoch 2/80: current_loss=0.02967 | best_loss=0.02722
Epoch 3/80: current_loss=0.03146 | best_loss=0.02722
Epoch 4/80: current_loss=0.02532 | best_loss=0.02532
Epoch 5/80: current_loss=0.02878 | best_loss=0.02532
Epoch 6/80: current_loss=0.02757 | best_loss=0.02532
Epoch 7/80: current_loss=0.02587 | best_loss=0.02532
Epoch 8/80: current_loss=0.02827 | best_loss=0.02532
Epoch 9/80: current_loss=0.02845 | best_loss=0.02532
Epoch 10/80: current_loss=0.02667 | best_loss=0.02532
Epoch 11/80: current_loss=0.02886 | best_loss=0.02532
Epoch 12/80: current_loss=0.02855 | best_loss=0.02532
Epoch 13/80: current_loss=0.03900 | best_loss=0.02532
Epoch 14/80: current_loss=0.02901 | best_loss=0.02532
Epoch 15/80: current_loss=0.02618 | best_loss=0.02532
Epoch 16/80: current_loss=0.03052 | best_loss=0.02532
Epoch 17/80: current_loss=0.03492 | best_loss=0.02532
Epoch 18/80: current_loss=0.03669 | best_loss=0.02532
Epoch 19/80: current_loss=0.02722 | best_loss=0.02532
Epoch 20/80: current_loss=0.02643 | best_loss=0.02532
Epoch 21/80: current_loss=0.02814 | best_loss=0.02532
Epoch 22/80: current_loss=0.02608 | best_loss=0.02532
Epoch 23/80: current_loss=0.02912 | best_loss=0.02532
Epoch 24/80: current_loss=0.02465 | best_loss=0.02465
Epoch 25/80: current_loss=0.02445 | best_loss=0.02445
Epoch 26/80: current_loss=0.02548 | best_loss=0.02445
Epoch 27/80: current_loss=0.02864 | best_loss=0.02445
Epoch 28/80: current_loss=0.02548 | best_loss=0.02445
Epoch 29/80: current_loss=0.02605 | best_loss=0.02445
Epoch 30/80: current_loss=0.02614 | best_loss=0.02445
Epoch 31/80: current_loss=0.02573 | best_loss=0.02445
Epoch 32/80: current_loss=0.02741 | best_loss=0.02445
Epoch 33/80: current_loss=0.03200 | best_loss=0.02445
Epoch 34/80: current_loss=0.02920 | best_loss=0.02445
Epoch 35/80: current_loss=0.02666 | best_loss=0.02445
Epoch 36/80: current_loss=0.02914 | best_loss=0.02445
Epoch 37/80: current_loss=0.02864 | best_loss=0.02445
Epoch 38/80: current_loss=0.02612 | best_loss=0.02445
Epoch 39/80: current_loss=0.02616 | best_loss=0.02445
Epoch 40/80: current_loss=0.02620 | best_loss=0.02445
Epoch 41/80: current_loss=0.02682 | best_loss=0.02445
Epoch 42/80: current_loss=0.02591 | best_loss=0.02445
Epoch 43/80: current_loss=0.02489 | best_loss=0.02445
Epoch 44/80: current_loss=0.02512 | best_loss=0.02445
Epoch 45/80: current_loss=0.02503 | best_loss=0.02445
Early Stopping at epoch 45
      explained_var=0.07104 | mse_loss=0.02401
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03200 | best_loss=0.03200
Epoch 1/80: current_loss=0.02832 | best_loss=0.02832
Epoch 2/80: current_loss=0.03648 | best_loss=0.02832
Epoch 3/80: current_loss=0.03578 | best_loss=0.02832
Epoch 4/80: current_loss=0.02806 | best_loss=0.02806
Epoch 5/80: current_loss=0.02765 | best_loss=0.02765
Epoch 6/80: current_loss=0.02952 | best_loss=0.02765
Epoch 7/80: current_loss=0.04413 | best_loss=0.02765
Epoch 8/80: current_loss=0.02930 | best_loss=0.02765
Epoch 9/80: current_loss=0.02821 | best_loss=0.02765
Epoch 10/80: current_loss=0.10622 | best_loss=0.02765
Epoch 11/80: current_loss=0.13170 | best_loss=0.02765
Epoch 12/80: current_loss=0.03843 | best_loss=0.02765
Epoch 13/80: current_loss=0.04929 | best_loss=0.02765
Epoch 14/80: current_loss=0.03765 | best_loss=0.02765
Epoch 15/80: current_loss=0.03414 | best_loss=0.02765
Epoch 16/80: current_loss=0.04562 | best_loss=0.02765
Epoch 17/80: current_loss=0.03346 | best_loss=0.02765
Epoch 18/80: current_loss=0.04406 | best_loss=0.02765
Epoch 19/80: current_loss=0.04558 | best_loss=0.02765
Epoch 20/80: current_loss=0.04552 | best_loss=0.02765
Epoch 21/80: current_loss=0.05294 | best_loss=0.02765
Epoch 22/80: current_loss=0.03455 | best_loss=0.02765
Epoch 23/80: current_loss=0.03068 | best_loss=0.02765
Epoch 24/80: current_loss=0.03107 | best_loss=0.02765
Epoch 25/80: current_loss=0.03643 | best_loss=0.02765
Early Stopping at epoch 25
      explained_var=0.04738 | mse_loss=0.02696
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04669 | best_loss=0.04669
Epoch 1/80: current_loss=0.05978 | best_loss=0.04669
Epoch 2/80: current_loss=0.04197 | best_loss=0.04197
Epoch 3/80: current_loss=0.03162 | best_loss=0.03162
Epoch 4/80: current_loss=0.03118 | best_loss=0.03118
Epoch 5/80: current_loss=0.05909 | best_loss=0.03118
Epoch 6/80: current_loss=0.06123 | best_loss=0.03118
Epoch 7/80: current_loss=0.03186 | best_loss=0.03118
Epoch 8/80: current_loss=0.03526 | best_loss=0.03118
Epoch 9/80: current_loss=0.03457 | best_loss=0.03118
Epoch 10/80: current_loss=0.03365 | best_loss=0.03118
Epoch 11/80: current_loss=0.03102 | best_loss=0.03102
Epoch 12/80: current_loss=0.03235 | best_loss=0.03102
Epoch 13/80: current_loss=0.04238 | best_loss=0.03102
Epoch 14/80: current_loss=0.03406 | best_loss=0.03102
Epoch 15/80: current_loss=0.03211 | best_loss=0.03102
Epoch 16/80: current_loss=0.03792 | best_loss=0.03102
Epoch 17/80: current_loss=0.03255 | best_loss=0.03102
Epoch 18/80: current_loss=0.03256 | best_loss=0.03102
Epoch 19/80: current_loss=0.03208 | best_loss=0.03102
Epoch 20/80: current_loss=0.03924 | best_loss=0.03102
Epoch 21/80: current_loss=0.03440 | best_loss=0.03102
Epoch 22/80: current_loss=0.03666 | best_loss=0.03102
Epoch 23/80: current_loss=0.04354 | best_loss=0.03102
Epoch 24/80: current_loss=0.03357 | best_loss=0.03102
Epoch 25/80: current_loss=0.03171 | best_loss=0.03102
Epoch 26/80: current_loss=0.03801 | best_loss=0.03102
Epoch 27/80: current_loss=0.03447 | best_loss=0.03102
Epoch 28/80: current_loss=0.03205 | best_loss=0.03102
Epoch 29/80: current_loss=0.03356 | best_loss=0.03102
Epoch 30/80: current_loss=0.03064 | best_loss=0.03064
Epoch 31/80: current_loss=0.03295 | best_loss=0.03064
Epoch 32/80: current_loss=0.03765 | best_loss=0.03064
Epoch 33/80: current_loss=0.03131 | best_loss=0.03064
Epoch 34/80: current_loss=0.03476 | best_loss=0.03064
Epoch 35/80: current_loss=0.03280 | best_loss=0.03064
Epoch 36/80: current_loss=0.04801 | best_loss=0.03064
Epoch 37/80: current_loss=0.03632 | best_loss=0.03064
Epoch 38/80: current_loss=0.03799 | best_loss=0.03064
Epoch 39/80: current_loss=0.03433 | best_loss=0.03064
Epoch 40/80: current_loss=0.03314 | best_loss=0.03064
Epoch 41/80: current_loss=0.04553 | best_loss=0.03064
Epoch 42/80: current_loss=0.04162 | best_loss=0.03064
Epoch 43/80: current_loss=0.03123 | best_loss=0.03064
Epoch 44/80: current_loss=0.03515 | best_loss=0.03064
Epoch 45/80: current_loss=0.03189 | best_loss=0.03064
Epoch 46/80: current_loss=0.03082 | best_loss=0.03064
Epoch 47/80: current_loss=0.03101 | best_loss=0.03064
Epoch 48/80: current_loss=0.03099 | best_loss=0.03064
Epoch 49/80: current_loss=0.03567 | best_loss=0.03064
Epoch 50/80: current_loss=0.03810 | best_loss=0.03064
Early Stopping at epoch 50
      explained_var=-0.00148 | mse_loss=0.02986
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02832 | best_loss=0.02832
Epoch 1/80: current_loss=0.03242 | best_loss=0.02832
Epoch 2/80: current_loss=0.02851 | best_loss=0.02832
Epoch 3/80: current_loss=0.02722 | best_loss=0.02722
Epoch 4/80: current_loss=0.03004 | best_loss=0.02722
Epoch 5/80: current_loss=0.03340 | best_loss=0.02722
Epoch 6/80: current_loss=0.03028 | best_loss=0.02722
Epoch 7/80: current_loss=0.03282 | best_loss=0.02722
Epoch 8/80: current_loss=0.04059 | best_loss=0.02722
Epoch 9/80: current_loss=0.02957 | best_loss=0.02722
Epoch 10/80: current_loss=0.03090 | best_loss=0.02722
Epoch 11/80: current_loss=0.03677 | best_loss=0.02722
Epoch 12/80: current_loss=0.03769 | best_loss=0.02722
Epoch 13/80: current_loss=0.03228 | best_loss=0.02722
Epoch 14/80: current_loss=0.03393 | best_loss=0.02722
Epoch 15/80: current_loss=0.06102 | best_loss=0.02722
Epoch 16/80: current_loss=0.02864 | best_loss=0.02722
Epoch 17/80: current_loss=0.03448 | best_loss=0.02722
Epoch 18/80: current_loss=0.03222 | best_loss=0.02722
Epoch 19/80: current_loss=0.03228 | best_loss=0.02722
Epoch 20/80: current_loss=0.04331 | best_loss=0.02722
Epoch 21/80: current_loss=0.04931 | best_loss=0.02722
Epoch 22/80: current_loss=0.04030 | best_loss=0.02722
Epoch 23/80: current_loss=0.02886 | best_loss=0.02722
Early Stopping at epoch 23
      explained_var=0.03043 | mse_loss=0.02757
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03862 | best_loss=0.03862
Epoch 1/80: current_loss=0.03474 | best_loss=0.03474
Epoch 2/80: current_loss=0.03251 | best_loss=0.03251
Epoch 3/80: current_loss=0.03494 | best_loss=0.03251
Epoch 4/80: current_loss=0.03515 | best_loss=0.03251
Epoch 5/80: current_loss=0.03510 | best_loss=0.03251
Epoch 6/80: current_loss=0.03308 | best_loss=0.03251
Epoch 7/80: current_loss=0.03707 | best_loss=0.03251
Epoch 8/80: current_loss=0.03451 | best_loss=0.03251
Epoch 9/80: current_loss=0.03541 | best_loss=0.03251
Epoch 10/80: current_loss=0.03229 | best_loss=0.03229
Epoch 11/80: current_loss=0.03277 | best_loss=0.03229
Epoch 12/80: current_loss=0.03372 | best_loss=0.03229
Epoch 13/80: current_loss=0.03562 | best_loss=0.03229
Epoch 14/80: current_loss=0.03340 | best_loss=0.03229
Epoch 15/80: current_loss=0.04480 | best_loss=0.03229
Epoch 16/80: current_loss=0.03560 | best_loss=0.03229
Epoch 17/80: current_loss=0.03341 | best_loss=0.03229
Epoch 18/80: current_loss=0.03966 | best_loss=0.03229
Epoch 19/80: current_loss=0.03390 | best_loss=0.03229
Epoch 20/80: current_loss=0.04581 | best_loss=0.03229
Epoch 21/80: current_loss=0.03527 | best_loss=0.03229
Epoch 22/80: current_loss=0.03351 | best_loss=0.03229
Epoch 23/80: current_loss=0.03348 | best_loss=0.03229
Epoch 24/80: current_loss=0.03385 | best_loss=0.03229
Epoch 25/80: current_loss=0.04917 | best_loss=0.03229
Epoch 26/80: current_loss=0.04070 | best_loss=0.03229
Epoch 27/80: current_loss=0.03293 | best_loss=0.03229
Epoch 28/80: current_loss=0.04517 | best_loss=0.03229
Epoch 29/80: current_loss=0.03250 | best_loss=0.03229
Epoch 30/80: current_loss=0.04796 | best_loss=0.03229
Early Stopping at epoch 30
      explained_var=-0.00394 | mse_loss=0.03305
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.02868| avg_loss=0.02829
----------------------------------------------


----------------------------------------------
Params for Trial 60
{'learning_rate': 0.1, 'weight_decay': 0.00034392858604640354, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=215.47543 | best_loss=215.47543
Epoch 1/80: current_loss=36.66707 | best_loss=36.66707
Epoch 2/80: current_loss=6.35832 | best_loss=6.35832
Epoch 3/80: current_loss=1.94121 | best_loss=1.94121
Epoch 4/80: current_loss=0.84910 | best_loss=0.84910
Epoch 5/80: current_loss=0.78347 | best_loss=0.78347
Epoch 6/80: current_loss=0.48511 | best_loss=0.48511
Epoch 7/80: current_loss=0.54519 | best_loss=0.48511
Epoch 8/80: current_loss=0.49933 | best_loss=0.48511
Epoch 9/80: current_loss=0.64638 | best_loss=0.48511
Epoch 10/80: current_loss=0.22797 | best_loss=0.22797
Epoch 11/80: current_loss=0.57380 | best_loss=0.22797
Epoch 12/80: current_loss=1.59848 | best_loss=0.22797
Epoch 13/80: current_loss=0.08977 | best_loss=0.08977
Epoch 14/80: current_loss=0.06774 | best_loss=0.06774
Epoch 15/80: current_loss=0.24031 | best_loss=0.06774
Epoch 16/80: current_loss=0.19015 | best_loss=0.06774
Epoch 17/80: current_loss=0.15914 | best_loss=0.06774
Epoch 18/80: current_loss=0.08207 | best_loss=0.06774
Epoch 19/80: current_loss=0.04455 | best_loss=0.04455
Epoch 20/80: current_loss=0.04072 | best_loss=0.04072
Epoch 21/80: current_loss=0.11034 | best_loss=0.04072
Epoch 22/80: current_loss=0.08814 | best_loss=0.04072
Epoch 23/80: current_loss=0.06988 | best_loss=0.04072
Epoch 24/80: current_loss=0.05963 | best_loss=0.04072
Epoch 25/80: current_loss=0.04845 | best_loss=0.04072
Epoch 26/80: current_loss=0.05170 | best_loss=0.04072
Epoch 27/80: current_loss=0.10544 | best_loss=0.04072
Epoch 28/80: current_loss=0.04484 | best_loss=0.04072
Epoch 29/80: current_loss=0.05347 | best_loss=0.04072
Epoch 30/80: current_loss=0.05268 | best_loss=0.04072
Epoch 31/80: current_loss=0.06784 | best_loss=0.04072
Epoch 32/80: current_loss=0.04087 | best_loss=0.04072
Epoch 33/80: current_loss=0.06938 | best_loss=0.04072
Epoch 34/80: current_loss=0.03552 | best_loss=0.03552
Epoch 35/80: current_loss=0.07264 | best_loss=0.03552
Epoch 36/80: current_loss=0.04358 | best_loss=0.03552
Epoch 37/80: current_loss=0.11032 | best_loss=0.03552
Epoch 38/80: current_loss=0.08731 | best_loss=0.03552
Epoch 39/80: current_loss=0.04432 | best_loss=0.03552
Epoch 40/80: current_loss=0.24257 | best_loss=0.03552
Epoch 41/80: current_loss=0.13190 | best_loss=0.03552
Epoch 42/80: current_loss=0.03558 | best_loss=0.03552
Epoch 43/80: current_loss=0.04790 | best_loss=0.03552
Epoch 44/80: current_loss=0.05408 | best_loss=0.03552
Epoch 45/80: current_loss=0.20100 | best_loss=0.03552
Epoch 46/80: current_loss=0.04391 | best_loss=0.03552
Epoch 47/80: current_loss=0.08394 | best_loss=0.03552
Epoch 48/80: current_loss=0.09754 | best_loss=0.03552
Epoch 49/80: current_loss=0.07101 | best_loss=0.03552
Epoch 50/80: current_loss=0.04394 | best_loss=0.03552
Epoch 51/80: current_loss=0.24035 | best_loss=0.03552
Epoch 52/80: current_loss=0.87528 | best_loss=0.03552
Epoch 53/80: current_loss=2.61584 | best_loss=0.03552
Epoch 54/80: current_loss=0.43913 | best_loss=0.03552
Early Stopping at epoch 54
      explained_var=-0.35282 | mse_loss=0.03494

----------------------------------------------
Params for Trial 61
{'learning_rate': 0.001, 'weight_decay': 2.088822658393222e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02880 | best_loss=0.02880
Epoch 1/80: current_loss=0.03028 | best_loss=0.02880
Epoch 2/80: current_loss=0.02686 | best_loss=0.02686
Epoch 3/80: current_loss=0.03035 | best_loss=0.02686
Epoch 4/80: current_loss=0.04441 | best_loss=0.02686
Epoch 5/80: current_loss=0.03814 | best_loss=0.02686
Epoch 6/80: current_loss=0.03657 | best_loss=0.02686
Epoch 7/80: current_loss=0.03042 | best_loss=0.02686
Epoch 8/80: current_loss=0.02745 | best_loss=0.02686
Epoch 9/80: current_loss=0.02651 | best_loss=0.02651
Epoch 10/80: current_loss=0.02566 | best_loss=0.02566
Epoch 11/80: current_loss=0.02745 | best_loss=0.02566
Epoch 12/80: current_loss=0.02562 | best_loss=0.02562
Epoch 13/80: current_loss=0.02731 | best_loss=0.02562
Epoch 14/80: current_loss=0.02599 | best_loss=0.02562
Epoch 15/80: current_loss=0.02583 | best_loss=0.02562
Epoch 16/80: current_loss=0.02605 | best_loss=0.02562
Epoch 17/80: current_loss=0.02712 | best_loss=0.02562
Epoch 18/80: current_loss=0.02654 | best_loss=0.02562
Epoch 19/80: current_loss=0.02618 | best_loss=0.02562
Epoch 20/80: current_loss=0.02548 | best_loss=0.02548
Epoch 21/80: current_loss=0.02719 | best_loss=0.02548
Epoch 22/80: current_loss=0.02939 | best_loss=0.02548
Epoch 23/80: current_loss=0.02955 | best_loss=0.02548
Epoch 24/80: current_loss=0.02774 | best_loss=0.02548
Epoch 25/80: current_loss=0.02558 | best_loss=0.02548
Epoch 26/80: current_loss=0.02814 | best_loss=0.02548
Epoch 27/80: current_loss=0.03330 | best_loss=0.02548
Epoch 28/80: current_loss=0.02650 | best_loss=0.02548
Epoch 29/80: current_loss=0.02539 | best_loss=0.02539
Epoch 30/80: current_loss=0.03075 | best_loss=0.02539
Epoch 31/80: current_loss=0.02537 | best_loss=0.02537
Epoch 32/80: current_loss=0.02604 | best_loss=0.02537
Epoch 33/80: current_loss=0.03267 | best_loss=0.02537
Epoch 34/80: current_loss=0.03400 | best_loss=0.02537
Epoch 35/80: current_loss=0.02962 | best_loss=0.02537
Epoch 36/80: current_loss=0.03229 | best_loss=0.02537
Epoch 37/80: current_loss=0.02939 | best_loss=0.02537
Epoch 38/80: current_loss=0.02585 | best_loss=0.02537
Epoch 39/80: current_loss=0.03086 | best_loss=0.02537
Epoch 40/80: current_loss=0.02614 | best_loss=0.02537
Epoch 41/80: current_loss=0.02649 | best_loss=0.02537
Epoch 42/80: current_loss=0.02575 | best_loss=0.02537
Epoch 43/80: current_loss=0.02656 | best_loss=0.02537
Epoch 44/80: current_loss=0.02793 | best_loss=0.02537
Epoch 45/80: current_loss=0.03484 | best_loss=0.02537
Epoch 46/80: current_loss=0.02569 | best_loss=0.02537
Epoch 47/80: current_loss=0.02787 | best_loss=0.02537
Epoch 48/80: current_loss=0.02614 | best_loss=0.02537
Epoch 49/80: current_loss=0.02554 | best_loss=0.02537
Epoch 50/80: current_loss=0.02774 | best_loss=0.02537
Epoch 51/80: current_loss=0.02561 | best_loss=0.02537
Early Stopping at epoch 51
      explained_var=0.03893 | mse_loss=0.02487
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03046 | best_loss=0.03046
Epoch 1/80: current_loss=0.02835 | best_loss=0.02835
Epoch 2/80: current_loss=0.03120 | best_loss=0.02835
Epoch 3/80: current_loss=0.02912 | best_loss=0.02835
Epoch 4/80: current_loss=0.02848 | best_loss=0.02835
Epoch 5/80: current_loss=0.02610 | best_loss=0.02610
Epoch 6/80: current_loss=0.03144 | best_loss=0.02610
Epoch 7/80: current_loss=0.02737 | best_loss=0.02610
Epoch 8/80: current_loss=0.02824 | best_loss=0.02610
Epoch 9/80: current_loss=0.03105 | best_loss=0.02610
Epoch 10/80: current_loss=0.03897 | best_loss=0.02610
Epoch 11/80: current_loss=0.02930 | best_loss=0.02610
Epoch 12/80: current_loss=0.03132 | best_loss=0.02610
Epoch 13/80: current_loss=0.02875 | best_loss=0.02610
Epoch 14/80: current_loss=0.03056 | best_loss=0.02610
Epoch 15/80: current_loss=0.03140 | best_loss=0.02610
Epoch 16/80: current_loss=0.02861 | best_loss=0.02610
Epoch 17/80: current_loss=0.03239 | best_loss=0.02610
Epoch 18/80: current_loss=0.02705 | best_loss=0.02610
Epoch 19/80: current_loss=0.02923 | best_loss=0.02610
Epoch 20/80: current_loss=0.02968 | best_loss=0.02610
Epoch 21/80: current_loss=0.03201 | best_loss=0.02610
Epoch 22/80: current_loss=0.03179 | best_loss=0.02610
Epoch 23/80: current_loss=0.02918 | best_loss=0.02610
Epoch 24/80: current_loss=0.02768 | best_loss=0.02610
Epoch 25/80: current_loss=0.02905 | best_loss=0.02610
Early Stopping at epoch 25
      explained_var=0.07648 | mse_loss=0.02568
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03052 | best_loss=0.03052
Epoch 1/80: current_loss=0.03951 | best_loss=0.03052
Epoch 2/80: current_loss=0.03335 | best_loss=0.03052
Epoch 3/80: current_loss=0.03540 | best_loss=0.03052
Epoch 4/80: current_loss=0.03668 | best_loss=0.03052
Epoch 5/80: current_loss=0.03261 | best_loss=0.03052
Epoch 6/80: current_loss=0.07610 | best_loss=0.03052
Epoch 7/80: current_loss=0.23066 | best_loss=0.03052
Epoch 8/80: current_loss=0.10965 | best_loss=0.03052
Epoch 9/80: current_loss=0.03378 | best_loss=0.03052
Epoch 10/80: current_loss=0.03834 | best_loss=0.03052
Epoch 11/80: current_loss=0.03743 | best_loss=0.03052
Epoch 12/80: current_loss=0.03573 | best_loss=0.03052
Epoch 13/80: current_loss=0.04078 | best_loss=0.03052
Epoch 14/80: current_loss=0.03600 | best_loss=0.03052
Epoch 15/80: current_loss=0.03269 | best_loss=0.03052
Epoch 16/80: current_loss=0.04148 | best_loss=0.03052
Epoch 17/80: current_loss=0.03171 | best_loss=0.03052
Epoch 18/80: current_loss=0.03596 | best_loss=0.03052
Epoch 19/80: current_loss=0.03630 | best_loss=0.03052
Epoch 20/80: current_loss=0.04420 | best_loss=0.03052
Early Stopping at epoch 20
      explained_var=0.03752 | mse_loss=0.02971
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03879 | best_loss=0.03879
Epoch 1/80: current_loss=0.04179 | best_loss=0.03879
Epoch 2/80: current_loss=0.02885 | best_loss=0.02885
Epoch 3/80: current_loss=0.02921 | best_loss=0.02885
Epoch 4/80: current_loss=0.03583 | best_loss=0.02885
Epoch 5/80: current_loss=0.03222 | best_loss=0.02885
Epoch 6/80: current_loss=0.03161 | best_loss=0.02885
Epoch 7/80: current_loss=0.02841 | best_loss=0.02841
Epoch 8/80: current_loss=0.02847 | best_loss=0.02841
Epoch 9/80: current_loss=0.05179 | best_loss=0.02841
Epoch 10/80: current_loss=0.05473 | best_loss=0.02841
Epoch 11/80: current_loss=0.03851 | best_loss=0.02841
Epoch 12/80: current_loss=0.03300 | best_loss=0.02841
Epoch 13/80: current_loss=0.03929 | best_loss=0.02841
Epoch 14/80: current_loss=0.03004 | best_loss=0.02841
Epoch 15/80: current_loss=0.03167 | best_loss=0.02841
Epoch 16/80: current_loss=0.03736 | best_loss=0.02841
Epoch 17/80: current_loss=0.02777 | best_loss=0.02777
Epoch 18/80: current_loss=0.03815 | best_loss=0.02777
Epoch 19/80: current_loss=0.03541 | best_loss=0.02777
Epoch 20/80: current_loss=0.03313 | best_loss=0.02777
Epoch 21/80: current_loss=0.02949 | best_loss=0.02777
Epoch 22/80: current_loss=0.02960 | best_loss=0.02777
Epoch 23/80: current_loss=0.02926 | best_loss=0.02777
Epoch 24/80: current_loss=0.03573 | best_loss=0.02777
Epoch 25/80: current_loss=0.02920 | best_loss=0.02777
Epoch 26/80: current_loss=0.03681 | best_loss=0.02777
Epoch 27/80: current_loss=0.02985 | best_loss=0.02777
Epoch 28/80: current_loss=0.03752 | best_loss=0.02777
Epoch 29/80: current_loss=0.03112 | best_loss=0.02777
Epoch 30/80: current_loss=0.02947 | best_loss=0.02777
Epoch 31/80: current_loss=0.02954 | best_loss=0.02777
Epoch 32/80: current_loss=0.03046 | best_loss=0.02777
Epoch 33/80: current_loss=0.02998 | best_loss=0.02777
Epoch 34/80: current_loss=0.03653 | best_loss=0.02777
Epoch 35/80: current_loss=0.02873 | best_loss=0.02777
Epoch 36/80: current_loss=0.02840 | best_loss=0.02777
Epoch 37/80: current_loss=0.02974 | best_loss=0.02777
Early Stopping at epoch 37
      explained_var=0.01085 | mse_loss=0.02807
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03231 | best_loss=0.03231
Epoch 1/80: current_loss=0.03263 | best_loss=0.03231
Epoch 2/80: current_loss=0.03178 | best_loss=0.03178
Epoch 3/80: current_loss=0.03306 | best_loss=0.03178
Epoch 4/80: current_loss=0.03411 | best_loss=0.03178
Epoch 5/80: current_loss=0.03406 | best_loss=0.03178
Epoch 6/80: current_loss=0.03785 | best_loss=0.03178
Epoch 7/80: current_loss=0.03413 | best_loss=0.03178
Epoch 8/80: current_loss=0.03309 | best_loss=0.03178
Epoch 9/80: current_loss=0.03393 | best_loss=0.03178
Epoch 10/80: current_loss=0.03241 | best_loss=0.03178
Epoch 11/80: current_loss=0.03784 | best_loss=0.03178
Epoch 12/80: current_loss=0.03274 | best_loss=0.03178
Epoch 13/80: current_loss=0.03241 | best_loss=0.03178
Epoch 14/80: current_loss=0.03843 | best_loss=0.03178
Epoch 15/80: current_loss=0.03587 | best_loss=0.03178
Epoch 16/80: current_loss=0.03266 | best_loss=0.03178
Epoch 17/80: current_loss=0.03232 | best_loss=0.03178
Epoch 18/80: current_loss=0.03502 | best_loss=0.03178
Epoch 19/80: current_loss=0.03308 | best_loss=0.03178
Epoch 20/80: current_loss=0.03625 | best_loss=0.03178
Epoch 21/80: current_loss=0.03829 | best_loss=0.03178
Epoch 22/80: current_loss=0.03963 | best_loss=0.03178
Early Stopping at epoch 22
      explained_var=-0.00032 | mse_loss=0.03251
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.03269| avg_loss=0.02817
----------------------------------------------


----------------------------------------------
Params for Trial 62
{'learning_rate': 0.001, 'weight_decay': 0.0007563500998507909, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02949 | best_loss=0.02949
Epoch 1/80: current_loss=0.02819 | best_loss=0.02819
Epoch 2/80: current_loss=0.02943 | best_loss=0.02819
Epoch 3/80: current_loss=0.02914 | best_loss=0.02819
Epoch 4/80: current_loss=0.02619 | best_loss=0.02619
Epoch 5/80: current_loss=0.02512 | best_loss=0.02512
Epoch 6/80: current_loss=0.03060 | best_loss=0.02512
Epoch 7/80: current_loss=0.03105 | best_loss=0.02512
Epoch 8/80: current_loss=0.02798 | best_loss=0.02512
Epoch 9/80: current_loss=0.02642 | best_loss=0.02512
Epoch 10/80: current_loss=0.02542 | best_loss=0.02512
Epoch 11/80: current_loss=0.02979 | best_loss=0.02512
Epoch 12/80: current_loss=0.02719 | best_loss=0.02512
Epoch 13/80: current_loss=0.02681 | best_loss=0.02512
Epoch 14/80: current_loss=0.02716 | best_loss=0.02512
Epoch 15/80: current_loss=0.02761 | best_loss=0.02512
Epoch 16/80: current_loss=0.03014 | best_loss=0.02512
Epoch 17/80: current_loss=0.03386 | best_loss=0.02512
Epoch 18/80: current_loss=0.03984 | best_loss=0.02512
Epoch 19/80: current_loss=0.05205 | best_loss=0.02512
Epoch 20/80: current_loss=0.02855 | best_loss=0.02512
Epoch 21/80: current_loss=0.02588 | best_loss=0.02512
Epoch 22/80: current_loss=0.02609 | best_loss=0.02512
Epoch 23/80: current_loss=0.03058 | best_loss=0.02512
Epoch 24/80: current_loss=0.02680 | best_loss=0.02512
Epoch 25/80: current_loss=0.02830 | best_loss=0.02512
Early Stopping at epoch 25
      explained_var=0.04837 | mse_loss=0.02468
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03856 | best_loss=0.03856
Epoch 1/80: current_loss=0.03568 | best_loss=0.03568
Epoch 2/80: current_loss=0.02854 | best_loss=0.02854
Epoch 3/80: current_loss=0.03201 | best_loss=0.02854
Epoch 4/80: current_loss=0.02907 | best_loss=0.02854
Epoch 5/80: current_loss=0.03016 | best_loss=0.02854
Epoch 6/80: current_loss=0.02997 | best_loss=0.02854
Epoch 7/80: current_loss=0.03018 | best_loss=0.02854
Epoch 8/80: current_loss=0.03021 | best_loss=0.02854
Epoch 9/80: current_loss=0.03030 | best_loss=0.02854
Epoch 10/80: current_loss=0.02957 | best_loss=0.02854
Epoch 11/80: current_loss=0.02969 | best_loss=0.02854
Epoch 12/80: current_loss=0.02809 | best_loss=0.02809
Epoch 13/80: current_loss=0.02928 | best_loss=0.02809
Epoch 14/80: current_loss=0.03071 | best_loss=0.02809
Epoch 15/80: current_loss=0.03020 | best_loss=0.02809
Epoch 16/80: current_loss=0.02883 | best_loss=0.02809
Epoch 17/80: current_loss=0.02821 | best_loss=0.02809
Epoch 18/80: current_loss=0.02851 | best_loss=0.02809
Epoch 19/80: current_loss=0.02889 | best_loss=0.02809
Epoch 20/80: current_loss=0.02806 | best_loss=0.02806
Epoch 21/80: current_loss=0.02793 | best_loss=0.02793
Epoch 22/80: current_loss=0.02798 | best_loss=0.02793
Epoch 23/80: current_loss=0.03277 | best_loss=0.02793
Epoch 24/80: current_loss=0.03053 | best_loss=0.02793
Epoch 25/80: current_loss=0.02802 | best_loss=0.02793
Epoch 26/80: current_loss=0.02767 | best_loss=0.02767
Epoch 27/80: current_loss=0.02787 | best_loss=0.02767
Epoch 28/80: current_loss=0.02840 | best_loss=0.02767
Epoch 29/80: current_loss=0.02754 | best_loss=0.02754
Epoch 30/80: current_loss=0.02868 | best_loss=0.02754
Epoch 31/80: current_loss=0.03019 | best_loss=0.02754
Epoch 32/80: current_loss=0.02798 | best_loss=0.02754
Epoch 33/80: current_loss=0.02759 | best_loss=0.02754
Epoch 34/80: current_loss=0.03235 | best_loss=0.02754
Epoch 35/80: current_loss=0.03420 | best_loss=0.02754
Epoch 36/80: current_loss=0.03057 | best_loss=0.02754
Epoch 37/80: current_loss=0.03010 | best_loss=0.02754
Epoch 38/80: current_loss=0.02853 | best_loss=0.02754
Epoch 39/80: current_loss=0.02761 | best_loss=0.02754
Epoch 40/80: current_loss=0.02777 | best_loss=0.02754
Epoch 41/80: current_loss=0.02792 | best_loss=0.02754
Epoch 42/80: current_loss=0.02779 | best_loss=0.02754
Epoch 43/80: current_loss=0.02741 | best_loss=0.02741
Epoch 44/80: current_loss=0.02869 | best_loss=0.02741
Epoch 45/80: current_loss=0.03056 | best_loss=0.02741
Epoch 46/80: current_loss=0.03183 | best_loss=0.02741
Epoch 47/80: current_loss=0.02993 | best_loss=0.02741
Epoch 48/80: current_loss=0.02935 | best_loss=0.02741
Epoch 49/80: current_loss=0.02775 | best_loss=0.02741
Epoch 50/80: current_loss=0.02847 | best_loss=0.02741
Epoch 51/80: current_loss=0.02925 | best_loss=0.02741
Epoch 52/80: current_loss=0.02816 | best_loss=0.02741
Epoch 53/80: current_loss=0.02798 | best_loss=0.02741
Epoch 54/80: current_loss=0.02872 | best_loss=0.02741
Epoch 55/80: current_loss=0.02872 | best_loss=0.02741
Epoch 56/80: current_loss=0.02803 | best_loss=0.02741
Epoch 57/80: current_loss=0.02806 | best_loss=0.02741
Epoch 58/80: current_loss=0.03370 | best_loss=0.02741
Epoch 59/80: current_loss=0.02864 | best_loss=0.02741
Epoch 60/80: current_loss=0.02967 | best_loss=0.02741
Epoch 61/80: current_loss=0.02887 | best_loss=0.02741
Epoch 62/80: current_loss=0.02768 | best_loss=0.02741
Epoch 63/80: current_loss=0.02797 | best_loss=0.02741
Early Stopping at epoch 63
      explained_var=0.03150 | mse_loss=0.02696
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03481 | best_loss=0.03481
Epoch 1/80: current_loss=0.03979 | best_loss=0.03481
Epoch 2/80: current_loss=0.03148 | best_loss=0.03148
Epoch 3/80: current_loss=0.03227 | best_loss=0.03148
Epoch 4/80: current_loss=0.03197 | best_loss=0.03148
Epoch 5/80: current_loss=0.03092 | best_loss=0.03092
Epoch 6/80: current_loss=0.03060 | best_loss=0.03060
Epoch 7/80: current_loss=0.02986 | best_loss=0.02986
Epoch 8/80: current_loss=0.03092 | best_loss=0.02986
Epoch 9/80: current_loss=0.03331 | best_loss=0.02986
Epoch 10/80: current_loss=0.03717 | best_loss=0.02986
Epoch 11/80: current_loss=0.03142 | best_loss=0.02986
Epoch 12/80: current_loss=0.02960 | best_loss=0.02960
Epoch 13/80: current_loss=0.02949 | best_loss=0.02949
Epoch 14/80: current_loss=0.03009 | best_loss=0.02949
Epoch 15/80: current_loss=0.03157 | best_loss=0.02949
Epoch 16/80: current_loss=0.03024 | best_loss=0.02949
Epoch 17/80: current_loss=0.02973 | best_loss=0.02949
Epoch 18/80: current_loss=0.02927 | best_loss=0.02927
Epoch 19/80: current_loss=0.03005 | best_loss=0.02927
Epoch 20/80: current_loss=0.03014 | best_loss=0.02927
Epoch 21/80: current_loss=0.02935 | best_loss=0.02927
Epoch 22/80: current_loss=0.03060 | best_loss=0.02927
Epoch 23/80: current_loss=0.02959 | best_loss=0.02927
Epoch 24/80: current_loss=0.03339 | best_loss=0.02927
Epoch 25/80: current_loss=0.02984 | best_loss=0.02927
Epoch 26/80: current_loss=0.03101 | best_loss=0.02927
Epoch 27/80: current_loss=0.03126 | best_loss=0.02927
Epoch 28/80: current_loss=0.02962 | best_loss=0.02927
Epoch 29/80: current_loss=0.02943 | best_loss=0.02927
Epoch 30/80: current_loss=0.02971 | best_loss=0.02927
Epoch 31/80: current_loss=0.03009 | best_loss=0.02927
Epoch 32/80: current_loss=0.02990 | best_loss=0.02927
Epoch 33/80: current_loss=0.03065 | best_loss=0.02927
Epoch 34/80: current_loss=0.03011 | best_loss=0.02927
Epoch 35/80: current_loss=0.02961 | best_loss=0.02927
Epoch 36/80: current_loss=0.03153 | best_loss=0.02927
Epoch 37/80: current_loss=0.02994 | best_loss=0.02927
Epoch 38/80: current_loss=0.03261 | best_loss=0.02927
Early Stopping at epoch 38
      explained_var=0.03844 | mse_loss=0.02849
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02846 | best_loss=0.02846
Epoch 1/80: current_loss=0.02816 | best_loss=0.02816
Epoch 2/80: current_loss=0.02836 | best_loss=0.02816
Epoch 3/80: current_loss=0.02835 | best_loss=0.02816
Epoch 4/80: current_loss=0.02835 | best_loss=0.02816
Epoch 5/80: current_loss=0.02890 | best_loss=0.02816
Epoch 6/80: current_loss=0.02868 | best_loss=0.02816
Epoch 7/80: current_loss=0.02851 | best_loss=0.02816
Epoch 8/80: current_loss=0.02841 | best_loss=0.02816
Epoch 9/80: current_loss=0.02894 | best_loss=0.02816
Epoch 10/80: current_loss=0.02930 | best_loss=0.02816
Epoch 11/80: current_loss=0.02885 | best_loss=0.02816
Epoch 12/80: current_loss=0.02855 | best_loss=0.02816
Epoch 13/80: current_loss=0.02851 | best_loss=0.02816
Epoch 14/80: current_loss=0.02946 | best_loss=0.02816
Epoch 15/80: current_loss=0.02911 | best_loss=0.02816
Epoch 16/80: current_loss=0.02930 | best_loss=0.02816
Epoch 17/80: current_loss=0.02920 | best_loss=0.02816
Epoch 18/80: current_loss=0.02865 | best_loss=0.02816
Epoch 19/80: current_loss=0.03263 | best_loss=0.02816
Epoch 20/80: current_loss=0.02868 | best_loss=0.02816
Epoch 21/80: current_loss=0.02875 | best_loss=0.02816
Early Stopping at epoch 21
      explained_var=-0.00473 | mse_loss=0.02851
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03684 | best_loss=0.03684
Epoch 1/80: current_loss=0.03500 | best_loss=0.03500
Epoch 2/80: current_loss=0.03536 | best_loss=0.03500
Epoch 3/80: current_loss=0.03359 | best_loss=0.03359
Epoch 4/80: current_loss=0.03437 | best_loss=0.03359
Epoch 5/80: current_loss=0.03435 | best_loss=0.03359
Epoch 6/80: current_loss=0.03530 | best_loss=0.03359
Epoch 7/80: current_loss=0.03410 | best_loss=0.03359
Epoch 8/80: current_loss=0.03505 | best_loss=0.03359
Epoch 9/80: current_loss=0.03395 | best_loss=0.03359
Epoch 10/80: current_loss=0.03537 | best_loss=0.03359
Epoch 11/80: current_loss=0.03383 | best_loss=0.03359
Epoch 12/80: current_loss=0.03297 | best_loss=0.03297
Epoch 13/80: current_loss=0.03426 | best_loss=0.03297
Epoch 14/80: current_loss=0.03368 | best_loss=0.03297
Epoch 15/80: current_loss=0.03258 | best_loss=0.03258
Epoch 16/80: current_loss=0.03634 | best_loss=0.03258
Epoch 17/80: current_loss=0.03450 | best_loss=0.03258
Epoch 18/80: current_loss=0.03455 | best_loss=0.03258
Epoch 19/80: current_loss=0.03408 | best_loss=0.03258
Epoch 20/80: current_loss=0.03382 | best_loss=0.03258
Epoch 21/80: current_loss=0.03497 | best_loss=0.03258
Epoch 22/80: current_loss=0.03488 | best_loss=0.03258
Epoch 23/80: current_loss=0.03222 | best_loss=0.03222
Epoch 24/80: current_loss=0.03327 | best_loss=0.03222
Epoch 25/80: current_loss=0.03382 | best_loss=0.03222
Epoch 26/80: current_loss=0.03425 | best_loss=0.03222
Epoch 27/80: current_loss=0.03548 | best_loss=0.03222
Epoch 28/80: current_loss=0.03389 | best_loss=0.03222
Epoch 29/80: current_loss=0.03482 | best_loss=0.03222
Epoch 30/80: current_loss=0.03386 | best_loss=0.03222
Epoch 31/80: current_loss=0.03552 | best_loss=0.03222
Epoch 32/80: current_loss=0.03420 | best_loss=0.03222
Epoch 33/80: current_loss=0.03427 | best_loss=0.03222
Epoch 34/80: current_loss=0.04778 | best_loss=0.03222
Epoch 35/80: current_loss=0.16647 | best_loss=0.03222
Epoch 36/80: current_loss=0.12605 | best_loss=0.03222
Epoch 37/80: current_loss=0.08176 | best_loss=0.03222
Epoch 38/80: current_loss=0.05471 | best_loss=0.03222
Epoch 39/80: current_loss=0.04420 | best_loss=0.03222
Epoch 40/80: current_loss=0.04025 | best_loss=0.03222
Epoch 41/80: current_loss=0.03374 | best_loss=0.03222
Epoch 42/80: current_loss=0.06305 | best_loss=0.03222
Epoch 43/80: current_loss=0.04181 | best_loss=0.03222
Early Stopping at epoch 43
      explained_var=-0.00908 | mse_loss=0.03288
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.02090| avg_loss=0.02831
----------------------------------------------


----------------------------------------------
Params for Trial 63
{'learning_rate': 0.001, 'weight_decay': 6.845269894026622e-06, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02935 | best_loss=0.02935
Epoch 1/80: current_loss=0.02838 | best_loss=0.02838
Epoch 2/80: current_loss=0.03495 | best_loss=0.02838
Epoch 3/80: current_loss=0.02829 | best_loss=0.02829
Epoch 4/80: current_loss=0.02818 | best_loss=0.02818
Epoch 5/80: current_loss=0.02664 | best_loss=0.02664
Epoch 6/80: current_loss=0.02763 | best_loss=0.02664
Epoch 7/80: current_loss=0.02967 | best_loss=0.02664
Epoch 8/80: current_loss=0.02709 | best_loss=0.02664
Epoch 9/80: current_loss=0.05279 | best_loss=0.02664
Epoch 10/80: current_loss=0.02742 | best_loss=0.02664
Epoch 11/80: current_loss=0.02714 | best_loss=0.02664
Epoch 12/80: current_loss=0.02797 | best_loss=0.02664
Epoch 13/80: current_loss=0.02715 | best_loss=0.02664
Epoch 14/80: current_loss=0.02639 | best_loss=0.02639
Epoch 15/80: current_loss=0.03408 | best_loss=0.02639
Epoch 16/80: current_loss=0.02570 | best_loss=0.02570
Epoch 17/80: current_loss=0.02693 | best_loss=0.02570
Epoch 18/80: current_loss=0.03411 | best_loss=0.02570
Epoch 19/80: current_loss=0.03071 | best_loss=0.02570
Epoch 20/80: current_loss=0.02903 | best_loss=0.02570
Epoch 21/80: current_loss=0.03329 | best_loss=0.02570
Epoch 22/80: current_loss=0.03867 | best_loss=0.02570
Epoch 23/80: current_loss=0.03121 | best_loss=0.02570
Epoch 24/80: current_loss=0.02598 | best_loss=0.02570
Epoch 25/80: current_loss=0.02653 | best_loss=0.02570
Epoch 26/80: current_loss=0.02853 | best_loss=0.02570
Epoch 27/80: current_loss=0.02651 | best_loss=0.02570
Epoch 28/80: current_loss=0.02581 | best_loss=0.02570
Epoch 29/80: current_loss=0.02535 | best_loss=0.02535
Epoch 30/80: current_loss=0.02728 | best_loss=0.02535
Epoch 31/80: current_loss=0.03110 | best_loss=0.02535
Epoch 32/80: current_loss=0.02864 | best_loss=0.02535
Epoch 33/80: current_loss=0.02775 | best_loss=0.02535
Epoch 34/80: current_loss=0.02581 | best_loss=0.02535
Epoch 35/80: current_loss=0.02699 | best_loss=0.02535
Epoch 36/80: current_loss=0.02751 | best_loss=0.02535
Epoch 37/80: current_loss=0.02656 | best_loss=0.02535
Epoch 38/80: current_loss=0.02594 | best_loss=0.02535
Epoch 39/80: current_loss=0.02657 | best_loss=0.02535
Epoch 40/80: current_loss=0.02566 | best_loss=0.02535
Epoch 41/80: current_loss=0.03088 | best_loss=0.02535
Epoch 42/80: current_loss=0.02930 | best_loss=0.02535
Epoch 43/80: current_loss=0.02985 | best_loss=0.02535
Epoch 44/80: current_loss=0.02606 | best_loss=0.02535
Epoch 45/80: current_loss=0.02656 | best_loss=0.02535
Epoch 46/80: current_loss=0.02631 | best_loss=0.02535
Epoch 47/80: current_loss=0.02601 | best_loss=0.02535
Epoch 48/80: current_loss=0.02725 | best_loss=0.02535
Epoch 49/80: current_loss=0.02742 | best_loss=0.02535
Early Stopping at epoch 49
      explained_var=0.04947 | mse_loss=0.02494
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03113 | best_loss=0.03113
Epoch 1/80: current_loss=0.03356 | best_loss=0.03113
Epoch 2/80: current_loss=0.03056 | best_loss=0.03056
Epoch 3/80: current_loss=0.02786 | best_loss=0.02786
Epoch 4/80: current_loss=0.03088 | best_loss=0.02786
Epoch 5/80: current_loss=0.02999 | best_loss=0.02786
Epoch 6/80: current_loss=0.02890 | best_loss=0.02786
Epoch 7/80: current_loss=0.03045 | best_loss=0.02786
Epoch 8/80: current_loss=0.02753 | best_loss=0.02753
Epoch 9/80: current_loss=0.02894 | best_loss=0.02753
Epoch 10/80: current_loss=0.03140 | best_loss=0.02753
Epoch 11/80: current_loss=0.02975 | best_loss=0.02753
Epoch 12/80: current_loss=0.03242 | best_loss=0.02753
Epoch 13/80: current_loss=0.05199 | best_loss=0.02753
Epoch 14/80: current_loss=0.15893 | best_loss=0.02753
Epoch 15/80: current_loss=0.20461 | best_loss=0.02753
Epoch 16/80: current_loss=0.17116 | best_loss=0.02753
Epoch 17/80: current_loss=0.05289 | best_loss=0.02753
Epoch 18/80: current_loss=0.02807 | best_loss=0.02753
Epoch 19/80: current_loss=0.03512 | best_loss=0.02753
Epoch 20/80: current_loss=0.03324 | best_loss=0.02753
Epoch 21/80: current_loss=0.03127 | best_loss=0.02753
Epoch 22/80: current_loss=0.03046 | best_loss=0.02753
Epoch 23/80: current_loss=0.05928 | best_loss=0.02753
Epoch 24/80: current_loss=0.03599 | best_loss=0.02753
Epoch 25/80: current_loss=0.03161 | best_loss=0.02753
Epoch 26/80: current_loss=0.03530 | best_loss=0.02753
Epoch 27/80: current_loss=0.03131 | best_loss=0.02753
Epoch 28/80: current_loss=0.03145 | best_loss=0.02753
Early Stopping at epoch 28
      explained_var=0.02691 | mse_loss=0.02704
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.05292 | best_loss=0.05292
Epoch 1/80: current_loss=0.03638 | best_loss=0.03638
Epoch 2/80: current_loss=0.03298 | best_loss=0.03298
Epoch 3/80: current_loss=0.03265 | best_loss=0.03265
Epoch 4/80: current_loss=0.04063 | best_loss=0.03265
Epoch 5/80: current_loss=0.03544 | best_loss=0.03265
Epoch 6/80: current_loss=0.03169 | best_loss=0.03169
Epoch 7/80: current_loss=0.03250 | best_loss=0.03169
Epoch 8/80: current_loss=0.03265 | best_loss=0.03169
Epoch 9/80: current_loss=0.04787 | best_loss=0.03169
Epoch 10/80: current_loss=0.03931 | best_loss=0.03169
Epoch 11/80: current_loss=0.03230 | best_loss=0.03169
Epoch 12/80: current_loss=0.03213 | best_loss=0.03169
Epoch 13/80: current_loss=0.03316 | best_loss=0.03169
Epoch 14/80: current_loss=0.03220 | best_loss=0.03169
Epoch 15/80: current_loss=0.03976 | best_loss=0.03169
Epoch 16/80: current_loss=0.03242 | best_loss=0.03169
Epoch 17/80: current_loss=0.03151 | best_loss=0.03151
Epoch 18/80: current_loss=0.03109 | best_loss=0.03109
Epoch 19/80: current_loss=0.04296 | best_loss=0.03109
Epoch 20/80: current_loss=0.04485 | best_loss=0.03109
Epoch 21/80: current_loss=0.05355 | best_loss=0.03109
Epoch 22/80: current_loss=0.03103 | best_loss=0.03103
Epoch 23/80: current_loss=0.03416 | best_loss=0.03103
Epoch 24/80: current_loss=0.05272 | best_loss=0.03103
Epoch 25/80: current_loss=0.03166 | best_loss=0.03103
Epoch 26/80: current_loss=0.03218 | best_loss=0.03103
Epoch 27/80: current_loss=0.03394 | best_loss=0.03103
Epoch 28/80: current_loss=0.03208 | best_loss=0.03103
Epoch 29/80: current_loss=0.03744 | best_loss=0.03103
Epoch 30/80: current_loss=0.03131 | best_loss=0.03103
Epoch 31/80: current_loss=0.04199 | best_loss=0.03103
Epoch 32/80: current_loss=0.03241 | best_loss=0.03103
Epoch 33/80: current_loss=0.03202 | best_loss=0.03103
Epoch 34/80: current_loss=0.03126 | best_loss=0.03103
Epoch 35/80: current_loss=0.03131 | best_loss=0.03103
Epoch 36/80: current_loss=0.03961 | best_loss=0.03103
Epoch 37/80: current_loss=0.03064 | best_loss=0.03064
Epoch 38/80: current_loss=0.05132 | best_loss=0.03064
Epoch 39/80: current_loss=0.03374 | best_loss=0.03064
Epoch 40/80: current_loss=0.03150 | best_loss=0.03064
Epoch 41/80: current_loss=0.03078 | best_loss=0.03064
Epoch 42/80: current_loss=0.03753 | best_loss=0.03064
Epoch 43/80: current_loss=0.03247 | best_loss=0.03064
Epoch 44/80: current_loss=0.03072 | best_loss=0.03064
Epoch 45/80: current_loss=0.03437 | best_loss=0.03064
Epoch 46/80: current_loss=0.03054 | best_loss=0.03054
Epoch 47/80: current_loss=0.03048 | best_loss=0.03048
Epoch 48/80: current_loss=0.03255 | best_loss=0.03048
Epoch 49/80: current_loss=0.03043 | best_loss=0.03043
Epoch 50/80: current_loss=0.04212 | best_loss=0.03043
Epoch 51/80: current_loss=0.04311 | best_loss=0.03043
Epoch 52/80: current_loss=0.03481 | best_loss=0.03043
Epoch 53/80: current_loss=0.04296 | best_loss=0.03043
Epoch 54/80: current_loss=0.05462 | best_loss=0.03043
Epoch 55/80: current_loss=0.03092 | best_loss=0.03043
Epoch 56/80: current_loss=0.03425 | best_loss=0.03043
Epoch 57/80: current_loss=0.06593 | best_loss=0.03043
Epoch 58/80: current_loss=0.04832 | best_loss=0.03043
Epoch 59/80: current_loss=0.06306 | best_loss=0.03043
Epoch 60/80: current_loss=0.03538 | best_loss=0.03043
Epoch 61/80: current_loss=0.04838 | best_loss=0.03043
Epoch 62/80: current_loss=0.04990 | best_loss=0.03043
Epoch 63/80: current_loss=0.06382 | best_loss=0.03043
Epoch 64/80: current_loss=0.04384 | best_loss=0.03043
Epoch 65/80: current_loss=0.05013 | best_loss=0.03043
Epoch 66/80: current_loss=0.03409 | best_loss=0.03043
Epoch 67/80: current_loss=0.03091 | best_loss=0.03043
Epoch 68/80: current_loss=0.03561 | best_loss=0.03043
Epoch 69/80: current_loss=0.03942 | best_loss=0.03043
Early Stopping at epoch 69
      explained_var=0.00268 | mse_loss=0.02967
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04592 | best_loss=0.04592
Epoch 1/80: current_loss=0.13237 | best_loss=0.04592
Epoch 2/80: current_loss=0.05640 | best_loss=0.04592
Epoch 3/80: current_loss=0.02813 | best_loss=0.02813
Epoch 4/80: current_loss=0.03016 | best_loss=0.02813
Epoch 5/80: current_loss=0.03150 | best_loss=0.02813
Epoch 6/80: current_loss=0.02976 | best_loss=0.02813
Epoch 7/80: current_loss=0.03558 | best_loss=0.02813
Epoch 8/80: current_loss=0.03965 | best_loss=0.02813
Epoch 9/80: current_loss=0.03069 | best_loss=0.02813
Epoch 10/80: current_loss=0.03292 | best_loss=0.02813
Epoch 11/80: current_loss=0.03598 | best_loss=0.02813
Epoch 12/80: current_loss=0.03106 | best_loss=0.02813
Epoch 13/80: current_loss=0.02901 | best_loss=0.02813
Epoch 14/80: current_loss=0.02924 | best_loss=0.02813
Epoch 15/80: current_loss=0.03204 | best_loss=0.02813
Epoch 16/80: current_loss=0.02898 | best_loss=0.02813
Epoch 17/80: current_loss=0.02869 | best_loss=0.02813
Epoch 18/80: current_loss=0.02781 | best_loss=0.02781
Epoch 19/80: current_loss=0.03287 | best_loss=0.02781
Epoch 20/80: current_loss=0.03509 | best_loss=0.02781
Epoch 21/80: current_loss=0.02861 | best_loss=0.02781
Epoch 22/80: current_loss=0.03492 | best_loss=0.02781
Epoch 23/80: current_loss=0.03608 | best_loss=0.02781
Epoch 24/80: current_loss=0.03167 | best_loss=0.02781
Epoch 25/80: current_loss=0.02842 | best_loss=0.02781
Epoch 26/80: current_loss=0.03265 | best_loss=0.02781
Epoch 27/80: current_loss=0.05659 | best_loss=0.02781
Epoch 28/80: current_loss=0.03601 | best_loss=0.02781
Epoch 29/80: current_loss=0.02933 | best_loss=0.02781
Epoch 30/80: current_loss=0.02808 | best_loss=0.02781
Epoch 31/80: current_loss=0.02942 | best_loss=0.02781
Epoch 32/80: current_loss=0.02828 | best_loss=0.02781
Epoch 33/80: current_loss=0.02907 | best_loss=0.02781
Epoch 34/80: current_loss=0.02815 | best_loss=0.02781
Epoch 35/80: current_loss=0.03277 | best_loss=0.02781
Epoch 36/80: current_loss=0.02864 | best_loss=0.02781
Epoch 37/80: current_loss=0.03684 | best_loss=0.02781
Epoch 38/80: current_loss=0.03228 | best_loss=0.02781
Early Stopping at epoch 38
      explained_var=0.00850 | mse_loss=0.02812
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.16372 | best_loss=0.16372
Epoch 1/80: current_loss=0.03247 | best_loss=0.03247
Epoch 2/80: current_loss=0.03409 | best_loss=0.03247
Epoch 3/80: current_loss=0.03487 | best_loss=0.03247
Epoch 4/80: current_loss=0.03497 | best_loss=0.03247
Epoch 5/80: current_loss=0.03201 | best_loss=0.03201
Epoch 6/80: current_loss=0.03240 | best_loss=0.03201
Epoch 7/80: current_loss=0.03600 | best_loss=0.03201
Epoch 8/80: current_loss=0.03118 | best_loss=0.03118
Epoch 9/80: current_loss=0.03885 | best_loss=0.03118
Epoch 10/80: current_loss=0.03380 | best_loss=0.03118
Epoch 11/80: current_loss=0.03135 | best_loss=0.03118
Epoch 12/80: current_loss=0.03368 | best_loss=0.03118
Epoch 13/80: current_loss=0.03417 | best_loss=0.03118
Epoch 14/80: current_loss=0.03556 | best_loss=0.03118
Epoch 15/80: current_loss=0.03338 | best_loss=0.03118
Epoch 16/80: current_loss=0.03254 | best_loss=0.03118
Epoch 17/80: current_loss=0.03485 | best_loss=0.03118
Epoch 18/80: current_loss=0.04067 | best_loss=0.03118
Epoch 19/80: current_loss=0.03700 | best_loss=0.03118
Epoch 20/80: current_loss=0.04167 | best_loss=0.03118
Epoch 21/80: current_loss=0.03778 | best_loss=0.03118
Epoch 22/80: current_loss=0.03684 | best_loss=0.03118
Epoch 23/80: current_loss=0.03370 | best_loss=0.03118
Epoch 24/80: current_loss=0.03305 | best_loss=0.03118
Epoch 25/80: current_loss=0.03572 | best_loss=0.03118
Epoch 26/80: current_loss=0.04613 | best_loss=0.03118
Epoch 27/80: current_loss=0.03618 | best_loss=0.03118
Epoch 28/80: current_loss=0.03704 | best_loss=0.03118
Early Stopping at epoch 28
      explained_var=0.02560 | mse_loss=0.03178
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=0.02263| avg_loss=0.02831
----------------------------------------------


----------------------------------------------
Params for Trial 64
{'learning_rate': 0.001, 'weight_decay': 0.0013421585019515697, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02938 | best_loss=0.02938
Epoch 1/80: current_loss=0.02684 | best_loss=0.02684
Epoch 2/80: current_loss=0.03425 | best_loss=0.02684
Epoch 3/80: current_loss=0.03033 | best_loss=0.02684
Epoch 4/80: current_loss=0.02919 | best_loss=0.02684
Epoch 5/80: current_loss=0.02524 | best_loss=0.02524
Epoch 6/80: current_loss=0.02807 | best_loss=0.02524
Epoch 7/80: current_loss=0.02660 | best_loss=0.02524
Epoch 8/80: current_loss=0.02983 | best_loss=0.02524
Epoch 9/80: current_loss=0.02609 | best_loss=0.02524
Epoch 10/80: current_loss=0.02662 | best_loss=0.02524
Epoch 11/80: current_loss=0.02535 | best_loss=0.02524
Epoch 12/80: current_loss=0.02630 | best_loss=0.02524
Epoch 13/80: current_loss=0.02708 | best_loss=0.02524
Epoch 14/80: current_loss=0.05632 | best_loss=0.02524
Epoch 15/80: current_loss=0.02740 | best_loss=0.02524
Epoch 16/80: current_loss=0.02571 | best_loss=0.02524
Epoch 17/80: current_loss=0.02738 | best_loss=0.02524
Epoch 18/80: current_loss=0.02554 | best_loss=0.02524
Epoch 19/80: current_loss=0.02689 | best_loss=0.02524
Epoch 20/80: current_loss=0.02648 | best_loss=0.02524
Epoch 21/80: current_loss=0.03027 | best_loss=0.02524
Epoch 22/80: current_loss=0.02603 | best_loss=0.02524
Epoch 23/80: current_loss=0.02567 | best_loss=0.02524
Epoch 24/80: current_loss=0.02972 | best_loss=0.02524
Epoch 25/80: current_loss=0.02658 | best_loss=0.02524
Early Stopping at epoch 25
      explained_var=0.05741 | mse_loss=0.02484
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=1.59294 | best_loss=1.59294
Epoch 1/80: current_loss=0.03745 | best_loss=0.03745
Epoch 2/80: current_loss=0.03072 | best_loss=0.03072
Epoch 3/80: current_loss=0.03152 | best_loss=0.03072
Epoch 4/80: current_loss=0.03033 | best_loss=0.03033
Epoch 5/80: current_loss=0.02926 | best_loss=0.02926
Epoch 6/80: current_loss=0.02837 | best_loss=0.02837
Epoch 7/80: current_loss=0.02885 | best_loss=0.02837
Epoch 8/80: current_loss=0.02818 | best_loss=0.02818
Epoch 9/80: current_loss=0.02770 | best_loss=0.02770
Epoch 10/80: current_loss=0.02774 | best_loss=0.02770
Epoch 11/80: current_loss=0.02782 | best_loss=0.02770
Epoch 12/80: current_loss=0.03034 | best_loss=0.02770
Epoch 13/80: current_loss=0.02904 | best_loss=0.02770
Epoch 14/80: current_loss=0.02783 | best_loss=0.02770
Epoch 15/80: current_loss=0.02776 | best_loss=0.02770
Epoch 16/80: current_loss=0.02770 | best_loss=0.02770
Epoch 17/80: current_loss=0.02879 | best_loss=0.02770
Epoch 18/80: current_loss=0.02844 | best_loss=0.02770
Epoch 19/80: current_loss=0.02795 | best_loss=0.02770
Epoch 20/80: current_loss=0.02923 | best_loss=0.02770
Epoch 21/80: current_loss=0.02898 | best_loss=0.02770
Epoch 22/80: current_loss=0.02795 | best_loss=0.02770
Epoch 23/80: current_loss=0.02877 | best_loss=0.02770
Epoch 24/80: current_loss=0.02792 | best_loss=0.02770
Epoch 25/80: current_loss=0.02794 | best_loss=0.02770
Epoch 26/80: current_loss=0.02817 | best_loss=0.02770
Epoch 27/80: current_loss=0.02802 | best_loss=0.02770
Epoch 28/80: current_loss=0.03159 | best_loss=0.02770
Epoch 29/80: current_loss=0.02790 | best_loss=0.02770
Early Stopping at epoch 29
      explained_var=0.01412 | mse_loss=0.02740
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04407 | best_loss=0.04407
Epoch 1/80: current_loss=0.03916 | best_loss=0.03916
Epoch 2/80: current_loss=0.03177 | best_loss=0.03177
Epoch 3/80: current_loss=0.03000 | best_loss=0.03000
Epoch 4/80: current_loss=0.03007 | best_loss=0.03000
Epoch 5/80: current_loss=0.03229 | best_loss=0.03000
Epoch 6/80: current_loss=0.03070 | best_loss=0.03000
Epoch 7/80: current_loss=0.03087 | best_loss=0.03000
Epoch 8/80: current_loss=0.03050 | best_loss=0.03000
Epoch 9/80: current_loss=0.03382 | best_loss=0.03000
Epoch 10/80: current_loss=0.03067 | best_loss=0.03000
Epoch 11/80: current_loss=0.03281 | best_loss=0.03000
Epoch 12/80: current_loss=0.04371 | best_loss=0.03000
Epoch 13/80: current_loss=0.02987 | best_loss=0.02987
Epoch 14/80: current_loss=0.03014 | best_loss=0.02987
Epoch 15/80: current_loss=0.02961 | best_loss=0.02961
Epoch 16/80: current_loss=0.03250 | best_loss=0.02961
Epoch 17/80: current_loss=0.03009 | best_loss=0.02961
Epoch 18/80: current_loss=0.03309 | best_loss=0.02961
Epoch 19/80: current_loss=0.03002 | best_loss=0.02961
Epoch 20/80: current_loss=0.03323 | best_loss=0.02961
Epoch 21/80: current_loss=0.03175 | best_loss=0.02961
Epoch 22/80: current_loss=0.03816 | best_loss=0.02961
Epoch 23/80: current_loss=0.03422 | best_loss=0.02961
Epoch 24/80: current_loss=0.03700 | best_loss=0.02961
Epoch 25/80: current_loss=0.03035 | best_loss=0.02961
Epoch 26/80: current_loss=0.02965 | best_loss=0.02961
Epoch 27/80: current_loss=0.02960 | best_loss=0.02960
Epoch 28/80: current_loss=0.02966 | best_loss=0.02960
Epoch 29/80: current_loss=0.02954 | best_loss=0.02954
Epoch 30/80: current_loss=0.03109 | best_loss=0.02954
Epoch 31/80: current_loss=0.03095 | best_loss=0.02954
Epoch 32/80: current_loss=0.03029 | best_loss=0.02954
Epoch 33/80: current_loss=0.03083 | best_loss=0.02954
Epoch 34/80: current_loss=0.02990 | best_loss=0.02954
Epoch 35/80: current_loss=0.03118 | best_loss=0.02954
Epoch 36/80: current_loss=0.03022 | best_loss=0.02954
Epoch 37/80: current_loss=0.02978 | best_loss=0.02954
Epoch 38/80: current_loss=0.02989 | best_loss=0.02954
Epoch 39/80: current_loss=0.02983 | best_loss=0.02954
Epoch 40/80: current_loss=0.03049 | best_loss=0.02954
Epoch 41/80: current_loss=0.03005 | best_loss=0.02954
Epoch 42/80: current_loss=0.03409 | best_loss=0.02954
Epoch 43/80: current_loss=0.03180 | best_loss=0.02954
Epoch 44/80: current_loss=0.03451 | best_loss=0.02954
Epoch 45/80: current_loss=0.03203 | best_loss=0.02954
Epoch 46/80: current_loss=0.03150 | best_loss=0.02954
Epoch 47/80: current_loss=0.02983 | best_loss=0.02954
Epoch 48/80: current_loss=0.02969 | best_loss=0.02954
Epoch 49/80: current_loss=0.03026 | best_loss=0.02954
Early Stopping at epoch 49
      explained_var=0.02844 | mse_loss=0.02875
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02877 | best_loss=0.02877
Epoch 1/80: current_loss=0.02886 | best_loss=0.02877
Epoch 2/80: current_loss=0.03265 | best_loss=0.02877
Epoch 3/80: current_loss=0.02978 | best_loss=0.02877
Epoch 4/80: current_loss=0.02828 | best_loss=0.02828
Epoch 5/80: current_loss=0.02835 | best_loss=0.02828
Epoch 6/80: current_loss=0.02823 | best_loss=0.02823
Epoch 7/80: current_loss=0.02898 | best_loss=0.02823
Epoch 8/80: current_loss=0.02862 | best_loss=0.02823
Epoch 9/80: current_loss=0.02840 | best_loss=0.02823
Epoch 10/80: current_loss=0.02938 | best_loss=0.02823
Epoch 11/80: current_loss=0.02862 | best_loss=0.02823
Epoch 12/80: current_loss=0.02814 | best_loss=0.02814
Epoch 13/80: current_loss=0.02764 | best_loss=0.02764
Epoch 14/80: current_loss=0.02863 | best_loss=0.02764
Epoch 15/80: current_loss=0.02830 | best_loss=0.02764
Epoch 16/80: current_loss=0.02822 | best_loss=0.02764
Epoch 17/80: current_loss=0.02861 | best_loss=0.02764
Epoch 18/80: current_loss=0.02817 | best_loss=0.02764
Epoch 19/80: current_loss=0.02805 | best_loss=0.02764
Epoch 20/80: current_loss=0.02839 | best_loss=0.02764
Epoch 21/80: current_loss=0.03041 | best_loss=0.02764
Epoch 22/80: current_loss=0.02895 | best_loss=0.02764
Epoch 23/80: current_loss=0.02884 | best_loss=0.02764
Epoch 24/80: current_loss=0.02862 | best_loss=0.02764
Epoch 25/80: current_loss=0.02842 | best_loss=0.02764
Epoch 26/80: current_loss=0.02853 | best_loss=0.02764
Epoch 27/80: current_loss=0.02908 | best_loss=0.02764
Epoch 28/80: current_loss=0.02833 | best_loss=0.02764
Epoch 29/80: current_loss=0.02846 | best_loss=0.02764
Epoch 30/80: current_loss=0.02906 | best_loss=0.02764
Epoch 31/80: current_loss=0.02945 | best_loss=0.02764
Epoch 32/80: current_loss=0.02943 | best_loss=0.02764
Epoch 33/80: current_loss=0.02870 | best_loss=0.02764
Early Stopping at epoch 33
      explained_var=0.01620 | mse_loss=0.02798
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03451 | best_loss=0.03451
Epoch 1/80: current_loss=0.03424 | best_loss=0.03424
Epoch 2/80: current_loss=0.03395 | best_loss=0.03395
Epoch 3/80: current_loss=0.03462 | best_loss=0.03395
Epoch 4/80: current_loss=0.03461 | best_loss=0.03395
Epoch 5/80: current_loss=0.03474 | best_loss=0.03395
Epoch 6/80: current_loss=0.03417 | best_loss=0.03395
Epoch 7/80: current_loss=0.03413 | best_loss=0.03395
Epoch 8/80: current_loss=0.03468 | best_loss=0.03395
Epoch 9/80: current_loss=0.03545 | best_loss=0.03395
Epoch 10/80: current_loss=0.03356 | best_loss=0.03356
Epoch 11/80: current_loss=0.03566 | best_loss=0.03356
Epoch 12/80: current_loss=0.03401 | best_loss=0.03356
Epoch 13/80: current_loss=0.03285 | best_loss=0.03285
Epoch 14/80: current_loss=0.03433 | best_loss=0.03285
Epoch 15/80: current_loss=0.03521 | best_loss=0.03285
Epoch 16/80: current_loss=0.03372 | best_loss=0.03285
Epoch 17/80: current_loss=0.03395 | best_loss=0.03285
Epoch 18/80: current_loss=0.03400 | best_loss=0.03285
Epoch 19/80: current_loss=0.03349 | best_loss=0.03285
Epoch 20/80: current_loss=0.03411 | best_loss=0.03285
Epoch 21/80: current_loss=0.03386 | best_loss=0.03285
Epoch 22/80: current_loss=0.03322 | best_loss=0.03285
Epoch 23/80: current_loss=0.03428 | best_loss=0.03285
Epoch 24/80: current_loss=0.03405 | best_loss=0.03285
Epoch 25/80: current_loss=0.03446 | best_loss=0.03285
Epoch 26/80: current_loss=0.03369 | best_loss=0.03285
Epoch 27/80: current_loss=0.03439 | best_loss=0.03285
Epoch 28/80: current_loss=0.03466 | best_loss=0.03285
Epoch 29/80: current_loss=0.03316 | best_loss=0.03285
Epoch 30/80: current_loss=0.03385 | best_loss=0.03285
Epoch 31/80: current_loss=0.05976 | best_loss=0.03285
Epoch 32/80: current_loss=0.06795 | best_loss=0.03285
Epoch 33/80: current_loss=0.10698 | best_loss=0.03285
Early Stopping at epoch 33
      explained_var=-0.03534 | mse_loss=0.03364
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.01617| avg_loss=0.02852
----------------------------------------------


----------------------------------------------
Params for Trial 65
{'learning_rate': 0.001, 'weight_decay': 0.0003253300984183563, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03351 | best_loss=0.03351
Epoch 1/80: current_loss=0.03011 | best_loss=0.03011
Epoch 2/80: current_loss=0.03678 | best_loss=0.03011
Epoch 3/80: current_loss=0.03394 | best_loss=0.03011
Epoch 4/80: current_loss=0.03014 | best_loss=0.03011
Epoch 5/80: current_loss=0.02677 | best_loss=0.02677
Epoch 6/80: current_loss=0.04173 | best_loss=0.02677
Epoch 7/80: current_loss=0.03374 | best_loss=0.02677
Epoch 8/80: current_loss=0.03573 | best_loss=0.02677
Epoch 9/80: current_loss=0.02880 | best_loss=0.02677
Epoch 10/80: current_loss=0.02591 | best_loss=0.02591
Epoch 11/80: current_loss=0.02671 | best_loss=0.02591
Epoch 12/80: current_loss=0.02837 | best_loss=0.02591
Epoch 13/80: current_loss=0.02701 | best_loss=0.02591
Epoch 14/80: current_loss=0.02511 | best_loss=0.02511
Epoch 15/80: current_loss=0.02521 | best_loss=0.02511
Epoch 16/80: current_loss=0.03351 | best_loss=0.02511
Epoch 17/80: current_loss=0.03666 | best_loss=0.02511
Epoch 18/80: current_loss=0.02952 | best_loss=0.02511
Epoch 19/80: current_loss=0.02609 | best_loss=0.02511
Epoch 20/80: current_loss=0.02634 | best_loss=0.02511
Epoch 21/80: current_loss=0.03058 | best_loss=0.02511
Epoch 22/80: current_loss=0.03864 | best_loss=0.02511
Epoch 23/80: current_loss=0.03242 | best_loss=0.02511
Epoch 24/80: current_loss=0.02810 | best_loss=0.02511
Epoch 25/80: current_loss=0.02774 | best_loss=0.02511
Epoch 26/80: current_loss=0.02582 | best_loss=0.02511
Epoch 27/80: current_loss=0.03790 | best_loss=0.02511
Epoch 28/80: current_loss=0.03351 | best_loss=0.02511
Epoch 29/80: current_loss=0.02857 | best_loss=0.02511
Epoch 30/80: current_loss=0.02625 | best_loss=0.02511
Epoch 31/80: current_loss=0.02602 | best_loss=0.02511
Epoch 32/80: current_loss=0.02547 | best_loss=0.02511
Epoch 33/80: current_loss=0.02892 | best_loss=0.02511
Epoch 34/80: current_loss=0.02505 | best_loss=0.02505
Epoch 35/80: current_loss=0.02711 | best_loss=0.02505
Epoch 36/80: current_loss=0.02585 | best_loss=0.02505
Epoch 37/80: current_loss=0.02519 | best_loss=0.02505
Epoch 38/80: current_loss=0.02591 | best_loss=0.02505
Epoch 39/80: current_loss=0.02744 | best_loss=0.02505
Epoch 40/80: current_loss=0.03202 | best_loss=0.02505
Epoch 41/80: current_loss=0.02592 | best_loss=0.02505
Epoch 42/80: current_loss=0.02536 | best_loss=0.02505
Epoch 43/80: current_loss=0.03455 | best_loss=0.02505
Epoch 44/80: current_loss=0.02954 | best_loss=0.02505
Epoch 45/80: current_loss=0.02735 | best_loss=0.02505
Epoch 46/80: current_loss=0.02627 | best_loss=0.02505
Epoch 47/80: current_loss=0.02579 | best_loss=0.02505
Epoch 48/80: current_loss=0.02725 | best_loss=0.02505
Epoch 49/80: current_loss=0.02734 | best_loss=0.02505
Epoch 50/80: current_loss=0.02621 | best_loss=0.02505
Epoch 51/80: current_loss=0.02804 | best_loss=0.02505
Epoch 52/80: current_loss=0.02897 | best_loss=0.02505
Epoch 53/80: current_loss=0.02658 | best_loss=0.02505
Epoch 54/80: current_loss=0.02678 | best_loss=0.02505
Early Stopping at epoch 54
      explained_var=0.05330 | mse_loss=0.02454
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02825 | best_loss=0.02825
Epoch 1/80: current_loss=0.03184 | best_loss=0.02825
Epoch 2/80: current_loss=0.02776 | best_loss=0.02776
Epoch 3/80: current_loss=0.03285 | best_loss=0.02776
Epoch 4/80: current_loss=0.03678 | best_loss=0.02776
Epoch 5/80: current_loss=0.03027 | best_loss=0.02776
Epoch 6/80: current_loss=0.03457 | best_loss=0.02776
Epoch 7/80: current_loss=0.03361 | best_loss=0.02776
Epoch 8/80: current_loss=0.02786 | best_loss=0.02776
Epoch 9/80: current_loss=0.02771 | best_loss=0.02771
Epoch 10/80: current_loss=0.02767 | best_loss=0.02767
Epoch 11/80: current_loss=0.02766 | best_loss=0.02766
Epoch 12/80: current_loss=0.02863 | best_loss=0.02766
Epoch 13/80: current_loss=0.03276 | best_loss=0.02766
Epoch 14/80: current_loss=0.03015 | best_loss=0.02766
Epoch 15/80: current_loss=0.03200 | best_loss=0.02766
Epoch 16/80: current_loss=0.03472 | best_loss=0.02766
Epoch 17/80: current_loss=0.02841 | best_loss=0.02766
Epoch 18/80: current_loss=0.02893 | best_loss=0.02766
Epoch 19/80: current_loss=0.02820 | best_loss=0.02766
Epoch 20/80: current_loss=0.02965 | best_loss=0.02766
Epoch 21/80: current_loss=0.02854 | best_loss=0.02766
Epoch 22/80: current_loss=0.02828 | best_loss=0.02766
Epoch 23/80: current_loss=0.02775 | best_loss=0.02766
Epoch 24/80: current_loss=0.02953 | best_loss=0.02766
Epoch 25/80: current_loss=0.03389 | best_loss=0.02766
Epoch 26/80: current_loss=0.02796 | best_loss=0.02766
Epoch 27/80: current_loss=0.02999 | best_loss=0.02766
Epoch 28/80: current_loss=0.02869 | best_loss=0.02766
Epoch 29/80: current_loss=0.02817 | best_loss=0.02766
Epoch 30/80: current_loss=0.03134 | best_loss=0.02766
Epoch 31/80: current_loss=0.03042 | best_loss=0.02766
Early Stopping at epoch 31
      explained_var=0.03555 | mse_loss=0.02717
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03396 | best_loss=0.03396
Epoch 1/80: current_loss=0.03001 | best_loss=0.03001
Epoch 2/80: current_loss=0.03063 | best_loss=0.03001
Epoch 3/80: current_loss=0.02946 | best_loss=0.02946
Epoch 4/80: current_loss=0.02997 | best_loss=0.02946
Epoch 5/80: current_loss=0.02989 | best_loss=0.02946
Epoch 6/80: current_loss=0.02932 | best_loss=0.02932
Epoch 7/80: current_loss=0.02963 | best_loss=0.02932
Epoch 8/80: current_loss=0.03011 | best_loss=0.02932
Epoch 9/80: current_loss=0.03411 | best_loss=0.02932
Epoch 10/80: current_loss=0.02976 | best_loss=0.02932
Epoch 11/80: current_loss=0.03716 | best_loss=0.02932
Epoch 12/80: current_loss=0.03160 | best_loss=0.02932
Epoch 13/80: current_loss=0.03122 | best_loss=0.02932
Epoch 14/80: current_loss=0.03406 | best_loss=0.02932
Epoch 15/80: current_loss=0.03124 | best_loss=0.02932
Epoch 16/80: current_loss=0.03226 | best_loss=0.02932
Epoch 17/80: current_loss=0.02969 | best_loss=0.02932
Epoch 18/80: current_loss=0.02994 | best_loss=0.02932
Epoch 19/80: current_loss=0.03370 | best_loss=0.02932
Epoch 20/80: current_loss=0.03462 | best_loss=0.02932
Epoch 21/80: current_loss=0.02956 | best_loss=0.02932
Epoch 22/80: current_loss=0.03013 | best_loss=0.02932
Epoch 23/80: current_loss=0.02980 | best_loss=0.02932
Epoch 24/80: current_loss=0.02970 | best_loss=0.02932
Epoch 25/80: current_loss=0.03086 | best_loss=0.02932
Epoch 26/80: current_loss=0.03388 | best_loss=0.02932
Early Stopping at epoch 26
      explained_var=0.04069 | mse_loss=0.02853
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02864 | best_loss=0.02864
Epoch 1/80: current_loss=0.02966 | best_loss=0.02864
Epoch 2/80: current_loss=0.02906 | best_loss=0.02864
Epoch 3/80: current_loss=0.02990 | best_loss=0.02864
Epoch 4/80: current_loss=0.02852 | best_loss=0.02852
Epoch 5/80: current_loss=0.02984 | best_loss=0.02852
Epoch 6/80: current_loss=0.02826 | best_loss=0.02826
Epoch 7/80: current_loss=0.02838 | best_loss=0.02826
Epoch 8/80: current_loss=0.02828 | best_loss=0.02826
Epoch 9/80: current_loss=0.02959 | best_loss=0.02826
Epoch 10/80: current_loss=0.02950 | best_loss=0.02826
Epoch 11/80: current_loss=0.02846 | best_loss=0.02826
Epoch 12/80: current_loss=0.03175 | best_loss=0.02826
Epoch 13/80: current_loss=0.02957 | best_loss=0.02826
Epoch 14/80: current_loss=0.03137 | best_loss=0.02826
Epoch 15/80: current_loss=0.02929 | best_loss=0.02826
Epoch 16/80: current_loss=0.02839 | best_loss=0.02826
Epoch 17/80: current_loss=0.02810 | best_loss=0.02810
Epoch 18/80: current_loss=0.02845 | best_loss=0.02810
Epoch 19/80: current_loss=0.03090 | best_loss=0.02810
Epoch 20/80: current_loss=0.02887 | best_loss=0.02810
Epoch 21/80: current_loss=0.02874 | best_loss=0.02810
Epoch 22/80: current_loss=0.02895 | best_loss=0.02810
Epoch 23/80: current_loss=0.02860 | best_loss=0.02810
Epoch 24/80: current_loss=0.02836 | best_loss=0.02810
Epoch 25/80: current_loss=0.02902 | best_loss=0.02810
Epoch 26/80: current_loss=0.02846 | best_loss=0.02810
Epoch 27/80: current_loss=0.02837 | best_loss=0.02810
Epoch 28/80: current_loss=0.02819 | best_loss=0.02810
Epoch 29/80: current_loss=0.03423 | best_loss=0.02810
Epoch 30/80: current_loss=0.02915 | best_loss=0.02810
Epoch 31/80: current_loss=0.02847 | best_loss=0.02810
Epoch 32/80: current_loss=0.02988 | best_loss=0.02810
Epoch 33/80: current_loss=0.05370 | best_loss=0.02810
Epoch 34/80: current_loss=0.15972 | best_loss=0.02810
Epoch 35/80: current_loss=0.06284 | best_loss=0.02810
Epoch 36/80: current_loss=0.03126 | best_loss=0.02810
Epoch 37/80: current_loss=0.03209 | best_loss=0.02810
Early Stopping at epoch 37
      explained_var=-0.00196 | mse_loss=0.02846
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03624 | best_loss=0.03624
Epoch 1/80: current_loss=0.03404 | best_loss=0.03404
Epoch 2/80: current_loss=0.03642 | best_loss=0.03404
Epoch 3/80: current_loss=0.04295 | best_loss=0.03404
Epoch 4/80: current_loss=0.06168 | best_loss=0.03404
Epoch 5/80: current_loss=0.04435 | best_loss=0.03404
Epoch 6/80: current_loss=0.03385 | best_loss=0.03385
Epoch 7/80: current_loss=0.03508 | best_loss=0.03385
Epoch 8/80: current_loss=0.03549 | best_loss=0.03385
Epoch 9/80: current_loss=0.03989 | best_loss=0.03385
Epoch 10/80: current_loss=0.03448 | best_loss=0.03385
Epoch 11/80: current_loss=0.03252 | best_loss=0.03252
Epoch 12/80: current_loss=0.03352 | best_loss=0.03252
Epoch 13/80: current_loss=0.04532 | best_loss=0.03252
Epoch 14/80: current_loss=0.03560 | best_loss=0.03252
Epoch 15/80: current_loss=0.03667 | best_loss=0.03252
Epoch 16/80: current_loss=0.03905 | best_loss=0.03252
Epoch 17/80: current_loss=0.03435 | best_loss=0.03252
Epoch 18/80: current_loss=0.03448 | best_loss=0.03252
Epoch 19/80: current_loss=0.03444 | best_loss=0.03252
Epoch 20/80: current_loss=0.03396 | best_loss=0.03252
Epoch 21/80: current_loss=0.03409 | best_loss=0.03252
Epoch 22/80: current_loss=0.03212 | best_loss=0.03212
Epoch 23/80: current_loss=0.03300 | best_loss=0.03212
Epoch 24/80: current_loss=0.03576 | best_loss=0.03212
Epoch 25/80: current_loss=0.03474 | best_loss=0.03212
Epoch 26/80: current_loss=0.03700 | best_loss=0.03212
Epoch 27/80: current_loss=0.03595 | best_loss=0.03212
Epoch 28/80: current_loss=0.04041 | best_loss=0.03212
Epoch 29/80: current_loss=0.03760 | best_loss=0.03212
Epoch 30/80: current_loss=0.03572 | best_loss=0.03212
Epoch 31/80: current_loss=0.03319 | best_loss=0.03212
Epoch 32/80: current_loss=0.03400 | best_loss=0.03212
Epoch 33/80: current_loss=0.03690 | best_loss=0.03212
Epoch 34/80: current_loss=0.04019 | best_loss=0.03212
Epoch 35/80: current_loss=0.06829 | best_loss=0.03212
Epoch 36/80: current_loss=0.05947 | best_loss=0.03212
Epoch 37/80: current_loss=0.03538 | best_loss=0.03212
Epoch 38/80: current_loss=0.03771 | best_loss=0.03212
Epoch 39/80: current_loss=0.03239 | best_loss=0.03212
Epoch 40/80: current_loss=0.03529 | best_loss=0.03212
Epoch 41/80: current_loss=0.05442 | best_loss=0.03212
Epoch 42/80: current_loss=0.03556 | best_loss=0.03212
Early Stopping at epoch 42
      explained_var=0.00354 | mse_loss=0.03283
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.02623| avg_loss=0.02831
----------------------------------------------


----------------------------------------------
Params for Trial 66
{'learning_rate': 0.001, 'weight_decay': 0.0019918366562063205, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03513 | best_loss=0.03513
Epoch 1/80: current_loss=0.02862 | best_loss=0.02862
Epoch 2/80: current_loss=0.02500 | best_loss=0.02500
Epoch 3/80: current_loss=0.02711 | best_loss=0.02500
Epoch 4/80: current_loss=0.02867 | best_loss=0.02500
Epoch 5/80: current_loss=0.03255 | best_loss=0.02500
Epoch 6/80: current_loss=0.02543 | best_loss=0.02500
Epoch 7/80: current_loss=0.02639 | best_loss=0.02500
Epoch 8/80: current_loss=0.02583 | best_loss=0.02500
Epoch 9/80: current_loss=0.02613 | best_loss=0.02500
Epoch 10/80: current_loss=0.02839 | best_loss=0.02500
Epoch 11/80: current_loss=0.02915 | best_loss=0.02500
Epoch 12/80: current_loss=0.02537 | best_loss=0.02500
Epoch 13/80: current_loss=0.02698 | best_loss=0.02500
Epoch 14/80: current_loss=0.02804 | best_loss=0.02500
Epoch 15/80: current_loss=0.02591 | best_loss=0.02500
Epoch 16/80: current_loss=0.02628 | best_loss=0.02500
Epoch 17/80: current_loss=0.02941 | best_loss=0.02500
Epoch 18/80: current_loss=0.04182 | best_loss=0.02500
Epoch 19/80: current_loss=0.02863 | best_loss=0.02500
Epoch 20/80: current_loss=0.02686 | best_loss=0.02500
Epoch 21/80: current_loss=0.02539 | best_loss=0.02500
Epoch 22/80: current_loss=0.02696 | best_loss=0.02500
Early Stopping at epoch 22
      explained_var=0.05501 | mse_loss=0.02445
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02821 | best_loss=0.02821
Epoch 1/80: current_loss=0.03195 | best_loss=0.02821
Epoch 2/80: current_loss=0.02900 | best_loss=0.02821
Epoch 3/80: current_loss=0.03003 | best_loss=0.02821
Epoch 4/80: current_loss=0.02887 | best_loss=0.02821
Epoch 5/80: current_loss=0.02800 | best_loss=0.02800
Epoch 6/80: current_loss=0.02826 | best_loss=0.02800
Epoch 7/80: current_loss=0.03264 | best_loss=0.02800
Epoch 8/80: current_loss=0.02841 | best_loss=0.02800
Epoch 9/80: current_loss=0.02805 | best_loss=0.02800
Epoch 10/80: current_loss=0.02844 | best_loss=0.02800
Epoch 11/80: current_loss=0.02922 | best_loss=0.02800
Epoch 12/80: current_loss=0.03467 | best_loss=0.02800
Epoch 13/80: current_loss=0.02877 | best_loss=0.02800
Epoch 14/80: current_loss=0.02932 | best_loss=0.02800
Epoch 15/80: current_loss=0.02846 | best_loss=0.02800
Epoch 16/80: current_loss=0.02879 | best_loss=0.02800
Epoch 17/80: current_loss=0.02809 | best_loss=0.02800
Epoch 18/80: current_loss=0.03051 | best_loss=0.02800
Epoch 19/80: current_loss=0.02874 | best_loss=0.02800
Epoch 20/80: current_loss=0.03071 | best_loss=0.02800
Epoch 21/80: current_loss=0.02801 | best_loss=0.02800
Epoch 22/80: current_loss=0.02826 | best_loss=0.02800
Epoch 23/80: current_loss=0.02848 | best_loss=0.02800
Epoch 24/80: current_loss=0.02866 | best_loss=0.02800
Epoch 25/80: current_loss=0.02794 | best_loss=0.02794
Epoch 26/80: current_loss=0.02851 | best_loss=0.02794
Epoch 27/80: current_loss=0.02811 | best_loss=0.02794
Epoch 28/80: current_loss=0.02878 | best_loss=0.02794
Epoch 29/80: current_loss=0.02862 | best_loss=0.02794
Epoch 30/80: current_loss=0.02805 | best_loss=0.02794
Epoch 31/80: current_loss=0.02826 | best_loss=0.02794
Epoch 32/80: current_loss=0.02979 | best_loss=0.02794
Epoch 33/80: current_loss=0.03100 | best_loss=0.02794
Epoch 34/80: current_loss=0.02798 | best_loss=0.02794
Epoch 35/80: current_loss=0.02820 | best_loss=0.02794
Epoch 36/80: current_loss=0.02810 | best_loss=0.02794
Epoch 37/80: current_loss=0.02908 | best_loss=0.02794
Epoch 38/80: current_loss=0.02846 | best_loss=0.02794
Epoch 39/80: current_loss=0.02887 | best_loss=0.02794
Epoch 40/80: current_loss=0.02831 | best_loss=0.02794
Epoch 41/80: current_loss=0.02795 | best_loss=0.02794
Epoch 42/80: current_loss=0.02801 | best_loss=0.02794
Epoch 43/80: current_loss=0.02967 | best_loss=0.02794
Epoch 44/80: current_loss=0.02807 | best_loss=0.02794
Epoch 45/80: current_loss=0.02795 | best_loss=0.02794
Early Stopping at epoch 45
      explained_var=0.01202 | mse_loss=0.02745
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02972 | best_loss=0.02972
Epoch 1/80: current_loss=0.02949 | best_loss=0.02949
Epoch 2/80: current_loss=0.03020 | best_loss=0.02949
Epoch 3/80: current_loss=0.03115 | best_loss=0.02949
Epoch 4/80: current_loss=0.02996 | best_loss=0.02949
Epoch 5/80: current_loss=0.03141 | best_loss=0.02949
Epoch 6/80: current_loss=0.03334 | best_loss=0.02949
Epoch 7/80: current_loss=0.03022 | best_loss=0.02949
Epoch 8/80: current_loss=0.03110 | best_loss=0.02949
Epoch 9/80: current_loss=0.03695 | best_loss=0.02949
Epoch 10/80: current_loss=0.03943 | best_loss=0.02949
Epoch 11/80: current_loss=0.03304 | best_loss=0.02949
Epoch 12/80: current_loss=0.03007 | best_loss=0.02949
Epoch 13/80: current_loss=0.03507 | best_loss=0.02949
Epoch 14/80: current_loss=0.03140 | best_loss=0.02949
Epoch 15/80: current_loss=0.03020 | best_loss=0.02949
Epoch 16/80: current_loss=0.02941 | best_loss=0.02941
Epoch 17/80: current_loss=0.03132 | best_loss=0.02941
Epoch 18/80: current_loss=0.03007 | best_loss=0.02941
Epoch 19/80: current_loss=0.02975 | best_loss=0.02941
Epoch 20/80: current_loss=0.03039 | best_loss=0.02941
Epoch 21/80: current_loss=0.03432 | best_loss=0.02941
Epoch 22/80: current_loss=0.03207 | best_loss=0.02941
Epoch 23/80: current_loss=0.02973 | best_loss=0.02941
Epoch 24/80: current_loss=0.02960 | best_loss=0.02941
Epoch 25/80: current_loss=0.02954 | best_loss=0.02941
Epoch 26/80: current_loss=0.02965 | best_loss=0.02941
Epoch 27/80: current_loss=0.03080 | best_loss=0.02941
Epoch 28/80: current_loss=0.02985 | best_loss=0.02941
Epoch 29/80: current_loss=0.03099 | best_loss=0.02941
Epoch 30/80: current_loss=0.03348 | best_loss=0.02941
Epoch 31/80: current_loss=0.03021 | best_loss=0.02941
Epoch 32/80: current_loss=0.03187 | best_loss=0.02941
Epoch 33/80: current_loss=0.03214 | best_loss=0.02941
Epoch 34/80: current_loss=0.02940 | best_loss=0.02940
Epoch 35/80: current_loss=0.03403 | best_loss=0.02940
Epoch 36/80: current_loss=0.03143 | best_loss=0.02940
Epoch 37/80: current_loss=0.03021 | best_loss=0.02940
Epoch 38/80: current_loss=0.03027 | best_loss=0.02940
Epoch 39/80: current_loss=0.02979 | best_loss=0.02940
Epoch 40/80: current_loss=0.03101 | best_loss=0.02940
Epoch 41/80: current_loss=0.02974 | best_loss=0.02940
Epoch 42/80: current_loss=0.02981 | best_loss=0.02940
Epoch 43/80: current_loss=0.03147 | best_loss=0.02940
Epoch 44/80: current_loss=0.03507 | best_loss=0.02940
Epoch 45/80: current_loss=0.03322 | best_loss=0.02940
Epoch 46/80: current_loss=0.03362 | best_loss=0.02940
Epoch 47/80: current_loss=0.02985 | best_loss=0.02940
Epoch 48/80: current_loss=0.03375 | best_loss=0.02940
Epoch 49/80: current_loss=0.03170 | best_loss=0.02940
Epoch 50/80: current_loss=0.03110 | best_loss=0.02940
Epoch 51/80: current_loss=0.03061 | best_loss=0.02940
Epoch 52/80: current_loss=0.03319 | best_loss=0.02940
Epoch 53/80: current_loss=0.03298 | best_loss=0.02940
Epoch 54/80: current_loss=0.02990 | best_loss=0.02940
Early Stopping at epoch 54
      explained_var=0.03390 | mse_loss=0.02861
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02857 | best_loss=0.02857
Epoch 1/80: current_loss=0.03195 | best_loss=0.02857
Epoch 2/80: current_loss=0.02835 | best_loss=0.02835
Epoch 3/80: current_loss=0.02880 | best_loss=0.02835
Epoch 4/80: current_loss=0.02905 | best_loss=0.02835
Epoch 5/80: current_loss=0.03071 | best_loss=0.02835
Epoch 6/80: current_loss=0.02846 | best_loss=0.02835
Epoch 7/80: current_loss=0.02856 | best_loss=0.02835
Epoch 8/80: current_loss=0.02850 | best_loss=0.02835
Epoch 9/80: current_loss=0.02844 | best_loss=0.02835
Epoch 10/80: current_loss=0.03036 | best_loss=0.02835
Epoch 11/80: current_loss=0.02843 | best_loss=0.02835
Epoch 12/80: current_loss=0.02859 | best_loss=0.02835
Epoch 13/80: current_loss=0.02819 | best_loss=0.02819
Epoch 14/80: current_loss=0.02834 | best_loss=0.02819
Epoch 15/80: current_loss=0.02841 | best_loss=0.02819
Epoch 16/80: current_loss=0.02901 | best_loss=0.02819
Epoch 17/80: current_loss=0.02834 | best_loss=0.02819
Epoch 18/80: current_loss=0.02997 | best_loss=0.02819
Epoch 19/80: current_loss=0.02990 | best_loss=0.02819
Epoch 20/80: current_loss=0.02848 | best_loss=0.02819
Epoch 21/80: current_loss=0.02850 | best_loss=0.02819
Epoch 22/80: current_loss=0.02872 | best_loss=0.02819
Epoch 23/80: current_loss=0.02871 | best_loss=0.02819
Epoch 24/80: current_loss=0.02832 | best_loss=0.02819
Epoch 25/80: current_loss=0.02847 | best_loss=0.02819
Epoch 26/80: current_loss=0.02853 | best_loss=0.02819
Epoch 27/80: current_loss=0.02822 | best_loss=0.02819
Epoch 28/80: current_loss=0.02843 | best_loss=0.02819
Epoch 29/80: current_loss=0.02957 | best_loss=0.02819
Epoch 30/80: current_loss=0.03066 | best_loss=0.02819
Epoch 31/80: current_loss=0.02867 | best_loss=0.02819
Epoch 32/80: current_loss=0.03046 | best_loss=0.02819
Epoch 33/80: current_loss=0.02897 | best_loss=0.02819
Early Stopping at epoch 33
      explained_var=-0.00326 | mse_loss=0.02855
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03470 | best_loss=0.03470
Epoch 1/80: current_loss=0.03383 | best_loss=0.03383
Epoch 2/80: current_loss=0.03362 | best_loss=0.03362
Epoch 3/80: current_loss=0.03332 | best_loss=0.03332
Epoch 4/80: current_loss=0.03270 | best_loss=0.03270
Epoch 5/80: current_loss=0.03286 | best_loss=0.03270
Epoch 6/80: current_loss=0.03278 | best_loss=0.03270
Epoch 7/80: current_loss=0.03427 | best_loss=0.03270
Epoch 8/80: current_loss=0.03419 | best_loss=0.03270
Epoch 9/80: current_loss=0.03334 | best_loss=0.03270
Epoch 10/80: current_loss=0.03396 | best_loss=0.03270
Epoch 11/80: current_loss=0.03372 | best_loss=0.03270
Epoch 12/80: current_loss=0.03375 | best_loss=0.03270
Epoch 13/80: current_loss=0.03306 | best_loss=0.03270
Epoch 14/80: current_loss=0.03390 | best_loss=0.03270
Epoch 15/80: current_loss=0.03394 | best_loss=0.03270
Epoch 16/80: current_loss=0.03317 | best_loss=0.03270
Epoch 17/80: current_loss=0.03355 | best_loss=0.03270
Epoch 18/80: current_loss=0.03414 | best_loss=0.03270
Epoch 19/80: current_loss=0.03343 | best_loss=0.03270
Epoch 20/80: current_loss=0.03304 | best_loss=0.03270
Epoch 21/80: current_loss=0.03357 | best_loss=0.03270
Epoch 22/80: current_loss=0.03382 | best_loss=0.03270
Epoch 23/80: current_loss=0.03486 | best_loss=0.03270
Epoch 24/80: current_loss=0.03311 | best_loss=0.03270
Early Stopping at epoch 24
      explained_var=-0.03041 | mse_loss=0.03347
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.01345| avg_loss=0.02851
----------------------------------------------


----------------------------------------------
Params for Trial 67
{'learning_rate': 0.001, 'weight_decay': 0.004728153681329948, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03061 | best_loss=0.03061
Epoch 1/80: current_loss=0.02796 | best_loss=0.02796
Epoch 2/80: current_loss=0.02529 | best_loss=0.02529
Epoch 3/80: current_loss=0.02601 | best_loss=0.02529
Epoch 4/80: current_loss=0.03171 | best_loss=0.02529
Epoch 5/80: current_loss=0.02660 | best_loss=0.02529
Epoch 6/80: current_loss=0.03072 | best_loss=0.02529
Epoch 7/80: current_loss=0.03203 | best_loss=0.02529
Epoch 8/80: current_loss=0.02493 | best_loss=0.02493
Epoch 9/80: current_loss=0.02725 | best_loss=0.02493
Epoch 10/80: current_loss=0.02650 | best_loss=0.02493
Epoch 11/80: current_loss=0.02792 | best_loss=0.02493
Epoch 12/80: current_loss=0.02899 | best_loss=0.02493
Epoch 13/80: current_loss=0.02552 | best_loss=0.02493
Epoch 14/80: current_loss=0.02731 | best_loss=0.02493
Epoch 15/80: current_loss=0.02997 | best_loss=0.02493
Epoch 16/80: current_loss=0.02552 | best_loss=0.02493
Epoch 17/80: current_loss=0.02609 | best_loss=0.02493
Epoch 18/80: current_loss=0.02735 | best_loss=0.02493
Epoch 19/80: current_loss=0.02610 | best_loss=0.02493
Epoch 20/80: current_loss=0.03135 | best_loss=0.02493
Epoch 21/80: current_loss=0.02619 | best_loss=0.02493
Epoch 22/80: current_loss=0.02549 | best_loss=0.02493
Epoch 23/80: current_loss=0.02588 | best_loss=0.02493
Epoch 24/80: current_loss=0.02843 | best_loss=0.02493
Epoch 25/80: current_loss=0.02633 | best_loss=0.02493
Epoch 26/80: current_loss=0.02953 | best_loss=0.02493
Epoch 27/80: current_loss=0.02728 | best_loss=0.02493
Epoch 28/80: current_loss=0.03323 | best_loss=0.02493
Early Stopping at epoch 28
      explained_var=0.05459 | mse_loss=0.02445
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03119 | best_loss=0.03119
Epoch 1/80: current_loss=0.02862 | best_loss=0.02862
Epoch 2/80: current_loss=0.02824 | best_loss=0.02824
Epoch 3/80: current_loss=0.02934 | best_loss=0.02824
Epoch 4/80: current_loss=0.02862 | best_loss=0.02824
Epoch 5/80: current_loss=0.03106 | best_loss=0.02824
Epoch 6/80: current_loss=0.02873 | best_loss=0.02824
Epoch 7/80: current_loss=0.02841 | best_loss=0.02824
Epoch 8/80: current_loss=0.02801 | best_loss=0.02801
Epoch 9/80: current_loss=0.02819 | best_loss=0.02801
Epoch 10/80: current_loss=0.02800 | best_loss=0.02800
Epoch 11/80: current_loss=0.02829 | best_loss=0.02800
Epoch 12/80: current_loss=0.02820 | best_loss=0.02800
Epoch 13/80: current_loss=0.02851 | best_loss=0.02800
Epoch 14/80: current_loss=0.02844 | best_loss=0.02800
Epoch 15/80: current_loss=0.02862 | best_loss=0.02800
Epoch 16/80: current_loss=0.02820 | best_loss=0.02800
Epoch 17/80: current_loss=0.02812 | best_loss=0.02800
Epoch 18/80: current_loss=0.02985 | best_loss=0.02800
Epoch 19/80: current_loss=0.02941 | best_loss=0.02800
Epoch 20/80: current_loss=0.03094 | best_loss=0.02800
Epoch 21/80: current_loss=0.02862 | best_loss=0.02800
Epoch 22/80: current_loss=0.02843 | best_loss=0.02800
Epoch 23/80: current_loss=0.02956 | best_loss=0.02800
Epoch 24/80: current_loss=0.02895 | best_loss=0.02800
Epoch 25/80: current_loss=0.03181 | best_loss=0.02800
Epoch 26/80: current_loss=0.02830 | best_loss=0.02800
Epoch 27/80: current_loss=0.02827 | best_loss=0.02800
Epoch 28/80: current_loss=0.02814 | best_loss=0.02800
Epoch 29/80: current_loss=0.02821 | best_loss=0.02800
Epoch 30/80: current_loss=0.02849 | best_loss=0.02800
Early Stopping at epoch 30
      explained_var=0.01058 | mse_loss=0.02751
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03052 | best_loss=0.03052
Epoch 1/80: current_loss=0.03218 | best_loss=0.03052
Epoch 2/80: current_loss=0.02994 | best_loss=0.02994
Epoch 3/80: current_loss=0.03019 | best_loss=0.02994
Epoch 4/80: current_loss=0.03287 | best_loss=0.02994
Epoch 5/80: current_loss=0.03010 | best_loss=0.02994
Epoch 6/80: current_loss=0.03037 | best_loss=0.02994
Epoch 7/80: current_loss=0.03216 | best_loss=0.02994
Epoch 8/80: current_loss=0.03300 | best_loss=0.02994
Epoch 9/80: current_loss=0.03040 | best_loss=0.02994
Epoch 10/80: current_loss=0.03034 | best_loss=0.02994
Epoch 11/80: current_loss=0.03057 | best_loss=0.02994
Epoch 12/80: current_loss=0.03041 | best_loss=0.02994
Epoch 13/80: current_loss=0.03137 | best_loss=0.02994
Epoch 14/80: current_loss=0.03067 | best_loss=0.02994
Epoch 15/80: current_loss=0.03312 | best_loss=0.02994
Epoch 16/80: current_loss=0.03501 | best_loss=0.02994
Epoch 17/80: current_loss=0.03239 | best_loss=0.02994
Epoch 18/80: current_loss=0.03380 | best_loss=0.02994
Epoch 19/80: current_loss=0.03202 | best_loss=0.02994
Epoch 20/80: current_loss=0.03065 | best_loss=0.02994
Epoch 21/80: current_loss=0.03117 | best_loss=0.02994
Epoch 22/80: current_loss=0.03137 | best_loss=0.02994
Early Stopping at epoch 22
      explained_var=0.01900 | mse_loss=0.02916
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02842 | best_loss=0.02842
Epoch 1/80: current_loss=0.02888 | best_loss=0.02842
Epoch 2/80: current_loss=0.02829 | best_loss=0.02829
Epoch 3/80: current_loss=0.02829 | best_loss=0.02829
Epoch 4/80: current_loss=0.02825 | best_loss=0.02825
Epoch 5/80: current_loss=0.02973 | best_loss=0.02825
Epoch 6/80: current_loss=0.02849 | best_loss=0.02825
Epoch 7/80: current_loss=0.02919 | best_loss=0.02825
Epoch 8/80: current_loss=0.02817 | best_loss=0.02817
Epoch 9/80: current_loss=0.02845 | best_loss=0.02817
Epoch 10/80: current_loss=0.02835 | best_loss=0.02817
Epoch 11/80: current_loss=0.02856 | best_loss=0.02817
Epoch 12/80: current_loss=0.02829 | best_loss=0.02817
Epoch 13/80: current_loss=0.02860 | best_loss=0.02817
Epoch 14/80: current_loss=0.02823 | best_loss=0.02817
Epoch 15/80: current_loss=0.02814 | best_loss=0.02814
Epoch 16/80: current_loss=0.02816 | best_loss=0.02814
Epoch 17/80: current_loss=0.02808 | best_loss=0.02808
Epoch 18/80: current_loss=0.02924 | best_loss=0.02808
Epoch 19/80: current_loss=0.02887 | best_loss=0.02808
Epoch 20/80: current_loss=0.02866 | best_loss=0.02808
Epoch 21/80: current_loss=0.03306 | best_loss=0.02808
Epoch 22/80: current_loss=0.02902 | best_loss=0.02808
Epoch 23/80: current_loss=0.02897 | best_loss=0.02808
Epoch 24/80: current_loss=0.02833 | best_loss=0.02808
Epoch 25/80: current_loss=0.02819 | best_loss=0.02808
Epoch 26/80: current_loss=0.03014 | best_loss=0.02808
Epoch 27/80: current_loss=0.02997 | best_loss=0.02808
Epoch 28/80: current_loss=0.02806 | best_loss=0.02806
Epoch 29/80: current_loss=0.02972 | best_loss=0.02806
Epoch 30/80: current_loss=0.02867 | best_loss=0.02806
Epoch 31/80: current_loss=0.02821 | best_loss=0.02806
Epoch 32/80: current_loss=0.02812 | best_loss=0.02806
Epoch 33/80: current_loss=0.02829 | best_loss=0.02806
Epoch 34/80: current_loss=0.02997 | best_loss=0.02806
Epoch 35/80: current_loss=0.02893 | best_loss=0.02806
Epoch 36/80: current_loss=0.02991 | best_loss=0.02806
Epoch 37/80: current_loss=0.02835 | best_loss=0.02806
Epoch 38/80: current_loss=0.02819 | best_loss=0.02806
Epoch 39/80: current_loss=0.02832 | best_loss=0.02806
Epoch 40/80: current_loss=0.02897 | best_loss=0.02806
Epoch 41/80: current_loss=0.03165 | best_loss=0.02806
Epoch 42/80: current_loss=0.02880 | best_loss=0.02806
Epoch 43/80: current_loss=0.03004 | best_loss=0.02806
Epoch 44/80: current_loss=0.02977 | best_loss=0.02806
Epoch 45/80: current_loss=0.02942 | best_loss=0.02806
Epoch 46/80: current_loss=0.02826 | best_loss=0.02806
Epoch 47/80: current_loss=0.02808 | best_loss=0.02806
Epoch 48/80: current_loss=0.02910 | best_loss=0.02806
Early Stopping at epoch 48
      explained_var=-0.00212 | mse_loss=0.02842
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03590 | best_loss=0.03590
Epoch 1/80: current_loss=0.03548 | best_loss=0.03548
Epoch 2/80: current_loss=0.03288 | best_loss=0.03288
Epoch 3/80: current_loss=0.03355 | best_loss=0.03288
Epoch 4/80: current_loss=0.03292 | best_loss=0.03288
Epoch 5/80: current_loss=0.03253 | best_loss=0.03253
Epoch 6/80: current_loss=0.03244 | best_loss=0.03244
Epoch 7/80: current_loss=0.03278 | best_loss=0.03244
Epoch 8/80: current_loss=0.03290 | best_loss=0.03244
Epoch 9/80: current_loss=0.03296 | best_loss=0.03244
Epoch 10/80: current_loss=0.03345 | best_loss=0.03244
Epoch 11/80: current_loss=0.03305 | best_loss=0.03244
Epoch 12/80: current_loss=0.03280 | best_loss=0.03244
Epoch 13/80: current_loss=0.03351 | best_loss=0.03244
Epoch 14/80: current_loss=0.03291 | best_loss=0.03244
Epoch 15/80: current_loss=0.03274 | best_loss=0.03244
Epoch 16/80: current_loss=0.03266 | best_loss=0.03244
Epoch 17/80: current_loss=0.03320 | best_loss=0.03244
Epoch 18/80: current_loss=0.03319 | best_loss=0.03244
Epoch 19/80: current_loss=0.03440 | best_loss=0.03244
Epoch 20/80: current_loss=0.03271 | best_loss=0.03244
Epoch 21/80: current_loss=0.03295 | best_loss=0.03244
Epoch 22/80: current_loss=0.03309 | best_loss=0.03244
Epoch 23/80: current_loss=0.03359 | best_loss=0.03244
Epoch 24/80: current_loss=0.03404 | best_loss=0.03244
Epoch 25/80: current_loss=0.03291 | best_loss=0.03244
Epoch 26/80: current_loss=0.03323 | best_loss=0.03244
Early Stopping at epoch 26
      explained_var=-0.02077 | mse_loss=0.03319
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.01225| avg_loss=0.02855
----------------------------------------------


----------------------------------------------
Params for Trial 68
{'learning_rate': 0.01, 'weight_decay': 0.0009024814261389566, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=1.96207 | best_loss=1.96207
Epoch 1/80: current_loss=0.99128 | best_loss=0.99128
Epoch 2/80: current_loss=0.70156 | best_loss=0.70156
Epoch 3/80: current_loss=0.13634 | best_loss=0.13634
Epoch 4/80: current_loss=0.06844 | best_loss=0.06844
Epoch 5/80: current_loss=0.04688 | best_loss=0.04688
Epoch 6/80: current_loss=0.05802 | best_loss=0.04688
Epoch 7/80: current_loss=0.06542 | best_loss=0.04688
Epoch 8/80: current_loss=0.04355 | best_loss=0.04355
Epoch 9/80: current_loss=0.03071 | best_loss=0.03071
Epoch 10/80: current_loss=0.05822 | best_loss=0.03071
Epoch 11/80: current_loss=0.04234 | best_loss=0.03071
Epoch 12/80: current_loss=0.12279 | best_loss=0.03071
Epoch 13/80: current_loss=0.07742 | best_loss=0.03071
Epoch 14/80: current_loss=0.05319 | best_loss=0.03071
Epoch 15/80: current_loss=0.03465 | best_loss=0.03071
Epoch 16/80: current_loss=0.02957 | best_loss=0.02957
Epoch 17/80: current_loss=0.04793 | best_loss=0.02957
Epoch 18/80: current_loss=0.05329 | best_loss=0.02957
Epoch 19/80: current_loss=0.04773 | best_loss=0.02957
Epoch 20/80: current_loss=0.03242 | best_loss=0.02957
Epoch 21/80: current_loss=0.03418 | best_loss=0.02957
Epoch 22/80: current_loss=0.02897 | best_loss=0.02897
Epoch 23/80: current_loss=0.02762 | best_loss=0.02762
Epoch 24/80: current_loss=0.08126 | best_loss=0.02762
Epoch 25/80: current_loss=0.04174 | best_loss=0.02762
Epoch 26/80: current_loss=0.02700 | best_loss=0.02700
Epoch 27/80: current_loss=0.03958 | best_loss=0.02700
Epoch 28/80: current_loss=0.06130 | best_loss=0.02700
Epoch 29/80: current_loss=0.03504 | best_loss=0.02700
Epoch 30/80: current_loss=0.03303 | best_loss=0.02700
Epoch 31/80: current_loss=0.04092 | best_loss=0.02700
Epoch 32/80: current_loss=0.03593 | best_loss=0.02700
Epoch 33/80: current_loss=0.03721 | best_loss=0.02700
Epoch 34/80: current_loss=0.02992 | best_loss=0.02700
Epoch 35/80: current_loss=0.02774 | best_loss=0.02700
Epoch 36/80: current_loss=0.06816 | best_loss=0.02700
Epoch 37/80: current_loss=0.04512 | best_loss=0.02700
Epoch 38/80: current_loss=0.02926 | best_loss=0.02700
Epoch 39/80: current_loss=0.05332 | best_loss=0.02700
Epoch 40/80: current_loss=0.12773 | best_loss=0.02700
Epoch 41/80: current_loss=0.05882 | best_loss=0.02700
Epoch 42/80: current_loss=0.05289 | best_loss=0.02700
Epoch 43/80: current_loss=0.03969 | best_loss=0.02700
Epoch 44/80: current_loss=0.03528 | best_loss=0.02700
Epoch 45/80: current_loss=0.03011 | best_loss=0.02700
Epoch 46/80: current_loss=0.04267 | best_loss=0.02700
Early Stopping at epoch 46
      explained_var=0.01857 | mse_loss=0.02661

----------------------------------------------
Params for Trial 69
{'learning_rate': 0.001, 'weight_decay': 0.00039103807841240886, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03295 | best_loss=0.03295
Epoch 1/80: current_loss=0.02836 | best_loss=0.02836
Epoch 2/80: current_loss=0.02744 | best_loss=0.02744
Epoch 3/80: current_loss=0.02843 | best_loss=0.02744
Epoch 4/80: current_loss=0.02597 | best_loss=0.02597
Epoch 5/80: current_loss=0.02723 | best_loss=0.02597
Epoch 6/80: current_loss=0.02673 | best_loss=0.02597
Epoch 7/80: current_loss=0.02693 | best_loss=0.02597
Epoch 8/80: current_loss=0.02729 | best_loss=0.02597
Epoch 9/80: current_loss=0.02637 | best_loss=0.02597
Epoch 10/80: current_loss=0.02581 | best_loss=0.02581
Epoch 11/80: current_loss=0.02824 | best_loss=0.02581
Epoch 12/80: current_loss=0.02735 | best_loss=0.02581
Epoch 13/80: current_loss=0.02556 | best_loss=0.02556
Epoch 14/80: current_loss=0.02795 | best_loss=0.02556
Epoch 15/80: current_loss=0.03691 | best_loss=0.02556
Epoch 16/80: current_loss=0.03191 | best_loss=0.02556
Epoch 17/80: current_loss=0.02801 | best_loss=0.02556
Epoch 18/80: current_loss=0.02925 | best_loss=0.02556
Epoch 19/80: current_loss=0.02654 | best_loss=0.02556
Epoch 20/80: current_loss=0.02557 | best_loss=0.02556
Epoch 21/80: current_loss=0.02864 | best_loss=0.02556
Epoch 22/80: current_loss=0.03196 | best_loss=0.02556
Epoch 23/80: current_loss=0.02595 | best_loss=0.02556
Epoch 24/80: current_loss=0.02517 | best_loss=0.02517
Epoch 25/80: current_loss=0.03097 | best_loss=0.02517
Epoch 26/80: current_loss=0.03841 | best_loss=0.02517
Epoch 27/80: current_loss=0.02843 | best_loss=0.02517
Epoch 28/80: current_loss=0.02562 | best_loss=0.02517
Epoch 29/80: current_loss=0.02922 | best_loss=0.02517
Epoch 30/80: current_loss=0.02603 | best_loss=0.02517
Epoch 31/80: current_loss=0.02677 | best_loss=0.02517
Epoch 32/80: current_loss=0.03096 | best_loss=0.02517
Epoch 33/80: current_loss=0.02584 | best_loss=0.02517
Epoch 34/80: current_loss=0.02692 | best_loss=0.02517
Epoch 35/80: current_loss=0.02806 | best_loss=0.02517
Epoch 36/80: current_loss=0.02694 | best_loss=0.02517
Epoch 37/80: current_loss=0.02587 | best_loss=0.02517
Epoch 38/80: current_loss=0.02553 | best_loss=0.02517
Epoch 39/80: current_loss=0.02673 | best_loss=0.02517
Epoch 40/80: current_loss=0.02806 | best_loss=0.02517
Epoch 41/80: current_loss=0.02948 | best_loss=0.02517
Epoch 42/80: current_loss=0.03182 | best_loss=0.02517
Epoch 43/80: current_loss=0.02803 | best_loss=0.02517
Epoch 44/80: current_loss=0.02558 | best_loss=0.02517
Early Stopping at epoch 44
      explained_var=0.04947 | mse_loss=0.02459
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02786 | best_loss=0.02786
Epoch 1/80: current_loss=0.02994 | best_loss=0.02786
Epoch 2/80: current_loss=0.02714 | best_loss=0.02714
Epoch 3/80: current_loss=0.03095 | best_loss=0.02714
Epoch 4/80: current_loss=0.02890 | best_loss=0.02714
Epoch 5/80: current_loss=0.02840 | best_loss=0.02714
Epoch 6/80: current_loss=0.02790 | best_loss=0.02714
Epoch 7/80: current_loss=0.02970 | best_loss=0.02714
Epoch 8/80: current_loss=0.03042 | best_loss=0.02714
Epoch 9/80: current_loss=0.03008 | best_loss=0.02714
Epoch 10/80: current_loss=0.02862 | best_loss=0.02714
Epoch 11/80: current_loss=0.02967 | best_loss=0.02714
Epoch 12/80: current_loss=0.02919 | best_loss=0.02714
Epoch 13/80: current_loss=0.02847 | best_loss=0.02714
Epoch 14/80: current_loss=0.02941 | best_loss=0.02714
Epoch 15/80: current_loss=0.02787 | best_loss=0.02714
Epoch 16/80: current_loss=0.02713 | best_loss=0.02713
Epoch 17/80: current_loss=0.02717 | best_loss=0.02713
Epoch 18/80: current_loss=0.03120 | best_loss=0.02713
Epoch 19/80: current_loss=0.02936 | best_loss=0.02713
Epoch 20/80: current_loss=0.02713 | best_loss=0.02713
Epoch 21/80: current_loss=0.03143 | best_loss=0.02713
Epoch 22/80: current_loss=0.03122 | best_loss=0.02713
Epoch 23/80: current_loss=0.02792 | best_loss=0.02713
Epoch 24/80: current_loss=0.03058 | best_loss=0.02713
Epoch 25/80: current_loss=0.02795 | best_loss=0.02713
Epoch 26/80: current_loss=0.02862 | best_loss=0.02713
Epoch 27/80: current_loss=0.02880 | best_loss=0.02713
Epoch 28/80: current_loss=0.03061 | best_loss=0.02713
Epoch 29/80: current_loss=0.02799 | best_loss=0.02713
Epoch 30/80: current_loss=0.03053 | best_loss=0.02713
Epoch 31/80: current_loss=0.02998 | best_loss=0.02713
Epoch 32/80: current_loss=0.03314 | best_loss=0.02713
Epoch 33/80: current_loss=0.03074 | best_loss=0.02713
Epoch 34/80: current_loss=0.02728 | best_loss=0.02713
Epoch 35/80: current_loss=0.02866 | best_loss=0.02713
Epoch 36/80: current_loss=0.02803 | best_loss=0.02713
Epoch 37/80: current_loss=0.02753 | best_loss=0.02713
Epoch 38/80: current_loss=0.02826 | best_loss=0.02713
Epoch 39/80: current_loss=0.02846 | best_loss=0.02713
Epoch 40/80: current_loss=0.02840 | best_loss=0.02713
Early Stopping at epoch 40
      explained_var=0.04198 | mse_loss=0.02662
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02960 | best_loss=0.02960
Epoch 1/80: current_loss=0.03377 | best_loss=0.02960
Epoch 2/80: current_loss=0.03276 | best_loss=0.02960
Epoch 3/80: current_loss=0.03173 | best_loss=0.02960
Epoch 4/80: current_loss=0.03023 | best_loss=0.02960
Epoch 5/80: current_loss=0.03373 | best_loss=0.02960
Epoch 6/80: current_loss=0.03138 | best_loss=0.02960
Epoch 7/80: current_loss=0.03193 | best_loss=0.02960
Epoch 8/80: current_loss=0.02952 | best_loss=0.02952
Epoch 9/80: current_loss=0.03292 | best_loss=0.02952
Epoch 10/80: current_loss=0.02939 | best_loss=0.02939
Epoch 11/80: current_loss=0.03054 | best_loss=0.02939
Epoch 12/80: current_loss=0.03280 | best_loss=0.02939
Epoch 13/80: current_loss=0.03577 | best_loss=0.02939
Epoch 14/80: current_loss=0.02913 | best_loss=0.02913
Epoch 15/80: current_loss=0.03008 | best_loss=0.02913
Epoch 16/80: current_loss=0.02937 | best_loss=0.02913
Epoch 17/80: current_loss=0.03208 | best_loss=0.02913
Epoch 18/80: current_loss=0.03446 | best_loss=0.02913
Epoch 19/80: current_loss=0.03225 | best_loss=0.02913
Epoch 20/80: current_loss=0.03002 | best_loss=0.02913
Epoch 21/80: current_loss=0.03645 | best_loss=0.02913
Epoch 22/80: current_loss=0.03358 | best_loss=0.02913
Epoch 23/80: current_loss=0.02977 | best_loss=0.02913
Epoch 24/80: current_loss=0.03008 | best_loss=0.02913
Epoch 25/80: current_loss=0.03030 | best_loss=0.02913
Epoch 26/80: current_loss=0.03219 | best_loss=0.02913
Epoch 27/80: current_loss=0.03293 | best_loss=0.02913
Epoch 28/80: current_loss=0.03469 | best_loss=0.02913
Epoch 29/80: current_loss=0.45449 | best_loss=0.02913
Epoch 30/80: current_loss=0.16277 | best_loss=0.02913
Epoch 31/80: current_loss=0.12378 | best_loss=0.02913
Epoch 32/80: current_loss=0.08109 | best_loss=0.02913
Epoch 33/80: current_loss=0.07564 | best_loss=0.02913
Epoch 34/80: current_loss=0.08457 | best_loss=0.02913
Early Stopping at epoch 34
      explained_var=0.04872 | mse_loss=0.02829
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.05482 | best_loss=0.05482
Epoch 1/80: current_loss=0.05117 | best_loss=0.05117
Epoch 2/80: current_loss=0.04775 | best_loss=0.04775
Epoch 3/80: current_loss=0.04257 | best_loss=0.04257
Epoch 4/80: current_loss=0.05007 | best_loss=0.04257
Epoch 5/80: current_loss=0.05769 | best_loss=0.04257
Epoch 6/80: current_loss=0.03887 | best_loss=0.03887
Epoch 7/80: current_loss=0.04382 | best_loss=0.03887
Epoch 8/80: current_loss=0.04243 | best_loss=0.03887
Epoch 9/80: current_loss=0.03518 | best_loss=0.03518
Epoch 10/80: current_loss=0.03992 | best_loss=0.03518
Epoch 11/80: current_loss=0.03995 | best_loss=0.03518
Epoch 12/80: current_loss=0.03940 | best_loss=0.03518
Epoch 13/80: current_loss=0.03600 | best_loss=0.03518
Epoch 14/80: current_loss=0.03855 | best_loss=0.03518
Epoch 15/80: current_loss=0.04152 | best_loss=0.03518
Epoch 16/80: current_loss=0.03425 | best_loss=0.03425
Epoch 17/80: current_loss=0.03172 | best_loss=0.03172
Epoch 18/80: current_loss=0.03285 | best_loss=0.03172
Epoch 19/80: current_loss=0.03204 | best_loss=0.03172
Epoch 20/80: current_loss=0.03032 | best_loss=0.03032
Epoch 21/80: current_loss=0.02956 | best_loss=0.02956
Epoch 22/80: current_loss=0.02956 | best_loss=0.02956
Epoch 23/80: current_loss=0.02973 | best_loss=0.02956
Epoch 24/80: current_loss=0.03033 | best_loss=0.02956
Epoch 25/80: current_loss=0.03121 | best_loss=0.02956
Epoch 26/80: current_loss=0.03099 | best_loss=0.02956
Epoch 27/80: current_loss=0.03735 | best_loss=0.02956
Epoch 28/80: current_loss=0.03662 | best_loss=0.02956
Epoch 29/80: current_loss=0.03244 | best_loss=0.02956
Epoch 30/80: current_loss=0.02930 | best_loss=0.02930
Epoch 31/80: current_loss=0.03060 | best_loss=0.02930
Epoch 32/80: current_loss=0.03425 | best_loss=0.02930
Epoch 33/80: current_loss=0.03381 | best_loss=0.02930
Epoch 34/80: current_loss=0.03028 | best_loss=0.02930
Epoch 35/80: current_loss=0.03300 | best_loss=0.02930
Epoch 36/80: current_loss=0.03109 | best_loss=0.02930
Epoch 37/80: current_loss=0.03170 | best_loss=0.02930
Epoch 38/80: current_loss=0.02955 | best_loss=0.02930
Epoch 39/80: current_loss=0.03208 | best_loss=0.02930
Epoch 40/80: current_loss=0.02875 | best_loss=0.02875
Epoch 41/80: current_loss=0.04106 | best_loss=0.02875
Epoch 42/80: current_loss=0.03345 | best_loss=0.02875
Epoch 43/80: current_loss=0.03524 | best_loss=0.02875
Epoch 44/80: current_loss=0.02893 | best_loss=0.02875
Epoch 45/80: current_loss=0.03451 | best_loss=0.02875
Epoch 46/80: current_loss=0.03393 | best_loss=0.02875
Epoch 47/80: current_loss=0.03144 | best_loss=0.02875
Epoch 48/80: current_loss=0.02832 | best_loss=0.02832
Epoch 49/80: current_loss=0.02971 | best_loss=0.02832
Epoch 50/80: current_loss=0.02912 | best_loss=0.02832
Epoch 51/80: current_loss=0.02844 | best_loss=0.02832
Epoch 52/80: current_loss=0.03477 | best_loss=0.02832
Epoch 53/80: current_loss=0.03493 | best_loss=0.02832
Epoch 54/80: current_loss=0.02952 | best_loss=0.02832
Epoch 55/80: current_loss=0.03105 | best_loss=0.02832
Epoch 56/80: current_loss=0.02865 | best_loss=0.02832
Epoch 57/80: current_loss=0.02833 | best_loss=0.02832
Epoch 58/80: current_loss=0.03125 | best_loss=0.02832
Epoch 59/80: current_loss=0.03108 | best_loss=0.02832
Epoch 60/80: current_loss=0.02800 | best_loss=0.02800
Epoch 61/80: current_loss=0.02957 | best_loss=0.02800
Epoch 62/80: current_loss=0.03481 | best_loss=0.02800
Epoch 63/80: current_loss=0.02875 | best_loss=0.02800
Epoch 64/80: current_loss=0.03382 | best_loss=0.02800
Epoch 65/80: current_loss=0.03140 | best_loss=0.02800
Epoch 66/80: current_loss=0.02874 | best_loss=0.02800
Epoch 67/80: current_loss=0.02911 | best_loss=0.02800
Epoch 68/80: current_loss=0.02862 | best_loss=0.02800
Epoch 69/80: current_loss=0.02846 | best_loss=0.02800
Epoch 70/80: current_loss=0.02933 | best_loss=0.02800
Epoch 71/80: current_loss=0.03054 | best_loss=0.02800
Epoch 72/80: current_loss=0.02842 | best_loss=0.02800
Epoch 73/80: current_loss=0.02864 | best_loss=0.02800
Epoch 74/80: current_loss=0.02807 | best_loss=0.02800
Epoch 75/80: current_loss=0.02806 | best_loss=0.02800
Epoch 76/80: current_loss=0.02820 | best_loss=0.02800
Epoch 77/80: current_loss=0.02814 | best_loss=0.02800
Epoch 78/80: current_loss=0.02801 | best_loss=0.02800
Epoch 79/80: current_loss=0.02804 | best_loss=0.02800
      explained_var=0.00045 | mse_loss=0.02836
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03173 | best_loss=0.03173
Epoch 1/80: current_loss=0.03322 | best_loss=0.03173
Epoch 2/80: current_loss=0.03294 | best_loss=0.03173
Epoch 3/80: current_loss=0.03294 | best_loss=0.03173
Epoch 4/80: current_loss=0.03273 | best_loss=0.03173
Epoch 5/80: current_loss=0.03318 | best_loss=0.03173
Epoch 6/80: current_loss=0.03438 | best_loss=0.03173
Epoch 7/80: current_loss=0.03283 | best_loss=0.03173
Epoch 8/80: current_loss=0.03284 | best_loss=0.03173
Epoch 9/80: current_loss=0.03285 | best_loss=0.03173
Epoch 10/80: current_loss=0.03389 | best_loss=0.03173
Epoch 11/80: current_loss=0.03393 | best_loss=0.03173
Epoch 12/80: current_loss=0.03329 | best_loss=0.03173
Epoch 13/80: current_loss=0.03333 | best_loss=0.03173
Epoch 14/80: current_loss=0.03283 | best_loss=0.03173
Epoch 15/80: current_loss=0.03340 | best_loss=0.03173
Epoch 16/80: current_loss=0.03373 | best_loss=0.03173
Epoch 17/80: current_loss=0.03405 | best_loss=0.03173
Epoch 18/80: current_loss=0.03323 | best_loss=0.03173
Epoch 19/80: current_loss=0.03241 | best_loss=0.03173
Epoch 20/80: current_loss=0.03297 | best_loss=0.03173
Early Stopping at epoch 20
      explained_var=0.00122 | mse_loss=0.03249
----------------------------------------------
Average early_stopping_point: 27| avg_exp_var=0.02837| avg_loss=0.02807
----------------------------------------------


----------------------------------------------
Params for Trial 70
{'learning_rate': 0.001, 'weight_decay': 0.0011353189314725343, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02953 | best_loss=0.02953
Epoch 1/80: current_loss=0.02573 | best_loss=0.02573
Epoch 2/80: current_loss=0.02704 | best_loss=0.02573
Epoch 3/80: current_loss=0.02851 | best_loss=0.02573
Epoch 4/80: current_loss=0.02638 | best_loss=0.02573
Epoch 5/80: current_loss=0.02469 | best_loss=0.02469
Epoch 6/80: current_loss=0.02713 | best_loss=0.02469
Epoch 7/80: current_loss=0.02774 | best_loss=0.02469
Epoch 8/80: current_loss=0.02571 | best_loss=0.02469
Epoch 9/80: current_loss=0.02554 | best_loss=0.02469
Epoch 10/80: current_loss=0.02581 | best_loss=0.02469
Epoch 11/80: current_loss=0.02743 | best_loss=0.02469
Epoch 12/80: current_loss=0.02771 | best_loss=0.02469
Epoch 13/80: current_loss=0.02790 | best_loss=0.02469
Epoch 14/80: current_loss=0.03369 | best_loss=0.02469
Epoch 15/80: current_loss=0.02700 | best_loss=0.02469
Epoch 16/80: current_loss=0.02839 | best_loss=0.02469
Epoch 17/80: current_loss=0.02584 | best_loss=0.02469
Epoch 18/80: current_loss=0.02676 | best_loss=0.02469
Epoch 19/80: current_loss=0.03241 | best_loss=0.02469
Epoch 20/80: current_loss=0.02721 | best_loss=0.02469
Epoch 21/80: current_loss=0.02816 | best_loss=0.02469
Epoch 22/80: current_loss=0.02650 | best_loss=0.02469
Epoch 23/80: current_loss=0.02597 | best_loss=0.02469
Epoch 24/80: current_loss=0.02651 | best_loss=0.02469
Epoch 25/80: current_loss=0.02942 | best_loss=0.02469
Early Stopping at epoch 25
      explained_var=0.07181 | mse_loss=0.02409
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03420 | best_loss=0.03420
Epoch 1/80: current_loss=0.04253 | best_loss=0.03420
Epoch 2/80: current_loss=0.02824 | best_loss=0.02824
Epoch 3/80: current_loss=0.02945 | best_loss=0.02824
Epoch 4/80: current_loss=0.02811 | best_loss=0.02811
Epoch 5/80: current_loss=0.02841 | best_loss=0.02811
Epoch 6/80: current_loss=0.03182 | best_loss=0.02811
Epoch 7/80: current_loss=0.02812 | best_loss=0.02811
Epoch 8/80: current_loss=0.02849 | best_loss=0.02811
Epoch 9/80: current_loss=0.02950 | best_loss=0.02811
Epoch 10/80: current_loss=0.02877 | best_loss=0.02811
Epoch 11/80: current_loss=0.02806 | best_loss=0.02806
Epoch 12/80: current_loss=0.02799 | best_loss=0.02799
Epoch 13/80: current_loss=0.02860 | best_loss=0.02799
Epoch 14/80: current_loss=0.02876 | best_loss=0.02799
Epoch 15/80: current_loss=0.02878 | best_loss=0.02799
Epoch 16/80: current_loss=0.02817 | best_loss=0.02799
Epoch 17/80: current_loss=0.02865 | best_loss=0.02799
Epoch 18/80: current_loss=0.02809 | best_loss=0.02799
Epoch 19/80: current_loss=0.02818 | best_loss=0.02799
Epoch 20/80: current_loss=0.03112 | best_loss=0.02799
Epoch 21/80: current_loss=0.02849 | best_loss=0.02799
Epoch 22/80: current_loss=0.03067 | best_loss=0.02799
Epoch 23/80: current_loss=0.02821 | best_loss=0.02799
Epoch 24/80: current_loss=0.02942 | best_loss=0.02799
Epoch 25/80: current_loss=0.02807 | best_loss=0.02799
Epoch 26/80: current_loss=0.02874 | best_loss=0.02799
Epoch 27/80: current_loss=0.02833 | best_loss=0.02799
Epoch 28/80: current_loss=0.02802 | best_loss=0.02799
Epoch 29/80: current_loss=0.02935 | best_loss=0.02799
Epoch 30/80: current_loss=0.02800 | best_loss=0.02799
Epoch 31/80: current_loss=0.02800 | best_loss=0.02799
Epoch 32/80: current_loss=0.03049 | best_loss=0.02799
Early Stopping at epoch 32
      explained_var=0.03211 | mse_loss=0.02748
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03166 | best_loss=0.03166
Epoch 1/80: current_loss=0.03098 | best_loss=0.03098
Epoch 2/80: current_loss=0.03231 | best_loss=0.03098
Epoch 3/80: current_loss=0.02951 | best_loss=0.02951
Epoch 4/80: current_loss=0.03137 | best_loss=0.02951
Epoch 5/80: current_loss=0.02975 | best_loss=0.02951
Epoch 6/80: current_loss=0.02945 | best_loss=0.02945
Epoch 7/80: current_loss=0.03028 | best_loss=0.02945
Epoch 8/80: current_loss=0.03111 | best_loss=0.02945
Epoch 9/80: current_loss=0.03799 | best_loss=0.02945
Epoch 10/80: current_loss=0.03636 | best_loss=0.02945
Epoch 11/80: current_loss=0.03110 | best_loss=0.02945
Epoch 12/80: current_loss=0.03761 | best_loss=0.02945
Epoch 13/80: current_loss=0.03200 | best_loss=0.02945
Epoch 14/80: current_loss=0.03638 | best_loss=0.02945
Epoch 15/80: current_loss=0.03434 | best_loss=0.02945
Epoch 16/80: current_loss=0.03599 | best_loss=0.02945
Epoch 17/80: current_loss=0.03150 | best_loss=0.02945
Epoch 18/80: current_loss=0.03092 | best_loss=0.02945
Epoch 19/80: current_loss=0.02959 | best_loss=0.02945
Epoch 20/80: current_loss=0.03224 | best_loss=0.02945
Epoch 21/80: current_loss=0.03029 | best_loss=0.02945
Epoch 22/80: current_loss=0.03203 | best_loss=0.02945
Epoch 23/80: current_loss=0.02977 | best_loss=0.02945
Epoch 24/80: current_loss=0.03020 | best_loss=0.02945
Epoch 25/80: current_loss=0.02981 | best_loss=0.02945
Epoch 26/80: current_loss=0.02954 | best_loss=0.02945
Early Stopping at epoch 26
      explained_var=0.03164 | mse_loss=0.02867
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02841 | best_loss=0.02841
Epoch 1/80: current_loss=0.02864 | best_loss=0.02841
Epoch 2/80: current_loss=0.02812 | best_loss=0.02812
Epoch 3/80: current_loss=0.02810 | best_loss=0.02810
Epoch 4/80: current_loss=0.02802 | best_loss=0.02802
Epoch 5/80: current_loss=0.02824 | best_loss=0.02802
Epoch 6/80: current_loss=0.02868 | best_loss=0.02802
Epoch 7/80: current_loss=0.02930 | best_loss=0.02802
Epoch 8/80: current_loss=0.02853 | best_loss=0.02802
Epoch 9/80: current_loss=0.02844 | best_loss=0.02802
Epoch 10/80: current_loss=0.02933 | best_loss=0.02802
Epoch 11/80: current_loss=0.02866 | best_loss=0.02802
Epoch 12/80: current_loss=0.02858 | best_loss=0.02802
Epoch 13/80: current_loss=0.02828 | best_loss=0.02802
Epoch 14/80: current_loss=0.03067 | best_loss=0.02802
Epoch 15/80: current_loss=0.02910 | best_loss=0.02802
Epoch 16/80: current_loss=0.02864 | best_loss=0.02802
Epoch 17/80: current_loss=0.02931 | best_loss=0.02802
Epoch 18/80: current_loss=0.02834 | best_loss=0.02802
Epoch 19/80: current_loss=0.02912 | best_loss=0.02802
Epoch 20/80: current_loss=0.02885 | best_loss=0.02802
Epoch 21/80: current_loss=0.02836 | best_loss=0.02802
Epoch 22/80: current_loss=0.02792 | best_loss=0.02792
Epoch 23/80: current_loss=0.02915 | best_loss=0.02792
Epoch 24/80: current_loss=0.02888 | best_loss=0.02792
Epoch 25/80: current_loss=0.02853 | best_loss=0.02792
Epoch 26/80: current_loss=0.02867 | best_loss=0.02792
Epoch 27/80: current_loss=0.02889 | best_loss=0.02792
Epoch 28/80: current_loss=0.02823 | best_loss=0.02792
Epoch 29/80: current_loss=0.02829 | best_loss=0.02792
Epoch 30/80: current_loss=0.02816 | best_loss=0.02792
Epoch 31/80: current_loss=0.02817 | best_loss=0.02792
Epoch 32/80: current_loss=0.03060 | best_loss=0.02792
Epoch 33/80: current_loss=0.02921 | best_loss=0.02792
Epoch 34/80: current_loss=0.02904 | best_loss=0.02792
Epoch 35/80: current_loss=0.02854 | best_loss=0.02792
Epoch 36/80: current_loss=0.03173 | best_loss=0.02792
Epoch 37/80: current_loss=0.03012 | best_loss=0.02792
Epoch 38/80: current_loss=0.03084 | best_loss=0.02792
Epoch 39/80: current_loss=0.03010 | best_loss=0.02792
Epoch 40/80: current_loss=0.03027 | best_loss=0.02792
Epoch 41/80: current_loss=0.02984 | best_loss=0.02792
Epoch 42/80: current_loss=0.02871 | best_loss=0.02792
Early Stopping at epoch 42
      explained_var=0.00206 | mse_loss=0.02830
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03440 | best_loss=0.03440
Epoch 1/80: current_loss=0.03393 | best_loss=0.03393
Epoch 2/80: current_loss=0.03362 | best_loss=0.03362
Epoch 3/80: current_loss=0.03411 | best_loss=0.03362
Epoch 4/80: current_loss=0.03399 | best_loss=0.03362
Epoch 5/80: current_loss=0.03456 | best_loss=0.03362
Epoch 6/80: current_loss=0.03400 | best_loss=0.03362
Epoch 7/80: current_loss=0.03447 | best_loss=0.03362
Epoch 8/80: current_loss=0.03527 | best_loss=0.03362
Epoch 9/80: current_loss=0.03446 | best_loss=0.03362
Epoch 10/80: current_loss=0.03443 | best_loss=0.03362
Epoch 11/80: current_loss=0.03516 | best_loss=0.03362
Epoch 12/80: current_loss=0.03388 | best_loss=0.03362
Epoch 13/80: current_loss=0.03347 | best_loss=0.03347
Epoch 14/80: current_loss=0.03287 | best_loss=0.03287
Epoch 15/80: current_loss=0.03628 | best_loss=0.03287
Epoch 16/80: current_loss=0.03373 | best_loss=0.03287
Epoch 17/80: current_loss=0.03469 | best_loss=0.03287
Epoch 18/80: current_loss=0.03399 | best_loss=0.03287
Epoch 19/80: current_loss=0.03440 | best_loss=0.03287
Epoch 20/80: current_loss=0.03375 | best_loss=0.03287
Epoch 21/80: current_loss=0.03416 | best_loss=0.03287
Epoch 22/80: current_loss=0.03346 | best_loss=0.03287
Epoch 23/80: current_loss=0.03405 | best_loss=0.03287
Epoch 24/80: current_loss=0.03316 | best_loss=0.03287
Epoch 25/80: current_loss=0.03420 | best_loss=0.03287
Epoch 26/80: current_loss=0.03334 | best_loss=0.03287
Epoch 27/80: current_loss=0.03586 | best_loss=0.03287
Epoch 28/80: current_loss=0.03379 | best_loss=0.03287
Epoch 29/80: current_loss=0.03327 | best_loss=0.03287
Epoch 30/80: current_loss=0.03492 | best_loss=0.03287
Epoch 31/80: current_loss=0.03454 | best_loss=0.03287
Epoch 32/80: current_loss=0.03469 | best_loss=0.03287
Epoch 33/80: current_loss=0.03359 | best_loss=0.03287
Epoch 34/80: current_loss=0.03398 | best_loss=0.03287
Early Stopping at epoch 34
      explained_var=-0.03503 | mse_loss=0.03363
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.02052| avg_loss=0.02843
----------------------------------------------


----------------------------------------------
Params for Trial 71
{'learning_rate': 0.001, 'weight_decay': 0.00032736291366959906, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03276 | best_loss=0.03276
Epoch 1/80: current_loss=0.02996 | best_loss=0.02996
Epoch 2/80: current_loss=0.02490 | best_loss=0.02490
Epoch 3/80: current_loss=0.02779 | best_loss=0.02490
Epoch 4/80: current_loss=0.02723 | best_loss=0.02490
Epoch 5/80: current_loss=0.02941 | best_loss=0.02490
Epoch 6/80: current_loss=0.02950 | best_loss=0.02490
Epoch 7/80: current_loss=0.04103 | best_loss=0.02490
Epoch 8/80: current_loss=0.03515 | best_loss=0.02490
Epoch 9/80: current_loss=0.02697 | best_loss=0.02490
Epoch 10/80: current_loss=0.02581 | best_loss=0.02490
Epoch 11/80: current_loss=0.02871 | best_loss=0.02490
Epoch 12/80: current_loss=0.02757 | best_loss=0.02490
Epoch 13/80: current_loss=0.03681 | best_loss=0.02490
Epoch 14/80: current_loss=0.03218 | best_loss=0.02490
Epoch 15/80: current_loss=0.02904 | best_loss=0.02490
Epoch 16/80: current_loss=0.03341 | best_loss=0.02490
Epoch 17/80: current_loss=0.02827 | best_loss=0.02490
Epoch 18/80: current_loss=0.03253 | best_loss=0.02490
Epoch 19/80: current_loss=0.02722 | best_loss=0.02490
Epoch 20/80: current_loss=0.03110 | best_loss=0.02490
Epoch 21/80: current_loss=0.02592 | best_loss=0.02490
Epoch 22/80: current_loss=0.02548 | best_loss=0.02490
Early Stopping at epoch 22
      explained_var=0.07281 | mse_loss=0.02444
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03151 | best_loss=0.03151
Epoch 1/80: current_loss=0.03198 | best_loss=0.03151
Epoch 2/80: current_loss=0.03036 | best_loss=0.03036
Epoch 3/80: current_loss=0.02832 | best_loss=0.02832
Epoch 4/80: current_loss=0.02828 | best_loss=0.02828
Epoch 5/80: current_loss=0.03268 | best_loss=0.02828
Epoch 6/80: current_loss=0.02860 | best_loss=0.02828
Epoch 7/80: current_loss=0.02896 | best_loss=0.02828
Epoch 8/80: current_loss=0.02947 | best_loss=0.02828
Epoch 9/80: current_loss=0.03039 | best_loss=0.02828
Epoch 10/80: current_loss=0.02794 | best_loss=0.02794
Epoch 11/80: current_loss=0.02968 | best_loss=0.02794
Epoch 12/80: current_loss=0.02886 | best_loss=0.02794
Epoch 13/80: current_loss=0.02805 | best_loss=0.02794
Epoch 14/80: current_loss=0.02958 | best_loss=0.02794
Epoch 15/80: current_loss=0.02776 | best_loss=0.02776
Epoch 16/80: current_loss=0.03023 | best_loss=0.02776
Epoch 17/80: current_loss=0.02748 | best_loss=0.02748
Epoch 18/80: current_loss=0.02814 | best_loss=0.02748
Epoch 19/80: current_loss=0.02965 | best_loss=0.02748
Epoch 20/80: current_loss=0.02708 | best_loss=0.02708
Epoch 21/80: current_loss=0.02775 | best_loss=0.02708
Epoch 22/80: current_loss=0.02826 | best_loss=0.02708
Epoch 23/80: current_loss=0.02860 | best_loss=0.02708
Epoch 24/80: current_loss=0.03407 | best_loss=0.02708
Epoch 25/80: current_loss=0.02958 | best_loss=0.02708
Epoch 26/80: current_loss=0.02822 | best_loss=0.02708
Epoch 27/80: current_loss=0.02796 | best_loss=0.02708
Epoch 28/80: current_loss=0.02826 | best_loss=0.02708
Epoch 29/80: current_loss=0.03218 | best_loss=0.02708
Epoch 30/80: current_loss=0.02794 | best_loss=0.02708
Epoch 31/80: current_loss=0.02801 | best_loss=0.02708
Epoch 32/80: current_loss=0.02775 | best_loss=0.02708
Epoch 33/80: current_loss=0.02823 | best_loss=0.02708
Epoch 34/80: current_loss=0.02820 | best_loss=0.02708
Epoch 35/80: current_loss=0.02843 | best_loss=0.02708
Epoch 36/80: current_loss=0.02915 | best_loss=0.02708
Epoch 37/80: current_loss=0.03435 | best_loss=0.02708
Epoch 38/80: current_loss=0.03156 | best_loss=0.02708
Epoch 39/80: current_loss=0.03187 | best_loss=0.02708
Epoch 40/80: current_loss=0.02861 | best_loss=0.02708
Early Stopping at epoch 40
      explained_var=0.04387 | mse_loss=0.02663
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03264 | best_loss=0.03264
Epoch 1/80: current_loss=0.04006 | best_loss=0.03264
Epoch 2/80: current_loss=0.03219 | best_loss=0.03219
Epoch 3/80: current_loss=0.02978 | best_loss=0.02978
Epoch 4/80: current_loss=0.02972 | best_loss=0.02972
Epoch 5/80: current_loss=0.03305 | best_loss=0.02972
Epoch 6/80: current_loss=0.03009 | best_loss=0.02972
Epoch 7/80: current_loss=0.03031 | best_loss=0.02972
Epoch 8/80: current_loss=0.03083 | best_loss=0.02972
Epoch 9/80: current_loss=0.03459 | best_loss=0.02972
Epoch 10/80: current_loss=0.03474 | best_loss=0.02972
Epoch 11/80: current_loss=0.03081 | best_loss=0.02972
Epoch 12/80: current_loss=0.03392 | best_loss=0.02972
Epoch 13/80: current_loss=0.03081 | best_loss=0.02972
Epoch 14/80: current_loss=0.03007 | best_loss=0.02972
Epoch 15/80: current_loss=0.02980 | best_loss=0.02972
Epoch 16/80: current_loss=0.02918 | best_loss=0.02918
Epoch 17/80: current_loss=0.02935 | best_loss=0.02918
Epoch 18/80: current_loss=0.02945 | best_loss=0.02918
Epoch 19/80: current_loss=0.03091 | best_loss=0.02918
Epoch 20/80: current_loss=0.03336 | best_loss=0.02918
Epoch 21/80: current_loss=0.03018 | best_loss=0.02918
Epoch 22/80: current_loss=0.03363 | best_loss=0.02918
Epoch 23/80: current_loss=0.03028 | best_loss=0.02918
Epoch 24/80: current_loss=0.03004 | best_loss=0.02918
Epoch 25/80: current_loss=0.03233 | best_loss=0.02918
Epoch 26/80: current_loss=0.03171 | best_loss=0.02918
Epoch 27/80: current_loss=0.03085 | best_loss=0.02918
Epoch 28/80: current_loss=0.03300 | best_loss=0.02918
Epoch 29/80: current_loss=0.03304 | best_loss=0.02918
Epoch 30/80: current_loss=0.03362 | best_loss=0.02918
Epoch 31/80: current_loss=0.03197 | best_loss=0.02918
Epoch 32/80: current_loss=0.03013 | best_loss=0.02918
Epoch 33/80: current_loss=0.03244 | best_loss=0.02918
Epoch 34/80: current_loss=0.03183 | best_loss=0.02918
Epoch 35/80: current_loss=0.02959 | best_loss=0.02918
Epoch 36/80: current_loss=0.03189 | best_loss=0.02918
Early Stopping at epoch 36
      explained_var=0.05029 | mse_loss=0.02843
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02810 | best_loss=0.02810
Epoch 1/80: current_loss=0.02822 | best_loss=0.02810
Epoch 2/80: current_loss=0.02849 | best_loss=0.02810
Epoch 3/80: current_loss=0.02845 | best_loss=0.02810
Epoch 4/80: current_loss=0.03117 | best_loss=0.02810
Epoch 5/80: current_loss=0.02936 | best_loss=0.02810
Epoch 6/80: current_loss=0.02948 | best_loss=0.02810
Epoch 7/80: current_loss=0.02887 | best_loss=0.02810
Epoch 8/80: current_loss=0.02946 | best_loss=0.02810
Epoch 9/80: current_loss=0.03037 | best_loss=0.02810
Epoch 10/80: current_loss=0.03111 | best_loss=0.02810
Epoch 11/80: current_loss=0.03120 | best_loss=0.02810
Epoch 12/80: current_loss=0.02797 | best_loss=0.02797
Epoch 13/80: current_loss=0.02834 | best_loss=0.02797
Epoch 14/80: current_loss=0.02850 | best_loss=0.02797
Epoch 15/80: current_loss=0.02871 | best_loss=0.02797
Epoch 16/80: current_loss=0.02867 | best_loss=0.02797
Epoch 17/80: current_loss=0.02841 | best_loss=0.02797
Epoch 18/80: current_loss=0.02856 | best_loss=0.02797
Epoch 19/80: current_loss=0.02877 | best_loss=0.02797
Epoch 20/80: current_loss=0.02842 | best_loss=0.02797
Epoch 21/80: current_loss=0.02864 | best_loss=0.02797
Epoch 22/80: current_loss=0.02954 | best_loss=0.02797
Epoch 23/80: current_loss=0.02812 | best_loss=0.02797
Epoch 24/80: current_loss=0.02907 | best_loss=0.02797
Epoch 25/80: current_loss=0.02862 | best_loss=0.02797
Epoch 26/80: current_loss=0.02865 | best_loss=0.02797
Epoch 27/80: current_loss=0.02991 | best_loss=0.02797
Epoch 28/80: current_loss=0.02863 | best_loss=0.02797
Epoch 29/80: current_loss=0.02810 | best_loss=0.02797
Epoch 30/80: current_loss=0.03318 | best_loss=0.02797
Epoch 31/80: current_loss=0.02871 | best_loss=0.02797
Epoch 32/80: current_loss=1.47763 | best_loss=0.02797
Early Stopping at epoch 32
      explained_var=0.00009 | mse_loss=0.02837
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.07382 | best_loss=0.07382
Epoch 1/80: current_loss=0.03351 | best_loss=0.03351
Epoch 2/80: current_loss=0.03252 | best_loss=0.03252
Epoch 3/80: current_loss=0.03328 | best_loss=0.03252
Epoch 4/80: current_loss=0.03338 | best_loss=0.03252
Epoch 5/80: current_loss=0.03326 | best_loss=0.03252
Epoch 6/80: current_loss=0.03383 | best_loss=0.03252
Epoch 7/80: current_loss=0.03319 | best_loss=0.03252
Epoch 8/80: current_loss=0.03314 | best_loss=0.03252
Epoch 9/80: current_loss=0.03357 | best_loss=0.03252
Epoch 10/80: current_loss=0.03334 | best_loss=0.03252
Epoch 11/80: current_loss=0.03324 | best_loss=0.03252
Epoch 12/80: current_loss=0.03302 | best_loss=0.03252
Epoch 13/80: current_loss=0.03330 | best_loss=0.03252
Epoch 14/80: current_loss=0.03348 | best_loss=0.03252
Epoch 15/80: current_loss=0.03288 | best_loss=0.03252
Epoch 16/80: current_loss=0.03319 | best_loss=0.03252
Epoch 17/80: current_loss=0.03412 | best_loss=0.03252
Epoch 18/80: current_loss=0.03429 | best_loss=0.03252
Epoch 19/80: current_loss=0.03385 | best_loss=0.03252
Epoch 20/80: current_loss=0.03374 | best_loss=0.03252
Epoch 21/80: current_loss=0.03361 | best_loss=0.03252
Epoch 22/80: current_loss=0.03352 | best_loss=0.03252
Early Stopping at epoch 22
      explained_var=-0.00799 | mse_loss=0.03296
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.03182| avg_loss=0.02816
----------------------------------------------


----------------------------------------------
Params for Trial 72
{'learning_rate': 0.001, 'weight_decay': 0.00040236505180497943, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02945 | best_loss=0.02945
Epoch 1/80: current_loss=0.02663 | best_loss=0.02663
Epoch 2/80: current_loss=0.02940 | best_loss=0.02663
Epoch 3/80: current_loss=0.03581 | best_loss=0.02663
Epoch 4/80: current_loss=0.03217 | best_loss=0.02663
Epoch 5/80: current_loss=0.03533 | best_loss=0.02663
Epoch 6/80: current_loss=0.02870 | best_loss=0.02663
Epoch 7/80: current_loss=0.02598 | best_loss=0.02598
Epoch 8/80: current_loss=0.02816 | best_loss=0.02598
Epoch 9/80: current_loss=0.03086 | best_loss=0.02598
Epoch 10/80: current_loss=0.02462 | best_loss=0.02462
Epoch 11/80: current_loss=0.02606 | best_loss=0.02462
Epoch 12/80: current_loss=0.02654 | best_loss=0.02462
Epoch 13/80: current_loss=0.02458 | best_loss=0.02458
Epoch 14/80: current_loss=0.02744 | best_loss=0.02458
Epoch 15/80: current_loss=0.02737 | best_loss=0.02458
Epoch 16/80: current_loss=0.02783 | best_loss=0.02458
Epoch 17/80: current_loss=0.02453 | best_loss=0.02453
Epoch 18/80: current_loss=0.02859 | best_loss=0.02453
Epoch 19/80: current_loss=0.02580 | best_loss=0.02453
Epoch 20/80: current_loss=0.02564 | best_loss=0.02453
Epoch 21/80: current_loss=0.03795 | best_loss=0.02453
Epoch 22/80: current_loss=0.03129 | best_loss=0.02453
Epoch 23/80: current_loss=0.03965 | best_loss=0.02453
Epoch 24/80: current_loss=0.03253 | best_loss=0.02453
Epoch 25/80: current_loss=0.02682 | best_loss=0.02453
Epoch 26/80: current_loss=0.02424 | best_loss=0.02424
Epoch 27/80: current_loss=0.02555 | best_loss=0.02424
Epoch 28/80: current_loss=0.03251 | best_loss=0.02424
Epoch 29/80: current_loss=0.04895 | best_loss=0.02424
Epoch 30/80: current_loss=0.02906 | best_loss=0.02424
Epoch 31/80: current_loss=0.02687 | best_loss=0.02424
Epoch 32/80: current_loss=0.02882 | best_loss=0.02424
Epoch 33/80: current_loss=0.02978 | best_loss=0.02424
Epoch 34/80: current_loss=0.03478 | best_loss=0.02424
Epoch 35/80: current_loss=0.03189 | best_loss=0.02424
Epoch 36/80: current_loss=0.03133 | best_loss=0.02424
Epoch 37/80: current_loss=0.02563 | best_loss=0.02424
Epoch 38/80: current_loss=0.02576 | best_loss=0.02424
Epoch 39/80: current_loss=0.02524 | best_loss=0.02424
Epoch 40/80: current_loss=0.02645 | best_loss=0.02424
Epoch 41/80: current_loss=0.02559 | best_loss=0.02424
Epoch 42/80: current_loss=0.02547 | best_loss=0.02424
Epoch 43/80: current_loss=0.02521 | best_loss=0.02424
Epoch 44/80: current_loss=0.02591 | best_loss=0.02424
Epoch 45/80: current_loss=0.02593 | best_loss=0.02424
Epoch 46/80: current_loss=0.02727 | best_loss=0.02424
Early Stopping at epoch 46
      explained_var=0.08119 | mse_loss=0.02375
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02850 | best_loss=0.02850
Epoch 1/80: current_loss=0.03121 | best_loss=0.02850
Epoch 2/80: current_loss=0.02901 | best_loss=0.02850
Epoch 3/80: current_loss=0.02843 | best_loss=0.02843
Epoch 4/80: current_loss=0.04937 | best_loss=0.02843
Epoch 5/80: current_loss=0.03250 | best_loss=0.02843
Epoch 6/80: current_loss=0.02887 | best_loss=0.02843
Epoch 7/80: current_loss=0.02788 | best_loss=0.02788
Epoch 8/80: current_loss=0.02824 | best_loss=0.02788
Epoch 9/80: current_loss=0.02780 | best_loss=0.02780
Epoch 10/80: current_loss=0.03011 | best_loss=0.02780
Epoch 11/80: current_loss=0.02813 | best_loss=0.02780
Epoch 12/80: current_loss=0.02817 | best_loss=0.02780
Epoch 13/80: current_loss=0.02899 | best_loss=0.02780
Epoch 14/80: current_loss=0.02796 | best_loss=0.02780
Epoch 15/80: current_loss=0.02935 | best_loss=0.02780
Epoch 16/80: current_loss=0.02841 | best_loss=0.02780
Epoch 17/80: current_loss=0.02806 | best_loss=0.02780
Epoch 18/80: current_loss=0.03133 | best_loss=0.02780
Epoch 19/80: current_loss=0.03198 | best_loss=0.02780
Epoch 20/80: current_loss=0.03157 | best_loss=0.02780
Epoch 21/80: current_loss=0.02887 | best_loss=0.02780
Epoch 22/80: current_loss=0.02817 | best_loss=0.02780
Epoch 23/80: current_loss=0.02779 | best_loss=0.02779
Epoch 24/80: current_loss=0.02827 | best_loss=0.02779
Epoch 25/80: current_loss=0.02886 | best_loss=0.02779
Epoch 26/80: current_loss=0.02747 | best_loss=0.02747
Epoch 27/80: current_loss=0.02785 | best_loss=0.02747
Epoch 28/80: current_loss=0.02816 | best_loss=0.02747
Epoch 29/80: current_loss=0.02966 | best_loss=0.02747
Epoch 30/80: current_loss=0.02808 | best_loss=0.02747
Epoch 31/80: current_loss=0.03003 | best_loss=0.02747
Epoch 32/80: current_loss=0.02845 | best_loss=0.02747
Epoch 33/80: current_loss=0.02793 | best_loss=0.02747
Epoch 34/80: current_loss=0.03100 | best_loss=0.02747
Epoch 35/80: current_loss=0.02792 | best_loss=0.02747
Epoch 36/80: current_loss=0.02848 | best_loss=0.02747
Epoch 37/80: current_loss=0.02836 | best_loss=0.02747
Epoch 38/80: current_loss=0.02854 | best_loss=0.02747
Epoch 39/80: current_loss=0.02935 | best_loss=0.02747
Epoch 40/80: current_loss=0.03109 | best_loss=0.02747
Epoch 41/80: current_loss=0.02857 | best_loss=0.02747
Epoch 42/80: current_loss=0.03065 | best_loss=0.02747
Epoch 43/80: current_loss=0.02996 | best_loss=0.02747
Epoch 44/80: current_loss=0.02899 | best_loss=0.02747
Epoch 45/80: current_loss=0.02892 | best_loss=0.02747
Epoch 46/80: current_loss=0.03000 | best_loss=0.02747
Early Stopping at epoch 46
      explained_var=0.04277 | mse_loss=0.02698
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02946 | best_loss=0.02946
Epoch 1/80: current_loss=0.03356 | best_loss=0.02946
Epoch 2/80: current_loss=0.03602 | best_loss=0.02946
Epoch 3/80: current_loss=0.02932 | best_loss=0.02932
Epoch 4/80: current_loss=0.03182 | best_loss=0.02932
Epoch 5/80: current_loss=0.03031 | best_loss=0.02932
Epoch 6/80: current_loss=0.03093 | best_loss=0.02932
Epoch 7/80: current_loss=0.03493 | best_loss=0.02932
Epoch 8/80: current_loss=0.02949 | best_loss=0.02932
Epoch 9/80: current_loss=0.03142 | best_loss=0.02932
Epoch 10/80: current_loss=0.02962 | best_loss=0.02932
Epoch 11/80: current_loss=0.03140 | best_loss=0.02932
Epoch 12/80: current_loss=0.03088 | best_loss=0.02932
Epoch 13/80: current_loss=0.03539 | best_loss=0.02932
Epoch 14/80: current_loss=0.03007 | best_loss=0.02932
Epoch 15/80: current_loss=0.02955 | best_loss=0.02932
Epoch 16/80: current_loss=0.02986 | best_loss=0.02932
Epoch 17/80: current_loss=0.02986 | best_loss=0.02932
Epoch 18/80: current_loss=0.02940 | best_loss=0.02932
Epoch 19/80: current_loss=0.03070 | best_loss=0.02932
Epoch 20/80: current_loss=0.02994 | best_loss=0.02932
Epoch 21/80: current_loss=0.02978 | best_loss=0.02932
Epoch 22/80: current_loss=0.03054 | best_loss=0.02932
Epoch 23/80: current_loss=0.03000 | best_loss=0.02932
Early Stopping at epoch 23
      explained_var=0.03651 | mse_loss=0.02859
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02928 | best_loss=0.02928
Epoch 1/80: current_loss=0.02906 | best_loss=0.02906
Epoch 2/80: current_loss=0.02972 | best_loss=0.02906
Epoch 3/80: current_loss=0.02896 | best_loss=0.02896
Epoch 4/80: current_loss=0.02815 | best_loss=0.02815
Epoch 5/80: current_loss=0.02895 | best_loss=0.02815
Epoch 6/80: current_loss=0.02992 | best_loss=0.02815
Epoch 7/80: current_loss=0.02830 | best_loss=0.02815
Epoch 8/80: current_loss=0.02845 | best_loss=0.02815
Epoch 9/80: current_loss=0.03106 | best_loss=0.02815
Epoch 10/80: current_loss=0.02812 | best_loss=0.02812
Epoch 11/80: current_loss=0.02918 | best_loss=0.02812
Epoch 12/80: current_loss=0.02877 | best_loss=0.02812
Epoch 13/80: current_loss=0.02974 | best_loss=0.02812
Epoch 14/80: current_loss=0.02789 | best_loss=0.02789
Epoch 15/80: current_loss=0.02909 | best_loss=0.02789
Epoch 16/80: current_loss=0.02766 | best_loss=0.02766
Epoch 17/80: current_loss=0.02991 | best_loss=0.02766
Epoch 18/80: current_loss=0.03095 | best_loss=0.02766
Epoch 19/80: current_loss=0.02930 | best_loss=0.02766
Epoch 20/80: current_loss=0.02843 | best_loss=0.02766
Epoch 21/80: current_loss=0.02798 | best_loss=0.02766
Epoch 22/80: current_loss=0.02884 | best_loss=0.02766
Epoch 23/80: current_loss=0.02980 | best_loss=0.02766
Epoch 24/80: current_loss=0.02927 | best_loss=0.02766
Epoch 25/80: current_loss=0.03046 | best_loss=0.02766
Epoch 26/80: current_loss=0.03031 | best_loss=0.02766
Epoch 27/80: current_loss=0.02876 | best_loss=0.02766
Epoch 28/80: current_loss=0.03301 | best_loss=0.02766
Epoch 29/80: current_loss=0.02955 | best_loss=0.02766
Epoch 30/80: current_loss=0.02823 | best_loss=0.02766
Epoch 31/80: current_loss=0.02812 | best_loss=0.02766
Epoch 32/80: current_loss=0.03104 | best_loss=0.02766
Epoch 33/80: current_loss=0.02911 | best_loss=0.02766
Epoch 34/80: current_loss=0.03010 | best_loss=0.02766
Epoch 35/80: current_loss=0.02897 | best_loss=0.02766
Epoch 36/80: current_loss=0.03045 | best_loss=0.02766
Early Stopping at epoch 36
      explained_var=0.01249 | mse_loss=0.02801
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03305 | best_loss=0.03305
Epoch 1/80: current_loss=0.03245 | best_loss=0.03245
Epoch 2/80: current_loss=0.03469 | best_loss=0.03245
Epoch 3/80: current_loss=0.03434 | best_loss=0.03245
Epoch 4/80: current_loss=0.03588 | best_loss=0.03245
Epoch 5/80: current_loss=0.03561 | best_loss=0.03245
Epoch 6/80: current_loss=0.03378 | best_loss=0.03245
Epoch 7/80: current_loss=0.03340 | best_loss=0.03245
Epoch 8/80: current_loss=0.03454 | best_loss=0.03245
Epoch 9/80: current_loss=0.03367 | best_loss=0.03245
Epoch 10/80: current_loss=0.03308 | best_loss=0.03245
Epoch 11/80: current_loss=0.03392 | best_loss=0.03245
Epoch 12/80: current_loss=0.03485 | best_loss=0.03245
Epoch 13/80: current_loss=0.03510 | best_loss=0.03245
Epoch 14/80: current_loss=0.03404 | best_loss=0.03245
Epoch 15/80: current_loss=0.03452 | best_loss=0.03245
Epoch 16/80: current_loss=0.03346 | best_loss=0.03245
Epoch 17/80: current_loss=0.03282 | best_loss=0.03245
Epoch 18/80: current_loss=0.03293 | best_loss=0.03245
Epoch 19/80: current_loss=0.03307 | best_loss=0.03245
Epoch 20/80: current_loss=0.03375 | best_loss=0.03245
Epoch 21/80: current_loss=0.03387 | best_loss=0.03245
Early Stopping at epoch 21
      explained_var=-0.01934 | mse_loss=0.03313
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.03072| avg_loss=0.02809
----------------------------------------------


----------------------------------------------
Params for Trial 73
{'learning_rate': 0.001, 'weight_decay': 0.00041034917527452386, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02790 | best_loss=0.02790
Epoch 1/80: current_loss=0.02600 | best_loss=0.02600
Epoch 2/80: current_loss=0.03485 | best_loss=0.02600
Epoch 3/80: current_loss=0.03064 | best_loss=0.02600
Epoch 4/80: current_loss=0.03064 | best_loss=0.02600
Epoch 5/80: current_loss=0.02869 | best_loss=0.02600
Epoch 6/80: current_loss=0.02912 | best_loss=0.02600
Epoch 7/80: current_loss=0.02765 | best_loss=0.02600
Epoch 8/80: current_loss=0.03014 | best_loss=0.02600
Epoch 9/80: current_loss=0.02615 | best_loss=0.02600
Epoch 10/80: current_loss=0.02564 | best_loss=0.02564
Epoch 11/80: current_loss=0.02648 | best_loss=0.02564
Epoch 12/80: current_loss=0.02691 | best_loss=0.02564
Epoch 13/80: current_loss=0.02938 | best_loss=0.02564
Epoch 14/80: current_loss=0.03283 | best_loss=0.02564
Epoch 15/80: current_loss=0.03644 | best_loss=0.02564
Epoch 16/80: current_loss=0.02506 | best_loss=0.02506
Epoch 17/80: current_loss=0.02571 | best_loss=0.02506
Epoch 18/80: current_loss=0.02769 | best_loss=0.02506
Epoch 19/80: current_loss=0.02935 | best_loss=0.02506
Epoch 20/80: current_loss=0.02652 | best_loss=0.02506
Epoch 21/80: current_loss=0.02918 | best_loss=0.02506
Epoch 22/80: current_loss=0.02778 | best_loss=0.02506
Epoch 23/80: current_loss=0.02480 | best_loss=0.02480
Epoch 24/80: current_loss=0.02539 | best_loss=0.02480
Epoch 25/80: current_loss=0.02892 | best_loss=0.02480
Epoch 26/80: current_loss=0.02695 | best_loss=0.02480
Epoch 27/80: current_loss=0.02787 | best_loss=0.02480
Epoch 28/80: current_loss=0.02900 | best_loss=0.02480
Epoch 29/80: current_loss=0.02590 | best_loss=0.02480
Epoch 30/80: current_loss=0.02674 | best_loss=0.02480
Epoch 31/80: current_loss=0.02772 | best_loss=0.02480
Epoch 32/80: current_loss=0.02871 | best_loss=0.02480
Epoch 33/80: current_loss=0.03500 | best_loss=0.02480
Epoch 34/80: current_loss=0.03116 | best_loss=0.02480
Epoch 35/80: current_loss=0.03177 | best_loss=0.02480
Epoch 36/80: current_loss=0.03066 | best_loss=0.02480
Epoch 37/80: current_loss=0.02646 | best_loss=0.02480
Epoch 38/80: current_loss=0.02805 | best_loss=0.02480
Epoch 39/80: current_loss=0.02650 | best_loss=0.02480
Epoch 40/80: current_loss=0.02573 | best_loss=0.02480
Epoch 41/80: current_loss=0.02633 | best_loss=0.02480
Epoch 42/80: current_loss=0.02632 | best_loss=0.02480
Epoch 43/80: current_loss=0.02683 | best_loss=0.02480
Early Stopping at epoch 43
      explained_var=0.06038 | mse_loss=0.02432
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02913 | best_loss=0.02913
Epoch 1/80: current_loss=0.03093 | best_loss=0.02913
Epoch 2/80: current_loss=0.03182 | best_loss=0.02913
Epoch 3/80: current_loss=0.02810 | best_loss=0.02810
Epoch 4/80: current_loss=0.02832 | best_loss=0.02810
Epoch 5/80: current_loss=0.02843 | best_loss=0.02810
Epoch 6/80: current_loss=0.02928 | best_loss=0.02810
Epoch 7/80: current_loss=0.02922 | best_loss=0.02810
Epoch 8/80: current_loss=0.02904 | best_loss=0.02810
Epoch 9/80: current_loss=0.03275 | best_loss=0.02810
Epoch 10/80: current_loss=0.03008 | best_loss=0.02810
Epoch 11/80: current_loss=0.03412 | best_loss=0.02810
Epoch 12/80: current_loss=0.03153 | best_loss=0.02810
Epoch 13/80: current_loss=0.02851 | best_loss=0.02810
Epoch 14/80: current_loss=0.03436 | best_loss=0.02810
Epoch 15/80: current_loss=0.02825 | best_loss=0.02810
Epoch 16/80: current_loss=0.02897 | best_loss=0.02810
Epoch 17/80: current_loss=0.03060 | best_loss=0.02810
Epoch 18/80: current_loss=0.02976 | best_loss=0.02810
Epoch 19/80: current_loss=0.02972 | best_loss=0.02810
Epoch 20/80: current_loss=0.02892 | best_loss=0.02810
Epoch 21/80: current_loss=0.02777 | best_loss=0.02777
Epoch 22/80: current_loss=0.02762 | best_loss=0.02762
Epoch 23/80: current_loss=0.02943 | best_loss=0.02762
Epoch 24/80: current_loss=0.02906 | best_loss=0.02762
Epoch 25/80: current_loss=0.02809 | best_loss=0.02762
Epoch 26/80: current_loss=0.02747 | best_loss=0.02747
Epoch 27/80: current_loss=0.03013 | best_loss=0.02747
Epoch 28/80: current_loss=0.02784 | best_loss=0.02747
Epoch 29/80: current_loss=0.02763 | best_loss=0.02747
Epoch 30/80: current_loss=0.02818 | best_loss=0.02747
Epoch 31/80: current_loss=0.02790 | best_loss=0.02747
Epoch 32/80: current_loss=0.02758 | best_loss=0.02747
Epoch 33/80: current_loss=0.02750 | best_loss=0.02747
Epoch 34/80: current_loss=0.02792 | best_loss=0.02747
Epoch 35/80: current_loss=0.02819 | best_loss=0.02747
Epoch 36/80: current_loss=0.02819 | best_loss=0.02747
Epoch 37/80: current_loss=0.03022 | best_loss=0.02747
Epoch 38/80: current_loss=0.02910 | best_loss=0.02747
Epoch 39/80: current_loss=0.02977 | best_loss=0.02747
Epoch 40/80: current_loss=0.02794 | best_loss=0.02747
Epoch 41/80: current_loss=0.02778 | best_loss=0.02747
Epoch 42/80: current_loss=0.02735 | best_loss=0.02735
Epoch 43/80: current_loss=0.02719 | best_loss=0.02719
Epoch 44/80: current_loss=0.02794 | best_loss=0.02719
Epoch 45/80: current_loss=0.02955 | best_loss=0.02719
Epoch 46/80: current_loss=0.02722 | best_loss=0.02719
Epoch 47/80: current_loss=0.02732 | best_loss=0.02719
Epoch 48/80: current_loss=0.02843 | best_loss=0.02719
Epoch 49/80: current_loss=0.02818 | best_loss=0.02719
Epoch 50/80: current_loss=0.02778 | best_loss=0.02719
Epoch 51/80: current_loss=0.02774 | best_loss=0.02719
Epoch 52/80: current_loss=0.03095 | best_loss=0.02719
Epoch 53/80: current_loss=0.03042 | best_loss=0.02719
Epoch 54/80: current_loss=0.02905 | best_loss=0.02719
Epoch 55/80: current_loss=0.02989 | best_loss=0.02719
Epoch 56/80: current_loss=0.02822 | best_loss=0.02719
Epoch 57/80: current_loss=0.02959 | best_loss=0.02719
Epoch 58/80: current_loss=0.02828 | best_loss=0.02719
Epoch 59/80: current_loss=0.02927 | best_loss=0.02719
Epoch 60/80: current_loss=0.03315 | best_loss=0.02719
Epoch 61/80: current_loss=0.02769 | best_loss=0.02719
Epoch 62/80: current_loss=0.02801 | best_loss=0.02719
Epoch 63/80: current_loss=0.02796 | best_loss=0.02719
Early Stopping at epoch 63
      explained_var=0.04660 | mse_loss=0.02672
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03044 | best_loss=0.03044
Epoch 1/80: current_loss=0.02993 | best_loss=0.02993
Epoch 2/80: current_loss=0.03488 | best_loss=0.02993
Epoch 3/80: current_loss=0.03153 | best_loss=0.02993
Epoch 4/80: current_loss=0.03403 | best_loss=0.02993
Epoch 5/80: current_loss=0.03898 | best_loss=0.02993
Epoch 6/80: current_loss=0.03781 | best_loss=0.02993
Epoch 7/80: current_loss=0.03636 | best_loss=0.02993
Epoch 8/80: current_loss=0.04095 | best_loss=0.02993
Epoch 9/80: current_loss=0.03402 | best_loss=0.02993
Epoch 10/80: current_loss=0.03008 | best_loss=0.02993
Epoch 11/80: current_loss=0.03178 | best_loss=0.02993
Epoch 12/80: current_loss=0.03057 | best_loss=0.02993
Epoch 13/80: current_loss=0.03027 | best_loss=0.02993
Epoch 14/80: current_loss=0.02950 | best_loss=0.02950
Epoch 15/80: current_loss=0.03462 | best_loss=0.02950
Epoch 16/80: current_loss=0.03111 | best_loss=0.02950
Epoch 17/80: current_loss=0.02950 | best_loss=0.02950
Epoch 18/80: current_loss=0.03036 | best_loss=0.02950
Epoch 19/80: current_loss=0.03008 | best_loss=0.02950
Epoch 20/80: current_loss=0.03230 | best_loss=0.02950
Epoch 21/80: current_loss=0.02941 | best_loss=0.02941
Epoch 22/80: current_loss=0.02933 | best_loss=0.02933
Epoch 23/80: current_loss=0.03185 | best_loss=0.02933
Epoch 24/80: current_loss=0.02995 | best_loss=0.02933
Epoch 25/80: current_loss=0.03522 | best_loss=0.02933
Epoch 26/80: current_loss=0.02953 | best_loss=0.02933
Epoch 27/80: current_loss=0.02996 | best_loss=0.02933
Epoch 28/80: current_loss=0.03021 | best_loss=0.02933
Epoch 29/80: current_loss=0.03006 | best_loss=0.02933
Epoch 30/80: current_loss=0.03156 | best_loss=0.02933
Epoch 31/80: current_loss=0.03345 | best_loss=0.02933
Epoch 32/80: current_loss=0.03069 | best_loss=0.02933
Epoch 33/80: current_loss=0.03093 | best_loss=0.02933
Epoch 34/80: current_loss=0.03018 | best_loss=0.02933
Epoch 35/80: current_loss=0.02986 | best_loss=0.02933
Epoch 36/80: current_loss=0.02990 | best_loss=0.02933
Epoch 37/80: current_loss=0.03019 | best_loss=0.02933
Epoch 38/80: current_loss=0.02994 | best_loss=0.02933
Epoch 39/80: current_loss=0.03096 | best_loss=0.02933
Epoch 40/80: current_loss=0.03611 | best_loss=0.02933
Epoch 41/80: current_loss=0.03127 | best_loss=0.02933
Epoch 42/80: current_loss=0.02972 | best_loss=0.02933
Early Stopping at epoch 42
      explained_var=0.03407 | mse_loss=0.02857
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02851 | best_loss=0.02851
Epoch 1/80: current_loss=0.03054 | best_loss=0.02851
Epoch 2/80: current_loss=0.02818 | best_loss=0.02818
Epoch 3/80: current_loss=0.02850 | best_loss=0.02818
Epoch 4/80: current_loss=0.02819 | best_loss=0.02818
Epoch 5/80: current_loss=0.02884 | best_loss=0.02818
Epoch 6/80: current_loss=0.02871 | best_loss=0.02818
Epoch 7/80: current_loss=0.02878 | best_loss=0.02818
Epoch 8/80: current_loss=0.02861 | best_loss=0.02818
Epoch 9/80: current_loss=0.02811 | best_loss=0.02811
Epoch 10/80: current_loss=0.03079 | best_loss=0.02811
Epoch 11/80: current_loss=0.02932 | best_loss=0.02811
Epoch 12/80: current_loss=0.19634 | best_loss=0.02811
Epoch 13/80: current_loss=2.98118 | best_loss=0.02811
Epoch 14/80: current_loss=0.11325 | best_loss=0.02811
Epoch 15/80: current_loss=0.07832 | best_loss=0.02811
Epoch 16/80: current_loss=0.06295 | best_loss=0.02811
Epoch 17/80: current_loss=0.04182 | best_loss=0.02811
Epoch 18/80: current_loss=0.04482 | best_loss=0.02811
Epoch 19/80: current_loss=0.04760 | best_loss=0.02811
Epoch 20/80: current_loss=0.03916 | best_loss=0.02811
Epoch 21/80: current_loss=0.03279 | best_loss=0.02811
Epoch 22/80: current_loss=0.02916 | best_loss=0.02811
Epoch 23/80: current_loss=0.03020 | best_loss=0.02811
Epoch 24/80: current_loss=0.03453 | best_loss=0.02811
Epoch 25/80: current_loss=0.03523 | best_loss=0.02811
Epoch 26/80: current_loss=0.03081 | best_loss=0.02811
Epoch 27/80: current_loss=0.03214 | best_loss=0.02811
Epoch 28/80: current_loss=0.03002 | best_loss=0.02811
Epoch 29/80: current_loss=0.04354 | best_loss=0.02811
Early Stopping at epoch 29
      explained_var=-0.00404 | mse_loss=0.02849
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03314 | best_loss=0.03314
Epoch 1/80: current_loss=0.04746 | best_loss=0.03314
Epoch 2/80: current_loss=0.05653 | best_loss=0.03314
Epoch 3/80: current_loss=0.04375 | best_loss=0.03314
Epoch 4/80: current_loss=0.03284 | best_loss=0.03284
Epoch 5/80: current_loss=0.03368 | best_loss=0.03284
Epoch 6/80: current_loss=0.03524 | best_loss=0.03284
Epoch 7/80: current_loss=0.03701 | best_loss=0.03284
Epoch 8/80: current_loss=0.04305 | best_loss=0.03284
Epoch 9/80: current_loss=0.03796 | best_loss=0.03284
Epoch 10/80: current_loss=0.03311 | best_loss=0.03284
Epoch 11/80: current_loss=0.03660 | best_loss=0.03284
Epoch 12/80: current_loss=0.03340 | best_loss=0.03284
Epoch 13/80: current_loss=0.04111 | best_loss=0.03284
Epoch 14/80: current_loss=0.03532 | best_loss=0.03284
Epoch 15/80: current_loss=0.03406 | best_loss=0.03284
Epoch 16/80: current_loss=0.03525 | best_loss=0.03284
Epoch 17/80: current_loss=0.04831 | best_loss=0.03284
Epoch 18/80: current_loss=0.03861 | best_loss=0.03284
Epoch 19/80: current_loss=0.03788 | best_loss=0.03284
Epoch 20/80: current_loss=0.04115 | best_loss=0.03284
Epoch 21/80: current_loss=0.03897 | best_loss=0.03284
Epoch 22/80: current_loss=0.03587 | best_loss=0.03284
Epoch 23/80: current_loss=0.03629 | best_loss=0.03284
Epoch 24/80: current_loss=0.03374 | best_loss=0.03284
Early Stopping at epoch 24
      explained_var=-0.03009 | mse_loss=0.03350
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.02138| avg_loss=0.02832
----------------------------------------------


----------------------------------------------
Params for Trial 74
{'learning_rate': 0.001, 'weight_decay': 0.0005978900717198284, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03251 | best_loss=0.03251
Epoch 1/80: current_loss=0.02839 | best_loss=0.02839
Epoch 2/80: current_loss=0.02835 | best_loss=0.02835
Epoch 3/80: current_loss=0.02462 | best_loss=0.02462
Epoch 4/80: current_loss=0.02597 | best_loss=0.02462
Epoch 5/80: current_loss=0.02819 | best_loss=0.02462
Epoch 6/80: current_loss=0.02687 | best_loss=0.02462
Epoch 7/80: current_loss=0.02682 | best_loss=0.02462
Epoch 8/80: current_loss=0.02811 | best_loss=0.02462
Epoch 9/80: current_loss=0.02586 | best_loss=0.02462
Epoch 10/80: current_loss=0.02666 | best_loss=0.02462
Epoch 11/80: current_loss=0.02745 | best_loss=0.02462
Epoch 12/80: current_loss=0.02476 | best_loss=0.02462
Epoch 13/80: current_loss=0.02595 | best_loss=0.02462
Epoch 14/80: current_loss=0.02595 | best_loss=0.02462
Epoch 15/80: current_loss=0.02861 | best_loss=0.02462
Epoch 16/80: current_loss=0.02657 | best_loss=0.02462
Epoch 17/80: current_loss=0.02668 | best_loss=0.02462
Epoch 18/80: current_loss=0.02715 | best_loss=0.02462
Epoch 19/80: current_loss=0.02630 | best_loss=0.02462
Epoch 20/80: current_loss=0.02730 | best_loss=0.02462
Epoch 21/80: current_loss=0.02628 | best_loss=0.02462
Epoch 22/80: current_loss=0.02828 | best_loss=0.02462
Epoch 23/80: current_loss=0.02680 | best_loss=0.02462
Early Stopping at epoch 23
      explained_var=0.07516 | mse_loss=0.02418
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02940 | best_loss=0.02940
Epoch 1/80: current_loss=0.03161 | best_loss=0.02940
Epoch 2/80: current_loss=0.02989 | best_loss=0.02940
Epoch 3/80: current_loss=0.02821 | best_loss=0.02821
Epoch 4/80: current_loss=0.02803 | best_loss=0.02803
Epoch 5/80: current_loss=0.02711 | best_loss=0.02711
Epoch 6/80: current_loss=0.03256 | best_loss=0.02711
Epoch 7/80: current_loss=0.03257 | best_loss=0.02711
Epoch 8/80: current_loss=0.02807 | best_loss=0.02711
Epoch 9/80: current_loss=0.02894 | best_loss=0.02711
Epoch 10/80: current_loss=0.02816 | best_loss=0.02711
Epoch 11/80: current_loss=0.03016 | best_loss=0.02711
Epoch 12/80: current_loss=0.02766 | best_loss=0.02711
Epoch 13/80: current_loss=0.02871 | best_loss=0.02711
Epoch 14/80: current_loss=0.02891 | best_loss=0.02711
Epoch 15/80: current_loss=0.02809 | best_loss=0.02711
Epoch 16/80: current_loss=0.03027 | best_loss=0.02711
Epoch 17/80: current_loss=0.02829 | best_loss=0.02711
Epoch 18/80: current_loss=0.03009 | best_loss=0.02711
Epoch 19/80: current_loss=0.03397 | best_loss=0.02711
Epoch 20/80: current_loss=0.02891 | best_loss=0.02711
Epoch 21/80: current_loss=0.02848 | best_loss=0.02711
Epoch 22/80: current_loss=0.02921 | best_loss=0.02711
Epoch 23/80: current_loss=0.02830 | best_loss=0.02711
Epoch 24/80: current_loss=0.02989 | best_loss=0.02711
Epoch 25/80: current_loss=0.02803 | best_loss=0.02711
Early Stopping at epoch 25
      explained_var=0.04677 | mse_loss=0.02663
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03162 | best_loss=0.03162
Epoch 1/80: current_loss=0.04047 | best_loss=0.03162
Epoch 2/80: current_loss=0.03301 | best_loss=0.03162
Epoch 3/80: current_loss=0.03110 | best_loss=0.03110
Epoch 4/80: current_loss=0.03036 | best_loss=0.03036
Epoch 5/80: current_loss=0.03505 | best_loss=0.03036
Epoch 6/80: current_loss=0.21394 | best_loss=0.03036
Epoch 7/80: current_loss=0.03794 | best_loss=0.03036
Epoch 8/80: current_loss=0.02949 | best_loss=0.02949
Epoch 9/80: current_loss=0.02975 | best_loss=0.02949
Epoch 10/80: current_loss=0.02992 | best_loss=0.02949
Epoch 11/80: current_loss=0.03121 | best_loss=0.02949
Epoch 12/80: current_loss=0.03260 | best_loss=0.02949
Epoch 13/80: current_loss=0.03180 | best_loss=0.02949
Epoch 14/80: current_loss=0.03118 | best_loss=0.02949
Epoch 15/80: current_loss=0.03225 | best_loss=0.02949
Epoch 16/80: current_loss=0.03045 | best_loss=0.02949
Epoch 17/80: current_loss=0.02968 | best_loss=0.02949
Epoch 18/80: current_loss=0.03173 | best_loss=0.02949
Epoch 19/80: current_loss=0.02959 | best_loss=0.02949
Epoch 20/80: current_loss=0.02946 | best_loss=0.02946
Epoch 21/80: current_loss=0.03095 | best_loss=0.02946
Epoch 22/80: current_loss=0.02952 | best_loss=0.02946
Epoch 23/80: current_loss=0.03247 | best_loss=0.02946
Epoch 24/80: current_loss=0.03083 | best_loss=0.02946
Epoch 25/80: current_loss=0.03060 | best_loss=0.02946
Epoch 26/80: current_loss=0.02976 | best_loss=0.02946
Epoch 27/80: current_loss=0.03113 | best_loss=0.02946
Epoch 28/80: current_loss=0.03355 | best_loss=0.02946
Epoch 29/80: current_loss=0.02941 | best_loss=0.02941
Epoch 30/80: current_loss=0.03199 | best_loss=0.02941
Epoch 31/80: current_loss=0.03018 | best_loss=0.02941
Epoch 32/80: current_loss=0.03089 | best_loss=0.02941
Epoch 33/80: current_loss=0.02926 | best_loss=0.02926
Epoch 34/80: current_loss=0.03034 | best_loss=0.02926
Epoch 35/80: current_loss=0.03018 | best_loss=0.02926
Epoch 36/80: current_loss=0.03021 | best_loss=0.02926
Epoch 37/80: current_loss=0.02965 | best_loss=0.02926
Epoch 38/80: current_loss=0.03022 | best_loss=0.02926
Epoch 39/80: current_loss=0.03100 | best_loss=0.02926
Epoch 40/80: current_loss=0.03131 | best_loss=0.02926
Epoch 41/80: current_loss=0.02959 | best_loss=0.02926
Epoch 42/80: current_loss=0.03037 | best_loss=0.02926
Epoch 43/80: current_loss=0.02945 | best_loss=0.02926
Epoch 44/80: current_loss=0.03114 | best_loss=0.02926
Epoch 45/80: current_loss=0.02928 | best_loss=0.02926
Epoch 46/80: current_loss=0.03211 | best_loss=0.02926
Epoch 47/80: current_loss=0.02973 | best_loss=0.02926
Epoch 48/80: current_loss=0.02985 | best_loss=0.02926
Epoch 49/80: current_loss=0.03043 | best_loss=0.02926
Epoch 50/80: current_loss=0.03007 | best_loss=0.02926
Epoch 51/80: current_loss=0.02949 | best_loss=0.02926
Epoch 52/80: current_loss=0.02958 | best_loss=0.02926
Epoch 53/80: current_loss=0.03266 | best_loss=0.02926
Early Stopping at epoch 53
      explained_var=0.03804 | mse_loss=0.02846
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02765 | best_loss=0.02765
Epoch 1/80: current_loss=0.03107 | best_loss=0.02765
Epoch 2/80: current_loss=0.02909 | best_loss=0.02765
Epoch 3/80: current_loss=0.02846 | best_loss=0.02765
Epoch 4/80: current_loss=0.02904 | best_loss=0.02765
Epoch 5/80: current_loss=0.02864 | best_loss=0.02765
Epoch 6/80: current_loss=0.02798 | best_loss=0.02765
Epoch 7/80: current_loss=0.02762 | best_loss=0.02762
Epoch 8/80: current_loss=0.02882 | best_loss=0.02762
Epoch 9/80: current_loss=0.02984 | best_loss=0.02762
Epoch 10/80: current_loss=0.02837 | best_loss=0.02762
Epoch 11/80: current_loss=0.02930 | best_loss=0.02762
Epoch 12/80: current_loss=0.02836 | best_loss=0.02762
Epoch 13/80: current_loss=0.02947 | best_loss=0.02762
Epoch 14/80: current_loss=0.03001 | best_loss=0.02762
Epoch 15/80: current_loss=0.02884 | best_loss=0.02762
Epoch 16/80: current_loss=0.02842 | best_loss=0.02762
Epoch 17/80: current_loss=0.02867 | best_loss=0.02762
Epoch 18/80: current_loss=0.02940 | best_loss=0.02762
Epoch 19/80: current_loss=0.02864 | best_loss=0.02762
Epoch 20/80: current_loss=0.03117 | best_loss=0.02762
Epoch 21/80: current_loss=0.02856 | best_loss=0.02762
Epoch 22/80: current_loss=0.02828 | best_loss=0.02762
Epoch 23/80: current_loss=0.02827 | best_loss=0.02762
Epoch 24/80: current_loss=0.03005 | best_loss=0.02762
Epoch 25/80: current_loss=0.02854 | best_loss=0.02762
Epoch 26/80: current_loss=0.02970 | best_loss=0.02762
Epoch 27/80: current_loss=0.03054 | best_loss=0.02762
Early Stopping at epoch 27
      explained_var=0.02174 | mse_loss=0.02799
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04131 | best_loss=0.04131
Epoch 1/80: current_loss=0.03497 | best_loss=0.03497
Epoch 2/80: current_loss=0.03327 | best_loss=0.03327
Epoch 3/80: current_loss=0.03352 | best_loss=0.03327
Epoch 4/80: current_loss=0.03398 | best_loss=0.03327
Epoch 5/80: current_loss=0.03333 | best_loss=0.03327
Epoch 6/80: current_loss=0.03306 | best_loss=0.03306
Epoch 7/80: current_loss=0.03540 | best_loss=0.03306
Epoch 8/80: current_loss=0.03878 | best_loss=0.03306
Epoch 9/80: current_loss=0.03480 | best_loss=0.03306
Epoch 10/80: current_loss=0.03432 | best_loss=0.03306
Epoch 11/80: current_loss=0.03315 | best_loss=0.03306
Epoch 12/80: current_loss=0.03413 | best_loss=0.03306
Epoch 13/80: current_loss=0.03382 | best_loss=0.03306
Epoch 14/80: current_loss=0.03386 | best_loss=0.03306
Epoch 15/80: current_loss=0.03411 | best_loss=0.03306
Epoch 16/80: current_loss=0.03380 | best_loss=0.03306
Epoch 17/80: current_loss=0.03314 | best_loss=0.03306
Epoch 18/80: current_loss=0.03482 | best_loss=0.03306
Epoch 19/80: current_loss=0.03337 | best_loss=0.03306
Epoch 20/80: current_loss=0.03433 | best_loss=0.03306
Epoch 21/80: current_loss=0.03428 | best_loss=0.03306
Epoch 22/80: current_loss=0.03326 | best_loss=0.03306
Epoch 23/80: current_loss=0.03402 | best_loss=0.03306
Epoch 24/80: current_loss=0.03359 | best_loss=0.03306
Epoch 25/80: current_loss=0.03425 | best_loss=0.03306
Epoch 26/80: current_loss=0.03361 | best_loss=0.03306
Early Stopping at epoch 26
      explained_var=-0.04130 | mse_loss=0.03386
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.02808| avg_loss=0.02822
----------------------------------------------


----------------------------------------------
Params for Trial 75
{'learning_rate': 0.0001, 'weight_decay': 0.00022729235465922035, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04271 | best_loss=0.04271
Epoch 1/80: current_loss=0.03033 | best_loss=0.03033
Epoch 2/80: current_loss=0.03246 | best_loss=0.03033
Epoch 3/80: current_loss=0.02883 | best_loss=0.02883
Epoch 4/80: current_loss=0.03038 | best_loss=0.02883
Epoch 5/80: current_loss=0.02793 | best_loss=0.02793
Epoch 6/80: current_loss=0.02828 | best_loss=0.02793
Epoch 7/80: current_loss=0.02792 | best_loss=0.02792
Epoch 8/80: current_loss=0.02655 | best_loss=0.02655
Epoch 9/80: current_loss=0.02780 | best_loss=0.02655
Epoch 10/80: current_loss=0.02600 | best_loss=0.02600
Epoch 11/80: current_loss=0.02634 | best_loss=0.02600
Epoch 12/80: current_loss=0.02568 | best_loss=0.02568
Epoch 13/80: current_loss=0.02692 | best_loss=0.02568
Epoch 14/80: current_loss=0.02670 | best_loss=0.02568
Epoch 15/80: current_loss=0.02674 | best_loss=0.02568
Epoch 16/80: current_loss=0.02620 | best_loss=0.02568
Epoch 17/80: current_loss=0.02747 | best_loss=0.02568
Epoch 18/80: current_loss=0.02824 | best_loss=0.02568
Epoch 19/80: current_loss=0.02615 | best_loss=0.02568
Epoch 20/80: current_loss=0.02778 | best_loss=0.02568
Epoch 21/80: current_loss=0.02614 | best_loss=0.02568
Epoch 22/80: current_loss=0.02845 | best_loss=0.02568
Epoch 23/80: current_loss=0.02574 | best_loss=0.02568
Epoch 24/80: current_loss=0.02758 | best_loss=0.02568
Epoch 25/80: current_loss=0.02773 | best_loss=0.02568
Epoch 26/80: current_loss=0.02575 | best_loss=0.02568
Epoch 27/80: current_loss=0.03017 | best_loss=0.02568
Epoch 28/80: current_loss=0.02573 | best_loss=0.02568
Epoch 29/80: current_loss=0.02712 | best_loss=0.02568
Epoch 30/80: current_loss=0.02556 | best_loss=0.02556
Epoch 31/80: current_loss=0.02724 | best_loss=0.02556
Epoch 32/80: current_loss=0.02488 | best_loss=0.02488
Epoch 33/80: current_loss=0.02875 | best_loss=0.02488
Epoch 34/80: current_loss=0.02615 | best_loss=0.02488
Epoch 35/80: current_loss=0.02756 | best_loss=0.02488
Epoch 36/80: current_loss=0.02639 | best_loss=0.02488
Epoch 37/80: current_loss=0.02709 | best_loss=0.02488
Epoch 38/80: current_loss=0.02644 | best_loss=0.02488
Epoch 39/80: current_loss=0.02528 | best_loss=0.02488
Epoch 40/80: current_loss=0.02938 | best_loss=0.02488
Epoch 41/80: current_loss=0.02637 | best_loss=0.02488
Epoch 42/80: current_loss=0.02545 | best_loss=0.02488
Epoch 43/80: current_loss=0.02572 | best_loss=0.02488
Epoch 44/80: current_loss=0.02516 | best_loss=0.02488
Epoch 45/80: current_loss=0.02777 | best_loss=0.02488
Epoch 46/80: current_loss=0.02586 | best_loss=0.02488
Epoch 47/80: current_loss=0.02706 | best_loss=0.02488
Epoch 48/80: current_loss=0.02541 | best_loss=0.02488
Epoch 49/80: current_loss=0.02847 | best_loss=0.02488
Epoch 50/80: current_loss=0.02631 | best_loss=0.02488
Epoch 51/80: current_loss=0.02629 | best_loss=0.02488
Epoch 52/80: current_loss=0.02621 | best_loss=0.02488
Early Stopping at epoch 52
      explained_var=0.05718 | mse_loss=0.02443
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02891 | best_loss=0.02891
Epoch 1/80: current_loss=0.02809 | best_loss=0.02809
Epoch 2/80: current_loss=0.02818 | best_loss=0.02809
Epoch 3/80: current_loss=0.02866 | best_loss=0.02809
Epoch 4/80: current_loss=0.02820 | best_loss=0.02809
Epoch 5/80: current_loss=0.02883 | best_loss=0.02809
Epoch 6/80: current_loss=0.02858 | best_loss=0.02809
Epoch 7/80: current_loss=0.02861 | best_loss=0.02809
Epoch 8/80: current_loss=0.02847 | best_loss=0.02809
Epoch 9/80: current_loss=0.02823 | best_loss=0.02809
Epoch 10/80: current_loss=0.02867 | best_loss=0.02809
Epoch 11/80: current_loss=0.02795 | best_loss=0.02795
Epoch 12/80: current_loss=0.02802 | best_loss=0.02795
Epoch 13/80: current_loss=0.02784 | best_loss=0.02784
Epoch 14/80: current_loss=0.02785 | best_loss=0.02784
Epoch 15/80: current_loss=0.02778 | best_loss=0.02778
Epoch 16/80: current_loss=0.02852 | best_loss=0.02778
Epoch 17/80: current_loss=0.02885 | best_loss=0.02778
Epoch 18/80: current_loss=0.02780 | best_loss=0.02778
Epoch 19/80: current_loss=0.02823 | best_loss=0.02778
Epoch 20/80: current_loss=0.02764 | best_loss=0.02764
Epoch 21/80: current_loss=0.02811 | best_loss=0.02764
Epoch 22/80: current_loss=0.02720 | best_loss=0.02720
Epoch 23/80: current_loss=0.02758 | best_loss=0.02720
Epoch 24/80: current_loss=0.02859 | best_loss=0.02720
Epoch 25/80: current_loss=0.02763 | best_loss=0.02720
Epoch 26/80: current_loss=0.02761 | best_loss=0.02720
Epoch 27/80: current_loss=0.02748 | best_loss=0.02720
Epoch 28/80: current_loss=0.02885 | best_loss=0.02720
Epoch 29/80: current_loss=0.02760 | best_loss=0.02720
Epoch 30/80: current_loss=0.02796 | best_loss=0.02720
Epoch 31/80: current_loss=0.02863 | best_loss=0.02720
Epoch 32/80: current_loss=0.02865 | best_loss=0.02720
Epoch 33/80: current_loss=0.02866 | best_loss=0.02720
Epoch 34/80: current_loss=0.02837 | best_loss=0.02720
Epoch 35/80: current_loss=0.02883 | best_loss=0.02720
Epoch 36/80: current_loss=0.02874 | best_loss=0.02720
Epoch 37/80: current_loss=0.03005 | best_loss=0.02720
Epoch 38/80: current_loss=0.02823 | best_loss=0.02720
Epoch 39/80: current_loss=0.02857 | best_loss=0.02720
Epoch 40/80: current_loss=0.02850 | best_loss=0.02720
Epoch 41/80: current_loss=0.02874 | best_loss=0.02720
Epoch 42/80: current_loss=0.02815 | best_loss=0.02720
Early Stopping at epoch 42
      explained_var=0.03691 | mse_loss=0.02679
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02962 | best_loss=0.02962
Epoch 1/80: current_loss=0.03063 | best_loss=0.02962
Epoch 2/80: current_loss=0.03067 | best_loss=0.02962
Epoch 3/80: current_loss=0.02971 | best_loss=0.02962
Epoch 4/80: current_loss=0.03324 | best_loss=0.02962
Epoch 5/80: current_loss=0.03033 | best_loss=0.02962
Epoch 6/80: current_loss=0.03108 | best_loss=0.02962
Epoch 7/80: current_loss=0.03069 | best_loss=0.02962
Epoch 8/80: current_loss=0.03042 | best_loss=0.02962
Epoch 9/80: current_loss=0.03034 | best_loss=0.02962
Epoch 10/80: current_loss=0.03038 | best_loss=0.02962
Epoch 11/80: current_loss=0.03099 | best_loss=0.02962
Epoch 12/80: current_loss=0.02984 | best_loss=0.02962
Epoch 13/80: current_loss=0.03000 | best_loss=0.02962
Epoch 14/80: current_loss=0.03064 | best_loss=0.02962
Epoch 15/80: current_loss=0.02976 | best_loss=0.02962
Epoch 16/80: current_loss=0.03270 | best_loss=0.02962
Epoch 17/80: current_loss=0.02961 | best_loss=0.02961
Epoch 18/80: current_loss=0.03091 | best_loss=0.02961
Epoch 19/80: current_loss=0.03089 | best_loss=0.02961
Epoch 20/80: current_loss=0.02960 | best_loss=0.02960
Epoch 21/80: current_loss=0.03017 | best_loss=0.02960
Epoch 22/80: current_loss=0.03320 | best_loss=0.02960
Epoch 23/80: current_loss=0.02934 | best_loss=0.02934
Epoch 24/80: current_loss=0.03228 | best_loss=0.02934
Epoch 25/80: current_loss=0.02941 | best_loss=0.02934
Epoch 26/80: current_loss=0.03046 | best_loss=0.02934
Epoch 27/80: current_loss=0.02998 | best_loss=0.02934
Epoch 28/80: current_loss=0.03129 | best_loss=0.02934
Epoch 29/80: current_loss=0.02997 | best_loss=0.02934
Epoch 30/80: current_loss=0.03042 | best_loss=0.02934
Epoch 31/80: current_loss=0.02988 | best_loss=0.02934
Epoch 32/80: current_loss=0.03101 | best_loss=0.02934
Epoch 33/80: current_loss=0.02987 | best_loss=0.02934
Epoch 34/80: current_loss=0.03106 | best_loss=0.02934
Epoch 35/80: current_loss=0.02968 | best_loss=0.02934
Epoch 36/80: current_loss=0.03045 | best_loss=0.02934
Epoch 37/80: current_loss=0.02998 | best_loss=0.02934
Epoch 38/80: current_loss=0.03118 | best_loss=0.02934
Epoch 39/80: current_loss=0.02988 | best_loss=0.02934
Epoch 40/80: current_loss=0.03384 | best_loss=0.02934
Epoch 41/80: current_loss=0.02979 | best_loss=0.02934
Epoch 42/80: current_loss=0.03186 | best_loss=0.02934
Epoch 43/80: current_loss=0.02989 | best_loss=0.02934
Early Stopping at epoch 43
      explained_var=0.03333 | mse_loss=0.02859
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02836 | best_loss=0.02836
Epoch 1/80: current_loss=0.02809 | best_loss=0.02809
Epoch 2/80: current_loss=0.02795 | best_loss=0.02795
Epoch 3/80: current_loss=0.02818 | best_loss=0.02795
Epoch 4/80: current_loss=0.02824 | best_loss=0.02795
Epoch 5/80: current_loss=0.02868 | best_loss=0.02795
Epoch 6/80: current_loss=0.02921 | best_loss=0.02795
Epoch 7/80: current_loss=0.02981 | best_loss=0.02795
Epoch 8/80: current_loss=0.03015 | best_loss=0.02795
Epoch 9/80: current_loss=0.02965 | best_loss=0.02795
Epoch 10/80: current_loss=0.02902 | best_loss=0.02795
Epoch 11/80: current_loss=0.02876 | best_loss=0.02795
Epoch 12/80: current_loss=0.02857 | best_loss=0.02795
Epoch 13/80: current_loss=0.02836 | best_loss=0.02795
Epoch 14/80: current_loss=0.02880 | best_loss=0.02795
Epoch 15/80: current_loss=0.02896 | best_loss=0.02795
Epoch 16/80: current_loss=0.02876 | best_loss=0.02795
Epoch 17/80: current_loss=0.02914 | best_loss=0.02795
Epoch 18/80: current_loss=0.02836 | best_loss=0.02795
Epoch 19/80: current_loss=0.02817 | best_loss=0.02795
Epoch 20/80: current_loss=0.02812 | best_loss=0.02795
Epoch 21/80: current_loss=0.02911 | best_loss=0.02795
Epoch 22/80: current_loss=0.02870 | best_loss=0.02795
Early Stopping at epoch 22
      explained_var=0.00659 | mse_loss=0.02828
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03337 | best_loss=0.03337
Epoch 1/80: current_loss=0.03382 | best_loss=0.03337
Epoch 2/80: current_loss=0.03439 | best_loss=0.03337
Epoch 3/80: current_loss=0.03421 | best_loss=0.03337
Epoch 4/80: current_loss=0.03406 | best_loss=0.03337
Epoch 5/80: current_loss=0.03466 | best_loss=0.03337
Epoch 6/80: current_loss=0.03494 | best_loss=0.03337
Epoch 7/80: current_loss=0.03515 | best_loss=0.03337
Epoch 8/80: current_loss=0.03484 | best_loss=0.03337
Epoch 9/80: current_loss=0.03451 | best_loss=0.03337
Epoch 10/80: current_loss=0.03447 | best_loss=0.03337
Epoch 11/80: current_loss=0.03431 | best_loss=0.03337
Epoch 12/80: current_loss=0.03446 | best_loss=0.03337
Epoch 13/80: current_loss=0.03511 | best_loss=0.03337
Epoch 14/80: current_loss=0.03468 | best_loss=0.03337
Epoch 15/80: current_loss=0.03435 | best_loss=0.03337
Epoch 16/80: current_loss=0.03415 | best_loss=0.03337
Epoch 17/80: current_loss=0.03437 | best_loss=0.03337
Epoch 18/80: current_loss=0.03409 | best_loss=0.03337
Epoch 19/80: current_loss=0.03438 | best_loss=0.03337
Epoch 20/80: current_loss=0.03415 | best_loss=0.03337
Early Stopping at epoch 20
      explained_var=-0.04935 | mse_loss=0.03416
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.01693| avg_loss=0.02845
----------------------------------------------


----------------------------------------------
Params for Trial 76
{'learning_rate': 0.001, 'weight_decay': 0.0016006985460057176, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03434 | best_loss=0.03434
Epoch 1/80: current_loss=0.03374 | best_loss=0.03374
Epoch 2/80: current_loss=0.02915 | best_loss=0.02915
Epoch 3/80: current_loss=0.02867 | best_loss=0.02867
Epoch 4/80: current_loss=0.03152 | best_loss=0.02867
Epoch 5/80: current_loss=0.02785 | best_loss=0.02785
Epoch 6/80: current_loss=0.02937 | best_loss=0.02785
Epoch 7/80: current_loss=0.02815 | best_loss=0.02785
Epoch 8/80: current_loss=0.02727 | best_loss=0.02727
Epoch 9/80: current_loss=0.02730 | best_loss=0.02727
Epoch 10/80: current_loss=0.02713 | best_loss=0.02713
Epoch 11/80: current_loss=0.02731 | best_loss=0.02713
Epoch 12/80: current_loss=0.02694 | best_loss=0.02694
Epoch 13/80: current_loss=0.02637 | best_loss=0.02637
Epoch 14/80: current_loss=0.02655 | best_loss=0.02637
Epoch 15/80: current_loss=0.02642 | best_loss=0.02637
Epoch 16/80: current_loss=0.02686 | best_loss=0.02637
Epoch 17/80: current_loss=0.02562 | best_loss=0.02562
Epoch 18/80: current_loss=0.02604 | best_loss=0.02562
Epoch 19/80: current_loss=0.02670 | best_loss=0.02562
Epoch 20/80: current_loss=0.02549 | best_loss=0.02549
Epoch 21/80: current_loss=0.02651 | best_loss=0.02549
Epoch 22/80: current_loss=0.02613 | best_loss=0.02549
Epoch 23/80: current_loss=0.02591 | best_loss=0.02549
Epoch 24/80: current_loss=0.02584 | best_loss=0.02549
Epoch 25/80: current_loss=0.02665 | best_loss=0.02549
Epoch 26/80: current_loss=0.02592 | best_loss=0.02549
Epoch 27/80: current_loss=0.02664 | best_loss=0.02549
Epoch 28/80: current_loss=0.02616 | best_loss=0.02549
Epoch 29/80: current_loss=0.02617 | best_loss=0.02549
Epoch 30/80: current_loss=0.02711 | best_loss=0.02549
Epoch 31/80: current_loss=0.02645 | best_loss=0.02549
Epoch 32/80: current_loss=0.02529 | best_loss=0.02529
Epoch 33/80: current_loss=0.02712 | best_loss=0.02529
Epoch 34/80: current_loss=0.02556 | best_loss=0.02529
Epoch 35/80: current_loss=0.02649 | best_loss=0.02529
Epoch 36/80: current_loss=0.02617 | best_loss=0.02529
Epoch 37/80: current_loss=0.02643 | best_loss=0.02529
Epoch 38/80: current_loss=0.02664 | best_loss=0.02529
Epoch 39/80: current_loss=0.02676 | best_loss=0.02529
Epoch 40/80: current_loss=0.02617 | best_loss=0.02529
Epoch 41/80: current_loss=0.02630 | best_loss=0.02529
Epoch 42/80: current_loss=0.02631 | best_loss=0.02529
Epoch 43/80: current_loss=0.02565 | best_loss=0.02529
Epoch 44/80: current_loss=0.02650 | best_loss=0.02529
Epoch 45/80: current_loss=0.02650 | best_loss=0.02529
Epoch 46/80: current_loss=0.02656 | best_loss=0.02529
Epoch 47/80: current_loss=0.02593 | best_loss=0.02529
Epoch 48/80: current_loss=0.02741 | best_loss=0.02529
Epoch 49/80: current_loss=0.02668 | best_loss=0.02529
Epoch 50/80: current_loss=0.02665 | best_loss=0.02529
Epoch 51/80: current_loss=0.02660 | best_loss=0.02529
Epoch 52/80: current_loss=0.02675 | best_loss=0.02529
Early Stopping at epoch 52
      explained_var=0.04633 | mse_loss=0.02481
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02827 | best_loss=0.02827
Epoch 1/80: current_loss=0.02829 | best_loss=0.02827
Epoch 2/80: current_loss=0.02806 | best_loss=0.02806
Epoch 3/80: current_loss=0.02799 | best_loss=0.02799
Epoch 4/80: current_loss=0.02795 | best_loss=0.02795
Epoch 5/80: current_loss=0.02852 | best_loss=0.02795
Epoch 6/80: current_loss=0.02812 | best_loss=0.02795
Epoch 7/80: current_loss=0.02861 | best_loss=0.02795
Epoch 8/80: current_loss=0.02801 | best_loss=0.02795
Epoch 9/80: current_loss=0.02817 | best_loss=0.02795
Epoch 10/80: current_loss=0.02820 | best_loss=0.02795
Epoch 11/80: current_loss=0.02801 | best_loss=0.02795
Epoch 12/80: current_loss=0.02795 | best_loss=0.02795
Epoch 13/80: current_loss=0.02797 | best_loss=0.02795
Epoch 14/80: current_loss=0.02801 | best_loss=0.02795
Epoch 15/80: current_loss=0.02829 | best_loss=0.02795
Epoch 16/80: current_loss=0.02799 | best_loss=0.02795
Epoch 17/80: current_loss=0.02811 | best_loss=0.02795
Epoch 18/80: current_loss=0.02850 | best_loss=0.02795
Epoch 19/80: current_loss=0.02794 | best_loss=0.02794
Epoch 20/80: current_loss=0.02853 | best_loss=0.02794
Epoch 21/80: current_loss=0.02795 | best_loss=0.02794
Epoch 22/80: current_loss=0.02798 | best_loss=0.02794
Epoch 23/80: current_loss=0.02841 | best_loss=0.02794
Epoch 24/80: current_loss=0.02796 | best_loss=0.02794
Epoch 25/80: current_loss=0.02795 | best_loss=0.02794
Epoch 26/80: current_loss=0.02805 | best_loss=0.02794
Epoch 27/80: current_loss=0.02796 | best_loss=0.02794
Epoch 28/80: current_loss=0.02810 | best_loss=0.02794
Epoch 29/80: current_loss=0.02802 | best_loss=0.02794
Epoch 30/80: current_loss=0.02801 | best_loss=0.02794
Epoch 31/80: current_loss=0.02818 | best_loss=0.02794
Epoch 32/80: current_loss=0.02805 | best_loss=0.02794
Epoch 33/80: current_loss=0.02823 | best_loss=0.02794
Epoch 34/80: current_loss=0.02801 | best_loss=0.02794
Epoch 35/80: current_loss=0.02820 | best_loss=0.02794
Epoch 36/80: current_loss=0.02806 | best_loss=0.02794
Epoch 37/80: current_loss=0.02807 | best_loss=0.02794
Epoch 38/80: current_loss=0.02882 | best_loss=0.02794
Epoch 39/80: current_loss=0.02805 | best_loss=0.02794
Early Stopping at epoch 39
      explained_var=0.01145 | mse_loss=0.02747
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03219 | best_loss=0.03219
Epoch 1/80: current_loss=0.03053 | best_loss=0.03053
Epoch 2/80: current_loss=0.03100 | best_loss=0.03053
Epoch 3/80: current_loss=0.03069 | best_loss=0.03053
Epoch 4/80: current_loss=0.03184 | best_loss=0.03053
Epoch 5/80: current_loss=0.02998 | best_loss=0.02998
Epoch 6/80: current_loss=0.03098 | best_loss=0.02998
Epoch 7/80: current_loss=0.03027 | best_loss=0.02998
Epoch 8/80: current_loss=0.03198 | best_loss=0.02998
Epoch 9/80: current_loss=0.03087 | best_loss=0.02998
Epoch 10/80: current_loss=0.03062 | best_loss=0.02998
Epoch 11/80: current_loss=0.03082 | best_loss=0.02998
Epoch 12/80: current_loss=0.03022 | best_loss=0.02998
Epoch 13/80: current_loss=0.03206 | best_loss=0.02998
Epoch 14/80: current_loss=0.03122 | best_loss=0.02998
Epoch 15/80: current_loss=0.03099 | best_loss=0.02998
Epoch 16/80: current_loss=0.03020 | best_loss=0.02998
Epoch 17/80: current_loss=0.03100 | best_loss=0.02998
Epoch 18/80: current_loss=0.03162 | best_loss=0.02998
Epoch 19/80: current_loss=0.03092 | best_loss=0.02998
Epoch 20/80: current_loss=0.03055 | best_loss=0.02998
Epoch 21/80: current_loss=0.03054 | best_loss=0.02998
Epoch 22/80: current_loss=0.03098 | best_loss=0.02998
Epoch 23/80: current_loss=0.03050 | best_loss=0.02998
Epoch 24/80: current_loss=0.03090 | best_loss=0.02998
Epoch 25/80: current_loss=0.03070 | best_loss=0.02998
Early Stopping at epoch 25
      explained_var=0.01700 | mse_loss=0.02922
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02891 | best_loss=0.02891
Epoch 1/80: current_loss=0.02808 | best_loss=0.02808
Epoch 2/80: current_loss=0.02826 | best_loss=0.02808
Epoch 3/80: current_loss=0.02816 | best_loss=0.02808
Epoch 4/80: current_loss=0.02818 | best_loss=0.02808
Epoch 5/80: current_loss=0.02828 | best_loss=0.02808
Epoch 6/80: current_loss=0.02851 | best_loss=0.02808
Epoch 7/80: current_loss=0.02841 | best_loss=0.02808
Epoch 8/80: current_loss=0.02844 | best_loss=0.02808
Epoch 9/80: current_loss=0.02834 | best_loss=0.02808
Epoch 10/80: current_loss=0.02829 | best_loss=0.02808
Epoch 11/80: current_loss=0.02845 | best_loss=0.02808
Epoch 12/80: current_loss=0.02832 | best_loss=0.02808
Epoch 13/80: current_loss=0.02841 | best_loss=0.02808
Epoch 14/80: current_loss=0.02839 | best_loss=0.02808
Epoch 15/80: current_loss=0.02844 | best_loss=0.02808
Epoch 16/80: current_loss=0.02840 | best_loss=0.02808
Epoch 17/80: current_loss=0.02844 | best_loss=0.02808
Epoch 18/80: current_loss=0.02856 | best_loss=0.02808
Epoch 19/80: current_loss=0.02840 | best_loss=0.02808
Epoch 20/80: current_loss=0.02860 | best_loss=0.02808
Epoch 21/80: current_loss=0.02861 | best_loss=0.02808
Early Stopping at epoch 21
      explained_var=-0.00215 | mse_loss=0.02843
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03347 | best_loss=0.03347
Epoch 1/80: current_loss=0.03332 | best_loss=0.03332
Epoch 2/80: current_loss=0.03336 | best_loss=0.03332
Epoch 3/80: current_loss=0.03331 | best_loss=0.03331
Epoch 4/80: current_loss=0.03331 | best_loss=0.03331
Epoch 5/80: current_loss=0.03323 | best_loss=0.03323
Epoch 6/80: current_loss=0.03330 | best_loss=0.03323
Epoch 7/80: current_loss=0.03315 | best_loss=0.03315
Epoch 8/80: current_loss=0.03340 | best_loss=0.03315
Epoch 9/80: current_loss=0.03379 | best_loss=0.03315
Epoch 10/80: current_loss=0.03369 | best_loss=0.03315
Epoch 11/80: current_loss=0.03372 | best_loss=0.03315
Epoch 12/80: current_loss=0.03376 | best_loss=0.03315
Epoch 13/80: current_loss=0.03353 | best_loss=0.03315
Epoch 14/80: current_loss=0.03364 | best_loss=0.03315
Epoch 15/80: current_loss=0.03356 | best_loss=0.03315
Epoch 16/80: current_loss=0.03363 | best_loss=0.03315
Epoch 17/80: current_loss=0.03381 | best_loss=0.03315
Epoch 18/80: current_loss=0.03354 | best_loss=0.03315
Epoch 19/80: current_loss=0.03350 | best_loss=0.03315
Epoch 20/80: current_loss=0.03351 | best_loss=0.03315
Epoch 21/80: current_loss=0.03366 | best_loss=0.03315
Epoch 22/80: current_loss=0.03374 | best_loss=0.03315
Epoch 23/80: current_loss=0.03392 | best_loss=0.03315
Epoch 24/80: current_loss=0.03387 | best_loss=0.03315
Epoch 25/80: current_loss=0.03447 | best_loss=0.03315
Epoch 26/80: current_loss=0.03394 | best_loss=0.03315
Epoch 27/80: current_loss=0.03390 | best_loss=0.03315
Early Stopping at epoch 27
      explained_var=-0.04419 | mse_loss=0.03395
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=0.00569| avg_loss=0.02878
----------------------------------------------


----------------------------------------------
Params for Trial 77
{'learning_rate': 0.001, 'weight_decay': 0.0009452833854882376, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02904 | best_loss=0.02904
Epoch 1/80: current_loss=0.03030 | best_loss=0.02904
Epoch 2/80: current_loss=0.02951 | best_loss=0.02904
Epoch 3/80: current_loss=0.02554 | best_loss=0.02554
Epoch 4/80: current_loss=0.02748 | best_loss=0.02554
Epoch 5/80: current_loss=0.02884 | best_loss=0.02554
Epoch 6/80: current_loss=0.02833 | best_loss=0.02554
Epoch 7/80: current_loss=0.03297 | best_loss=0.02554
Epoch 8/80: current_loss=0.02960 | best_loss=0.02554
Epoch 9/80: current_loss=0.02716 | best_loss=0.02554
Epoch 10/80: current_loss=0.03532 | best_loss=0.02554
Epoch 11/80: current_loss=0.03696 | best_loss=0.02554
Epoch 12/80: current_loss=0.02660 | best_loss=0.02554
Epoch 13/80: current_loss=0.03311 | best_loss=0.02554
Epoch 14/80: current_loss=0.02625 | best_loss=0.02554
Epoch 15/80: current_loss=0.02746 | best_loss=0.02554
Epoch 16/80: current_loss=0.02646 | best_loss=0.02554
Epoch 17/80: current_loss=0.03358 | best_loss=0.02554
Epoch 18/80: current_loss=0.02864 | best_loss=0.02554
Epoch 19/80: current_loss=0.02658 | best_loss=0.02554
Epoch 20/80: current_loss=0.02469 | best_loss=0.02469
Epoch 21/80: current_loss=0.02654 | best_loss=0.02469
Epoch 22/80: current_loss=0.02546 | best_loss=0.02469
Epoch 23/80: current_loss=0.02532 | best_loss=0.02469
Epoch 24/80: current_loss=0.02560 | best_loss=0.02469
Epoch 25/80: current_loss=0.02612 | best_loss=0.02469
Epoch 26/80: current_loss=0.02803 | best_loss=0.02469
Epoch 27/80: current_loss=0.02906 | best_loss=0.02469
Epoch 28/80: current_loss=0.02932 | best_loss=0.02469
Epoch 29/80: current_loss=0.02939 | best_loss=0.02469
Epoch 30/80: current_loss=0.02590 | best_loss=0.02469
Epoch 31/80: current_loss=0.02559 | best_loss=0.02469
Epoch 32/80: current_loss=0.02769 | best_loss=0.02469
Epoch 33/80: current_loss=0.02852 | best_loss=0.02469
Epoch 34/80: current_loss=0.02724 | best_loss=0.02469
Epoch 35/80: current_loss=0.02672 | best_loss=0.02469
Epoch 36/80: current_loss=0.02622 | best_loss=0.02469
Epoch 37/80: current_loss=0.02658 | best_loss=0.02469
Epoch 38/80: current_loss=0.02803 | best_loss=0.02469
Epoch 39/80: current_loss=0.02579 | best_loss=0.02469
Epoch 40/80: current_loss=0.02785 | best_loss=0.02469
Early Stopping at epoch 40
      explained_var=0.06646 | mse_loss=0.02419
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02858 | best_loss=0.02858
Epoch 1/80: current_loss=0.02843 | best_loss=0.02843
Epoch 2/80: current_loss=0.02824 | best_loss=0.02824
Epoch 3/80: current_loss=0.02877 | best_loss=0.02824
Epoch 4/80: current_loss=0.02866 | best_loss=0.02824
Epoch 5/80: current_loss=0.02816 | best_loss=0.02816
Epoch 6/80: current_loss=0.04653 | best_loss=0.02816
Epoch 7/80: current_loss=0.03119 | best_loss=0.02816
Epoch 8/80: current_loss=0.02777 | best_loss=0.02777
Epoch 9/80: current_loss=0.02850 | best_loss=0.02777
Epoch 10/80: current_loss=0.03339 | best_loss=0.02777
Epoch 11/80: current_loss=0.03517 | best_loss=0.02777
Epoch 12/80: current_loss=0.03388 | best_loss=0.02777
Epoch 13/80: current_loss=0.02807 | best_loss=0.02777
Epoch 14/80: current_loss=0.02764 | best_loss=0.02764
Epoch 15/80: current_loss=0.02889 | best_loss=0.02764
Epoch 16/80: current_loss=0.02804 | best_loss=0.02764
Epoch 17/80: current_loss=0.03135 | best_loss=0.02764
Epoch 18/80: current_loss=0.02833 | best_loss=0.02764
Epoch 19/80: current_loss=0.02922 | best_loss=0.02764
Epoch 20/80: current_loss=0.02849 | best_loss=0.02764
Epoch 21/80: current_loss=0.02803 | best_loss=0.02764
Epoch 22/80: current_loss=0.03536 | best_loss=0.02764
Epoch 23/80: current_loss=0.02917 | best_loss=0.02764
Epoch 24/80: current_loss=0.02836 | best_loss=0.02764
Epoch 25/80: current_loss=0.02993 | best_loss=0.02764
Epoch 26/80: current_loss=0.02979 | best_loss=0.02764
Epoch 27/80: current_loss=0.02801 | best_loss=0.02764
Epoch 28/80: current_loss=0.02776 | best_loss=0.02764
Epoch 29/80: current_loss=0.03293 | best_loss=0.02764
Epoch 30/80: current_loss=0.02881 | best_loss=0.02764
Epoch 31/80: current_loss=0.03055 | best_loss=0.02764
Epoch 32/80: current_loss=0.02797 | best_loss=0.02764
Epoch 33/80: current_loss=0.02782 | best_loss=0.02764
Epoch 34/80: current_loss=0.02924 | best_loss=0.02764
Early Stopping at epoch 34
      explained_var=0.02331 | mse_loss=0.02718
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04855 | best_loss=0.04855
Epoch 1/80: current_loss=0.03480 | best_loss=0.03480
Epoch 2/80: current_loss=0.03137 | best_loss=0.03137
Epoch 3/80: current_loss=0.02974 | best_loss=0.02974
Epoch 4/80: current_loss=0.03006 | best_loss=0.02974
Epoch 5/80: current_loss=0.03001 | best_loss=0.02974
Epoch 6/80: current_loss=0.03155 | best_loss=0.02974
Epoch 7/80: current_loss=0.02991 | best_loss=0.02974
Epoch 8/80: current_loss=0.03075 | best_loss=0.02974
Epoch 9/80: current_loss=0.03139 | best_loss=0.02974
Epoch 10/80: current_loss=0.03075 | best_loss=0.02974
Epoch 11/80: current_loss=0.02932 | best_loss=0.02932
Epoch 12/80: current_loss=0.02956 | best_loss=0.02932
Epoch 13/80: current_loss=0.02989 | best_loss=0.02932
Epoch 14/80: current_loss=0.03386 | best_loss=0.02932
Epoch 15/80: current_loss=0.03313 | best_loss=0.02932
Epoch 16/80: current_loss=0.02987 | best_loss=0.02932
Epoch 17/80: current_loss=0.03021 | best_loss=0.02932
Epoch 18/80: current_loss=0.03630 | best_loss=0.02932
Epoch 19/80: current_loss=0.03077 | best_loss=0.02932
Epoch 20/80: current_loss=0.02986 | best_loss=0.02932
Epoch 21/80: current_loss=0.03248 | best_loss=0.02932
Epoch 22/80: current_loss=0.03011 | best_loss=0.02932
Epoch 23/80: current_loss=0.03236 | best_loss=0.02932
Epoch 24/80: current_loss=0.03475 | best_loss=0.02932
Epoch 25/80: current_loss=0.03492 | best_loss=0.02932
Epoch 26/80: current_loss=0.03356 | best_loss=0.02932
Epoch 27/80: current_loss=0.03739 | best_loss=0.02932
Epoch 28/80: current_loss=0.04197 | best_loss=0.02932
Epoch 29/80: current_loss=0.05440 | best_loss=0.02932
Epoch 30/80: current_loss=0.03241 | best_loss=0.02932
Epoch 31/80: current_loss=0.03466 | best_loss=0.02932
Early Stopping at epoch 31
      explained_var=0.03411 | mse_loss=0.02857
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04371 | best_loss=0.04371
Epoch 1/80: current_loss=0.03454 | best_loss=0.03454
Epoch 2/80: current_loss=0.03996 | best_loss=0.03454
Epoch 3/80: current_loss=0.03280 | best_loss=0.03280
Epoch 4/80: current_loss=0.05117 | best_loss=0.03280
Epoch 5/80: current_loss=0.02883 | best_loss=0.02883
Epoch 6/80: current_loss=0.03694 | best_loss=0.02883
Epoch 7/80: current_loss=0.03124 | best_loss=0.02883
Epoch 8/80: current_loss=0.03032 | best_loss=0.02883
Epoch 9/80: current_loss=0.03492 | best_loss=0.02883
Epoch 10/80: current_loss=0.03026 | best_loss=0.02883
Epoch 11/80: current_loss=0.02928 | best_loss=0.02883
Epoch 12/80: current_loss=0.02784 | best_loss=0.02784
Epoch 13/80: current_loss=0.02789 | best_loss=0.02784
Epoch 14/80: current_loss=0.02881 | best_loss=0.02784
Epoch 15/80: current_loss=0.03202 | best_loss=0.02784
Epoch 16/80: current_loss=0.02904 | best_loss=0.02784
Epoch 17/80: current_loss=0.02935 | best_loss=0.02784
Epoch 18/80: current_loss=0.02856 | best_loss=0.02784
Epoch 19/80: current_loss=0.02973 | best_loss=0.02784
Epoch 20/80: current_loss=0.02997 | best_loss=0.02784
Epoch 21/80: current_loss=0.02835 | best_loss=0.02784
Epoch 22/80: current_loss=0.02875 | best_loss=0.02784
Epoch 23/80: current_loss=0.02886 | best_loss=0.02784
Epoch 24/80: current_loss=0.02818 | best_loss=0.02784
Epoch 25/80: current_loss=0.02810 | best_loss=0.02784
Epoch 26/80: current_loss=0.02863 | best_loss=0.02784
Epoch 27/80: current_loss=0.03204 | best_loss=0.02784
Epoch 28/80: current_loss=0.02928 | best_loss=0.02784
Epoch 29/80: current_loss=0.02831 | best_loss=0.02784
Epoch 30/80: current_loss=0.02960 | best_loss=0.02784
Epoch 31/80: current_loss=0.03238 | best_loss=0.02784
Epoch 32/80: current_loss=0.02977 | best_loss=0.02784
Early Stopping at epoch 32
      explained_var=0.01670 | mse_loss=0.02815
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03272 | best_loss=0.03272
Epoch 1/80: current_loss=0.03326 | best_loss=0.03272
Epoch 2/80: current_loss=0.03844 | best_loss=0.03272
Epoch 3/80: current_loss=0.03412 | best_loss=0.03272
Epoch 4/80: current_loss=0.03477 | best_loss=0.03272
Epoch 5/80: current_loss=0.03280 | best_loss=0.03272
Epoch 6/80: current_loss=0.03312 | best_loss=0.03272
Epoch 7/80: current_loss=0.03483 | best_loss=0.03272
Epoch 8/80: current_loss=0.03357 | best_loss=0.03272
Epoch 9/80: current_loss=0.03326 | best_loss=0.03272
Epoch 10/80: current_loss=0.03704 | best_loss=0.03272
Epoch 11/80: current_loss=0.03287 | best_loss=0.03272
Epoch 12/80: current_loss=0.03539 | best_loss=0.03272
Epoch 13/80: current_loss=0.03310 | best_loss=0.03272
Epoch 14/80: current_loss=0.03461 | best_loss=0.03272
Epoch 15/80: current_loss=0.03314 | best_loss=0.03272
Epoch 16/80: current_loss=0.03348 | best_loss=0.03272
Epoch 17/80: current_loss=0.03449 | best_loss=0.03272
Epoch 18/80: current_loss=0.03546 | best_loss=0.03272
Epoch 19/80: current_loss=0.03438 | best_loss=0.03272
Epoch 20/80: current_loss=0.03314 | best_loss=0.03272
Early Stopping at epoch 20
      explained_var=-0.02317 | mse_loss=0.03335
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.02348| avg_loss=0.02829
----------------------------------------------


----------------------------------------------
Params for Trial 78
{'learning_rate': 1e-05, 'weight_decay': 0.0012981871351322595, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.17435 | best_loss=0.17435
Epoch 1/80: current_loss=0.11599 | best_loss=0.11599
Epoch 2/80: current_loss=0.07227 | best_loss=0.07227
Epoch 3/80: current_loss=0.04484 | best_loss=0.04484
Epoch 4/80: current_loss=0.03603 | best_loss=0.03603
Epoch 5/80: current_loss=0.03774 | best_loss=0.03603
Epoch 6/80: current_loss=0.03791 | best_loss=0.03603
Epoch 7/80: current_loss=0.03705 | best_loss=0.03603
Epoch 8/80: current_loss=0.03599 | best_loss=0.03599
Epoch 9/80: current_loss=0.03552 | best_loss=0.03552
Epoch 10/80: current_loss=0.03524 | best_loss=0.03524
Epoch 11/80: current_loss=0.03522 | best_loss=0.03522
Epoch 12/80: current_loss=0.03493 | best_loss=0.03493
Epoch 13/80: current_loss=0.03425 | best_loss=0.03425
Epoch 14/80: current_loss=0.03451 | best_loss=0.03425
Epoch 15/80: current_loss=0.03501 | best_loss=0.03425
Epoch 16/80: current_loss=0.03427 | best_loss=0.03425
Epoch 17/80: current_loss=0.03339 | best_loss=0.03339
Epoch 18/80: current_loss=0.03390 | best_loss=0.03339
Epoch 19/80: current_loss=0.03331 | best_loss=0.03331
Epoch 20/80: current_loss=0.03286 | best_loss=0.03286
Epoch 21/80: current_loss=0.03309 | best_loss=0.03286
Epoch 22/80: current_loss=0.03321 | best_loss=0.03286
Epoch 23/80: current_loss=0.03305 | best_loss=0.03286
Epoch 24/80: current_loss=0.03272 | best_loss=0.03272
Epoch 25/80: current_loss=0.03245 | best_loss=0.03245
Epoch 26/80: current_loss=0.03243 | best_loss=0.03243
Epoch 27/80: current_loss=0.03278 | best_loss=0.03243
Epoch 28/80: current_loss=0.03225 | best_loss=0.03225
Epoch 29/80: current_loss=0.03148 | best_loss=0.03148
Epoch 30/80: current_loss=0.03165 | best_loss=0.03148
Epoch 31/80: current_loss=0.03152 | best_loss=0.03148
Epoch 32/80: current_loss=0.03164 | best_loss=0.03148
Epoch 33/80: current_loss=0.03147 | best_loss=0.03147
Epoch 34/80: current_loss=0.03096 | best_loss=0.03096
Epoch 35/80: current_loss=0.03094 | best_loss=0.03094
Epoch 36/80: current_loss=0.03093 | best_loss=0.03093
Epoch 37/80: current_loss=0.03089 | best_loss=0.03089
Epoch 38/80: current_loss=0.03062 | best_loss=0.03062
Epoch 39/80: current_loss=0.03087 | best_loss=0.03062
Epoch 40/80: current_loss=0.03114 | best_loss=0.03062
Epoch 41/80: current_loss=0.03120 | best_loss=0.03062
Epoch 42/80: current_loss=0.02988 | best_loss=0.02988
Epoch 43/80: current_loss=0.03005 | best_loss=0.02988
Epoch 44/80: current_loss=0.03028 | best_loss=0.02988
Epoch 45/80: current_loss=0.03079 | best_loss=0.02988
Epoch 46/80: current_loss=0.03019 | best_loss=0.02988
Epoch 47/80: current_loss=0.02985 | best_loss=0.02985
Epoch 48/80: current_loss=0.02968 | best_loss=0.02968
Epoch 49/80: current_loss=0.02967 | best_loss=0.02967
Epoch 50/80: current_loss=0.02941 | best_loss=0.02941
Epoch 51/80: current_loss=0.02965 | best_loss=0.02941
Epoch 52/80: current_loss=0.03042 | best_loss=0.02941
Epoch 53/80: current_loss=0.02989 | best_loss=0.02941
Epoch 54/80: current_loss=0.02961 | best_loss=0.02941
Epoch 55/80: current_loss=0.02948 | best_loss=0.02941
Epoch 56/80: current_loss=0.02926 | best_loss=0.02926
Epoch 57/80: current_loss=0.02893 | best_loss=0.02893
Epoch 58/80: current_loss=0.02915 | best_loss=0.02893
Epoch 59/80: current_loss=0.02925 | best_loss=0.02893
Epoch 60/80: current_loss=0.02901 | best_loss=0.02893
Epoch 61/80: current_loss=0.02868 | best_loss=0.02868
Epoch 62/80: current_loss=0.02909 | best_loss=0.02868
Epoch 63/80: current_loss=0.02915 | best_loss=0.02868
Epoch 64/80: current_loss=0.02816 | best_loss=0.02816
Epoch 65/80: current_loss=0.02881 | best_loss=0.02816
Epoch 66/80: current_loss=0.02923 | best_loss=0.02816
Epoch 67/80: current_loss=0.02914 | best_loss=0.02816
Epoch 68/80: current_loss=0.02888 | best_loss=0.02816
Epoch 69/80: current_loss=0.02878 | best_loss=0.02816
Epoch 70/80: current_loss=0.02848 | best_loss=0.02816
Epoch 71/80: current_loss=0.02868 | best_loss=0.02816
Epoch 72/80: current_loss=0.02845 | best_loss=0.02816
Epoch 73/80: current_loss=0.02813 | best_loss=0.02813
Epoch 74/80: current_loss=0.02857 | best_loss=0.02813
Epoch 75/80: current_loss=0.02833 | best_loss=0.02813
Epoch 76/80: current_loss=0.02827 | best_loss=0.02813
Epoch 77/80: current_loss=0.02836 | best_loss=0.02813
Epoch 78/80: current_loss=0.02812 | best_loss=0.02812
Epoch 79/80: current_loss=0.02784 | best_loss=0.02784
      explained_var=-0.03465 | mse_loss=0.02735

----------------------------------------------
Params for Trial 79
{'learning_rate': 0.001, 'weight_decay': 0.0007059421684398006, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03021 | best_loss=0.03021
Epoch 1/80: current_loss=0.02906 | best_loss=0.02906
Epoch 2/80: current_loss=0.02875 | best_loss=0.02875
Epoch 3/80: current_loss=0.02674 | best_loss=0.02674
Epoch 4/80: current_loss=0.05039 | best_loss=0.02674
Epoch 5/80: current_loss=0.03587 | best_loss=0.02674
Epoch 6/80: current_loss=0.02978 | best_loss=0.02674
Epoch 7/80: current_loss=0.02685 | best_loss=0.02674
Epoch 8/80: current_loss=0.02770 | best_loss=0.02674
Epoch 9/80: current_loss=0.02548 | best_loss=0.02548
Epoch 10/80: current_loss=0.02812 | best_loss=0.02548
Epoch 11/80: current_loss=0.02652 | best_loss=0.02548
Epoch 12/80: current_loss=0.02622 | best_loss=0.02548
Epoch 13/80: current_loss=0.02730 | best_loss=0.02548
Epoch 14/80: current_loss=0.02890 | best_loss=0.02548
Epoch 15/80: current_loss=0.02668 | best_loss=0.02548
Epoch 16/80: current_loss=0.02640 | best_loss=0.02548
Epoch 17/80: current_loss=0.02522 | best_loss=0.02522
Epoch 18/80: current_loss=0.04709 | best_loss=0.02522
Epoch 19/80: current_loss=0.03863 | best_loss=0.02522
Epoch 20/80: current_loss=0.02990 | best_loss=0.02522
Epoch 21/80: current_loss=0.02671 | best_loss=0.02522
Epoch 22/80: current_loss=0.04183 | best_loss=0.02522
Epoch 23/80: current_loss=0.03205 | best_loss=0.02522
Epoch 24/80: current_loss=0.02579 | best_loss=0.02522
Epoch 25/80: current_loss=0.02740 | best_loss=0.02522
Epoch 26/80: current_loss=0.02564 | best_loss=0.02522
Epoch 27/80: current_loss=0.02797 | best_loss=0.02522
Epoch 28/80: current_loss=0.02623 | best_loss=0.02522
Epoch 29/80: current_loss=0.02506 | best_loss=0.02506
Epoch 30/80: current_loss=0.02532 | best_loss=0.02506
Epoch 31/80: current_loss=0.02556 | best_loss=0.02506
Epoch 32/80: current_loss=0.02827 | best_loss=0.02506
Epoch 33/80: current_loss=0.02522 | best_loss=0.02506
Epoch 34/80: current_loss=0.02566 | best_loss=0.02506
Epoch 35/80: current_loss=0.02569 | best_loss=0.02506
Epoch 36/80: current_loss=0.02650 | best_loss=0.02506
Epoch 37/80: current_loss=0.03773 | best_loss=0.02506
Epoch 38/80: current_loss=0.03012 | best_loss=0.02506
Epoch 39/80: current_loss=0.03671 | best_loss=0.02506
Epoch 40/80: current_loss=0.02747 | best_loss=0.02506
Epoch 41/80: current_loss=0.02538 | best_loss=0.02506
Epoch 42/80: current_loss=0.02555 | best_loss=0.02506
Epoch 43/80: current_loss=0.02558 | best_loss=0.02506
Epoch 44/80: current_loss=0.02567 | best_loss=0.02506
Epoch 45/80: current_loss=0.02876 | best_loss=0.02506
Epoch 46/80: current_loss=0.03495 | best_loss=0.02506
Epoch 47/80: current_loss=0.02789 | best_loss=0.02506
Epoch 48/80: current_loss=0.02693 | best_loss=0.02506
Epoch 49/80: current_loss=0.03909 | best_loss=0.02506
Early Stopping at epoch 49
      explained_var=0.04776 | mse_loss=0.02461
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03032 | best_loss=0.03032
Epoch 1/80: current_loss=0.02855 | best_loss=0.02855
Epoch 2/80: current_loss=0.03502 | best_loss=0.02855
Epoch 3/80: current_loss=0.02950 | best_loss=0.02855
Epoch 4/80: current_loss=0.03220 | best_loss=0.02855
Epoch 5/80: current_loss=0.03175 | best_loss=0.02855
Epoch 6/80: current_loss=0.02745 | best_loss=0.02745
Epoch 7/80: current_loss=0.02787 | best_loss=0.02745
Epoch 8/80: current_loss=0.02914 | best_loss=0.02745
Epoch 9/80: current_loss=0.02968 | best_loss=0.02745
Epoch 10/80: current_loss=0.02910 | best_loss=0.02745
Epoch 11/80: current_loss=0.02827 | best_loss=0.02745
Epoch 12/80: current_loss=0.02796 | best_loss=0.02745
Epoch 13/80: current_loss=0.03141 | best_loss=0.02745
Epoch 14/80: current_loss=0.03254 | best_loss=0.02745
Epoch 15/80: current_loss=0.03229 | best_loss=0.02745
Epoch 16/80: current_loss=0.02741 | best_loss=0.02741
Epoch 17/80: current_loss=0.02969 | best_loss=0.02741
Epoch 18/80: current_loss=0.03153 | best_loss=0.02741
Epoch 19/80: current_loss=0.02953 | best_loss=0.02741
Epoch 20/80: current_loss=0.02802 | best_loss=0.02741
Epoch 21/80: current_loss=0.02813 | best_loss=0.02741
Epoch 22/80: current_loss=0.02754 | best_loss=0.02741
Epoch 23/80: current_loss=0.02742 | best_loss=0.02741
Epoch 24/80: current_loss=0.02793 | best_loss=0.02741
Epoch 25/80: current_loss=0.02829 | best_loss=0.02741
Epoch 26/80: current_loss=0.02792 | best_loss=0.02741
Epoch 27/80: current_loss=0.02813 | best_loss=0.02741
Epoch 28/80: current_loss=0.02779 | best_loss=0.02741
Epoch 29/80: current_loss=0.02761 | best_loss=0.02741
Epoch 30/80: current_loss=0.02748 | best_loss=0.02741
Epoch 31/80: current_loss=0.02896 | best_loss=0.02741
Epoch 32/80: current_loss=0.02862 | best_loss=0.02741
Epoch 33/80: current_loss=0.03012 | best_loss=0.02741
Epoch 34/80: current_loss=0.02862 | best_loss=0.02741
Epoch 35/80: current_loss=0.02881 | best_loss=0.02741
Epoch 36/80: current_loss=0.02806 | best_loss=0.02741
Early Stopping at epoch 36
      explained_var=0.04131 | mse_loss=0.02688
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02987 | best_loss=0.02987
Epoch 1/80: current_loss=0.03526 | best_loss=0.02987
Epoch 2/80: current_loss=0.03452 | best_loss=0.02987
Epoch 3/80: current_loss=0.03228 | best_loss=0.02987
Epoch 4/80: current_loss=0.03616 | best_loss=0.02987
Epoch 5/80: current_loss=0.03035 | best_loss=0.02987
Epoch 6/80: current_loss=0.03185 | best_loss=0.02987
Epoch 7/80: current_loss=0.03369 | best_loss=0.02987
Epoch 8/80: current_loss=0.03440 | best_loss=0.02987
Epoch 9/80: current_loss=0.03320 | best_loss=0.02987
Epoch 10/80: current_loss=0.02990 | best_loss=0.02987
Epoch 11/80: current_loss=0.02977 | best_loss=0.02977
Epoch 12/80: current_loss=0.02948 | best_loss=0.02948
Epoch 13/80: current_loss=0.03053 | best_loss=0.02948
Epoch 14/80: current_loss=0.02976 | best_loss=0.02948
Epoch 15/80: current_loss=0.03064 | best_loss=0.02948
Epoch 16/80: current_loss=0.03673 | best_loss=0.02948
Epoch 17/80: current_loss=0.03557 | best_loss=0.02948
Epoch 18/80: current_loss=0.03064 | best_loss=0.02948
Epoch 19/80: current_loss=0.02975 | best_loss=0.02948
Epoch 20/80: current_loss=0.02962 | best_loss=0.02948
Epoch 21/80: current_loss=0.03060 | best_loss=0.02948
Epoch 22/80: current_loss=0.02978 | best_loss=0.02948
Epoch 23/80: current_loss=0.09837 | best_loss=0.02948
Epoch 24/80: current_loss=0.08475 | best_loss=0.02948
Epoch 25/80: current_loss=0.04135 | best_loss=0.02948
Epoch 26/80: current_loss=0.03289 | best_loss=0.02948
Epoch 27/80: current_loss=0.03781 | best_loss=0.02948
Epoch 28/80: current_loss=0.03800 | best_loss=0.02948
Epoch 29/80: current_loss=0.03141 | best_loss=0.02948
Epoch 30/80: current_loss=0.04695 | best_loss=0.02948
Epoch 31/80: current_loss=0.03355 | best_loss=0.02948
Epoch 32/80: current_loss=0.03486 | best_loss=0.02948
Early Stopping at epoch 32
      explained_var=0.02856 | mse_loss=0.02874
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04150 | best_loss=0.04150
Epoch 1/80: current_loss=0.03032 | best_loss=0.03032
Epoch 2/80: current_loss=0.03071 | best_loss=0.03032
Epoch 3/80: current_loss=0.04883 | best_loss=0.03032
Epoch 4/80: current_loss=0.03119 | best_loss=0.03032
Epoch 5/80: current_loss=0.03275 | best_loss=0.03032
Epoch 6/80: current_loss=0.05728 | best_loss=0.03032
Epoch 7/80: current_loss=0.03408 | best_loss=0.03032
Epoch 8/80: current_loss=0.03449 | best_loss=0.03032
Epoch 9/80: current_loss=0.03083 | best_loss=0.03032
Epoch 10/80: current_loss=0.02907 | best_loss=0.02907
Epoch 11/80: current_loss=0.02876 | best_loss=0.02876
Epoch 12/80: current_loss=0.02894 | best_loss=0.02876
Epoch 13/80: current_loss=0.03098 | best_loss=0.02876
Epoch 14/80: current_loss=0.02964 | best_loss=0.02876
Epoch 15/80: current_loss=0.03280 | best_loss=0.02876
Epoch 16/80: current_loss=0.03469 | best_loss=0.02876
Epoch 17/80: current_loss=0.03397 | best_loss=0.02876
Epoch 18/80: current_loss=0.03192 | best_loss=0.02876
Epoch 19/80: current_loss=0.02958 | best_loss=0.02876
Epoch 20/80: current_loss=0.03183 | best_loss=0.02876
Epoch 21/80: current_loss=0.02998 | best_loss=0.02876
Epoch 22/80: current_loss=0.02857 | best_loss=0.02857
Epoch 23/80: current_loss=0.02907 | best_loss=0.02857
Epoch 24/80: current_loss=0.02871 | best_loss=0.02857
Epoch 25/80: current_loss=0.03003 | best_loss=0.02857
Epoch 26/80: current_loss=0.02952 | best_loss=0.02857
Epoch 27/80: current_loss=0.02829 | best_loss=0.02829
Epoch 28/80: current_loss=0.03749 | best_loss=0.02829
Epoch 29/80: current_loss=0.04597 | best_loss=0.02829
Epoch 30/80: current_loss=0.02932 | best_loss=0.02829
Epoch 31/80: current_loss=0.02990 | best_loss=0.02829
Epoch 32/80: current_loss=0.03241 | best_loss=0.02829
Epoch 33/80: current_loss=0.03041 | best_loss=0.02829
Epoch 34/80: current_loss=0.02921 | best_loss=0.02829
Epoch 35/80: current_loss=0.02935 | best_loss=0.02829
Epoch 36/80: current_loss=0.02881 | best_loss=0.02829
Epoch 37/80: current_loss=0.03070 | best_loss=0.02829
Epoch 38/80: current_loss=0.03388 | best_loss=0.02829
Epoch 39/80: current_loss=0.04058 | best_loss=0.02829
Epoch 40/80: current_loss=0.02908 | best_loss=0.02829
Epoch 41/80: current_loss=0.02880 | best_loss=0.02829
Epoch 42/80: current_loss=0.02873 | best_loss=0.02829
Epoch 43/80: current_loss=0.03090 | best_loss=0.02829
Epoch 44/80: current_loss=0.02862 | best_loss=0.02829
Epoch 45/80: current_loss=0.02829 | best_loss=0.02829
Epoch 46/80: current_loss=0.02813 | best_loss=0.02813
Epoch 47/80: current_loss=0.02909 | best_loss=0.02813
Epoch 48/80: current_loss=0.02817 | best_loss=0.02813
Epoch 49/80: current_loss=0.02806 | best_loss=0.02806
Epoch 50/80: current_loss=0.03107 | best_loss=0.02806
Epoch 51/80: current_loss=0.02873 | best_loss=0.02806
Epoch 52/80: current_loss=0.02840 | best_loss=0.02806
Epoch 53/80: current_loss=0.02809 | best_loss=0.02806
Epoch 54/80: current_loss=0.02801 | best_loss=0.02801
Epoch 55/80: current_loss=0.02959 | best_loss=0.02801
Epoch 56/80: current_loss=0.02781 | best_loss=0.02781
Epoch 57/80: current_loss=0.03296 | best_loss=0.02781
Epoch 58/80: current_loss=0.02846 | best_loss=0.02781
Epoch 59/80: current_loss=0.02801 | best_loss=0.02781
Epoch 60/80: current_loss=0.02780 | best_loss=0.02780
Epoch 61/80: current_loss=0.03151 | best_loss=0.02780
Epoch 62/80: current_loss=0.02792 | best_loss=0.02780
Epoch 63/80: current_loss=0.02810 | best_loss=0.02780
Epoch 64/80: current_loss=0.02851 | best_loss=0.02780
Epoch 65/80: current_loss=0.02858 | best_loss=0.02780
Epoch 66/80: current_loss=0.03015 | best_loss=0.02780
Epoch 67/80: current_loss=0.02913 | best_loss=0.02780
Epoch 68/80: current_loss=0.02907 | best_loss=0.02780
Epoch 69/80: current_loss=0.02817 | best_loss=0.02780
Epoch 70/80: current_loss=0.02842 | best_loss=0.02780
Epoch 71/80: current_loss=0.02937 | best_loss=0.02780
Epoch 72/80: current_loss=0.02794 | best_loss=0.02780
Epoch 73/80: current_loss=0.02829 | best_loss=0.02780
Epoch 74/80: current_loss=0.02870 | best_loss=0.02780
Epoch 75/80: current_loss=0.02861 | best_loss=0.02780
Epoch 76/80: current_loss=0.03193 | best_loss=0.02780
Epoch 77/80: current_loss=0.02782 | best_loss=0.02780
Epoch 78/80: current_loss=0.02789 | best_loss=0.02780
Epoch 79/80: current_loss=0.02810 | best_loss=0.02780
      explained_var=0.01083 | mse_loss=0.02815
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03220 | best_loss=0.03220
Epoch 1/80: current_loss=0.03234 | best_loss=0.03220
Epoch 2/80: current_loss=0.03271 | best_loss=0.03220
Epoch 3/80: current_loss=0.03299 | best_loss=0.03220
Epoch 4/80: current_loss=0.03501 | best_loss=0.03220
Epoch 5/80: current_loss=0.03256 | best_loss=0.03220
Epoch 6/80: current_loss=0.03301 | best_loss=0.03220
Epoch 7/80: current_loss=0.03375 | best_loss=0.03220
Epoch 8/80: current_loss=0.03317 | best_loss=0.03220
Epoch 9/80: current_loss=0.03395 | best_loss=0.03220
Epoch 10/80: current_loss=0.03363 | best_loss=0.03220
Epoch 11/80: current_loss=0.03304 | best_loss=0.03220
Epoch 12/80: current_loss=0.03397 | best_loss=0.03220
Epoch 13/80: current_loss=0.03291 | best_loss=0.03220
Epoch 14/80: current_loss=0.03261 | best_loss=0.03220
Epoch 15/80: current_loss=0.03322 | best_loss=0.03220
Epoch 16/80: current_loss=0.03429 | best_loss=0.03220
Epoch 17/80: current_loss=0.03448 | best_loss=0.03220
Epoch 18/80: current_loss=0.03326 | best_loss=0.03220
Epoch 19/80: current_loss=0.03307 | best_loss=0.03220
Epoch 20/80: current_loss=0.04364 | best_loss=0.03220
Early Stopping at epoch 20
      explained_var=-0.01101 | mse_loss=0.03295
----------------------------------------------
Average early_stopping_point: 27| avg_exp_var=0.02349| avg_loss=0.02826
----------------------------------------------


----------------------------------------------
Params for Trial 80
{'learning_rate': 0.001, 'weight_decay': 0.0003637460901328516, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02850 | best_loss=0.02850
Epoch 1/80: current_loss=0.02861 | best_loss=0.02850
Epoch 2/80: current_loss=0.02661 | best_loss=0.02661
Epoch 3/80: current_loss=0.03637 | best_loss=0.02661
Epoch 4/80: current_loss=0.02978 | best_loss=0.02661
Epoch 5/80: current_loss=0.02489 | best_loss=0.02489
Epoch 6/80: current_loss=0.02976 | best_loss=0.02489
Epoch 7/80: current_loss=0.02890 | best_loss=0.02489
Epoch 8/80: current_loss=0.02853 | best_loss=0.02489
Epoch 9/80: current_loss=0.02773 | best_loss=0.02489
Epoch 10/80: current_loss=0.02519 | best_loss=0.02489
Epoch 11/80: current_loss=0.02660 | best_loss=0.02489
Epoch 12/80: current_loss=0.02695 | best_loss=0.02489
Epoch 13/80: current_loss=0.02798 | best_loss=0.02489
Epoch 14/80: current_loss=0.02590 | best_loss=0.02489
Epoch 15/80: current_loss=0.02588 | best_loss=0.02489
Epoch 16/80: current_loss=0.02878 | best_loss=0.02489
Epoch 17/80: current_loss=0.02900 | best_loss=0.02489
Epoch 18/80: current_loss=0.02650 | best_loss=0.02489
Epoch 19/80: current_loss=0.02834 | best_loss=0.02489
Epoch 20/80: current_loss=0.02631 | best_loss=0.02489
Epoch 21/80: current_loss=0.02870 | best_loss=0.02489
Epoch 22/80: current_loss=0.04423 | best_loss=0.02489
Epoch 23/80: current_loss=0.03979 | best_loss=0.02489
Epoch 24/80: current_loss=0.02541 | best_loss=0.02489
Epoch 25/80: current_loss=0.02636 | best_loss=0.02489
Early Stopping at epoch 25
      explained_var=0.05991 | mse_loss=0.02427
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02737 | best_loss=0.02737
Epoch 1/80: current_loss=0.03518 | best_loss=0.02737
Epoch 2/80: current_loss=0.02977 | best_loss=0.02737
Epoch 3/80: current_loss=0.03053 | best_loss=0.02737
Epoch 4/80: current_loss=0.03194 | best_loss=0.02737
Epoch 5/80: current_loss=0.02917 | best_loss=0.02737
Epoch 6/80: current_loss=0.03004 | best_loss=0.02737
Epoch 7/80: current_loss=0.02916 | best_loss=0.02737
Epoch 8/80: current_loss=0.02781 | best_loss=0.02737
Epoch 9/80: current_loss=0.02757 | best_loss=0.02737
Epoch 10/80: current_loss=0.02988 | best_loss=0.02737
Epoch 11/80: current_loss=0.03041 | best_loss=0.02737
Epoch 12/80: current_loss=0.02847 | best_loss=0.02737
Epoch 13/80: current_loss=0.02759 | best_loss=0.02737
Epoch 14/80: current_loss=0.02845 | best_loss=0.02737
Epoch 15/80: current_loss=0.02972 | best_loss=0.02737
Epoch 16/80: current_loss=0.02815 | best_loss=0.02737
Epoch 17/80: current_loss=0.02794 | best_loss=0.02737
Epoch 18/80: current_loss=0.02788 | best_loss=0.02737
Epoch 19/80: current_loss=0.02809 | best_loss=0.02737
Epoch 20/80: current_loss=0.02768 | best_loss=0.02737
Early Stopping at epoch 20
      explained_var=0.03158 | mse_loss=0.02691
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02957 | best_loss=0.02957
Epoch 1/80: current_loss=0.03026 | best_loss=0.02957
Epoch 2/80: current_loss=0.03427 | best_loss=0.02957
Epoch 3/80: current_loss=0.03070 | best_loss=0.02957
Epoch 4/80: current_loss=0.03053 | best_loss=0.02957
Epoch 5/80: current_loss=0.03094 | best_loss=0.02957
Epoch 6/80: current_loss=0.03134 | best_loss=0.02957
Epoch 7/80: current_loss=0.02955 | best_loss=0.02955
Epoch 8/80: current_loss=0.02923 | best_loss=0.02923
Epoch 9/80: current_loss=0.02938 | best_loss=0.02923
Epoch 10/80: current_loss=0.02983 | best_loss=0.02923
Epoch 11/80: current_loss=0.03080 | best_loss=0.02923
Epoch 12/80: current_loss=0.02929 | best_loss=0.02923
Epoch 13/80: current_loss=0.02965 | best_loss=0.02923
Epoch 14/80: current_loss=0.03216 | best_loss=0.02923
Epoch 15/80: current_loss=0.02909 | best_loss=0.02909
Epoch 16/80: current_loss=0.04044 | best_loss=0.02909
Epoch 17/80: current_loss=0.03299 | best_loss=0.02909
Epoch 18/80: current_loss=0.03267 | best_loss=0.02909
Epoch 19/80: current_loss=0.02934 | best_loss=0.02909
Epoch 20/80: current_loss=0.03163 | best_loss=0.02909
Epoch 21/80: current_loss=0.03434 | best_loss=0.02909
Epoch 22/80: current_loss=0.03184 | best_loss=0.02909
Epoch 23/80: current_loss=0.03167 | best_loss=0.02909
Epoch 24/80: current_loss=0.03367 | best_loss=0.02909
Epoch 25/80: current_loss=0.03216 | best_loss=0.02909
Epoch 26/80: current_loss=0.03134 | best_loss=0.02909
Epoch 27/80: current_loss=0.03242 | best_loss=0.02909
Epoch 28/80: current_loss=0.03853 | best_loss=0.02909
Epoch 29/80: current_loss=0.03205 | best_loss=0.02909
Epoch 30/80: current_loss=0.03002 | best_loss=0.02909
Epoch 31/80: current_loss=0.03155 | best_loss=0.02909
Epoch 32/80: current_loss=0.02965 | best_loss=0.02909
Epoch 33/80: current_loss=0.02962 | best_loss=0.02909
Epoch 34/80: current_loss=0.03148 | best_loss=0.02909
Epoch 35/80: current_loss=0.03449 | best_loss=0.02909
Early Stopping at epoch 35
      explained_var=0.05343 | mse_loss=0.02830
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02806 | best_loss=0.02806
Epoch 1/80: current_loss=0.02813 | best_loss=0.02806
Epoch 2/80: current_loss=0.02943 | best_loss=0.02806
Epoch 3/80: current_loss=0.02806 | best_loss=0.02806
Epoch 4/80: current_loss=0.02883 | best_loss=0.02806
Epoch 5/80: current_loss=0.02804 | best_loss=0.02804
Epoch 6/80: current_loss=0.02798 | best_loss=0.02798
Epoch 7/80: current_loss=0.02785 | best_loss=0.02785
Epoch 8/80: current_loss=0.02825 | best_loss=0.02785
Epoch 9/80: current_loss=0.02798 | best_loss=0.02785
Epoch 10/80: current_loss=0.02804 | best_loss=0.02785
Epoch 11/80: current_loss=0.02861 | best_loss=0.02785
Epoch 12/80: current_loss=0.02856 | best_loss=0.02785
Epoch 13/80: current_loss=0.02745 | best_loss=0.02745
Epoch 14/80: current_loss=0.02886 | best_loss=0.02745
Epoch 15/80: current_loss=0.02933 | best_loss=0.02745
Epoch 16/80: current_loss=0.02812 | best_loss=0.02745
Epoch 17/80: current_loss=0.03058 | best_loss=0.02745
Epoch 18/80: current_loss=0.03050 | best_loss=0.02745
Epoch 19/80: current_loss=0.58968 | best_loss=0.02745
Epoch 20/80: current_loss=0.35850 | best_loss=0.02745
Epoch 21/80: current_loss=0.90211 | best_loss=0.02745
Epoch 22/80: current_loss=0.18931 | best_loss=0.02745
Epoch 23/80: current_loss=0.05463 | best_loss=0.02745
Epoch 24/80: current_loss=0.03544 | best_loss=0.02745
Epoch 25/80: current_loss=0.03125 | best_loss=0.02745
Epoch 26/80: current_loss=0.03744 | best_loss=0.02745
Epoch 27/80: current_loss=0.03071 | best_loss=0.02745
Epoch 28/80: current_loss=0.04681 | best_loss=0.02745
Epoch 29/80: current_loss=0.02948 | best_loss=0.02745
Epoch 30/80: current_loss=0.02965 | best_loss=0.02745
Epoch 31/80: current_loss=0.03124 | best_loss=0.02745
Epoch 32/80: current_loss=0.03036 | best_loss=0.02745
Epoch 33/80: current_loss=0.03182 | best_loss=0.02745
Early Stopping at epoch 33
      explained_var=0.01839 | mse_loss=0.02784
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04961 | best_loss=0.04961
Epoch 1/80: current_loss=0.03901 | best_loss=0.03901
Epoch 2/80: current_loss=0.03528 | best_loss=0.03528
Epoch 3/80: current_loss=0.05155 | best_loss=0.03528
Epoch 4/80: current_loss=0.04609 | best_loss=0.03528
Epoch 5/80: current_loss=0.03219 | best_loss=0.03219
Epoch 6/80: current_loss=0.03810 | best_loss=0.03219
Epoch 7/80: current_loss=0.04293 | best_loss=0.03219
Epoch 8/80: current_loss=0.04083 | best_loss=0.03219
Epoch 9/80: current_loss=0.03480 | best_loss=0.03219
Epoch 10/80: current_loss=0.03953 | best_loss=0.03219
Epoch 11/80: current_loss=0.03950 | best_loss=0.03219
Epoch 12/80: current_loss=0.03637 | best_loss=0.03219
Epoch 13/80: current_loss=0.03451 | best_loss=0.03219
Epoch 14/80: current_loss=0.04098 | best_loss=0.03219
Epoch 15/80: current_loss=0.04814 | best_loss=0.03219
Epoch 16/80: current_loss=0.03563 | best_loss=0.03219
Epoch 17/80: current_loss=0.03178 | best_loss=0.03178
Epoch 18/80: current_loss=0.03421 | best_loss=0.03178
Epoch 19/80: current_loss=0.03370 | best_loss=0.03178
Epoch 20/80: current_loss=0.03535 | best_loss=0.03178
Epoch 21/80: current_loss=0.03759 | best_loss=0.03178
Epoch 22/80: current_loss=0.03402 | best_loss=0.03178
Epoch 23/80: current_loss=0.03924 | best_loss=0.03178
Epoch 24/80: current_loss=0.03839 | best_loss=0.03178
Epoch 25/80: current_loss=0.04285 | best_loss=0.03178
Epoch 26/80: current_loss=0.03954 | best_loss=0.03178
Epoch 27/80: current_loss=0.04063 | best_loss=0.03178
Epoch 28/80: current_loss=0.04194 | best_loss=0.03178
Epoch 29/80: current_loss=0.04050 | best_loss=0.03178
Epoch 30/80: current_loss=0.04667 | best_loss=0.03178
Epoch 31/80: current_loss=0.03939 | best_loss=0.03178
Epoch 32/80: current_loss=0.03324 | best_loss=0.03178
Epoch 33/80: current_loss=0.04278 | best_loss=0.03178
Epoch 34/80: current_loss=0.03867 | best_loss=0.03178
Epoch 35/80: current_loss=0.03698 | best_loss=0.03178
Epoch 36/80: current_loss=0.04486 | best_loss=0.03178
Epoch 37/80: current_loss=0.04181 | best_loss=0.03178
Early Stopping at epoch 37
      explained_var=-0.00074 | mse_loss=0.03255
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.03251| avg_loss=0.02798
----------------------------------------------


----------------------------------------------
Params for Trial 81
{'learning_rate': 0.001, 'weight_decay': 0.00028140477332968844, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02821 | best_loss=0.02821
Epoch 1/80: current_loss=0.02728 | best_loss=0.02728
Epoch 2/80: current_loss=0.02806 | best_loss=0.02728
Epoch 3/80: current_loss=0.02584 | best_loss=0.02584
Epoch 4/80: current_loss=0.02853 | best_loss=0.02584
Epoch 5/80: current_loss=0.04521 | best_loss=0.02584
Epoch 6/80: current_loss=0.03339 | best_loss=0.02584
Epoch 7/80: current_loss=0.02822 | best_loss=0.02584
Epoch 8/80: current_loss=0.02727 | best_loss=0.02584
Epoch 9/80: current_loss=0.02577 | best_loss=0.02577
Epoch 10/80: current_loss=0.02496 | best_loss=0.02496
Epoch 11/80: current_loss=0.03240 | best_loss=0.02496
Epoch 12/80: current_loss=0.03076 | best_loss=0.02496
Epoch 13/80: current_loss=0.02729 | best_loss=0.02496
Epoch 14/80: current_loss=0.02981 | best_loss=0.02496
Epoch 15/80: current_loss=0.03179 | best_loss=0.02496
Epoch 16/80: current_loss=0.03029 | best_loss=0.02496
Epoch 17/80: current_loss=0.02499 | best_loss=0.02496
Epoch 18/80: current_loss=0.02728 | best_loss=0.02496
Epoch 19/80: current_loss=0.03343 | best_loss=0.02496
Epoch 20/80: current_loss=0.02764 | best_loss=0.02496
Epoch 21/80: current_loss=0.02649 | best_loss=0.02496
Epoch 22/80: current_loss=0.02753 | best_loss=0.02496
Epoch 23/80: current_loss=0.03485 | best_loss=0.02496
Epoch 24/80: current_loss=0.02832 | best_loss=0.02496
Epoch 25/80: current_loss=0.02831 | best_loss=0.02496
Epoch 26/80: current_loss=0.03325 | best_loss=0.02496
Epoch 27/80: current_loss=0.02493 | best_loss=0.02493
Epoch 28/80: current_loss=0.02910 | best_loss=0.02493
Epoch 29/80: current_loss=0.02716 | best_loss=0.02493
Epoch 30/80: current_loss=0.02766 | best_loss=0.02493
Epoch 31/80: current_loss=0.02657 | best_loss=0.02493
Epoch 32/80: current_loss=0.02938 | best_loss=0.02493
Epoch 33/80: current_loss=0.02522 | best_loss=0.02493
Epoch 34/80: current_loss=0.02692 | best_loss=0.02493
Epoch 35/80: current_loss=0.02624 | best_loss=0.02493
Epoch 36/80: current_loss=0.02772 | best_loss=0.02493
Epoch 37/80: current_loss=0.02549 | best_loss=0.02493
Epoch 38/80: current_loss=0.02523 | best_loss=0.02493
Epoch 39/80: current_loss=0.02683 | best_loss=0.02493
Epoch 40/80: current_loss=0.02757 | best_loss=0.02493
Epoch 41/80: current_loss=0.02712 | best_loss=0.02493
Epoch 42/80: current_loss=0.03264 | best_loss=0.02493
Epoch 43/80: current_loss=0.02680 | best_loss=0.02493
Epoch 44/80: current_loss=0.02534 | best_loss=0.02493
Epoch 45/80: current_loss=0.02647 | best_loss=0.02493
Epoch 46/80: current_loss=0.02630 | best_loss=0.02493
Epoch 47/80: current_loss=0.02798 | best_loss=0.02493
Early Stopping at epoch 47
      explained_var=0.05560 | mse_loss=0.02439
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03456 | best_loss=0.03456
Epoch 1/80: current_loss=0.02902 | best_loss=0.02902
Epoch 2/80: current_loss=0.02847 | best_loss=0.02847
Epoch 3/80: current_loss=0.03403 | best_loss=0.02847
Epoch 4/80: current_loss=0.03030 | best_loss=0.02847
Epoch 5/80: current_loss=0.02868 | best_loss=0.02847
Epoch 6/80: current_loss=0.03136 | best_loss=0.02847
Epoch 7/80: current_loss=0.02861 | best_loss=0.02847
Epoch 8/80: current_loss=0.02711 | best_loss=0.02711
Epoch 9/80: current_loss=0.02782 | best_loss=0.02711
Epoch 10/80: current_loss=0.02812 | best_loss=0.02711
Epoch 11/80: current_loss=0.02860 | best_loss=0.02711
Epoch 12/80: current_loss=0.02887 | best_loss=0.02711
Epoch 13/80: current_loss=0.02974 | best_loss=0.02711
Epoch 14/80: current_loss=0.03006 | best_loss=0.02711
Epoch 15/80: current_loss=0.03112 | best_loss=0.02711
Epoch 16/80: current_loss=0.02812 | best_loss=0.02711
Epoch 17/80: current_loss=0.03157 | best_loss=0.02711
Epoch 18/80: current_loss=0.03182 | best_loss=0.02711
Epoch 19/80: current_loss=0.02817 | best_loss=0.02711
Epoch 20/80: current_loss=0.02749 | best_loss=0.02711
Epoch 21/80: current_loss=0.03141 | best_loss=0.02711
Epoch 22/80: current_loss=0.02847 | best_loss=0.02711
Epoch 23/80: current_loss=0.03725 | best_loss=0.02711
Epoch 24/80: current_loss=0.02958 | best_loss=0.02711
Epoch 25/80: current_loss=0.02833 | best_loss=0.02711
Epoch 26/80: current_loss=0.02790 | best_loss=0.02711
Epoch 27/80: current_loss=0.02870 | best_loss=0.02711
Epoch 28/80: current_loss=0.02804 | best_loss=0.02711
Early Stopping at epoch 28
      explained_var=0.04472 | mse_loss=0.02656
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03205 | best_loss=0.03205
Epoch 1/80: current_loss=0.03710 | best_loss=0.03205
Epoch 2/80: current_loss=0.03760 | best_loss=0.03205
Epoch 3/80: current_loss=0.03185 | best_loss=0.03185
Epoch 4/80: current_loss=0.03037 | best_loss=0.03037
Epoch 5/80: current_loss=0.03038 | best_loss=0.03037
Epoch 6/80: current_loss=0.03072 | best_loss=0.03037
Epoch 7/80: current_loss=0.03498 | best_loss=0.03037
Epoch 8/80: current_loss=0.03095 | best_loss=0.03037
Epoch 9/80: current_loss=0.03312 | best_loss=0.03037
Epoch 10/80: current_loss=0.03038 | best_loss=0.03037
Epoch 11/80: current_loss=0.02976 | best_loss=0.02976
Epoch 12/80: current_loss=0.03367 | best_loss=0.02976
Epoch 13/80: current_loss=0.03107 | best_loss=0.02976
Epoch 14/80: current_loss=0.02979 | best_loss=0.02976
Epoch 15/80: current_loss=0.03090 | best_loss=0.02976
Epoch 16/80: current_loss=0.03556 | best_loss=0.02976
Epoch 17/80: current_loss=0.03205 | best_loss=0.02976
Epoch 18/80: current_loss=0.03040 | best_loss=0.02976
Epoch 19/80: current_loss=0.03054 | best_loss=0.02976
Epoch 20/80: current_loss=0.02954 | best_loss=0.02954
Epoch 21/80: current_loss=0.03364 | best_loss=0.02954
Epoch 22/80: current_loss=0.03395 | best_loss=0.02954
Epoch 23/80: current_loss=0.03320 | best_loss=0.02954
Epoch 24/80: current_loss=0.03184 | best_loss=0.02954
Epoch 25/80: current_loss=0.02943 | best_loss=0.02943
Epoch 26/80: current_loss=0.03010 | best_loss=0.02943
Epoch 27/80: current_loss=0.03080 | best_loss=0.02943
Epoch 28/80: current_loss=0.03078 | best_loss=0.02943
Epoch 29/80: current_loss=0.03653 | best_loss=0.02943
Epoch 30/80: current_loss=0.03386 | best_loss=0.02943
Epoch 31/80: current_loss=0.15621 | best_loss=0.02943
Epoch 32/80: current_loss=0.90675 | best_loss=0.02943
Epoch 33/80: current_loss=0.16089 | best_loss=0.02943
Epoch 34/80: current_loss=0.12642 | best_loss=0.02943
Epoch 35/80: current_loss=0.11570 | best_loss=0.02943
Epoch 36/80: current_loss=0.09144 | best_loss=0.02943
Epoch 37/80: current_loss=0.07955 | best_loss=0.02943
Epoch 38/80: current_loss=0.06967 | best_loss=0.02943
Epoch 39/80: current_loss=0.06867 | best_loss=0.02943
Epoch 40/80: current_loss=0.03937 | best_loss=0.02943
Epoch 41/80: current_loss=0.02975 | best_loss=0.02943
Epoch 42/80: current_loss=0.03676 | best_loss=0.02943
Epoch 43/80: current_loss=0.03353 | best_loss=0.02943
Epoch 44/80: current_loss=0.03685 | best_loss=0.02943
Epoch 45/80: current_loss=0.03141 | best_loss=0.02943
Early Stopping at epoch 45
      explained_var=0.03124 | mse_loss=0.02866
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04342 | best_loss=0.04342
Epoch 1/80: current_loss=0.03480 | best_loss=0.03480
Epoch 2/80: current_loss=0.05024 | best_loss=0.03480
Epoch 3/80: current_loss=0.03251 | best_loss=0.03251
Epoch 4/80: current_loss=0.03348 | best_loss=0.03251
Epoch 5/80: current_loss=0.03290 | best_loss=0.03251
Epoch 6/80: current_loss=0.03194 | best_loss=0.03194
Epoch 7/80: current_loss=0.03564 | best_loss=0.03194
Epoch 8/80: current_loss=0.03063 | best_loss=0.03063
Epoch 9/80: current_loss=0.03521 | best_loss=0.03063
Epoch 10/80: current_loss=0.03518 | best_loss=0.03063
Epoch 11/80: current_loss=0.02750 | best_loss=0.02750
Epoch 12/80: current_loss=0.03622 | best_loss=0.02750
Epoch 13/80: current_loss=0.03534 | best_loss=0.02750
Epoch 14/80: current_loss=0.03334 | best_loss=0.02750
Epoch 15/80: current_loss=0.03168 | best_loss=0.02750
Epoch 16/80: current_loss=0.03611 | best_loss=0.02750
Epoch 17/80: current_loss=0.03452 | best_loss=0.02750
Epoch 18/80: current_loss=0.05267 | best_loss=0.02750
Epoch 19/80: current_loss=0.03497 | best_loss=0.02750
Epoch 20/80: current_loss=0.04103 | best_loss=0.02750
Epoch 21/80: current_loss=0.03720 | best_loss=0.02750
Epoch 22/80: current_loss=0.03311 | best_loss=0.02750
Epoch 23/80: current_loss=0.03815 | best_loss=0.02750
Epoch 24/80: current_loss=0.03157 | best_loss=0.02750
Epoch 25/80: current_loss=0.03299 | best_loss=0.02750
Epoch 26/80: current_loss=0.03015 | best_loss=0.02750
Epoch 27/80: current_loss=0.03051 | best_loss=0.02750
Epoch 28/80: current_loss=0.03909 | best_loss=0.02750
Epoch 29/80: current_loss=0.03536 | best_loss=0.02750
Epoch 30/80: current_loss=0.03495 | best_loss=0.02750
Epoch 31/80: current_loss=0.03058 | best_loss=0.02750
Early Stopping at epoch 31
      explained_var=0.01728 | mse_loss=0.02808
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04134 | best_loss=0.04134
Epoch 1/80: current_loss=0.03486 | best_loss=0.03486
Epoch 2/80: current_loss=0.04249 | best_loss=0.03486
Epoch 3/80: current_loss=0.03570 | best_loss=0.03486
Epoch 4/80: current_loss=0.03887 | best_loss=0.03486
Epoch 5/80: current_loss=0.03378 | best_loss=0.03378
Epoch 6/80: current_loss=0.03164 | best_loss=0.03164
Epoch 7/80: current_loss=0.04209 | best_loss=0.03164
Epoch 8/80: current_loss=0.03134 | best_loss=0.03134
Epoch 9/80: current_loss=0.03274 | best_loss=0.03134
Epoch 10/80: current_loss=0.05231 | best_loss=0.03134
Epoch 11/80: current_loss=0.04818 | best_loss=0.03134
Epoch 12/80: current_loss=0.03182 | best_loss=0.03134
Epoch 13/80: current_loss=0.03220 | best_loss=0.03134
Epoch 14/80: current_loss=0.04655 | best_loss=0.03134
Epoch 15/80: current_loss=0.04128 | best_loss=0.03134
Epoch 16/80: current_loss=0.03480 | best_loss=0.03134
Epoch 17/80: current_loss=0.03395 | best_loss=0.03134
Epoch 18/80: current_loss=0.03642 | best_loss=0.03134
Epoch 19/80: current_loss=0.03702 | best_loss=0.03134
Epoch 20/80: current_loss=0.03170 | best_loss=0.03134
Epoch 21/80: current_loss=0.04030 | best_loss=0.03134
Epoch 22/80: current_loss=0.04758 | best_loss=0.03134
Epoch 23/80: current_loss=0.04246 | best_loss=0.03134
Epoch 24/80: current_loss=0.04871 | best_loss=0.03134
Epoch 25/80: current_loss=0.03712 | best_loss=0.03134
Epoch 26/80: current_loss=0.03633 | best_loss=0.03134
Epoch 27/80: current_loss=0.03706 | best_loss=0.03134
Epoch 28/80: current_loss=0.04069 | best_loss=0.03134
Early Stopping at epoch 28
      explained_var=0.01541 | mse_loss=0.03222
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.03285| avg_loss=0.02798
----------------------------------------------


----------------------------------------------
Params for Trial 82
{'learning_rate': 0.001, 'weight_decay': 0.0002579656617129846, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03318 | best_loss=0.03318
Epoch 1/80: current_loss=0.02694 | best_loss=0.02694
Epoch 2/80: current_loss=0.02674 | best_loss=0.02674
Epoch 3/80: current_loss=0.03010 | best_loss=0.02674
Epoch 4/80: current_loss=0.02942 | best_loss=0.02674
Epoch 5/80: current_loss=0.02928 | best_loss=0.02674
Epoch 6/80: current_loss=0.02642 | best_loss=0.02642
Epoch 7/80: current_loss=0.02792 | best_loss=0.02642
Epoch 8/80: current_loss=0.02645 | best_loss=0.02642
Epoch 9/80: current_loss=0.02625 | best_loss=0.02625
Epoch 10/80: current_loss=0.02521 | best_loss=0.02521
Epoch 11/80: current_loss=0.02691 | best_loss=0.02521
Epoch 12/80: current_loss=0.02664 | best_loss=0.02521
Epoch 13/80: current_loss=0.02989 | best_loss=0.02521
Epoch 14/80: current_loss=0.02929 | best_loss=0.02521
Epoch 15/80: current_loss=0.02822 | best_loss=0.02521
Epoch 16/80: current_loss=0.02666 | best_loss=0.02521
Epoch 17/80: current_loss=0.02546 | best_loss=0.02521
Epoch 18/80: current_loss=0.02657 | best_loss=0.02521
Epoch 19/80: current_loss=0.02695 | best_loss=0.02521
Epoch 20/80: current_loss=0.02684 | best_loss=0.02521
Epoch 21/80: current_loss=0.02742 | best_loss=0.02521
Epoch 22/80: current_loss=0.02688 | best_loss=0.02521
Epoch 23/80: current_loss=0.02518 | best_loss=0.02518
Epoch 24/80: current_loss=0.03676 | best_loss=0.02518
Epoch 25/80: current_loss=0.04467 | best_loss=0.02518
Epoch 26/80: current_loss=0.02667 | best_loss=0.02518
Epoch 27/80: current_loss=0.02521 | best_loss=0.02518
Epoch 28/80: current_loss=0.02798 | best_loss=0.02518
Epoch 29/80: current_loss=0.02498 | best_loss=0.02498
Epoch 30/80: current_loss=0.02944 | best_loss=0.02498
Epoch 31/80: current_loss=0.02555 | best_loss=0.02498
Epoch 32/80: current_loss=0.02819 | best_loss=0.02498
Epoch 33/80: current_loss=0.03358 | best_loss=0.02498
Epoch 34/80: current_loss=0.02678 | best_loss=0.02498
Epoch 35/80: current_loss=0.03079 | best_loss=0.02498
Epoch 36/80: current_loss=0.03121 | best_loss=0.02498
Epoch 37/80: current_loss=0.02633 | best_loss=0.02498
Epoch 38/80: current_loss=0.02584 | best_loss=0.02498
Epoch 39/80: current_loss=0.02607 | best_loss=0.02498
Epoch 40/80: current_loss=0.02615 | best_loss=0.02498
Epoch 41/80: current_loss=0.02870 | best_loss=0.02498
Epoch 42/80: current_loss=0.02699 | best_loss=0.02498
Epoch 43/80: current_loss=0.02861 | best_loss=0.02498
Epoch 44/80: current_loss=0.02524 | best_loss=0.02498
Epoch 45/80: current_loss=0.02820 | best_loss=0.02498
Epoch 46/80: current_loss=0.02519 | best_loss=0.02498
Epoch 47/80: current_loss=0.02509 | best_loss=0.02498
Epoch 48/80: current_loss=0.02520 | best_loss=0.02498
Epoch 49/80: current_loss=0.02545 | best_loss=0.02498
Early Stopping at epoch 49
      explained_var=0.05691 | mse_loss=0.02451
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03281 | best_loss=0.03281
Epoch 1/80: current_loss=0.03039 | best_loss=0.03039
Epoch 2/80: current_loss=0.03003 | best_loss=0.03003
Epoch 3/80: current_loss=0.02950 | best_loss=0.02950
Epoch 4/80: current_loss=0.02880 | best_loss=0.02880
Epoch 5/80: current_loss=0.03046 | best_loss=0.02880
Epoch 6/80: current_loss=0.02813 | best_loss=0.02813
Epoch 7/80: current_loss=0.02851 | best_loss=0.02813
Epoch 8/80: current_loss=0.02822 | best_loss=0.02813
Epoch 9/80: current_loss=0.03030 | best_loss=0.02813
Epoch 10/80: current_loss=0.02807 | best_loss=0.02807
Epoch 11/80: current_loss=0.03203 | best_loss=0.02807
Epoch 12/80: current_loss=0.02830 | best_loss=0.02807
Epoch 13/80: current_loss=0.02995 | best_loss=0.02807
Epoch 14/80: current_loss=0.02870 | best_loss=0.02807
Epoch 15/80: current_loss=0.02819 | best_loss=0.02807
Epoch 16/80: current_loss=0.03030 | best_loss=0.02807
Epoch 17/80: current_loss=0.02827 | best_loss=0.02807
Epoch 18/80: current_loss=0.02789 | best_loss=0.02789
Epoch 19/80: current_loss=0.02853 | best_loss=0.02789
Epoch 20/80: current_loss=0.02851 | best_loss=0.02789
Epoch 21/80: current_loss=0.02777 | best_loss=0.02777
Epoch 22/80: current_loss=0.03120 | best_loss=0.02777
Epoch 23/80: current_loss=0.02844 | best_loss=0.02777
Epoch 24/80: current_loss=0.02850 | best_loss=0.02777
Epoch 25/80: current_loss=0.02887 | best_loss=0.02777
Epoch 26/80: current_loss=0.02833 | best_loss=0.02777
Epoch 27/80: current_loss=0.02841 | best_loss=0.02777
Epoch 28/80: current_loss=0.03186 | best_loss=0.02777
Epoch 29/80: current_loss=0.03110 | best_loss=0.02777
Epoch 30/80: current_loss=0.03022 | best_loss=0.02777
Epoch 31/80: current_loss=0.02779 | best_loss=0.02777
Epoch 32/80: current_loss=0.02773 | best_loss=0.02773
Epoch 33/80: current_loss=0.03104 | best_loss=0.02773
Epoch 34/80: current_loss=0.03142 | best_loss=0.02773
Epoch 35/80: current_loss=0.02766 | best_loss=0.02766
Epoch 36/80: current_loss=0.02888 | best_loss=0.02766
Epoch 37/80: current_loss=0.02787 | best_loss=0.02766
Epoch 38/80: current_loss=0.02807 | best_loss=0.02766
Epoch 39/80: current_loss=0.02785 | best_loss=0.02766
Epoch 40/80: current_loss=0.02899 | best_loss=0.02766
Epoch 41/80: current_loss=0.02834 | best_loss=0.02766
Epoch 42/80: current_loss=0.02768 | best_loss=0.02766
Epoch 43/80: current_loss=0.02783 | best_loss=0.02766
Epoch 44/80: current_loss=0.02815 | best_loss=0.02766
Epoch 45/80: current_loss=0.02772 | best_loss=0.02766
Epoch 46/80: current_loss=0.03045 | best_loss=0.02766
Epoch 47/80: current_loss=0.03024 | best_loss=0.02766
Epoch 48/80: current_loss=0.02778 | best_loss=0.02766
Epoch 49/80: current_loss=0.02822 | best_loss=0.02766
Epoch 50/80: current_loss=0.02810 | best_loss=0.02766
Epoch 51/80: current_loss=0.02881 | best_loss=0.02766
Epoch 52/80: current_loss=0.02842 | best_loss=0.02766
Epoch 53/80: current_loss=0.02830 | best_loss=0.02766
Epoch 54/80: current_loss=0.02872 | best_loss=0.02766
Epoch 55/80: current_loss=0.02901 | best_loss=0.02766
Early Stopping at epoch 55
      explained_var=0.02343 | mse_loss=0.02715
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02957 | best_loss=0.02957
Epoch 1/80: current_loss=0.03028 | best_loss=0.02957
Epoch 2/80: current_loss=0.03264 | best_loss=0.02957
Epoch 3/80: current_loss=0.03119 | best_loss=0.02957
Epoch 4/80: current_loss=0.03265 | best_loss=0.02957
Epoch 5/80: current_loss=0.02982 | best_loss=0.02957
Epoch 6/80: current_loss=0.02934 | best_loss=0.02934
Epoch 7/80: current_loss=0.02922 | best_loss=0.02922
Epoch 8/80: current_loss=0.03097 | best_loss=0.02922
Epoch 9/80: current_loss=0.03302 | best_loss=0.02922
Epoch 10/80: current_loss=0.02956 | best_loss=0.02922
Epoch 11/80: current_loss=0.02997 | best_loss=0.02922
Epoch 12/80: current_loss=0.03296 | best_loss=0.02922
Epoch 13/80: current_loss=0.03126 | best_loss=0.02922
Epoch 14/80: current_loss=0.02984 | best_loss=0.02922
Epoch 15/80: current_loss=0.03346 | best_loss=0.02922
Epoch 16/80: current_loss=0.03352 | best_loss=0.02922
Epoch 17/80: current_loss=0.03241 | best_loss=0.02922
Epoch 18/80: current_loss=0.03012 | best_loss=0.02922
Epoch 19/80: current_loss=0.03515 | best_loss=0.02922
Epoch 20/80: current_loss=0.03052 | best_loss=0.02922
Epoch 21/80: current_loss=0.03690 | best_loss=0.02922
Epoch 22/80: current_loss=0.03285 | best_loss=0.02922
Epoch 23/80: current_loss=0.02972 | best_loss=0.02922
Epoch 24/80: current_loss=0.02969 | best_loss=0.02922
Epoch 25/80: current_loss=0.03011 | best_loss=0.02922
Epoch 26/80: current_loss=0.03054 | best_loss=0.02922
Epoch 27/80: current_loss=0.03193 | best_loss=0.02922
Early Stopping at epoch 27
      explained_var=0.03865 | mse_loss=0.02845
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03101 | best_loss=0.03101
Epoch 1/80: current_loss=0.02875 | best_loss=0.02875
Epoch 2/80: current_loss=0.02928 | best_loss=0.02875
Epoch 3/80: current_loss=0.02932 | best_loss=0.02875
Epoch 4/80: current_loss=0.02788 | best_loss=0.02788
Epoch 5/80: current_loss=0.02805 | best_loss=0.02788
Epoch 6/80: current_loss=0.02815 | best_loss=0.02788
Epoch 7/80: current_loss=0.02791 | best_loss=0.02788
Epoch 8/80: current_loss=0.02823 | best_loss=0.02788
Epoch 9/80: current_loss=0.02766 | best_loss=0.02766
Epoch 10/80: current_loss=0.02774 | best_loss=0.02766
Epoch 11/80: current_loss=0.02842 | best_loss=0.02766
Epoch 12/80: current_loss=0.03030 | best_loss=0.02766
Epoch 13/80: current_loss=0.03054 | best_loss=0.02766
Epoch 14/80: current_loss=0.02793 | best_loss=0.02766
Epoch 15/80: current_loss=0.02945 | best_loss=0.02766
Epoch 16/80: current_loss=0.02820 | best_loss=0.02766
Epoch 17/80: current_loss=0.02822 | best_loss=0.02766
Epoch 18/80: current_loss=0.02830 | best_loss=0.02766
Epoch 19/80: current_loss=0.02820 | best_loss=0.02766
Epoch 20/80: current_loss=0.03002 | best_loss=0.02766
Epoch 21/80: current_loss=0.02862 | best_loss=0.02766
Epoch 22/80: current_loss=0.02819 | best_loss=0.02766
Epoch 23/80: current_loss=0.02857 | best_loss=0.02766
Epoch 24/80: current_loss=0.02894 | best_loss=0.02766
Epoch 25/80: current_loss=0.02907 | best_loss=0.02766
Epoch 26/80: current_loss=0.02851 | best_loss=0.02766
Epoch 27/80: current_loss=0.02901 | best_loss=0.02766
Epoch 28/80: current_loss=0.02896 | best_loss=0.02766
Epoch 29/80: current_loss=0.02998 | best_loss=0.02766
Early Stopping at epoch 29
      explained_var=0.02092 | mse_loss=0.02798
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03505 | best_loss=0.03505
Epoch 1/80: current_loss=0.03466 | best_loss=0.03466
Epoch 2/80: current_loss=0.03414 | best_loss=0.03414
Epoch 3/80: current_loss=0.03538 | best_loss=0.03414
Epoch 4/80: current_loss=0.03312 | best_loss=0.03312
Epoch 5/80: current_loss=0.03462 | best_loss=0.03312
Epoch 6/80: current_loss=0.03487 | best_loss=0.03312
Epoch 7/80: current_loss=0.03419 | best_loss=0.03312
Epoch 8/80: current_loss=0.03720 | best_loss=0.03312
Epoch 9/80: current_loss=0.03386 | best_loss=0.03312
Epoch 10/80: current_loss=0.03345 | best_loss=0.03312
Epoch 11/80: current_loss=0.03407 | best_loss=0.03312
Epoch 12/80: current_loss=0.03509 | best_loss=0.03312
Epoch 13/80: current_loss=0.03614 | best_loss=0.03312
Epoch 14/80: current_loss=0.03514 | best_loss=0.03312
Epoch 15/80: current_loss=0.03546 | best_loss=0.03312
Epoch 16/80: current_loss=0.03443 | best_loss=0.03312
Epoch 17/80: current_loss=0.03460 | best_loss=0.03312
Epoch 18/80: current_loss=0.03456 | best_loss=0.03312
Epoch 19/80: current_loss=0.03426 | best_loss=0.03312
Epoch 20/80: current_loss=0.03447 | best_loss=0.03312
Epoch 21/80: current_loss=0.03436 | best_loss=0.03312
Epoch 22/80: current_loss=0.03421 | best_loss=0.03312
Epoch 23/80: current_loss=0.03382 | best_loss=0.03312
Epoch 24/80: current_loss=0.03401 | best_loss=0.03312
Early Stopping at epoch 24
      explained_var=-0.03051 | mse_loss=0.03384
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.02188| avg_loss=0.02839
----------------------------------------------


----------------------------------------------
Params for Trial 83
{'learning_rate': 0.001, 'weight_decay': 0.00027352441499345635, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02970 | best_loss=0.02970
Epoch 1/80: current_loss=0.02763 | best_loss=0.02763
Epoch 2/80: current_loss=0.02725 | best_loss=0.02725
Epoch 3/80: current_loss=0.02858 | best_loss=0.02725
Epoch 4/80: current_loss=0.04263 | best_loss=0.02725
Epoch 5/80: current_loss=0.02623 | best_loss=0.02623
Epoch 6/80: current_loss=0.02934 | best_loss=0.02623
Epoch 7/80: current_loss=0.02562 | best_loss=0.02562
Epoch 8/80: current_loss=0.02679 | best_loss=0.02562
Epoch 9/80: current_loss=0.02725 | best_loss=0.02562
Epoch 10/80: current_loss=0.02773 | best_loss=0.02562
Epoch 11/80: current_loss=0.02892 | best_loss=0.02562
Epoch 12/80: current_loss=0.02804 | best_loss=0.02562
Epoch 13/80: current_loss=0.02537 | best_loss=0.02537
Epoch 14/80: current_loss=0.02579 | best_loss=0.02537
Epoch 15/80: current_loss=0.02648 | best_loss=0.02537
Epoch 16/80: current_loss=0.02550 | best_loss=0.02537
Epoch 17/80: current_loss=0.02713 | best_loss=0.02537
Epoch 18/80: current_loss=0.02666 | best_loss=0.02537
Epoch 19/80: current_loss=0.02667 | best_loss=0.02537
Epoch 20/80: current_loss=0.02946 | best_loss=0.02537
Epoch 21/80: current_loss=0.03277 | best_loss=0.02537
Epoch 22/80: current_loss=0.02636 | best_loss=0.02537
Epoch 23/80: current_loss=0.02583 | best_loss=0.02537
Epoch 24/80: current_loss=0.02532 | best_loss=0.02532
Epoch 25/80: current_loss=0.02562 | best_loss=0.02532
Epoch 26/80: current_loss=0.02566 | best_loss=0.02532
Epoch 27/80: current_loss=0.02559 | best_loss=0.02532
Epoch 28/80: current_loss=0.02578 | best_loss=0.02532
Epoch 29/80: current_loss=0.02964 | best_loss=0.02532
Epoch 30/80: current_loss=0.03050 | best_loss=0.02532
Epoch 31/80: current_loss=0.02708 | best_loss=0.02532
Epoch 32/80: current_loss=0.02574 | best_loss=0.02532
Epoch 33/80: current_loss=0.02536 | best_loss=0.02532
Epoch 34/80: current_loss=0.02639 | best_loss=0.02532
Epoch 35/80: current_loss=0.02795 | best_loss=0.02532
Epoch 36/80: current_loss=0.02809 | best_loss=0.02532
Epoch 37/80: current_loss=0.02569 | best_loss=0.02532
Epoch 38/80: current_loss=0.02589 | best_loss=0.02532
Epoch 39/80: current_loss=0.02556 | best_loss=0.02532
Epoch 40/80: current_loss=0.02567 | best_loss=0.02532
Epoch 41/80: current_loss=0.02649 | best_loss=0.02532
Epoch 42/80: current_loss=0.02734 | best_loss=0.02532
Epoch 43/80: current_loss=0.02696 | best_loss=0.02532
Epoch 44/80: current_loss=0.02597 | best_loss=0.02532
Early Stopping at epoch 44
      explained_var=0.04439 | mse_loss=0.02492

----------------------------------------------
Params for Trial 84
{'learning_rate': 0.001, 'weight_decay': 1.6097203234986964e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03117 | best_loss=0.03117
Epoch 1/80: current_loss=0.02625 | best_loss=0.02625
Epoch 2/80: current_loss=0.03867 | best_loss=0.02625
Epoch 3/80: current_loss=0.02550 | best_loss=0.02550
Epoch 4/80: current_loss=0.02548 | best_loss=0.02548
Epoch 5/80: current_loss=0.02582 | best_loss=0.02548
Epoch 6/80: current_loss=0.03271 | best_loss=0.02548
Epoch 7/80: current_loss=0.02816 | best_loss=0.02548
Epoch 8/80: current_loss=0.04015 | best_loss=0.02548
Epoch 9/80: current_loss=0.03003 | best_loss=0.02548
Epoch 10/80: current_loss=0.02749 | best_loss=0.02548
Epoch 11/80: current_loss=0.02793 | best_loss=0.02548
Epoch 12/80: current_loss=0.02523 | best_loss=0.02523
Epoch 13/80: current_loss=0.02579 | best_loss=0.02523
Epoch 14/80: current_loss=0.04116 | best_loss=0.02523
Epoch 15/80: current_loss=0.03070 | best_loss=0.02523
Epoch 16/80: current_loss=0.03087 | best_loss=0.02523
Epoch 17/80: current_loss=0.03209 | best_loss=0.02523
Epoch 18/80: current_loss=0.02855 | best_loss=0.02523
Epoch 19/80: current_loss=0.02826 | best_loss=0.02523
Epoch 20/80: current_loss=0.02746 | best_loss=0.02523
Epoch 21/80: current_loss=0.02605 | best_loss=0.02523
Epoch 22/80: current_loss=0.02835 | best_loss=0.02523
Epoch 23/80: current_loss=0.02959 | best_loss=0.02523
Epoch 24/80: current_loss=0.02818 | best_loss=0.02523
Epoch 25/80: current_loss=0.02667 | best_loss=0.02523
Epoch 26/80: current_loss=0.02628 | best_loss=0.02523
Epoch 27/80: current_loss=0.02649 | best_loss=0.02523
Epoch 28/80: current_loss=0.02606 | best_loss=0.02523
Epoch 29/80: current_loss=0.03262 | best_loss=0.02523
Epoch 30/80: current_loss=0.03609 | best_loss=0.02523
Epoch 31/80: current_loss=0.02834 | best_loss=0.02523
Epoch 32/80: current_loss=0.02748 | best_loss=0.02523
Early Stopping at epoch 32
      explained_var=0.04453 | mse_loss=0.02467
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02932 | best_loss=0.02932
Epoch 1/80: current_loss=0.02883 | best_loss=0.02883
Epoch 2/80: current_loss=0.02752 | best_loss=0.02752
Epoch 3/80: current_loss=0.02925 | best_loss=0.02752
Epoch 4/80: current_loss=0.02840 | best_loss=0.02752
Epoch 5/80: current_loss=0.03033 | best_loss=0.02752
Epoch 6/80: current_loss=0.02836 | best_loss=0.02752
Epoch 7/80: current_loss=0.02854 | best_loss=0.02752
Epoch 8/80: current_loss=0.02766 | best_loss=0.02752
Epoch 9/80: current_loss=0.02804 | best_loss=0.02752
Epoch 10/80: current_loss=0.02746 | best_loss=0.02746
Epoch 11/80: current_loss=0.02787 | best_loss=0.02746
Epoch 12/80: current_loss=0.02824 | best_loss=0.02746
Epoch 13/80: current_loss=0.03238 | best_loss=0.02746
Epoch 14/80: current_loss=0.02893 | best_loss=0.02746
Epoch 15/80: current_loss=0.02876 | best_loss=0.02746
Epoch 16/80: current_loss=0.02933 | best_loss=0.02746
Epoch 17/80: current_loss=0.02886 | best_loss=0.02746
Epoch 18/80: current_loss=0.02790 | best_loss=0.02746
Epoch 19/80: current_loss=0.02853 | best_loss=0.02746
Epoch 20/80: current_loss=0.02779 | best_loss=0.02746
Epoch 21/80: current_loss=0.02974 | best_loss=0.02746
Epoch 22/80: current_loss=0.03151 | best_loss=0.02746
Epoch 23/80: current_loss=0.02977 | best_loss=0.02746
Epoch 24/80: current_loss=0.02846 | best_loss=0.02746
Epoch 25/80: current_loss=0.03249 | best_loss=0.02746
Epoch 26/80: current_loss=0.02831 | best_loss=0.02746
Epoch 27/80: current_loss=0.03066 | best_loss=0.02746
Epoch 28/80: current_loss=0.02932 | best_loss=0.02746
Epoch 29/80: current_loss=0.02943 | best_loss=0.02746
Epoch 30/80: current_loss=0.02815 | best_loss=0.02746
Early Stopping at epoch 30
      explained_var=0.03086 | mse_loss=0.02693
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03290 | best_loss=0.03290
Epoch 1/80: current_loss=0.03042 | best_loss=0.03042
Epoch 2/80: current_loss=0.03460 | best_loss=0.03042
Epoch 3/80: current_loss=0.03105 | best_loss=0.03042
Epoch 4/80: current_loss=0.02985 | best_loss=0.02985
Epoch 5/80: current_loss=0.03653 | best_loss=0.02985
Epoch 6/80: current_loss=0.02975 | best_loss=0.02975
Epoch 7/80: current_loss=0.02898 | best_loss=0.02898
Epoch 8/80: current_loss=0.02949 | best_loss=0.02898
Epoch 9/80: current_loss=0.03033 | best_loss=0.02898
Epoch 10/80: current_loss=0.03394 | best_loss=0.02898
Epoch 11/80: current_loss=0.02992 | best_loss=0.02898
Epoch 12/80: current_loss=0.02993 | best_loss=0.02898
Epoch 13/80: current_loss=0.03055 | best_loss=0.02898
Epoch 14/80: current_loss=0.02945 | best_loss=0.02898
Epoch 15/80: current_loss=0.02944 | best_loss=0.02898
Epoch 16/80: current_loss=0.03321 | best_loss=0.02898
Epoch 17/80: current_loss=0.03246 | best_loss=0.02898
Epoch 18/80: current_loss=0.03013 | best_loss=0.02898
Epoch 19/80: current_loss=0.02988 | best_loss=0.02898
Epoch 20/80: current_loss=0.07476 | best_loss=0.02898
Epoch 21/80: current_loss=0.09959 | best_loss=0.02898
Epoch 22/80: current_loss=0.15057 | best_loss=0.02898
Epoch 23/80: current_loss=0.08840 | best_loss=0.02898
Epoch 24/80: current_loss=0.03229 | best_loss=0.02898
Epoch 25/80: current_loss=0.03171 | best_loss=0.02898
Epoch 26/80: current_loss=0.03890 | best_loss=0.02898
Epoch 27/80: current_loss=0.03402 | best_loss=0.02898
Early Stopping at epoch 27
      explained_var=0.04812 | mse_loss=0.02816
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04160 | best_loss=0.04160
Epoch 1/80: current_loss=0.04403 | best_loss=0.04160
Epoch 2/80: current_loss=0.03529 | best_loss=0.03529
Epoch 3/80: current_loss=0.03078 | best_loss=0.03078
Epoch 4/80: current_loss=0.03047 | best_loss=0.03047
Epoch 5/80: current_loss=0.02809 | best_loss=0.02809
Epoch 6/80: current_loss=0.04028 | best_loss=0.02809
Epoch 7/80: current_loss=0.03174 | best_loss=0.02809
Epoch 8/80: current_loss=0.04073 | best_loss=0.02809
Epoch 9/80: current_loss=0.04277 | best_loss=0.02809
Epoch 10/80: current_loss=0.03150 | best_loss=0.02809
Epoch 11/80: current_loss=0.02932 | best_loss=0.02809
Epoch 12/80: current_loss=0.03005 | best_loss=0.02809
Epoch 13/80: current_loss=0.03081 | best_loss=0.02809
Epoch 14/80: current_loss=0.03236 | best_loss=0.02809
Epoch 15/80: current_loss=0.02814 | best_loss=0.02809
Epoch 16/80: current_loss=0.03032 | best_loss=0.02809
Epoch 17/80: current_loss=0.02850 | best_loss=0.02809
Epoch 18/80: current_loss=0.02952 | best_loss=0.02809
Epoch 19/80: current_loss=0.02851 | best_loss=0.02809
Epoch 20/80: current_loss=0.02834 | best_loss=0.02809
Epoch 21/80: current_loss=0.02926 | best_loss=0.02809
Epoch 22/80: current_loss=0.03132 | best_loss=0.02809
Epoch 23/80: current_loss=0.03110 | best_loss=0.02809
Epoch 24/80: current_loss=0.03786 | best_loss=0.02809
Epoch 25/80: current_loss=0.02761 | best_loss=0.02761
Epoch 26/80: current_loss=0.03431 | best_loss=0.02761
Epoch 27/80: current_loss=0.03215 | best_loss=0.02761
Epoch 28/80: current_loss=0.03582 | best_loss=0.02761
Epoch 29/80: current_loss=0.03508 | best_loss=0.02761
Epoch 30/80: current_loss=0.02926 | best_loss=0.02761
Epoch 31/80: current_loss=0.04978 | best_loss=0.02761
Epoch 32/80: current_loss=0.02833 | best_loss=0.02761
Epoch 33/80: current_loss=0.02793 | best_loss=0.02761
Epoch 34/80: current_loss=0.03184 | best_loss=0.02761
Epoch 35/80: current_loss=0.02931 | best_loss=0.02761
Epoch 36/80: current_loss=0.02984 | best_loss=0.02761
Epoch 37/80: current_loss=0.02771 | best_loss=0.02761
Epoch 38/80: current_loss=0.04123 | best_loss=0.02761
Epoch 39/80: current_loss=0.02854 | best_loss=0.02761
Epoch 40/80: current_loss=0.03332 | best_loss=0.02761
Epoch 41/80: current_loss=0.02811 | best_loss=0.02761
Epoch 42/80: current_loss=0.02949 | best_loss=0.02761
Epoch 43/80: current_loss=0.03381 | best_loss=0.02761
Epoch 44/80: current_loss=0.04376 | best_loss=0.02761
Epoch 45/80: current_loss=0.02841 | best_loss=0.02761
Early Stopping at epoch 45
      explained_var=0.01565 | mse_loss=0.02792
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03227 | best_loss=0.03227
Epoch 1/80: current_loss=0.03188 | best_loss=0.03188
Epoch 2/80: current_loss=0.03418 | best_loss=0.03188
Epoch 3/80: current_loss=0.03211 | best_loss=0.03188
Epoch 4/80: current_loss=0.03247 | best_loss=0.03188
Epoch 5/80: current_loss=0.03208 | best_loss=0.03188
Epoch 6/80: current_loss=0.03257 | best_loss=0.03188
Epoch 7/80: current_loss=0.03206 | best_loss=0.03188
Epoch 8/80: current_loss=0.03271 | best_loss=0.03188
Epoch 9/80: current_loss=0.03211 | best_loss=0.03188
Epoch 10/80: current_loss=0.03544 | best_loss=0.03188
Epoch 11/80: current_loss=0.03265 | best_loss=0.03188
Epoch 12/80: current_loss=0.03365 | best_loss=0.03188
Epoch 13/80: current_loss=0.03302 | best_loss=0.03188
Epoch 14/80: current_loss=0.04197 | best_loss=0.03188
Epoch 15/80: current_loss=0.03305 | best_loss=0.03188
Epoch 16/80: current_loss=0.03581 | best_loss=0.03188
Epoch 17/80: current_loss=0.03248 | best_loss=0.03188
Epoch 18/80: current_loss=0.05177 | best_loss=0.03188
Epoch 19/80: current_loss=0.03332 | best_loss=0.03188
Epoch 20/80: current_loss=0.03896 | best_loss=0.03188
Epoch 21/80: current_loss=0.05205 | best_loss=0.03188
Early Stopping at epoch 21
      explained_var=-0.00229 | mse_loss=0.03263
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.02737| avg_loss=0.02806
----------------------------------------------


----------------------------------------------
Params for Trial 85
{'learning_rate': 0.1, 'weight_decay': 0.0005470036438079246, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=23.03667 | best_loss=23.03667
Epoch 1/80: current_loss=12.54920 | best_loss=12.54920
Epoch 2/80: current_loss=5.76568 | best_loss=5.76568
Epoch 3/80: current_loss=0.97207 | best_loss=0.97207
Epoch 4/80: current_loss=4.67609 | best_loss=0.97207
Epoch 5/80: current_loss=2.65268 | best_loss=0.97207
Epoch 6/80: current_loss=3.66301 | best_loss=0.97207
Epoch 7/80: current_loss=0.95407 | best_loss=0.95407
Epoch 8/80: current_loss=1.07295 | best_loss=0.95407
Epoch 9/80: current_loss=1.94417 | best_loss=0.95407
Epoch 10/80: current_loss=0.08794 | best_loss=0.08794
Epoch 11/80: current_loss=0.35629 | best_loss=0.08794
Epoch 12/80: current_loss=0.13264 | best_loss=0.08794
Epoch 13/80: current_loss=0.10904 | best_loss=0.08794
Epoch 14/80: current_loss=0.11687 | best_loss=0.08794
Epoch 15/80: current_loss=0.25509 | best_loss=0.08794
Epoch 16/80: current_loss=0.02693 | best_loss=0.02693
Epoch 17/80: current_loss=0.03790 | best_loss=0.02693
Epoch 18/80: current_loss=0.60007 | best_loss=0.02693
Epoch 19/80: current_loss=0.23461 | best_loss=0.02693
Epoch 20/80: current_loss=0.33729 | best_loss=0.02693
Epoch 21/80: current_loss=0.30796 | best_loss=0.02693
Epoch 22/80: current_loss=0.11767 | best_loss=0.02693
Epoch 23/80: current_loss=0.06520 | best_loss=0.02693
Epoch 24/80: current_loss=0.15433 | best_loss=0.02693
Epoch 25/80: current_loss=0.04299 | best_loss=0.02693
Epoch 26/80: current_loss=0.08056 | best_loss=0.02693
Epoch 27/80: current_loss=0.04894 | best_loss=0.02693
Epoch 28/80: current_loss=0.09424 | best_loss=0.02693
Epoch 29/80: current_loss=0.11314 | best_loss=0.02693
Epoch 30/80: current_loss=0.13107 | best_loss=0.02693
Epoch 31/80: current_loss=0.10268 | best_loss=0.02693
Epoch 32/80: current_loss=0.07983 | best_loss=0.02693
Epoch 33/80: current_loss=0.10945 | best_loss=0.02693
Epoch 34/80: current_loss=0.08868 | best_loss=0.02693
Epoch 35/80: current_loss=0.05317 | best_loss=0.02693
Epoch 36/80: current_loss=0.05248 | best_loss=0.02693
Early Stopping at epoch 36
      explained_var=0.00615 | mse_loss=0.02656

----------------------------------------------
Params for Trial 86
{'learning_rate': 0.001, 'weight_decay': 0.0010953000114728565, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03098 | best_loss=0.03098
Epoch 1/80: current_loss=0.03121 | best_loss=0.03098
Epoch 2/80: current_loss=0.02724 | best_loss=0.02724
Epoch 3/80: current_loss=0.03141 | best_loss=0.02724
Epoch 4/80: current_loss=0.03103 | best_loss=0.02724
Epoch 5/80: current_loss=0.02802 | best_loss=0.02724
Epoch 6/80: current_loss=0.02582 | best_loss=0.02582
Epoch 7/80: current_loss=0.02722 | best_loss=0.02582
Epoch 8/80: current_loss=0.04273 | best_loss=0.02582
Epoch 9/80: current_loss=0.03824 | best_loss=0.02582
Epoch 10/80: current_loss=0.03422 | best_loss=0.02582
Epoch 11/80: current_loss=0.02963 | best_loss=0.02582
Epoch 12/80: current_loss=0.02619 | best_loss=0.02582
Epoch 13/80: current_loss=0.02682 | best_loss=0.02582
Epoch 14/80: current_loss=0.02704 | best_loss=0.02582
Epoch 15/80: current_loss=0.03621 | best_loss=0.02582
Epoch 16/80: current_loss=0.02826 | best_loss=0.02582
Epoch 17/80: current_loss=0.03231 | best_loss=0.02582
Epoch 18/80: current_loss=0.02979 | best_loss=0.02582
Epoch 19/80: current_loss=0.02694 | best_loss=0.02582
Epoch 20/80: current_loss=0.02586 | best_loss=0.02582
Epoch 21/80: current_loss=0.03042 | best_loss=0.02582
Epoch 22/80: current_loss=0.02683 | best_loss=0.02582
Epoch 23/80: current_loss=0.02721 | best_loss=0.02582
Epoch 24/80: current_loss=0.02545 | best_loss=0.02545
Epoch 25/80: current_loss=0.02550 | best_loss=0.02545
Epoch 26/80: current_loss=0.02563 | best_loss=0.02545
Epoch 27/80: current_loss=0.02534 | best_loss=0.02534
Epoch 28/80: current_loss=0.02745 | best_loss=0.02534
Epoch 29/80: current_loss=0.02576 | best_loss=0.02534
Epoch 30/80: current_loss=0.02642 | best_loss=0.02534
Epoch 31/80: current_loss=0.02477 | best_loss=0.02477
Epoch 32/80: current_loss=0.03275 | best_loss=0.02477
Epoch 33/80: current_loss=0.02550 | best_loss=0.02477
Epoch 34/80: current_loss=0.02869 | best_loss=0.02477
Epoch 35/80: current_loss=0.02689 | best_loss=0.02477
Epoch 36/80: current_loss=0.02527 | best_loss=0.02477
Epoch 37/80: current_loss=0.02552 | best_loss=0.02477
Epoch 38/80: current_loss=0.02578 | best_loss=0.02477
Epoch 39/80: current_loss=0.02588 | best_loss=0.02477
Epoch 40/80: current_loss=0.02627 | best_loss=0.02477
Epoch 41/80: current_loss=0.02654 | best_loss=0.02477
Epoch 42/80: current_loss=0.02883 | best_loss=0.02477
Epoch 43/80: current_loss=0.03108 | best_loss=0.02477
Epoch 44/80: current_loss=0.02624 | best_loss=0.02477
Epoch 45/80: current_loss=0.02961 | best_loss=0.02477
Epoch 46/80: current_loss=0.02506 | best_loss=0.02477
Epoch 47/80: current_loss=0.02676 | best_loss=0.02477
Epoch 48/80: current_loss=0.02922 | best_loss=0.02477
Epoch 49/80: current_loss=0.02996 | best_loss=0.02477
Epoch 50/80: current_loss=0.03222 | best_loss=0.02477
Epoch 51/80: current_loss=0.02596 | best_loss=0.02477
Early Stopping at epoch 51
      explained_var=0.05935 | mse_loss=0.02431
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03541 | best_loss=0.03541
Epoch 1/80: current_loss=0.02985 | best_loss=0.02985
Epoch 2/80: current_loss=0.02987 | best_loss=0.02985
Epoch 3/80: current_loss=0.02942 | best_loss=0.02942
Epoch 4/80: current_loss=0.02928 | best_loss=0.02928
Epoch 5/80: current_loss=0.02977 | best_loss=0.02928
Epoch 6/80: current_loss=0.02964 | best_loss=0.02928
Epoch 7/80: current_loss=0.02813 | best_loss=0.02813
Epoch 8/80: current_loss=0.02960 | best_loss=0.02813
Epoch 9/80: current_loss=0.02836 | best_loss=0.02813
Epoch 10/80: current_loss=0.02944 | best_loss=0.02813
Epoch 11/80: current_loss=0.03039 | best_loss=0.02813
Epoch 12/80: current_loss=0.02815 | best_loss=0.02813
Epoch 13/80: current_loss=0.02969 | best_loss=0.02813
Epoch 14/80: current_loss=0.03223 | best_loss=0.02813
Epoch 15/80: current_loss=0.03510 | best_loss=0.02813
Epoch 16/80: current_loss=0.03528 | best_loss=0.02813
Epoch 17/80: current_loss=0.02943 | best_loss=0.02813
Epoch 18/80: current_loss=0.02794 | best_loss=0.02794
Epoch 19/80: current_loss=0.02843 | best_loss=0.02794
Epoch 20/80: current_loss=0.03019 | best_loss=0.02794
Epoch 21/80: current_loss=0.02806 | best_loss=0.02794
Epoch 22/80: current_loss=0.03049 | best_loss=0.02794
Epoch 23/80: current_loss=0.03557 | best_loss=0.02794
Epoch 24/80: current_loss=0.02760 | best_loss=0.02760
Epoch 25/80: current_loss=0.02957 | best_loss=0.02760
Epoch 26/80: current_loss=0.02894 | best_loss=0.02760
Epoch 27/80: current_loss=0.02778 | best_loss=0.02760
Epoch 28/80: current_loss=0.02921 | best_loss=0.02760
Epoch 29/80: current_loss=0.02804 | best_loss=0.02760
Epoch 30/80: current_loss=0.02917 | best_loss=0.02760
Epoch 31/80: current_loss=0.02835 | best_loss=0.02760
Epoch 32/80: current_loss=0.02777 | best_loss=0.02760
Epoch 33/80: current_loss=0.02931 | best_loss=0.02760
Epoch 34/80: current_loss=0.02805 | best_loss=0.02760
Epoch 35/80: current_loss=0.02818 | best_loss=0.02760
Epoch 36/80: current_loss=0.02972 | best_loss=0.02760
Epoch 37/80: current_loss=0.02795 | best_loss=0.02760
Epoch 38/80: current_loss=0.02901 | best_loss=0.02760
Epoch 39/80: current_loss=0.03030 | best_loss=0.02760
Epoch 40/80: current_loss=0.02785 | best_loss=0.02760
Epoch 41/80: current_loss=0.02798 | best_loss=0.02760
Epoch 42/80: current_loss=0.02792 | best_loss=0.02760
Epoch 43/80: current_loss=0.02818 | best_loss=0.02760
Epoch 44/80: current_loss=0.02956 | best_loss=0.02760
Early Stopping at epoch 44
      explained_var=0.02488 | mse_loss=0.02714
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03469 | best_loss=0.03469
Epoch 1/80: current_loss=0.02966 | best_loss=0.02966
Epoch 2/80: current_loss=0.02996 | best_loss=0.02966
Epoch 3/80: current_loss=0.03283 | best_loss=0.02966
Epoch 4/80: current_loss=0.03313 | best_loss=0.02966
Epoch 5/80: current_loss=0.03034 | best_loss=0.02966
Epoch 6/80: current_loss=0.03149 | best_loss=0.02966
Epoch 7/80: current_loss=0.03277 | best_loss=0.02966
Epoch 8/80: current_loss=0.03172 | best_loss=0.02966
Epoch 9/80: current_loss=0.03463 | best_loss=0.02966
Epoch 10/80: current_loss=0.03614 | best_loss=0.02966
Epoch 11/80: current_loss=0.03361 | best_loss=0.02966
Epoch 12/80: current_loss=0.03056 | best_loss=0.02966
Epoch 13/80: current_loss=0.03415 | best_loss=0.02966
Epoch 14/80: current_loss=0.03224 | best_loss=0.02966
Epoch 15/80: current_loss=0.03184 | best_loss=0.02966
Epoch 16/80: current_loss=0.02952 | best_loss=0.02952
Epoch 17/80: current_loss=0.02990 | best_loss=0.02952
Epoch 18/80: current_loss=0.03494 | best_loss=0.02952
Epoch 19/80: current_loss=0.03698 | best_loss=0.02952
Epoch 20/80: current_loss=0.03124 | best_loss=0.02952
Epoch 21/80: current_loss=0.03403 | best_loss=0.02952
Epoch 22/80: current_loss=0.03303 | best_loss=0.02952
Epoch 23/80: current_loss=0.03138 | best_loss=0.02952
Epoch 24/80: current_loss=0.03066 | best_loss=0.02952
Epoch 25/80: current_loss=0.03123 | best_loss=0.02952
Epoch 26/80: current_loss=0.03061 | best_loss=0.02952
Epoch 27/80: current_loss=0.03450 | best_loss=0.02952
Epoch 28/80: current_loss=0.02979 | best_loss=0.02952
Epoch 29/80: current_loss=0.02972 | best_loss=0.02952
Epoch 30/80: current_loss=0.03064 | best_loss=0.02952
Epoch 31/80: current_loss=0.03068 | best_loss=0.02952
Epoch 32/80: current_loss=0.03052 | best_loss=0.02952
Epoch 33/80: current_loss=0.03002 | best_loss=0.02952
Epoch 34/80: current_loss=0.03069 | best_loss=0.02952
Epoch 35/80: current_loss=0.03278 | best_loss=0.02952
Epoch 36/80: current_loss=0.03030 | best_loss=0.02952
Early Stopping at epoch 36
      explained_var=0.03004 | mse_loss=0.02874
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02829 | best_loss=0.02829
Epoch 1/80: current_loss=0.02844 | best_loss=0.02829
Epoch 2/80: current_loss=0.02825 | best_loss=0.02825
Epoch 3/80: current_loss=0.02819 | best_loss=0.02819
Epoch 4/80: current_loss=0.02811 | best_loss=0.02811
Epoch 5/80: current_loss=0.02894 | best_loss=0.02811
Epoch 6/80: current_loss=0.02847 | best_loss=0.02811
Epoch 7/80: current_loss=0.02867 | best_loss=0.02811
Epoch 8/80: current_loss=0.02832 | best_loss=0.02811
Epoch 9/80: current_loss=0.02819 | best_loss=0.02811
Epoch 10/80: current_loss=0.02875 | best_loss=0.02811
Epoch 11/80: current_loss=0.02909 | best_loss=0.02811
Epoch 12/80: current_loss=0.02858 | best_loss=0.02811
Epoch 13/80: current_loss=0.02836 | best_loss=0.02811
Epoch 14/80: current_loss=0.02894 | best_loss=0.02811
Epoch 15/80: current_loss=0.02921 | best_loss=0.02811
Epoch 16/80: current_loss=0.02932 | best_loss=0.02811
Epoch 17/80: current_loss=0.02976 | best_loss=0.02811
Epoch 18/80: current_loss=0.02797 | best_loss=0.02797
Epoch 19/80: current_loss=0.02836 | best_loss=0.02797
Epoch 20/80: current_loss=0.02865 | best_loss=0.02797
Epoch 21/80: current_loss=0.02856 | best_loss=0.02797
Epoch 22/80: current_loss=0.02838 | best_loss=0.02797
Epoch 23/80: current_loss=0.02884 | best_loss=0.02797
Epoch 24/80: current_loss=0.02851 | best_loss=0.02797
Epoch 25/80: current_loss=0.02939 | best_loss=0.02797
Epoch 26/80: current_loss=0.02819 | best_loss=0.02797
Epoch 27/80: current_loss=0.02850 | best_loss=0.02797
Epoch 28/80: current_loss=0.02826 | best_loss=0.02797
Epoch 29/80: current_loss=0.02925 | best_loss=0.02797
Epoch 30/80: current_loss=0.02831 | best_loss=0.02797
Epoch 31/80: current_loss=0.02830 | best_loss=0.02797
Epoch 32/80: current_loss=0.02816 | best_loss=0.02797
Epoch 33/80: current_loss=0.02811 | best_loss=0.02797
Epoch 34/80: current_loss=0.02892 | best_loss=0.02797
Epoch 35/80: current_loss=0.02833 | best_loss=0.02797
Epoch 36/80: current_loss=0.02831 | best_loss=0.02797
Epoch 37/80: current_loss=0.02933 | best_loss=0.02797
Epoch 38/80: current_loss=0.02915 | best_loss=0.02797
Early Stopping at epoch 38
      explained_var=0.00100 | mse_loss=0.02833
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03372 | best_loss=0.03372
Epoch 1/80: current_loss=0.03487 | best_loss=0.03372
Epoch 2/80: current_loss=0.03340 | best_loss=0.03340
Epoch 3/80: current_loss=0.03350 | best_loss=0.03340
Epoch 4/80: current_loss=0.03532 | best_loss=0.03340
Epoch 5/80: current_loss=0.03371 | best_loss=0.03340
Epoch 6/80: current_loss=0.03330 | best_loss=0.03330
Epoch 7/80: current_loss=0.03387 | best_loss=0.03330
Epoch 8/80: current_loss=0.03445 | best_loss=0.03330
Epoch 9/80: current_loss=0.03324 | best_loss=0.03324
Epoch 10/80: current_loss=0.03355 | best_loss=0.03324
Epoch 11/80: current_loss=0.03525 | best_loss=0.03324
Epoch 12/80: current_loss=0.03396 | best_loss=0.03324
Epoch 13/80: current_loss=0.03399 | best_loss=0.03324
Epoch 14/80: current_loss=0.03393 | best_loss=0.03324
Epoch 15/80: current_loss=0.03429 | best_loss=0.03324
Epoch 16/80: current_loss=0.03427 | best_loss=0.03324
Epoch 17/80: current_loss=0.03410 | best_loss=0.03324
Epoch 18/80: current_loss=0.03568 | best_loss=0.03324
Epoch 19/80: current_loss=0.03419 | best_loss=0.03324
Epoch 20/80: current_loss=0.03381 | best_loss=0.03324
Epoch 21/80: current_loss=0.03979 | best_loss=0.03324
Epoch 22/80: current_loss=0.03334 | best_loss=0.03324
Epoch 23/80: current_loss=0.03368 | best_loss=0.03324
Epoch 24/80: current_loss=0.03358 | best_loss=0.03324
Epoch 25/80: current_loss=0.03677 | best_loss=0.03324
Epoch 26/80: current_loss=0.03473 | best_loss=0.03324
Epoch 27/80: current_loss=0.03581 | best_loss=0.03324
Epoch 28/80: current_loss=0.03445 | best_loss=0.03324
Epoch 29/80: current_loss=0.03334 | best_loss=0.03324
Early Stopping at epoch 29
      explained_var=-0.04678 | mse_loss=0.03402
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.01370| avg_loss=0.02851
----------------------------------------------


----------------------------------------------
Params for Trial 87
{'learning_rate': 0.001, 'weight_decay': 0.0008461979432719264, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03179 | best_loss=0.03179
Epoch 1/80: current_loss=0.03076 | best_loss=0.03076
Epoch 2/80: current_loss=0.02625 | best_loss=0.02625
Epoch 3/80: current_loss=0.02601 | best_loss=0.02601
Epoch 4/80: current_loss=0.02649 | best_loss=0.02601
Epoch 5/80: current_loss=0.02565 | best_loss=0.02565
Epoch 6/80: current_loss=0.02870 | best_loss=0.02565
Epoch 7/80: current_loss=0.03491 | best_loss=0.02565
Epoch 8/80: current_loss=0.04207 | best_loss=0.02565
Epoch 9/80: current_loss=0.02974 | best_loss=0.02565
Epoch 10/80: current_loss=0.03088 | best_loss=0.02565
Epoch 11/80: current_loss=0.02687 | best_loss=0.02565
Epoch 12/80: current_loss=0.02892 | best_loss=0.02565
Epoch 13/80: current_loss=0.02550 | best_loss=0.02550
Epoch 14/80: current_loss=0.02640 | best_loss=0.02550
Epoch 15/80: current_loss=0.02599 | best_loss=0.02550
Epoch 16/80: current_loss=0.02706 | best_loss=0.02550
Epoch 17/80: current_loss=0.02553 | best_loss=0.02550
Epoch 18/80: current_loss=0.02860 | best_loss=0.02550
Epoch 19/80: current_loss=0.02536 | best_loss=0.02536
Epoch 20/80: current_loss=0.02491 | best_loss=0.02491
Epoch 21/80: current_loss=0.02475 | best_loss=0.02475
Epoch 22/80: current_loss=0.02936 | best_loss=0.02475
Epoch 23/80: current_loss=0.03016 | best_loss=0.02475
Epoch 24/80: current_loss=0.02443 | best_loss=0.02443
Epoch 25/80: current_loss=0.02794 | best_loss=0.02443
Epoch 26/80: current_loss=0.03455 | best_loss=0.02443
Epoch 27/80: current_loss=0.02828 | best_loss=0.02443
Epoch 28/80: current_loss=0.02645 | best_loss=0.02443
Epoch 29/80: current_loss=0.02614 | best_loss=0.02443
Epoch 30/80: current_loss=0.02665 | best_loss=0.02443
Epoch 31/80: current_loss=0.03007 | best_loss=0.02443
Epoch 32/80: current_loss=0.02554 | best_loss=0.02443
Epoch 33/80: current_loss=0.02601 | best_loss=0.02443
Epoch 34/80: current_loss=0.02619 | best_loss=0.02443
Epoch 35/80: current_loss=0.02534 | best_loss=0.02443
Epoch 36/80: current_loss=0.02645 | best_loss=0.02443
Epoch 37/80: current_loss=0.02666 | best_loss=0.02443
Epoch 38/80: current_loss=0.02581 | best_loss=0.02443
Epoch 39/80: current_loss=0.02540 | best_loss=0.02443
Epoch 40/80: current_loss=0.02539 | best_loss=0.02443
Epoch 41/80: current_loss=0.02588 | best_loss=0.02443
Epoch 42/80: current_loss=0.02734 | best_loss=0.02443
Epoch 43/80: current_loss=0.02636 | best_loss=0.02443
Epoch 44/80: current_loss=0.02670 | best_loss=0.02443
Early Stopping at epoch 44
      explained_var=0.07241 | mse_loss=0.02395
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02778 | best_loss=0.02778
Epoch 1/80: current_loss=0.02796 | best_loss=0.02778
Epoch 2/80: current_loss=0.02899 | best_loss=0.02778
Epoch 3/80: current_loss=0.03122 | best_loss=0.02778
Epoch 4/80: current_loss=0.02996 | best_loss=0.02778
Epoch 5/80: current_loss=0.03106 | best_loss=0.02778
Epoch 6/80: current_loss=0.02817 | best_loss=0.02778
Epoch 7/80: current_loss=0.02816 | best_loss=0.02778
Epoch 8/80: current_loss=0.03192 | best_loss=0.02778
Epoch 9/80: current_loss=0.03049 | best_loss=0.02778
Epoch 10/80: current_loss=0.02772 | best_loss=0.02772
Epoch 11/80: current_loss=0.02907 | best_loss=0.02772
Epoch 12/80: current_loss=0.02842 | best_loss=0.02772
Epoch 13/80: current_loss=0.02822 | best_loss=0.02772
Epoch 14/80: current_loss=0.02808 | best_loss=0.02772
Epoch 15/80: current_loss=0.02847 | best_loss=0.02772
Epoch 16/80: current_loss=0.02841 | best_loss=0.02772
Epoch 17/80: current_loss=0.02773 | best_loss=0.02772
Epoch 18/80: current_loss=0.02781 | best_loss=0.02772
Epoch 19/80: current_loss=0.02869 | best_loss=0.02772
Epoch 20/80: current_loss=0.02826 | best_loss=0.02772
Epoch 21/80: current_loss=0.02818 | best_loss=0.02772
Epoch 22/80: current_loss=0.02765 | best_loss=0.02765
Epoch 23/80: current_loss=0.02798 | best_loss=0.02765
Epoch 24/80: current_loss=0.02834 | best_loss=0.02765
Epoch 25/80: current_loss=0.02817 | best_loss=0.02765
Epoch 26/80: current_loss=0.02773 | best_loss=0.02765
Epoch 27/80: current_loss=0.02970 | best_loss=0.02765
Epoch 28/80: current_loss=0.03064 | best_loss=0.02765
Epoch 29/80: current_loss=0.03029 | best_loss=0.02765
Epoch 30/80: current_loss=0.02791 | best_loss=0.02765
Epoch 31/80: current_loss=0.02889 | best_loss=0.02765
Epoch 32/80: current_loss=0.02952 | best_loss=0.02765
Epoch 33/80: current_loss=0.02754 | best_loss=0.02754
Epoch 34/80: current_loss=0.02824 | best_loss=0.02754
Epoch 35/80: current_loss=0.03075 | best_loss=0.02754
Epoch 36/80: current_loss=0.02821 | best_loss=0.02754
Epoch 37/80: current_loss=0.02844 | best_loss=0.02754
Epoch 38/80: current_loss=0.02885 | best_loss=0.02754
Epoch 39/80: current_loss=0.02815 | best_loss=0.02754
Epoch 40/80: current_loss=0.03038 | best_loss=0.02754
Epoch 41/80: current_loss=0.02823 | best_loss=0.02754
Epoch 42/80: current_loss=0.03281 | best_loss=0.02754
Epoch 43/80: current_loss=0.02990 | best_loss=0.02754
Epoch 44/80: current_loss=0.02883 | best_loss=0.02754
Epoch 45/80: current_loss=0.02989 | best_loss=0.02754
Epoch 46/80: current_loss=0.02809 | best_loss=0.02754
Epoch 47/80: current_loss=0.02855 | best_loss=0.02754
Epoch 48/80: current_loss=0.02754 | best_loss=0.02754
Epoch 49/80: current_loss=0.02847 | best_loss=0.02754
Epoch 50/80: current_loss=0.02823 | best_loss=0.02754
Epoch 51/80: current_loss=0.03045 | best_loss=0.02754
Epoch 52/80: current_loss=0.03936 | best_loss=0.02754
Epoch 53/80: current_loss=0.02691 | best_loss=0.02691
Epoch 54/80: current_loss=7.28986 | best_loss=0.02691
Epoch 55/80: current_loss=0.17105 | best_loss=0.02691
Epoch 56/80: current_loss=0.14932 | best_loss=0.02691
Epoch 57/80: current_loss=0.09222 | best_loss=0.02691
Epoch 58/80: current_loss=0.07292 | best_loss=0.02691
Epoch 59/80: current_loss=0.06079 | best_loss=0.02691
Epoch 60/80: current_loss=0.05247 | best_loss=0.02691
Epoch 61/80: current_loss=0.04602 | best_loss=0.02691
Epoch 62/80: current_loss=0.05469 | best_loss=0.02691
Epoch 63/80: current_loss=0.03824 | best_loss=0.02691
Epoch 64/80: current_loss=0.03530 | best_loss=0.02691
Epoch 65/80: current_loss=0.03756 | best_loss=0.02691
Epoch 66/80: current_loss=0.05133 | best_loss=0.02691
Epoch 67/80: current_loss=0.03079 | best_loss=0.02691
Epoch 68/80: current_loss=0.03077 | best_loss=0.02691
Epoch 69/80: current_loss=0.03588 | best_loss=0.02691
Epoch 70/80: current_loss=0.03186 | best_loss=0.02691
Epoch 71/80: current_loss=0.03592 | best_loss=0.02691
Epoch 72/80: current_loss=0.03049 | best_loss=0.02691
Epoch 73/80: current_loss=0.03203 | best_loss=0.02691
Early Stopping at epoch 73
      explained_var=0.05251 | mse_loss=0.02635
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04562 | best_loss=0.04562
Epoch 1/80: current_loss=0.04455 | best_loss=0.04455
Epoch 2/80: current_loss=0.03176 | best_loss=0.03176
Epoch 3/80: current_loss=0.04102 | best_loss=0.03176
Epoch 4/80: current_loss=0.03703 | best_loss=0.03176
Epoch 5/80: current_loss=0.03507 | best_loss=0.03176
Epoch 6/80: current_loss=0.03287 | best_loss=0.03176
Epoch 7/80: current_loss=0.03145 | best_loss=0.03145
Epoch 8/80: current_loss=0.03081 | best_loss=0.03081
Epoch 9/80: current_loss=0.03144 | best_loss=0.03081
Epoch 10/80: current_loss=0.03053 | best_loss=0.03053
Epoch 11/80: current_loss=0.03170 | best_loss=0.03053
Epoch 12/80: current_loss=0.03088 | best_loss=0.03053
Epoch 13/80: current_loss=0.03095 | best_loss=0.03053
Epoch 14/80: current_loss=0.03112 | best_loss=0.03053
Epoch 15/80: current_loss=0.03061 | best_loss=0.03053
Epoch 16/80: current_loss=0.03114 | best_loss=0.03053
Epoch 17/80: current_loss=0.03084 | best_loss=0.03053
Epoch 18/80: current_loss=0.03073 | best_loss=0.03053
Epoch 19/80: current_loss=0.03071 | best_loss=0.03053
Epoch 20/80: current_loss=0.03123 | best_loss=0.03053
Epoch 21/80: current_loss=0.03135 | best_loss=0.03053
Epoch 22/80: current_loss=0.03060 | best_loss=0.03053
Epoch 23/80: current_loss=0.03087 | best_loss=0.03053
Epoch 24/80: current_loss=0.03152 | best_loss=0.03053
Epoch 25/80: current_loss=0.03064 | best_loss=0.03053
Epoch 26/80: current_loss=0.03066 | best_loss=0.03053
Epoch 27/80: current_loss=0.03120 | best_loss=0.03053
Epoch 28/80: current_loss=0.03050 | best_loss=0.03050
Epoch 29/80: current_loss=0.03114 | best_loss=0.03050
Epoch 30/80: current_loss=0.03056 | best_loss=0.03050
Epoch 31/80: current_loss=0.03093 | best_loss=0.03050
Epoch 32/80: current_loss=0.03151 | best_loss=0.03050
Epoch 33/80: current_loss=0.03041 | best_loss=0.03041
Epoch 34/80: current_loss=0.03066 | best_loss=0.03041
Epoch 35/80: current_loss=0.03127 | best_loss=0.03041
Epoch 36/80: current_loss=0.03074 | best_loss=0.03041
Epoch 37/80: current_loss=0.03078 | best_loss=0.03041
Epoch 38/80: current_loss=0.03078 | best_loss=0.03041
Epoch 39/80: current_loss=0.03122 | best_loss=0.03041
Epoch 40/80: current_loss=0.03062 | best_loss=0.03041
Epoch 41/80: current_loss=0.03107 | best_loss=0.03041
Epoch 42/80: current_loss=0.03066 | best_loss=0.03041
Epoch 43/80: current_loss=0.03108 | best_loss=0.03041
Epoch 44/80: current_loss=0.03078 | best_loss=0.03041
Epoch 45/80: current_loss=0.03091 | best_loss=0.03041
Epoch 46/80: current_loss=0.03078 | best_loss=0.03041
Epoch 47/80: current_loss=0.03083 | best_loss=0.03041
Epoch 48/80: current_loss=0.03070 | best_loss=0.03041
Epoch 49/80: current_loss=0.03051 | best_loss=0.03041
Epoch 50/80: current_loss=0.03143 | best_loss=0.03041
Epoch 51/80: current_loss=0.03134 | best_loss=0.03041
Epoch 52/80: current_loss=0.03078 | best_loss=0.03041
Epoch 53/80: current_loss=0.03099 | best_loss=0.03041
Early Stopping at epoch 53
      explained_var=0.01036 | mse_loss=0.02964
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02824 | best_loss=0.02824
Epoch 1/80: current_loss=0.02810 | best_loss=0.02810
Epoch 2/80: current_loss=0.02816 | best_loss=0.02810
Epoch 3/80: current_loss=0.02823 | best_loss=0.02810
Epoch 4/80: current_loss=0.02826 | best_loss=0.02810
Epoch 5/80: current_loss=0.02819 | best_loss=0.02810
Epoch 6/80: current_loss=0.02845 | best_loss=0.02810
Epoch 7/80: current_loss=0.02833 | best_loss=0.02810
Epoch 8/80: current_loss=0.02834 | best_loss=0.02810
Epoch 9/80: current_loss=0.02831 | best_loss=0.02810
Epoch 10/80: current_loss=0.02830 | best_loss=0.02810
Epoch 11/80: current_loss=0.02830 | best_loss=0.02810
Epoch 12/80: current_loss=0.02856 | best_loss=0.02810
Epoch 13/80: current_loss=0.02812 | best_loss=0.02810
Epoch 14/80: current_loss=0.02894 | best_loss=0.02810
Epoch 15/80: current_loss=0.02885 | best_loss=0.02810
Epoch 16/80: current_loss=0.02825 | best_loss=0.02810
Epoch 17/80: current_loss=0.02837 | best_loss=0.02810
Epoch 18/80: current_loss=0.02860 | best_loss=0.02810
Epoch 19/80: current_loss=0.02828 | best_loss=0.02810
Epoch 20/80: current_loss=0.02821 | best_loss=0.02810
Epoch 21/80: current_loss=0.02809 | best_loss=0.02809
Epoch 22/80: current_loss=0.02812 | best_loss=0.02809
Epoch 23/80: current_loss=0.02819 | best_loss=0.02809
Epoch 24/80: current_loss=0.02829 | best_loss=0.02809
Epoch 25/80: current_loss=0.02850 | best_loss=0.02809
Epoch 26/80: current_loss=0.02828 | best_loss=0.02809
Epoch 27/80: current_loss=0.02831 | best_loss=0.02809
Epoch 28/80: current_loss=0.02846 | best_loss=0.02809
Epoch 29/80: current_loss=0.02830 | best_loss=0.02809
Epoch 30/80: current_loss=0.02827 | best_loss=0.02809
Epoch 31/80: current_loss=0.02836 | best_loss=0.02809
Epoch 32/80: current_loss=0.02832 | best_loss=0.02809
Epoch 33/80: current_loss=0.02833 | best_loss=0.02809
Epoch 34/80: current_loss=0.02864 | best_loss=0.02809
Epoch 35/80: current_loss=0.02823 | best_loss=0.02809
Epoch 36/80: current_loss=0.02847 | best_loss=0.02809
Epoch 37/80: current_loss=0.02868 | best_loss=0.02809
Epoch 38/80: current_loss=0.02833 | best_loss=0.02809
Epoch 39/80: current_loss=0.02937 | best_loss=0.02809
Epoch 40/80: current_loss=0.02846 | best_loss=0.02809
Epoch 41/80: current_loss=0.02870 | best_loss=0.02809
Early Stopping at epoch 41
      explained_var=-0.00286 | mse_loss=0.02845
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03463 | best_loss=0.03463
Epoch 1/80: current_loss=0.03294 | best_loss=0.03294
Epoch 2/80: current_loss=0.03393 | best_loss=0.03294
Epoch 3/80: current_loss=0.03366 | best_loss=0.03294
Epoch 4/80: current_loss=0.03371 | best_loss=0.03294
Epoch 5/80: current_loss=0.03291 | best_loss=0.03291
Epoch 6/80: current_loss=0.03340 | best_loss=0.03291
Epoch 7/80: current_loss=0.03367 | best_loss=0.03291
Epoch 8/80: current_loss=0.03413 | best_loss=0.03291
Epoch 9/80: current_loss=0.03362 | best_loss=0.03291
Epoch 10/80: current_loss=0.03322 | best_loss=0.03291
Epoch 11/80: current_loss=0.03352 | best_loss=0.03291
Epoch 12/80: current_loss=0.03399 | best_loss=0.03291
Epoch 13/80: current_loss=0.03383 | best_loss=0.03291
Epoch 14/80: current_loss=0.03340 | best_loss=0.03291
Epoch 15/80: current_loss=0.03384 | best_loss=0.03291
Epoch 16/80: current_loss=0.03354 | best_loss=0.03291
Epoch 17/80: current_loss=0.03341 | best_loss=0.03291
Epoch 18/80: current_loss=0.03355 | best_loss=0.03291
Epoch 19/80: current_loss=0.03349 | best_loss=0.03291
Epoch 20/80: current_loss=0.03350 | best_loss=0.03291
Epoch 21/80: current_loss=0.03395 | best_loss=0.03291
Epoch 22/80: current_loss=0.03303 | best_loss=0.03291
Epoch 23/80: current_loss=0.03359 | best_loss=0.03291
Epoch 24/80: current_loss=0.03368 | best_loss=0.03291
Epoch 25/80: current_loss=0.03364 | best_loss=0.03291
Early Stopping at epoch 25
      explained_var=-0.03698 | mse_loss=0.03369
----------------------------------------------
Average early_stopping_point: 27| avg_exp_var=0.01909| avg_loss=0.02842
----------------------------------------------


----------------------------------------------
Params for Trial 88
{'learning_rate': 0.01, 'weight_decay': 0.0005114278644016745, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02574 | best_loss=0.02574
Epoch 1/80: current_loss=0.02564 | best_loss=0.02564
Epoch 2/80: current_loss=0.02673 | best_loss=0.02564
Epoch 3/80: current_loss=0.02600 | best_loss=0.02564
Epoch 4/80: current_loss=0.02807 | best_loss=0.02564
Epoch 5/80: current_loss=0.02615 | best_loss=0.02564
Epoch 6/80: current_loss=0.02902 | best_loss=0.02564
Epoch 7/80: current_loss=0.02997 | best_loss=0.02564
Epoch 8/80: current_loss=0.02636 | best_loss=0.02564
Epoch 9/80: current_loss=0.02606 | best_loss=0.02564
Epoch 10/80: current_loss=0.02802 | best_loss=0.02564
Epoch 11/80: current_loss=0.02729 | best_loss=0.02564
Epoch 12/80: current_loss=0.02733 | best_loss=0.02564
Epoch 13/80: current_loss=0.02783 | best_loss=0.02564
Epoch 14/80: current_loss=0.02899 | best_loss=0.02564
Epoch 15/80: current_loss=0.02571 | best_loss=0.02564
Epoch 16/80: current_loss=0.02685 | best_loss=0.02564
Epoch 17/80: current_loss=0.02835 | best_loss=0.02564
Epoch 18/80: current_loss=0.02650 | best_loss=0.02564
Epoch 19/80: current_loss=0.02650 | best_loss=0.02564
Epoch 20/80: current_loss=0.02632 | best_loss=0.02564
Epoch 21/80: current_loss=0.02608 | best_loss=0.02564
Early Stopping at epoch 21
      explained_var=0.03240 | mse_loss=0.02501

----------------------------------------------
Params for Trial 89
{'learning_rate': 0.001, 'weight_decay': 0.00024750486992092626, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02825 | best_loss=0.02825
Epoch 1/80: current_loss=0.03441 | best_loss=0.02825
Epoch 2/80: current_loss=0.03429 | best_loss=0.02825
Epoch 3/80: current_loss=0.02796 | best_loss=0.02796
Epoch 4/80: current_loss=0.02778 | best_loss=0.02778
Epoch 5/80: current_loss=0.02729 | best_loss=0.02729
Epoch 6/80: current_loss=0.02875 | best_loss=0.02729
Epoch 7/80: current_loss=0.02689 | best_loss=0.02689
Epoch 8/80: current_loss=0.02790 | best_loss=0.02689
Epoch 9/80: current_loss=0.02779 | best_loss=0.02689
Epoch 10/80: current_loss=0.02652 | best_loss=0.02652
Epoch 11/80: current_loss=0.02707 | best_loss=0.02652
Epoch 12/80: current_loss=0.02495 | best_loss=0.02495
Epoch 13/80: current_loss=0.02911 | best_loss=0.02495
Epoch 14/80: current_loss=0.02584 | best_loss=0.02495
Epoch 15/80: current_loss=0.02502 | best_loss=0.02495
Epoch 16/80: current_loss=0.02497 | best_loss=0.02495
Epoch 17/80: current_loss=0.02611 | best_loss=0.02495
Epoch 18/80: current_loss=0.02658 | best_loss=0.02495
Epoch 19/80: current_loss=0.02769 | best_loss=0.02495
Epoch 20/80: current_loss=0.02985 | best_loss=0.02495
Epoch 21/80: current_loss=0.02813 | best_loss=0.02495
Epoch 22/80: current_loss=0.02671 | best_loss=0.02495
Epoch 23/80: current_loss=0.02873 | best_loss=0.02495
Epoch 24/80: current_loss=0.02582 | best_loss=0.02495
Epoch 25/80: current_loss=0.02596 | best_loss=0.02495
Epoch 26/80: current_loss=0.02531 | best_loss=0.02495
Epoch 27/80: current_loss=0.02736 | best_loss=0.02495
Epoch 28/80: current_loss=0.02626 | best_loss=0.02495
Epoch 29/80: current_loss=0.02664 | best_loss=0.02495
Epoch 30/80: current_loss=0.02536 | best_loss=0.02495
Epoch 31/80: current_loss=0.02538 | best_loss=0.02495
Epoch 32/80: current_loss=0.02531 | best_loss=0.02495
Early Stopping at epoch 32
      explained_var=0.05550 | mse_loss=0.02439
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02822 | best_loss=0.02822
Epoch 1/80: current_loss=0.03106 | best_loss=0.02822
Epoch 2/80: current_loss=0.03684 | best_loss=0.02822
Epoch 3/80: current_loss=0.02967 | best_loss=0.02822
Epoch 4/80: current_loss=0.02836 | best_loss=0.02822
Epoch 5/80: current_loss=0.02847 | best_loss=0.02822
Epoch 6/80: current_loss=0.02898 | best_loss=0.02822
Epoch 7/80: current_loss=0.03281 | best_loss=0.02822
Epoch 8/80: current_loss=0.02719 | best_loss=0.02719
Epoch 9/80: current_loss=0.03614 | best_loss=0.02719
Epoch 10/80: current_loss=0.03021 | best_loss=0.02719
Epoch 11/80: current_loss=0.02775 | best_loss=0.02719
Epoch 12/80: current_loss=0.02913 | best_loss=0.02719
Epoch 13/80: current_loss=0.02915 | best_loss=0.02719
Epoch 14/80: current_loss=0.03040 | best_loss=0.02719
Epoch 15/80: current_loss=0.03618 | best_loss=0.02719
Epoch 16/80: current_loss=0.02900 | best_loss=0.02719
Epoch 17/80: current_loss=0.02831 | best_loss=0.02719
Epoch 18/80: current_loss=0.02790 | best_loss=0.02719
Epoch 19/80: current_loss=0.02812 | best_loss=0.02719
Epoch 20/80: current_loss=0.02957 | best_loss=0.02719
Epoch 21/80: current_loss=0.03068 | best_loss=0.02719
Epoch 22/80: current_loss=0.02830 | best_loss=0.02719
Epoch 23/80: current_loss=0.02814 | best_loss=0.02719
Epoch 24/80: current_loss=0.02968 | best_loss=0.02719
Epoch 25/80: current_loss=0.02931 | best_loss=0.02719
Epoch 26/80: current_loss=0.02827 | best_loss=0.02719
Epoch 27/80: current_loss=0.03507 | best_loss=0.02719
Epoch 28/80: current_loss=0.02796 | best_loss=0.02719
Early Stopping at epoch 28
      explained_var=0.05350 | mse_loss=0.02675
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03013 | best_loss=0.03013
Epoch 1/80: current_loss=0.02923 | best_loss=0.02923
Epoch 2/80: current_loss=0.02870 | best_loss=0.02870
Epoch 3/80: current_loss=0.03081 | best_loss=0.02870
Epoch 4/80: current_loss=0.03291 | best_loss=0.02870
Epoch 5/80: current_loss=0.03412 | best_loss=0.02870
Epoch 6/80: current_loss=0.02953 | best_loss=0.02870
Epoch 7/80: current_loss=0.02982 | best_loss=0.02870
Epoch 8/80: current_loss=0.03060 | best_loss=0.02870
Epoch 9/80: current_loss=0.03235 | best_loss=0.02870
Epoch 10/80: current_loss=0.03020 | best_loss=0.02870
Epoch 11/80: current_loss=0.03522 | best_loss=0.02870
Epoch 12/80: current_loss=0.03585 | best_loss=0.02870
Epoch 13/80: current_loss=0.02974 | best_loss=0.02870
Epoch 14/80: current_loss=0.03400 | best_loss=0.02870
Epoch 15/80: current_loss=0.03571 | best_loss=0.02870
Epoch 16/80: current_loss=0.02966 | best_loss=0.02870
Epoch 17/80: current_loss=0.02989 | best_loss=0.02870
Epoch 18/80: current_loss=0.03345 | best_loss=0.02870
Epoch 19/80: current_loss=0.03422 | best_loss=0.02870
Epoch 20/80: current_loss=0.03303 | best_loss=0.02870
Epoch 21/80: current_loss=0.03119 | best_loss=0.02870
Epoch 22/80: current_loss=0.02984 | best_loss=0.02870
Early Stopping at epoch 22
      explained_var=0.05747 | mse_loss=0.02791
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02821 | best_loss=0.02821
Epoch 1/80: current_loss=0.02774 | best_loss=0.02774
Epoch 2/80: current_loss=0.02932 | best_loss=0.02774
Epoch 3/80: current_loss=0.02864 | best_loss=0.02774
Epoch 4/80: current_loss=0.02939 | best_loss=0.02774
Epoch 5/80: current_loss=0.02977 | best_loss=0.02774
Epoch 6/80: current_loss=0.02869 | best_loss=0.02774
Epoch 7/80: current_loss=0.02977 | best_loss=0.02774
Epoch 8/80: current_loss=0.03811 | best_loss=0.02774
Epoch 9/80: current_loss=0.02898 | best_loss=0.02774
Epoch 10/80: current_loss=0.02840 | best_loss=0.02774
Epoch 11/80: current_loss=0.03191 | best_loss=0.02774
Epoch 12/80: current_loss=0.03083 | best_loss=0.02774
Epoch 13/80: current_loss=0.02911 | best_loss=0.02774
Epoch 14/80: current_loss=0.02962 | best_loss=0.02774
Epoch 15/80: current_loss=0.02923 | best_loss=0.02774
Epoch 16/80: current_loss=0.02957 | best_loss=0.02774
Epoch 17/80: current_loss=0.02849 | best_loss=0.02774
Epoch 18/80: current_loss=0.02936 | best_loss=0.02774
Epoch 19/80: current_loss=0.02949 | best_loss=0.02774
Epoch 20/80: current_loss=0.02829 | best_loss=0.02774
Epoch 21/80: current_loss=0.02832 | best_loss=0.02774
Early Stopping at epoch 21
      explained_var=0.00848 | mse_loss=0.02813
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03618 | best_loss=0.03618
Epoch 1/80: current_loss=0.03442 | best_loss=0.03442
Epoch 2/80: current_loss=0.03325 | best_loss=0.03325
Epoch 3/80: current_loss=0.03393 | best_loss=0.03325
Epoch 4/80: current_loss=0.03415 | best_loss=0.03325
Epoch 5/80: current_loss=0.03597 | best_loss=0.03325
Epoch 6/80: current_loss=0.03401 | best_loss=0.03325
Epoch 7/80: current_loss=0.03482 | best_loss=0.03325
Epoch 8/80: current_loss=0.03544 | best_loss=0.03325
Epoch 9/80: current_loss=0.03683 | best_loss=0.03325
Epoch 10/80: current_loss=0.03401 | best_loss=0.03325
Epoch 11/80: current_loss=0.03500 | best_loss=0.03325
Epoch 12/80: current_loss=0.03563 | best_loss=0.03325
Epoch 13/80: current_loss=0.03343 | best_loss=0.03325
Epoch 14/80: current_loss=0.03357 | best_loss=0.03325
Epoch 15/80: current_loss=0.03440 | best_loss=0.03325
Epoch 16/80: current_loss=0.03362 | best_loss=0.03325
Epoch 17/80: current_loss=0.03378 | best_loss=0.03325
Epoch 18/80: current_loss=0.03480 | best_loss=0.03325
Epoch 19/80: current_loss=0.03491 | best_loss=0.03325
Epoch 20/80: current_loss=0.03396 | best_loss=0.03325
Epoch 21/80: current_loss=0.03376 | best_loss=0.03325
Epoch 22/80: current_loss=0.03377 | best_loss=0.03325
Early Stopping at epoch 22
      explained_var=-0.02774 | mse_loss=0.03387
----------------------------------------------
Average early_stopping_point: 5| avg_exp_var=0.02944| avg_loss=0.02821
----------------------------------------------


----------------------------------------------
Params for Trial 90
{'learning_rate': 0.001, 'weight_decay': 0.008528054468583603, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03195 | best_loss=0.03195
Epoch 1/80: current_loss=0.02948 | best_loss=0.02948
Epoch 2/80: current_loss=0.02610 | best_loss=0.02610
Epoch 3/80: current_loss=0.02853 | best_loss=0.02610
Epoch 4/80: current_loss=0.03254 | best_loss=0.02610
Epoch 5/80: current_loss=0.03839 | best_loss=0.02610
Epoch 6/80: current_loss=0.03324 | best_loss=0.02610
Epoch 7/80: current_loss=0.02591 | best_loss=0.02591
Epoch 8/80: current_loss=0.02693 | best_loss=0.02591
Epoch 9/80: current_loss=0.02627 | best_loss=0.02591
Epoch 10/80: current_loss=0.02875 | best_loss=0.02591
Epoch 11/80: current_loss=0.02701 | best_loss=0.02591
Epoch 12/80: current_loss=0.02600 | best_loss=0.02591
Epoch 13/80: current_loss=0.02551 | best_loss=0.02551
Epoch 14/80: current_loss=0.02967 | best_loss=0.02551
Epoch 15/80: current_loss=0.03832 | best_loss=0.02551
Epoch 16/80: current_loss=0.02813 | best_loss=0.02551
Epoch 17/80: current_loss=0.02601 | best_loss=0.02551
Epoch 18/80: current_loss=0.02763 | best_loss=0.02551
Epoch 19/80: current_loss=0.03024 | best_loss=0.02551
Epoch 20/80: current_loss=0.02665 | best_loss=0.02551
Epoch 21/80: current_loss=0.03328 | best_loss=0.02551
Epoch 22/80: current_loss=0.03833 | best_loss=0.02551
Epoch 23/80: current_loss=0.02959 | best_loss=0.02551
Epoch 24/80: current_loss=0.02719 | best_loss=0.02551
Epoch 25/80: current_loss=0.02888 | best_loss=0.02551
Epoch 26/80: current_loss=0.02623 | best_loss=0.02551
Epoch 27/80: current_loss=0.02756 | best_loss=0.02551
Epoch 28/80: current_loss=0.02649 | best_loss=0.02551
Epoch 29/80: current_loss=0.02710 | best_loss=0.02551
Epoch 30/80: current_loss=0.02841 | best_loss=0.02551
Epoch 31/80: current_loss=0.02665 | best_loss=0.02551
Epoch 32/80: current_loss=0.02800 | best_loss=0.02551
Epoch 33/80: current_loss=0.02631 | best_loss=0.02551
Early Stopping at epoch 33
      explained_var=0.03386 | mse_loss=0.02496

----------------------------------------------
Params for Trial 91
{'learning_rate': 0.001, 'weight_decay': 1.030629741702722e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03001 | best_loss=0.03001
Epoch 1/80: current_loss=0.02657 | best_loss=0.02657
Epoch 2/80: current_loss=0.02943 | best_loss=0.02657
Epoch 3/80: current_loss=0.03329 | best_loss=0.02657
Epoch 4/80: current_loss=0.03627 | best_loss=0.02657
Epoch 5/80: current_loss=0.02727 | best_loss=0.02657
Epoch 6/80: current_loss=0.02944 | best_loss=0.02657
Epoch 7/80: current_loss=0.02726 | best_loss=0.02657
Epoch 8/80: current_loss=0.02842 | best_loss=0.02657
Epoch 9/80: current_loss=0.02816 | best_loss=0.02657
Epoch 10/80: current_loss=0.02712 | best_loss=0.02657
Epoch 11/80: current_loss=0.02772 | best_loss=0.02657
Epoch 12/80: current_loss=0.02755 | best_loss=0.02657
Epoch 13/80: current_loss=0.02386 | best_loss=0.02386
Epoch 14/80: current_loss=0.05770 | best_loss=0.02386
Epoch 15/80: current_loss=0.03953 | best_loss=0.02386
Epoch 16/80: current_loss=0.03427 | best_loss=0.02386
Epoch 17/80: current_loss=0.03105 | best_loss=0.02386
Epoch 18/80: current_loss=0.03706 | best_loss=0.02386
Epoch 19/80: current_loss=0.03145 | best_loss=0.02386
Epoch 20/80: current_loss=0.02608 | best_loss=0.02386
Epoch 21/80: current_loss=0.02903 | best_loss=0.02386
Epoch 22/80: current_loss=0.02643 | best_loss=0.02386
Epoch 23/80: current_loss=0.02604 | best_loss=0.02386
Epoch 24/80: current_loss=0.02724 | best_loss=0.02386
Epoch 25/80: current_loss=0.02983 | best_loss=0.02386
Epoch 26/80: current_loss=0.03032 | best_loss=0.02386
Epoch 27/80: current_loss=0.02863 | best_loss=0.02386
Epoch 28/80: current_loss=0.02612 | best_loss=0.02386
Epoch 29/80: current_loss=0.02542 | best_loss=0.02386
Epoch 30/80: current_loss=0.02650 | best_loss=0.02386
Epoch 31/80: current_loss=0.02535 | best_loss=0.02386
Epoch 32/80: current_loss=0.02852 | best_loss=0.02386
Epoch 33/80: current_loss=0.02577 | best_loss=0.02386
Early Stopping at epoch 33
      explained_var=0.09206 | mse_loss=0.02344
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03367 | best_loss=0.03367
Epoch 1/80: current_loss=0.02860 | best_loss=0.02860
Epoch 2/80: current_loss=0.02948 | best_loss=0.02860
Epoch 3/80: current_loss=0.02960 | best_loss=0.02860
Epoch 4/80: current_loss=0.02700 | best_loss=0.02700
Epoch 5/80: current_loss=0.02823 | best_loss=0.02700
Epoch 6/80: current_loss=0.02851 | best_loss=0.02700
Epoch 7/80: current_loss=0.03265 | best_loss=0.02700
Epoch 8/80: current_loss=0.03004 | best_loss=0.02700
Epoch 9/80: current_loss=0.02953 | best_loss=0.02700
Epoch 10/80: current_loss=0.02907 | best_loss=0.02700
Epoch 11/80: current_loss=0.03073 | best_loss=0.02700
Epoch 12/80: current_loss=0.02748 | best_loss=0.02700
Epoch 13/80: current_loss=0.03375 | best_loss=0.02700
Epoch 14/80: current_loss=0.02819 | best_loss=0.02700
Epoch 15/80: current_loss=0.02967 | best_loss=0.02700
Epoch 16/80: current_loss=0.02938 | best_loss=0.02700
Epoch 17/80: current_loss=0.03001 | best_loss=0.02700
Epoch 18/80: current_loss=0.02753 | best_loss=0.02700
Epoch 19/80: current_loss=0.02836 | best_loss=0.02700
Epoch 20/80: current_loss=0.02792 | best_loss=0.02700
Epoch 21/80: current_loss=0.02935 | best_loss=0.02700
Epoch 22/80: current_loss=0.02758 | best_loss=0.02700
Epoch 23/80: current_loss=0.03015 | best_loss=0.02700
Epoch 24/80: current_loss=0.02785 | best_loss=0.02700
Early Stopping at epoch 24
      explained_var=0.04761 | mse_loss=0.02655
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03241 | best_loss=0.03241
Epoch 1/80: current_loss=0.03707 | best_loss=0.03241
Epoch 2/80: current_loss=0.03156 | best_loss=0.03156
Epoch 3/80: current_loss=0.02987 | best_loss=0.02987
Epoch 4/80: current_loss=0.05089 | best_loss=0.02987
Epoch 5/80: current_loss=0.04498 | best_loss=0.02987
Epoch 6/80: current_loss=0.04086 | best_loss=0.02987
Epoch 7/80: current_loss=0.03014 | best_loss=0.02987
Epoch 8/80: current_loss=0.03072 | best_loss=0.02987
Epoch 9/80: current_loss=0.03051 | best_loss=0.02987
Epoch 10/80: current_loss=0.02962 | best_loss=0.02962
Epoch 11/80: current_loss=0.03201 | best_loss=0.02962
Epoch 12/80: current_loss=0.03017 | best_loss=0.02962
Epoch 13/80: current_loss=0.03000 | best_loss=0.02962
Epoch 14/80: current_loss=0.03275 | best_loss=0.02962
Epoch 15/80: current_loss=0.03167 | best_loss=0.02962
Epoch 16/80: current_loss=0.03287 | best_loss=0.02962
Epoch 17/80: current_loss=0.03296 | best_loss=0.02962
Epoch 18/80: current_loss=0.03283 | best_loss=0.02962
Epoch 19/80: current_loss=0.03014 | best_loss=0.02962
Epoch 20/80: current_loss=0.03172 | best_loss=0.02962
Epoch 21/80: current_loss=0.03490 | best_loss=0.02962
Epoch 22/80: current_loss=0.03057 | best_loss=0.02962
Epoch 23/80: current_loss=0.03063 | best_loss=0.02962
Epoch 24/80: current_loss=0.03633 | best_loss=0.02962
Epoch 25/80: current_loss=0.03148 | best_loss=0.02962
Epoch 26/80: current_loss=0.02882 | best_loss=0.02882
Epoch 27/80: current_loss=0.03077 | best_loss=0.02882
Epoch 28/80: current_loss=0.03012 | best_loss=0.02882
Epoch 29/80: current_loss=0.02933 | best_loss=0.02882
Epoch 30/80: current_loss=0.03163 | best_loss=0.02882
Epoch 31/80: current_loss=0.02933 | best_loss=0.02882
Epoch 32/80: current_loss=0.03100 | best_loss=0.02882
Epoch 33/80: current_loss=0.03381 | best_loss=0.02882
Epoch 34/80: current_loss=0.03825 | best_loss=0.02882
Epoch 35/80: current_loss=0.03037 | best_loss=0.02882
Epoch 36/80: current_loss=0.02974 | best_loss=0.02882
Epoch 37/80: current_loss=0.03134 | best_loss=0.02882
Epoch 38/80: current_loss=0.02965 | best_loss=0.02882
Epoch 39/80: current_loss=0.02922 | best_loss=0.02882
Epoch 40/80: current_loss=0.03089 | best_loss=0.02882
Epoch 41/80: current_loss=0.03233 | best_loss=0.02882
Epoch 42/80: current_loss=0.03151 | best_loss=0.02882
Epoch 43/80: current_loss=0.03617 | best_loss=0.02882
Epoch 44/80: current_loss=0.03522 | best_loss=0.02882
Epoch 45/80: current_loss=0.03200 | best_loss=0.02882
Epoch 46/80: current_loss=0.02983 | best_loss=0.02882
Early Stopping at epoch 46
      explained_var=0.05921 | mse_loss=0.02797
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02815 | best_loss=0.02815
Epoch 1/80: current_loss=0.02942 | best_loss=0.02815
Epoch 2/80: current_loss=0.02887 | best_loss=0.02815
Epoch 3/80: current_loss=0.02841 | best_loss=0.02815
Epoch 4/80: current_loss=0.02915 | best_loss=0.02815
Epoch 5/80: current_loss=0.03172 | best_loss=0.02815
Epoch 6/80: current_loss=0.05664 | best_loss=0.02815
Epoch 7/80: current_loss=0.03087 | best_loss=0.02815
Epoch 8/80: current_loss=0.02912 | best_loss=0.02815
Epoch 9/80: current_loss=0.02952 | best_loss=0.02815
Epoch 10/80: current_loss=0.02767 | best_loss=0.02767
Epoch 11/80: current_loss=0.02802 | best_loss=0.02767
Epoch 12/80: current_loss=0.02986 | best_loss=0.02767
Epoch 13/80: current_loss=0.02979 | best_loss=0.02767
Epoch 14/80: current_loss=0.02965 | best_loss=0.02767
Epoch 15/80: current_loss=0.02831 | best_loss=0.02767
Epoch 16/80: current_loss=0.02894 | best_loss=0.02767
Epoch 17/80: current_loss=0.02954 | best_loss=0.02767
Epoch 18/80: current_loss=0.03009 | best_loss=0.02767
Epoch 19/80: current_loss=0.02899 | best_loss=0.02767
Epoch 20/80: current_loss=0.02894 | best_loss=0.02767
Epoch 21/80: current_loss=0.03113 | best_loss=0.02767
Epoch 22/80: current_loss=0.03331 | best_loss=0.02767
Epoch 23/80: current_loss=0.02920 | best_loss=0.02767
Epoch 24/80: current_loss=0.02826 | best_loss=0.02767
Epoch 25/80: current_loss=0.02845 | best_loss=0.02767
Epoch 26/80: current_loss=0.02954 | best_loss=0.02767
Epoch 27/80: current_loss=0.03046 | best_loss=0.02767
Epoch 28/80: current_loss=0.02837 | best_loss=0.02767
Epoch 29/80: current_loss=0.02855 | best_loss=0.02767
Epoch 30/80: current_loss=0.02938 | best_loss=0.02767
Early Stopping at epoch 30
      explained_var=0.02119 | mse_loss=0.02808
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03568 | best_loss=0.03568
Epoch 1/80: current_loss=0.03496 | best_loss=0.03496
Epoch 2/80: current_loss=0.03253 | best_loss=0.03253
Epoch 3/80: current_loss=0.03487 | best_loss=0.03253
Epoch 4/80: current_loss=0.03299 | best_loss=0.03253
Epoch 5/80: current_loss=0.03445 | best_loss=0.03253
Epoch 6/80: current_loss=0.03669 | best_loss=0.03253
Epoch 7/80: current_loss=0.03564 | best_loss=0.03253
Epoch 8/80: current_loss=0.03344 | best_loss=0.03253
Epoch 9/80: current_loss=0.03283 | best_loss=0.03253
Epoch 10/80: current_loss=0.03619 | best_loss=0.03253
Epoch 11/80: current_loss=0.03343 | best_loss=0.03253
Epoch 12/80: current_loss=0.03430 | best_loss=0.03253
Epoch 13/80: current_loss=0.03333 | best_loss=0.03253
Epoch 14/80: current_loss=0.16260 | best_loss=0.03253
Epoch 15/80: current_loss=0.09601 | best_loss=0.03253
Epoch 16/80: current_loss=0.06013 | best_loss=0.03253
Epoch 17/80: current_loss=0.07649 | best_loss=0.03253
Epoch 18/80: current_loss=0.05238 | best_loss=0.03253
Epoch 19/80: current_loss=0.05359 | best_loss=0.03253
Epoch 20/80: current_loss=0.04417 | best_loss=0.03253
Epoch 21/80: current_loss=0.06915 | best_loss=0.03253
Epoch 22/80: current_loss=0.04435 | best_loss=0.03253
Early Stopping at epoch 22
      explained_var=-0.02331 | mse_loss=0.03335
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.03935| avg_loss=0.02788
----------------------------------------------


----------------------------------------------
Params for Trial 92
{'learning_rate': 0.001, 'weight_decay': 0.00026400873997665854, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03311 | best_loss=0.03311
Epoch 1/80: current_loss=0.02785 | best_loss=0.02785
Epoch 2/80: current_loss=0.02837 | best_loss=0.02785
Epoch 3/80: current_loss=0.03191 | best_loss=0.02785
Epoch 4/80: current_loss=0.02632 | best_loss=0.02632
Epoch 5/80: current_loss=0.02606 | best_loss=0.02606
Epoch 6/80: current_loss=0.02501 | best_loss=0.02501
Epoch 7/80: current_loss=0.02608 | best_loss=0.02501
Epoch 8/80: current_loss=0.02920 | best_loss=0.02501
Epoch 9/80: current_loss=0.02561 | best_loss=0.02501
Epoch 10/80: current_loss=0.02767 | best_loss=0.02501
Epoch 11/80: current_loss=0.02706 | best_loss=0.02501
Epoch 12/80: current_loss=0.03328 | best_loss=0.02501
Epoch 13/80: current_loss=0.03265 | best_loss=0.02501
Epoch 14/80: current_loss=0.02565 | best_loss=0.02501
Epoch 15/80: current_loss=0.03321 | best_loss=0.02501
Epoch 16/80: current_loss=0.03609 | best_loss=0.02501
Epoch 17/80: current_loss=0.03027 | best_loss=0.02501
Epoch 18/80: current_loss=0.02457 | best_loss=0.02457
Epoch 19/80: current_loss=0.02534 | best_loss=0.02457
Epoch 20/80: current_loss=0.02580 | best_loss=0.02457
Epoch 21/80: current_loss=0.02602 | best_loss=0.02457
Epoch 22/80: current_loss=0.02442 | best_loss=0.02442
Epoch 23/80: current_loss=0.02635 | best_loss=0.02442
Epoch 24/80: current_loss=0.02919 | best_loss=0.02442
Epoch 25/80: current_loss=0.03258 | best_loss=0.02442
Epoch 26/80: current_loss=0.02716 | best_loss=0.02442
Epoch 27/80: current_loss=0.02588 | best_loss=0.02442
Epoch 28/80: current_loss=0.04036 | best_loss=0.02442
Epoch 29/80: current_loss=0.03628 | best_loss=0.02442
Epoch 30/80: current_loss=0.03059 | best_loss=0.02442
Epoch 31/80: current_loss=0.02844 | best_loss=0.02442
Epoch 32/80: current_loss=0.02921 | best_loss=0.02442
Epoch 33/80: current_loss=0.02569 | best_loss=0.02442
Epoch 34/80: current_loss=0.02920 | best_loss=0.02442
Epoch 35/80: current_loss=0.02787 | best_loss=0.02442
Epoch 36/80: current_loss=0.02958 | best_loss=0.02442
Epoch 37/80: current_loss=0.02786 | best_loss=0.02442
Epoch 38/80: current_loss=0.02523 | best_loss=0.02442
Epoch 39/80: current_loss=0.02846 | best_loss=0.02442
Epoch 40/80: current_loss=0.02838 | best_loss=0.02442
Epoch 41/80: current_loss=0.02877 | best_loss=0.02442
Epoch 42/80: current_loss=0.02724 | best_loss=0.02442
Early Stopping at epoch 42
      explained_var=0.06918 | mse_loss=0.02403
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02990 | best_loss=0.02990
Epoch 1/80: current_loss=0.03421 | best_loss=0.02990
Epoch 2/80: current_loss=0.02839 | best_loss=0.02839
Epoch 3/80: current_loss=0.02995 | best_loss=0.02839
Epoch 4/80: current_loss=0.02874 | best_loss=0.02839
Epoch 5/80: current_loss=0.02878 | best_loss=0.02839
Epoch 6/80: current_loss=0.03058 | best_loss=0.02839
Epoch 7/80: current_loss=0.02799 | best_loss=0.02799
Epoch 8/80: current_loss=0.02828 | best_loss=0.02799
Epoch 9/80: current_loss=0.02941 | best_loss=0.02799
Epoch 10/80: current_loss=0.02843 | best_loss=0.02799
Epoch 11/80: current_loss=0.02793 | best_loss=0.02793
Epoch 12/80: current_loss=0.02805 | best_loss=0.02793
Epoch 13/80: current_loss=0.02805 | best_loss=0.02793
Epoch 14/80: current_loss=0.02857 | best_loss=0.02793
Epoch 15/80: current_loss=0.02888 | best_loss=0.02793
Epoch 16/80: current_loss=0.03290 | best_loss=0.02793
Epoch 17/80: current_loss=0.02993 | best_loss=0.02793
Epoch 18/80: current_loss=0.02855 | best_loss=0.02793
Epoch 19/80: current_loss=0.03030 | best_loss=0.02793
Epoch 20/80: current_loss=0.03364 | best_loss=0.02793
Epoch 21/80: current_loss=0.03052 | best_loss=0.02793
Epoch 22/80: current_loss=0.03119 | best_loss=0.02793
Epoch 23/80: current_loss=0.02924 | best_loss=0.02793
Epoch 24/80: current_loss=0.02931 | best_loss=0.02793
Epoch 25/80: current_loss=0.03333 | best_loss=0.02793
Epoch 26/80: current_loss=0.02807 | best_loss=0.02793
Epoch 27/80: current_loss=0.02779 | best_loss=0.02779
Epoch 28/80: current_loss=0.02842 | best_loss=0.02779
Epoch 29/80: current_loss=0.02822 | best_loss=0.02779
Epoch 30/80: current_loss=0.02985 | best_loss=0.02779
Epoch 31/80: current_loss=0.02750 | best_loss=0.02750
Epoch 32/80: current_loss=0.03088 | best_loss=0.02750
Epoch 33/80: current_loss=0.03120 | best_loss=0.02750
Epoch 34/80: current_loss=0.02921 | best_loss=0.02750
Epoch 35/80: current_loss=0.02912 | best_loss=0.02750
Epoch 36/80: current_loss=0.02758 | best_loss=0.02750
Epoch 37/80: current_loss=0.02943 | best_loss=0.02750
Epoch 38/80: current_loss=0.02917 | best_loss=0.02750
Epoch 39/80: current_loss=0.02783 | best_loss=0.02750
Epoch 40/80: current_loss=0.02921 | best_loss=0.02750
Epoch 41/80: current_loss=0.02793 | best_loss=0.02750
Epoch 42/80: current_loss=0.02800 | best_loss=0.02750
Epoch 43/80: current_loss=0.02847 | best_loss=0.02750
Epoch 44/80: current_loss=0.02794 | best_loss=0.02750
Epoch 45/80: current_loss=0.02767 | best_loss=0.02750
Epoch 46/80: current_loss=0.02871 | best_loss=0.02750
Epoch 47/80: current_loss=0.02789 | best_loss=0.02750
Epoch 48/80: current_loss=0.02902 | best_loss=0.02750
Epoch 49/80: current_loss=0.02827 | best_loss=0.02750
Epoch 50/80: current_loss=0.02792 | best_loss=0.02750
Epoch 51/80: current_loss=0.02953 | best_loss=0.02750
Early Stopping at epoch 51
      explained_var=0.02806 | mse_loss=0.02702
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03318 | best_loss=0.03318
Epoch 1/80: current_loss=0.03012 | best_loss=0.03012
Epoch 2/80: current_loss=0.03567 | best_loss=0.03012
Epoch 3/80: current_loss=0.03003 | best_loss=0.03003
Epoch 4/80: current_loss=0.02997 | best_loss=0.02997
Epoch 5/80: current_loss=0.03095 | best_loss=0.02997
Epoch 6/80: current_loss=0.02959 | best_loss=0.02959
Epoch 7/80: current_loss=0.03203 | best_loss=0.02959
Epoch 8/80: current_loss=0.02947 | best_loss=0.02947
Epoch 9/80: current_loss=0.02958 | best_loss=0.02947
Epoch 10/80: current_loss=0.03029 | best_loss=0.02947
Epoch 11/80: current_loss=0.03031 | best_loss=0.02947
Epoch 12/80: current_loss=0.03129 | best_loss=0.02947
Epoch 13/80: current_loss=0.02986 | best_loss=0.02947
Epoch 14/80: current_loss=0.03083 | best_loss=0.02947
Epoch 15/80: current_loss=0.03284 | best_loss=0.02947
Epoch 16/80: current_loss=0.03003 | best_loss=0.02947
Epoch 17/80: current_loss=0.03061 | best_loss=0.02947
Epoch 18/80: current_loss=0.03121 | best_loss=0.02947
Epoch 19/80: current_loss=0.03204 | best_loss=0.02947
Epoch 20/80: current_loss=0.03152 | best_loss=0.02947
Epoch 21/80: current_loss=0.02961 | best_loss=0.02947
Epoch 22/80: current_loss=0.02954 | best_loss=0.02947
Epoch 23/80: current_loss=0.03211 | best_loss=0.02947
Epoch 24/80: current_loss=0.03032 | best_loss=0.02947
Epoch 25/80: current_loss=0.03151 | best_loss=0.02947
Epoch 26/80: current_loss=0.03034 | best_loss=0.02947
Epoch 27/80: current_loss=0.03242 | best_loss=0.02947
Epoch 28/80: current_loss=0.02958 | best_loss=0.02947
Early Stopping at epoch 28
      explained_var=0.03180 | mse_loss=0.02868
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02930 | best_loss=0.02930
Epoch 1/80: current_loss=0.02829 | best_loss=0.02829
Epoch 2/80: current_loss=0.02848 | best_loss=0.02829
Epoch 3/80: current_loss=0.02908 | best_loss=0.02829
Epoch 4/80: current_loss=0.02950 | best_loss=0.02829
Epoch 5/80: current_loss=0.02999 | best_loss=0.02829
Epoch 6/80: current_loss=0.02912 | best_loss=0.02829
Epoch 7/80: current_loss=0.02936 | best_loss=0.02829
Epoch 8/80: current_loss=0.02856 | best_loss=0.02829
Epoch 9/80: current_loss=0.03291 | best_loss=0.02829
Epoch 10/80: current_loss=0.02832 | best_loss=0.02829
Epoch 11/80: current_loss=0.02841 | best_loss=0.02829
Epoch 12/80: current_loss=0.02832 | best_loss=0.02829
Epoch 13/80: current_loss=0.03030 | best_loss=0.02829
Epoch 14/80: current_loss=0.02817 | best_loss=0.02817
Epoch 15/80: current_loss=0.02818 | best_loss=0.02817
Epoch 16/80: current_loss=0.02816 | best_loss=0.02816
Epoch 17/80: current_loss=0.02840 | best_loss=0.02816
Epoch 18/80: current_loss=0.02888 | best_loss=0.02816
Epoch 19/80: current_loss=0.02892 | best_loss=0.02816
Epoch 20/80: current_loss=0.02949 | best_loss=0.02816
Epoch 21/80: current_loss=0.03048 | best_loss=0.02816
Epoch 22/80: current_loss=0.02857 | best_loss=0.02816
Epoch 23/80: current_loss=0.02841 | best_loss=0.02816
Epoch 24/80: current_loss=0.02946 | best_loss=0.02816
Epoch 25/80: current_loss=0.02824 | best_loss=0.02816
Epoch 26/80: current_loss=0.02813 | best_loss=0.02813
Epoch 27/80: current_loss=0.02797 | best_loss=0.02797
Epoch 28/80: current_loss=0.02873 | best_loss=0.02797
Epoch 29/80: current_loss=0.02863 | best_loss=0.02797
Epoch 30/80: current_loss=0.02893 | best_loss=0.02797
Epoch 31/80: current_loss=0.02891 | best_loss=0.02797
Epoch 32/80: current_loss=0.02870 | best_loss=0.02797
Epoch 33/80: current_loss=0.02851 | best_loss=0.02797
Epoch 34/80: current_loss=0.02873 | best_loss=0.02797
Epoch 35/80: current_loss=0.02891 | best_loss=0.02797
Epoch 36/80: current_loss=0.02891 | best_loss=0.02797
Epoch 37/80: current_loss=0.02932 | best_loss=0.02797
Epoch 38/80: current_loss=0.02916 | best_loss=0.02797
Epoch 39/80: current_loss=0.03001 | best_loss=0.02797
Epoch 40/80: current_loss=0.02889 | best_loss=0.02797
Epoch 41/80: current_loss=0.02856 | best_loss=0.02797
Epoch 42/80: current_loss=0.02899 | best_loss=0.02797
Epoch 43/80: current_loss=0.02851 | best_loss=0.02797
Epoch 44/80: current_loss=0.02876 | best_loss=0.02797
Epoch 45/80: current_loss=0.02843 | best_loss=0.02797
Epoch 46/80: current_loss=0.02897 | best_loss=0.02797
Epoch 47/80: current_loss=0.02877 | best_loss=0.02797
Early Stopping at epoch 47
      explained_var=0.00174 | mse_loss=0.02835
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03306 | best_loss=0.03306
Epoch 1/80: current_loss=0.03452 | best_loss=0.03306
Epoch 2/80: current_loss=0.03440 | best_loss=0.03306
Epoch 3/80: current_loss=0.08581 | best_loss=0.03306
Epoch 4/80: current_loss=0.03501 | best_loss=0.03306
Epoch 5/80: current_loss=0.03511 | best_loss=0.03306
Epoch 6/80: current_loss=0.03845 | best_loss=0.03306
Epoch 7/80: current_loss=0.03439 | best_loss=0.03306
Epoch 8/80: current_loss=0.03375 | best_loss=0.03306
Epoch 9/80: current_loss=0.03774 | best_loss=0.03306
Epoch 10/80: current_loss=0.03519 | best_loss=0.03306
Epoch 11/80: current_loss=0.04283 | best_loss=0.03306
Epoch 12/80: current_loss=0.03370 | best_loss=0.03306
Epoch 13/80: current_loss=0.03431 | best_loss=0.03306
Epoch 14/80: current_loss=0.03470 | best_loss=0.03306
Epoch 15/80: current_loss=0.03354 | best_loss=0.03306
Epoch 16/80: current_loss=0.03437 | best_loss=0.03306
Epoch 17/80: current_loss=0.03492 | best_loss=0.03306
Epoch 18/80: current_loss=0.03508 | best_loss=0.03306
Epoch 19/80: current_loss=0.03423 | best_loss=0.03306
Epoch 20/80: current_loss=0.03423 | best_loss=0.03306
Early Stopping at epoch 20
      explained_var=-0.03651 | mse_loss=0.03372
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.01886| avg_loss=0.02836
----------------------------------------------


----------------------------------------------
Params for Trial 93
{'learning_rate': 0.001, 'weight_decay': 0.0005770810810565897, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02696 | best_loss=0.02696
Epoch 1/80: current_loss=0.02699 | best_loss=0.02696
Epoch 2/80: current_loss=0.02753 | best_loss=0.02696
Epoch 3/80: current_loss=0.02716 | best_loss=0.02696
Epoch 4/80: current_loss=0.02700 | best_loss=0.02696
Epoch 5/80: current_loss=0.02551 | best_loss=0.02551
Epoch 6/80: current_loss=0.02687 | best_loss=0.02551
Epoch 7/80: current_loss=0.02687 | best_loss=0.02551
Epoch 8/80: current_loss=0.02627 | best_loss=0.02551
Epoch 9/80: current_loss=0.02606 | best_loss=0.02551
Epoch 10/80: current_loss=0.02628 | best_loss=0.02551
Epoch 11/80: current_loss=0.02564 | best_loss=0.02551
Epoch 12/80: current_loss=0.02624 | best_loss=0.02551
Epoch 13/80: current_loss=0.02657 | best_loss=0.02551
Epoch 14/80: current_loss=0.02607 | best_loss=0.02551
Epoch 15/80: current_loss=0.02635 | best_loss=0.02551
Epoch 16/80: current_loss=0.02597 | best_loss=0.02551
Epoch 17/80: current_loss=0.02661 | best_loss=0.02551
Epoch 18/80: current_loss=0.02647 | best_loss=0.02551
Epoch 19/80: current_loss=0.02558 | best_loss=0.02551
Epoch 20/80: current_loss=0.02582 | best_loss=0.02551
Epoch 21/80: current_loss=0.02683 | best_loss=0.02551
Epoch 22/80: current_loss=0.02588 | best_loss=0.02551
Epoch 23/80: current_loss=0.02542 | best_loss=0.02542
Epoch 24/80: current_loss=0.02595 | best_loss=0.02542
Epoch 25/80: current_loss=0.02547 | best_loss=0.02542
Epoch 26/80: current_loss=0.02696 | best_loss=0.02542
Epoch 27/80: current_loss=0.02571 | best_loss=0.02542
Epoch 28/80: current_loss=0.02580 | best_loss=0.02542
Epoch 29/80: current_loss=0.02705 | best_loss=0.02542
Epoch 30/80: current_loss=0.02549 | best_loss=0.02542
Epoch 31/80: current_loss=0.02686 | best_loss=0.02542
Epoch 32/80: current_loss=0.02690 | best_loss=0.02542
Epoch 33/80: current_loss=0.02587 | best_loss=0.02542
Epoch 34/80: current_loss=0.02737 | best_loss=0.02542
Epoch 35/80: current_loss=0.02577 | best_loss=0.02542
Epoch 36/80: current_loss=0.02580 | best_loss=0.02542
Epoch 37/80: current_loss=0.02587 | best_loss=0.02542
Epoch 38/80: current_loss=0.02554 | best_loss=0.02542
Epoch 39/80: current_loss=0.02599 | best_loss=0.02542
Epoch 40/80: current_loss=0.02597 | best_loss=0.02542
Epoch 41/80: current_loss=0.02622 | best_loss=0.02542
Epoch 42/80: current_loss=0.02589 | best_loss=0.02542
Epoch 43/80: current_loss=0.02702 | best_loss=0.02542
Early Stopping at epoch 43
      explained_var=0.06148 | mse_loss=0.02510

----------------------------------------------
Params for Trial 94
{'learning_rate': 0.001, 'weight_decay': 0.0007601221454717703, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03162 | best_loss=0.03162
Epoch 1/80: current_loss=0.03135 | best_loss=0.03135
Epoch 2/80: current_loss=0.02678 | best_loss=0.02678
Epoch 3/80: current_loss=0.02581 | best_loss=0.02581
Epoch 4/80: current_loss=0.02542 | best_loss=0.02542
Epoch 5/80: current_loss=0.02536 | best_loss=0.02536
Epoch 6/80: current_loss=0.02551 | best_loss=0.02536
Epoch 7/80: current_loss=0.02586 | best_loss=0.02536
Epoch 8/80: current_loss=0.02563 | best_loss=0.02536
Epoch 9/80: current_loss=0.02597 | best_loss=0.02536
Epoch 10/80: current_loss=0.02437 | best_loss=0.02437
Epoch 11/80: current_loss=0.02869 | best_loss=0.02437
Epoch 12/80: current_loss=0.03611 | best_loss=0.02437
Epoch 13/80: current_loss=0.03219 | best_loss=0.02437
Epoch 14/80: current_loss=0.02644 | best_loss=0.02437
Epoch 15/80: current_loss=0.02695 | best_loss=0.02437
Epoch 16/80: current_loss=0.02766 | best_loss=0.02437
Epoch 17/80: current_loss=0.02712 | best_loss=0.02437
Epoch 18/80: current_loss=0.02711 | best_loss=0.02437
Epoch 19/80: current_loss=0.03073 | best_loss=0.02437
Epoch 20/80: current_loss=0.03901 | best_loss=0.02437
Epoch 21/80: current_loss=0.03294 | best_loss=0.02437
Epoch 22/80: current_loss=0.02761 | best_loss=0.02437
Epoch 23/80: current_loss=0.02601 | best_loss=0.02437
Epoch 24/80: current_loss=0.02599 | best_loss=0.02437
Epoch 25/80: current_loss=0.02766 | best_loss=0.02437
Epoch 26/80: current_loss=0.02696 | best_loss=0.02437
Epoch 27/80: current_loss=0.02677 | best_loss=0.02437
Epoch 28/80: current_loss=0.02731 | best_loss=0.02437
Epoch 29/80: current_loss=0.02668 | best_loss=0.02437
Epoch 30/80: current_loss=0.02477 | best_loss=0.02437
Early Stopping at epoch 30
      explained_var=0.07727 | mse_loss=0.02387
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02840 | best_loss=0.02840
Epoch 1/80: current_loss=0.03138 | best_loss=0.02840
Epoch 2/80: current_loss=0.02795 | best_loss=0.02795
Epoch 3/80: current_loss=0.03123 | best_loss=0.02795
Epoch 4/80: current_loss=0.02804 | best_loss=0.02795
Epoch 5/80: current_loss=0.03357 | best_loss=0.02795
Epoch 6/80: current_loss=0.03041 | best_loss=0.02795
Epoch 7/80: current_loss=0.03055 | best_loss=0.02795
Epoch 8/80: current_loss=0.02798 | best_loss=0.02795
Epoch 9/80: current_loss=0.02778 | best_loss=0.02778
Epoch 10/80: current_loss=0.02947 | best_loss=0.02778
Epoch 11/80: current_loss=0.02762 | best_loss=0.02762
Epoch 12/80: current_loss=0.02848 | best_loss=0.02762
Epoch 13/80: current_loss=0.03609 | best_loss=0.02762
Epoch 14/80: current_loss=0.02970 | best_loss=0.02762
Epoch 15/80: current_loss=0.02768 | best_loss=0.02762
Epoch 16/80: current_loss=0.03455 | best_loss=0.02762
Epoch 17/80: current_loss=0.02771 | best_loss=0.02762
Epoch 18/80: current_loss=0.02994 | best_loss=0.02762
Epoch 19/80: current_loss=0.02901 | best_loss=0.02762
Epoch 20/80: current_loss=0.02970 | best_loss=0.02762
Epoch 21/80: current_loss=0.02782 | best_loss=0.02762
Epoch 22/80: current_loss=0.03025 | best_loss=0.02762
Epoch 23/80: current_loss=0.02901 | best_loss=0.02762
Epoch 24/80: current_loss=0.02775 | best_loss=0.02762
Epoch 25/80: current_loss=0.02827 | best_loss=0.02762
Epoch 26/80: current_loss=0.02912 | best_loss=0.02762
Epoch 27/80: current_loss=0.02955 | best_loss=0.02762
Epoch 28/80: current_loss=0.02881 | best_loss=0.02762
Epoch 29/80: current_loss=0.20085 | best_loss=0.02762
Epoch 30/80: current_loss=0.27180 | best_loss=0.02762
Epoch 31/80: current_loss=0.16166 | best_loss=0.02762
Early Stopping at epoch 31
      explained_var=0.02657 | mse_loss=0.02709
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.19119 | best_loss=0.19119
Epoch 1/80: current_loss=0.08195 | best_loss=0.08195
Epoch 2/80: current_loss=0.05176 | best_loss=0.05176
Epoch 3/80: current_loss=0.04289 | best_loss=0.04289
Epoch 4/80: current_loss=0.03612 | best_loss=0.03612
Epoch 5/80: current_loss=0.03387 | best_loss=0.03387
Epoch 6/80: current_loss=0.04595 | best_loss=0.03387
Epoch 7/80: current_loss=0.04785 | best_loss=0.03387
Epoch 8/80: current_loss=0.03150 | best_loss=0.03150
Epoch 9/80: current_loss=0.03242 | best_loss=0.03150
Epoch 10/80: current_loss=0.03236 | best_loss=0.03150
Epoch 11/80: current_loss=0.03611 | best_loss=0.03150
Epoch 12/80: current_loss=0.03242 | best_loss=0.03150
Epoch 13/80: current_loss=0.03560 | best_loss=0.03150
Epoch 14/80: current_loss=0.03213 | best_loss=0.03150
Epoch 15/80: current_loss=0.03385 | best_loss=0.03150
Epoch 16/80: current_loss=0.03691 | best_loss=0.03150
Epoch 17/80: current_loss=0.03338 | best_loss=0.03150
Epoch 18/80: current_loss=0.03386 | best_loss=0.03150
Epoch 19/80: current_loss=0.03231 | best_loss=0.03150
Epoch 20/80: current_loss=0.04192 | best_loss=0.03150
Epoch 21/80: current_loss=0.03409 | best_loss=0.03150
Epoch 22/80: current_loss=0.03220 | best_loss=0.03150
Epoch 23/80: current_loss=0.03375 | best_loss=0.03150
Epoch 24/80: current_loss=0.03358 | best_loss=0.03150
Epoch 25/80: current_loss=0.03653 | best_loss=0.03150
Epoch 26/80: current_loss=0.03577 | best_loss=0.03150
Epoch 27/80: current_loss=0.03252 | best_loss=0.03150
Epoch 28/80: current_loss=0.03981 | best_loss=0.03150
Early Stopping at epoch 28
      explained_var=-0.03958 | mse_loss=0.03079
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.05681 | best_loss=0.05681
Epoch 1/80: current_loss=0.03738 | best_loss=0.03738
Epoch 2/80: current_loss=0.04151 | best_loss=0.03738
Epoch 3/80: current_loss=0.03838 | best_loss=0.03738
Epoch 4/80: current_loss=0.03095 | best_loss=0.03095
Epoch 5/80: current_loss=0.03303 | best_loss=0.03095
Epoch 6/80: current_loss=0.03079 | best_loss=0.03079
Epoch 7/80: current_loss=0.03471 | best_loss=0.03079
Epoch 8/80: current_loss=0.03455 | best_loss=0.03079
Epoch 9/80: current_loss=0.03347 | best_loss=0.03079
Epoch 10/80: current_loss=0.03262 | best_loss=0.03079
Epoch 11/80: current_loss=0.03714 | best_loss=0.03079
Epoch 12/80: current_loss=0.03303 | best_loss=0.03079
Epoch 13/80: current_loss=0.03034 | best_loss=0.03034
Epoch 14/80: current_loss=0.02963 | best_loss=0.02963
Epoch 15/80: current_loss=0.02963 | best_loss=0.02963
Epoch 16/80: current_loss=0.02899 | best_loss=0.02899
Epoch 17/80: current_loss=0.02861 | best_loss=0.02861
Epoch 18/80: current_loss=0.02864 | best_loss=0.02861
Epoch 19/80: current_loss=0.02869 | best_loss=0.02861
Epoch 20/80: current_loss=0.02861 | best_loss=0.02861
Epoch 21/80: current_loss=0.02904 | best_loss=0.02861
Epoch 22/80: current_loss=0.02878 | best_loss=0.02861
Epoch 23/80: current_loss=0.02879 | best_loss=0.02861
Epoch 24/80: current_loss=0.02838 | best_loss=0.02838
Epoch 25/80: current_loss=0.02827 | best_loss=0.02827
Epoch 26/80: current_loss=0.02889 | best_loss=0.02827
Epoch 27/80: current_loss=0.02813 | best_loss=0.02813
Epoch 28/80: current_loss=0.02820 | best_loss=0.02813
Epoch 29/80: current_loss=0.02923 | best_loss=0.02813
Epoch 30/80: current_loss=0.02990 | best_loss=0.02813
Epoch 31/80: current_loss=0.02859 | best_loss=0.02813
Epoch 32/80: current_loss=0.02994 | best_loss=0.02813
Epoch 33/80: current_loss=0.02874 | best_loss=0.02813
Epoch 34/80: current_loss=0.02826 | best_loss=0.02813
Epoch 35/80: current_loss=0.02860 | best_loss=0.02813
Epoch 36/80: current_loss=0.02833 | best_loss=0.02813
Epoch 37/80: current_loss=0.02829 | best_loss=0.02813
Epoch 38/80: current_loss=0.02918 | best_loss=0.02813
Epoch 39/80: current_loss=0.02910 | best_loss=0.02813
Epoch 40/80: current_loss=0.02861 | best_loss=0.02813
Epoch 41/80: current_loss=0.02837 | best_loss=0.02813
Epoch 42/80: current_loss=0.02843 | best_loss=0.02813
Epoch 43/80: current_loss=0.02956 | best_loss=0.02813
Epoch 44/80: current_loss=0.02897 | best_loss=0.02813
Epoch 45/80: current_loss=0.02874 | best_loss=0.02813
Epoch 46/80: current_loss=0.02934 | best_loss=0.02813
Epoch 47/80: current_loss=0.03066 | best_loss=0.02813
Early Stopping at epoch 47
      explained_var=-0.00176 | mse_loss=0.02843
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03257 | best_loss=0.03257
Epoch 1/80: current_loss=0.03423 | best_loss=0.03257
Epoch 2/80: current_loss=0.03328 | best_loss=0.03257
Epoch 3/80: current_loss=0.03376 | best_loss=0.03257
Epoch 4/80: current_loss=0.03569 | best_loss=0.03257
Epoch 5/80: current_loss=0.03379 | best_loss=0.03257
Epoch 6/80: current_loss=0.03352 | best_loss=0.03257
Epoch 7/80: current_loss=0.03340 | best_loss=0.03257
Epoch 8/80: current_loss=0.03470 | best_loss=0.03257
Epoch 9/80: current_loss=0.03352 | best_loss=0.03257
Epoch 10/80: current_loss=0.03291 | best_loss=0.03257
Epoch 11/80: current_loss=0.03378 | best_loss=0.03257
Epoch 12/80: current_loss=0.03778 | best_loss=0.03257
Epoch 13/80: current_loss=0.03270 | best_loss=0.03257
Epoch 14/80: current_loss=0.03327 | best_loss=0.03257
Epoch 15/80: current_loss=0.03419 | best_loss=0.03257
Epoch 16/80: current_loss=0.03467 | best_loss=0.03257
Epoch 17/80: current_loss=0.03485 | best_loss=0.03257
Epoch 18/80: current_loss=0.03329 | best_loss=0.03257
Epoch 19/80: current_loss=0.03389 | best_loss=0.03257
Epoch 20/80: current_loss=0.03403 | best_loss=0.03257
Early Stopping at epoch 20
      explained_var=-0.02474 | mse_loss=0.03334
----------------------------------------------
Average early_stopping_point: 11| avg_exp_var=0.00755| avg_loss=0.02871
----------------------------------------------


----------------------------------------------
Params for Trial 95
{'learning_rate': 0.0001, 'weight_decay': 0.00013644006761028444, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03832 | best_loss=0.03832
Epoch 1/80: current_loss=0.02810 | best_loss=0.02810
Epoch 2/80: current_loss=0.02903 | best_loss=0.02810
Epoch 3/80: current_loss=0.02768 | best_loss=0.02768
Epoch 4/80: current_loss=0.02756 | best_loss=0.02756
Epoch 5/80: current_loss=0.02732 | best_loss=0.02732
Epoch 6/80: current_loss=0.02646 | best_loss=0.02646
Epoch 7/80: current_loss=0.02643 | best_loss=0.02643
Epoch 8/80: current_loss=0.02696 | best_loss=0.02643
Epoch 9/80: current_loss=0.02634 | best_loss=0.02634
Epoch 10/80: current_loss=0.02684 | best_loss=0.02634
Epoch 11/80: current_loss=0.02691 | best_loss=0.02634
Epoch 12/80: current_loss=0.02745 | best_loss=0.02634
Epoch 13/80: current_loss=0.02900 | best_loss=0.02634
Epoch 14/80: current_loss=0.02649 | best_loss=0.02634
Epoch 15/80: current_loss=0.02659 | best_loss=0.02634
Epoch 16/80: current_loss=0.02902 | best_loss=0.02634
Epoch 17/80: current_loss=0.02636 | best_loss=0.02634
Epoch 18/80: current_loss=0.02907 | best_loss=0.02634
Epoch 19/80: current_loss=0.02584 | best_loss=0.02584
Epoch 20/80: current_loss=0.02858 | best_loss=0.02584
Epoch 21/80: current_loss=0.02598 | best_loss=0.02584
Epoch 22/80: current_loss=0.02807 | best_loss=0.02584
Epoch 23/80: current_loss=0.02573 | best_loss=0.02573
Epoch 24/80: current_loss=0.02639 | best_loss=0.02573
Epoch 25/80: current_loss=0.02705 | best_loss=0.02573
Epoch 26/80: current_loss=0.02589 | best_loss=0.02573
Epoch 27/80: current_loss=0.02770 | best_loss=0.02573
Epoch 28/80: current_loss=0.02612 | best_loss=0.02573
Epoch 29/80: current_loss=0.02871 | best_loss=0.02573
Epoch 30/80: current_loss=0.02595 | best_loss=0.02573
Epoch 31/80: current_loss=0.02585 | best_loss=0.02573
Epoch 32/80: current_loss=0.02708 | best_loss=0.02573
Epoch 33/80: current_loss=0.02595 | best_loss=0.02573
Epoch 34/80: current_loss=0.02817 | best_loss=0.02573
Epoch 35/80: current_loss=0.02584 | best_loss=0.02573
Epoch 36/80: current_loss=0.02733 | best_loss=0.02573
Epoch 37/80: current_loss=0.02646 | best_loss=0.02573
Epoch 38/80: current_loss=0.02591 | best_loss=0.02573
Epoch 39/80: current_loss=0.02697 | best_loss=0.02573
Epoch 40/80: current_loss=0.02575 | best_loss=0.02573
Epoch 41/80: current_loss=0.02735 | best_loss=0.02573
Epoch 42/80: current_loss=0.02605 | best_loss=0.02573
Epoch 43/80: current_loss=0.02761 | best_loss=0.02573
Early Stopping at epoch 43
      explained_var=0.03192 | mse_loss=0.02536

----------------------------------------------
Params for Trial 96
{'learning_rate': 0.001, 'weight_decay': 0.0012460549947755136, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03620 | best_loss=0.03620
Epoch 1/80: current_loss=0.02729 | best_loss=0.02729
Epoch 2/80: current_loss=0.03002 | best_loss=0.02729
Epoch 3/80: current_loss=0.03022 | best_loss=0.02729
Epoch 4/80: current_loss=0.02796 | best_loss=0.02729
Epoch 5/80: current_loss=0.02644 | best_loss=0.02644
Epoch 6/80: current_loss=0.02572 | best_loss=0.02572
Epoch 7/80: current_loss=0.02694 | best_loss=0.02572
Epoch 8/80: current_loss=0.02549 | best_loss=0.02549
Epoch 9/80: current_loss=0.02491 | best_loss=0.02491
Epoch 10/80: current_loss=0.03073 | best_loss=0.02491
Epoch 11/80: current_loss=0.04000 | best_loss=0.02491
Epoch 12/80: current_loss=0.02586 | best_loss=0.02491
Epoch 13/80: current_loss=0.02549 | best_loss=0.02491
Epoch 14/80: current_loss=0.02473 | best_loss=0.02473
Epoch 15/80: current_loss=0.02703 | best_loss=0.02473
Epoch 16/80: current_loss=0.03327 | best_loss=0.02473
Epoch 17/80: current_loss=0.03111 | best_loss=0.02473
Epoch 18/80: current_loss=0.03099 | best_loss=0.02473
Epoch 19/80: current_loss=0.02913 | best_loss=0.02473
Epoch 20/80: current_loss=0.02743 | best_loss=0.02473
Epoch 21/80: current_loss=0.02821 | best_loss=0.02473
Epoch 22/80: current_loss=0.02675 | best_loss=0.02473
Epoch 23/80: current_loss=0.02519 | best_loss=0.02473
Epoch 24/80: current_loss=0.02624 | best_loss=0.02473
Epoch 25/80: current_loss=0.03108 | best_loss=0.02473
Epoch 26/80: current_loss=0.02786 | best_loss=0.02473
Epoch 27/80: current_loss=0.02675 | best_loss=0.02473
Epoch 28/80: current_loss=0.03190 | best_loss=0.02473
Epoch 29/80: current_loss=0.03862 | best_loss=0.02473
Epoch 30/80: current_loss=0.03201 | best_loss=0.02473
Epoch 31/80: current_loss=0.02812 | best_loss=0.02473
Epoch 32/80: current_loss=0.02638 | best_loss=0.02473
Epoch 33/80: current_loss=0.02577 | best_loss=0.02473
Epoch 34/80: current_loss=0.02561 | best_loss=0.02473
Early Stopping at epoch 34
      explained_var=0.06689 | mse_loss=0.02416
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03381 | best_loss=0.03381
Epoch 1/80: current_loss=0.03127 | best_loss=0.03127
Epoch 2/80: current_loss=0.02832 | best_loss=0.02832
Epoch 3/80: current_loss=0.03012 | best_loss=0.02832
Epoch 4/80: current_loss=0.02871 | best_loss=0.02832
Epoch 5/80: current_loss=0.02805 | best_loss=0.02805
Epoch 6/80: current_loss=0.04070 | best_loss=0.02805
Epoch 7/80: current_loss=0.02864 | best_loss=0.02805
Epoch 8/80: current_loss=0.03834 | best_loss=0.02805
Epoch 9/80: current_loss=0.02781 | best_loss=0.02781
Epoch 10/80: current_loss=0.02778 | best_loss=0.02778
Epoch 11/80: current_loss=0.02800 | best_loss=0.02778
Epoch 12/80: current_loss=0.03025 | best_loss=0.02778
Epoch 13/80: current_loss=0.02787 | best_loss=0.02778
Epoch 14/80: current_loss=0.02900 | best_loss=0.02778
Epoch 15/80: current_loss=0.02777 | best_loss=0.02777
Epoch 16/80: current_loss=0.02783 | best_loss=0.02777
Epoch 17/80: current_loss=0.02789 | best_loss=0.02777
Epoch 18/80: current_loss=0.02810 | best_loss=0.02777
Epoch 19/80: current_loss=0.02776 | best_loss=0.02776
Epoch 20/80: current_loss=0.02820 | best_loss=0.02776
Epoch 21/80: current_loss=0.02856 | best_loss=0.02776
Epoch 22/80: current_loss=0.02799 | best_loss=0.02776
Epoch 23/80: current_loss=0.02844 | best_loss=0.02776
Epoch 24/80: current_loss=0.02853 | best_loss=0.02776
Epoch 25/80: current_loss=0.02860 | best_loss=0.02776
Epoch 26/80: current_loss=0.02907 | best_loss=0.02776
Epoch 27/80: current_loss=0.02813 | best_loss=0.02776
Epoch 28/80: current_loss=0.02821 | best_loss=0.02776
Epoch 29/80: current_loss=0.02820 | best_loss=0.02776
Epoch 30/80: current_loss=0.03056 | best_loss=0.02776
Epoch 31/80: current_loss=0.03009 | best_loss=0.02776
Epoch 32/80: current_loss=0.03243 | best_loss=0.02776
Epoch 33/80: current_loss=0.02749 | best_loss=0.02749
Epoch 34/80: current_loss=0.02779 | best_loss=0.02749
Epoch 35/80: current_loss=0.02794 | best_loss=0.02749
Epoch 36/80: current_loss=0.02827 | best_loss=0.02749
Epoch 37/80: current_loss=0.02847 | best_loss=0.02749
Epoch 38/80: current_loss=0.02837 | best_loss=0.02749
Epoch 39/80: current_loss=0.02798 | best_loss=0.02749
Epoch 40/80: current_loss=0.02802 | best_loss=0.02749
Epoch 41/80: current_loss=0.02985 | best_loss=0.02749
Epoch 42/80: current_loss=0.02851 | best_loss=0.02749
Epoch 43/80: current_loss=0.02846 | best_loss=0.02749
Epoch 44/80: current_loss=0.03618 | best_loss=0.02749
Epoch 45/80: current_loss=0.02817 | best_loss=0.02749
Epoch 46/80: current_loss=0.02891 | best_loss=0.02749
Epoch 47/80: current_loss=0.03450 | best_loss=0.02749
Epoch 48/80: current_loss=0.02831 | best_loss=0.02749
Epoch 49/80: current_loss=0.02988 | best_loss=0.02749
Epoch 50/80: current_loss=0.02964 | best_loss=0.02749
Epoch 51/80: current_loss=0.02865 | best_loss=0.02749
Epoch 52/80: current_loss=0.02827 | best_loss=0.02749
Epoch 53/80: current_loss=0.02986 | best_loss=0.02749
Early Stopping at epoch 53
      explained_var=0.02971 | mse_loss=0.02698
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02999 | best_loss=0.02999
Epoch 1/80: current_loss=0.03011 | best_loss=0.02999
Epoch 2/80: current_loss=0.02968 | best_loss=0.02968
Epoch 3/80: current_loss=0.03030 | best_loss=0.02968
Epoch 4/80: current_loss=0.02978 | best_loss=0.02968
Epoch 5/80: current_loss=0.03047 | best_loss=0.02968
Epoch 6/80: current_loss=0.02977 | best_loss=0.02968
Epoch 7/80: current_loss=0.03047 | best_loss=0.02968
Epoch 8/80: current_loss=0.03050 | best_loss=0.02968
Epoch 9/80: current_loss=0.03560 | best_loss=0.02968
Epoch 10/80: current_loss=0.03236 | best_loss=0.02968
Epoch 11/80: current_loss=0.03182 | best_loss=0.02968
Epoch 12/80: current_loss=0.03189 | best_loss=0.02968
Epoch 13/80: current_loss=0.02999 | best_loss=0.02968
Epoch 14/80: current_loss=0.02981 | best_loss=0.02968
Epoch 15/80: current_loss=0.02995 | best_loss=0.02968
Epoch 16/80: current_loss=0.03096 | best_loss=0.02968
Epoch 17/80: current_loss=0.03306 | best_loss=0.02968
Epoch 18/80: current_loss=0.03532 | best_loss=0.02968
Epoch 19/80: current_loss=0.03026 | best_loss=0.02968
Epoch 20/80: current_loss=0.03208 | best_loss=0.02968
Epoch 21/80: current_loss=0.03490 | best_loss=0.02968
Epoch 22/80: current_loss=0.03026 | best_loss=0.02968
Early Stopping at epoch 22
      explained_var=0.02496 | mse_loss=0.02888
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03101 | best_loss=0.03101
Epoch 1/80: current_loss=0.03078 | best_loss=0.03078
Epoch 2/80: current_loss=0.02974 | best_loss=0.02974
Epoch 3/80: current_loss=0.02848 | best_loss=0.02848
Epoch 4/80: current_loss=0.03198 | best_loss=0.02848
Epoch 5/80: current_loss=0.02900 | best_loss=0.02848
Epoch 6/80: current_loss=0.02929 | best_loss=0.02848
Epoch 7/80: current_loss=0.02836 | best_loss=0.02836
Epoch 8/80: current_loss=0.02964 | best_loss=0.02836
Epoch 9/80: current_loss=0.02834 | best_loss=0.02834
Epoch 10/80: current_loss=0.02892 | best_loss=0.02834
Epoch 11/80: current_loss=0.02877 | best_loss=0.02834
Epoch 12/80: current_loss=0.02810 | best_loss=0.02810
Epoch 13/80: current_loss=0.02872 | best_loss=0.02810
Epoch 14/80: current_loss=0.02820 | best_loss=0.02810
Epoch 15/80: current_loss=0.03052 | best_loss=0.02810
Epoch 16/80: current_loss=0.03424 | best_loss=0.02810
Epoch 17/80: current_loss=0.03098 | best_loss=0.02810
Epoch 18/80: current_loss=0.02861 | best_loss=0.02810
Epoch 19/80: current_loss=0.03067 | best_loss=0.02810
Epoch 20/80: current_loss=0.02930 | best_loss=0.02810
Epoch 21/80: current_loss=0.03007 | best_loss=0.02810
Epoch 22/80: current_loss=0.02960 | best_loss=0.02810
Epoch 23/80: current_loss=0.02856 | best_loss=0.02810
Epoch 24/80: current_loss=0.02827 | best_loss=0.02810
Epoch 25/80: current_loss=0.03240 | best_loss=0.02810
Epoch 26/80: current_loss=0.02866 | best_loss=0.02810
Epoch 27/80: current_loss=0.02912 | best_loss=0.02810
Epoch 28/80: current_loss=0.02873 | best_loss=0.02810
Epoch 29/80: current_loss=0.02817 | best_loss=0.02810
Epoch 30/80: current_loss=0.02834 | best_loss=0.02810
Epoch 31/80: current_loss=0.02867 | best_loss=0.02810
Epoch 32/80: current_loss=0.02849 | best_loss=0.02810
Early Stopping at epoch 32
      explained_var=0.00024 | mse_loss=0.02849
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.04026 | best_loss=0.04026
Epoch 1/80: current_loss=0.03357 | best_loss=0.03357
Epoch 2/80: current_loss=0.03360 | best_loss=0.03357
Epoch 3/80: current_loss=0.03290 | best_loss=0.03290
Epoch 4/80: current_loss=0.03285 | best_loss=0.03285
Epoch 5/80: current_loss=0.03277 | best_loss=0.03277
Epoch 6/80: current_loss=0.03402 | best_loss=0.03277
Epoch 7/80: current_loss=0.03367 | best_loss=0.03277
Epoch 8/80: current_loss=0.03414 | best_loss=0.03277
Epoch 9/80: current_loss=0.03450 | best_loss=0.03277
Epoch 10/80: current_loss=0.03398 | best_loss=0.03277
Epoch 11/80: current_loss=0.03415 | best_loss=0.03277
Epoch 12/80: current_loss=0.03379 | best_loss=0.03277
Epoch 13/80: current_loss=0.03459 | best_loss=0.03277
Epoch 14/80: current_loss=0.03359 | best_loss=0.03277
Epoch 15/80: current_loss=0.03343 | best_loss=0.03277
Epoch 16/80: current_loss=0.03369 | best_loss=0.03277
Epoch 17/80: current_loss=0.03367 | best_loss=0.03277
Epoch 18/80: current_loss=0.03418 | best_loss=0.03277
Epoch 19/80: current_loss=0.03396 | best_loss=0.03277
Epoch 20/80: current_loss=0.03378 | best_loss=0.03277
Epoch 21/80: current_loss=0.03382 | best_loss=0.03277
Epoch 22/80: current_loss=0.03360 | best_loss=0.03277
Epoch 23/80: current_loss=0.03332 | best_loss=0.03277
Epoch 24/80: current_loss=0.03392 | best_loss=0.03277
Epoch 25/80: current_loss=0.03855 | best_loss=0.03277
Early Stopping at epoch 25
      explained_var=-0.03189 | mse_loss=0.03352
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.01798| avg_loss=0.02841
----------------------------------------------


----------------------------------------------
Params for Trial 97
{'learning_rate': 0.001, 'weight_decay': 0.0017919604728389603, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03038 | best_loss=0.03038
Epoch 1/80: current_loss=0.03071 | best_loss=0.03038
Epoch 2/80: current_loss=0.02850 | best_loss=0.02850
Epoch 3/80: current_loss=0.02758 | best_loss=0.02758
Epoch 4/80: current_loss=0.02726 | best_loss=0.02726
Epoch 5/80: current_loss=0.02607 | best_loss=0.02607
Epoch 6/80: current_loss=0.02595 | best_loss=0.02595
Epoch 7/80: current_loss=0.02674 | best_loss=0.02595
Epoch 8/80: current_loss=0.02401 | best_loss=0.02401
Epoch 9/80: current_loss=0.03038 | best_loss=0.02401
Epoch 10/80: current_loss=0.02913 | best_loss=0.02401
Epoch 11/80: current_loss=0.02599 | best_loss=0.02401
Epoch 12/80: current_loss=0.02630 | best_loss=0.02401
Epoch 13/80: current_loss=0.02541 | best_loss=0.02401
Epoch 14/80: current_loss=0.02506 | best_loss=0.02401
Epoch 15/80: current_loss=0.02517 | best_loss=0.02401
Epoch 16/80: current_loss=0.02591 | best_loss=0.02401
Epoch 17/80: current_loss=0.02623 | best_loss=0.02401
Epoch 18/80: current_loss=0.02621 | best_loss=0.02401
Epoch 19/80: current_loss=0.02973 | best_loss=0.02401
Epoch 20/80: current_loss=0.03364 | best_loss=0.02401
Epoch 21/80: current_loss=0.02872 | best_loss=0.02401
Epoch 22/80: current_loss=0.02637 | best_loss=0.02401
Epoch 23/80: current_loss=0.02836 | best_loss=0.02401
Epoch 24/80: current_loss=0.03043 | best_loss=0.02401
Epoch 25/80: current_loss=0.03627 | best_loss=0.02401
Epoch 26/80: current_loss=0.03102 | best_loss=0.02401
Epoch 27/80: current_loss=0.02900 | best_loss=0.02401
Epoch 28/80: current_loss=0.02564 | best_loss=0.02401
Early Stopping at epoch 28
      explained_var=0.08581 | mse_loss=0.02360
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02839 | best_loss=0.02839
Epoch 1/80: current_loss=0.02968 | best_loss=0.02839
Epoch 2/80: current_loss=0.03071 | best_loss=0.02839
Epoch 3/80: current_loss=0.03037 | best_loss=0.02839
Epoch 4/80: current_loss=0.03085 | best_loss=0.02839
Epoch 5/80: current_loss=0.02830 | best_loss=0.02830
Epoch 6/80: current_loss=0.03009 | best_loss=0.02830
Epoch 7/80: current_loss=0.02822 | best_loss=0.02822
Epoch 8/80: current_loss=0.03050 | best_loss=0.02822
Epoch 9/80: current_loss=0.02820 | best_loss=0.02820
Epoch 10/80: current_loss=0.02950 | best_loss=0.02820
Epoch 11/80: current_loss=0.03113 | best_loss=0.02820
Epoch 12/80: current_loss=0.03199 | best_loss=0.02820
Epoch 13/80: current_loss=0.02937 | best_loss=0.02820
Epoch 14/80: current_loss=0.02787 | best_loss=0.02787
Epoch 15/80: current_loss=0.02865 | best_loss=0.02787
Epoch 16/80: current_loss=0.03787 | best_loss=0.02787
Epoch 17/80: current_loss=0.03471 | best_loss=0.02787
Epoch 18/80: current_loss=0.02943 | best_loss=0.02787
Epoch 19/80: current_loss=0.02808 | best_loss=0.02787
Epoch 20/80: current_loss=0.02827 | best_loss=0.02787
Epoch 21/80: current_loss=0.02814 | best_loss=0.02787
Epoch 22/80: current_loss=0.02822 | best_loss=0.02787
Epoch 23/80: current_loss=0.02805 | best_loss=0.02787
Epoch 24/80: current_loss=0.03146 | best_loss=0.02787
Epoch 25/80: current_loss=0.02902 | best_loss=0.02787
Epoch 26/80: current_loss=0.02788 | best_loss=0.02787
Epoch 27/80: current_loss=0.02776 | best_loss=0.02776
Epoch 28/80: current_loss=0.02808 | best_loss=0.02776
Epoch 29/80: current_loss=0.02813 | best_loss=0.02776
Epoch 30/80: current_loss=0.02791 | best_loss=0.02776
Epoch 31/80: current_loss=0.02809 | best_loss=0.02776
Epoch 32/80: current_loss=0.02794 | best_loss=0.02776
Epoch 33/80: current_loss=0.02909 | best_loss=0.02776
Epoch 34/80: current_loss=0.02789 | best_loss=0.02776
Epoch 35/80: current_loss=0.02795 | best_loss=0.02776
Epoch 36/80: current_loss=0.02878 | best_loss=0.02776
Epoch 37/80: current_loss=0.02814 | best_loss=0.02776
Epoch 38/80: current_loss=0.02826 | best_loss=0.02776
Epoch 39/80: current_loss=0.02830 | best_loss=0.02776
Epoch 40/80: current_loss=0.03168 | best_loss=0.02776
Epoch 41/80: current_loss=0.03205 | best_loss=0.02776
Epoch 42/80: current_loss=0.03417 | best_loss=0.02776
Epoch 43/80: current_loss=0.03277 | best_loss=0.02776
Epoch 44/80: current_loss=0.03036 | best_loss=0.02776
Epoch 45/80: current_loss=0.02857 | best_loss=0.02776
Epoch 46/80: current_loss=0.02811 | best_loss=0.02776
Epoch 47/80: current_loss=0.02919 | best_loss=0.02776
Early Stopping at epoch 47
      explained_var=0.01997 | mse_loss=0.02724
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02952 | best_loss=0.02952
Epoch 1/80: current_loss=0.03014 | best_loss=0.02952
Epoch 2/80: current_loss=0.03010 | best_loss=0.02952
Epoch 3/80: current_loss=0.03161 | best_loss=0.02952
Epoch 4/80: current_loss=0.03242 | best_loss=0.02952
Epoch 5/80: current_loss=0.03181 | best_loss=0.02952
Epoch 6/80: current_loss=0.03467 | best_loss=0.02952
Epoch 7/80: current_loss=0.03145 | best_loss=0.02952
Epoch 8/80: current_loss=0.03047 | best_loss=0.02952
Epoch 9/80: current_loss=0.03301 | best_loss=0.02952
Epoch 10/80: current_loss=0.03255 | best_loss=0.02952
Epoch 11/80: current_loss=0.03226 | best_loss=0.02952
Epoch 12/80: current_loss=0.03104 | best_loss=0.02952
Epoch 13/80: current_loss=0.02951 | best_loss=0.02951
Epoch 14/80: current_loss=0.03557 | best_loss=0.02951
Epoch 15/80: current_loss=0.03006 | best_loss=0.02951
Epoch 16/80: current_loss=0.02982 | best_loss=0.02951
Epoch 17/80: current_loss=0.03136 | best_loss=0.02951
Epoch 18/80: current_loss=0.03299 | best_loss=0.02951
Epoch 19/80: current_loss=0.03127 | best_loss=0.02951
Epoch 20/80: current_loss=0.02976 | best_loss=0.02951
Epoch 21/80: current_loss=0.03188 | best_loss=0.02951
Epoch 22/80: current_loss=0.03006 | best_loss=0.02951
Epoch 23/80: current_loss=0.03387 | best_loss=0.02951
Epoch 24/80: current_loss=0.03750 | best_loss=0.02951
Epoch 25/80: current_loss=0.03768 | best_loss=0.02951
Epoch 26/80: current_loss=0.03265 | best_loss=0.02951
Epoch 27/80: current_loss=0.03351 | best_loss=0.02951
Epoch 28/80: current_loss=0.03203 | best_loss=0.02951
Epoch 29/80: current_loss=0.03170 | best_loss=0.02951
Epoch 30/80: current_loss=0.02951 | best_loss=0.02951
Epoch 31/80: current_loss=0.02982 | best_loss=0.02951
Epoch 32/80: current_loss=0.03187 | best_loss=0.02951
Epoch 33/80: current_loss=0.03124 | best_loss=0.02951
Epoch 34/80: current_loss=0.03304 | best_loss=0.02951
Epoch 35/80: current_loss=0.03088 | best_loss=0.02951
Epoch 36/80: current_loss=0.03138 | best_loss=0.02951
Epoch 37/80: current_loss=0.03187 | best_loss=0.02951
Epoch 38/80: current_loss=0.03506 | best_loss=0.02951
Epoch 39/80: current_loss=0.03050 | best_loss=0.02951
Epoch 40/80: current_loss=0.03036 | best_loss=0.02951
Epoch 41/80: current_loss=0.03039 | best_loss=0.02951
Epoch 42/80: current_loss=0.03099 | best_loss=0.02951
Epoch 43/80: current_loss=0.02964 | best_loss=0.02951
Epoch 44/80: current_loss=0.03349 | best_loss=0.02951
Epoch 45/80: current_loss=0.03342 | best_loss=0.02951
Epoch 46/80: current_loss=0.03424 | best_loss=0.02951
Epoch 47/80: current_loss=0.03143 | best_loss=0.02951
Epoch 48/80: current_loss=0.02998 | best_loss=0.02951
Epoch 49/80: current_loss=0.03069 | best_loss=0.02951
Epoch 50/80: current_loss=0.03162 | best_loss=0.02951
Early Stopping at epoch 50
      explained_var=0.02816 | mse_loss=0.02874
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02959 | best_loss=0.02959
Epoch 1/80: current_loss=0.03141 | best_loss=0.02959
Epoch 2/80: current_loss=0.03044 | best_loss=0.02959
Epoch 3/80: current_loss=0.02831 | best_loss=0.02831
Epoch 4/80: current_loss=0.02842 | best_loss=0.02831
Epoch 5/80: current_loss=0.02838 | best_loss=0.02831
Epoch 6/80: current_loss=0.02947 | best_loss=0.02831
Epoch 7/80: current_loss=0.02913 | best_loss=0.02831
Epoch 8/80: current_loss=0.02829 | best_loss=0.02829
Epoch 9/80: current_loss=0.02893 | best_loss=0.02829
Epoch 10/80: current_loss=0.03042 | best_loss=0.02829
Epoch 11/80: current_loss=0.02947 | best_loss=0.02829
Epoch 12/80: current_loss=0.02850 | best_loss=0.02829
Epoch 13/80: current_loss=0.02858 | best_loss=0.02829
Epoch 14/80: current_loss=0.03147 | best_loss=0.02829
Epoch 15/80: current_loss=0.02975 | best_loss=0.02829
Epoch 16/80: current_loss=0.03005 | best_loss=0.02829
Epoch 17/80: current_loss=0.02897 | best_loss=0.02829
Epoch 18/80: current_loss=0.02863 | best_loss=0.02829
Epoch 19/80: current_loss=0.03100 | best_loss=0.02829
Epoch 20/80: current_loss=0.02899 | best_loss=0.02829
Epoch 21/80: current_loss=0.02849 | best_loss=0.02829
Epoch 22/80: current_loss=0.02843 | best_loss=0.02829
Epoch 23/80: current_loss=0.02851 | best_loss=0.02829
Epoch 24/80: current_loss=0.02841 | best_loss=0.02829
Epoch 25/80: current_loss=0.03229 | best_loss=0.02829
Epoch 26/80: current_loss=0.03050 | best_loss=0.02829
Epoch 27/80: current_loss=0.02830 | best_loss=0.02829
Epoch 28/80: current_loss=0.02846 | best_loss=0.02829
Early Stopping at epoch 28
      explained_var=-0.01110 | mse_loss=0.02868
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03308 | best_loss=0.03308
Epoch 1/80: current_loss=0.03396 | best_loss=0.03308
Epoch 2/80: current_loss=0.04112 | best_loss=0.03308
Epoch 3/80: current_loss=0.03470 | best_loss=0.03308
Epoch 4/80: current_loss=0.03345 | best_loss=0.03308
Epoch 5/80: current_loss=0.03386 | best_loss=0.03308
Epoch 6/80: current_loss=0.03416 | best_loss=0.03308
Epoch 7/80: current_loss=0.03438 | best_loss=0.03308
Epoch 8/80: current_loss=0.03331 | best_loss=0.03308
Epoch 9/80: current_loss=0.03518 | best_loss=0.03308
Epoch 10/80: current_loss=0.03361 | best_loss=0.03308
Epoch 11/80: current_loss=0.03358 | best_loss=0.03308
Epoch 12/80: current_loss=0.03403 | best_loss=0.03308
Epoch 13/80: current_loss=0.03296 | best_loss=0.03296
Epoch 14/80: current_loss=0.03385 | best_loss=0.03296
Epoch 15/80: current_loss=0.03411 | best_loss=0.03296
Epoch 16/80: current_loss=0.03357 | best_loss=0.03296
Epoch 17/80: current_loss=0.03360 | best_loss=0.03296
Epoch 18/80: current_loss=0.03329 | best_loss=0.03296
Epoch 19/80: current_loss=0.03343 | best_loss=0.03296
Epoch 20/80: current_loss=0.03307 | best_loss=0.03296
Epoch 21/80: current_loss=0.03314 | best_loss=0.03296
Epoch 22/80: current_loss=0.03342 | best_loss=0.03296
Epoch 23/80: current_loss=0.03395 | best_loss=0.03296
Epoch 24/80: current_loss=0.03471 | best_loss=0.03296
Epoch 25/80: current_loss=0.03331 | best_loss=0.03296
Epoch 26/80: current_loss=0.03338 | best_loss=0.03296
Epoch 27/80: current_loss=0.03381 | best_loss=0.03296
Epoch 28/80: current_loss=0.03363 | best_loss=0.03296
Epoch 29/80: current_loss=0.03341 | best_loss=0.03296
Epoch 30/80: current_loss=0.03336 | best_loss=0.03296
Epoch 31/80: current_loss=0.03399 | best_loss=0.03296
Epoch 32/80: current_loss=0.03351 | best_loss=0.03296
Epoch 33/80: current_loss=0.03385 | best_loss=0.03296
Early Stopping at epoch 33
      explained_var=-0.03801 | mse_loss=0.03372
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.01697| avg_loss=0.02840
----------------------------------------------


----------------------------------------------
Params for Trial 98
{'learning_rate': 0.001, 'weight_decay': 0.0005310603270520943, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.02693 | best_loss=0.02693
Epoch 1/80: current_loss=0.02504 | best_loss=0.02504
Epoch 2/80: current_loss=0.04443 | best_loss=0.02504
Epoch 3/80: current_loss=0.03184 | best_loss=0.02504
Epoch 4/80: current_loss=0.02747 | best_loss=0.02504
Epoch 5/80: current_loss=0.02706 | best_loss=0.02504
Epoch 6/80: current_loss=0.02798 | best_loss=0.02504
Epoch 7/80: current_loss=0.02798 | best_loss=0.02504
Epoch 8/80: current_loss=0.02688 | best_loss=0.02504
Epoch 9/80: current_loss=0.02643 | best_loss=0.02504
Epoch 10/80: current_loss=0.02655 | best_loss=0.02504
Epoch 11/80: current_loss=0.02529 | best_loss=0.02504
Epoch 12/80: current_loss=0.02611 | best_loss=0.02504
Epoch 13/80: current_loss=0.02838 | best_loss=0.02504
Epoch 14/80: current_loss=0.02763 | best_loss=0.02504
Epoch 15/80: current_loss=0.02649 | best_loss=0.02504
Epoch 16/80: current_loss=0.02614 | best_loss=0.02504
Epoch 17/80: current_loss=0.02626 | best_loss=0.02504
Epoch 18/80: current_loss=0.02646 | best_loss=0.02504
Epoch 19/80: current_loss=0.02642 | best_loss=0.02504
Epoch 20/80: current_loss=0.02898 | best_loss=0.02504
Epoch 21/80: current_loss=0.02674 | best_loss=0.02504
Early Stopping at epoch 21
      explained_var=0.05059 | mse_loss=0.02453
Fold 1: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03201 | best_loss=0.03201
Epoch 1/80: current_loss=0.03060 | best_loss=0.03060
Epoch 2/80: current_loss=0.02837 | best_loss=0.02837
Epoch 3/80: current_loss=0.02746 | best_loss=0.02746
Epoch 4/80: current_loss=0.02816 | best_loss=0.02746
Epoch 5/80: current_loss=0.03065 | best_loss=0.02746
Epoch 6/80: current_loss=0.05177 | best_loss=0.02746
Epoch 7/80: current_loss=0.03395 | best_loss=0.02746
Epoch 8/80: current_loss=0.03082 | best_loss=0.02746
Epoch 9/80: current_loss=0.02763 | best_loss=0.02746
Epoch 10/80: current_loss=0.02779 | best_loss=0.02746
Epoch 11/80: current_loss=0.02816 | best_loss=0.02746
Epoch 12/80: current_loss=0.02814 | best_loss=0.02746
Epoch 13/80: current_loss=0.03133 | best_loss=0.02746
Epoch 14/80: current_loss=0.02895 | best_loss=0.02746
Epoch 15/80: current_loss=0.02820 | best_loss=0.02746
Epoch 16/80: current_loss=0.02983 | best_loss=0.02746
Epoch 17/80: current_loss=0.02767 | best_loss=0.02746
Epoch 18/80: current_loss=0.02920 | best_loss=0.02746
Epoch 19/80: current_loss=0.02785 | best_loss=0.02746
Epoch 20/80: current_loss=0.02763 | best_loss=0.02746
Epoch 21/80: current_loss=0.02782 | best_loss=0.02746
Epoch 22/80: current_loss=0.02930 | best_loss=0.02746
Epoch 23/80: current_loss=0.03071 | best_loss=0.02746
Early Stopping at epoch 23
      explained_var=0.02428 | mse_loss=0.02711
Fold 2: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03260 | best_loss=0.03260
Epoch 1/80: current_loss=0.03508 | best_loss=0.03260
Epoch 2/80: current_loss=0.02914 | best_loss=0.02914
Epoch 3/80: current_loss=0.02947 | best_loss=0.02914
Epoch 4/80: current_loss=0.02993 | best_loss=0.02914
Epoch 5/80: current_loss=0.02980 | best_loss=0.02914
Epoch 6/80: current_loss=0.03040 | best_loss=0.02914
Epoch 7/80: current_loss=0.02967 | best_loss=0.02914
Epoch 8/80: current_loss=0.02943 | best_loss=0.02914
Epoch 9/80: current_loss=0.02974 | best_loss=0.02914
Epoch 10/80: current_loss=0.02893 | best_loss=0.02893
Epoch 11/80: current_loss=0.03208 | best_loss=0.02893
Epoch 12/80: current_loss=0.03197 | best_loss=0.02893
Epoch 13/80: current_loss=0.03022 | best_loss=0.02893
Epoch 14/80: current_loss=0.03223 | best_loss=0.02893
Epoch 15/80: current_loss=0.04049 | best_loss=0.02893
Epoch 16/80: current_loss=0.03008 | best_loss=0.02893
Epoch 17/80: current_loss=0.03097 | best_loss=0.02893
Epoch 18/80: current_loss=0.02992 | best_loss=0.02893
Epoch 19/80: current_loss=0.02997 | best_loss=0.02893
Epoch 20/80: current_loss=0.02923 | best_loss=0.02893
Epoch 21/80: current_loss=0.03169 | best_loss=0.02893
Epoch 22/80: current_loss=0.03275 | best_loss=0.02893
Epoch 23/80: current_loss=0.02963 | best_loss=0.02893
Epoch 24/80: current_loss=0.03027 | best_loss=0.02893
Epoch 25/80: current_loss=0.03153 | best_loss=0.02893
Epoch 26/80: current_loss=0.02976 | best_loss=0.02893
Epoch 27/80: current_loss=0.02943 | best_loss=0.02893
Epoch 28/80: current_loss=0.02972 | best_loss=0.02893
Epoch 29/80: current_loss=0.03611 | best_loss=0.02893
Epoch 30/80: current_loss=0.03116 | best_loss=0.02893
Early Stopping at epoch 30
      explained_var=0.05096 | mse_loss=0.02822
Fold 3: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03052 | best_loss=0.03052
Epoch 1/80: current_loss=0.03179 | best_loss=0.03052
Epoch 2/80: current_loss=0.02868 | best_loss=0.02868
Epoch 3/80: current_loss=0.02901 | best_loss=0.02868
Epoch 4/80: current_loss=0.02836 | best_loss=0.02836
Epoch 5/80: current_loss=0.02857 | best_loss=0.02836
Epoch 6/80: current_loss=0.03358 | best_loss=0.02836
Epoch 7/80: current_loss=0.02857 | best_loss=0.02836
Epoch 8/80: current_loss=0.03006 | best_loss=0.02836
Epoch 9/80: current_loss=0.02852 | best_loss=0.02836
Epoch 10/80: current_loss=0.02878 | best_loss=0.02836
Epoch 11/80: current_loss=0.02926 | best_loss=0.02836
Epoch 12/80: current_loss=0.02946 | best_loss=0.02836
Epoch 13/80: current_loss=0.02931 | best_loss=0.02836
Epoch 14/80: current_loss=0.02870 | best_loss=0.02836
Epoch 15/80: current_loss=0.03117 | best_loss=0.02836
Epoch 16/80: current_loss=0.02822 | best_loss=0.02822
Epoch 17/80: current_loss=0.02835 | best_loss=0.02822
Epoch 18/80: current_loss=0.02918 | best_loss=0.02822
Epoch 19/80: current_loss=0.02902 | best_loss=0.02822
Epoch 20/80: current_loss=0.02834 | best_loss=0.02822
Epoch 21/80: current_loss=0.02830 | best_loss=0.02822
Epoch 22/80: current_loss=0.02835 | best_loss=0.02822
Epoch 23/80: current_loss=0.02910 | best_loss=0.02822
Epoch 24/80: current_loss=0.02895 | best_loss=0.02822
Epoch 25/80: current_loss=0.02877 | best_loss=0.02822
Epoch 26/80: current_loss=0.02826 | best_loss=0.02822
Epoch 27/80: current_loss=0.02963 | best_loss=0.02822
Epoch 28/80: current_loss=0.02916 | best_loss=0.02822
Epoch 29/80: current_loss=0.02838 | best_loss=0.02822
Epoch 30/80: current_loss=0.02899 | best_loss=0.02822
Epoch 31/80: current_loss=0.02837 | best_loss=0.02822
Epoch 32/80: current_loss=0.02856 | best_loss=0.02822
Epoch 33/80: current_loss=0.03101 | best_loss=0.02822
Epoch 34/80: current_loss=0.02834 | best_loss=0.02822
Epoch 35/80: current_loss=0.02799 | best_loss=0.02799
Epoch 36/80: current_loss=0.02879 | best_loss=0.02799
Epoch 37/80: current_loss=0.02824 | best_loss=0.02799
Epoch 38/80: current_loss=0.02804 | best_loss=0.02799
Epoch 39/80: current_loss=0.02849 | best_loss=0.02799
Epoch 40/80: current_loss=0.02881 | best_loss=0.02799
Epoch 41/80: current_loss=0.02938 | best_loss=0.02799
Epoch 42/80: current_loss=0.02889 | best_loss=0.02799
Epoch 43/80: current_loss=0.02863 | best_loss=0.02799
Epoch 44/80: current_loss=0.02891 | best_loss=0.02799
Epoch 45/80: current_loss=0.02819 | best_loss=0.02799
Epoch 46/80: current_loss=0.02856 | best_loss=0.02799
Epoch 47/80: current_loss=0.02887 | best_loss=0.02799
Epoch 48/80: current_loss=0.03147 | best_loss=0.02799
Epoch 49/80: current_loss=0.02786 | best_loss=0.02786
Epoch 50/80: current_loss=0.03906 | best_loss=0.02786
Epoch 51/80: current_loss=0.82154 | best_loss=0.02786
Epoch 52/80: current_loss=0.08106 | best_loss=0.02786
Epoch 53/80: current_loss=0.02999 | best_loss=0.02786
Epoch 54/80: current_loss=0.02814 | best_loss=0.02786
Epoch 55/80: current_loss=0.02807 | best_loss=0.02786
Epoch 56/80: current_loss=0.02812 | best_loss=0.02786
Epoch 57/80: current_loss=0.02837 | best_loss=0.02786
Epoch 58/80: current_loss=0.03325 | best_loss=0.02786
Epoch 59/80: current_loss=0.02948 | best_loss=0.02786
Epoch 60/80: current_loss=0.02819 | best_loss=0.02786
Epoch 61/80: current_loss=0.02877 | best_loss=0.02786
Epoch 62/80: current_loss=0.04298 | best_loss=0.02786
Epoch 63/80: current_loss=0.04058 | best_loss=0.02786
Epoch 64/80: current_loss=0.03227 | best_loss=0.02786
Epoch 65/80: current_loss=0.03806 | best_loss=0.02786
Epoch 66/80: current_loss=0.02967 | best_loss=0.02786
Epoch 67/80: current_loss=0.02909 | best_loss=0.02786
Epoch 68/80: current_loss=0.03011 | best_loss=0.02786
Epoch 69/80: current_loss=0.02899 | best_loss=0.02786
Early Stopping at epoch 69
      explained_var=0.00539 | mse_loss=0.02821
Fold 4: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.03333 | best_loss=0.03333
Epoch 1/80: current_loss=0.03240 | best_loss=0.03240
Epoch 2/80: current_loss=0.03598 | best_loss=0.03240
Epoch 3/80: current_loss=0.04112 | best_loss=0.03240
Epoch 4/80: current_loss=0.04930 | best_loss=0.03240
Epoch 5/80: current_loss=0.03953 | best_loss=0.03240
Epoch 6/80: current_loss=0.03199 | best_loss=0.03199
Epoch 7/80: current_loss=0.03322 | best_loss=0.03199
Epoch 8/80: current_loss=0.03843 | best_loss=0.03199
Epoch 9/80: current_loss=0.03698 | best_loss=0.03199
Epoch 10/80: current_loss=0.03754 | best_loss=0.03199
Epoch 11/80: current_loss=0.04373 | best_loss=0.03199
Epoch 12/80: current_loss=0.04431 | best_loss=0.03199
Epoch 13/80: current_loss=0.05393 | best_loss=0.03199
Epoch 14/80: current_loss=0.03204 | best_loss=0.03199
Epoch 15/80: current_loss=0.03681 | best_loss=0.03199
Epoch 16/80: current_loss=0.04448 | best_loss=0.03199
Epoch 17/80: current_loss=0.03293 | best_loss=0.03199
Epoch 18/80: current_loss=0.03693 | best_loss=0.03199
Epoch 19/80: current_loss=0.03441 | best_loss=0.03199
Epoch 20/80: current_loss=0.03456 | best_loss=0.03199
Epoch 21/80: current_loss=0.03384 | best_loss=0.03199
Epoch 22/80: current_loss=0.03482 | best_loss=0.03199
Epoch 23/80: current_loss=0.03581 | best_loss=0.03199
Epoch 24/80: current_loss=0.03465 | best_loss=0.03199
Epoch 25/80: current_loss=0.03586 | best_loss=0.03199
Epoch 26/80: current_loss=0.03304 | best_loss=0.03199
Early Stopping at epoch 26
      explained_var=-0.00621 | mse_loss=0.03270
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.02500| avg_loss=0.02815
----------------------------------------------


----------------------------------------------
Params for Trial 99
{'learning_rate': 1e-05, 'weight_decay': 0.0015308271125635997, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=360, num_val_ids=90
Epoch 0/80: current_loss=0.20369 | best_loss=0.20369
Epoch 1/80: current_loss=0.13371 | best_loss=0.13371
Epoch 2/80: current_loss=0.08110 | best_loss=0.08110
Epoch 3/80: current_loss=0.04720 | best_loss=0.04720
Epoch 4/80: current_loss=0.03500 | best_loss=0.03500
Epoch 5/80: current_loss=0.03713 | best_loss=0.03500
Epoch 6/80: current_loss=0.03676 | best_loss=0.03500
Epoch 7/80: current_loss=0.03482 | best_loss=0.03482
Epoch 8/80: current_loss=0.03435 | best_loss=0.03435
Epoch 9/80: current_loss=0.03452 | best_loss=0.03435
Epoch 10/80: current_loss=0.03447 | best_loss=0.03435
Epoch 11/80: current_loss=0.03382 | best_loss=0.03382
Epoch 12/80: current_loss=0.03341 | best_loss=0.03341
Epoch 13/80: current_loss=0.03353 | best_loss=0.03341
Epoch 14/80: current_loss=0.03291 | best_loss=0.03291
Epoch 15/80: current_loss=0.03310 | best_loss=0.03291
Epoch 16/80: current_loss=0.03313 | best_loss=0.03291
Epoch 17/80: current_loss=0.03243 | best_loss=0.03243
Epoch 18/80: current_loss=0.03177 | best_loss=0.03177
Epoch 19/80: current_loss=0.03195 | best_loss=0.03177
Epoch 20/80: current_loss=0.03222 | best_loss=0.03177
Epoch 21/80: current_loss=0.03168 | best_loss=0.03168
Epoch 22/80: current_loss=0.03111 | best_loss=0.03111
Epoch 23/80: current_loss=0.03108 | best_loss=0.03108
Epoch 24/80: current_loss=0.03084 | best_loss=0.03084
Epoch 25/80: current_loss=0.03010 | best_loss=0.03010
Epoch 26/80: current_loss=0.03040 | best_loss=0.03010
Epoch 27/80: current_loss=0.03073 | best_loss=0.03010
Epoch 28/80: current_loss=0.03056 | best_loss=0.03010
Epoch 29/80: current_loss=0.03021 | best_loss=0.03010
Epoch 30/80: current_loss=0.03038 | best_loss=0.03010
Epoch 31/80: current_loss=0.03017 | best_loss=0.03010
Epoch 32/80: current_loss=0.03002 | best_loss=0.03002
Epoch 33/80: current_loss=0.03049 | best_loss=0.03002
Epoch 34/80: current_loss=0.02960 | best_loss=0.02960
Epoch 35/80: current_loss=0.02936 | best_loss=0.02936
Epoch 36/80: current_loss=0.02985 | best_loss=0.02936
Epoch 37/80: current_loss=0.02963 | best_loss=0.02936
Epoch 38/80: current_loss=0.02959 | best_loss=0.02936
Epoch 39/80: current_loss=0.02908 | best_loss=0.02908
Epoch 40/80: current_loss=0.02887 | best_loss=0.02887
Epoch 41/80: current_loss=0.02900 | best_loss=0.02887
Epoch 42/80: current_loss=0.02901 | best_loss=0.02887
Epoch 43/80: current_loss=0.02896 | best_loss=0.02887
Epoch 44/80: current_loss=0.02870 | best_loss=0.02870
Epoch 45/80: current_loss=0.02844 | best_loss=0.02844
Epoch 46/80: current_loss=0.02870 | best_loss=0.02844
Epoch 47/80: current_loss=0.02870 | best_loss=0.02844
Epoch 48/80: current_loss=0.02899 | best_loss=0.02844
Epoch 49/80: current_loss=0.02896 | best_loss=0.02844
Epoch 50/80: current_loss=0.02860 | best_loss=0.02844
Epoch 51/80: current_loss=0.02828 | best_loss=0.02828
Epoch 52/80: current_loss=0.02832 | best_loss=0.02828
Epoch 53/80: current_loss=0.02830 | best_loss=0.02828
Epoch 54/80: current_loss=0.02802 | best_loss=0.02802
Epoch 55/80: current_loss=0.02820 | best_loss=0.02802
Epoch 56/80: current_loss=0.02787 | best_loss=0.02787
Epoch 57/80: current_loss=0.02787 | best_loss=0.02787
Epoch 58/80: current_loss=0.02823 | best_loss=0.02787
Epoch 59/80: current_loss=0.02837 | best_loss=0.02787
Epoch 60/80: current_loss=0.02774 | best_loss=0.02774
Epoch 61/80: current_loss=0.02755 | best_loss=0.02755
Epoch 62/80: current_loss=0.02748 | best_loss=0.02748
Epoch 63/80: current_loss=0.02914 | best_loss=0.02748
Epoch 64/80: current_loss=0.02851 | best_loss=0.02748
Epoch 65/80: current_loss=0.02781 | best_loss=0.02748
Epoch 66/80: current_loss=0.02768 | best_loss=0.02748
Epoch 67/80: current_loss=0.02762 | best_loss=0.02748
Epoch 68/80: current_loss=0.02736 | best_loss=0.02736
Epoch 69/80: current_loss=0.02747 | best_loss=0.02736
Epoch 70/80: current_loss=0.02793 | best_loss=0.02736
Epoch 71/80: current_loss=0.02765 | best_loss=0.02736
Epoch 72/80: current_loss=0.02813 | best_loss=0.02736
Epoch 73/80: current_loss=0.02776 | best_loss=0.02736
Epoch 74/80: current_loss=0.02752 | best_loss=0.02736
Epoch 75/80: current_loss=0.02712 | best_loss=0.02712
Epoch 76/80: current_loss=0.02735 | best_loss=0.02712
Epoch 77/80: current_loss=0.02759 | best_loss=0.02712
Epoch 78/80: current_loss=0.02798 | best_loss=0.02712
Epoch 79/80: current_loss=0.02713 | best_loss=0.02712
      explained_var=-0.01005 | mse_loss=0.02665
Optuna study finished, study statistics:
  Finished trials:  100
  Pruned trials:  20
  Completed trials:  80
  Best Trial:  91
  Value:  0.02787834571331548
  AVG stopping:  11
  Params: 
    learning_rate: 0.001
    weight_decay: 1.030629741702722e-05
    n_layers: 2
    hidden_size: 512
    dropout: 0.30000000000000004
----------------------------------------------

Check best params: {'learning_rate': 0.001, 'weight_decay': 1.030629741702722e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004, 'avg_epochs': 11}
--------------------------------------------------------------
Test CNN results: avg_loss=0.0330, avg_expvar=0.0101, avg_r2score=-0.1765, avg_mae=0.1508
--------------------------------------------------------------
