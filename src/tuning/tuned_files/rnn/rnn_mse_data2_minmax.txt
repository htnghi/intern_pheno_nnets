[I 2024-01-06 01:14:35,637] A new study created in memory with name: cnn_mseloss_data2_minmax
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:14:52,965] Trial 0 finished with value: 0.03543768059625677 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 0 with value: 0.03543768059625677.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:16:16,364] Trial 1 finished with value: 0.03537835388482055 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}. Best is trial 1 with value: 0.03537835388482055.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:18:48,166] Trial 2 finished with value: 0.03483302574838244 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:19:02,802] Trial 3 finished with value: 0.03493876340177719 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:19:35,852] Trial 4 finished with value: 0.03530993035098426 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:19:58,405] Trial 5 finished with value: 0.03498199144537663 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:20:37,317] Trial 6 finished with value: 0.034907734225393214 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:22:00,758] Trial 7 finished with value: 0.040862564384618195 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:22:38,476] Trial 8 finished with value: 0.03936272069236722 and parameters: {'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:25:16,163] Trial 9 finished with value: 0.0442035096232895 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:30:10,633] Trial 10 finished with value: 0.03619067615875378 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0030741327184301173, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:31:27,797] Trial 11 finished with value: 0.03491453296174042 and parameters: {'learning_rate': 0.0001, 'weight_decay': 9.241814952461722e-05, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:32:53,934] Trial 12 finished with value: 0.034863141769863285 and parameters: {'learning_rate': 0.0001, 'weight_decay': 5.3093270404624407e-05, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:34:39,087] Trial 13 finished with value: 0.035022313471680964 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0021571880692069795, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:35:53,800] Trial 14 finished with value: 0.034991374514898664 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0031673230575684688, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:37:51,417] Trial 15 finished with value: 0.03615161207551288 and parameters: {'learning_rate': 0.01, 'weight_decay': 3.113526657617861e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:39:51,154] Trial 16 finished with value: 0.03496973311211009 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0017102089384645458, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:41:49,257] Trial 17 finished with value: 0.03492662320417176 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.003309128484976863, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:43:46,564] Trial 18 finished with value: 0.03498631821618079 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0014650333661503008, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:46:13,579] Trial 19 finished with value: 0.03531045637623677 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0070228743096221145, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:47:36,363] Trial 20 finished with value: 0.03525993509391032 and parameters: {'learning_rate': 0.1, 'weight_decay': 0.003887858949015846, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:48:09,249] Trial 21 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:48:43,721] Trial 22 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:49:16,445] Trial 23 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:50:21,670] Trial 24 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:50:52,630] Trial 25 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:51:13,243] Trial 26 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:51:30,697] Trial 27 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:52:39,892] Trial 28 finished with value: 0.03493178240893509 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.004169508644681581, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:52:46,084] Trial 29 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:53:40,438] Trial 30 finished with value: 0.03514316565558192 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.002485867808970739, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:54:45,693] Trial 31 finished with value: 0.034932060039045185 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0001226870856730668, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:55:31,221] Trial 32 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:55:52,324] Trial 33 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:57:15,486] Trial 34 finished with value: 0.03488699143689001 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.000520514148969476, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:57:45,481] Trial 35 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 01:57:55,567] Trial 36 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:00:43,767] Trial 37 finished with value: 0.03492748890259949 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0013148507628590455, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:01:45,120] Trial 38 finished with value: 0.03484659402053299 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0005587848003536554, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:02:08,108] Trial 39 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:02:40,084] Trial 40 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:03:26,699] Trial 41 finished with value: 0.03487326717213308 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0005277571401622937, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:04:07,826] Trial 42 finished with value: 0.03491681047693683 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.000464566432343273, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:05:00,994] Trial 43 finished with value: 0.03494336095423307 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0012347884575060878, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:05:08,710] Trial 44 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:05:59,522] Trial 45 finished with value: 0.03498680807215121 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0026426647880227784, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.35}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:06:04,004] Trial 46 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:06:44,911] Trial 47 finished with value: 0.034863599384550505 and parameters: {'learning_rate': 0.001, 'weight_decay': 1.2594496663688958e-05, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:08:38,655] Trial 48 finished with value: 0.035160489180293644 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.007755358268918008, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.5}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:09:10,894] Trial 49 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:10:07,199] Trial 50 finished with value: 0.03496169039177412 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0019218232438353256, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:10:54,338] Trial 51 finished with value: 0.034875455507153985 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0004473741407538023, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:11:18,746] Trial 52 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:11:51,544] Trial 53 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:12:41,594] Trial 54 finished with value: 0.034968297212746635 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0008127037762370201, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:13:04,819] Trial 55 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:14:06,856] Trial 56 finished with value: 0.035121718960648925 and parameters: {'learning_rate': 0.01, 'weight_decay': 0.0017536502072936031, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:14:46,469] Trial 57 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:16:45,603] Trial 58 finished with value: 0.03547438151241409 and parameters: {'learning_rate': 0.001, 'weight_decay': 5.274299431281701e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:17:06,827] Trial 59 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:17:18,596] Trial 60 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:17:42,912] Trial 61 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:18:05,989] Trial 62 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:18:38,052] Trial 63 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:19:33,961] Trial 64 finished with value: 0.03496392171657452 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.000987597261822922, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:19:53,852] Trial 65 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:20:06,366] Trial 66 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:20:27,439] Trial 67 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:21:32,703] Trial 68 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:22:30,173] Trial 69 finished with value: 0.034874348421200775 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.00033641982680501174, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:23:11,537] Trial 70 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:24:25,672] Trial 71 finished with value: 0.03490299595072802 and parameters: {'learning_rate': 0.0001, 'weight_decay': 0.0003020300022401741, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:25:20,320] Trial 72 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:26:17,002] Trial 73 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:26:38,302] Trial 74 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:27:08,005] Trial 75 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:27:33,717] Trial 76 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:28:01,260] Trial 77 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:28:31,021] Trial 78 finished with value: 0.03493392308560311 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.00117282122489052, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:29:01,451] Trial 79 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:29:24,308] Trial 80 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:30:16,851] Trial 81 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:31:11,839] Trial 82 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:32:03,036] Trial 83 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:32:55,469] Trial 84 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:33:53,355] Trial 85 finished with value: 0.034970100223640645 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0019154039069742815, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:34:14,409] Trial 86 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:34:26,569] Trial 87 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:35:36,139] Trial 88 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:36:25,676] Trial 89 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:37:14,833] Trial 90 finished with value: 0.03491952373856548 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0014459985711409511, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:37:49,504] Trial 91 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:38:22,233] Trial 92 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:38:51,357] Trial 93 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:39:12,525] Trial 94 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:39:41,949] Trial 95 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:40:47,089] Trial 96 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:41:04,714] Trial 97 pruned. 
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:41:50,381] Trial 98 finished with value: 0.034916730613771 and parameters: {'learning_rate': 0.001, 'weight_decay': 0.0012666824502220821, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}. Best is trial 2 with value: 0.03483302574838244.
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
/home/ra56kop/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)
  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
[I 2024-01-06 02:42:25,688] Trial 99 pruned. 
GPU CUDA available, using GPU for training the models.
-----------------------------------------------
Input arguments: 
   + data_dir: /home/ra56kop/nnets_genomic_prediction/src
   + model: RNN
   + tuned: 1
   + minmax_scale: 1
   + standa_scale: 0
   + pca_fitting: 0
   + dataset: pheno_2
   + gpucuda: 2
   + data_variants: [1, 0, 0, 2]
-----------------------------------------------

---------------------------------------------------------
Tuning RNN with dataset pheno-2
---------------------------------------------------------


----------------------------------------------
Params for Trial 0
{'learning_rate': 0.0001, 'weight_decay': 0.0015599452877625745, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.27385 | best_loss=0.27385
Epoch 1/80: current_loss=0.21274 | best_loss=0.21274
Epoch 2/80: current_loss=0.15759 | best_loss=0.15759
Epoch 3/80: current_loss=0.11056 | best_loss=0.11056
Epoch 4/80: current_loss=0.07554 | best_loss=0.07554
Epoch 5/80: current_loss=0.05489 | best_loss=0.05489
Epoch 6/80: current_loss=0.04778 | best_loss=0.04778
Epoch 7/80: current_loss=0.04669 | best_loss=0.04669
Epoch 8/80: current_loss=0.04572 | best_loss=0.04572
Epoch 9/80: current_loss=0.04628 | best_loss=0.04572
Epoch 10/80: current_loss=0.04557 | best_loss=0.04557
Epoch 11/80: current_loss=0.04528 | best_loss=0.04528
Epoch 12/80: current_loss=0.04475 | best_loss=0.04475
Epoch 13/80: current_loss=0.04475 | best_loss=0.04475
Epoch 14/80: current_loss=0.04454 | best_loss=0.04454
Epoch 15/80: current_loss=0.04428 | best_loss=0.04428
Epoch 16/80: current_loss=0.04407 | best_loss=0.04407
Epoch 17/80: current_loss=0.04371 | best_loss=0.04371
Epoch 18/80: current_loss=0.04396 | best_loss=0.04371
Epoch 19/80: current_loss=0.04413 | best_loss=0.04371
Epoch 20/80: current_loss=0.04367 | best_loss=0.04367
Epoch 21/80: current_loss=0.04323 | best_loss=0.04323
Epoch 22/80: current_loss=0.04305 | best_loss=0.04305
Epoch 23/80: current_loss=0.04355 | best_loss=0.04305
Epoch 24/80: current_loss=0.04313 | best_loss=0.04305
Epoch 25/80: current_loss=0.04309 | best_loss=0.04305
Epoch 26/80: current_loss=0.04276 | best_loss=0.04276
Epoch 27/80: current_loss=0.04267 | best_loss=0.04267
Epoch 28/80: current_loss=0.04259 | best_loss=0.04259
Epoch 29/80: current_loss=0.04217 | best_loss=0.04217
Epoch 30/80: current_loss=0.04217 | best_loss=0.04217
Epoch 31/80: current_loss=0.04180 | best_loss=0.04180
Epoch 32/80: current_loss=0.04182 | best_loss=0.04180
Epoch 33/80: current_loss=0.04203 | best_loss=0.04180
Epoch 34/80: current_loss=0.04152 | best_loss=0.04152
Epoch 35/80: current_loss=0.04158 | best_loss=0.04152
Epoch 36/80: current_loss=0.04177 | best_loss=0.04152
Epoch 37/80: current_loss=0.04109 | best_loss=0.04109
Epoch 38/80: current_loss=0.04102 | best_loss=0.04102
Epoch 39/80: current_loss=0.04133 | best_loss=0.04102
Epoch 40/80: current_loss=0.04121 | best_loss=0.04102
Epoch 41/80: current_loss=0.04093 | best_loss=0.04093
Epoch 42/80: current_loss=0.04091 | best_loss=0.04091
Epoch 43/80: current_loss=0.04066 | best_loss=0.04066
Epoch 44/80: current_loss=0.04056 | best_loss=0.04056
Epoch 45/80: current_loss=0.04060 | best_loss=0.04056
Epoch 46/80: current_loss=0.04041 | best_loss=0.04041
Epoch 47/80: current_loss=0.04032 | best_loss=0.04032
Epoch 48/80: current_loss=0.04032 | best_loss=0.04032
Epoch 49/80: current_loss=0.04044 | best_loss=0.04032
Epoch 50/80: current_loss=0.04023 | best_loss=0.04023
Epoch 51/80: current_loss=0.04020 | best_loss=0.04020
Epoch 52/80: current_loss=0.04017 | best_loss=0.04017
Epoch 53/80: current_loss=0.04017 | best_loss=0.04017
Epoch 54/80: current_loss=0.04068 | best_loss=0.04017
Epoch 55/80: current_loss=0.04019 | best_loss=0.04017
Epoch 56/80: current_loss=0.03994 | best_loss=0.03994
Epoch 57/80: current_loss=0.03984 | best_loss=0.03984
Epoch 58/80: current_loss=0.03998 | best_loss=0.03984
Epoch 59/80: current_loss=0.04006 | best_loss=0.03984
Epoch 60/80: current_loss=0.03968 | best_loss=0.03968
Epoch 61/80: current_loss=0.03978 | best_loss=0.03968
Epoch 62/80: current_loss=0.03981 | best_loss=0.03968
Epoch 63/80: current_loss=0.04011 | best_loss=0.03968
Epoch 64/80: current_loss=0.03973 | best_loss=0.03968
Epoch 65/80: current_loss=0.03945 | best_loss=0.03945
Epoch 66/80: current_loss=0.03987 | best_loss=0.03945
Epoch 67/80: current_loss=0.03931 | best_loss=0.03931
Epoch 68/80: current_loss=0.03936 | best_loss=0.03931
Epoch 69/80: current_loss=0.03926 | best_loss=0.03926
Epoch 70/80: current_loss=0.03928 | best_loss=0.03926
Epoch 71/80: current_loss=0.03938 | best_loss=0.03926
Epoch 72/80: current_loss=0.03939 | best_loss=0.03926
Epoch 73/80: current_loss=0.03956 | best_loss=0.03926
Epoch 74/80: current_loss=0.03910 | best_loss=0.03910
Epoch 75/80: current_loss=0.03914 | best_loss=0.03910
Epoch 76/80: current_loss=0.03914 | best_loss=0.03910
Epoch 77/80: current_loss=0.03925 | best_loss=0.03910
Epoch 78/80: current_loss=0.03900 | best_loss=0.03900
Epoch 79/80: current_loss=0.03926 | best_loss=0.03900
      explained_var=-0.00924 | mse_loss=0.04016
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04288 | best_loss=0.04288
Epoch 1/80: current_loss=0.04258 | best_loss=0.04258
Epoch 2/80: current_loss=0.04250 | best_loss=0.04250
Epoch 3/80: current_loss=0.04254 | best_loss=0.04250
Epoch 4/80: current_loss=0.04223 | best_loss=0.04223
Epoch 5/80: current_loss=0.04259 | best_loss=0.04223
Epoch 6/80: current_loss=0.04260 | best_loss=0.04223
Epoch 7/80: current_loss=0.04222 | best_loss=0.04222
Epoch 8/80: current_loss=0.04275 | best_loss=0.04222
Epoch 9/80: current_loss=0.04258 | best_loss=0.04222
Epoch 10/80: current_loss=0.04238 | best_loss=0.04222
Epoch 11/80: current_loss=0.04205 | best_loss=0.04205
Epoch 12/80: current_loss=0.04229 | best_loss=0.04205
Epoch 13/80: current_loss=0.04251 | best_loss=0.04205
Epoch 14/80: current_loss=0.04250 | best_loss=0.04205
Epoch 15/80: current_loss=0.04227 | best_loss=0.04205
Epoch 16/80: current_loss=0.04203 | best_loss=0.04203
Epoch 17/80: current_loss=0.04233 | best_loss=0.04203
Epoch 18/80: current_loss=0.04238 | best_loss=0.04203
Epoch 19/80: current_loss=0.04238 | best_loss=0.04203
Epoch 20/80: current_loss=0.04219 | best_loss=0.04203
Epoch 21/80: current_loss=0.04212 | best_loss=0.04203
Epoch 22/80: current_loss=0.04199 | best_loss=0.04199
Epoch 23/80: current_loss=0.04211 | best_loss=0.04199
Epoch 24/80: current_loss=0.04252 | best_loss=0.04199
Epoch 25/80: current_loss=0.04208 | best_loss=0.04199
Epoch 26/80: current_loss=0.04198 | best_loss=0.04198
Epoch 27/80: current_loss=0.04216 | best_loss=0.04198
Epoch 28/80: current_loss=0.04237 | best_loss=0.04198
Epoch 29/80: current_loss=0.04223 | best_loss=0.04198
Epoch 30/80: current_loss=0.04230 | best_loss=0.04198
Epoch 31/80: current_loss=0.04216 | best_loss=0.04198
Epoch 32/80: current_loss=0.04184 | best_loss=0.04184
Epoch 33/80: current_loss=0.04184 | best_loss=0.04184
Epoch 34/80: current_loss=0.04224 | best_loss=0.04184
Epoch 35/80: current_loss=0.04212 | best_loss=0.04184
Epoch 36/80: current_loss=0.04188 | best_loss=0.04184
Epoch 37/80: current_loss=0.04197 | best_loss=0.04184
Epoch 38/80: current_loss=0.04200 | best_loss=0.04184
Epoch 39/80: current_loss=0.04213 | best_loss=0.04184
Epoch 40/80: current_loss=0.04229 | best_loss=0.04184
Epoch 41/80: current_loss=0.04205 | best_loss=0.04184
Epoch 42/80: current_loss=0.04203 | best_loss=0.04184
Epoch 43/80: current_loss=0.04187 | best_loss=0.04184
Epoch 44/80: current_loss=0.04204 | best_loss=0.04184
Epoch 45/80: current_loss=0.04229 | best_loss=0.04184
Epoch 46/80: current_loss=0.04185 | best_loss=0.04184
Epoch 47/80: current_loss=0.04192 | best_loss=0.04184
Epoch 48/80: current_loss=0.04209 | best_loss=0.04184
Epoch 49/80: current_loss=0.04195 | best_loss=0.04184
Epoch 50/80: current_loss=0.04205 | best_loss=0.04184
Epoch 51/80: current_loss=0.04217 | best_loss=0.04184
Epoch 52/80: current_loss=0.04217 | best_loss=0.04184
Early Stopping at epoch 52
      explained_var=0.02788 | mse_loss=0.04026
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02824 | best_loss=0.02824
Epoch 1/80: current_loss=0.02826 | best_loss=0.02824
Epoch 2/80: current_loss=0.02829 | best_loss=0.02824
Epoch 3/80: current_loss=0.02829 | best_loss=0.02824
Epoch 4/80: current_loss=0.02826 | best_loss=0.02824
Epoch 5/80: current_loss=0.02827 | best_loss=0.02824
Epoch 6/80: current_loss=0.02833 | best_loss=0.02824
Epoch 7/80: current_loss=0.02827 | best_loss=0.02824
Epoch 8/80: current_loss=0.02830 | best_loss=0.02824
Epoch 9/80: current_loss=0.02830 | best_loss=0.02824
Epoch 10/80: current_loss=0.02829 | best_loss=0.02824
Epoch 11/80: current_loss=0.02834 | best_loss=0.02824
Epoch 12/80: current_loss=0.02842 | best_loss=0.02824
Epoch 13/80: current_loss=0.02848 | best_loss=0.02824
Epoch 14/80: current_loss=0.02831 | best_loss=0.02824
Epoch 15/80: current_loss=0.02835 | best_loss=0.02824
Epoch 16/80: current_loss=0.02832 | best_loss=0.02824
Epoch 17/80: current_loss=0.02837 | best_loss=0.02824
Epoch 18/80: current_loss=0.02830 | best_loss=0.02824
Epoch 19/80: current_loss=0.02831 | best_loss=0.02824
Epoch 20/80: current_loss=0.02833 | best_loss=0.02824
Early Stopping at epoch 20
      explained_var=-0.00990 | mse_loss=0.02864
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03368 | best_loss=0.03368
Epoch 1/80: current_loss=0.03332 | best_loss=0.03332
Epoch 2/80: current_loss=0.03334 | best_loss=0.03332
Epoch 3/80: current_loss=0.03354 | best_loss=0.03332
Epoch 4/80: current_loss=0.03336 | best_loss=0.03332
Epoch 5/80: current_loss=0.03338 | best_loss=0.03332
Epoch 6/80: current_loss=0.03357 | best_loss=0.03332
Epoch 7/80: current_loss=0.03342 | best_loss=0.03332
Epoch 8/80: current_loss=0.03331 | best_loss=0.03331
Epoch 9/80: current_loss=0.03353 | best_loss=0.03331
Epoch 10/80: current_loss=0.03348 | best_loss=0.03331
Epoch 11/80: current_loss=0.03355 | best_loss=0.03331
Epoch 12/80: current_loss=0.03355 | best_loss=0.03331
Epoch 13/80: current_loss=0.03342 | best_loss=0.03331
Epoch 14/80: current_loss=0.03337 | best_loss=0.03331
Epoch 15/80: current_loss=0.03340 | best_loss=0.03331
Epoch 16/80: current_loss=0.03342 | best_loss=0.03331
Epoch 17/80: current_loss=0.03348 | best_loss=0.03331
Epoch 18/80: current_loss=0.03352 | best_loss=0.03331
Epoch 19/80: current_loss=0.03356 | best_loss=0.03331
Epoch 20/80: current_loss=0.03369 | best_loss=0.03331
Epoch 21/80: current_loss=0.03380 | best_loss=0.03331
Epoch 22/80: current_loss=0.03353 | best_loss=0.03331
Epoch 23/80: current_loss=0.03351 | best_loss=0.03331
Epoch 24/80: current_loss=0.03371 | best_loss=0.03331
Epoch 25/80: current_loss=0.03352 | best_loss=0.03331
Epoch 26/80: current_loss=0.03334 | best_loss=0.03331
Epoch 27/80: current_loss=0.03350 | best_loss=0.03331
Epoch 28/80: current_loss=0.03346 | best_loss=0.03331
Early Stopping at epoch 28
      explained_var=-0.01209 | mse_loss=0.03309
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03531 | best_loss=0.03531
Epoch 1/80: current_loss=0.03533 | best_loss=0.03531
Epoch 2/80: current_loss=0.03526 | best_loss=0.03526
Epoch 3/80: current_loss=0.03529 | best_loss=0.03526
Epoch 4/80: current_loss=0.03524 | best_loss=0.03524
Epoch 5/80: current_loss=0.03545 | best_loss=0.03524
Epoch 6/80: current_loss=0.03532 | best_loss=0.03524
Epoch 7/80: current_loss=0.03527 | best_loss=0.03524
Epoch 8/80: current_loss=0.03530 | best_loss=0.03524
Epoch 9/80: current_loss=0.03531 | best_loss=0.03524
Epoch 10/80: current_loss=0.03531 | best_loss=0.03524
Epoch 11/80: current_loss=0.03530 | best_loss=0.03524
Epoch 12/80: current_loss=0.03543 | best_loss=0.03524
Epoch 13/80: current_loss=0.03531 | best_loss=0.03524
Epoch 14/80: current_loss=0.03534 | best_loss=0.03524
Epoch 15/80: current_loss=0.03537 | best_loss=0.03524
Epoch 16/80: current_loss=0.03530 | best_loss=0.03524
Epoch 17/80: current_loss=0.03529 | best_loss=0.03524
Epoch 18/80: current_loss=0.03527 | best_loss=0.03524
Epoch 19/80: current_loss=0.03533 | best_loss=0.03524
Epoch 20/80: current_loss=0.03530 | best_loss=0.03524
Epoch 21/80: current_loss=0.03532 | best_loss=0.03524
Epoch 22/80: current_loss=0.03533 | best_loss=0.03524
Epoch 23/80: current_loss=0.03530 | best_loss=0.03524
Epoch 24/80: current_loss=0.03531 | best_loss=0.03524
Early Stopping at epoch 24
      explained_var=0.02373 | mse_loss=0.03504
----------------------------------------------
Average early_stopping_point: 24| avg_exp_var=0.00408| avg_loss=0.03544
----------------------------------------------


----------------------------------------------
Params for Trial 1
{'learning_rate': 1e-05, 'weight_decay': 0.005247564363846735, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.07487 | best_loss=0.07487
Epoch 1/80: current_loss=0.06751 | best_loss=0.06751
Epoch 2/80: current_loss=0.06140 | best_loss=0.06140
Epoch 3/80: current_loss=0.05624 | best_loss=0.05624
Epoch 4/80: current_loss=0.05206 | best_loss=0.05206
Epoch 5/80: current_loss=0.04894 | best_loss=0.04894
Epoch 6/80: current_loss=0.04667 | best_loss=0.04667
Epoch 7/80: current_loss=0.04503 | best_loss=0.04503
Epoch 8/80: current_loss=0.04379 | best_loss=0.04379
Epoch 9/80: current_loss=0.04283 | best_loss=0.04283
Epoch 10/80: current_loss=0.04211 | best_loss=0.04211
Epoch 11/80: current_loss=0.04163 | best_loss=0.04163
Epoch 12/80: current_loss=0.04127 | best_loss=0.04127
Epoch 13/80: current_loss=0.04101 | best_loss=0.04101
Epoch 14/80: current_loss=0.04091 | best_loss=0.04091
Epoch 15/80: current_loss=0.04078 | best_loss=0.04078
Epoch 16/80: current_loss=0.04071 | best_loss=0.04071
Epoch 17/80: current_loss=0.04063 | best_loss=0.04063
Epoch 18/80: current_loss=0.04054 | best_loss=0.04054
Epoch 19/80: current_loss=0.04049 | best_loss=0.04049
Epoch 20/80: current_loss=0.04045 | best_loss=0.04045
Epoch 21/80: current_loss=0.04043 | best_loss=0.04043
Epoch 22/80: current_loss=0.04036 | best_loss=0.04036
Epoch 23/80: current_loss=0.04028 | best_loss=0.04028
Epoch 24/80: current_loss=0.04023 | best_loss=0.04023
Epoch 25/80: current_loss=0.04021 | best_loss=0.04021
Epoch 26/80: current_loss=0.04018 | best_loss=0.04018
Epoch 27/80: current_loss=0.04016 | best_loss=0.04016
Epoch 28/80: current_loss=0.04017 | best_loss=0.04016
Epoch 29/80: current_loss=0.04014 | best_loss=0.04014
Epoch 30/80: current_loss=0.04010 | best_loss=0.04010
Epoch 31/80: current_loss=0.04007 | best_loss=0.04007
Epoch 32/80: current_loss=0.04000 | best_loss=0.04000
Epoch 33/80: current_loss=0.03994 | best_loss=0.03994
Epoch 34/80: current_loss=0.03991 | best_loss=0.03991
Epoch 35/80: current_loss=0.03987 | best_loss=0.03987
Epoch 36/80: current_loss=0.03981 | best_loss=0.03981
Epoch 37/80: current_loss=0.03984 | best_loss=0.03981
Epoch 38/80: current_loss=0.03981 | best_loss=0.03981
Epoch 39/80: current_loss=0.03972 | best_loss=0.03972
Epoch 40/80: current_loss=0.03968 | best_loss=0.03968
Epoch 41/80: current_loss=0.03965 | best_loss=0.03965
Epoch 42/80: current_loss=0.03964 | best_loss=0.03964
Epoch 43/80: current_loss=0.03961 | best_loss=0.03961
Epoch 44/80: current_loss=0.03960 | best_loss=0.03960
Epoch 45/80: current_loss=0.03958 | best_loss=0.03958
Epoch 46/80: current_loss=0.03953 | best_loss=0.03953
Epoch 47/80: current_loss=0.03954 | best_loss=0.03953
Epoch 48/80: current_loss=0.03949 | best_loss=0.03949
Epoch 49/80: current_loss=0.03946 | best_loss=0.03946
Epoch 50/80: current_loss=0.03943 | best_loss=0.03943
Epoch 51/80: current_loss=0.03941 | best_loss=0.03941
Epoch 52/80: current_loss=0.03937 | best_loss=0.03937
Epoch 53/80: current_loss=0.03934 | best_loss=0.03934
Epoch 54/80: current_loss=0.03931 | best_loss=0.03931
Epoch 55/80: current_loss=0.03932 | best_loss=0.03931
Epoch 56/80: current_loss=0.03930 | best_loss=0.03930
Epoch 57/80: current_loss=0.03925 | best_loss=0.03925
Epoch 58/80: current_loss=0.03927 | best_loss=0.03925
Epoch 59/80: current_loss=0.03926 | best_loss=0.03925
Epoch 60/80: current_loss=0.03926 | best_loss=0.03925
Epoch 61/80: current_loss=0.03919 | best_loss=0.03919
Epoch 62/80: current_loss=0.03915 | best_loss=0.03915
Epoch 63/80: current_loss=0.03912 | best_loss=0.03912
Epoch 64/80: current_loss=0.03911 | best_loss=0.03911
Epoch 65/80: current_loss=0.03909 | best_loss=0.03909
Epoch 66/80: current_loss=0.03904 | best_loss=0.03904
Epoch 67/80: current_loss=0.03906 | best_loss=0.03904
Epoch 68/80: current_loss=0.03903 | best_loss=0.03903
Epoch 69/80: current_loss=0.03899 | best_loss=0.03899
Epoch 70/80: current_loss=0.03901 | best_loss=0.03899
Epoch 71/80: current_loss=0.03901 | best_loss=0.03899
Epoch 72/80: current_loss=0.03900 | best_loss=0.03899
Epoch 73/80: current_loss=0.03894 | best_loss=0.03894
Epoch 74/80: current_loss=0.03892 | best_loss=0.03892
Epoch 75/80: current_loss=0.03893 | best_loss=0.03892
Epoch 76/80: current_loss=0.03893 | best_loss=0.03892
Epoch 77/80: current_loss=0.03890 | best_loss=0.03890
Epoch 78/80: current_loss=0.03889 | best_loss=0.03889
Epoch 79/80: current_loss=0.03888 | best_loss=0.03888
      explained_var=-0.00960 | mse_loss=0.03983
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04328 | best_loss=0.04328
Epoch 1/80: current_loss=0.04302 | best_loss=0.04302
Epoch 2/80: current_loss=0.04286 | best_loss=0.04286
Epoch 3/80: current_loss=0.04275 | best_loss=0.04275
Epoch 4/80: current_loss=0.04273 | best_loss=0.04273
Epoch 5/80: current_loss=0.04268 | best_loss=0.04268
Epoch 6/80: current_loss=0.04268 | best_loss=0.04268
Epoch 7/80: current_loss=0.04265 | best_loss=0.04265
Epoch 8/80: current_loss=0.04264 | best_loss=0.04264
Epoch 9/80: current_loss=0.04260 | best_loss=0.04260
Epoch 10/80: current_loss=0.04255 | best_loss=0.04255
Epoch 11/80: current_loss=0.04255 | best_loss=0.04255
Epoch 12/80: current_loss=0.04258 | best_loss=0.04255
Epoch 13/80: current_loss=0.04258 | best_loss=0.04255
Epoch 14/80: current_loss=0.04255 | best_loss=0.04255
Epoch 15/80: current_loss=0.04253 | best_loss=0.04253
Epoch 16/80: current_loss=0.04253 | best_loss=0.04253
Epoch 17/80: current_loss=0.04251 | best_loss=0.04251
Epoch 18/80: current_loss=0.04247 | best_loss=0.04247
Epoch 19/80: current_loss=0.04248 | best_loss=0.04247
Epoch 20/80: current_loss=0.04246 | best_loss=0.04246
Epoch 21/80: current_loss=0.04244 | best_loss=0.04244
Epoch 22/80: current_loss=0.04241 | best_loss=0.04241
Epoch 23/80: current_loss=0.04242 | best_loss=0.04241
Epoch 24/80: current_loss=0.04240 | best_loss=0.04240
Epoch 25/80: current_loss=0.04238 | best_loss=0.04238
Epoch 26/80: current_loss=0.04239 | best_loss=0.04238
Epoch 27/80: current_loss=0.04236 | best_loss=0.04236
Epoch 28/80: current_loss=0.04234 | best_loss=0.04234
Epoch 29/80: current_loss=0.04235 | best_loss=0.04234
Epoch 30/80: current_loss=0.04235 | best_loss=0.04234
Epoch 31/80: current_loss=0.04236 | best_loss=0.04234
Epoch 32/80: current_loss=0.04235 | best_loss=0.04234
Epoch 33/80: current_loss=0.04232 | best_loss=0.04232
Epoch 34/80: current_loss=0.04239 | best_loss=0.04232
Epoch 35/80: current_loss=0.04232 | best_loss=0.04232
Epoch 36/80: current_loss=0.04227 | best_loss=0.04227
Epoch 37/80: current_loss=0.04227 | best_loss=0.04227
Epoch 38/80: current_loss=0.04225 | best_loss=0.04225
Epoch 39/80: current_loss=0.04223 | best_loss=0.04223
Epoch 40/80: current_loss=0.04223 | best_loss=0.04223
Epoch 41/80: current_loss=0.04223 | best_loss=0.04223
Epoch 42/80: current_loss=0.04226 | best_loss=0.04223
Epoch 43/80: current_loss=0.04223 | best_loss=0.04223
Epoch 44/80: current_loss=0.04223 | best_loss=0.04223
Epoch 45/80: current_loss=0.04222 | best_loss=0.04222
Epoch 46/80: current_loss=0.04222 | best_loss=0.04222
Epoch 47/80: current_loss=0.04221 | best_loss=0.04221
Epoch 48/80: current_loss=0.04219 | best_loss=0.04219
Epoch 49/80: current_loss=0.04221 | best_loss=0.04219
Epoch 50/80: current_loss=0.04218 | best_loss=0.04218
Epoch 51/80: current_loss=0.04217 | best_loss=0.04217
Epoch 52/80: current_loss=0.04213 | best_loss=0.04213
Epoch 53/80: current_loss=0.04215 | best_loss=0.04213
Epoch 54/80: current_loss=0.04218 | best_loss=0.04213
Epoch 55/80: current_loss=0.04216 | best_loss=0.04213
Epoch 56/80: current_loss=0.04217 | best_loss=0.04213
Epoch 57/80: current_loss=0.04217 | best_loss=0.04213
Epoch 58/80: current_loss=0.04214 | best_loss=0.04213
Epoch 59/80: current_loss=0.04217 | best_loss=0.04213
Epoch 60/80: current_loss=0.04216 | best_loss=0.04213
Epoch 61/80: current_loss=0.04214 | best_loss=0.04213
Epoch 62/80: current_loss=0.04216 | best_loss=0.04213
Epoch 63/80: current_loss=0.04216 | best_loss=0.04213
Epoch 64/80: current_loss=0.04216 | best_loss=0.04213
Epoch 65/80: current_loss=0.04211 | best_loss=0.04211
Epoch 66/80: current_loss=0.04209 | best_loss=0.04209
Epoch 67/80: current_loss=0.04211 | best_loss=0.04209
Epoch 68/80: current_loss=0.04204 | best_loss=0.04204
Epoch 69/80: current_loss=0.04203 | best_loss=0.04203
Epoch 70/80: current_loss=0.04200 | best_loss=0.04200
Epoch 71/80: current_loss=0.04204 | best_loss=0.04200
Epoch 72/80: current_loss=0.04208 | best_loss=0.04200
Epoch 73/80: current_loss=0.04209 | best_loss=0.04200
Epoch 74/80: current_loss=0.04204 | best_loss=0.04200
Epoch 75/80: current_loss=0.04203 | best_loss=0.04200
Epoch 76/80: current_loss=0.04200 | best_loss=0.04200
Epoch 77/80: current_loss=0.04202 | best_loss=0.04200
Epoch 78/80: current_loss=0.04201 | best_loss=0.04200
Epoch 79/80: current_loss=0.04201 | best_loss=0.04200
      explained_var=0.02034 | mse_loss=0.04048
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02873 | best_loss=0.02873
Epoch 1/80: current_loss=0.02848 | best_loss=0.02848
Epoch 2/80: current_loss=0.02832 | best_loss=0.02832
Epoch 3/80: current_loss=0.02824 | best_loss=0.02824
Epoch 4/80: current_loss=0.02821 | best_loss=0.02821
Epoch 5/80: current_loss=0.02817 | best_loss=0.02817
Epoch 6/80: current_loss=0.02817 | best_loss=0.02817
Epoch 7/80: current_loss=0.02821 | best_loss=0.02817
Epoch 8/80: current_loss=0.02817 | best_loss=0.02817
Epoch 9/80: current_loss=0.02815 | best_loss=0.02815
Epoch 10/80: current_loss=0.02819 | best_loss=0.02815
Epoch 11/80: current_loss=0.02820 | best_loss=0.02815
Epoch 12/80: current_loss=0.02821 | best_loss=0.02815
Epoch 13/80: current_loss=0.02815 | best_loss=0.02815
Epoch 14/80: current_loss=0.02816 | best_loss=0.02815
Epoch 15/80: current_loss=0.02816 | best_loss=0.02815
Epoch 16/80: current_loss=0.02821 | best_loss=0.02815
Epoch 17/80: current_loss=0.02822 | best_loss=0.02815
Epoch 18/80: current_loss=0.02824 | best_loss=0.02815
Epoch 19/80: current_loss=0.02822 | best_loss=0.02815
Epoch 20/80: current_loss=0.02821 | best_loss=0.02815
Epoch 21/80: current_loss=0.02820 | best_loss=0.02815
Epoch 22/80: current_loss=0.02818 | best_loss=0.02815
Epoch 23/80: current_loss=0.02815 | best_loss=0.02815
Epoch 24/80: current_loss=0.02814 | best_loss=0.02814
Epoch 25/80: current_loss=0.02820 | best_loss=0.02814
Epoch 26/80: current_loss=0.02821 | best_loss=0.02814
Epoch 27/80: current_loss=0.02819 | best_loss=0.02814
Epoch 28/80: current_loss=0.02814 | best_loss=0.02814
Epoch 29/80: current_loss=0.02814 | best_loss=0.02814
Epoch 30/80: current_loss=0.02819 | best_loss=0.02814
Epoch 31/80: current_loss=0.02821 | best_loss=0.02814
Epoch 32/80: current_loss=0.02817 | best_loss=0.02814
Epoch 33/80: current_loss=0.02818 | best_loss=0.02814
Epoch 34/80: current_loss=0.02814 | best_loss=0.02814
Epoch 35/80: current_loss=0.02812 | best_loss=0.02812
Epoch 36/80: current_loss=0.02814 | best_loss=0.02812
Epoch 37/80: current_loss=0.02811 | best_loss=0.02811
Epoch 38/80: current_loss=0.02814 | best_loss=0.02811
Epoch 39/80: current_loss=0.02815 | best_loss=0.02811
Epoch 40/80: current_loss=0.02812 | best_loss=0.02811
Epoch 41/80: current_loss=0.02813 | best_loss=0.02811
Epoch 42/80: current_loss=0.02813 | best_loss=0.02811
Epoch 43/80: current_loss=0.02816 | best_loss=0.02811
Epoch 44/80: current_loss=0.02815 | best_loss=0.02811
Epoch 45/80: current_loss=0.02815 | best_loss=0.02811
Epoch 46/80: current_loss=0.02814 | best_loss=0.02811
Epoch 47/80: current_loss=0.02818 | best_loss=0.02811
Epoch 48/80: current_loss=0.02814 | best_loss=0.02811
Epoch 49/80: current_loss=0.02817 | best_loss=0.02811
Epoch 50/80: current_loss=0.02816 | best_loss=0.02811
Epoch 51/80: current_loss=0.02817 | best_loss=0.02811
Epoch 52/80: current_loss=0.02820 | best_loss=0.02811
Epoch 53/80: current_loss=0.02819 | best_loss=0.02811
Epoch 54/80: current_loss=0.02819 | best_loss=0.02811
Epoch 55/80: current_loss=0.02821 | best_loss=0.02811
Epoch 56/80: current_loss=0.02823 | best_loss=0.02811
Epoch 57/80: current_loss=0.02828 | best_loss=0.02811
Early Stopping at epoch 57
      explained_var=-0.00074 | mse_loss=0.02846
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03303 | best_loss=0.03303
Epoch 1/80: current_loss=0.03306 | best_loss=0.03303
Epoch 2/80: current_loss=0.03309 | best_loss=0.03303
Epoch 3/80: current_loss=0.03307 | best_loss=0.03303
Epoch 4/80: current_loss=0.03308 | best_loss=0.03303
Epoch 5/80: current_loss=0.03309 | best_loss=0.03303
Epoch 6/80: current_loss=0.03306 | best_loss=0.03303
Epoch 7/80: current_loss=0.03308 | best_loss=0.03303
Epoch 8/80: current_loss=0.03309 | best_loss=0.03303
Epoch 9/80: current_loss=0.03308 | best_loss=0.03303
Epoch 10/80: current_loss=0.03306 | best_loss=0.03303
Epoch 11/80: current_loss=0.03307 | best_loss=0.03303
Epoch 12/80: current_loss=0.03306 | best_loss=0.03303
Epoch 13/80: current_loss=0.03307 | best_loss=0.03303
Epoch 14/80: current_loss=0.03310 | best_loss=0.03303
Epoch 15/80: current_loss=0.03309 | best_loss=0.03303
Epoch 16/80: current_loss=0.03308 | best_loss=0.03303
Epoch 17/80: current_loss=0.03307 | best_loss=0.03303
Epoch 18/80: current_loss=0.03309 | best_loss=0.03303
Epoch 19/80: current_loss=0.03310 | best_loss=0.03303
Epoch 20/80: current_loss=0.03311 | best_loss=0.03303
Early Stopping at epoch 20
      explained_var=-0.00719 | mse_loss=0.03284
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03555 | best_loss=0.03555
Epoch 1/80: current_loss=0.03555 | best_loss=0.03555
Epoch 2/80: current_loss=0.03555 | best_loss=0.03555
Epoch 3/80: current_loss=0.03555 | best_loss=0.03555
Epoch 4/80: current_loss=0.03554 | best_loss=0.03554
Epoch 5/80: current_loss=0.03553 | best_loss=0.03553
Epoch 6/80: current_loss=0.03553 | best_loss=0.03553
Epoch 7/80: current_loss=0.03553 | best_loss=0.03553
Epoch 8/80: current_loss=0.03553 | best_loss=0.03553
Epoch 9/80: current_loss=0.03552 | best_loss=0.03552
Epoch 10/80: current_loss=0.03552 | best_loss=0.03552
Epoch 11/80: current_loss=0.03552 | best_loss=0.03552
Epoch 12/80: current_loss=0.03553 | best_loss=0.03552
Epoch 13/80: current_loss=0.03553 | best_loss=0.03552
Epoch 14/80: current_loss=0.03552 | best_loss=0.03552
Epoch 15/80: current_loss=0.03553 | best_loss=0.03552
Epoch 16/80: current_loss=0.03552 | best_loss=0.03552
Epoch 17/80: current_loss=0.03552 | best_loss=0.03552
Epoch 18/80: current_loss=0.03552 | best_loss=0.03552
Epoch 19/80: current_loss=0.03553 | best_loss=0.03552
Epoch 20/80: current_loss=0.03553 | best_loss=0.03552
Epoch 21/80: current_loss=0.03553 | best_loss=0.03552
Epoch 22/80: current_loss=0.03552 | best_loss=0.03552
Epoch 23/80: current_loss=0.03552 | best_loss=0.03552
Epoch 24/80: current_loss=0.03552 | best_loss=0.03552
Epoch 25/80: current_loss=0.03552 | best_loss=0.03552
Epoch 26/80: current_loss=0.03552 | best_loss=0.03552
Epoch 27/80: current_loss=0.03552 | best_loss=0.03552
Epoch 28/80: current_loss=0.03552 | best_loss=0.03552
Epoch 29/80: current_loss=0.03551 | best_loss=0.03551
Epoch 30/80: current_loss=0.03552 | best_loss=0.03551
Epoch 31/80: current_loss=0.03551 | best_loss=0.03551
Epoch 32/80: current_loss=0.03551 | best_loss=0.03551
Epoch 33/80: current_loss=0.03551 | best_loss=0.03551
Epoch 34/80: current_loss=0.03551 | best_loss=0.03551
Epoch 35/80: current_loss=0.03552 | best_loss=0.03551
Epoch 36/80: current_loss=0.03551 | best_loss=0.03551
Epoch 37/80: current_loss=0.03551 | best_loss=0.03551
Epoch 38/80: current_loss=0.03551 | best_loss=0.03551
Epoch 39/80: current_loss=0.03551 | best_loss=0.03551
Epoch 40/80: current_loss=0.03551 | best_loss=0.03551
Epoch 41/80: current_loss=0.03551 | best_loss=0.03551
Epoch 42/80: current_loss=0.03551 | best_loss=0.03551
Epoch 43/80: current_loss=0.03551 | best_loss=0.03551
Epoch 44/80: current_loss=0.03551 | best_loss=0.03551
Epoch 45/80: current_loss=0.03550 | best_loss=0.03550
Epoch 46/80: current_loss=0.03550 | best_loss=0.03550
Epoch 47/80: current_loss=0.03550 | best_loss=0.03550
Epoch 48/80: current_loss=0.03551 | best_loss=0.03550
Epoch 49/80: current_loss=0.03551 | best_loss=0.03550
Epoch 50/80: current_loss=0.03551 | best_loss=0.03550
Epoch 51/80: current_loss=0.03551 | best_loss=0.03550
Epoch 52/80: current_loss=0.03551 | best_loss=0.03550
Epoch 53/80: current_loss=0.03551 | best_loss=0.03550
Epoch 54/80: current_loss=0.03551 | best_loss=0.03550
Epoch 55/80: current_loss=0.03551 | best_loss=0.03550
Epoch 56/80: current_loss=0.03551 | best_loss=0.03550
Epoch 57/80: current_loss=0.03551 | best_loss=0.03550
Epoch 58/80: current_loss=0.03551 | best_loss=0.03550
Epoch 59/80: current_loss=0.03550 | best_loss=0.03550
Epoch 60/80: current_loss=0.03550 | best_loss=0.03550
Epoch 61/80: current_loss=0.03550 | best_loss=0.03550
Epoch 62/80: current_loss=0.03550 | best_loss=0.03550
Epoch 63/80: current_loss=0.03550 | best_loss=0.03550
Epoch 64/80: current_loss=0.03550 | best_loss=0.03550
Epoch 65/80: current_loss=0.03550 | best_loss=0.03550
Epoch 66/80: current_loss=0.03550 | best_loss=0.03550
Epoch 67/80: current_loss=0.03550 | best_loss=0.03550
Epoch 68/80: current_loss=0.03550 | best_loss=0.03550
Epoch 69/80: current_loss=0.03550 | best_loss=0.03550
Epoch 70/80: current_loss=0.03550 | best_loss=0.03550
Epoch 71/80: current_loss=0.03550 | best_loss=0.03550
Epoch 72/80: current_loss=0.03550 | best_loss=0.03550
Epoch 73/80: current_loss=0.03550 | best_loss=0.03550
Epoch 74/80: current_loss=0.03550 | best_loss=0.03550
Epoch 75/80: current_loss=0.03550 | best_loss=0.03550
Epoch 76/80: current_loss=0.03550 | best_loss=0.03550
Epoch 77/80: current_loss=0.03550 | best_loss=0.03550
Epoch 78/80: current_loss=0.03550 | best_loss=0.03550
Epoch 79/80: current_loss=0.03550 | best_loss=0.03550
      explained_var=0.01662 | mse_loss=0.03529
----------------------------------------------
Average early_stopping_point: 55| avg_exp_var=0.00388| avg_loss=0.03538
----------------------------------------------


----------------------------------------------
Params for Trial 2
{'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04846 | best_loss=0.04846
Epoch 1/80: current_loss=0.04053 | best_loss=0.04053
Epoch 2/80: current_loss=0.03980 | best_loss=0.03980
Epoch 3/80: current_loss=0.03926 | best_loss=0.03926
Epoch 4/80: current_loss=0.03851 | best_loss=0.03851
Epoch 5/80: current_loss=0.03817 | best_loss=0.03817
Epoch 6/80: current_loss=0.03874 | best_loss=0.03817
Epoch 7/80: current_loss=0.03795 | best_loss=0.03795
Epoch 8/80: current_loss=0.03824 | best_loss=0.03795
Epoch 9/80: current_loss=0.03852 | best_loss=0.03795
Epoch 10/80: current_loss=0.03771 | best_loss=0.03771
Epoch 11/80: current_loss=0.03720 | best_loss=0.03720
Epoch 12/80: current_loss=0.03972 | best_loss=0.03720
Epoch 13/80: current_loss=0.03733 | best_loss=0.03720
Epoch 14/80: current_loss=0.03763 | best_loss=0.03720
Epoch 15/80: current_loss=0.03749 | best_loss=0.03720
Epoch 16/80: current_loss=0.03754 | best_loss=0.03720
Epoch 17/80: current_loss=0.03896 | best_loss=0.03720
Epoch 18/80: current_loss=0.03805 | best_loss=0.03720
Epoch 19/80: current_loss=0.03803 | best_loss=0.03720
Epoch 20/80: current_loss=0.03846 | best_loss=0.03720
Epoch 21/80: current_loss=0.03957 | best_loss=0.03720
Epoch 22/80: current_loss=0.03788 | best_loss=0.03720
Epoch 23/80: current_loss=0.03757 | best_loss=0.03720
Epoch 24/80: current_loss=0.03868 | best_loss=0.03720
Epoch 25/80: current_loss=0.03751 | best_loss=0.03720
Epoch 26/80: current_loss=0.03776 | best_loss=0.03720
Epoch 27/80: current_loss=0.03740 | best_loss=0.03720
Epoch 28/80: current_loss=0.03731 | best_loss=0.03720
Epoch 29/80: current_loss=0.03823 | best_loss=0.03720
Epoch 30/80: current_loss=0.03715 | best_loss=0.03715
Epoch 31/80: current_loss=0.03764 | best_loss=0.03715
Epoch 32/80: current_loss=0.03737 | best_loss=0.03715
Epoch 33/80: current_loss=0.03732 | best_loss=0.03715
Epoch 34/80: current_loss=0.03730 | best_loss=0.03715
Epoch 35/80: current_loss=0.03723 | best_loss=0.03715
Epoch 36/80: current_loss=0.03837 | best_loss=0.03715
Epoch 37/80: current_loss=0.03787 | best_loss=0.03715
Epoch 38/80: current_loss=0.03756 | best_loss=0.03715
Epoch 39/80: current_loss=0.03793 | best_loss=0.03715
Epoch 40/80: current_loss=0.03746 | best_loss=0.03715
Epoch 41/80: current_loss=0.03744 | best_loss=0.03715
Epoch 42/80: current_loss=0.03759 | best_loss=0.03715
Epoch 43/80: current_loss=0.03724 | best_loss=0.03715
Epoch 44/80: current_loss=0.03734 | best_loss=0.03715
Epoch 45/80: current_loss=0.03740 | best_loss=0.03715
Epoch 46/80: current_loss=0.03735 | best_loss=0.03715
Epoch 47/80: current_loss=0.03762 | best_loss=0.03715
Epoch 48/80: current_loss=0.03805 | best_loss=0.03715
Epoch 49/80: current_loss=0.03797 | best_loss=0.03715
Epoch 50/80: current_loss=0.03780 | best_loss=0.03715
Early Stopping at epoch 50
      explained_var=0.02965 | mse_loss=0.03798
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04118 | best_loss=0.04118
Epoch 1/80: current_loss=0.04220 | best_loss=0.04118
Epoch 2/80: current_loss=0.04158 | best_loss=0.04118
Epoch 3/80: current_loss=0.04601 | best_loss=0.04118
Epoch 4/80: current_loss=0.04150 | best_loss=0.04118
Epoch 5/80: current_loss=0.04106 | best_loss=0.04106
Epoch 6/80: current_loss=0.04124 | best_loss=0.04106
Epoch 7/80: current_loss=0.04140 | best_loss=0.04106
Epoch 8/80: current_loss=0.04131 | best_loss=0.04106
Epoch 9/80: current_loss=0.04157 | best_loss=0.04106
Epoch 10/80: current_loss=0.04122 | best_loss=0.04106
Epoch 11/80: current_loss=0.04172 | best_loss=0.04106
Epoch 12/80: current_loss=0.04182 | best_loss=0.04106
Epoch 13/80: current_loss=0.04257 | best_loss=0.04106
Epoch 14/80: current_loss=0.04133 | best_loss=0.04106
Epoch 15/80: current_loss=0.04104 | best_loss=0.04104
Epoch 16/80: current_loss=0.04097 | best_loss=0.04097
Epoch 17/80: current_loss=0.04117 | best_loss=0.04097
Epoch 18/80: current_loss=0.04105 | best_loss=0.04097
Epoch 19/80: current_loss=0.04125 | best_loss=0.04097
Epoch 20/80: current_loss=0.04127 | best_loss=0.04097
Epoch 21/80: current_loss=0.04109 | best_loss=0.04097
Epoch 22/80: current_loss=0.04124 | best_loss=0.04097
Epoch 23/80: current_loss=0.04127 | best_loss=0.04097
Epoch 24/80: current_loss=0.04226 | best_loss=0.04097
Epoch 25/80: current_loss=0.04188 | best_loss=0.04097
Epoch 26/80: current_loss=0.04146 | best_loss=0.04097
Epoch 27/80: current_loss=0.04190 | best_loss=0.04097
Epoch 28/80: current_loss=0.04213 | best_loss=0.04097
Epoch 29/80: current_loss=0.04370 | best_loss=0.04097
Epoch 30/80: current_loss=0.04117 | best_loss=0.04097
Epoch 31/80: current_loss=0.04207 | best_loss=0.04097
Epoch 32/80: current_loss=0.04160 | best_loss=0.04097
Epoch 33/80: current_loss=0.04169 | best_loss=0.04097
Epoch 34/80: current_loss=0.04183 | best_loss=0.04097
Epoch 35/80: current_loss=0.04129 | best_loss=0.04097
Epoch 36/80: current_loss=0.04140 | best_loss=0.04097
Early Stopping at epoch 36
      explained_var=0.04192 | mse_loss=0.03951
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03032 | best_loss=0.03032
Epoch 1/80: current_loss=0.02991 | best_loss=0.02991
Epoch 2/80: current_loss=0.02852 | best_loss=0.02852
Epoch 3/80: current_loss=0.02821 | best_loss=0.02821
Epoch 4/80: current_loss=0.02839 | best_loss=0.02821
Epoch 5/80: current_loss=0.02866 | best_loss=0.02821
Epoch 6/80: current_loss=0.02879 | best_loss=0.02821
Epoch 7/80: current_loss=0.02833 | best_loss=0.02821
Epoch 8/80: current_loss=0.03016 | best_loss=0.02821
Epoch 9/80: current_loss=0.02929 | best_loss=0.02821
Epoch 10/80: current_loss=0.02845 | best_loss=0.02821
Epoch 11/80: current_loss=0.02841 | best_loss=0.02821
Epoch 12/80: current_loss=0.03053 | best_loss=0.02821
Epoch 13/80: current_loss=0.02878 | best_loss=0.02821
Epoch 14/80: current_loss=0.02883 | best_loss=0.02821
Epoch 15/80: current_loss=0.03057 | best_loss=0.02821
Epoch 16/80: current_loss=0.02992 | best_loss=0.02821
Epoch 17/80: current_loss=0.02956 | best_loss=0.02821
Epoch 18/80: current_loss=0.02899 | best_loss=0.02821
Epoch 19/80: current_loss=0.02826 | best_loss=0.02821
Epoch 20/80: current_loss=0.02825 | best_loss=0.02821
Epoch 21/80: current_loss=0.02941 | best_loss=0.02821
Epoch 22/80: current_loss=0.02882 | best_loss=0.02821
Epoch 23/80: current_loss=0.02826 | best_loss=0.02821
Early Stopping at epoch 23
      explained_var=-0.01159 | mse_loss=0.02869
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03362 | best_loss=0.03362
Epoch 1/80: current_loss=0.03410 | best_loss=0.03362
Epoch 2/80: current_loss=0.03480 | best_loss=0.03362
Epoch 3/80: current_loss=0.03423 | best_loss=0.03362
Epoch 4/80: current_loss=0.03324 | best_loss=0.03324
Epoch 5/80: current_loss=0.03437 | best_loss=0.03324
Epoch 6/80: current_loss=0.03310 | best_loss=0.03310
Epoch 7/80: current_loss=0.03315 | best_loss=0.03310
Epoch 8/80: current_loss=0.03531 | best_loss=0.03310
Epoch 9/80: current_loss=0.03361 | best_loss=0.03310
Epoch 10/80: current_loss=0.03347 | best_loss=0.03310
Epoch 11/80: current_loss=0.03440 | best_loss=0.03310
Epoch 12/80: current_loss=0.03343 | best_loss=0.03310
Epoch 13/80: current_loss=0.03440 | best_loss=0.03310
Epoch 14/80: current_loss=0.03339 | best_loss=0.03310
Epoch 15/80: current_loss=0.03333 | best_loss=0.03310
Epoch 16/80: current_loss=0.03331 | best_loss=0.03310
Epoch 17/80: current_loss=0.03326 | best_loss=0.03310
Epoch 18/80: current_loss=0.03320 | best_loss=0.03310
Epoch 19/80: current_loss=0.03323 | best_loss=0.03310
Epoch 20/80: current_loss=0.03354 | best_loss=0.03310
Epoch 21/80: current_loss=0.03341 | best_loss=0.03310
Epoch 22/80: current_loss=0.03395 | best_loss=0.03310
Epoch 23/80: current_loss=0.03325 | best_loss=0.03310
Epoch 24/80: current_loss=0.03330 | best_loss=0.03310
Epoch 25/80: current_loss=0.03365 | best_loss=0.03310
Epoch 26/80: current_loss=0.03352 | best_loss=0.03310
Early Stopping at epoch 26
      explained_var=-0.01141 | mse_loss=0.03297
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03566 | best_loss=0.03566
Epoch 1/80: current_loss=0.03550 | best_loss=0.03550
Epoch 2/80: current_loss=0.03554 | best_loss=0.03550
Epoch 3/80: current_loss=0.03539 | best_loss=0.03539
Epoch 4/80: current_loss=0.03547 | best_loss=0.03539
Epoch 5/80: current_loss=0.03607 | best_loss=0.03539
Epoch 6/80: current_loss=0.03580 | best_loss=0.03539
Epoch 7/80: current_loss=0.03695 | best_loss=0.03539
Epoch 8/80: current_loss=0.03528 | best_loss=0.03528
Epoch 9/80: current_loss=0.03541 | best_loss=0.03528
Epoch 10/80: current_loss=0.03530 | best_loss=0.03528
Epoch 11/80: current_loss=0.03555 | best_loss=0.03528
Epoch 12/80: current_loss=0.03527 | best_loss=0.03527
Epoch 13/80: current_loss=0.03535 | best_loss=0.03527
Epoch 14/80: current_loss=0.03544 | best_loss=0.03527
Epoch 15/80: current_loss=0.03526 | best_loss=0.03526
Epoch 16/80: current_loss=0.03529 | best_loss=0.03526
Epoch 17/80: current_loss=0.03560 | best_loss=0.03526
Epoch 18/80: current_loss=0.03539 | best_loss=0.03526
Epoch 19/80: current_loss=0.03564 | best_loss=0.03526
Epoch 20/80: current_loss=0.03554 | best_loss=0.03526
Epoch 21/80: current_loss=0.03577 | best_loss=0.03526
Epoch 22/80: current_loss=0.03552 | best_loss=0.03526
Epoch 23/80: current_loss=0.03702 | best_loss=0.03526
Epoch 24/80: current_loss=0.03529 | best_loss=0.03526
Epoch 25/80: current_loss=0.03526 | best_loss=0.03526
Epoch 26/80: current_loss=0.03558 | best_loss=0.03526
Epoch 27/80: current_loss=0.03531 | best_loss=0.03526
Epoch 28/80: current_loss=0.03566 | best_loss=0.03526
Epoch 29/80: current_loss=0.03537 | best_loss=0.03526
Epoch 30/80: current_loss=0.03526 | best_loss=0.03526
Epoch 31/80: current_loss=0.03567 | best_loss=0.03526
Epoch 32/80: current_loss=0.03534 | best_loss=0.03526
Epoch 33/80: current_loss=0.03546 | best_loss=0.03526
Epoch 34/80: current_loss=0.03537 | best_loss=0.03526
Epoch 35/80: current_loss=0.03528 | best_loss=0.03526
Epoch 36/80: current_loss=0.03690 | best_loss=0.03526
Epoch 37/80: current_loss=0.03530 | best_loss=0.03526
Epoch 38/80: current_loss=0.03592 | best_loss=0.03526
Epoch 39/80: current_loss=0.03537 | best_loss=0.03526
Epoch 40/80: current_loss=0.03540 | best_loss=0.03526
Epoch 41/80: current_loss=0.03547 | best_loss=0.03526
Epoch 42/80: current_loss=0.03619 | best_loss=0.03526
Epoch 43/80: current_loss=0.03658 | best_loss=0.03526
Epoch 44/80: current_loss=0.03535 | best_loss=0.03526
Epoch 45/80: current_loss=0.03540 | best_loss=0.03526
Early Stopping at epoch 45
      explained_var=0.02474 | mse_loss=0.03501
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.01466| avg_loss=0.03483
----------------------------------------------


----------------------------------------------
Params for Trial 3
{'learning_rate': 0.001, 'weight_decay': 0.004951769151595011, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.09863 | best_loss=0.09863
Epoch 1/80: current_loss=0.04682 | best_loss=0.04682
Epoch 2/80: current_loss=0.04544 | best_loss=0.04544
Epoch 3/80: current_loss=0.04472 | best_loss=0.04472
Epoch 4/80: current_loss=0.04376 | best_loss=0.04376
Epoch 5/80: current_loss=0.04292 | best_loss=0.04292
Epoch 6/80: current_loss=0.04287 | best_loss=0.04287
Epoch 7/80: current_loss=0.04225 | best_loss=0.04225
Epoch 8/80: current_loss=0.04151 | best_loss=0.04151
Epoch 9/80: current_loss=0.04137 | best_loss=0.04137
Epoch 10/80: current_loss=0.04089 | best_loss=0.04089
Epoch 11/80: current_loss=0.04105 | best_loss=0.04089
Epoch 12/80: current_loss=0.04026 | best_loss=0.04026
Epoch 13/80: current_loss=0.03982 | best_loss=0.03982
Epoch 14/80: current_loss=0.04038 | best_loss=0.03982
Epoch 15/80: current_loss=0.03950 | best_loss=0.03950
Epoch 16/80: current_loss=0.03933 | best_loss=0.03933
Epoch 17/80: current_loss=0.03950 | best_loss=0.03933
Epoch 18/80: current_loss=0.03912 | best_loss=0.03912
Epoch 19/80: current_loss=0.03993 | best_loss=0.03912
Epoch 20/80: current_loss=0.04008 | best_loss=0.03912
Epoch 21/80: current_loss=0.03860 | best_loss=0.03860
Epoch 22/80: current_loss=0.03868 | best_loss=0.03860
Epoch 23/80: current_loss=0.03869 | best_loss=0.03860
Epoch 24/80: current_loss=0.03828 | best_loss=0.03828
Epoch 25/80: current_loss=0.03822 | best_loss=0.03822
Epoch 26/80: current_loss=0.03811 | best_loss=0.03811
Epoch 27/80: current_loss=0.03902 | best_loss=0.03811
Epoch 28/80: current_loss=0.03794 | best_loss=0.03794
Epoch 29/80: current_loss=0.03798 | best_loss=0.03794
Epoch 30/80: current_loss=0.03862 | best_loss=0.03794
Epoch 31/80: current_loss=0.03819 | best_loss=0.03794
Epoch 32/80: current_loss=0.03786 | best_loss=0.03786
Epoch 33/80: current_loss=0.03769 | best_loss=0.03769
Epoch 34/80: current_loss=0.03778 | best_loss=0.03769
Epoch 35/80: current_loss=0.03807 | best_loss=0.03769
Epoch 36/80: current_loss=0.03811 | best_loss=0.03769
Epoch 37/80: current_loss=0.03793 | best_loss=0.03769
Epoch 38/80: current_loss=0.03787 | best_loss=0.03769
Epoch 39/80: current_loss=0.03796 | best_loss=0.03769
Epoch 40/80: current_loss=0.03781 | best_loss=0.03769
Epoch 41/80: current_loss=0.03762 | best_loss=0.03762
Epoch 42/80: current_loss=0.03762 | best_loss=0.03762
Epoch 43/80: current_loss=0.03782 | best_loss=0.03762
Epoch 44/80: current_loss=0.03771 | best_loss=0.03762
Epoch 45/80: current_loss=0.03752 | best_loss=0.03752
Epoch 46/80: current_loss=0.03779 | best_loss=0.03752
Epoch 47/80: current_loss=0.03758 | best_loss=0.03752
Epoch 48/80: current_loss=0.03830 | best_loss=0.03752
Epoch 49/80: current_loss=0.03774 | best_loss=0.03752
Epoch 50/80: current_loss=0.03791 | best_loss=0.03752
Epoch 51/80: current_loss=0.03764 | best_loss=0.03752
Epoch 52/80: current_loss=0.03762 | best_loss=0.03752
Epoch 53/80: current_loss=0.03759 | best_loss=0.03752
Epoch 54/80: current_loss=0.03747 | best_loss=0.03747
Epoch 55/80: current_loss=0.03747 | best_loss=0.03747
Epoch 56/80: current_loss=0.03805 | best_loss=0.03747
Epoch 57/80: current_loss=0.03834 | best_loss=0.03747
Epoch 58/80: current_loss=0.03744 | best_loss=0.03744
Epoch 59/80: current_loss=0.03771 | best_loss=0.03744
Epoch 60/80: current_loss=0.03755 | best_loss=0.03744
Epoch 61/80: current_loss=0.03767 | best_loss=0.03744
Epoch 62/80: current_loss=0.03746 | best_loss=0.03744
Epoch 63/80: current_loss=0.03746 | best_loss=0.03744
Epoch 64/80: current_loss=0.03774 | best_loss=0.03744
Epoch 65/80: current_loss=0.03747 | best_loss=0.03744
Epoch 66/80: current_loss=0.03770 | best_loss=0.03744
Epoch 67/80: current_loss=0.03768 | best_loss=0.03744
Epoch 68/80: current_loss=0.03756 | best_loss=0.03744
Epoch 69/80: current_loss=0.03880 | best_loss=0.03744
Epoch 70/80: current_loss=0.03750 | best_loss=0.03744
Epoch 71/80: current_loss=0.03762 | best_loss=0.03744
Epoch 72/80: current_loss=0.03752 | best_loss=0.03744
Epoch 73/80: current_loss=0.03764 | best_loss=0.03744
Epoch 74/80: current_loss=0.03795 | best_loss=0.03744
Epoch 75/80: current_loss=0.03752 | best_loss=0.03744
Epoch 76/80: current_loss=0.03760 | best_loss=0.03744
Epoch 77/80: current_loss=0.03753 | best_loss=0.03744
Epoch 78/80: current_loss=0.03795 | best_loss=0.03744
Early Stopping at epoch 78
      explained_var=0.01979 | mse_loss=0.03833
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04118 | best_loss=0.04118
Epoch 1/80: current_loss=0.04192 | best_loss=0.04118
Epoch 2/80: current_loss=0.04119 | best_loss=0.04118
Epoch 3/80: current_loss=0.04141 | best_loss=0.04118
Epoch 4/80: current_loss=0.04132 | best_loss=0.04118
Epoch 5/80: current_loss=0.04135 | best_loss=0.04118
Epoch 6/80: current_loss=0.04129 | best_loss=0.04118
Epoch 7/80: current_loss=0.04153 | best_loss=0.04118
Epoch 8/80: current_loss=0.04118 | best_loss=0.04118
Epoch 9/80: current_loss=0.04115 | best_loss=0.04115
Epoch 10/80: current_loss=0.04114 | best_loss=0.04114
Epoch 11/80: current_loss=0.04165 | best_loss=0.04114
Epoch 12/80: current_loss=0.04123 | best_loss=0.04114
Epoch 13/80: current_loss=0.04116 | best_loss=0.04114
Epoch 14/80: current_loss=0.04142 | best_loss=0.04114
Epoch 15/80: current_loss=0.04129 | best_loss=0.04114
Epoch 16/80: current_loss=0.04132 | best_loss=0.04114
Epoch 17/80: current_loss=0.04246 | best_loss=0.04114
Epoch 18/80: current_loss=0.04136 | best_loss=0.04114
Epoch 19/80: current_loss=0.04156 | best_loss=0.04114
Epoch 20/80: current_loss=0.04179 | best_loss=0.04114
Epoch 21/80: current_loss=0.04122 | best_loss=0.04114
Epoch 22/80: current_loss=0.04279 | best_loss=0.04114
Epoch 23/80: current_loss=0.04151 | best_loss=0.04114
Epoch 24/80: current_loss=0.04155 | best_loss=0.04114
Epoch 25/80: current_loss=0.04142 | best_loss=0.04114
Epoch 26/80: current_loss=0.04133 | best_loss=0.04114
Epoch 27/80: current_loss=0.04287 | best_loss=0.04114
Epoch 28/80: current_loss=0.04135 | best_loss=0.04114
Epoch 29/80: current_loss=0.04244 | best_loss=0.04114
Epoch 30/80: current_loss=0.04143 | best_loss=0.04114
Early Stopping at epoch 30
      explained_var=0.03901 | mse_loss=0.03965
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02843 | best_loss=0.02843
Epoch 1/80: current_loss=0.02814 | best_loss=0.02814
Epoch 2/80: current_loss=0.02812 | best_loss=0.02812
Epoch 3/80: current_loss=0.02943 | best_loss=0.02812
Epoch 4/80: current_loss=0.02810 | best_loss=0.02810
Epoch 5/80: current_loss=0.02814 | best_loss=0.02810
Epoch 6/80: current_loss=0.02916 | best_loss=0.02810
Epoch 7/80: current_loss=0.02819 | best_loss=0.02810
Epoch 8/80: current_loss=0.02859 | best_loss=0.02810
Epoch 9/80: current_loss=0.02839 | best_loss=0.02810
Epoch 10/80: current_loss=0.02839 | best_loss=0.02810
Epoch 11/80: current_loss=0.02884 | best_loss=0.02810
Epoch 12/80: current_loss=0.02819 | best_loss=0.02810
Epoch 13/80: current_loss=0.02886 | best_loss=0.02810
Epoch 14/80: current_loss=0.02817 | best_loss=0.02810
Epoch 15/80: current_loss=0.02814 | best_loss=0.02810
Epoch 16/80: current_loss=0.02829 | best_loss=0.02810
Epoch 17/80: current_loss=0.02869 | best_loss=0.02810
Epoch 18/80: current_loss=0.02815 | best_loss=0.02810
Epoch 19/80: current_loss=0.02816 | best_loss=0.02810
Epoch 20/80: current_loss=0.02862 | best_loss=0.02810
Epoch 21/80: current_loss=0.02815 | best_loss=0.02810
Epoch 22/80: current_loss=0.02906 | best_loss=0.02810
Epoch 23/80: current_loss=0.02829 | best_loss=0.02810
Epoch 24/80: current_loss=0.02834 | best_loss=0.02810
Early Stopping at epoch 24
      explained_var=-0.00723 | mse_loss=0.02858
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03313 | best_loss=0.03313
Epoch 1/80: current_loss=0.03315 | best_loss=0.03313
Epoch 2/80: current_loss=0.03303 | best_loss=0.03303
Epoch 3/80: current_loss=0.03331 | best_loss=0.03303
Epoch 4/80: current_loss=0.03304 | best_loss=0.03303
Epoch 5/80: current_loss=0.03405 | best_loss=0.03303
Epoch 6/80: current_loss=0.03314 | best_loss=0.03303
Epoch 7/80: current_loss=0.03303 | best_loss=0.03303
Epoch 8/80: current_loss=0.03318 | best_loss=0.03303
Epoch 9/80: current_loss=0.03319 | best_loss=0.03303
Epoch 10/80: current_loss=0.03333 | best_loss=0.03303
Epoch 11/80: current_loss=0.03329 | best_loss=0.03303
Epoch 12/80: current_loss=0.03313 | best_loss=0.03303
Epoch 13/80: current_loss=0.03413 | best_loss=0.03303
Epoch 14/80: current_loss=0.03311 | best_loss=0.03303
Epoch 15/80: current_loss=0.03309 | best_loss=0.03303
Epoch 16/80: current_loss=0.03307 | best_loss=0.03303
Epoch 17/80: current_loss=0.03307 | best_loss=0.03303
Epoch 18/80: current_loss=0.03325 | best_loss=0.03303
Epoch 19/80: current_loss=0.03305 | best_loss=0.03303
Epoch 20/80: current_loss=0.03308 | best_loss=0.03303
Epoch 21/80: current_loss=0.03329 | best_loss=0.03303
Epoch 22/80: current_loss=0.03311 | best_loss=0.03303
Epoch 23/80: current_loss=0.03314 | best_loss=0.03303
Epoch 24/80: current_loss=0.03326 | best_loss=0.03303
Epoch 25/80: current_loss=0.03304 | best_loss=0.03303
Epoch 26/80: current_loss=0.03340 | best_loss=0.03303
Epoch 27/80: current_loss=0.03306 | best_loss=0.03303
Early Stopping at epoch 27
      explained_var=-0.00973 | mse_loss=0.03292
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03539 | best_loss=0.03539
Epoch 1/80: current_loss=0.03541 | best_loss=0.03539
Epoch 2/80: current_loss=0.03545 | best_loss=0.03539
Epoch 3/80: current_loss=0.03542 | best_loss=0.03539
Epoch 4/80: current_loss=0.03541 | best_loss=0.03539
Epoch 5/80: current_loss=0.03543 | best_loss=0.03539
Epoch 6/80: current_loss=0.03540 | best_loss=0.03539
Epoch 7/80: current_loss=0.03541 | best_loss=0.03539
Epoch 8/80: current_loss=0.03543 | best_loss=0.03539
Epoch 9/80: current_loss=0.03540 | best_loss=0.03539
Epoch 10/80: current_loss=0.03552 | best_loss=0.03539
Epoch 11/80: current_loss=0.03546 | best_loss=0.03539
Epoch 12/80: current_loss=0.03541 | best_loss=0.03539
Epoch 13/80: current_loss=0.03543 | best_loss=0.03539
Epoch 14/80: current_loss=0.03547 | best_loss=0.03539
Epoch 15/80: current_loss=0.03542 | best_loss=0.03539
Epoch 16/80: current_loss=0.03548 | best_loss=0.03539
Epoch 17/80: current_loss=0.03548 | best_loss=0.03539
Epoch 18/80: current_loss=0.03542 | best_loss=0.03539
Epoch 19/80: current_loss=0.03555 | best_loss=0.03539
Epoch 20/80: current_loss=0.03542 | best_loss=0.03539
Early Stopping at epoch 20
      explained_var=0.01920 | mse_loss=0.03521
----------------------------------------------
Average early_stopping_point: 15| avg_exp_var=0.01221| avg_loss=0.03494
----------------------------------------------


----------------------------------------------
Params for Trial 4
{'learning_rate': 0.001, 'weight_decay': 0.008948273514793754, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04352 | best_loss=0.04352
Epoch 1/80: current_loss=0.03944 | best_loss=0.03944
Epoch 2/80: current_loss=0.03903 | best_loss=0.03903
Epoch 3/80: current_loss=0.03916 | best_loss=0.03903
Epoch 4/80: current_loss=0.03842 | best_loss=0.03842
Epoch 5/80: current_loss=0.03825 | best_loss=0.03825
Epoch 6/80: current_loss=0.03847 | best_loss=0.03825
Epoch 7/80: current_loss=0.03877 | best_loss=0.03825
Epoch 8/80: current_loss=0.03827 | best_loss=0.03825
Epoch 9/80: current_loss=0.03800 | best_loss=0.03800
Epoch 10/80: current_loss=0.03835 | best_loss=0.03800
Epoch 11/80: current_loss=0.03792 | best_loss=0.03792
Epoch 12/80: current_loss=0.03792 | best_loss=0.03792
Epoch 13/80: current_loss=0.03797 | best_loss=0.03792
Epoch 14/80: current_loss=0.03884 | best_loss=0.03792
Epoch 15/80: current_loss=0.04007 | best_loss=0.03792
Epoch 16/80: current_loss=0.03793 | best_loss=0.03792
Epoch 17/80: current_loss=0.03847 | best_loss=0.03792
Epoch 18/80: current_loss=0.03801 | best_loss=0.03792
Epoch 19/80: current_loss=0.03802 | best_loss=0.03792
Epoch 20/80: current_loss=0.03801 | best_loss=0.03792
Epoch 21/80: current_loss=0.03803 | best_loss=0.03792
Epoch 22/80: current_loss=0.03800 | best_loss=0.03792
Epoch 23/80: current_loss=0.03814 | best_loss=0.03792
Epoch 24/80: current_loss=0.03798 | best_loss=0.03792
Epoch 25/80: current_loss=0.03825 | best_loss=0.03792
Epoch 26/80: current_loss=0.03921 | best_loss=0.03792
Epoch 27/80: current_loss=0.03801 | best_loss=0.03792
Epoch 28/80: current_loss=0.03804 | best_loss=0.03792
Epoch 29/80: current_loss=0.03835 | best_loss=0.03792
Epoch 30/80: current_loss=0.03811 | best_loss=0.03792
Epoch 31/80: current_loss=0.03846 | best_loss=0.03792
Early Stopping at epoch 31
      explained_var=0.00916 | mse_loss=0.03880
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04337 | best_loss=0.04337
Epoch 1/80: current_loss=0.04230 | best_loss=0.04230
Epoch 2/80: current_loss=0.04240 | best_loss=0.04230
Epoch 3/80: current_loss=0.04261 | best_loss=0.04230
Epoch 4/80: current_loss=0.04272 | best_loss=0.04230
Epoch 5/80: current_loss=0.04270 | best_loss=0.04230
Epoch 6/80: current_loss=0.04249 | best_loss=0.04230
Epoch 7/80: current_loss=0.04306 | best_loss=0.04230
Epoch 8/80: current_loss=0.04312 | best_loss=0.04230
Epoch 9/80: current_loss=0.04302 | best_loss=0.04230
Epoch 10/80: current_loss=0.04261 | best_loss=0.04230
Epoch 11/80: current_loss=0.04255 | best_loss=0.04230
Epoch 12/80: current_loss=0.04261 | best_loss=0.04230
Epoch 13/80: current_loss=0.04260 | best_loss=0.04230
Epoch 14/80: current_loss=0.04263 | best_loss=0.04230
Epoch 15/80: current_loss=0.04264 | best_loss=0.04230
Epoch 16/80: current_loss=0.04273 | best_loss=0.04230
Epoch 17/80: current_loss=0.04384 | best_loss=0.04230
Epoch 18/80: current_loss=0.04263 | best_loss=0.04230
Epoch 19/80: current_loss=0.04310 | best_loss=0.04230
Epoch 20/80: current_loss=0.04268 | best_loss=0.04230
Epoch 21/80: current_loss=0.04295 | best_loss=0.04230
Early Stopping at epoch 21
      explained_var=0.00902 | mse_loss=0.04086
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02824 | best_loss=0.02824
Epoch 1/80: current_loss=0.02894 | best_loss=0.02824
Epoch 2/80: current_loss=0.02824 | best_loss=0.02824
Epoch 3/80: current_loss=0.02808 | best_loss=0.02808
Epoch 4/80: current_loss=0.02809 | best_loss=0.02808
Epoch 5/80: current_loss=0.02835 | best_loss=0.02808
Epoch 6/80: current_loss=0.02828 | best_loss=0.02808
Epoch 7/80: current_loss=0.02797 | best_loss=0.02797
Epoch 8/80: current_loss=0.02845 | best_loss=0.02797
Epoch 9/80: current_loss=0.02816 | best_loss=0.02797
Epoch 10/80: current_loss=0.02818 | best_loss=0.02797
Epoch 11/80: current_loss=0.02814 | best_loss=0.02797
Epoch 12/80: current_loss=0.02827 | best_loss=0.02797
Epoch 13/80: current_loss=0.02840 | best_loss=0.02797
Epoch 14/80: current_loss=0.02811 | best_loss=0.02797
Epoch 15/80: current_loss=0.02808 | best_loss=0.02797
Epoch 16/80: current_loss=0.02868 | best_loss=0.02797
Epoch 17/80: current_loss=0.02804 | best_loss=0.02797
Epoch 18/80: current_loss=0.02845 | best_loss=0.02797
Epoch 19/80: current_loss=0.02817 | best_loss=0.02797
Epoch 20/80: current_loss=0.02822 | best_loss=0.02797
Epoch 21/80: current_loss=0.02809 | best_loss=0.02797
Epoch 22/80: current_loss=0.02800 | best_loss=0.02797
Epoch 23/80: current_loss=0.02827 | best_loss=0.02797
Epoch 24/80: current_loss=0.02820 | best_loss=0.02797
Epoch 25/80: current_loss=0.02810 | best_loss=0.02797
Epoch 26/80: current_loss=0.02822 | best_loss=0.02797
Epoch 27/80: current_loss=0.02824 | best_loss=0.02797
Early Stopping at epoch 27
      explained_var=0.00014 | mse_loss=0.02839
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03285 | best_loss=0.03285
Epoch 1/80: current_loss=0.03296 | best_loss=0.03285
Epoch 2/80: current_loss=0.03296 | best_loss=0.03285
Epoch 3/80: current_loss=0.03298 | best_loss=0.03285
Epoch 4/80: current_loss=0.03285 | best_loss=0.03285
Epoch 5/80: current_loss=0.03287 | best_loss=0.03285
Epoch 6/80: current_loss=0.03298 | best_loss=0.03285
Epoch 7/80: current_loss=0.03298 | best_loss=0.03285
Epoch 8/80: current_loss=0.03290 | best_loss=0.03285
Epoch 9/80: current_loss=0.03294 | best_loss=0.03285
Epoch 10/80: current_loss=0.03289 | best_loss=0.03285
Epoch 11/80: current_loss=0.03291 | best_loss=0.03285
Epoch 12/80: current_loss=0.03319 | best_loss=0.03285
Epoch 13/80: current_loss=0.03287 | best_loss=0.03285
Epoch 14/80: current_loss=0.03298 | best_loss=0.03285
Epoch 15/80: current_loss=0.03284 | best_loss=0.03284
Epoch 16/80: current_loss=0.03294 | best_loss=0.03284
Epoch 17/80: current_loss=0.03294 | best_loss=0.03284
Epoch 18/80: current_loss=0.03307 | best_loss=0.03284
Epoch 19/80: current_loss=0.03292 | best_loss=0.03284
Epoch 20/80: current_loss=0.03320 | best_loss=0.03284
Epoch 21/80: current_loss=0.03290 | best_loss=0.03284
Epoch 22/80: current_loss=0.03285 | best_loss=0.03284
Epoch 23/80: current_loss=0.03304 | best_loss=0.03284
Epoch 24/80: current_loss=0.03288 | best_loss=0.03284
Epoch 25/80: current_loss=0.03288 | best_loss=0.03284
Epoch 26/80: current_loss=0.03299 | best_loss=0.03284
Epoch 27/80: current_loss=0.03288 | best_loss=0.03284
Epoch 28/80: current_loss=0.03303 | best_loss=0.03284
Epoch 29/80: current_loss=0.03286 | best_loss=0.03284
Epoch 30/80: current_loss=0.03290 | best_loss=0.03284
Epoch 31/80: current_loss=0.03293 | best_loss=0.03284
Epoch 32/80: current_loss=0.03285 | best_loss=0.03284
Epoch 33/80: current_loss=0.03294 | best_loss=0.03284
Epoch 34/80: current_loss=0.03285 | best_loss=0.03284
Epoch 35/80: current_loss=0.03289 | best_loss=0.03284
Early Stopping at epoch 35
      explained_var=-0.00022 | mse_loss=0.03261
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03619 | best_loss=0.03619
Epoch 1/80: current_loss=0.03619 | best_loss=0.03619
Epoch 2/80: current_loss=0.03619 | best_loss=0.03619
Epoch 3/80: current_loss=0.03619 | best_loss=0.03619
Epoch 4/80: current_loss=0.03619 | best_loss=0.03619
Epoch 5/80: current_loss=0.03620 | best_loss=0.03619
Epoch 6/80: current_loss=0.03619 | best_loss=0.03619
Epoch 7/80: current_loss=0.03619 | best_loss=0.03619
Epoch 8/80: current_loss=0.03622 | best_loss=0.03619
Epoch 9/80: current_loss=0.03619 | best_loss=0.03619
Epoch 10/80: current_loss=0.03620 | best_loss=0.03619
Epoch 11/80: current_loss=0.03619 | best_loss=0.03619
Epoch 12/80: current_loss=0.03619 | best_loss=0.03619
Epoch 13/80: current_loss=0.03619 | best_loss=0.03619
Epoch 14/80: current_loss=0.03619 | best_loss=0.03619
Epoch 15/80: current_loss=0.03621 | best_loss=0.03619
Epoch 16/80: current_loss=0.03619 | best_loss=0.03619
Epoch 17/80: current_loss=0.03619 | best_loss=0.03619
Epoch 18/80: current_loss=0.03626 | best_loss=0.03619
Epoch 19/80: current_loss=0.03619 | best_loss=0.03619
Epoch 20/80: current_loss=0.03619 | best_loss=0.03619
Epoch 21/80: current_loss=0.03619 | best_loss=0.03619
Epoch 22/80: current_loss=0.03619 | best_loss=0.03619
Epoch 23/80: current_loss=0.03619 | best_loss=0.03619
Epoch 24/80: current_loss=0.03620 | best_loss=0.03619
Epoch 25/80: current_loss=0.03620 | best_loss=0.03619
Epoch 26/80: current_loss=0.03622 | best_loss=0.03619
Epoch 27/80: current_loss=0.03619 | best_loss=0.03619
Epoch 28/80: current_loss=0.03619 | best_loss=0.03619
Epoch 29/80: current_loss=0.03623 | best_loss=0.03619
Epoch 30/80: current_loss=0.03619 | best_loss=0.03619
Epoch 31/80: current_loss=0.03619 | best_loss=0.03619
Early Stopping at epoch 31
      explained_var=0.00005 | mse_loss=0.03588
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.00363| avg_loss=0.03531
----------------------------------------------


----------------------------------------------
Params for Trial 5
{'learning_rate': 0.001, 'weight_decay': 0.005426960877312877, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04147 | best_loss=0.04147
Epoch 1/80: current_loss=0.03966 | best_loss=0.03966
Epoch 2/80: current_loss=0.03891 | best_loss=0.03891
Epoch 3/80: current_loss=0.03883 | best_loss=0.03883
Epoch 4/80: current_loss=0.03827 | best_loss=0.03827
Epoch 5/80: current_loss=0.03819 | best_loss=0.03819
Epoch 6/80: current_loss=0.03800 | best_loss=0.03800
Epoch 7/80: current_loss=0.03772 | best_loss=0.03772
Epoch 8/80: current_loss=0.03798 | best_loss=0.03772
Epoch 9/80: current_loss=0.03836 | best_loss=0.03772
Epoch 10/80: current_loss=0.03820 | best_loss=0.03772
Epoch 11/80: current_loss=0.03775 | best_loss=0.03772
Epoch 12/80: current_loss=0.03769 | best_loss=0.03769
Epoch 13/80: current_loss=0.03866 | best_loss=0.03769
Epoch 14/80: current_loss=0.03784 | best_loss=0.03769
Epoch 15/80: current_loss=0.03794 | best_loss=0.03769
Epoch 16/80: current_loss=0.03750 | best_loss=0.03750
Epoch 17/80: current_loss=0.03739 | best_loss=0.03739
Epoch 18/80: current_loss=0.03767 | best_loss=0.03739
Epoch 19/80: current_loss=0.03754 | best_loss=0.03739
Epoch 20/80: current_loss=0.03759 | best_loss=0.03739
Epoch 21/80: current_loss=0.03753 | best_loss=0.03739
Epoch 22/80: current_loss=0.03742 | best_loss=0.03739
Epoch 23/80: current_loss=0.03756 | best_loss=0.03739
Epoch 24/80: current_loss=0.03821 | best_loss=0.03739
Epoch 25/80: current_loss=0.03755 | best_loss=0.03739
Epoch 26/80: current_loss=0.03851 | best_loss=0.03739
Epoch 27/80: current_loss=0.04164 | best_loss=0.03739
Epoch 28/80: current_loss=0.03826 | best_loss=0.03739
Epoch 29/80: current_loss=0.03852 | best_loss=0.03739
Epoch 30/80: current_loss=0.03751 | best_loss=0.03739
Epoch 31/80: current_loss=0.03784 | best_loss=0.03739
Epoch 32/80: current_loss=0.03748 | best_loss=0.03739
Epoch 33/80: current_loss=0.03774 | best_loss=0.03739
Epoch 34/80: current_loss=0.03780 | best_loss=0.03739
Epoch 35/80: current_loss=0.03791 | best_loss=0.03739
Epoch 36/80: current_loss=0.03843 | best_loss=0.03739
Epoch 37/80: current_loss=0.03936 | best_loss=0.03739
Early Stopping at epoch 37
      explained_var=0.02261 | mse_loss=0.03833
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04165 | best_loss=0.04165
Epoch 1/80: current_loss=0.04129 | best_loss=0.04129
Epoch 2/80: current_loss=0.04234 | best_loss=0.04129
Epoch 3/80: current_loss=0.04288 | best_loss=0.04129
Epoch 4/80: current_loss=0.04139 | best_loss=0.04129
Epoch 5/80: current_loss=0.04140 | best_loss=0.04129
Epoch 6/80: current_loss=0.04157 | best_loss=0.04129
Epoch 7/80: current_loss=0.04167 | best_loss=0.04129
Epoch 8/80: current_loss=0.04240 | best_loss=0.04129
Epoch 9/80: current_loss=0.04522 | best_loss=0.04129
Epoch 10/80: current_loss=0.04395 | best_loss=0.04129
Epoch 11/80: current_loss=0.04368 | best_loss=0.04129
Epoch 12/80: current_loss=0.04138 | best_loss=0.04129
Epoch 13/80: current_loss=0.04183 | best_loss=0.04129
Epoch 14/80: current_loss=0.04200 | best_loss=0.04129
Epoch 15/80: current_loss=0.04271 | best_loss=0.04129
Epoch 16/80: current_loss=0.04382 | best_loss=0.04129
Epoch 17/80: current_loss=0.04222 | best_loss=0.04129
Epoch 18/80: current_loss=0.04202 | best_loss=0.04129
Epoch 19/80: current_loss=0.04148 | best_loss=0.04129
Epoch 20/80: current_loss=0.04224 | best_loss=0.04129
Epoch 21/80: current_loss=0.04151 | best_loss=0.04129
Early Stopping at epoch 21
      explained_var=0.03530 | mse_loss=0.03980
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02854 | best_loss=0.02854
Epoch 1/80: current_loss=0.02857 | best_loss=0.02854
Epoch 2/80: current_loss=0.02860 | best_loss=0.02854
Epoch 3/80: current_loss=0.02880 | best_loss=0.02854
Epoch 4/80: current_loss=0.02955 | best_loss=0.02854
Epoch 5/80: current_loss=0.02825 | best_loss=0.02825
Epoch 6/80: current_loss=0.02843 | best_loss=0.02825
Epoch 7/80: current_loss=0.02917 | best_loss=0.02825
Epoch 8/80: current_loss=0.02935 | best_loss=0.02825
Epoch 9/80: current_loss=0.02967 | best_loss=0.02825
Epoch 10/80: current_loss=0.02856 | best_loss=0.02825
Epoch 11/80: current_loss=0.02987 | best_loss=0.02825
Epoch 12/80: current_loss=0.02809 | best_loss=0.02809
Epoch 13/80: current_loss=0.02909 | best_loss=0.02809
Epoch 14/80: current_loss=0.02841 | best_loss=0.02809
Epoch 15/80: current_loss=0.02852 | best_loss=0.02809
Epoch 16/80: current_loss=0.02848 | best_loss=0.02809
Epoch 17/80: current_loss=0.02815 | best_loss=0.02809
Epoch 18/80: current_loss=0.02850 | best_loss=0.02809
Epoch 19/80: current_loss=0.02860 | best_loss=0.02809
Epoch 20/80: current_loss=0.02966 | best_loss=0.02809
Epoch 21/80: current_loss=0.02822 | best_loss=0.02809
Epoch 22/80: current_loss=0.02822 | best_loss=0.02809
Epoch 23/80: current_loss=0.02954 | best_loss=0.02809
Epoch 24/80: current_loss=0.02818 | best_loss=0.02809
Epoch 25/80: current_loss=0.02912 | best_loss=0.02809
Epoch 26/80: current_loss=0.03162 | best_loss=0.02809
Epoch 27/80: current_loss=0.02828 | best_loss=0.02809
Epoch 28/80: current_loss=0.02876 | best_loss=0.02809
Epoch 29/80: current_loss=0.02859 | best_loss=0.02809
Epoch 30/80: current_loss=0.02826 | best_loss=0.02809
Epoch 31/80: current_loss=0.02818 | best_loss=0.02809
Epoch 32/80: current_loss=0.02811 | best_loss=0.02809
Early Stopping at epoch 32
      explained_var=-0.00529 | mse_loss=0.02852
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03314 | best_loss=0.03314
Epoch 1/80: current_loss=0.03321 | best_loss=0.03314
Epoch 2/80: current_loss=0.03370 | best_loss=0.03314
Epoch 3/80: current_loss=0.03320 | best_loss=0.03314
Epoch 4/80: current_loss=0.03360 | best_loss=0.03314
Epoch 5/80: current_loss=0.03336 | best_loss=0.03314
Epoch 6/80: current_loss=0.03356 | best_loss=0.03314
Epoch 7/80: current_loss=0.03347 | best_loss=0.03314
Epoch 8/80: current_loss=0.03408 | best_loss=0.03314
Epoch 9/80: current_loss=0.03338 | best_loss=0.03314
Epoch 10/80: current_loss=0.03320 | best_loss=0.03314
Epoch 11/80: current_loss=0.03369 | best_loss=0.03314
Epoch 12/80: current_loss=0.03350 | best_loss=0.03314
Epoch 13/80: current_loss=0.03325 | best_loss=0.03314
Epoch 14/80: current_loss=0.03322 | best_loss=0.03314
Epoch 15/80: current_loss=0.03326 | best_loss=0.03314
Epoch 16/80: current_loss=0.03318 | best_loss=0.03314
Epoch 17/80: current_loss=0.03371 | best_loss=0.03314
Epoch 18/80: current_loss=0.03330 | best_loss=0.03314
Epoch 19/80: current_loss=0.03353 | best_loss=0.03314
Epoch 20/80: current_loss=0.03319 | best_loss=0.03314
Early Stopping at epoch 20
      explained_var=-0.01413 | mse_loss=0.03306
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03532 | best_loss=0.03532
Epoch 1/80: current_loss=0.03555 | best_loss=0.03532
Epoch 2/80: current_loss=0.03584 | best_loss=0.03532
Epoch 3/80: current_loss=0.03561 | best_loss=0.03532
Epoch 4/80: current_loss=0.03539 | best_loss=0.03532
Epoch 5/80: current_loss=0.03533 | best_loss=0.03532
Epoch 6/80: current_loss=0.03595 | best_loss=0.03532
Epoch 7/80: current_loss=0.03544 | best_loss=0.03532
Epoch 8/80: current_loss=0.03549 | best_loss=0.03532
Epoch 9/80: current_loss=0.03554 | best_loss=0.03532
Epoch 10/80: current_loss=0.03536 | best_loss=0.03532
Epoch 11/80: current_loss=0.03543 | best_loss=0.03532
Epoch 12/80: current_loss=0.03549 | best_loss=0.03532
Epoch 13/80: current_loss=0.03609 | best_loss=0.03532
Epoch 14/80: current_loss=0.03543 | best_loss=0.03532
Epoch 15/80: current_loss=0.03554 | best_loss=0.03532
Epoch 16/80: current_loss=0.03543 | best_loss=0.03532
Epoch 17/80: current_loss=0.03669 | best_loss=0.03532
Epoch 18/80: current_loss=0.03592 | best_loss=0.03532
Epoch 19/80: current_loss=0.03574 | best_loss=0.03532
Epoch 20/80: current_loss=0.03567 | best_loss=0.03532
Early Stopping at epoch 20
      explained_var=0.01925 | mse_loss=0.03519
----------------------------------------------
Average early_stopping_point: 6| avg_exp_var=0.01155| avg_loss=0.03498
----------------------------------------------


----------------------------------------------
Params for Trial 6
{'learning_rate': 0.0001, 'weight_decay': 0.0007404466099364386, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04524 | best_loss=0.04524
Epoch 1/80: current_loss=0.04235 | best_loss=0.04235
Epoch 2/80: current_loss=0.04172 | best_loss=0.04172
Epoch 3/80: current_loss=0.04107 | best_loss=0.04107
Epoch 4/80: current_loss=0.04065 | best_loss=0.04065
Epoch 5/80: current_loss=0.04023 | best_loss=0.04023
Epoch 6/80: current_loss=0.03982 | best_loss=0.03982
Epoch 7/80: current_loss=0.03963 | best_loss=0.03963
Epoch 8/80: current_loss=0.03941 | best_loss=0.03941
Epoch 9/80: current_loss=0.03918 | best_loss=0.03918
Epoch 10/80: current_loss=0.03894 | best_loss=0.03894
Epoch 11/80: current_loss=0.03878 | best_loss=0.03878
Epoch 12/80: current_loss=0.03872 | best_loss=0.03872
Epoch 13/80: current_loss=0.03863 | best_loss=0.03863
Epoch 14/80: current_loss=0.03848 | best_loss=0.03848
Epoch 15/80: current_loss=0.03837 | best_loss=0.03837
Epoch 16/80: current_loss=0.03828 | best_loss=0.03828
Epoch 17/80: current_loss=0.03817 | best_loss=0.03817
Epoch 18/80: current_loss=0.03821 | best_loss=0.03817
Epoch 19/80: current_loss=0.03802 | best_loss=0.03802
Epoch 20/80: current_loss=0.03804 | best_loss=0.03802
Epoch 21/80: current_loss=0.03812 | best_loss=0.03802
Epoch 22/80: current_loss=0.03794 | best_loss=0.03794
Epoch 23/80: current_loss=0.03805 | best_loss=0.03794
Epoch 24/80: current_loss=0.03783 | best_loss=0.03783
Epoch 25/80: current_loss=0.03785 | best_loss=0.03783
Epoch 26/80: current_loss=0.03778 | best_loss=0.03778
Epoch 27/80: current_loss=0.03779 | best_loss=0.03778
Epoch 28/80: current_loss=0.03775 | best_loss=0.03775
Epoch 29/80: current_loss=0.03771 | best_loss=0.03771
Epoch 30/80: current_loss=0.03775 | best_loss=0.03771
Epoch 31/80: current_loss=0.03773 | best_loss=0.03771
Epoch 32/80: current_loss=0.03768 | best_loss=0.03768
Epoch 33/80: current_loss=0.03761 | best_loss=0.03761
Epoch 34/80: current_loss=0.03755 | best_loss=0.03755
Epoch 35/80: current_loss=0.03751 | best_loss=0.03751
Epoch 36/80: current_loss=0.03763 | best_loss=0.03751
Epoch 37/80: current_loss=0.03750 | best_loss=0.03750
Epoch 38/80: current_loss=0.03770 | best_loss=0.03750
Epoch 39/80: current_loss=0.03757 | best_loss=0.03750
Epoch 40/80: current_loss=0.03782 | best_loss=0.03750
Epoch 41/80: current_loss=0.03760 | best_loss=0.03750
Epoch 42/80: current_loss=0.03770 | best_loss=0.03750
Epoch 43/80: current_loss=0.03759 | best_loss=0.03750
Epoch 44/80: current_loss=0.03766 | best_loss=0.03750
Epoch 45/80: current_loss=0.03755 | best_loss=0.03750
Epoch 46/80: current_loss=0.03761 | best_loss=0.03750
Epoch 47/80: current_loss=0.03759 | best_loss=0.03750
Epoch 48/80: current_loss=0.03765 | best_loss=0.03750
Epoch 49/80: current_loss=0.03757 | best_loss=0.03750
Epoch 50/80: current_loss=0.03758 | best_loss=0.03750
Epoch 51/80: current_loss=0.03756 | best_loss=0.03750
Epoch 52/80: current_loss=0.03759 | best_loss=0.03750
Epoch 53/80: current_loss=0.03766 | best_loss=0.03750
Epoch 54/80: current_loss=0.03757 | best_loss=0.03750
Epoch 55/80: current_loss=0.03762 | best_loss=0.03750
Epoch 56/80: current_loss=0.03774 | best_loss=0.03750
Epoch 57/80: current_loss=0.03752 | best_loss=0.03750
Early Stopping at epoch 57
      explained_var=0.02028 | mse_loss=0.03852
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04103 | best_loss=0.04103
Epoch 1/80: current_loss=0.04118 | best_loss=0.04103
Epoch 2/80: current_loss=0.04112 | best_loss=0.04103
Epoch 3/80: current_loss=0.04110 | best_loss=0.04103
Epoch 4/80: current_loss=0.04113 | best_loss=0.04103
Epoch 5/80: current_loss=0.04127 | best_loss=0.04103
Epoch 6/80: current_loss=0.04124 | best_loss=0.04103
Epoch 7/80: current_loss=0.04125 | best_loss=0.04103
Epoch 8/80: current_loss=0.04117 | best_loss=0.04103
Epoch 9/80: current_loss=0.04145 | best_loss=0.04103
Epoch 10/80: current_loss=0.04127 | best_loss=0.04103
Epoch 11/80: current_loss=0.04119 | best_loss=0.04103
Epoch 12/80: current_loss=0.04121 | best_loss=0.04103
Epoch 13/80: current_loss=0.04149 | best_loss=0.04103
Epoch 14/80: current_loss=0.04131 | best_loss=0.04103
Epoch 15/80: current_loss=0.04146 | best_loss=0.04103
Epoch 16/80: current_loss=0.04136 | best_loss=0.04103
Epoch 17/80: current_loss=0.04137 | best_loss=0.04103
Epoch 18/80: current_loss=0.04152 | best_loss=0.04103
Epoch 19/80: current_loss=0.04148 | best_loss=0.04103
Epoch 20/80: current_loss=0.04158 | best_loss=0.04103
Early Stopping at epoch 20
      explained_var=0.04129 | mse_loss=0.03953
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02805 | best_loss=0.02805
Epoch 1/80: current_loss=0.02866 | best_loss=0.02805
Epoch 2/80: current_loss=0.02838 | best_loss=0.02805
Epoch 3/80: current_loss=0.02829 | best_loss=0.02805
Epoch 4/80: current_loss=0.02833 | best_loss=0.02805
Epoch 5/80: current_loss=0.02872 | best_loss=0.02805
Epoch 6/80: current_loss=0.02845 | best_loss=0.02805
Epoch 7/80: current_loss=0.02862 | best_loss=0.02805
Epoch 8/80: current_loss=0.02852 | best_loss=0.02805
Epoch 9/80: current_loss=0.02900 | best_loss=0.02805
Epoch 10/80: current_loss=0.02870 | best_loss=0.02805
Epoch 11/80: current_loss=0.02844 | best_loss=0.02805
Epoch 12/80: current_loss=0.02836 | best_loss=0.02805
Epoch 13/80: current_loss=0.02878 | best_loss=0.02805
Epoch 14/80: current_loss=0.02834 | best_loss=0.02805
Epoch 15/80: current_loss=0.02840 | best_loss=0.02805
Epoch 16/80: current_loss=0.02864 | best_loss=0.02805
Epoch 17/80: current_loss=0.02856 | best_loss=0.02805
Epoch 18/80: current_loss=0.02848 | best_loss=0.02805
Epoch 19/80: current_loss=0.02848 | best_loss=0.02805
Epoch 20/80: current_loss=0.02885 | best_loss=0.02805
Early Stopping at epoch 20
      explained_var=-0.00384 | mse_loss=0.02848
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03304 | best_loss=0.03304
Epoch 1/80: current_loss=0.03299 | best_loss=0.03299
Epoch 2/80: current_loss=0.03304 | best_loss=0.03299
Epoch 3/80: current_loss=0.03305 | best_loss=0.03299
Epoch 4/80: current_loss=0.03295 | best_loss=0.03295
Epoch 5/80: current_loss=0.03301 | best_loss=0.03295
Epoch 6/80: current_loss=0.03309 | best_loss=0.03295
Epoch 7/80: current_loss=0.03308 | best_loss=0.03295
Epoch 8/80: current_loss=0.03305 | best_loss=0.03295
Epoch 9/80: current_loss=0.03307 | best_loss=0.03295
Epoch 10/80: current_loss=0.03301 | best_loss=0.03295
Epoch 11/80: current_loss=0.03300 | best_loss=0.03295
Epoch 12/80: current_loss=0.03307 | best_loss=0.03295
Epoch 13/80: current_loss=0.03304 | best_loss=0.03295
Epoch 14/80: current_loss=0.03300 | best_loss=0.03295
Epoch 15/80: current_loss=0.03307 | best_loss=0.03295
Epoch 16/80: current_loss=0.03305 | best_loss=0.03295
Epoch 17/80: current_loss=0.03314 | best_loss=0.03295
Epoch 18/80: current_loss=0.03309 | best_loss=0.03295
Epoch 19/80: current_loss=0.03300 | best_loss=0.03295
Epoch 20/80: current_loss=0.03304 | best_loss=0.03295
Epoch 21/80: current_loss=0.03304 | best_loss=0.03295
Epoch 22/80: current_loss=0.03300 | best_loss=0.03295
Epoch 23/80: current_loss=0.03299 | best_loss=0.03295
Epoch 24/80: current_loss=0.03312 | best_loss=0.03295
Early Stopping at epoch 24
      explained_var=-0.00767 | mse_loss=0.03290
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03525 | best_loss=0.03525
Epoch 1/80: current_loss=0.03531 | best_loss=0.03525
Epoch 2/80: current_loss=0.03550 | best_loss=0.03525
Epoch 3/80: current_loss=0.03538 | best_loss=0.03525
Epoch 4/80: current_loss=0.03536 | best_loss=0.03525
Epoch 5/80: current_loss=0.03542 | best_loss=0.03525
Epoch 6/80: current_loss=0.03533 | best_loss=0.03525
Epoch 7/80: current_loss=0.03544 | best_loss=0.03525
Epoch 8/80: current_loss=0.03544 | best_loss=0.03525
Epoch 9/80: current_loss=0.03543 | best_loss=0.03525
Epoch 10/80: current_loss=0.03546 | best_loss=0.03525
Epoch 11/80: current_loss=0.03543 | best_loss=0.03525
Epoch 12/80: current_loss=0.03542 | best_loss=0.03525
Epoch 13/80: current_loss=0.03540 | best_loss=0.03525
Epoch 14/80: current_loss=0.03537 | best_loss=0.03525
Epoch 15/80: current_loss=0.03535 | best_loss=0.03525
Epoch 16/80: current_loss=0.03534 | best_loss=0.03525
Epoch 17/80: current_loss=0.03531 | best_loss=0.03525
Epoch 18/80: current_loss=0.03534 | best_loss=0.03525
Epoch 19/80: current_loss=0.03530 | best_loss=0.03525
Epoch 20/80: current_loss=0.03530 | best_loss=0.03525
Early Stopping at epoch 20
      explained_var=0.02221 | mse_loss=0.03511
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=0.01445| avg_loss=0.03491
----------------------------------------------


----------------------------------------------
Params for Trial 7
{'learning_rate': 0.1, 'weight_decay': 0.004722149304398, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=4.79221 | best_loss=4.79221
Epoch 1/80: current_loss=1.44368 | best_loss=1.44368
Epoch 2/80: current_loss=0.07569 | best_loss=0.07569
Epoch 3/80: current_loss=0.16081 | best_loss=0.07569
Epoch 4/80: current_loss=0.50759 | best_loss=0.07569
Epoch 5/80: current_loss=0.10994 | best_loss=0.07569
Epoch 6/80: current_loss=0.07781 | best_loss=0.07569
Epoch 7/80: current_loss=0.07740 | best_loss=0.07569
Epoch 8/80: current_loss=0.25979 | best_loss=0.07569
Epoch 9/80: current_loss=0.08087 | best_loss=0.07569
Epoch 10/80: current_loss=0.10872 | best_loss=0.07569
Epoch 11/80: current_loss=0.11545 | best_loss=0.07569
Epoch 12/80: current_loss=0.06818 | best_loss=0.06818
Epoch 13/80: current_loss=0.05586 | best_loss=0.05586
Epoch 14/80: current_loss=0.07064 | best_loss=0.05586
Epoch 15/80: current_loss=0.09903 | best_loss=0.05586
Epoch 16/80: current_loss=0.04580 | best_loss=0.04580
Epoch 17/80: current_loss=0.06250 | best_loss=0.04580
Epoch 18/80: current_loss=0.06942 | best_loss=0.04580
Epoch 19/80: current_loss=0.26732 | best_loss=0.04580
Epoch 20/80: current_loss=0.13490 | best_loss=0.04580
Epoch 21/80: current_loss=0.10243 | best_loss=0.04580
Epoch 22/80: current_loss=0.16836 | best_loss=0.04580
Epoch 23/80: current_loss=0.11008 | best_loss=0.04580
Epoch 24/80: current_loss=0.09799 | best_loss=0.04580
Epoch 25/80: current_loss=0.08557 | best_loss=0.04580
Epoch 26/80: current_loss=0.12790 | best_loss=0.04580
Epoch 27/80: current_loss=0.07541 | best_loss=0.04580
Epoch 28/80: current_loss=0.07310 | best_loss=0.04580
Epoch 29/80: current_loss=0.33698 | best_loss=0.04580
Epoch 30/80: current_loss=0.26674 | best_loss=0.04580
Epoch 31/80: current_loss=0.11573 | best_loss=0.04580
Epoch 32/80: current_loss=0.17304 | best_loss=0.04580
Epoch 33/80: current_loss=0.06732 | best_loss=0.04580
Epoch 34/80: current_loss=1.28477 | best_loss=0.04580
Epoch 35/80: current_loss=0.67588 | best_loss=0.04580
Epoch 36/80: current_loss=0.54223 | best_loss=0.04580
Early Stopping at epoch 36
      explained_var=-0.18173 | mse_loss=0.04623
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.83307 | best_loss=0.83307
Epoch 1/80: current_loss=1.20967 | best_loss=0.83307
Epoch 2/80: current_loss=10.59072 | best_loss=0.83307
Epoch 3/80: current_loss=0.43899 | best_loss=0.43899
Epoch 4/80: current_loss=1.93232 | best_loss=0.43899
Epoch 5/80: current_loss=2.24250 | best_loss=0.43899
Epoch 6/80: current_loss=1.93556 | best_loss=0.43899
Epoch 7/80: current_loss=0.25687 | best_loss=0.25687
Epoch 8/80: current_loss=0.08373 | best_loss=0.08373
Epoch 9/80: current_loss=0.14123 | best_loss=0.08373
Epoch 10/80: current_loss=0.33407 | best_loss=0.08373
Epoch 11/80: current_loss=0.72143 | best_loss=0.08373
Epoch 12/80: current_loss=2.36598 | best_loss=0.08373
Epoch 13/80: current_loss=0.16245 | best_loss=0.08373
Epoch 14/80: current_loss=0.20340 | best_loss=0.08373
Epoch 15/80: current_loss=0.18586 | best_loss=0.08373
Epoch 16/80: current_loss=6.50089 | best_loss=0.08373
Epoch 17/80: current_loss=2.35774 | best_loss=0.08373
Epoch 18/80: current_loss=2.45182 | best_loss=0.08373
Epoch 19/80: current_loss=9.09202 | best_loss=0.08373
Epoch 20/80: current_loss=2.85463 | best_loss=0.08373
Epoch 21/80: current_loss=0.73799 | best_loss=0.08373
Epoch 22/80: current_loss=0.41568 | best_loss=0.08373
Epoch 23/80: current_loss=1.57796 | best_loss=0.08373
Epoch 24/80: current_loss=0.47260 | best_loss=0.08373
Epoch 25/80: current_loss=0.32462 | best_loss=0.08373
Epoch 26/80: current_loss=0.04659 | best_loss=0.04659
Epoch 27/80: current_loss=1.72423 | best_loss=0.04659
Epoch 28/80: current_loss=1.10850 | best_loss=0.04659
Epoch 29/80: current_loss=1.48957 | best_loss=0.04659
Epoch 30/80: current_loss=11.75501 | best_loss=0.04659
Epoch 31/80: current_loss=29.03016 | best_loss=0.04659
Epoch 32/80: current_loss=21.92367 | best_loss=0.04659
Epoch 33/80: current_loss=1.03601 | best_loss=0.04659
Epoch 34/80: current_loss=5.08762 | best_loss=0.04659
Epoch 35/80: current_loss=4.02501 | best_loss=0.04659
Epoch 36/80: current_loss=0.11004 | best_loss=0.04659
Epoch 37/80: current_loss=0.42697 | best_loss=0.04659
Epoch 38/80: current_loss=1.05044 | best_loss=0.04659
Epoch 39/80: current_loss=0.13562 | best_loss=0.04659
Epoch 40/80: current_loss=0.06084 | best_loss=0.04659
Epoch 41/80: current_loss=0.08648 | best_loss=0.04659
Epoch 42/80: current_loss=0.09767 | best_loss=0.04659
Epoch 43/80: current_loss=0.21058 | best_loss=0.04659
Epoch 44/80: current_loss=0.26572 | best_loss=0.04659
Epoch 45/80: current_loss=0.25000 | best_loss=0.04659
Epoch 46/80: current_loss=0.63493 | best_loss=0.04659
Early Stopping at epoch 46
      explained_var=0.00527 | mse_loss=0.04550
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=4.15756 | best_loss=4.15756
Epoch 1/80: current_loss=1.29177 | best_loss=1.29177
Epoch 2/80: current_loss=2.33907 | best_loss=1.29177
Epoch 3/80: current_loss=0.37618 | best_loss=0.37618
Epoch 4/80: current_loss=0.26888 | best_loss=0.26888
Epoch 5/80: current_loss=0.14777 | best_loss=0.14777
Epoch 6/80: current_loss=0.08409 | best_loss=0.08409
Epoch 7/80: current_loss=0.04330 | best_loss=0.04330
Epoch 8/80: current_loss=0.05531 | best_loss=0.04330
Epoch 9/80: current_loss=0.30409 | best_loss=0.04330
Epoch 10/80: current_loss=0.49593 | best_loss=0.04330
Epoch 11/80: current_loss=0.06071 | best_loss=0.04330
Epoch 12/80: current_loss=0.05777 | best_loss=0.04330
Epoch 13/80: current_loss=14.89697 | best_loss=0.04330
Epoch 14/80: current_loss=0.43171 | best_loss=0.04330
Epoch 15/80: current_loss=12.86777 | best_loss=0.04330
Epoch 16/80: current_loss=0.09408 | best_loss=0.04330
Epoch 17/80: current_loss=0.80324 | best_loss=0.04330
Epoch 18/80: current_loss=0.09993 | best_loss=0.04330
Epoch 19/80: current_loss=0.03078 | best_loss=0.03078
Epoch 20/80: current_loss=0.48030 | best_loss=0.03078
Epoch 21/80: current_loss=0.05252 | best_loss=0.03078
Epoch 22/80: current_loss=0.26693 | best_loss=0.03078
Epoch 23/80: current_loss=0.04818 | best_loss=0.03078
Epoch 24/80: current_loss=0.03902 | best_loss=0.03078
Epoch 25/80: current_loss=0.03955 | best_loss=0.03078
Epoch 26/80: current_loss=0.04724 | best_loss=0.03078
Epoch 27/80: current_loss=0.05912 | best_loss=0.03078
Epoch 28/80: current_loss=0.04473 | best_loss=0.03078
Epoch 29/80: current_loss=0.04777 | best_loss=0.03078
Epoch 30/80: current_loss=0.03208 | best_loss=0.03078
Epoch 31/80: current_loss=0.06238 | best_loss=0.03078
Epoch 32/80: current_loss=0.04779 | best_loss=0.03078
Epoch 33/80: current_loss=0.07580 | best_loss=0.03078
Epoch 34/80: current_loss=0.02854 | best_loss=0.02854
Epoch 35/80: current_loss=0.03139 | best_loss=0.02854
Epoch 36/80: current_loss=0.03314 | best_loss=0.02854
Epoch 37/80: current_loss=0.03241 | best_loss=0.02854
Epoch 38/80: current_loss=0.02812 | best_loss=0.02812
Epoch 39/80: current_loss=0.02862 | best_loss=0.02812
Epoch 40/80: current_loss=0.02783 | best_loss=0.02783
Epoch 41/80: current_loss=0.02796 | best_loss=0.02783
Epoch 42/80: current_loss=0.02785 | best_loss=0.02783
Epoch 43/80: current_loss=0.02836 | best_loss=0.02783
Epoch 44/80: current_loss=0.02888 | best_loss=0.02783
Epoch 45/80: current_loss=0.02816 | best_loss=0.02783
Epoch 46/80: current_loss=0.02806 | best_loss=0.02783
Epoch 47/80: current_loss=0.02792 | best_loss=0.02783
Epoch 48/80: current_loss=0.02786 | best_loss=0.02783
Epoch 49/80: current_loss=0.02791 | best_loss=0.02783
Epoch 50/80: current_loss=0.02798 | best_loss=0.02783
Epoch 51/80: current_loss=0.02800 | best_loss=0.02783
Epoch 52/80: current_loss=0.02836 | best_loss=0.02783
Epoch 53/80: current_loss=0.02938 | best_loss=0.02783
Epoch 54/80: current_loss=0.02933 | best_loss=0.02783
Epoch 55/80: current_loss=0.02894 | best_loss=0.02783
Epoch 56/80: current_loss=0.02852 | best_loss=0.02783
Epoch 57/80: current_loss=0.02793 | best_loss=0.02783
Epoch 58/80: current_loss=0.02789 | best_loss=0.02783
Epoch 59/80: current_loss=0.02882 | best_loss=0.02783
Epoch 60/80: current_loss=0.02840 | best_loss=0.02783
Early Stopping at epoch 60
      explained_var=0.00419 | mse_loss=0.02825
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=12.91951 | best_loss=12.91951
Epoch 1/80: current_loss=0.34174 | best_loss=0.34174
Epoch 2/80: current_loss=5.06145 | best_loss=0.34174
Epoch 3/80: current_loss=1.60841 | best_loss=0.34174
Epoch 4/80: current_loss=0.52158 | best_loss=0.34174
Epoch 5/80: current_loss=0.47351 | best_loss=0.34174
Epoch 6/80: current_loss=0.08836 | best_loss=0.08836
Epoch 7/80: current_loss=0.20494 | best_loss=0.08836
Epoch 8/80: current_loss=0.14786 | best_loss=0.08836
Epoch 9/80: current_loss=0.09179 | best_loss=0.08836
Epoch 10/80: current_loss=0.88833 | best_loss=0.08836
Epoch 11/80: current_loss=0.11601 | best_loss=0.08836
Epoch 12/80: current_loss=0.21532 | best_loss=0.08836
Epoch 13/80: current_loss=0.41442 | best_loss=0.08836
Epoch 14/80: current_loss=0.32614 | best_loss=0.08836
Epoch 15/80: current_loss=2.44467 | best_loss=0.08836
Epoch 16/80: current_loss=0.36053 | best_loss=0.08836
Epoch 17/80: current_loss=41.51817 | best_loss=0.08836
Epoch 18/80: current_loss=12.15153 | best_loss=0.08836
Epoch 19/80: current_loss=0.08691 | best_loss=0.08691
Epoch 20/80: current_loss=3.78111 | best_loss=0.08691
Epoch 21/80: current_loss=1.96738 | best_loss=0.08691
Epoch 22/80: current_loss=0.75924 | best_loss=0.08691
Epoch 23/80: current_loss=5.46253 | best_loss=0.08691
Epoch 24/80: current_loss=0.14187 | best_loss=0.08691
Epoch 25/80: current_loss=0.82479 | best_loss=0.08691
Epoch 26/80: current_loss=0.20325 | best_loss=0.08691
Epoch 27/80: current_loss=0.18668 | best_loss=0.08691
Epoch 28/80: current_loss=0.03918 | best_loss=0.03918
Epoch 29/80: current_loss=0.03519 | best_loss=0.03519
Epoch 30/80: current_loss=0.03305 | best_loss=0.03305
Epoch 31/80: current_loss=0.26318 | best_loss=0.03305
Epoch 32/80: current_loss=0.14832 | best_loss=0.03305
Epoch 33/80: current_loss=0.05539 | best_loss=0.03305
Epoch 34/80: current_loss=0.03827 | best_loss=0.03305
Epoch 35/80: current_loss=0.10758 | best_loss=0.03305
Epoch 36/80: current_loss=0.06131 | best_loss=0.03305
Epoch 37/80: current_loss=0.10338 | best_loss=0.03305
Epoch 38/80: current_loss=0.11539 | best_loss=0.03305
Epoch 39/80: current_loss=0.03492 | best_loss=0.03305
Epoch 40/80: current_loss=0.15370 | best_loss=0.03305
Epoch 41/80: current_loss=0.04053 | best_loss=0.03305
Epoch 42/80: current_loss=0.10271 | best_loss=0.03305
Epoch 43/80: current_loss=0.13210 | best_loss=0.03305
Epoch 44/80: current_loss=0.04619 | best_loss=0.03305
Epoch 45/80: current_loss=0.05283 | best_loss=0.03305
Epoch 46/80: current_loss=0.04024 | best_loss=0.03305
Epoch 47/80: current_loss=0.03717 | best_loss=0.03305
Epoch 48/80: current_loss=0.03761 | best_loss=0.03305
Epoch 49/80: current_loss=0.08404 | best_loss=0.03305
Epoch 50/80: current_loss=0.04616 | best_loss=0.03305
Early Stopping at epoch 50
      explained_var=-0.00706 | mse_loss=0.03285
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=3.05778 | best_loss=3.05778
Epoch 1/80: current_loss=3.29290 | best_loss=3.05778
Epoch 2/80: current_loss=3.81164 | best_loss=3.05778
Epoch 3/80: current_loss=6.94162 | best_loss=3.05778
Epoch 4/80: current_loss=0.30892 | best_loss=0.30892
Epoch 5/80: current_loss=1.64894 | best_loss=0.30892
Epoch 6/80: current_loss=0.18148 | best_loss=0.18148
Epoch 7/80: current_loss=0.20795 | best_loss=0.18148
Epoch 8/80: current_loss=0.11092 | best_loss=0.11092
Epoch 9/80: current_loss=0.16958 | best_loss=0.11092
Epoch 10/80: current_loss=5.60064 | best_loss=0.11092
Epoch 11/80: current_loss=0.31143 | best_loss=0.11092
Epoch 12/80: current_loss=0.19861 | best_loss=0.11092
Epoch 13/80: current_loss=0.10522 | best_loss=0.10522
Epoch 14/80: current_loss=0.09744 | best_loss=0.09744
Epoch 15/80: current_loss=0.11887 | best_loss=0.09744
Epoch 16/80: current_loss=0.86571 | best_loss=0.09744
Epoch 17/80: current_loss=16.88515 | best_loss=0.09744
Epoch 18/80: current_loss=1.74448 | best_loss=0.09744
Epoch 19/80: current_loss=6.05958 | best_loss=0.09744
Epoch 20/80: current_loss=0.76783 | best_loss=0.09744
Epoch 21/80: current_loss=0.15631 | best_loss=0.09744
Epoch 22/80: current_loss=0.68620 | best_loss=0.09744
Epoch 23/80: current_loss=0.05143 | best_loss=0.05143
Epoch 24/80: current_loss=0.08801 | best_loss=0.05143
Epoch 25/80: current_loss=0.74689 | best_loss=0.05143
Epoch 26/80: current_loss=0.05127 | best_loss=0.05127
Epoch 27/80: current_loss=1.06029 | best_loss=0.05127
Epoch 28/80: current_loss=5.34416 | best_loss=0.05127
Epoch 29/80: current_loss=6.69654 | best_loss=0.05127
Epoch 30/80: current_loss=0.61969 | best_loss=0.05127
Epoch 31/80: current_loss=5.69299 | best_loss=0.05127
Epoch 32/80: current_loss=0.18104 | best_loss=0.05127
Epoch 33/80: current_loss=6.17684 | best_loss=0.05127
Epoch 34/80: current_loss=2.76151 | best_loss=0.05127
Epoch 35/80: current_loss=13.50576 | best_loss=0.05127
Epoch 36/80: current_loss=0.34084 | best_loss=0.05127
Epoch 37/80: current_loss=1.10556 | best_loss=0.05127
Epoch 38/80: current_loss=0.24907 | best_loss=0.05127
Epoch 39/80: current_loss=0.36786 | best_loss=0.05127
Epoch 40/80: current_loss=0.08560 | best_loss=0.05127
Epoch 41/80: current_loss=0.18289 | best_loss=0.05127
Epoch 42/80: current_loss=0.27656 | best_loss=0.05127
Epoch 43/80: current_loss=0.17092 | best_loss=0.05127
Epoch 44/80: current_loss=0.12498 | best_loss=0.05127
Epoch 45/80: current_loss=0.06588 | best_loss=0.05127
Epoch 46/80: current_loss=0.15435 | best_loss=0.05127
Early Stopping at epoch 46
      explained_var=-0.31443 | mse_loss=0.05150
----------------------------------------------
Average early_stopping_point: 27| avg_exp_var=-0.09875| avg_loss=0.04086
----------------------------------------------


----------------------------------------------
Params for Trial 8
{'learning_rate': 1e-05, 'weight_decay': 0.006364104148996763, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.38755 | best_loss=0.38755
Epoch 1/80: current_loss=0.37264 | best_loss=0.37264
Epoch 2/80: current_loss=0.35823 | best_loss=0.35823
Epoch 3/80: current_loss=0.34406 | best_loss=0.34406
Epoch 4/80: current_loss=0.33005 | best_loss=0.33005
Epoch 5/80: current_loss=0.31645 | best_loss=0.31645
Epoch 6/80: current_loss=0.30311 | best_loss=0.30311
Epoch 7/80: current_loss=0.28973 | best_loss=0.28973
Epoch 8/80: current_loss=0.27639 | best_loss=0.27639
Epoch 9/80: current_loss=0.26327 | best_loss=0.26327
Epoch 10/80: current_loss=0.25009 | best_loss=0.25009
Epoch 11/80: current_loss=0.23720 | best_loss=0.23720
Epoch 12/80: current_loss=0.22445 | best_loss=0.22445
Epoch 13/80: current_loss=0.21192 | best_loss=0.21192
Epoch 14/80: current_loss=0.19960 | best_loss=0.19960
Epoch 15/80: current_loss=0.18728 | best_loss=0.18728
Epoch 16/80: current_loss=0.17551 | best_loss=0.17551
Epoch 17/80: current_loss=0.16409 | best_loss=0.16409
Epoch 18/80: current_loss=0.15261 | best_loss=0.15261
Epoch 19/80: current_loss=0.14177 | best_loss=0.14177
Epoch 20/80: current_loss=0.13152 | best_loss=0.13152
Epoch 21/80: current_loss=0.12168 | best_loss=0.12168
Epoch 22/80: current_loss=0.11240 | best_loss=0.11240
Epoch 23/80: current_loss=0.10370 | best_loss=0.10370
Epoch 24/80: current_loss=0.09616 | best_loss=0.09616
Epoch 25/80: current_loss=0.08918 | best_loss=0.08918
Epoch 26/80: current_loss=0.08293 | best_loss=0.08293
Epoch 27/80: current_loss=0.07756 | best_loss=0.07756
Epoch 28/80: current_loss=0.07315 | best_loss=0.07315
Epoch 29/80: current_loss=0.06964 | best_loss=0.06964
Epoch 30/80: current_loss=0.06655 | best_loss=0.06655
Epoch 31/80: current_loss=0.06427 | best_loss=0.06427
Epoch 32/80: current_loss=0.06235 | best_loss=0.06235
Epoch 33/80: current_loss=0.06088 | best_loss=0.06088
Epoch 34/80: current_loss=0.05977 | best_loss=0.05977
Epoch 35/80: current_loss=0.05891 | best_loss=0.05891
Epoch 36/80: current_loss=0.05821 | best_loss=0.05821
Epoch 37/80: current_loss=0.05757 | best_loss=0.05757
Epoch 38/80: current_loss=0.05713 | best_loss=0.05713
Epoch 39/80: current_loss=0.05673 | best_loss=0.05673
Epoch 40/80: current_loss=0.05649 | best_loss=0.05649
Epoch 41/80: current_loss=0.05630 | best_loss=0.05630
Epoch 42/80: current_loss=0.05611 | best_loss=0.05611
Epoch 43/80: current_loss=0.05597 | best_loss=0.05597
Epoch 44/80: current_loss=0.05576 | best_loss=0.05576
Epoch 45/80: current_loss=0.05557 | best_loss=0.05557
Epoch 46/80: current_loss=0.05540 | best_loss=0.05540
Epoch 47/80: current_loss=0.05525 | best_loss=0.05525
Epoch 48/80: current_loss=0.05509 | best_loss=0.05509
Epoch 49/80: current_loss=0.05494 | best_loss=0.05494
Epoch 50/80: current_loss=0.05478 | best_loss=0.05478
Epoch 51/80: current_loss=0.05466 | best_loss=0.05466
Epoch 52/80: current_loss=0.05455 | best_loss=0.05455
Epoch 53/80: current_loss=0.05439 | best_loss=0.05439
Epoch 54/80: current_loss=0.05426 | best_loss=0.05426
Epoch 55/80: current_loss=0.05415 | best_loss=0.05415
Epoch 56/80: current_loss=0.05411 | best_loss=0.05411
Epoch 57/80: current_loss=0.05396 | best_loss=0.05396
Epoch 58/80: current_loss=0.05384 | best_loss=0.05384
Epoch 59/80: current_loss=0.05373 | best_loss=0.05373
Epoch 60/80: current_loss=0.05362 | best_loss=0.05362
Epoch 61/80: current_loss=0.05349 | best_loss=0.05349
Epoch 62/80: current_loss=0.05327 | best_loss=0.05327
Epoch 63/80: current_loss=0.05315 | best_loss=0.05315
Epoch 64/80: current_loss=0.05302 | best_loss=0.05302
Epoch 65/80: current_loss=0.05290 | best_loss=0.05290
Epoch 66/80: current_loss=0.05275 | best_loss=0.05275
Epoch 67/80: current_loss=0.05272 | best_loss=0.05272
Epoch 68/80: current_loss=0.05252 | best_loss=0.05252
Epoch 69/80: current_loss=0.05240 | best_loss=0.05240
Epoch 70/80: current_loss=0.05231 | best_loss=0.05231
Epoch 71/80: current_loss=0.05218 | best_loss=0.05218
Epoch 72/80: current_loss=0.05211 | best_loss=0.05211
Epoch 73/80: current_loss=0.05208 | best_loss=0.05208
Epoch 74/80: current_loss=0.05202 | best_loss=0.05202
Epoch 75/80: current_loss=0.05193 | best_loss=0.05193
Epoch 76/80: current_loss=0.05180 | best_loss=0.05180
Epoch 77/80: current_loss=0.05167 | best_loss=0.05167
Epoch 78/80: current_loss=0.05146 | best_loss=0.05146
Epoch 79/80: current_loss=0.05133 | best_loss=0.05133
      explained_var=-0.26498 | mse_loss=0.05203
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05730 | best_loss=0.05730
Epoch 1/80: current_loss=0.05670 | best_loss=0.05670
Epoch 2/80: current_loss=0.05640 | best_loss=0.05640
Epoch 3/80: current_loss=0.05609 | best_loss=0.05609
Epoch 4/80: current_loss=0.05571 | best_loss=0.05571
Epoch 5/80: current_loss=0.05536 | best_loss=0.05536
Epoch 6/80: current_loss=0.05530 | best_loss=0.05530
Epoch 7/80: current_loss=0.05521 | best_loss=0.05521
Epoch 8/80: current_loss=0.05484 | best_loss=0.05484
Epoch 9/80: current_loss=0.05468 | best_loss=0.05468
Epoch 10/80: current_loss=0.05450 | best_loss=0.05450
Epoch 11/80: current_loss=0.05444 | best_loss=0.05444
Epoch 12/80: current_loss=0.05432 | best_loss=0.05432
Epoch 13/80: current_loss=0.05381 | best_loss=0.05381
Epoch 14/80: current_loss=0.05362 | best_loss=0.05362
Epoch 15/80: current_loss=0.05351 | best_loss=0.05351
Epoch 16/80: current_loss=0.05333 | best_loss=0.05333
Epoch 17/80: current_loss=0.05311 | best_loss=0.05311
Epoch 18/80: current_loss=0.05288 | best_loss=0.05288
Epoch 19/80: current_loss=0.05279 | best_loss=0.05279
Epoch 20/80: current_loss=0.05266 | best_loss=0.05266
Epoch 21/80: current_loss=0.05246 | best_loss=0.05246
Epoch 22/80: current_loss=0.05250 | best_loss=0.05246
Epoch 23/80: current_loss=0.05226 | best_loss=0.05226
Epoch 24/80: current_loss=0.05198 | best_loss=0.05198
Epoch 25/80: current_loss=0.05175 | best_loss=0.05175
Epoch 26/80: current_loss=0.05165 | best_loss=0.05165
Epoch 27/80: current_loss=0.05163 | best_loss=0.05163
Epoch 28/80: current_loss=0.05148 | best_loss=0.05148
Epoch 29/80: current_loss=0.05131 | best_loss=0.05131
Epoch 30/80: current_loss=0.05126 | best_loss=0.05126
Epoch 31/80: current_loss=0.05117 | best_loss=0.05117
Epoch 32/80: current_loss=0.05099 | best_loss=0.05099
Epoch 33/80: current_loss=0.05088 | best_loss=0.05088
Epoch 34/80: current_loss=0.05070 | best_loss=0.05070
Epoch 35/80: current_loss=0.05056 | best_loss=0.05056
Epoch 36/80: current_loss=0.05048 | best_loss=0.05048
Epoch 37/80: current_loss=0.05037 | best_loss=0.05037
Epoch 38/80: current_loss=0.05032 | best_loss=0.05032
Epoch 39/80: current_loss=0.05008 | best_loss=0.05008
Epoch 40/80: current_loss=0.04998 | best_loss=0.04998
Epoch 41/80: current_loss=0.04993 | best_loss=0.04993
Epoch 42/80: current_loss=0.04984 | best_loss=0.04984
Epoch 43/80: current_loss=0.04975 | best_loss=0.04975
Epoch 44/80: current_loss=0.04960 | best_loss=0.04960
Epoch 45/80: current_loss=0.04951 | best_loss=0.04951
Epoch 46/80: current_loss=0.04937 | best_loss=0.04937
Epoch 47/80: current_loss=0.04942 | best_loss=0.04937
Epoch 48/80: current_loss=0.04935 | best_loss=0.04935
Epoch 49/80: current_loss=0.04922 | best_loss=0.04922
Epoch 50/80: current_loss=0.04906 | best_loss=0.04906
Epoch 51/80: current_loss=0.04900 | best_loss=0.04900
Epoch 52/80: current_loss=0.04881 | best_loss=0.04881
Epoch 53/80: current_loss=0.04865 | best_loss=0.04865
Epoch 54/80: current_loss=0.04855 | best_loss=0.04855
Epoch 55/80: current_loss=0.04859 | best_loss=0.04855
Epoch 56/80: current_loss=0.04855 | best_loss=0.04855
Epoch 57/80: current_loss=0.04844 | best_loss=0.04844
Epoch 58/80: current_loss=0.04837 | best_loss=0.04837
Epoch 59/80: current_loss=0.04822 | best_loss=0.04822
Epoch 60/80: current_loss=0.04817 | best_loss=0.04817
Epoch 61/80: current_loss=0.04808 | best_loss=0.04808
Epoch 62/80: current_loss=0.04806 | best_loss=0.04806
Epoch 63/80: current_loss=0.04796 | best_loss=0.04796
Epoch 64/80: current_loss=0.04788 | best_loss=0.04788
Epoch 65/80: current_loss=0.04781 | best_loss=0.04781
Epoch 66/80: current_loss=0.04775 | best_loss=0.04775
Epoch 67/80: current_loss=0.04764 | best_loss=0.04764
Epoch 68/80: current_loss=0.04760 | best_loss=0.04760
Epoch 69/80: current_loss=0.04752 | best_loss=0.04752
Epoch 70/80: current_loss=0.04740 | best_loss=0.04740
Epoch 71/80: current_loss=0.04733 | best_loss=0.04733
Epoch 72/80: current_loss=0.04726 | best_loss=0.04726
Epoch 73/80: current_loss=0.04723 | best_loss=0.04723
Epoch 74/80: current_loss=0.04724 | best_loss=0.04723
Epoch 75/80: current_loss=0.04718 | best_loss=0.04718
Epoch 76/80: current_loss=0.04711 | best_loss=0.04711
Epoch 77/80: current_loss=0.04705 | best_loss=0.04705
Epoch 78/80: current_loss=0.04695 | best_loss=0.04695
Epoch 79/80: current_loss=0.04687 | best_loss=0.04687
      explained_var=-0.07403 | mse_loss=0.04503
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03111 | best_loss=0.03111
Epoch 1/80: current_loss=0.03097 | best_loss=0.03097
Epoch 2/80: current_loss=0.03094 | best_loss=0.03094
Epoch 3/80: current_loss=0.03086 | best_loss=0.03086
Epoch 4/80: current_loss=0.03079 | best_loss=0.03079
Epoch 5/80: current_loss=0.03074 | best_loss=0.03074
Epoch 6/80: current_loss=0.03071 | best_loss=0.03071
Epoch 7/80: current_loss=0.03068 | best_loss=0.03068
Epoch 8/80: current_loss=0.03065 | best_loss=0.03065
Epoch 9/80: current_loss=0.03061 | best_loss=0.03061
Epoch 10/80: current_loss=0.03060 | best_loss=0.03060
Epoch 11/80: current_loss=0.03060 | best_loss=0.03060
Epoch 12/80: current_loss=0.03059 | best_loss=0.03059
Epoch 13/80: current_loss=0.03055 | best_loss=0.03055
Epoch 14/80: current_loss=0.03052 | best_loss=0.03052
Epoch 15/80: current_loss=0.03050 | best_loss=0.03050
Epoch 16/80: current_loss=0.03048 | best_loss=0.03048
Epoch 17/80: current_loss=0.03047 | best_loss=0.03047
Epoch 18/80: current_loss=0.03045 | best_loss=0.03045
Epoch 19/80: current_loss=0.03042 | best_loss=0.03042
Epoch 20/80: current_loss=0.03038 | best_loss=0.03038
Epoch 21/80: current_loss=0.03035 | best_loss=0.03035
Epoch 22/80: current_loss=0.03032 | best_loss=0.03032
Epoch 23/80: current_loss=0.03031 | best_loss=0.03031
Epoch 24/80: current_loss=0.03029 | best_loss=0.03029
Epoch 25/80: current_loss=0.03028 | best_loss=0.03028
Epoch 26/80: current_loss=0.03025 | best_loss=0.03025
Epoch 27/80: current_loss=0.03021 | best_loss=0.03021
Epoch 28/80: current_loss=0.03019 | best_loss=0.03019
Epoch 29/80: current_loss=0.03019 | best_loss=0.03019
Epoch 30/80: current_loss=0.03016 | best_loss=0.03016
Epoch 31/80: current_loss=0.03014 | best_loss=0.03014
Epoch 32/80: current_loss=0.03013 | best_loss=0.03013
Epoch 33/80: current_loss=0.03010 | best_loss=0.03010
Epoch 34/80: current_loss=0.03010 | best_loss=0.03010
Epoch 35/80: current_loss=0.03008 | best_loss=0.03008
Epoch 36/80: current_loss=0.03006 | best_loss=0.03006
Epoch 37/80: current_loss=0.03006 | best_loss=0.03006
Epoch 38/80: current_loss=0.03004 | best_loss=0.03004
Epoch 39/80: current_loss=0.03000 | best_loss=0.03000
Epoch 40/80: current_loss=0.02997 | best_loss=0.02997
Epoch 41/80: current_loss=0.02995 | best_loss=0.02995
Epoch 42/80: current_loss=0.02994 | best_loss=0.02994
Epoch 43/80: current_loss=0.02994 | best_loss=0.02994
Epoch 44/80: current_loss=0.02993 | best_loss=0.02993
Epoch 45/80: current_loss=0.02989 | best_loss=0.02989
Epoch 46/80: current_loss=0.02987 | best_loss=0.02987
Epoch 47/80: current_loss=0.02987 | best_loss=0.02987
Epoch 48/80: current_loss=0.02984 | best_loss=0.02984
Epoch 49/80: current_loss=0.02983 | best_loss=0.02983
Epoch 50/80: current_loss=0.02983 | best_loss=0.02983
Epoch 51/80: current_loss=0.02982 | best_loss=0.02982
Epoch 52/80: current_loss=0.02979 | best_loss=0.02979
Epoch 53/80: current_loss=0.02978 | best_loss=0.02978
Epoch 54/80: current_loss=0.02976 | best_loss=0.02976
Epoch 55/80: current_loss=0.02975 | best_loss=0.02975
Epoch 56/80: current_loss=0.02976 | best_loss=0.02975
Epoch 57/80: current_loss=0.02975 | best_loss=0.02975
Epoch 58/80: current_loss=0.02974 | best_loss=0.02974
Epoch 59/80: current_loss=0.02975 | best_loss=0.02974
Epoch 60/80: current_loss=0.02977 | best_loss=0.02974
Epoch 61/80: current_loss=0.02976 | best_loss=0.02974
Epoch 62/80: current_loss=0.02975 | best_loss=0.02974
Epoch 63/80: current_loss=0.02970 | best_loss=0.02970
Epoch 64/80: current_loss=0.02971 | best_loss=0.02970
Epoch 65/80: current_loss=0.02973 | best_loss=0.02970
Epoch 66/80: current_loss=0.02974 | best_loss=0.02970
Epoch 67/80: current_loss=0.02974 | best_loss=0.02970
Epoch 68/80: current_loss=0.02971 | best_loss=0.02970
Epoch 69/80: current_loss=0.02971 | best_loss=0.02970
Epoch 70/80: current_loss=0.02966 | best_loss=0.02966
Epoch 71/80: current_loss=0.02965 | best_loss=0.02965
Epoch 72/80: current_loss=0.02965 | best_loss=0.02965
Epoch 73/80: current_loss=0.02960 | best_loss=0.02960
Epoch 74/80: current_loss=0.02960 | best_loss=0.02960
Epoch 75/80: current_loss=0.02959 | best_loss=0.02959
Epoch 76/80: current_loss=0.02958 | best_loss=0.02958
Epoch 77/80: current_loss=0.02958 | best_loss=0.02958
Epoch 78/80: current_loss=0.02957 | best_loss=0.02957
Epoch 79/80: current_loss=0.02957 | best_loss=0.02957
      explained_var=-0.05151 | mse_loss=0.02982
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03505 | best_loss=0.03505
Epoch 1/80: current_loss=0.03513 | best_loss=0.03505
Epoch 2/80: current_loss=0.03512 | best_loss=0.03505
Epoch 3/80: current_loss=0.03513 | best_loss=0.03505
Epoch 4/80: current_loss=0.03514 | best_loss=0.03505
Epoch 5/80: current_loss=0.03512 | best_loss=0.03505
Epoch 6/80: current_loss=0.03504 | best_loss=0.03504
Epoch 7/80: current_loss=0.03502 | best_loss=0.03502
Epoch 8/80: current_loss=0.03497 | best_loss=0.03497
Epoch 9/80: current_loss=0.03492 | best_loss=0.03492
Epoch 10/80: current_loss=0.03492 | best_loss=0.03492
Epoch 11/80: current_loss=0.03494 | best_loss=0.03492
Epoch 12/80: current_loss=0.03495 | best_loss=0.03492
Epoch 13/80: current_loss=0.03502 | best_loss=0.03492
Epoch 14/80: current_loss=0.03499 | best_loss=0.03492
Epoch 15/80: current_loss=0.03502 | best_loss=0.03492
Epoch 16/80: current_loss=0.03501 | best_loss=0.03492
Epoch 17/80: current_loss=0.03497 | best_loss=0.03492
Epoch 18/80: current_loss=0.03495 | best_loss=0.03492
Epoch 19/80: current_loss=0.03495 | best_loss=0.03492
Epoch 20/80: current_loss=0.03489 | best_loss=0.03489
Epoch 21/80: current_loss=0.03495 | best_loss=0.03489
Epoch 22/80: current_loss=0.03494 | best_loss=0.03489
Epoch 23/80: current_loss=0.03495 | best_loss=0.03489
Epoch 24/80: current_loss=0.03494 | best_loss=0.03489
Epoch 25/80: current_loss=0.03491 | best_loss=0.03489
Epoch 26/80: current_loss=0.03487 | best_loss=0.03487
Epoch 27/80: current_loss=0.03480 | best_loss=0.03480
Epoch 28/80: current_loss=0.03479 | best_loss=0.03479
Epoch 29/80: current_loss=0.03481 | best_loss=0.03479
Epoch 30/80: current_loss=0.03483 | best_loss=0.03479
Epoch 31/80: current_loss=0.03478 | best_loss=0.03478
Epoch 32/80: current_loss=0.03476 | best_loss=0.03476
Epoch 33/80: current_loss=0.03475 | best_loss=0.03475
Epoch 34/80: current_loss=0.03466 | best_loss=0.03466
Epoch 35/80: current_loss=0.03471 | best_loss=0.03466
Epoch 36/80: current_loss=0.03474 | best_loss=0.03466
Epoch 37/80: current_loss=0.03472 | best_loss=0.03466
Epoch 38/80: current_loss=0.03473 | best_loss=0.03466
Epoch 39/80: current_loss=0.03474 | best_loss=0.03466
Epoch 40/80: current_loss=0.03469 | best_loss=0.03466
Epoch 41/80: current_loss=0.03466 | best_loss=0.03466
Epoch 42/80: current_loss=0.03466 | best_loss=0.03466
Epoch 43/80: current_loss=0.03467 | best_loss=0.03466
Epoch 44/80: current_loss=0.03468 | best_loss=0.03466
Epoch 45/80: current_loss=0.03472 | best_loss=0.03466
Epoch 46/80: current_loss=0.03471 | best_loss=0.03466
Epoch 47/80: current_loss=0.03474 | best_loss=0.03466
Epoch 48/80: current_loss=0.03471 | best_loss=0.03466
Epoch 49/80: current_loss=0.03469 | best_loss=0.03466
Epoch 50/80: current_loss=0.03470 | best_loss=0.03466
Epoch 51/80: current_loss=0.03470 | best_loss=0.03466
Epoch 52/80: current_loss=0.03470 | best_loss=0.03466
Epoch 53/80: current_loss=0.03468 | best_loss=0.03466
Epoch 54/80: current_loss=0.03469 | best_loss=0.03466
Epoch 55/80: current_loss=0.03471 | best_loss=0.03466
Epoch 56/80: current_loss=0.03466 | best_loss=0.03466
Epoch 57/80: current_loss=0.03468 | best_loss=0.03466
Epoch 58/80: current_loss=0.03473 | best_loss=0.03466
Epoch 59/80: current_loss=0.03469 | best_loss=0.03466
Epoch 60/80: current_loss=0.03464 | best_loss=0.03464
Epoch 61/80: current_loss=0.03471 | best_loss=0.03464
Epoch 62/80: current_loss=0.03468 | best_loss=0.03464
Epoch 63/80: current_loss=0.03470 | best_loss=0.03464
Epoch 64/80: current_loss=0.03466 | best_loss=0.03464
Epoch 65/80: current_loss=0.03467 | best_loss=0.03464
Epoch 66/80: current_loss=0.03466 | best_loss=0.03464
Epoch 67/80: current_loss=0.03462 | best_loss=0.03462
Epoch 68/80: current_loss=0.03463 | best_loss=0.03462
Epoch 69/80: current_loss=0.03463 | best_loss=0.03462
Epoch 70/80: current_loss=0.03467 | best_loss=0.03462
Epoch 71/80: current_loss=0.03464 | best_loss=0.03462
Epoch 72/80: current_loss=0.03463 | best_loss=0.03462
Epoch 73/80: current_loss=0.03462 | best_loss=0.03462
Epoch 74/80: current_loss=0.03455 | best_loss=0.03455
Epoch 75/80: current_loss=0.03451 | best_loss=0.03451
Epoch 76/80: current_loss=0.03449 | best_loss=0.03449
Epoch 77/80: current_loss=0.03446 | best_loss=0.03446
Epoch 78/80: current_loss=0.03449 | best_loss=0.03446
Epoch 79/80: current_loss=0.03445 | best_loss=0.03445
      explained_var=-0.04116 | mse_loss=0.03415
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03638 | best_loss=0.03638
Epoch 1/80: current_loss=0.03637 | best_loss=0.03637
Epoch 2/80: current_loss=0.03636 | best_loss=0.03636
Epoch 3/80: current_loss=0.03636 | best_loss=0.03636
Epoch 4/80: current_loss=0.03635 | best_loss=0.03635
Epoch 5/80: current_loss=0.03634 | best_loss=0.03634
Epoch 6/80: current_loss=0.03634 | best_loss=0.03634
Epoch 7/80: current_loss=0.03634 | best_loss=0.03634
Epoch 8/80: current_loss=0.03633 | best_loss=0.03633
Epoch 9/80: current_loss=0.03632 | best_loss=0.03632
Epoch 10/80: current_loss=0.03632 | best_loss=0.03632
Epoch 11/80: current_loss=0.03630 | best_loss=0.03630
Epoch 12/80: current_loss=0.03630 | best_loss=0.03630
Epoch 13/80: current_loss=0.03630 | best_loss=0.03630
Epoch 14/80: current_loss=0.03630 | best_loss=0.03630
Epoch 15/80: current_loss=0.03631 | best_loss=0.03630
Epoch 16/80: current_loss=0.03629 | best_loss=0.03629
Epoch 17/80: current_loss=0.03628 | best_loss=0.03628
Epoch 18/80: current_loss=0.03628 | best_loss=0.03628
Epoch 19/80: current_loss=0.03628 | best_loss=0.03628
Epoch 20/80: current_loss=0.03627 | best_loss=0.03627
Epoch 21/80: current_loss=0.03627 | best_loss=0.03627
Epoch 22/80: current_loss=0.03626 | best_loss=0.03626
Epoch 23/80: current_loss=0.03625 | best_loss=0.03625
Epoch 24/80: current_loss=0.03626 | best_loss=0.03625
Epoch 25/80: current_loss=0.03625 | best_loss=0.03625
Epoch 26/80: current_loss=0.03624 | best_loss=0.03624
Epoch 27/80: current_loss=0.03623 | best_loss=0.03623
Epoch 28/80: current_loss=0.03623 | best_loss=0.03623
Epoch 29/80: current_loss=0.03623 | best_loss=0.03623
Epoch 30/80: current_loss=0.03622 | best_loss=0.03622
Epoch 31/80: current_loss=0.03622 | best_loss=0.03622
Epoch 32/80: current_loss=0.03621 | best_loss=0.03621
Epoch 33/80: current_loss=0.03621 | best_loss=0.03621
Epoch 34/80: current_loss=0.03620 | best_loss=0.03620
Epoch 35/80: current_loss=0.03619 | best_loss=0.03619
Epoch 36/80: current_loss=0.03618 | best_loss=0.03618
Epoch 37/80: current_loss=0.03618 | best_loss=0.03618
Epoch 38/80: current_loss=0.03617 | best_loss=0.03617
Epoch 39/80: current_loss=0.03617 | best_loss=0.03617
Epoch 40/80: current_loss=0.03616 | best_loss=0.03616
Epoch 41/80: current_loss=0.03616 | best_loss=0.03616
Epoch 42/80: current_loss=0.03614 | best_loss=0.03614
Epoch 43/80: current_loss=0.03614 | best_loss=0.03614
Epoch 44/80: current_loss=0.03614 | best_loss=0.03614
Epoch 45/80: current_loss=0.03613 | best_loss=0.03613
Epoch 46/80: current_loss=0.03613 | best_loss=0.03613
Epoch 47/80: current_loss=0.03613 | best_loss=0.03613
Epoch 48/80: current_loss=0.03612 | best_loss=0.03612
Epoch 49/80: current_loss=0.03613 | best_loss=0.03612
Epoch 50/80: current_loss=0.03612 | best_loss=0.03612
Epoch 51/80: current_loss=0.03612 | best_loss=0.03612
Epoch 52/80: current_loss=0.03612 | best_loss=0.03612
Epoch 53/80: current_loss=0.03611 | best_loss=0.03611
Epoch 54/80: current_loss=0.03611 | best_loss=0.03611
Epoch 55/80: current_loss=0.03609 | best_loss=0.03609
Epoch 56/80: current_loss=0.03609 | best_loss=0.03609
Epoch 57/80: current_loss=0.03610 | best_loss=0.03609
Epoch 58/80: current_loss=0.03609 | best_loss=0.03609
Epoch 59/80: current_loss=0.03608 | best_loss=0.03608
Epoch 60/80: current_loss=0.03607 | best_loss=0.03607
Epoch 61/80: current_loss=0.03606 | best_loss=0.03606
Epoch 62/80: current_loss=0.03605 | best_loss=0.03605
Epoch 63/80: current_loss=0.03606 | best_loss=0.03605
Epoch 64/80: current_loss=0.03606 | best_loss=0.03605
Epoch 65/80: current_loss=0.03605 | best_loss=0.03605
Epoch 66/80: current_loss=0.03606 | best_loss=0.03605
Epoch 67/80: current_loss=0.03605 | best_loss=0.03605
Epoch 68/80: current_loss=0.03604 | best_loss=0.03604
Epoch 69/80: current_loss=0.03603 | best_loss=0.03603
Epoch 70/80: current_loss=0.03602 | best_loss=0.03602
Epoch 71/80: current_loss=0.03602 | best_loss=0.03602
Epoch 72/80: current_loss=0.03602 | best_loss=0.03602
Epoch 73/80: current_loss=0.03602 | best_loss=0.03602
Epoch 74/80: current_loss=0.03602 | best_loss=0.03602
Epoch 75/80: current_loss=0.03603 | best_loss=0.03602
Epoch 76/80: current_loss=0.03602 | best_loss=0.03602
Epoch 77/80: current_loss=0.03603 | best_loss=0.03602
Epoch 78/80: current_loss=0.03603 | best_loss=0.03602
Epoch 79/80: current_loss=0.03600 | best_loss=0.03600
      explained_var=0.00467 | mse_loss=0.03578
----------------------------------------------
Average early_stopping_point: 80| avg_exp_var=-0.08540| avg_loss=0.03936
----------------------------------------------


----------------------------------------------
Params for Trial 9
{'learning_rate': 0.1, 'weight_decay': 0.008081203814832131, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=5.57713 | best_loss=5.57713
Epoch 1/80: current_loss=2.34700 | best_loss=2.34700
Epoch 2/80: current_loss=0.91338 | best_loss=0.91338
Epoch 3/80: current_loss=2.01408 | best_loss=0.91338
Epoch 4/80: current_loss=0.09773 | best_loss=0.09773
Epoch 5/80: current_loss=0.23768 | best_loss=0.09773
Epoch 6/80: current_loss=0.18612 | best_loss=0.09773
Epoch 7/80: current_loss=0.08839 | best_loss=0.08839
Epoch 8/80: current_loss=0.06885 | best_loss=0.06885
Epoch 9/80: current_loss=0.06071 | best_loss=0.06071
Epoch 10/80: current_loss=0.24256 | best_loss=0.06071
Epoch 11/80: current_loss=0.12538 | best_loss=0.06071
Epoch 12/80: current_loss=0.07002 | best_loss=0.06071
Epoch 13/80: current_loss=0.05471 | best_loss=0.05471
Epoch 14/80: current_loss=0.06375 | best_loss=0.05471
Epoch 15/80: current_loss=0.73761 | best_loss=0.05471
Epoch 16/80: current_loss=0.04821 | best_loss=0.04821
Epoch 17/80: current_loss=0.27920 | best_loss=0.04821
Epoch 18/80: current_loss=0.06513 | best_loss=0.04821
Epoch 19/80: current_loss=0.06478 | best_loss=0.04821
Epoch 20/80: current_loss=0.12149 | best_loss=0.04821
Epoch 21/80: current_loss=0.25691 | best_loss=0.04821
Epoch 22/80: current_loss=0.27789 | best_loss=0.04821
Epoch 23/80: current_loss=0.93400 | best_loss=0.04821
Epoch 24/80: current_loss=1.12377 | best_loss=0.04821
Epoch 25/80: current_loss=0.04872 | best_loss=0.04821
Epoch 26/80: current_loss=0.07007 | best_loss=0.04821
Epoch 27/80: current_loss=0.08104 | best_loss=0.04821
Epoch 28/80: current_loss=0.23763 | best_loss=0.04821
Epoch 29/80: current_loss=26.85273 | best_loss=0.04821
Epoch 30/80: current_loss=70.47063 | best_loss=0.04821
Epoch 31/80: current_loss=0.59720 | best_loss=0.04821
Epoch 32/80: current_loss=1.32615 | best_loss=0.04821
Epoch 33/80: current_loss=3.87608 | best_loss=0.04821
Epoch 34/80: current_loss=2.89549 | best_loss=0.04821
Epoch 35/80: current_loss=1.08815 | best_loss=0.04821
Epoch 36/80: current_loss=3.01369 | best_loss=0.04821
Early Stopping at epoch 36
      explained_var=-0.01393 | mse_loss=0.04959
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=34.00688 | best_loss=34.00688
Epoch 1/80: current_loss=2.31690 | best_loss=2.31690
Epoch 2/80: current_loss=1.35159 | best_loss=1.35159
Epoch 3/80: current_loss=4.27201 | best_loss=1.35159
Epoch 4/80: current_loss=2.10092 | best_loss=1.35159
Epoch 5/80: current_loss=0.23890 | best_loss=0.23890
Epoch 6/80: current_loss=0.44462 | best_loss=0.23890
Epoch 7/80: current_loss=0.16828 | best_loss=0.16828
Epoch 8/80: current_loss=0.27630 | best_loss=0.16828
Epoch 9/80: current_loss=0.06147 | best_loss=0.06147
Epoch 10/80: current_loss=0.15318 | best_loss=0.06147
Epoch 11/80: current_loss=0.06633 | best_loss=0.06147
Epoch 12/80: current_loss=0.17943 | best_loss=0.06147
Epoch 13/80: current_loss=0.44548 | best_loss=0.06147
Epoch 14/80: current_loss=0.12664 | best_loss=0.06147
Epoch 15/80: current_loss=0.21342 | best_loss=0.06147
Epoch 16/80: current_loss=0.08313 | best_loss=0.06147
Epoch 17/80: current_loss=0.06585 | best_loss=0.06147
Epoch 18/80: current_loss=0.49795 | best_loss=0.06147
Epoch 19/80: current_loss=1.15000 | best_loss=0.06147
Epoch 20/80: current_loss=4.50690 | best_loss=0.06147
Epoch 21/80: current_loss=41.33653 | best_loss=0.06147
Epoch 22/80: current_loss=0.76818 | best_loss=0.06147
Epoch 23/80: current_loss=4.12407 | best_loss=0.06147
Epoch 24/80: current_loss=2.33097 | best_loss=0.06147
Epoch 25/80: current_loss=0.16991 | best_loss=0.06147
Epoch 26/80: current_loss=0.37058 | best_loss=0.06147
Epoch 27/80: current_loss=0.58252 | best_loss=0.06147
Epoch 28/80: current_loss=3.27934 | best_loss=0.06147
Epoch 29/80: current_loss=1.45460 | best_loss=0.06147
Early Stopping at epoch 29
      explained_var=-0.00848 | mse_loss=0.05990
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=3.21608 | best_loss=3.21608
Epoch 1/80: current_loss=2.64795 | best_loss=2.64795
Epoch 2/80: current_loss=3.16444 | best_loss=2.64795
Epoch 3/80: current_loss=1.33816 | best_loss=1.33816
Epoch 4/80: current_loss=2.75455 | best_loss=1.33816
Epoch 5/80: current_loss=0.56569 | best_loss=0.56569
Epoch 6/80: current_loss=0.05722 | best_loss=0.05722
Epoch 7/80: current_loss=0.09913 | best_loss=0.05722
Epoch 8/80: current_loss=0.41268 | best_loss=0.05722
Epoch 9/80: current_loss=0.07207 | best_loss=0.05722
Epoch 10/80: current_loss=0.06390 | best_loss=0.05722
Epoch 11/80: current_loss=0.12917 | best_loss=0.05722
Epoch 12/80: current_loss=0.05339 | best_loss=0.05339
Epoch 13/80: current_loss=0.04677 | best_loss=0.04677
Epoch 14/80: current_loss=0.06012 | best_loss=0.04677
Epoch 15/80: current_loss=0.02929 | best_loss=0.02929
Epoch 16/80: current_loss=0.03850 | best_loss=0.02929
Epoch 17/80: current_loss=0.03149 | best_loss=0.02929
Epoch 18/80: current_loss=0.03097 | best_loss=0.02929
Epoch 19/80: current_loss=0.03461 | best_loss=0.02929
Epoch 20/80: current_loss=0.03039 | best_loss=0.02929
Epoch 21/80: current_loss=0.02902 | best_loss=0.02902
Epoch 22/80: current_loss=0.02805 | best_loss=0.02805
Epoch 23/80: current_loss=0.02777 | best_loss=0.02777
Epoch 24/80: current_loss=0.02872 | best_loss=0.02777
Epoch 25/80: current_loss=0.02906 | best_loss=0.02777
Epoch 26/80: current_loss=0.02877 | best_loss=0.02777
Epoch 27/80: current_loss=0.02971 | best_loss=0.02777
Epoch 28/80: current_loss=0.02809 | best_loss=0.02777
Epoch 29/80: current_loss=0.02861 | best_loss=0.02777
Epoch 30/80: current_loss=0.02839 | best_loss=0.02777
Epoch 31/80: current_loss=0.02815 | best_loss=0.02777
Epoch 32/80: current_loss=0.02839 | best_loss=0.02777
Epoch 33/80: current_loss=0.02835 | best_loss=0.02777
Epoch 34/80: current_loss=0.02816 | best_loss=0.02777
Epoch 35/80: current_loss=0.02837 | best_loss=0.02777
Epoch 36/80: current_loss=0.02819 | best_loss=0.02777
Epoch 37/80: current_loss=0.02847 | best_loss=0.02777
Epoch 38/80: current_loss=0.02826 | best_loss=0.02777
Epoch 39/80: current_loss=0.02826 | best_loss=0.02777
Epoch 40/80: current_loss=0.02810 | best_loss=0.02777
Epoch 41/80: current_loss=0.02823 | best_loss=0.02777
Epoch 42/80: current_loss=0.02833 | best_loss=0.02777
Epoch 43/80: current_loss=0.02838 | best_loss=0.02777
Early Stopping at epoch 43
      explained_var=0.00977 | mse_loss=0.02817
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=12.49168 | best_loss=12.49168
Epoch 1/80: current_loss=1.37713 | best_loss=1.37713
Epoch 2/80: current_loss=3.59155 | best_loss=1.37713
Epoch 3/80: current_loss=1.64338 | best_loss=1.37713
Epoch 4/80: current_loss=0.37161 | best_loss=0.37161
Epoch 5/80: current_loss=0.71564 | best_loss=0.37161
Epoch 6/80: current_loss=0.42402 | best_loss=0.37161
Epoch 7/80: current_loss=0.07180 | best_loss=0.07180
Epoch 8/80: current_loss=0.12072 | best_loss=0.07180
Epoch 9/80: current_loss=0.06049 | best_loss=0.06049
Epoch 10/80: current_loss=0.07253 | best_loss=0.06049
Epoch 11/80: current_loss=0.06680 | best_loss=0.06049
Epoch 12/80: current_loss=0.06330 | best_loss=0.06049
Epoch 13/80: current_loss=0.03545 | best_loss=0.03545
Epoch 14/80: current_loss=0.04485 | best_loss=0.03545
Epoch 15/80: current_loss=0.03464 | best_loss=0.03464
Epoch 16/80: current_loss=0.06515 | best_loss=0.03464
Epoch 17/80: current_loss=0.04512 | best_loss=0.03464
Epoch 18/80: current_loss=0.04782 | best_loss=0.03464
Epoch 19/80: current_loss=0.03757 | best_loss=0.03464
Epoch 20/80: current_loss=0.03306 | best_loss=0.03306
Epoch 21/80: current_loss=0.03736 | best_loss=0.03306
Epoch 22/80: current_loss=0.04354 | best_loss=0.03306
Epoch 23/80: current_loss=0.03336 | best_loss=0.03306
Epoch 24/80: current_loss=0.03547 | best_loss=0.03306
Epoch 25/80: current_loss=0.04812 | best_loss=0.03306
Epoch 26/80: current_loss=0.04769 | best_loss=0.03306
Epoch 27/80: current_loss=0.03667 | best_loss=0.03306
Epoch 28/80: current_loss=0.03260 | best_loss=0.03260
Epoch 29/80: current_loss=0.04024 | best_loss=0.03260
Epoch 30/80: current_loss=0.03440 | best_loss=0.03260
Epoch 31/80: current_loss=0.04342 | best_loss=0.03260
Epoch 32/80: current_loss=0.03739 | best_loss=0.03260
Epoch 33/80: current_loss=0.04064 | best_loss=0.03260
Epoch 34/80: current_loss=0.03816 | best_loss=0.03260
Epoch 35/80: current_loss=0.03550 | best_loss=0.03260
Epoch 36/80: current_loss=0.03499 | best_loss=0.03260
Epoch 37/80: current_loss=0.03559 | best_loss=0.03260
Epoch 38/80: current_loss=0.03503 | best_loss=0.03260
Epoch 39/80: current_loss=0.04231 | best_loss=0.03260
Epoch 40/80: current_loss=0.04256 | best_loss=0.03260
Epoch 41/80: current_loss=0.04195 | best_loss=0.03260
Epoch 42/80: current_loss=0.06314 | best_loss=0.03260
Epoch 43/80: current_loss=0.05051 | best_loss=0.03260
Epoch 44/80: current_loss=0.04877 | best_loss=0.03260
Epoch 45/80: current_loss=0.03396 | best_loss=0.03260
Epoch 46/80: current_loss=0.03313 | best_loss=0.03260
Epoch 47/80: current_loss=0.04079 | best_loss=0.03260
Epoch 48/80: current_loss=0.03363 | best_loss=0.03260
Early Stopping at epoch 48
      explained_var=0.01820 | mse_loss=0.03271
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=1.92900 | best_loss=1.92900
Epoch 1/80: current_loss=0.72095 | best_loss=0.72095
Epoch 2/80: current_loss=4.65436 | best_loss=0.72095
Epoch 3/80: current_loss=0.20749 | best_loss=0.20749
Epoch 4/80: current_loss=0.69140 | best_loss=0.20749
Epoch 5/80: current_loss=0.18935 | best_loss=0.18935
Epoch 6/80: current_loss=0.09078 | best_loss=0.09078
Epoch 7/80: current_loss=0.07406 | best_loss=0.07406
Epoch 8/80: current_loss=0.18265 | best_loss=0.07406
Epoch 9/80: current_loss=0.28204 | best_loss=0.07406
Epoch 10/80: current_loss=0.06206 | best_loss=0.06206
Epoch 11/80: current_loss=0.23797 | best_loss=0.06206
Epoch 12/80: current_loss=0.05133 | best_loss=0.05133
Epoch 13/80: current_loss=0.24158 | best_loss=0.05133
Epoch 14/80: current_loss=0.25114 | best_loss=0.05133
Epoch 15/80: current_loss=0.08885 | best_loss=0.05133
Epoch 16/80: current_loss=0.13039 | best_loss=0.05133
Epoch 17/80: current_loss=0.41226 | best_loss=0.05133
Epoch 18/80: current_loss=3.28972 | best_loss=0.05133
Epoch 19/80: current_loss=73.39223 | best_loss=0.05133
Epoch 20/80: current_loss=4.40338 | best_loss=0.05133
Epoch 21/80: current_loss=2.16201 | best_loss=0.05133
Epoch 22/80: current_loss=1.77319 | best_loss=0.05133
Epoch 23/80: current_loss=10.68978 | best_loss=0.05133
Epoch 24/80: current_loss=0.34715 | best_loss=0.05133
Epoch 25/80: current_loss=1.55165 | best_loss=0.05133
Epoch 26/80: current_loss=0.44090 | best_loss=0.05133
Epoch 27/80: current_loss=1.41907 | best_loss=0.05133
Epoch 28/80: current_loss=0.49869 | best_loss=0.05133
Epoch 29/80: current_loss=0.19634 | best_loss=0.05133
Epoch 30/80: current_loss=1.50976 | best_loss=0.05133
Epoch 31/80: current_loss=0.45090 | best_loss=0.05133
Epoch 32/80: current_loss=0.95008 | best_loss=0.05133
Early Stopping at epoch 32
      explained_var=-0.02143 | mse_loss=0.05065
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=-0.00317| avg_loss=0.04420
----------------------------------------------


----------------------------------------------
Params for Trial 10
{'learning_rate': 0.01, 'weight_decay': 0.0030741327184301173, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.67691 | best_loss=0.67691
Epoch 1/80: current_loss=0.11751 | best_loss=0.11751
Epoch 2/80: current_loss=0.10951 | best_loss=0.10951
Epoch 3/80: current_loss=0.09149 | best_loss=0.09149
Epoch 4/80: current_loss=0.06476 | best_loss=0.06476
Epoch 5/80: current_loss=0.06117 | best_loss=0.06117
Epoch 6/80: current_loss=0.05518 | best_loss=0.05518
Epoch 7/80: current_loss=0.04711 | best_loss=0.04711
Epoch 8/80: current_loss=0.04442 | best_loss=0.04442
Epoch 9/80: current_loss=0.04725 | best_loss=0.04442
Epoch 10/80: current_loss=0.04156 | best_loss=0.04156
Epoch 11/80: current_loss=0.05299 | best_loss=0.04156
Epoch 12/80: current_loss=0.04105 | best_loss=0.04105
Epoch 13/80: current_loss=0.04223 | best_loss=0.04105
Epoch 14/80: current_loss=0.04542 | best_loss=0.04105
Epoch 15/80: current_loss=0.03936 | best_loss=0.03936
Epoch 16/80: current_loss=0.06078 | best_loss=0.03936
Epoch 17/80: current_loss=0.05143 | best_loss=0.03936
Epoch 18/80: current_loss=0.04148 | best_loss=0.03936
Epoch 19/80: current_loss=0.07847 | best_loss=0.03936
Epoch 20/80: current_loss=0.04214 | best_loss=0.03936
Epoch 21/80: current_loss=0.11412 | best_loss=0.03936
Epoch 22/80: current_loss=0.06150 | best_loss=0.03936
Epoch 23/80: current_loss=0.04574 | best_loss=0.03936
Epoch 24/80: current_loss=0.04046 | best_loss=0.03936
Epoch 25/80: current_loss=0.04672 | best_loss=0.03936
Epoch 26/80: current_loss=0.04424 | best_loss=0.03936
Epoch 27/80: current_loss=0.06662 | best_loss=0.03936
Epoch 28/80: current_loss=0.05352 | best_loss=0.03936
Epoch 29/80: current_loss=0.08182 | best_loss=0.03936
Epoch 30/80: current_loss=0.05163 | best_loss=0.03936
Epoch 31/80: current_loss=0.06036 | best_loss=0.03936
Epoch 32/80: current_loss=0.04135 | best_loss=0.03936
Epoch 33/80: current_loss=0.04903 | best_loss=0.03936
Epoch 34/80: current_loss=0.08669 | best_loss=0.03936
Epoch 35/80: current_loss=0.05237 | best_loss=0.03936
Early Stopping at epoch 35
      explained_var=-0.01674 | mse_loss=0.03985
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.20358 | best_loss=0.20358
Epoch 1/80: current_loss=0.17026 | best_loss=0.17026
Epoch 2/80: current_loss=0.29714 | best_loss=0.17026
Epoch 3/80: current_loss=0.11397 | best_loss=0.11397
Epoch 4/80: current_loss=0.08250 | best_loss=0.08250
Epoch 5/80: current_loss=0.06550 | best_loss=0.06550
Epoch 6/80: current_loss=0.04469 | best_loss=0.04469
Epoch 7/80: current_loss=0.05570 | best_loss=0.04469
Epoch 8/80: current_loss=0.11725 | best_loss=0.04469
Epoch 9/80: current_loss=0.06059 | best_loss=0.04469
Epoch 10/80: current_loss=0.04648 | best_loss=0.04469
Epoch 11/80: current_loss=0.05123 | best_loss=0.04469
Epoch 12/80: current_loss=0.04827 | best_loss=0.04469
Epoch 13/80: current_loss=0.04324 | best_loss=0.04324
Epoch 14/80: current_loss=0.05450 | best_loss=0.04324
Epoch 15/80: current_loss=0.04618 | best_loss=0.04324
Epoch 16/80: current_loss=0.04323 | best_loss=0.04323
Epoch 17/80: current_loss=0.05874 | best_loss=0.04323
Epoch 18/80: current_loss=0.04284 | best_loss=0.04284
Epoch 19/80: current_loss=0.05064 | best_loss=0.04284
Epoch 20/80: current_loss=0.06621 | best_loss=0.04284
Epoch 21/80: current_loss=0.06111 | best_loss=0.04284
Epoch 22/80: current_loss=0.05979 | best_loss=0.04284
Epoch 23/80: current_loss=0.04741 | best_loss=0.04284
Epoch 24/80: current_loss=0.04331 | best_loss=0.04284
Epoch 25/80: current_loss=0.04357 | best_loss=0.04284
Epoch 26/80: current_loss=0.04605 | best_loss=0.04284
Epoch 27/80: current_loss=0.04267 | best_loss=0.04267
Epoch 28/80: current_loss=0.04271 | best_loss=0.04267
Epoch 29/80: current_loss=0.04284 | best_loss=0.04267
Epoch 30/80: current_loss=0.04263 | best_loss=0.04263
Epoch 31/80: current_loss=0.04279 | best_loss=0.04263
Epoch 32/80: current_loss=0.04275 | best_loss=0.04263
Epoch 33/80: current_loss=0.04275 | best_loss=0.04263
Epoch 34/80: current_loss=0.04267 | best_loss=0.04263
Epoch 35/80: current_loss=0.04275 | best_loss=0.04263
Epoch 36/80: current_loss=0.04270 | best_loss=0.04263
Epoch 37/80: current_loss=0.04276 | best_loss=0.04263
Epoch 38/80: current_loss=0.04263 | best_loss=0.04263
Epoch 39/80: current_loss=0.04281 | best_loss=0.04263
Epoch 40/80: current_loss=0.04268 | best_loss=0.04263
Epoch 41/80: current_loss=0.04280 | best_loss=0.04263
Epoch 42/80: current_loss=0.04268 | best_loss=0.04263
Epoch 43/80: current_loss=0.04276 | best_loss=0.04263
Epoch 44/80: current_loss=0.04273 | best_loss=0.04263
Epoch 45/80: current_loss=0.04268 | best_loss=0.04263
Epoch 46/80: current_loss=0.04277 | best_loss=0.04263
Epoch 47/80: current_loss=0.04267 | best_loss=0.04263
Epoch 48/80: current_loss=0.04275 | best_loss=0.04263
Epoch 49/80: current_loss=0.04264 | best_loss=0.04263
Epoch 50/80: current_loss=0.04278 | best_loss=0.04263
Epoch 51/80: current_loss=0.04266 | best_loss=0.04263
Epoch 52/80: current_loss=0.04268 | best_loss=0.04263
Epoch 53/80: current_loss=0.04276 | best_loss=0.04263
Epoch 54/80: current_loss=0.04276 | best_loss=0.04263
Epoch 55/80: current_loss=0.04273 | best_loss=0.04263
Epoch 56/80: current_loss=0.04280 | best_loss=0.04263
Epoch 57/80: current_loss=0.04264 | best_loss=0.04263
Epoch 58/80: current_loss=0.04280 | best_loss=0.04263
Early Stopping at epoch 58
      explained_var=0.00000 | mse_loss=0.04123
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.53818 | best_loss=0.53818
Epoch 1/80: current_loss=0.03122 | best_loss=0.03122
Epoch 2/80: current_loss=0.03824 | best_loss=0.03122
Epoch 3/80: current_loss=0.04164 | best_loss=0.03122
Epoch 4/80: current_loss=0.04632 | best_loss=0.03122
Epoch 5/80: current_loss=0.09704 | best_loss=0.03122
Epoch 6/80: current_loss=0.04834 | best_loss=0.03122
Epoch 7/80: current_loss=0.03617 | best_loss=0.03122
Epoch 8/80: current_loss=0.03723 | best_loss=0.03122
Epoch 9/80: current_loss=0.04899 | best_loss=0.03122
Epoch 10/80: current_loss=0.04346 | best_loss=0.03122
Epoch 11/80: current_loss=0.03391 | best_loss=0.03122
Epoch 12/80: current_loss=0.09987 | best_loss=0.03122
Epoch 13/80: current_loss=0.04425 | best_loss=0.03122
Epoch 14/80: current_loss=0.10971 | best_loss=0.03122
Epoch 15/80: current_loss=0.10369 | best_loss=0.03122
Epoch 16/80: current_loss=0.05034 | best_loss=0.03122
Epoch 17/80: current_loss=0.03921 | best_loss=0.03122
Epoch 18/80: current_loss=0.10650 | best_loss=0.03122
Epoch 19/80: current_loss=0.03280 | best_loss=0.03122
Epoch 20/80: current_loss=0.03087 | best_loss=0.03087
Epoch 21/80: current_loss=0.05607 | best_loss=0.03087
Epoch 22/80: current_loss=0.04689 | best_loss=0.03087
Epoch 23/80: current_loss=0.04301 | best_loss=0.03087
Epoch 24/80: current_loss=0.02954 | best_loss=0.02954
Epoch 25/80: current_loss=0.07984 | best_loss=0.02954
Epoch 26/80: current_loss=0.03084 | best_loss=0.02954
Epoch 27/80: current_loss=0.03317 | best_loss=0.02954
Epoch 28/80: current_loss=0.06954 | best_loss=0.02954
Epoch 29/80: current_loss=0.06952 | best_loss=0.02954
Epoch 30/80: current_loss=0.28260 | best_loss=0.02954
Epoch 31/80: current_loss=0.03182 | best_loss=0.02954
Epoch 32/80: current_loss=0.10266 | best_loss=0.02954
Epoch 33/80: current_loss=0.03418 | best_loss=0.02954
Epoch 34/80: current_loss=0.19720 | best_loss=0.02954
Epoch 35/80: current_loss=0.08588 | best_loss=0.02954
Epoch 36/80: current_loss=0.04699 | best_loss=0.02954
Epoch 37/80: current_loss=0.09638 | best_loss=0.02954
Epoch 38/80: current_loss=0.11085 | best_loss=0.02954
Epoch 39/80: current_loss=0.06460 | best_loss=0.02954
Epoch 40/80: current_loss=0.09663 | best_loss=0.02954
Epoch 41/80: current_loss=0.05789 | best_loss=0.02954
Epoch 42/80: current_loss=0.03479 | best_loss=0.02954
Epoch 43/80: current_loss=0.03035 | best_loss=0.02954
Epoch 44/80: current_loss=0.03127 | best_loss=0.02954
Early Stopping at epoch 44
      explained_var=-0.05083 | mse_loss=0.02994
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.09991 | best_loss=0.09991
Epoch 1/80: current_loss=0.05550 | best_loss=0.05550
Epoch 2/80: current_loss=0.03821 | best_loss=0.03821
Epoch 3/80: current_loss=0.06306 | best_loss=0.03821
Epoch 4/80: current_loss=0.03727 | best_loss=0.03727
Epoch 5/80: current_loss=0.04151 | best_loss=0.03727
Epoch 6/80: current_loss=0.05967 | best_loss=0.03727
Epoch 7/80: current_loss=0.06679 | best_loss=0.03727
Epoch 8/80: current_loss=0.04167 | best_loss=0.03727
Epoch 9/80: current_loss=0.03661 | best_loss=0.03661
Epoch 10/80: current_loss=0.04367 | best_loss=0.03661
Epoch 11/80: current_loss=0.03348 | best_loss=0.03348
Epoch 12/80: current_loss=0.03293 | best_loss=0.03293
Epoch 13/80: current_loss=0.03286 | best_loss=0.03286
Epoch 14/80: current_loss=0.03297 | best_loss=0.03286
Epoch 15/80: current_loss=0.03291 | best_loss=0.03286
Epoch 16/80: current_loss=0.03284 | best_loss=0.03284
Epoch 17/80: current_loss=0.03284 | best_loss=0.03284
Epoch 18/80: current_loss=0.03283 | best_loss=0.03283
Epoch 19/80: current_loss=0.03317 | best_loss=0.03283
Epoch 20/80: current_loss=0.03290 | best_loss=0.03283
Epoch 21/80: current_loss=0.03297 | best_loss=0.03283
Epoch 22/80: current_loss=0.03302 | best_loss=0.03283
Epoch 23/80: current_loss=0.03294 | best_loss=0.03283
Epoch 24/80: current_loss=0.03295 | best_loss=0.03283
Epoch 25/80: current_loss=0.03310 | best_loss=0.03283
Epoch 26/80: current_loss=0.03286 | best_loss=0.03283
Epoch 27/80: current_loss=0.03284 | best_loss=0.03283
Epoch 28/80: current_loss=0.03307 | best_loss=0.03283
Epoch 29/80: current_loss=0.03288 | best_loss=0.03283
Epoch 30/80: current_loss=0.03318 | best_loss=0.03283
Epoch 31/80: current_loss=0.03286 | best_loss=0.03283
Epoch 32/80: current_loss=0.03284 | best_loss=0.03283
Epoch 33/80: current_loss=0.03286 | best_loss=0.03283
Epoch 34/80: current_loss=0.03295 | best_loss=0.03283
Epoch 35/80: current_loss=0.03291 | best_loss=0.03283
Epoch 36/80: current_loss=0.03285 | best_loss=0.03283
Epoch 37/80: current_loss=0.03288 | best_loss=0.03283
Epoch 38/80: current_loss=0.03283 | best_loss=0.03283
Early Stopping at epoch 38
      explained_var=0.00000 | mse_loss=0.03260
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.10118 | best_loss=0.10118
Epoch 1/80: current_loss=0.06359 | best_loss=0.06359
Epoch 2/80: current_loss=0.04265 | best_loss=0.04265
Epoch 3/80: current_loss=0.09217 | best_loss=0.04265
Epoch 4/80: current_loss=0.08049 | best_loss=0.04265
Epoch 5/80: current_loss=0.11997 | best_loss=0.04265
Epoch 6/80: current_loss=0.06208 | best_loss=0.04265
Epoch 7/80: current_loss=0.19879 | best_loss=0.04265
Epoch 8/80: current_loss=0.06157 | best_loss=0.04265
Epoch 9/80: current_loss=0.16279 | best_loss=0.04265
Epoch 10/80: current_loss=0.07642 | best_loss=0.04265
Epoch 11/80: current_loss=0.17389 | best_loss=0.04265
Epoch 12/80: current_loss=0.13539 | best_loss=0.04265
Epoch 13/80: current_loss=0.06933 | best_loss=0.04265
Epoch 14/80: current_loss=0.07421 | best_loss=0.04265
Epoch 15/80: current_loss=0.08849 | best_loss=0.04265
Epoch 16/80: current_loss=0.04252 | best_loss=0.04252
Epoch 17/80: current_loss=0.46902 | best_loss=0.04252
Epoch 18/80: current_loss=0.03787 | best_loss=0.03787
Epoch 19/80: current_loss=0.06238 | best_loss=0.03787
Epoch 20/80: current_loss=0.05229 | best_loss=0.03787
Epoch 21/80: current_loss=0.21244 | best_loss=0.03787
Epoch 22/80: current_loss=0.04031 | best_loss=0.03787
Epoch 23/80: current_loss=0.04679 | best_loss=0.03787
Epoch 24/80: current_loss=0.12855 | best_loss=0.03787
Epoch 25/80: current_loss=0.04274 | best_loss=0.03787
Epoch 26/80: current_loss=0.10306 | best_loss=0.03787
Epoch 27/80: current_loss=0.08935 | best_loss=0.03787
Epoch 28/80: current_loss=0.06058 | best_loss=0.03787
Epoch 29/80: current_loss=0.05595 | best_loss=0.03787
Epoch 30/80: current_loss=0.09668 | best_loss=0.03787
Epoch 31/80: current_loss=0.04928 | best_loss=0.03787
Epoch 32/80: current_loss=0.10282 | best_loss=0.03787
Epoch 33/80: current_loss=0.10960 | best_loss=0.03787
Epoch 34/80: current_loss=0.06073 | best_loss=0.03787
Epoch 35/80: current_loss=0.04547 | best_loss=0.03787
Epoch 36/80: current_loss=0.05916 | best_loss=0.03787
Epoch 37/80: current_loss=0.09851 | best_loss=0.03787
Epoch 38/80: current_loss=0.07223 | best_loss=0.03787
Early Stopping at epoch 38
      explained_var=-0.02996 | mse_loss=0.03733
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=-0.01951| avg_loss=0.03619
----------------------------------------------


----------------------------------------------
Params for Trial 11
{'learning_rate': 0.0001, 'weight_decay': 9.241814952461722e-05, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.08575 | best_loss=0.08575
Epoch 1/80: current_loss=0.05498 | best_loss=0.05498
Epoch 2/80: current_loss=0.05083 | best_loss=0.05083
Epoch 3/80: current_loss=0.04858 | best_loss=0.04858
Epoch 4/80: current_loss=0.04739 | best_loss=0.04739
Epoch 5/80: current_loss=0.04565 | best_loss=0.04565
Epoch 6/80: current_loss=0.04464 | best_loss=0.04464
Epoch 7/80: current_loss=0.04379 | best_loss=0.04379
Epoch 8/80: current_loss=0.04276 | best_loss=0.04276
Epoch 9/80: current_loss=0.04224 | best_loss=0.04224
Epoch 10/80: current_loss=0.04177 | best_loss=0.04177
Epoch 11/80: current_loss=0.04119 | best_loss=0.04119
Epoch 12/80: current_loss=0.04079 | best_loss=0.04079
Epoch 13/80: current_loss=0.04051 | best_loss=0.04051
Epoch 14/80: current_loss=0.04041 | best_loss=0.04041
Epoch 15/80: current_loss=0.04005 | best_loss=0.04005
Epoch 16/80: current_loss=0.04028 | best_loss=0.04005
Epoch 17/80: current_loss=0.03953 | best_loss=0.03953
Epoch 18/80: current_loss=0.03928 | best_loss=0.03928
Epoch 19/80: current_loss=0.03946 | best_loss=0.03928
Epoch 20/80: current_loss=0.03905 | best_loss=0.03905
Epoch 21/80: current_loss=0.03897 | best_loss=0.03897
Epoch 22/80: current_loss=0.03864 | best_loss=0.03864
Epoch 23/80: current_loss=0.03868 | best_loss=0.03864
Epoch 24/80: current_loss=0.03862 | best_loss=0.03862
Epoch 25/80: current_loss=0.03840 | best_loss=0.03840
Epoch 26/80: current_loss=0.03829 | best_loss=0.03829
Epoch 27/80: current_loss=0.03845 | best_loss=0.03829
Epoch 28/80: current_loss=0.03836 | best_loss=0.03829
Epoch 29/80: current_loss=0.03853 | best_loss=0.03829
Epoch 30/80: current_loss=0.03839 | best_loss=0.03829
Epoch 31/80: current_loss=0.03808 | best_loss=0.03808
Epoch 32/80: current_loss=0.04006 | best_loss=0.03808
Epoch 33/80: current_loss=0.03810 | best_loss=0.03808
Epoch 34/80: current_loss=0.03796 | best_loss=0.03796
Epoch 35/80: current_loss=0.03787 | best_loss=0.03787
Epoch 36/80: current_loss=0.03802 | best_loss=0.03787
Epoch 37/80: current_loss=0.03787 | best_loss=0.03787
Epoch 38/80: current_loss=0.03790 | best_loss=0.03787
Epoch 39/80: current_loss=0.03786 | best_loss=0.03786
Epoch 40/80: current_loss=0.03772 | best_loss=0.03772
Epoch 41/80: current_loss=0.03769 | best_loss=0.03769
Epoch 42/80: current_loss=0.03770 | best_loss=0.03769
Epoch 43/80: current_loss=0.03778 | best_loss=0.03769
Epoch 44/80: current_loss=0.03807 | best_loss=0.03769
Epoch 45/80: current_loss=0.03758 | best_loss=0.03758
Epoch 46/80: current_loss=0.03758 | best_loss=0.03758
Epoch 47/80: current_loss=0.03757 | best_loss=0.03757
Epoch 48/80: current_loss=0.03793 | best_loss=0.03757
Epoch 49/80: current_loss=0.03760 | best_loss=0.03757
Epoch 50/80: current_loss=0.03765 | best_loss=0.03757
Epoch 51/80: current_loss=0.03757 | best_loss=0.03757
Epoch 52/80: current_loss=0.03764 | best_loss=0.03757
Epoch 53/80: current_loss=0.03778 | best_loss=0.03757
Epoch 54/80: current_loss=0.03759 | best_loss=0.03757
Epoch 55/80: current_loss=0.03757 | best_loss=0.03757
Epoch 56/80: current_loss=0.03752 | best_loss=0.03752
Epoch 57/80: current_loss=0.03791 | best_loss=0.03752
Epoch 58/80: current_loss=0.03806 | best_loss=0.03752
Epoch 59/80: current_loss=0.03768 | best_loss=0.03752
Epoch 60/80: current_loss=0.03768 | best_loss=0.03752
Epoch 61/80: current_loss=0.03801 | best_loss=0.03752
Epoch 62/80: current_loss=0.03785 | best_loss=0.03752
Epoch 63/80: current_loss=0.03781 | best_loss=0.03752
Epoch 64/80: current_loss=0.03748 | best_loss=0.03748
Epoch 65/80: current_loss=0.03755 | best_loss=0.03748
Epoch 66/80: current_loss=0.03745 | best_loss=0.03745
Epoch 67/80: current_loss=0.03757 | best_loss=0.03745
Epoch 68/80: current_loss=0.03738 | best_loss=0.03738
Epoch 69/80: current_loss=0.03738 | best_loss=0.03738
Epoch 70/80: current_loss=0.03769 | best_loss=0.03738
Epoch 71/80: current_loss=0.03733 | best_loss=0.03733
Epoch 72/80: current_loss=0.03802 | best_loss=0.03733
Epoch 73/80: current_loss=0.03735 | best_loss=0.03733
Epoch 74/80: current_loss=0.03739 | best_loss=0.03733
Epoch 75/80: current_loss=0.03745 | best_loss=0.03733
Epoch 76/80: current_loss=0.03727 | best_loss=0.03727
Epoch 77/80: current_loss=0.03777 | best_loss=0.03727
Epoch 78/80: current_loss=0.03736 | best_loss=0.03727
Epoch 79/80: current_loss=0.03801 | best_loss=0.03727
      explained_var=0.02526 | mse_loss=0.03819
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04139 | best_loss=0.04139
Epoch 1/80: current_loss=0.04152 | best_loss=0.04139
Epoch 2/80: current_loss=0.04179 | best_loss=0.04139
Epoch 3/80: current_loss=0.04162 | best_loss=0.04139
Epoch 4/80: current_loss=0.04154 | best_loss=0.04139
Epoch 5/80: current_loss=0.04161 | best_loss=0.04139
Epoch 6/80: current_loss=0.04151 | best_loss=0.04139
Epoch 7/80: current_loss=0.04149 | best_loss=0.04139
Epoch 8/80: current_loss=0.04188 | best_loss=0.04139
Epoch 9/80: current_loss=0.04177 | best_loss=0.04139
Epoch 10/80: current_loss=0.04167 | best_loss=0.04139
Epoch 11/80: current_loss=0.04158 | best_loss=0.04139
Epoch 12/80: current_loss=0.04166 | best_loss=0.04139
Epoch 13/80: current_loss=0.04139 | best_loss=0.04139
Epoch 14/80: current_loss=0.04145 | best_loss=0.04139
Epoch 15/80: current_loss=0.04158 | best_loss=0.04139
Epoch 16/80: current_loss=0.04152 | best_loss=0.04139
Epoch 17/80: current_loss=0.04139 | best_loss=0.04139
Epoch 18/80: current_loss=0.04137 | best_loss=0.04137
Epoch 19/80: current_loss=0.04145 | best_loss=0.04137
Epoch 20/80: current_loss=0.04168 | best_loss=0.04137
Epoch 21/80: current_loss=0.04219 | best_loss=0.04137
Epoch 22/80: current_loss=0.04153 | best_loss=0.04137
Epoch 23/80: current_loss=0.04148 | best_loss=0.04137
Epoch 24/80: current_loss=0.04150 | best_loss=0.04137
Epoch 25/80: current_loss=0.04151 | best_loss=0.04137
Epoch 26/80: current_loss=0.04161 | best_loss=0.04137
Epoch 27/80: current_loss=0.04206 | best_loss=0.04137
Epoch 28/80: current_loss=0.04161 | best_loss=0.04137
Epoch 29/80: current_loss=0.04153 | best_loss=0.04137
Epoch 30/80: current_loss=0.04167 | best_loss=0.04137
Epoch 31/80: current_loss=0.04140 | best_loss=0.04137
Epoch 32/80: current_loss=0.04164 | best_loss=0.04137
Epoch 33/80: current_loss=0.04150 | best_loss=0.04137
Epoch 34/80: current_loss=0.04157 | best_loss=0.04137
Epoch 35/80: current_loss=0.04153 | best_loss=0.04137
Epoch 36/80: current_loss=0.04190 | best_loss=0.04137
Epoch 37/80: current_loss=0.04161 | best_loss=0.04137
Epoch 38/80: current_loss=0.04173 | best_loss=0.04137
Early Stopping at epoch 38
      explained_var=0.03263 | mse_loss=0.03990
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02902 | best_loss=0.02902
Epoch 1/80: current_loss=0.02829 | best_loss=0.02829
Epoch 2/80: current_loss=0.02874 | best_loss=0.02829
Epoch 3/80: current_loss=0.02888 | best_loss=0.02829
Epoch 4/80: current_loss=0.02845 | best_loss=0.02829
Epoch 5/80: current_loss=0.02828 | best_loss=0.02828
Epoch 6/80: current_loss=0.02907 | best_loss=0.02828
Epoch 7/80: current_loss=0.02811 | best_loss=0.02811
Epoch 8/80: current_loss=0.02856 | best_loss=0.02811
Epoch 9/80: current_loss=0.02798 | best_loss=0.02798
Epoch 10/80: current_loss=0.02850 | best_loss=0.02798
Epoch 11/80: current_loss=0.02826 | best_loss=0.02798
Epoch 12/80: current_loss=0.02915 | best_loss=0.02798
Epoch 13/80: current_loss=0.02837 | best_loss=0.02798
Epoch 14/80: current_loss=0.02827 | best_loss=0.02798
Epoch 15/80: current_loss=0.02824 | best_loss=0.02798
Epoch 16/80: current_loss=0.02844 | best_loss=0.02798
Epoch 17/80: current_loss=0.02814 | best_loss=0.02798
Epoch 18/80: current_loss=0.02853 | best_loss=0.02798
Epoch 19/80: current_loss=0.02820 | best_loss=0.02798
Epoch 20/80: current_loss=0.02860 | best_loss=0.02798
Epoch 21/80: current_loss=0.02852 | best_loss=0.02798
Epoch 22/80: current_loss=0.02824 | best_loss=0.02798
Epoch 23/80: current_loss=0.02805 | best_loss=0.02798
Epoch 24/80: current_loss=0.02843 | best_loss=0.02798
Epoch 25/80: current_loss=0.02832 | best_loss=0.02798
Epoch 26/80: current_loss=0.02887 | best_loss=0.02798
Epoch 27/80: current_loss=0.02812 | best_loss=0.02798
Epoch 28/80: current_loss=0.02887 | best_loss=0.02798
Epoch 29/80: current_loss=0.02830 | best_loss=0.02798
Early Stopping at epoch 29
      explained_var=-0.00269 | mse_loss=0.02846
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03346 | best_loss=0.03346
Epoch 1/80: current_loss=0.03317 | best_loss=0.03317
Epoch 2/80: current_loss=0.03366 | best_loss=0.03317
Epoch 3/80: current_loss=0.03303 | best_loss=0.03303
Epoch 4/80: current_loss=0.03313 | best_loss=0.03303
Epoch 5/80: current_loss=0.03295 | best_loss=0.03295
Epoch 6/80: current_loss=0.03308 | best_loss=0.03295
Epoch 7/80: current_loss=0.03337 | best_loss=0.03295
Epoch 8/80: current_loss=0.03296 | best_loss=0.03295
Epoch 9/80: current_loss=0.03298 | best_loss=0.03295
Epoch 10/80: current_loss=0.03291 | best_loss=0.03291
Epoch 11/80: current_loss=0.03295 | best_loss=0.03291
Epoch 12/80: current_loss=0.03306 | best_loss=0.03291
Epoch 13/80: current_loss=0.03318 | best_loss=0.03291
Epoch 14/80: current_loss=0.03335 | best_loss=0.03291
Epoch 15/80: current_loss=0.03334 | best_loss=0.03291
Epoch 16/80: current_loss=0.03321 | best_loss=0.03291
Epoch 17/80: current_loss=0.03371 | best_loss=0.03291
Epoch 18/80: current_loss=0.03333 | best_loss=0.03291
Epoch 19/80: current_loss=0.03338 | best_loss=0.03291
Epoch 20/80: current_loss=0.03346 | best_loss=0.03291
Epoch 21/80: current_loss=0.03341 | best_loss=0.03291
Epoch 22/80: current_loss=0.03332 | best_loss=0.03291
Epoch 23/80: current_loss=0.03340 | best_loss=0.03291
Epoch 24/80: current_loss=0.03344 | best_loss=0.03291
Epoch 25/80: current_loss=0.03356 | best_loss=0.03291
Epoch 26/80: current_loss=0.03344 | best_loss=0.03291
Epoch 27/80: current_loss=0.03371 | best_loss=0.03291
Epoch 28/80: current_loss=0.03329 | best_loss=0.03291
Epoch 29/80: current_loss=0.03335 | best_loss=0.03291
Epoch 30/80: current_loss=0.03327 | best_loss=0.03291
Early Stopping at epoch 30
      explained_var=-0.00745 | mse_loss=0.03287
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03532 | best_loss=0.03532
Epoch 1/80: current_loss=0.03561 | best_loss=0.03532
Epoch 2/80: current_loss=0.03560 | best_loss=0.03532
Epoch 3/80: current_loss=0.03544 | best_loss=0.03532
Epoch 4/80: current_loss=0.03544 | best_loss=0.03532
Epoch 5/80: current_loss=0.03543 | best_loss=0.03532
Epoch 6/80: current_loss=0.03542 | best_loss=0.03532
Epoch 7/80: current_loss=0.03543 | best_loss=0.03532
Epoch 8/80: current_loss=0.03535 | best_loss=0.03532
Epoch 9/80: current_loss=0.03538 | best_loss=0.03532
Epoch 10/80: current_loss=0.03543 | best_loss=0.03532
Epoch 11/80: current_loss=0.03537 | best_loss=0.03532
Epoch 12/80: current_loss=0.03546 | best_loss=0.03532
Epoch 13/80: current_loss=0.03544 | best_loss=0.03532
Epoch 14/80: current_loss=0.03544 | best_loss=0.03532
Epoch 15/80: current_loss=0.03541 | best_loss=0.03532
Epoch 16/80: current_loss=0.03543 | best_loss=0.03532
Epoch 17/80: current_loss=0.03540 | best_loss=0.03532
Epoch 18/80: current_loss=0.03537 | best_loss=0.03532
Epoch 19/80: current_loss=0.03537 | best_loss=0.03532
Epoch 20/80: current_loss=0.03556 | best_loss=0.03532
Early Stopping at epoch 20
      explained_var=0.02058 | mse_loss=0.03514
----------------------------------------------
Average early_stopping_point: 23| avg_exp_var=0.01367| avg_loss=0.03491
----------------------------------------------


----------------------------------------------
Params for Trial 12
{'learning_rate': 0.0001, 'weight_decay': 5.3093270404624407e-05, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04703 | best_loss=0.04703
Epoch 1/80: current_loss=0.04361 | best_loss=0.04361
Epoch 2/80: current_loss=0.04129 | best_loss=0.04129
Epoch 3/80: current_loss=0.04102 | best_loss=0.04102
Epoch 4/80: current_loss=0.04029 | best_loss=0.04029
Epoch 5/80: current_loss=0.03990 | best_loss=0.03990
Epoch 6/80: current_loss=0.04002 | best_loss=0.03990
Epoch 7/80: current_loss=0.03943 | best_loss=0.03943
Epoch 8/80: current_loss=0.03918 | best_loss=0.03918
Epoch 9/80: current_loss=0.03906 | best_loss=0.03906
Epoch 10/80: current_loss=0.03899 | best_loss=0.03899
Epoch 11/80: current_loss=0.03880 | best_loss=0.03880
Epoch 12/80: current_loss=0.03873 | best_loss=0.03873
Epoch 13/80: current_loss=0.03873 | best_loss=0.03873
Epoch 14/80: current_loss=0.03835 | best_loss=0.03835
Epoch 15/80: current_loss=0.03832 | best_loss=0.03832
Epoch 16/80: current_loss=0.03880 | best_loss=0.03832
Epoch 17/80: current_loss=0.03823 | best_loss=0.03823
Epoch 18/80: current_loss=0.03813 | best_loss=0.03813
Epoch 19/80: current_loss=0.03808 | best_loss=0.03808
Epoch 20/80: current_loss=0.03803 | best_loss=0.03803
Epoch 21/80: current_loss=0.03787 | best_loss=0.03787
Epoch 22/80: current_loss=0.03778 | best_loss=0.03778
Epoch 23/80: current_loss=0.03800 | best_loss=0.03778
Epoch 24/80: current_loss=0.03770 | best_loss=0.03770
Epoch 25/80: current_loss=0.03780 | best_loss=0.03770
Epoch 26/80: current_loss=0.03780 | best_loss=0.03770
Epoch 27/80: current_loss=0.03776 | best_loss=0.03770
Epoch 28/80: current_loss=0.03771 | best_loss=0.03770
Epoch 29/80: current_loss=0.03779 | best_loss=0.03770
Epoch 30/80: current_loss=0.03791 | best_loss=0.03770
Epoch 31/80: current_loss=0.03782 | best_loss=0.03770
Epoch 32/80: current_loss=0.03798 | best_loss=0.03770
Epoch 33/80: current_loss=0.03781 | best_loss=0.03770
Epoch 34/80: current_loss=0.03851 | best_loss=0.03770
Epoch 35/80: current_loss=0.03775 | best_loss=0.03770
Epoch 36/80: current_loss=0.03785 | best_loss=0.03770
Epoch 37/80: current_loss=0.03772 | best_loss=0.03770
Epoch 38/80: current_loss=0.03764 | best_loss=0.03764
Epoch 39/80: current_loss=0.03799 | best_loss=0.03764
Epoch 40/80: current_loss=0.03768 | best_loss=0.03764
Epoch 41/80: current_loss=0.03773 | best_loss=0.03764
Epoch 42/80: current_loss=0.03761 | best_loss=0.03761
Epoch 43/80: current_loss=0.03762 | best_loss=0.03761
Epoch 44/80: current_loss=0.03769 | best_loss=0.03761
Epoch 45/80: current_loss=0.03761 | best_loss=0.03761
Epoch 46/80: current_loss=0.03760 | best_loss=0.03760
Epoch 47/80: current_loss=0.03762 | best_loss=0.03760
Epoch 48/80: current_loss=0.03767 | best_loss=0.03760
Epoch 49/80: current_loss=0.03770 | best_loss=0.03760
Epoch 50/80: current_loss=0.03756 | best_loss=0.03756
Epoch 51/80: current_loss=0.03790 | best_loss=0.03756
Epoch 52/80: current_loss=0.03759 | best_loss=0.03756
Epoch 53/80: current_loss=0.03749 | best_loss=0.03749
Epoch 54/80: current_loss=0.03803 | best_loss=0.03749
Epoch 55/80: current_loss=0.03743 | best_loss=0.03743
Epoch 56/80: current_loss=0.03772 | best_loss=0.03743
Epoch 57/80: current_loss=0.03753 | best_loss=0.03743
Epoch 58/80: current_loss=0.03741 | best_loss=0.03741
Epoch 59/80: current_loss=0.03770 | best_loss=0.03741
Epoch 60/80: current_loss=0.03753 | best_loss=0.03741
Epoch 61/80: current_loss=0.03767 | best_loss=0.03741
Epoch 62/80: current_loss=0.03755 | best_loss=0.03741
Epoch 63/80: current_loss=0.03761 | best_loss=0.03741
Epoch 64/80: current_loss=0.03750 | best_loss=0.03741
Epoch 65/80: current_loss=0.03760 | best_loss=0.03741
Epoch 66/80: current_loss=0.03745 | best_loss=0.03741
Epoch 67/80: current_loss=0.03740 | best_loss=0.03740
Epoch 68/80: current_loss=0.03763 | best_loss=0.03740
Epoch 69/80: current_loss=0.03740 | best_loss=0.03740
Epoch 70/80: current_loss=0.03743 | best_loss=0.03740
Epoch 71/80: current_loss=0.03762 | best_loss=0.03740
Epoch 72/80: current_loss=0.03751 | best_loss=0.03740
Epoch 73/80: current_loss=0.03758 | best_loss=0.03740
Epoch 74/80: current_loss=0.03772 | best_loss=0.03740
Epoch 75/80: current_loss=0.03751 | best_loss=0.03740
Epoch 76/80: current_loss=0.03780 | best_loss=0.03740
Epoch 77/80: current_loss=0.03751 | best_loss=0.03740
Epoch 78/80: current_loss=0.03747 | best_loss=0.03740
Epoch 79/80: current_loss=0.03754 | best_loss=0.03740
      explained_var=0.02321 | mse_loss=0.03834
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04190 | best_loss=0.04190
Epoch 1/80: current_loss=0.04126 | best_loss=0.04126
Epoch 2/80: current_loss=0.04163 | best_loss=0.04126
Epoch 3/80: current_loss=0.04135 | best_loss=0.04126
Epoch 4/80: current_loss=0.04149 | best_loss=0.04126
Epoch 5/80: current_loss=0.04149 | best_loss=0.04126
Epoch 6/80: current_loss=0.04162 | best_loss=0.04126
Epoch 7/80: current_loss=0.04148 | best_loss=0.04126
Epoch 8/80: current_loss=0.04145 | best_loss=0.04126
Epoch 9/80: current_loss=0.04167 | best_loss=0.04126
Epoch 10/80: current_loss=0.04145 | best_loss=0.04126
Epoch 11/80: current_loss=0.04169 | best_loss=0.04126
Epoch 12/80: current_loss=0.04132 | best_loss=0.04126
Epoch 13/80: current_loss=0.04133 | best_loss=0.04126
Epoch 14/80: current_loss=0.04151 | best_loss=0.04126
Epoch 15/80: current_loss=0.04126 | best_loss=0.04126
Epoch 16/80: current_loss=0.04149 | best_loss=0.04126
Epoch 17/80: current_loss=0.04157 | best_loss=0.04126
Epoch 18/80: current_loss=0.04123 | best_loss=0.04123
Epoch 19/80: current_loss=0.04182 | best_loss=0.04123
Epoch 20/80: current_loss=0.04130 | best_loss=0.04123
Epoch 21/80: current_loss=0.04142 | best_loss=0.04123
Epoch 22/80: current_loss=0.04157 | best_loss=0.04123
Epoch 23/80: current_loss=0.04167 | best_loss=0.04123
Epoch 24/80: current_loss=0.04161 | best_loss=0.04123
Epoch 25/80: current_loss=0.04146 | best_loss=0.04123
Epoch 26/80: current_loss=0.04146 | best_loss=0.04123
Epoch 27/80: current_loss=0.04154 | best_loss=0.04123
Epoch 28/80: current_loss=0.04124 | best_loss=0.04123
Epoch 29/80: current_loss=0.04150 | best_loss=0.04123
Epoch 30/80: current_loss=0.04117 | best_loss=0.04117
Epoch 31/80: current_loss=0.04113 | best_loss=0.04113
Epoch 32/80: current_loss=0.04144 | best_loss=0.04113
Epoch 33/80: current_loss=0.04144 | best_loss=0.04113
Epoch 34/80: current_loss=0.04162 | best_loss=0.04113
Epoch 35/80: current_loss=0.04149 | best_loss=0.04113
Epoch 36/80: current_loss=0.04142 | best_loss=0.04113
Epoch 37/80: current_loss=0.04148 | best_loss=0.04113
Epoch 38/80: current_loss=0.04178 | best_loss=0.04113
Epoch 39/80: current_loss=0.04153 | best_loss=0.04113
Epoch 40/80: current_loss=0.04153 | best_loss=0.04113
Epoch 41/80: current_loss=0.04170 | best_loss=0.04113
Epoch 42/80: current_loss=0.04142 | best_loss=0.04113
Epoch 43/80: current_loss=0.04180 | best_loss=0.04113
Epoch 44/80: current_loss=0.04174 | best_loss=0.04113
Epoch 45/80: current_loss=0.04163 | best_loss=0.04113
Epoch 46/80: current_loss=0.04155 | best_loss=0.04113
Epoch 47/80: current_loss=0.04149 | best_loss=0.04113
Epoch 48/80: current_loss=0.04148 | best_loss=0.04113
Epoch 49/80: current_loss=0.04143 | best_loss=0.04113
Epoch 50/80: current_loss=0.04136 | best_loss=0.04113
Epoch 51/80: current_loss=0.04135 | best_loss=0.04113
Early Stopping at epoch 51
      explained_var=0.03889 | mse_loss=0.03965
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02840 | best_loss=0.02840
Epoch 1/80: current_loss=0.02812 | best_loss=0.02812
Epoch 2/80: current_loss=0.02851 | best_loss=0.02812
Epoch 3/80: current_loss=0.02795 | best_loss=0.02795
Epoch 4/80: current_loss=0.02845 | best_loss=0.02795
Epoch 5/80: current_loss=0.02811 | best_loss=0.02795
Epoch 6/80: current_loss=0.02810 | best_loss=0.02795
Epoch 7/80: current_loss=0.02828 | best_loss=0.02795
Epoch 8/80: current_loss=0.02815 | best_loss=0.02795
Epoch 9/80: current_loss=0.02831 | best_loss=0.02795
Epoch 10/80: current_loss=0.02822 | best_loss=0.02795
Epoch 11/80: current_loss=0.02821 | best_loss=0.02795
Epoch 12/80: current_loss=0.02857 | best_loss=0.02795
Epoch 13/80: current_loss=0.02840 | best_loss=0.02795
Epoch 14/80: current_loss=0.02812 | best_loss=0.02795
Epoch 15/80: current_loss=0.02892 | best_loss=0.02795
Epoch 16/80: current_loss=0.02836 | best_loss=0.02795
Epoch 17/80: current_loss=0.02899 | best_loss=0.02795
Epoch 18/80: current_loss=0.02817 | best_loss=0.02795
Epoch 19/80: current_loss=0.02849 | best_loss=0.02795
Epoch 20/80: current_loss=0.02831 | best_loss=0.02795
Epoch 21/80: current_loss=0.02823 | best_loss=0.02795
Epoch 22/80: current_loss=0.02827 | best_loss=0.02795
Epoch 23/80: current_loss=0.02834 | best_loss=0.02795
Early Stopping at epoch 23
      explained_var=-0.00062 | mse_loss=0.02839
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03324 | best_loss=0.03324
Epoch 1/80: current_loss=0.03308 | best_loss=0.03308
Epoch 2/80: current_loss=0.03315 | best_loss=0.03308
Epoch 3/80: current_loss=0.03317 | best_loss=0.03308
Epoch 4/80: current_loss=0.03314 | best_loss=0.03308
Epoch 5/80: current_loss=0.03332 | best_loss=0.03308
Epoch 6/80: current_loss=0.03320 | best_loss=0.03308
Epoch 7/80: current_loss=0.03313 | best_loss=0.03308
Epoch 8/80: current_loss=0.03312 | best_loss=0.03308
Epoch 9/80: current_loss=0.03318 | best_loss=0.03308
Epoch 10/80: current_loss=0.03309 | best_loss=0.03308
Epoch 11/80: current_loss=0.03319 | best_loss=0.03308
Epoch 12/80: current_loss=0.03306 | best_loss=0.03306
Epoch 13/80: current_loss=0.03317 | best_loss=0.03306
Epoch 14/80: current_loss=0.03303 | best_loss=0.03303
Epoch 15/80: current_loss=0.03293 | best_loss=0.03293
Epoch 16/80: current_loss=0.03314 | best_loss=0.03293
Epoch 17/80: current_loss=0.03297 | best_loss=0.03293
Epoch 18/80: current_loss=0.03330 | best_loss=0.03293
Epoch 19/80: current_loss=0.03309 | best_loss=0.03293
Epoch 20/80: current_loss=0.03316 | best_loss=0.03293
Epoch 21/80: current_loss=0.03314 | best_loss=0.03293
Epoch 22/80: current_loss=0.03312 | best_loss=0.03293
Epoch 23/80: current_loss=0.03318 | best_loss=0.03293
Epoch 24/80: current_loss=0.03301 | best_loss=0.03293
Epoch 25/80: current_loss=0.03326 | best_loss=0.03293
Epoch 26/80: current_loss=0.03301 | best_loss=0.03293
Epoch 27/80: current_loss=0.03318 | best_loss=0.03293
Epoch 28/80: current_loss=0.03310 | best_loss=0.03293
Epoch 29/80: current_loss=0.03309 | best_loss=0.03293
Epoch 30/80: current_loss=0.03304 | best_loss=0.03293
Epoch 31/80: current_loss=0.03330 | best_loss=0.03293
Epoch 32/80: current_loss=0.03309 | best_loss=0.03293
Epoch 33/80: current_loss=0.03308 | best_loss=0.03293
Epoch 34/80: current_loss=0.03317 | best_loss=0.03293
Epoch 35/80: current_loss=0.03308 | best_loss=0.03293
Early Stopping at epoch 35
      explained_var=-0.00724 | mse_loss=0.03284
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03539 | best_loss=0.03539
Epoch 1/80: current_loss=0.03534 | best_loss=0.03534
Epoch 2/80: current_loss=0.03532 | best_loss=0.03532
Epoch 3/80: current_loss=0.03534 | best_loss=0.03532
Epoch 4/80: current_loss=0.03534 | best_loss=0.03532
Epoch 5/80: current_loss=0.03554 | best_loss=0.03532
Epoch 6/80: current_loss=0.03553 | best_loss=0.03532
Epoch 7/80: current_loss=0.03550 | best_loss=0.03532
Epoch 8/80: current_loss=0.03550 | best_loss=0.03532
Epoch 9/80: current_loss=0.03549 | best_loss=0.03532
Epoch 10/80: current_loss=0.03548 | best_loss=0.03532
Epoch 11/80: current_loss=0.03549 | best_loss=0.03532
Epoch 12/80: current_loss=0.03537 | best_loss=0.03532
Epoch 13/80: current_loss=0.03541 | best_loss=0.03532
Epoch 14/80: current_loss=0.03540 | best_loss=0.03532
Epoch 15/80: current_loss=0.03559 | best_loss=0.03532
Epoch 16/80: current_loss=0.03545 | best_loss=0.03532
Epoch 17/80: current_loss=0.03550 | best_loss=0.03532
Epoch 18/80: current_loss=0.03548 | best_loss=0.03532
Epoch 19/80: current_loss=0.03552 | best_loss=0.03532
Epoch 20/80: current_loss=0.03553 | best_loss=0.03532
Epoch 21/80: current_loss=0.03552 | best_loss=0.03532
Epoch 22/80: current_loss=0.03563 | best_loss=0.03532
Early Stopping at epoch 22
      explained_var=0.02180 | mse_loss=0.03510
----------------------------------------------
Average early_stopping_point: 26| avg_exp_var=0.01521| avg_loss=0.03486
----------------------------------------------


----------------------------------------------
Params for Trial 13
{'learning_rate': 0.0001, 'weight_decay': 0.0021571880692069795, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04840 | best_loss=0.04840
Epoch 1/80: current_loss=0.04403 | best_loss=0.04403
Epoch 2/80: current_loss=0.04137 | best_loss=0.04137
Epoch 3/80: current_loss=0.04080 | best_loss=0.04080
Epoch 4/80: current_loss=0.04058 | best_loss=0.04058
Epoch 5/80: current_loss=0.03988 | best_loss=0.03988
Epoch 6/80: current_loss=0.04033 | best_loss=0.03988
Epoch 7/80: current_loss=0.03936 | best_loss=0.03936
Epoch 8/80: current_loss=0.03911 | best_loss=0.03911
Epoch 9/80: current_loss=0.03891 | best_loss=0.03891
Epoch 10/80: current_loss=0.03915 | best_loss=0.03891
Epoch 11/80: current_loss=0.03869 | best_loss=0.03869
Epoch 12/80: current_loss=0.03906 | best_loss=0.03869
Epoch 13/80: current_loss=0.03847 | best_loss=0.03847
Epoch 14/80: current_loss=0.03850 | best_loss=0.03847
Epoch 15/80: current_loss=0.03840 | best_loss=0.03840
Epoch 16/80: current_loss=0.03831 | best_loss=0.03831
Epoch 17/80: current_loss=0.03878 | best_loss=0.03831
Epoch 18/80: current_loss=0.03825 | best_loss=0.03825
Epoch 19/80: current_loss=0.03930 | best_loss=0.03825
Epoch 20/80: current_loss=0.03815 | best_loss=0.03815
Epoch 21/80: current_loss=0.03833 | best_loss=0.03815
Epoch 22/80: current_loss=0.03814 | best_loss=0.03814
Epoch 23/80: current_loss=0.03834 | best_loss=0.03814
Epoch 24/80: current_loss=0.03808 | best_loss=0.03808
Epoch 25/80: current_loss=0.03861 | best_loss=0.03808
Epoch 26/80: current_loss=0.03808 | best_loss=0.03808
Epoch 27/80: current_loss=0.03849 | best_loss=0.03808
Epoch 28/80: current_loss=0.03833 | best_loss=0.03808
Epoch 29/80: current_loss=0.03798 | best_loss=0.03798
Epoch 30/80: current_loss=0.03803 | best_loss=0.03798
Epoch 31/80: current_loss=0.03795 | best_loss=0.03795
Epoch 32/80: current_loss=0.03797 | best_loss=0.03795
Epoch 33/80: current_loss=0.03795 | best_loss=0.03795
Epoch 34/80: current_loss=0.03778 | best_loss=0.03778
Epoch 35/80: current_loss=0.03845 | best_loss=0.03778
Epoch 36/80: current_loss=0.03790 | best_loss=0.03778
Epoch 37/80: current_loss=0.03796 | best_loss=0.03778
Epoch 38/80: current_loss=0.03792 | best_loss=0.03778
Epoch 39/80: current_loss=0.03786 | best_loss=0.03778
Epoch 40/80: current_loss=0.03809 | best_loss=0.03778
Epoch 41/80: current_loss=0.03787 | best_loss=0.03778
Epoch 42/80: current_loss=0.03781 | best_loss=0.03778
Epoch 43/80: current_loss=0.03780 | best_loss=0.03778
Epoch 44/80: current_loss=0.03817 | best_loss=0.03778
Epoch 45/80: current_loss=0.03804 | best_loss=0.03778
Epoch 46/80: current_loss=0.03779 | best_loss=0.03778
Epoch 47/80: current_loss=0.03803 | best_loss=0.03778
Epoch 48/80: current_loss=0.03792 | best_loss=0.03778
Epoch 49/80: current_loss=0.03826 | best_loss=0.03778
Epoch 50/80: current_loss=0.03778 | best_loss=0.03778
Epoch 51/80: current_loss=0.03776 | best_loss=0.03776
Epoch 52/80: current_loss=0.03785 | best_loss=0.03776
Epoch 53/80: current_loss=0.03791 | best_loss=0.03776
Epoch 54/80: current_loss=0.03772 | best_loss=0.03772
Epoch 55/80: current_loss=0.03774 | best_loss=0.03772
Epoch 56/80: current_loss=0.03804 | best_loss=0.03772
Epoch 57/80: current_loss=0.03772 | best_loss=0.03772
Epoch 58/80: current_loss=0.03777 | best_loss=0.03772
Epoch 59/80: current_loss=0.03848 | best_loss=0.03772
Epoch 60/80: current_loss=0.03789 | best_loss=0.03772
Epoch 61/80: current_loss=0.03779 | best_loss=0.03772
Epoch 62/80: current_loss=0.03777 | best_loss=0.03772
Epoch 63/80: current_loss=0.03839 | best_loss=0.03772
Epoch 64/80: current_loss=0.03778 | best_loss=0.03772
Epoch 65/80: current_loss=0.03826 | best_loss=0.03772
Epoch 66/80: current_loss=0.03774 | best_loss=0.03772
Epoch 67/80: current_loss=0.03810 | best_loss=0.03772
Epoch 68/80: current_loss=0.03775 | best_loss=0.03772
Epoch 69/80: current_loss=0.03799 | best_loss=0.03772
Epoch 70/80: current_loss=0.03798 | best_loss=0.03772
Epoch 71/80: current_loss=0.03788 | best_loss=0.03772
Epoch 72/80: current_loss=0.03798 | best_loss=0.03772
Epoch 73/80: current_loss=0.03802 | best_loss=0.03772
Epoch 74/80: current_loss=0.03782 | best_loss=0.03772
Early Stopping at epoch 74
      explained_var=0.01285 | mse_loss=0.03863
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04177 | best_loss=0.04177
Epoch 1/80: current_loss=0.04167 | best_loss=0.04167
Epoch 2/80: current_loss=0.04181 | best_loss=0.04167
Epoch 3/80: current_loss=0.04217 | best_loss=0.04167
Epoch 4/80: current_loss=0.04171 | best_loss=0.04167
Epoch 5/80: current_loss=0.04165 | best_loss=0.04165
Epoch 6/80: current_loss=0.04191 | best_loss=0.04165
Epoch 7/80: current_loss=0.04185 | best_loss=0.04165
Epoch 8/80: current_loss=0.04177 | best_loss=0.04165
Epoch 9/80: current_loss=0.04177 | best_loss=0.04165
Epoch 10/80: current_loss=0.04173 | best_loss=0.04165
Epoch 11/80: current_loss=0.04239 | best_loss=0.04165
Epoch 12/80: current_loss=0.04183 | best_loss=0.04165
Epoch 13/80: current_loss=0.04208 | best_loss=0.04165
Epoch 14/80: current_loss=0.04173 | best_loss=0.04165
Epoch 15/80: current_loss=0.04181 | best_loss=0.04165
Epoch 16/80: current_loss=0.04185 | best_loss=0.04165
Epoch 17/80: current_loss=0.04206 | best_loss=0.04165
Epoch 18/80: current_loss=0.04192 | best_loss=0.04165
Epoch 19/80: current_loss=0.04198 | best_loss=0.04165
Epoch 20/80: current_loss=0.04184 | best_loss=0.04165
Epoch 21/80: current_loss=0.04235 | best_loss=0.04165
Epoch 22/80: current_loss=0.04200 | best_loss=0.04165
Epoch 23/80: current_loss=0.04219 | best_loss=0.04165
Epoch 24/80: current_loss=0.04192 | best_loss=0.04165
Epoch 25/80: current_loss=0.04195 | best_loss=0.04165
Early Stopping at epoch 25
      explained_var=0.02600 | mse_loss=0.04016
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02801 | best_loss=0.02801
Epoch 1/80: current_loss=0.02791 | best_loss=0.02791
Epoch 2/80: current_loss=0.02821 | best_loss=0.02791
Epoch 3/80: current_loss=0.02788 | best_loss=0.02788
Epoch 4/80: current_loss=0.02876 | best_loss=0.02788
Epoch 5/80: current_loss=0.02788 | best_loss=0.02788
Epoch 6/80: current_loss=0.02831 | best_loss=0.02788
Epoch 7/80: current_loss=0.02809 | best_loss=0.02788
Epoch 8/80: current_loss=0.02857 | best_loss=0.02788
Epoch 9/80: current_loss=0.02801 | best_loss=0.02788
Epoch 10/80: current_loss=0.02807 | best_loss=0.02788
Epoch 11/80: current_loss=0.02822 | best_loss=0.02788
Epoch 12/80: current_loss=0.02813 | best_loss=0.02788
Epoch 13/80: current_loss=0.02846 | best_loss=0.02788
Epoch 14/80: current_loss=0.02806 | best_loss=0.02788
Epoch 15/80: current_loss=0.02831 | best_loss=0.02788
Epoch 16/80: current_loss=0.02793 | best_loss=0.02788
Epoch 17/80: current_loss=0.02833 | best_loss=0.02788
Epoch 18/80: current_loss=0.02794 | best_loss=0.02788
Epoch 19/80: current_loss=0.02818 | best_loss=0.02788
Epoch 20/80: current_loss=0.02831 | best_loss=0.02788
Epoch 21/80: current_loss=0.02803 | best_loss=0.02788
Epoch 22/80: current_loss=0.02809 | best_loss=0.02788
Epoch 23/80: current_loss=0.02846 | best_loss=0.02788
Epoch 24/80: current_loss=0.02796 | best_loss=0.02788
Epoch 25/80: current_loss=0.02841 | best_loss=0.02788
Early Stopping at epoch 25
      explained_var=0.00192 | mse_loss=0.02831
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03283 | best_loss=0.03283
Epoch 1/80: current_loss=0.03299 | best_loss=0.03283
Epoch 2/80: current_loss=0.03319 | best_loss=0.03283
Epoch 3/80: current_loss=0.03295 | best_loss=0.03283
Epoch 4/80: current_loss=0.03283 | best_loss=0.03283
Epoch 5/80: current_loss=0.03283 | best_loss=0.03283
Epoch 6/80: current_loss=0.03299 | best_loss=0.03283
Epoch 7/80: current_loss=0.03300 | best_loss=0.03283
Epoch 8/80: current_loss=0.03298 | best_loss=0.03283
Epoch 9/80: current_loss=0.03285 | best_loss=0.03283
Epoch 10/80: current_loss=0.03297 | best_loss=0.03283
Epoch 11/80: current_loss=0.03315 | best_loss=0.03283
Epoch 12/80: current_loss=0.03287 | best_loss=0.03283
Epoch 13/80: current_loss=0.03287 | best_loss=0.03283
Epoch 14/80: current_loss=0.03306 | best_loss=0.03283
Epoch 15/80: current_loss=0.03289 | best_loss=0.03283
Epoch 16/80: current_loss=0.03355 | best_loss=0.03283
Epoch 17/80: current_loss=0.03284 | best_loss=0.03283
Epoch 18/80: current_loss=0.03291 | best_loss=0.03283
Epoch 19/80: current_loss=0.03284 | best_loss=0.03283
Epoch 20/80: current_loss=0.03305 | best_loss=0.03283
Epoch 21/80: current_loss=0.03287 | best_loss=0.03283
Epoch 22/80: current_loss=0.03284 | best_loss=0.03283
Epoch 23/80: current_loss=0.03306 | best_loss=0.03283
Epoch 24/80: current_loss=0.03304 | best_loss=0.03283
Epoch 25/80: current_loss=0.03284 | best_loss=0.03283
Early Stopping at epoch 25
      explained_var=-0.00176 | mse_loss=0.03266
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03582 | best_loss=0.03582
Epoch 1/80: current_loss=0.03567 | best_loss=0.03567
Epoch 2/80: current_loss=0.03560 | best_loss=0.03560
Epoch 3/80: current_loss=0.03564 | best_loss=0.03560
Epoch 4/80: current_loss=0.03562 | best_loss=0.03560
Epoch 5/80: current_loss=0.03561 | best_loss=0.03560
Epoch 6/80: current_loss=0.03561 | best_loss=0.03560
Epoch 7/80: current_loss=0.03562 | best_loss=0.03560
Epoch 8/80: current_loss=0.03571 | best_loss=0.03560
Epoch 9/80: current_loss=0.03560 | best_loss=0.03560
Epoch 10/80: current_loss=0.03561 | best_loss=0.03560
Epoch 11/80: current_loss=0.03566 | best_loss=0.03560
Epoch 12/80: current_loss=0.03564 | best_loss=0.03560
Epoch 13/80: current_loss=0.03572 | best_loss=0.03560
Epoch 14/80: current_loss=0.03569 | best_loss=0.03560
Epoch 15/80: current_loss=0.03562 | best_loss=0.03560
Epoch 16/80: current_loss=0.03566 | best_loss=0.03560
Epoch 17/80: current_loss=0.03577 | best_loss=0.03560
Epoch 18/80: current_loss=0.03565 | best_loss=0.03560
Epoch 19/80: current_loss=0.03567 | best_loss=0.03560
Epoch 20/80: current_loss=0.03565 | best_loss=0.03560
Epoch 21/80: current_loss=0.03561 | best_loss=0.03560
Epoch 22/80: current_loss=0.03563 | best_loss=0.03560
Early Stopping at epoch 22
      explained_var=0.01508 | mse_loss=0.03535
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.01082| avg_loss=0.03502
----------------------------------------------


----------------------------------------------
Params for Trial 14
{'learning_rate': 0.0001, 'weight_decay': 0.0031673230575684688, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04533 | best_loss=0.04533
Epoch 1/80: current_loss=0.04454 | best_loss=0.04454
Epoch 2/80: current_loss=0.04283 | best_loss=0.04283
Epoch 3/80: current_loss=0.04254 | best_loss=0.04254
Epoch 4/80: current_loss=0.04189 | best_loss=0.04189
Epoch 5/80: current_loss=0.04108 | best_loss=0.04108
Epoch 6/80: current_loss=0.04069 | best_loss=0.04069
Epoch 7/80: current_loss=0.04050 | best_loss=0.04050
Epoch 8/80: current_loss=0.03999 | best_loss=0.03999
Epoch 9/80: current_loss=0.03979 | best_loss=0.03979
Epoch 10/80: current_loss=0.04024 | best_loss=0.03979
Epoch 11/80: current_loss=0.03933 | best_loss=0.03933
Epoch 12/80: current_loss=0.03910 | best_loss=0.03910
Epoch 13/80: current_loss=0.03926 | best_loss=0.03910
Epoch 14/80: current_loss=0.03874 | best_loss=0.03874
Epoch 15/80: current_loss=0.03856 | best_loss=0.03856
Epoch 16/80: current_loss=0.03836 | best_loss=0.03836
Epoch 17/80: current_loss=0.03854 | best_loss=0.03836
Epoch 18/80: current_loss=0.03818 | best_loss=0.03818
Epoch 19/80: current_loss=0.03819 | best_loss=0.03818
Epoch 20/80: current_loss=0.03814 | best_loss=0.03814
Epoch 21/80: current_loss=0.03836 | best_loss=0.03814
Epoch 22/80: current_loss=0.03867 | best_loss=0.03814
Epoch 23/80: current_loss=0.03793 | best_loss=0.03793
Epoch 24/80: current_loss=0.03801 | best_loss=0.03793
Epoch 25/80: current_loss=0.03790 | best_loss=0.03790
Epoch 26/80: current_loss=0.03793 | best_loss=0.03790
Epoch 27/80: current_loss=0.03797 | best_loss=0.03790
Epoch 28/80: current_loss=0.03792 | best_loss=0.03790
Epoch 29/80: current_loss=0.03780 | best_loss=0.03780
Epoch 30/80: current_loss=0.03798 | best_loss=0.03780
Epoch 31/80: current_loss=0.03816 | best_loss=0.03780
Epoch 32/80: current_loss=0.03801 | best_loss=0.03780
Epoch 33/80: current_loss=0.03791 | best_loss=0.03780
Epoch 34/80: current_loss=0.03803 | best_loss=0.03780
Epoch 35/80: current_loss=0.03799 | best_loss=0.03780
Epoch 36/80: current_loss=0.03778 | best_loss=0.03778
Epoch 37/80: current_loss=0.03799 | best_loss=0.03778
Epoch 38/80: current_loss=0.03775 | best_loss=0.03775
Epoch 39/80: current_loss=0.03813 | best_loss=0.03775
Epoch 40/80: current_loss=0.03781 | best_loss=0.03775
Epoch 41/80: current_loss=0.03774 | best_loss=0.03774
Epoch 42/80: current_loss=0.03790 | best_loss=0.03774
Epoch 43/80: current_loss=0.03765 | best_loss=0.03765
Epoch 44/80: current_loss=0.03794 | best_loss=0.03765
Epoch 45/80: current_loss=0.03783 | best_loss=0.03765
Epoch 46/80: current_loss=0.03782 | best_loss=0.03765
Epoch 47/80: current_loss=0.03763 | best_loss=0.03763
Epoch 48/80: current_loss=0.03855 | best_loss=0.03763
Epoch 49/80: current_loss=0.03760 | best_loss=0.03760
Epoch 50/80: current_loss=0.03761 | best_loss=0.03760
Epoch 51/80: current_loss=0.03776 | best_loss=0.03760
Epoch 52/80: current_loss=0.03764 | best_loss=0.03760
Epoch 53/80: current_loss=0.03779 | best_loss=0.03760
Epoch 54/80: current_loss=0.03759 | best_loss=0.03759
Epoch 55/80: current_loss=0.03768 | best_loss=0.03759
Epoch 56/80: current_loss=0.03760 | best_loss=0.03759
Epoch 57/80: current_loss=0.03778 | best_loss=0.03759
Epoch 58/80: current_loss=0.03782 | best_loss=0.03759
Epoch 59/80: current_loss=0.03767 | best_loss=0.03759
Epoch 60/80: current_loss=0.03774 | best_loss=0.03759
Epoch 61/80: current_loss=0.03788 | best_loss=0.03759
Epoch 62/80: current_loss=0.03766 | best_loss=0.03759
Epoch 63/80: current_loss=0.03778 | best_loss=0.03759
Epoch 64/80: current_loss=0.03770 | best_loss=0.03759
Epoch 65/80: current_loss=0.03763 | best_loss=0.03759
Epoch 66/80: current_loss=0.03785 | best_loss=0.03759
Epoch 67/80: current_loss=0.03778 | best_loss=0.03759
Epoch 68/80: current_loss=0.03792 | best_loss=0.03759
Epoch 69/80: current_loss=0.03780 | best_loss=0.03759
Epoch 70/80: current_loss=0.03780 | best_loss=0.03759
Epoch 71/80: current_loss=0.03808 | best_loss=0.03759
Epoch 72/80: current_loss=0.03776 | best_loss=0.03759
Epoch 73/80: current_loss=0.03770 | best_loss=0.03759
Epoch 74/80: current_loss=0.03801 | best_loss=0.03759
Early Stopping at epoch 74
      explained_var=0.01742 | mse_loss=0.03859
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04146 | best_loss=0.04146
Epoch 1/80: current_loss=0.04167 | best_loss=0.04146
Epoch 2/80: current_loss=0.04145 | best_loss=0.04145
Epoch 3/80: current_loss=0.04142 | best_loss=0.04142
Epoch 4/80: current_loss=0.04179 | best_loss=0.04142
Epoch 5/80: current_loss=0.04156 | best_loss=0.04142
Epoch 6/80: current_loss=0.04145 | best_loss=0.04142
Epoch 7/80: current_loss=0.04138 | best_loss=0.04138
Epoch 8/80: current_loss=0.04146 | best_loss=0.04138
Epoch 9/80: current_loss=0.04139 | best_loss=0.04138
Epoch 10/80: current_loss=0.04170 | best_loss=0.04138
Epoch 11/80: current_loss=0.04176 | best_loss=0.04138
Epoch 12/80: current_loss=0.04138 | best_loss=0.04138
Epoch 13/80: current_loss=0.04161 | best_loss=0.04138
Epoch 14/80: current_loss=0.04149 | best_loss=0.04138
Epoch 15/80: current_loss=0.04140 | best_loss=0.04138
Epoch 16/80: current_loss=0.04137 | best_loss=0.04137
Epoch 17/80: current_loss=0.04143 | best_loss=0.04137
Epoch 18/80: current_loss=0.04165 | best_loss=0.04137
Epoch 19/80: current_loss=0.04145 | best_loss=0.04137
Epoch 20/80: current_loss=0.04175 | best_loss=0.04137
Epoch 21/80: current_loss=0.04140 | best_loss=0.04137
Epoch 22/80: current_loss=0.04180 | best_loss=0.04137
Epoch 23/80: current_loss=0.04143 | best_loss=0.04137
Epoch 24/80: current_loss=0.04155 | best_loss=0.04137
Epoch 25/80: current_loss=0.04203 | best_loss=0.04137
Epoch 26/80: current_loss=0.04145 | best_loss=0.04137
Epoch 27/80: current_loss=0.04176 | best_loss=0.04137
Epoch 28/80: current_loss=0.04160 | best_loss=0.04137
Epoch 29/80: current_loss=0.04162 | best_loss=0.04137
Epoch 30/80: current_loss=0.04168 | best_loss=0.04137
Epoch 31/80: current_loss=0.04189 | best_loss=0.04137
Epoch 32/80: current_loss=0.04140 | best_loss=0.04137
Epoch 33/80: current_loss=0.04163 | best_loss=0.04137
Epoch 34/80: current_loss=0.04162 | best_loss=0.04137
Epoch 35/80: current_loss=0.04143 | best_loss=0.04137
Epoch 36/80: current_loss=0.04185 | best_loss=0.04137
Early Stopping at epoch 36
      explained_var=0.03290 | mse_loss=0.03988
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02926 | best_loss=0.02926
Epoch 1/80: current_loss=0.02797 | best_loss=0.02797
Epoch 2/80: current_loss=0.02846 | best_loss=0.02797
Epoch 3/80: current_loss=0.02829 | best_loss=0.02797
Epoch 4/80: current_loss=0.02800 | best_loss=0.02797
Epoch 5/80: current_loss=0.02900 | best_loss=0.02797
Epoch 6/80: current_loss=0.02806 | best_loss=0.02797
Epoch 7/80: current_loss=0.02898 | best_loss=0.02797
Epoch 8/80: current_loss=0.02808 | best_loss=0.02797
Epoch 9/80: current_loss=0.02870 | best_loss=0.02797
Epoch 10/80: current_loss=0.02805 | best_loss=0.02797
Epoch 11/80: current_loss=0.02854 | best_loss=0.02797
Epoch 12/80: current_loss=0.02825 | best_loss=0.02797
Epoch 13/80: current_loss=0.02893 | best_loss=0.02797
Epoch 14/80: current_loss=0.02834 | best_loss=0.02797
Epoch 15/80: current_loss=0.02872 | best_loss=0.02797
Epoch 16/80: current_loss=0.02808 | best_loss=0.02797
Epoch 17/80: current_loss=0.02830 | best_loss=0.02797
Epoch 18/80: current_loss=0.02823 | best_loss=0.02797
Epoch 19/80: current_loss=0.02804 | best_loss=0.02797
Epoch 20/80: current_loss=0.02813 | best_loss=0.02797
Epoch 21/80: current_loss=0.02820 | best_loss=0.02797
Early Stopping at epoch 21
      explained_var=-0.00111 | mse_loss=0.02841
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03426 | best_loss=0.03426
Epoch 1/80: current_loss=0.03316 | best_loss=0.03316
Epoch 2/80: current_loss=0.03324 | best_loss=0.03316
Epoch 3/80: current_loss=0.03302 | best_loss=0.03302
Epoch 4/80: current_loss=0.03311 | best_loss=0.03302
Epoch 5/80: current_loss=0.03324 | best_loss=0.03302
Epoch 6/80: current_loss=0.03306 | best_loss=0.03302
Epoch 7/80: current_loss=0.03319 | best_loss=0.03302
Epoch 8/80: current_loss=0.03306 | best_loss=0.03302
Epoch 9/80: current_loss=0.03356 | best_loss=0.03302
Epoch 10/80: current_loss=0.03307 | best_loss=0.03302
Epoch 11/80: current_loss=0.03306 | best_loss=0.03302
Epoch 12/80: current_loss=0.03320 | best_loss=0.03302
Epoch 13/80: current_loss=0.03307 | best_loss=0.03302
Epoch 14/80: current_loss=0.03313 | best_loss=0.03302
Epoch 15/80: current_loss=0.03387 | best_loss=0.03302
Epoch 16/80: current_loss=0.03308 | best_loss=0.03302
Epoch 17/80: current_loss=0.03316 | best_loss=0.03302
Epoch 18/80: current_loss=0.03361 | best_loss=0.03302
Epoch 19/80: current_loss=0.03313 | best_loss=0.03302
Epoch 20/80: current_loss=0.03311 | best_loss=0.03302
Epoch 21/80: current_loss=0.03324 | best_loss=0.03302
Epoch 22/80: current_loss=0.03314 | best_loss=0.03302
Epoch 23/80: current_loss=0.03307 | best_loss=0.03302
Early Stopping at epoch 23
      explained_var=-0.00883 | mse_loss=0.03290
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03540 | best_loss=0.03540
Epoch 1/80: current_loss=0.03541 | best_loss=0.03540
Epoch 2/80: current_loss=0.03541 | best_loss=0.03540
Epoch 3/80: current_loss=0.03543 | best_loss=0.03540
Epoch 4/80: current_loss=0.03557 | best_loss=0.03540
Epoch 5/80: current_loss=0.03559 | best_loss=0.03540
Epoch 6/80: current_loss=0.03537 | best_loss=0.03537
Epoch 7/80: current_loss=0.03546 | best_loss=0.03537
Epoch 8/80: current_loss=0.03549 | best_loss=0.03537
Epoch 9/80: current_loss=0.03539 | best_loss=0.03537
Epoch 10/80: current_loss=0.03545 | best_loss=0.03537
Epoch 11/80: current_loss=0.03545 | best_loss=0.03537
Epoch 12/80: current_loss=0.03543 | best_loss=0.03537
Epoch 13/80: current_loss=0.03544 | best_loss=0.03537
Epoch 14/80: current_loss=0.03544 | best_loss=0.03537
Epoch 15/80: current_loss=0.03549 | best_loss=0.03537
Epoch 16/80: current_loss=0.03548 | best_loss=0.03537
Epoch 17/80: current_loss=0.03546 | best_loss=0.03537
Epoch 18/80: current_loss=0.03549 | best_loss=0.03537
Epoch 19/80: current_loss=0.03554 | best_loss=0.03537
Epoch 20/80: current_loss=0.03551 | best_loss=0.03537
Epoch 21/80: current_loss=0.03544 | best_loss=0.03537
Epoch 22/80: current_loss=0.03545 | best_loss=0.03537
Epoch 23/80: current_loss=0.03544 | best_loss=0.03537
Epoch 24/80: current_loss=0.03544 | best_loss=0.03537
Epoch 25/80: current_loss=0.03543 | best_loss=0.03537
Epoch 26/80: current_loss=0.03543 | best_loss=0.03537
Early Stopping at epoch 26
      explained_var=0.01967 | mse_loss=0.03518
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.01201| avg_loss=0.03499
----------------------------------------------


----------------------------------------------
Params for Trial 15
{'learning_rate': 0.01, 'weight_decay': 3.113526657617861e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.31989 | best_loss=0.31989
Epoch 1/80: current_loss=0.11904 | best_loss=0.11904
Epoch 2/80: current_loss=0.10927 | best_loss=0.10927
Epoch 3/80: current_loss=0.11515 | best_loss=0.10927
Epoch 4/80: current_loss=0.04169 | best_loss=0.04169
Epoch 5/80: current_loss=0.09232 | best_loss=0.04169
Epoch 6/80: current_loss=0.03752 | best_loss=0.03752
Epoch 7/80: current_loss=0.06715 | best_loss=0.03752
Epoch 8/80: current_loss=0.06041 | best_loss=0.03752
Epoch 9/80: current_loss=0.05065 | best_loss=0.03752
Epoch 10/80: current_loss=0.04550 | best_loss=0.03752
Epoch 11/80: current_loss=0.07417 | best_loss=0.03752
Epoch 12/80: current_loss=0.05844 | best_loss=0.03752
Epoch 13/80: current_loss=0.07469 | best_loss=0.03752
Epoch 14/80: current_loss=0.04995 | best_loss=0.03752
Epoch 15/80: current_loss=0.04051 | best_loss=0.03752
Epoch 16/80: current_loss=0.03878 | best_loss=0.03752
Epoch 17/80: current_loss=0.09468 | best_loss=0.03752
Epoch 18/80: current_loss=0.05710 | best_loss=0.03752
Epoch 19/80: current_loss=0.06234 | best_loss=0.03752
Epoch 20/80: current_loss=0.04530 | best_loss=0.03752
Epoch 21/80: current_loss=0.05855 | best_loss=0.03752
Epoch 22/80: current_loss=0.15255 | best_loss=0.03752
Epoch 23/80: current_loss=0.07307 | best_loss=0.03752
Epoch 24/80: current_loss=0.05377 | best_loss=0.03752
Epoch 25/80: current_loss=0.06068 | best_loss=0.03752
Epoch 26/80: current_loss=0.03972 | best_loss=0.03752
Early Stopping at epoch 26
      explained_var=0.01899 | mse_loss=0.03842
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.21396 | best_loss=0.21396
Epoch 1/80: current_loss=0.05974 | best_loss=0.05974
Epoch 2/80: current_loss=0.06409 | best_loss=0.05974
Epoch 3/80: current_loss=0.10107 | best_loss=0.05974
Epoch 4/80: current_loss=0.22432 | best_loss=0.05974
Epoch 5/80: current_loss=0.04827 | best_loss=0.04827
Epoch 6/80: current_loss=0.05545 | best_loss=0.04827
Epoch 7/80: current_loss=0.04419 | best_loss=0.04419
Epoch 8/80: current_loss=0.16583 | best_loss=0.04419
Epoch 9/80: current_loss=0.28289 | best_loss=0.04419
Epoch 10/80: current_loss=0.05490 | best_loss=0.04419
Epoch 11/80: current_loss=0.04488 | best_loss=0.04419
Epoch 12/80: current_loss=0.06008 | best_loss=0.04419
Epoch 13/80: current_loss=0.08183 | best_loss=0.04419
Epoch 14/80: current_loss=0.12793 | best_loss=0.04419
Epoch 15/80: current_loss=0.05849 | best_loss=0.04419
Epoch 16/80: current_loss=0.13799 | best_loss=0.04419
Epoch 17/80: current_loss=0.04593 | best_loss=0.04419
Epoch 18/80: current_loss=0.16510 | best_loss=0.04419
Epoch 19/80: current_loss=0.11296 | best_loss=0.04419
Epoch 20/80: current_loss=0.21345 | best_loss=0.04419
Epoch 21/80: current_loss=0.04541 | best_loss=0.04419
Epoch 22/80: current_loss=0.14538 | best_loss=0.04419
Epoch 23/80: current_loss=0.06171 | best_loss=0.04419
Epoch 24/80: current_loss=0.04654 | best_loss=0.04419
Epoch 25/80: current_loss=0.11842 | best_loss=0.04419
Epoch 26/80: current_loss=0.08568 | best_loss=0.04419
Epoch 27/80: current_loss=0.09645 | best_loss=0.04419
Early Stopping at epoch 27
      explained_var=-0.01535 | mse_loss=0.04256
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05431 | best_loss=0.05431
Epoch 1/80: current_loss=0.03575 | best_loss=0.03575
Epoch 2/80: current_loss=0.08804 | best_loss=0.03575
Epoch 3/80: current_loss=0.04156 | best_loss=0.03575
Epoch 4/80: current_loss=0.05445 | best_loss=0.03575
Epoch 5/80: current_loss=0.03835 | best_loss=0.03575
Epoch 6/80: current_loss=0.07496 | best_loss=0.03575
Epoch 7/80: current_loss=0.04735 | best_loss=0.03575
Epoch 8/80: current_loss=0.03034 | best_loss=0.03034
Epoch 9/80: current_loss=0.06043 | best_loss=0.03034
Epoch 10/80: current_loss=0.03369 | best_loss=0.03034
Epoch 11/80: current_loss=0.03537 | best_loss=0.03034
Epoch 12/80: current_loss=0.05240 | best_loss=0.03034
Epoch 13/80: current_loss=0.06331 | best_loss=0.03034
Epoch 14/80: current_loss=0.07004 | best_loss=0.03034
Epoch 15/80: current_loss=0.02832 | best_loss=0.02832
Epoch 16/80: current_loss=0.04174 | best_loss=0.02832
Epoch 17/80: current_loss=0.06267 | best_loss=0.02832
Epoch 18/80: current_loss=0.04779 | best_loss=0.02832
Epoch 19/80: current_loss=0.05929 | best_loss=0.02832
Epoch 20/80: current_loss=0.11011 | best_loss=0.02832
Epoch 21/80: current_loss=0.04657 | best_loss=0.02832
Epoch 22/80: current_loss=0.05730 | best_loss=0.02832
Epoch 23/80: current_loss=0.04779 | best_loss=0.02832
Epoch 24/80: current_loss=0.02933 | best_loss=0.02832
Epoch 25/80: current_loss=0.03415 | best_loss=0.02832
Epoch 26/80: current_loss=0.02842 | best_loss=0.02832
Epoch 27/80: current_loss=0.04480 | best_loss=0.02832
Epoch 28/80: current_loss=0.07469 | best_loss=0.02832
Epoch 29/80: current_loss=0.03288 | best_loss=0.02832
Epoch 30/80: current_loss=0.06706 | best_loss=0.02832
Epoch 31/80: current_loss=0.20186 | best_loss=0.02832
Epoch 32/80: current_loss=0.09874 | best_loss=0.02832
Epoch 33/80: current_loss=0.12253 | best_loss=0.02832
Epoch 34/80: current_loss=0.02832 | best_loss=0.02832
Epoch 35/80: current_loss=0.06671 | best_loss=0.02832
Early Stopping at epoch 35
      explained_var=-0.00041 | mse_loss=0.02881
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05407 | best_loss=0.05407
Epoch 1/80: current_loss=0.05221 | best_loss=0.05221
Epoch 2/80: current_loss=0.04938 | best_loss=0.04938
Epoch 3/80: current_loss=0.08709 | best_loss=0.04938
Epoch 4/80: current_loss=0.04943 | best_loss=0.04938
Epoch 5/80: current_loss=0.04433 | best_loss=0.04433
Epoch 6/80: current_loss=0.03578 | best_loss=0.03578
Epoch 7/80: current_loss=0.04487 | best_loss=0.03578
Epoch 8/80: current_loss=0.03744 | best_loss=0.03578
Epoch 9/80: current_loss=0.03554 | best_loss=0.03554
Epoch 10/80: current_loss=0.04067 | best_loss=0.03554
Epoch 11/80: current_loss=0.04171 | best_loss=0.03554
Epoch 12/80: current_loss=0.04122 | best_loss=0.03554
Epoch 13/80: current_loss=0.03712 | best_loss=0.03554
Epoch 14/80: current_loss=0.04533 | best_loss=0.03554
Epoch 15/80: current_loss=0.06096 | best_loss=0.03554
Epoch 16/80: current_loss=0.07617 | best_loss=0.03554
Epoch 17/80: current_loss=0.04565 | best_loss=0.03554
Epoch 18/80: current_loss=0.06720 | best_loss=0.03554
Epoch 19/80: current_loss=0.05368 | best_loss=0.03554
Epoch 20/80: current_loss=0.12891 | best_loss=0.03554
Epoch 21/80: current_loss=0.04955 | best_loss=0.03554
Epoch 22/80: current_loss=0.04567 | best_loss=0.03554
Epoch 23/80: current_loss=0.04454 | best_loss=0.03554
Epoch 24/80: current_loss=0.06796 | best_loss=0.03554
Epoch 25/80: current_loss=0.09246 | best_loss=0.03554
Epoch 26/80: current_loss=0.14288 | best_loss=0.03554
Epoch 27/80: current_loss=0.12183 | best_loss=0.03554
Epoch 28/80: current_loss=0.14278 | best_loss=0.03554
Epoch 29/80: current_loss=0.03996 | best_loss=0.03554
Early Stopping at epoch 29
      explained_var=-0.08074 | mse_loss=0.03523
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.08710 | best_loss=0.08710
Epoch 1/80: current_loss=0.07172 | best_loss=0.07172
Epoch 2/80: current_loss=0.03887 | best_loss=0.03887
Epoch 3/80: current_loss=0.03607 | best_loss=0.03607
Epoch 4/80: current_loss=0.03628 | best_loss=0.03607
Epoch 5/80: current_loss=0.04401 | best_loss=0.03607
Epoch 6/80: current_loss=0.09393 | best_loss=0.03607
Epoch 7/80: current_loss=0.04537 | best_loss=0.03607
Epoch 8/80: current_loss=0.05387 | best_loss=0.03607
Epoch 9/80: current_loss=0.05799 | best_loss=0.03607
Epoch 10/80: current_loss=0.04646 | best_loss=0.03607
Epoch 11/80: current_loss=0.03771 | best_loss=0.03607
Epoch 12/80: current_loss=0.05167 | best_loss=0.03607
Epoch 13/80: current_loss=0.04758 | best_loss=0.03607
Epoch 14/80: current_loss=0.06724 | best_loss=0.03607
Epoch 15/80: current_loss=0.03874 | best_loss=0.03607
Epoch 16/80: current_loss=0.07456 | best_loss=0.03607
Epoch 17/80: current_loss=0.03982 | best_loss=0.03607
Epoch 18/80: current_loss=0.03973 | best_loss=0.03607
Epoch 19/80: current_loss=0.04540 | best_loss=0.03607
Epoch 20/80: current_loss=0.16981 | best_loss=0.03607
Epoch 21/80: current_loss=0.05011 | best_loss=0.03607
Epoch 22/80: current_loss=0.05658 | best_loss=0.03607
Epoch 23/80: current_loss=0.03791 | best_loss=0.03607
Early Stopping at epoch 23
      explained_var=0.00499 | mse_loss=0.03573
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=-0.01450| avg_loss=0.03615
----------------------------------------------


----------------------------------------------
Params for Trial 16
{'learning_rate': 0.0001, 'weight_decay': 0.0017102089384645458, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05001 | best_loss=0.05001
Epoch 1/80: current_loss=0.04322 | best_loss=0.04322
Epoch 2/80: current_loss=0.04061 | best_loss=0.04061
Epoch 3/80: current_loss=0.04026 | best_loss=0.04026
Epoch 4/80: current_loss=0.03975 | best_loss=0.03975
Epoch 5/80: current_loss=0.03944 | best_loss=0.03944
Epoch 6/80: current_loss=0.03923 | best_loss=0.03923
Epoch 7/80: current_loss=0.03952 | best_loss=0.03923
Epoch 8/80: current_loss=0.03902 | best_loss=0.03902
Epoch 9/80: current_loss=0.03882 | best_loss=0.03882
Epoch 10/80: current_loss=0.03861 | best_loss=0.03861
Epoch 11/80: current_loss=0.03847 | best_loss=0.03847
Epoch 12/80: current_loss=0.03842 | best_loss=0.03842
Epoch 13/80: current_loss=0.03840 | best_loss=0.03840
Epoch 14/80: current_loss=0.03864 | best_loss=0.03840
Epoch 15/80: current_loss=0.03819 | best_loss=0.03819
Epoch 16/80: current_loss=0.03830 | best_loss=0.03819
Epoch 17/80: current_loss=0.03820 | best_loss=0.03819
Epoch 18/80: current_loss=0.03817 | best_loss=0.03817
Epoch 19/80: current_loss=0.03814 | best_loss=0.03814
Epoch 20/80: current_loss=0.03795 | best_loss=0.03795
Epoch 21/80: current_loss=0.03789 | best_loss=0.03789
Epoch 22/80: current_loss=0.03778 | best_loss=0.03778
Epoch 23/80: current_loss=0.03796 | best_loss=0.03778
Epoch 24/80: current_loss=0.03777 | best_loss=0.03777
Epoch 25/80: current_loss=0.03773 | best_loss=0.03773
Epoch 26/80: current_loss=0.03853 | best_loss=0.03773
Epoch 27/80: current_loss=0.03772 | best_loss=0.03772
Epoch 28/80: current_loss=0.03778 | best_loss=0.03772
Epoch 29/80: current_loss=0.03819 | best_loss=0.03772
Epoch 30/80: current_loss=0.03772 | best_loss=0.03772
Epoch 31/80: current_loss=0.03793 | best_loss=0.03772
Epoch 32/80: current_loss=0.03793 | best_loss=0.03772
Epoch 33/80: current_loss=0.03800 | best_loss=0.03772
Epoch 34/80: current_loss=0.03776 | best_loss=0.03772
Epoch 35/80: current_loss=0.03839 | best_loss=0.03772
Epoch 36/80: current_loss=0.03776 | best_loss=0.03772
Epoch 37/80: current_loss=0.03780 | best_loss=0.03772
Epoch 38/80: current_loss=0.03780 | best_loss=0.03772
Epoch 39/80: current_loss=0.03825 | best_loss=0.03772
Epoch 40/80: current_loss=0.03812 | best_loss=0.03772
Epoch 41/80: current_loss=0.03770 | best_loss=0.03770
Epoch 42/80: current_loss=0.03773 | best_loss=0.03770
Epoch 43/80: current_loss=0.03773 | best_loss=0.03770
Epoch 44/80: current_loss=0.03816 | best_loss=0.03770
Epoch 45/80: current_loss=0.03778 | best_loss=0.03770
Epoch 46/80: current_loss=0.03845 | best_loss=0.03770
Epoch 47/80: current_loss=0.03778 | best_loss=0.03770
Epoch 48/80: current_loss=0.03847 | best_loss=0.03770
Epoch 49/80: current_loss=0.03777 | best_loss=0.03770
Epoch 50/80: current_loss=0.03881 | best_loss=0.03770
Epoch 51/80: current_loss=0.03773 | best_loss=0.03770
Epoch 52/80: current_loss=0.03792 | best_loss=0.03770
Epoch 53/80: current_loss=0.03807 | best_loss=0.03770
Epoch 54/80: current_loss=0.03774 | best_loss=0.03770
Epoch 55/80: current_loss=0.03765 | best_loss=0.03765
Epoch 56/80: current_loss=0.03775 | best_loss=0.03765
Epoch 57/80: current_loss=0.03757 | best_loss=0.03757
Epoch 58/80: current_loss=0.03807 | best_loss=0.03757
Epoch 59/80: current_loss=0.03762 | best_loss=0.03757
Epoch 60/80: current_loss=0.03772 | best_loss=0.03757
Epoch 61/80: current_loss=0.03767 | best_loss=0.03757
Epoch 62/80: current_loss=0.03792 | best_loss=0.03757
Epoch 63/80: current_loss=0.03787 | best_loss=0.03757
Epoch 64/80: current_loss=0.03768 | best_loss=0.03757
Epoch 65/80: current_loss=0.03787 | best_loss=0.03757
Epoch 66/80: current_loss=0.03767 | best_loss=0.03757
Epoch 67/80: current_loss=0.03855 | best_loss=0.03757
Epoch 68/80: current_loss=0.03766 | best_loss=0.03757
Epoch 69/80: current_loss=0.03764 | best_loss=0.03757
Epoch 70/80: current_loss=0.03798 | best_loss=0.03757
Epoch 71/80: current_loss=0.03784 | best_loss=0.03757
Epoch 72/80: current_loss=0.03774 | best_loss=0.03757
Epoch 73/80: current_loss=0.03771 | best_loss=0.03757
Epoch 74/80: current_loss=0.03770 | best_loss=0.03757
Epoch 75/80: current_loss=0.03787 | best_loss=0.03757
Epoch 76/80: current_loss=0.03788 | best_loss=0.03757
Epoch 77/80: current_loss=0.03771 | best_loss=0.03757
Early Stopping at epoch 77
      explained_var=0.01708 | mse_loss=0.03845
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04298 | best_loss=0.04298
Epoch 1/80: current_loss=0.04170 | best_loss=0.04170
Epoch 2/80: current_loss=0.04162 | best_loss=0.04162
Epoch 3/80: current_loss=0.04163 | best_loss=0.04162
Epoch 4/80: current_loss=0.04206 | best_loss=0.04162
Epoch 5/80: current_loss=0.04227 | best_loss=0.04162
Epoch 6/80: current_loss=0.04174 | best_loss=0.04162
Epoch 7/80: current_loss=0.04209 | best_loss=0.04162
Epoch 8/80: current_loss=0.04169 | best_loss=0.04162
Epoch 9/80: current_loss=0.04215 | best_loss=0.04162
Epoch 10/80: current_loss=0.04191 | best_loss=0.04162
Epoch 11/80: current_loss=0.04183 | best_loss=0.04162
Epoch 12/80: current_loss=0.04178 | best_loss=0.04162
Epoch 13/80: current_loss=0.04240 | best_loss=0.04162
Epoch 14/80: current_loss=0.04180 | best_loss=0.04162
Epoch 15/80: current_loss=0.04190 | best_loss=0.04162
Epoch 16/80: current_loss=0.04188 | best_loss=0.04162
Epoch 17/80: current_loss=0.04185 | best_loss=0.04162
Epoch 18/80: current_loss=0.04261 | best_loss=0.04162
Epoch 19/80: current_loss=0.04210 | best_loss=0.04162
Epoch 20/80: current_loss=0.04177 | best_loss=0.04162
Epoch 21/80: current_loss=0.04181 | best_loss=0.04162
Epoch 22/80: current_loss=0.04206 | best_loss=0.04162
Early Stopping at epoch 22
      explained_var=0.02735 | mse_loss=0.04012
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02790 | best_loss=0.02790
Epoch 1/80: current_loss=0.02869 | best_loss=0.02790
Epoch 2/80: current_loss=0.02817 | best_loss=0.02790
Epoch 3/80: current_loss=0.02816 | best_loss=0.02790
Epoch 4/80: current_loss=0.02884 | best_loss=0.02790
Epoch 5/80: current_loss=0.02822 | best_loss=0.02790
Epoch 6/80: current_loss=0.02797 | best_loss=0.02790
Epoch 7/80: current_loss=0.02853 | best_loss=0.02790
Epoch 8/80: current_loss=0.02853 | best_loss=0.02790
Epoch 9/80: current_loss=0.02804 | best_loss=0.02790
Epoch 10/80: current_loss=0.02830 | best_loss=0.02790
Epoch 11/80: current_loss=0.02813 | best_loss=0.02790
Epoch 12/80: current_loss=0.02803 | best_loss=0.02790
Epoch 13/80: current_loss=0.02837 | best_loss=0.02790
Epoch 14/80: current_loss=0.02819 | best_loss=0.02790
Epoch 15/80: current_loss=0.02826 | best_loss=0.02790
Epoch 16/80: current_loss=0.02817 | best_loss=0.02790
Epoch 17/80: current_loss=0.02809 | best_loss=0.02790
Epoch 18/80: current_loss=0.02794 | best_loss=0.02790
Epoch 19/80: current_loss=0.02811 | best_loss=0.02790
Epoch 20/80: current_loss=0.02794 | best_loss=0.02790
Early Stopping at epoch 20
      explained_var=0.00162 | mse_loss=0.02831
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03287 | best_loss=0.03287
Epoch 1/80: current_loss=0.03324 | best_loss=0.03287
Epoch 2/80: current_loss=0.03319 | best_loss=0.03287
Epoch 3/80: current_loss=0.03295 | best_loss=0.03287
Epoch 4/80: current_loss=0.03294 | best_loss=0.03287
Epoch 5/80: current_loss=0.03290 | best_loss=0.03287
Epoch 6/80: current_loss=0.03302 | best_loss=0.03287
Epoch 7/80: current_loss=0.03287 | best_loss=0.03287
Epoch 8/80: current_loss=0.03295 | best_loss=0.03287
Epoch 9/80: current_loss=0.03286 | best_loss=0.03286
Epoch 10/80: current_loss=0.03348 | best_loss=0.03286
Epoch 11/80: current_loss=0.03300 | best_loss=0.03286
Epoch 12/80: current_loss=0.03309 | best_loss=0.03286
Epoch 13/80: current_loss=0.03288 | best_loss=0.03286
Epoch 14/80: current_loss=0.03295 | best_loss=0.03286
Epoch 15/80: current_loss=0.03286 | best_loss=0.03286
Epoch 16/80: current_loss=0.03287 | best_loss=0.03286
Epoch 17/80: current_loss=0.03354 | best_loss=0.03286
Epoch 18/80: current_loss=0.03289 | best_loss=0.03286
Epoch 19/80: current_loss=0.03288 | best_loss=0.03286
Epoch 20/80: current_loss=0.03302 | best_loss=0.03286
Epoch 21/80: current_loss=0.03287 | best_loss=0.03286
Epoch 22/80: current_loss=0.03322 | best_loss=0.03286
Epoch 23/80: current_loss=0.03294 | best_loss=0.03286
Epoch 24/80: current_loss=0.03288 | best_loss=0.03286
Epoch 25/80: current_loss=0.03288 | best_loss=0.03286
Epoch 26/80: current_loss=0.03304 | best_loss=0.03286
Epoch 27/80: current_loss=0.03287 | best_loss=0.03286
Epoch 28/80: current_loss=0.03291 | best_loss=0.03286
Epoch 29/80: current_loss=0.03304 | best_loss=0.03286
Epoch 30/80: current_loss=0.03290 | best_loss=0.03286
Epoch 31/80: current_loss=0.03285 | best_loss=0.03285
Epoch 32/80: current_loss=0.03303 | best_loss=0.03285
Epoch 33/80: current_loss=0.03312 | best_loss=0.03285
Epoch 34/80: current_loss=0.03290 | best_loss=0.03285
Epoch 35/80: current_loss=0.03287 | best_loss=0.03285
Epoch 36/80: current_loss=0.03287 | best_loss=0.03285
Epoch 37/80: current_loss=0.03288 | best_loss=0.03285
Epoch 38/80: current_loss=0.03297 | best_loss=0.03285
Epoch 39/80: current_loss=0.03287 | best_loss=0.03285
Epoch 40/80: current_loss=0.03324 | best_loss=0.03285
Epoch 41/80: current_loss=0.03290 | best_loss=0.03285
Epoch 42/80: current_loss=0.03316 | best_loss=0.03285
Epoch 43/80: current_loss=0.03314 | best_loss=0.03285
Epoch 44/80: current_loss=0.03289 | best_loss=0.03285
Epoch 45/80: current_loss=0.03300 | best_loss=0.03285
Epoch 46/80: current_loss=0.03291 | best_loss=0.03285
Epoch 47/80: current_loss=0.03302 | best_loss=0.03285
Epoch 48/80: current_loss=0.03322 | best_loss=0.03285
Epoch 49/80: current_loss=0.03296 | best_loss=0.03285
Epoch 50/80: current_loss=0.03291 | best_loss=0.03285
Epoch 51/80: current_loss=0.03288 | best_loss=0.03285
Early Stopping at epoch 51
      explained_var=-0.00299 | mse_loss=0.03270
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03602 | best_loss=0.03602
Epoch 1/80: current_loss=0.03563 | best_loss=0.03563
Epoch 2/80: current_loss=0.03566 | best_loss=0.03563
Epoch 3/80: current_loss=0.03557 | best_loss=0.03557
Epoch 4/80: current_loss=0.03547 | best_loss=0.03547
Epoch 5/80: current_loss=0.03548 | best_loss=0.03547
Epoch 6/80: current_loss=0.03548 | best_loss=0.03547
Epoch 7/80: current_loss=0.03548 | best_loss=0.03547
Epoch 8/80: current_loss=0.03562 | best_loss=0.03547
Epoch 9/80: current_loss=0.03555 | best_loss=0.03547
Epoch 10/80: current_loss=0.03552 | best_loss=0.03547
Epoch 11/80: current_loss=0.03554 | best_loss=0.03547
Epoch 12/80: current_loss=0.03557 | best_loss=0.03547
Epoch 13/80: current_loss=0.03549 | best_loss=0.03547
Epoch 14/80: current_loss=0.03551 | best_loss=0.03547
Epoch 15/80: current_loss=0.03552 | best_loss=0.03547
Epoch 16/80: current_loss=0.03553 | best_loss=0.03547
Epoch 17/80: current_loss=0.03556 | best_loss=0.03547
Epoch 18/80: current_loss=0.03555 | best_loss=0.03547
Epoch 19/80: current_loss=0.03553 | best_loss=0.03547
Epoch 20/80: current_loss=0.03577 | best_loss=0.03547
Epoch 21/80: current_loss=0.03556 | best_loss=0.03547
Epoch 22/80: current_loss=0.03568 | best_loss=0.03547
Epoch 23/80: current_loss=0.03563 | best_loss=0.03547
Epoch 24/80: current_loss=0.03547 | best_loss=0.03547
Early Stopping at epoch 24
      explained_var=0.01812 | mse_loss=0.03526
----------------------------------------------
Average early_stopping_point: 18| avg_exp_var=0.01224| avg_loss=0.03497
----------------------------------------------


----------------------------------------------
Params for Trial 17
{'learning_rate': 0.0001, 'weight_decay': 0.003309128484976863, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04608 | best_loss=0.04608
Epoch 1/80: current_loss=0.04076 | best_loss=0.04076
Epoch 2/80: current_loss=0.03931 | best_loss=0.03931
Epoch 3/80: current_loss=0.03896 | best_loss=0.03896
Epoch 4/80: current_loss=0.03958 | best_loss=0.03896
Epoch 5/80: current_loss=0.03819 | best_loss=0.03819
Epoch 6/80: current_loss=0.03877 | best_loss=0.03819
Epoch 7/80: current_loss=0.03822 | best_loss=0.03819
Epoch 8/80: current_loss=0.03787 | best_loss=0.03787
Epoch 9/80: current_loss=0.03769 | best_loss=0.03769
Epoch 10/80: current_loss=0.03791 | best_loss=0.03769
Epoch 11/80: current_loss=0.03822 | best_loss=0.03769
Epoch 12/80: current_loss=0.03793 | best_loss=0.03769
Epoch 13/80: current_loss=0.03785 | best_loss=0.03769
Epoch 14/80: current_loss=0.03812 | best_loss=0.03769
Epoch 15/80: current_loss=0.03788 | best_loss=0.03769
Epoch 16/80: current_loss=0.04049 | best_loss=0.03769
Epoch 17/80: current_loss=0.03797 | best_loss=0.03769
Epoch 18/80: current_loss=0.03765 | best_loss=0.03765
Epoch 19/80: current_loss=0.03815 | best_loss=0.03765
Epoch 20/80: current_loss=0.03824 | best_loss=0.03765
Epoch 21/80: current_loss=0.03778 | best_loss=0.03765
Epoch 22/80: current_loss=0.03778 | best_loss=0.03765
Epoch 23/80: current_loss=0.03768 | best_loss=0.03765
Epoch 24/80: current_loss=0.03757 | best_loss=0.03757
Epoch 25/80: current_loss=0.03865 | best_loss=0.03757
Epoch 26/80: current_loss=0.03739 | best_loss=0.03739
Epoch 27/80: current_loss=0.03743 | best_loss=0.03739
Epoch 28/80: current_loss=0.03750 | best_loss=0.03739
Epoch 29/80: current_loss=0.03785 | best_loss=0.03739
Epoch 30/80: current_loss=0.03929 | best_loss=0.03739
Epoch 31/80: current_loss=0.03767 | best_loss=0.03739
Epoch 32/80: current_loss=0.03790 | best_loss=0.03739
Epoch 33/80: current_loss=0.03930 | best_loss=0.03739
Epoch 34/80: current_loss=0.03886 | best_loss=0.03739
Epoch 35/80: current_loss=0.03824 | best_loss=0.03739
Epoch 36/80: current_loss=0.03783 | best_loss=0.03739
Epoch 37/80: current_loss=0.03776 | best_loss=0.03739
Epoch 38/80: current_loss=0.03771 | best_loss=0.03739
Epoch 39/80: current_loss=0.03992 | best_loss=0.03739
Epoch 40/80: current_loss=0.03807 | best_loss=0.03739
Epoch 41/80: current_loss=0.03787 | best_loss=0.03739
Epoch 42/80: current_loss=0.03749 | best_loss=0.03739
Epoch 43/80: current_loss=0.03763 | best_loss=0.03739
Epoch 44/80: current_loss=0.03753 | best_loss=0.03739
Epoch 45/80: current_loss=0.03761 | best_loss=0.03739
Epoch 46/80: current_loss=0.03787 | best_loss=0.03739
Early Stopping at epoch 46
      explained_var=0.02184 | mse_loss=0.03831
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04190 | best_loss=0.04190
Epoch 1/80: current_loss=0.04138 | best_loss=0.04138
Epoch 2/80: current_loss=0.04134 | best_loss=0.04134
Epoch 3/80: current_loss=0.04243 | best_loss=0.04134
Epoch 4/80: current_loss=0.04257 | best_loss=0.04134
Epoch 5/80: current_loss=0.04232 | best_loss=0.04134
Epoch 6/80: current_loss=0.04143 | best_loss=0.04134
Epoch 7/80: current_loss=0.04136 | best_loss=0.04134
Epoch 8/80: current_loss=0.04147 | best_loss=0.04134
Epoch 9/80: current_loss=0.04145 | best_loss=0.04134
Epoch 10/80: current_loss=0.04136 | best_loss=0.04134
Epoch 11/80: current_loss=0.04158 | best_loss=0.04134
Epoch 12/80: current_loss=0.04180 | best_loss=0.04134
Epoch 13/80: current_loss=0.04143 | best_loss=0.04134
Epoch 14/80: current_loss=0.04146 | best_loss=0.04134
Epoch 15/80: current_loss=0.04301 | best_loss=0.04134
Epoch 16/80: current_loss=0.04174 | best_loss=0.04134
Epoch 17/80: current_loss=0.04362 | best_loss=0.04134
Epoch 18/80: current_loss=0.04168 | best_loss=0.04134
Epoch 19/80: current_loss=0.04209 | best_loss=0.04134
Epoch 20/80: current_loss=0.04177 | best_loss=0.04134
Epoch 21/80: current_loss=0.04159 | best_loss=0.04134
Epoch 22/80: current_loss=0.04167 | best_loss=0.04134
Early Stopping at epoch 22
      explained_var=0.03464 | mse_loss=0.03983
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02974 | best_loss=0.02974
Epoch 1/80: current_loss=0.02819 | best_loss=0.02819
Epoch 2/80: current_loss=0.02825 | best_loss=0.02819
Epoch 3/80: current_loss=0.02826 | best_loss=0.02819
Epoch 4/80: current_loss=0.02831 | best_loss=0.02819
Epoch 5/80: current_loss=0.02803 | best_loss=0.02803
Epoch 6/80: current_loss=0.02834 | best_loss=0.02803
Epoch 7/80: current_loss=0.02818 | best_loss=0.02803
Epoch 8/80: current_loss=0.02877 | best_loss=0.02803
Epoch 9/80: current_loss=0.02800 | best_loss=0.02800
Epoch 10/80: current_loss=0.02801 | best_loss=0.02800
Epoch 11/80: current_loss=0.02821 | best_loss=0.02800
Epoch 12/80: current_loss=0.02801 | best_loss=0.02800
Epoch 13/80: current_loss=0.02806 | best_loss=0.02800
Epoch 14/80: current_loss=0.02805 | best_loss=0.02800
Epoch 15/80: current_loss=0.02863 | best_loss=0.02800
Epoch 16/80: current_loss=0.03084 | best_loss=0.02800
Epoch 17/80: current_loss=0.02928 | best_loss=0.02800
Epoch 18/80: current_loss=0.02909 | best_loss=0.02800
Epoch 19/80: current_loss=0.02889 | best_loss=0.02800
Epoch 20/80: current_loss=0.02813 | best_loss=0.02800
Epoch 21/80: current_loss=0.02896 | best_loss=0.02800
Epoch 22/80: current_loss=0.02925 | best_loss=0.02800
Epoch 23/80: current_loss=0.02811 | best_loss=0.02800
Epoch 24/80: current_loss=0.02805 | best_loss=0.02800
Epoch 25/80: current_loss=0.02809 | best_loss=0.02800
Epoch 26/80: current_loss=0.02804 | best_loss=0.02800
Epoch 27/80: current_loss=0.02819 | best_loss=0.02800
Epoch 28/80: current_loss=0.02814 | best_loss=0.02800
Epoch 29/80: current_loss=0.02803 | best_loss=0.02800
Early Stopping at epoch 29
      explained_var=-0.00225 | mse_loss=0.02843
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03363 | best_loss=0.03363
Epoch 1/80: current_loss=0.03299 | best_loss=0.03299
Epoch 2/80: current_loss=0.03382 | best_loss=0.03299
Epoch 3/80: current_loss=0.03302 | best_loss=0.03299
Epoch 4/80: current_loss=0.03323 | best_loss=0.03299
Epoch 5/80: current_loss=0.03303 | best_loss=0.03299
Epoch 6/80: current_loss=0.03339 | best_loss=0.03299
Epoch 7/80: current_loss=0.03339 | best_loss=0.03299
Epoch 8/80: current_loss=0.03396 | best_loss=0.03299
Epoch 9/80: current_loss=0.03351 | best_loss=0.03299
Epoch 10/80: current_loss=0.03309 | best_loss=0.03299
Epoch 11/80: current_loss=0.03358 | best_loss=0.03299
Epoch 12/80: current_loss=0.03306 | best_loss=0.03299
Epoch 13/80: current_loss=0.03311 | best_loss=0.03299
Epoch 14/80: current_loss=0.03321 | best_loss=0.03299
Epoch 15/80: current_loss=0.03310 | best_loss=0.03299
Epoch 16/80: current_loss=0.03302 | best_loss=0.03299
Epoch 17/80: current_loss=0.03386 | best_loss=0.03299
Epoch 18/80: current_loss=0.03377 | best_loss=0.03299
Epoch 19/80: current_loss=0.03303 | best_loss=0.03299
Epoch 20/80: current_loss=0.03307 | best_loss=0.03299
Epoch 21/80: current_loss=0.03361 | best_loss=0.03299
Early Stopping at epoch 21
      explained_var=-0.00741 | mse_loss=0.03284
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03539 | best_loss=0.03539
Epoch 1/80: current_loss=0.03564 | best_loss=0.03539
Epoch 2/80: current_loss=0.03547 | best_loss=0.03539
Epoch 3/80: current_loss=0.03545 | best_loss=0.03539
Epoch 4/80: current_loss=0.03544 | best_loss=0.03539
Epoch 5/80: current_loss=0.03569 | best_loss=0.03539
Epoch 6/80: current_loss=0.03552 | best_loss=0.03539
Epoch 7/80: current_loss=0.03546 | best_loss=0.03539
Epoch 8/80: current_loss=0.03553 | best_loss=0.03539
Epoch 9/80: current_loss=0.03596 | best_loss=0.03539
Epoch 10/80: current_loss=0.03876 | best_loss=0.03539
Epoch 11/80: current_loss=0.03635 | best_loss=0.03539
Epoch 12/80: current_loss=0.03562 | best_loss=0.03539
Epoch 13/80: current_loss=0.03564 | best_loss=0.03539
Epoch 14/80: current_loss=0.03549 | best_loss=0.03539
Epoch 15/80: current_loss=0.03601 | best_loss=0.03539
Epoch 16/80: current_loss=0.03544 | best_loss=0.03539
Epoch 17/80: current_loss=0.03550 | best_loss=0.03539
Epoch 18/80: current_loss=0.03547 | best_loss=0.03539
Epoch 19/80: current_loss=0.03562 | best_loss=0.03539
Epoch 20/80: current_loss=0.03604 | best_loss=0.03539
Early Stopping at epoch 20
      explained_var=0.01868 | mse_loss=0.03522
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=0.01310| avg_loss=0.03493
----------------------------------------------


----------------------------------------------
Params for Trial 18
{'learning_rate': 0.0001, 'weight_decay': 0.0014650333661503008, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05034 | best_loss=0.05034
Epoch 1/80: current_loss=0.04365 | best_loss=0.04365
Epoch 2/80: current_loss=0.04232 | best_loss=0.04232
Epoch 3/80: current_loss=0.04133 | best_loss=0.04133
Epoch 4/80: current_loss=0.04118 | best_loss=0.04118
Epoch 5/80: current_loss=0.04023 | best_loss=0.04023
Epoch 6/80: current_loss=0.03967 | best_loss=0.03967
Epoch 7/80: current_loss=0.03932 | best_loss=0.03932
Epoch 8/80: current_loss=0.03940 | best_loss=0.03932
Epoch 9/80: current_loss=0.03959 | best_loss=0.03932
Epoch 10/80: current_loss=0.03881 | best_loss=0.03881
Epoch 11/80: current_loss=0.03875 | best_loss=0.03875
Epoch 12/80: current_loss=0.03858 | best_loss=0.03858
Epoch 13/80: current_loss=0.03858 | best_loss=0.03858
Epoch 14/80: current_loss=0.03877 | best_loss=0.03858
Epoch 15/80: current_loss=0.03863 | best_loss=0.03858
Epoch 16/80: current_loss=0.03842 | best_loss=0.03842
Epoch 17/80: current_loss=0.03847 | best_loss=0.03842
Epoch 18/80: current_loss=0.03843 | best_loss=0.03842
Epoch 19/80: current_loss=0.03831 | best_loss=0.03831
Epoch 20/80: current_loss=0.03866 | best_loss=0.03831
Epoch 21/80: current_loss=0.03820 | best_loss=0.03820
Epoch 22/80: current_loss=0.03872 | best_loss=0.03820
Epoch 23/80: current_loss=0.03808 | best_loss=0.03808
Epoch 24/80: current_loss=0.03817 | best_loss=0.03808
Epoch 25/80: current_loss=0.03831 | best_loss=0.03808
Epoch 26/80: current_loss=0.03803 | best_loss=0.03803
Epoch 27/80: current_loss=0.03804 | best_loss=0.03803
Epoch 28/80: current_loss=0.03804 | best_loss=0.03803
Epoch 29/80: current_loss=0.03822 | best_loss=0.03803
Epoch 30/80: current_loss=0.03799 | best_loss=0.03799
Epoch 31/80: current_loss=0.03810 | best_loss=0.03799
Epoch 32/80: current_loss=0.03844 | best_loss=0.03799
Epoch 33/80: current_loss=0.03799 | best_loss=0.03799
Epoch 34/80: current_loss=0.03834 | best_loss=0.03799
Epoch 35/80: current_loss=0.03798 | best_loss=0.03798
Epoch 36/80: current_loss=0.03823 | best_loss=0.03798
Epoch 37/80: current_loss=0.03828 | best_loss=0.03798
Epoch 38/80: current_loss=0.03804 | best_loss=0.03798
Epoch 39/80: current_loss=0.03878 | best_loss=0.03798
Epoch 40/80: current_loss=0.03795 | best_loss=0.03795
Epoch 41/80: current_loss=0.03815 | best_loss=0.03795
Epoch 42/80: current_loss=0.03821 | best_loss=0.03795
Epoch 43/80: current_loss=0.03801 | best_loss=0.03795
Epoch 44/80: current_loss=0.03835 | best_loss=0.03795
Epoch 45/80: current_loss=0.03783 | best_loss=0.03783
Epoch 46/80: current_loss=0.03798 | best_loss=0.03783
Epoch 47/80: current_loss=0.03809 | best_loss=0.03783
Epoch 48/80: current_loss=0.03816 | best_loss=0.03783
Epoch 49/80: current_loss=0.03772 | best_loss=0.03772
Epoch 50/80: current_loss=0.03853 | best_loss=0.03772
Epoch 51/80: current_loss=0.03770 | best_loss=0.03770
Epoch 52/80: current_loss=0.03801 | best_loss=0.03770
Epoch 53/80: current_loss=0.03779 | best_loss=0.03770
Epoch 54/80: current_loss=0.03786 | best_loss=0.03770
Epoch 55/80: current_loss=0.03785 | best_loss=0.03770
Epoch 56/80: current_loss=0.03802 | best_loss=0.03770
Epoch 57/80: current_loss=0.03776 | best_loss=0.03770
Epoch 58/80: current_loss=0.03788 | best_loss=0.03770
Epoch 59/80: current_loss=0.03792 | best_loss=0.03770
Epoch 60/80: current_loss=0.03778 | best_loss=0.03770
Epoch 61/80: current_loss=0.03819 | best_loss=0.03770
Epoch 62/80: current_loss=0.03786 | best_loss=0.03770
Epoch 63/80: current_loss=0.03802 | best_loss=0.03770
Epoch 64/80: current_loss=0.03781 | best_loss=0.03770
Epoch 65/80: current_loss=0.03785 | best_loss=0.03770
Epoch 66/80: current_loss=0.03769 | best_loss=0.03769
Epoch 67/80: current_loss=0.03773 | best_loss=0.03769
Epoch 68/80: current_loss=0.03797 | best_loss=0.03769
Epoch 69/80: current_loss=0.03768 | best_loss=0.03768
Epoch 70/80: current_loss=0.03765 | best_loss=0.03765
Epoch 71/80: current_loss=0.03772 | best_loss=0.03765
Epoch 72/80: current_loss=0.03790 | best_loss=0.03765
Epoch 73/80: current_loss=0.03769 | best_loss=0.03765
Epoch 74/80: current_loss=0.03758 | best_loss=0.03758
Epoch 75/80: current_loss=0.03763 | best_loss=0.03758
Epoch 76/80: current_loss=0.03823 | best_loss=0.03758
Epoch 77/80: current_loss=0.03762 | best_loss=0.03758
Epoch 78/80: current_loss=0.03783 | best_loss=0.03758
Epoch 79/80: current_loss=0.03785 | best_loss=0.03758
      explained_var=0.01719 | mse_loss=0.03850
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04167 | best_loss=0.04167
Epoch 1/80: current_loss=0.04161 | best_loss=0.04161
Epoch 2/80: current_loss=0.04188 | best_loss=0.04161
Epoch 3/80: current_loss=0.04330 | best_loss=0.04161
Epoch 4/80: current_loss=0.04158 | best_loss=0.04158
Epoch 5/80: current_loss=0.04164 | best_loss=0.04158
Epoch 6/80: current_loss=0.04168 | best_loss=0.04158
Epoch 7/80: current_loss=0.04178 | best_loss=0.04158
Epoch 8/80: current_loss=0.04176 | best_loss=0.04158
Epoch 9/80: current_loss=0.04191 | best_loss=0.04158
Epoch 10/80: current_loss=0.04163 | best_loss=0.04158
Epoch 11/80: current_loss=0.04197 | best_loss=0.04158
Epoch 12/80: current_loss=0.04173 | best_loss=0.04158
Epoch 13/80: current_loss=0.04191 | best_loss=0.04158
Epoch 14/80: current_loss=0.04189 | best_loss=0.04158
Epoch 15/80: current_loss=0.04170 | best_loss=0.04158
Epoch 16/80: current_loss=0.04220 | best_loss=0.04158
Epoch 17/80: current_loss=0.04172 | best_loss=0.04158
Epoch 18/80: current_loss=0.04182 | best_loss=0.04158
Epoch 19/80: current_loss=0.04167 | best_loss=0.04158
Epoch 20/80: current_loss=0.04193 | best_loss=0.04158
Epoch 21/80: current_loss=0.04215 | best_loss=0.04158
Epoch 22/80: current_loss=0.04178 | best_loss=0.04158
Epoch 23/80: current_loss=0.04172 | best_loss=0.04158
Epoch 24/80: current_loss=0.04173 | best_loss=0.04158
Early Stopping at epoch 24
      explained_var=0.02702 | mse_loss=0.04012
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02869 | best_loss=0.02869
Epoch 1/80: current_loss=0.02829 | best_loss=0.02829
Epoch 2/80: current_loss=0.02788 | best_loss=0.02788
Epoch 3/80: current_loss=0.02838 | best_loss=0.02788
Epoch 4/80: current_loss=0.02811 | best_loss=0.02788
Epoch 5/80: current_loss=0.02806 | best_loss=0.02788
Epoch 6/80: current_loss=0.02808 | best_loss=0.02788
Epoch 7/80: current_loss=0.02792 | best_loss=0.02788
Epoch 8/80: current_loss=0.02800 | best_loss=0.02788
Epoch 9/80: current_loss=0.02806 | best_loss=0.02788
Epoch 10/80: current_loss=0.02830 | best_loss=0.02788
Epoch 11/80: current_loss=0.02828 | best_loss=0.02788
Epoch 12/80: current_loss=0.02798 | best_loss=0.02788
Epoch 13/80: current_loss=0.02829 | best_loss=0.02788
Epoch 14/80: current_loss=0.02797 | best_loss=0.02788
Epoch 15/80: current_loss=0.02833 | best_loss=0.02788
Epoch 16/80: current_loss=0.02892 | best_loss=0.02788
Epoch 17/80: current_loss=0.02798 | best_loss=0.02788
Epoch 18/80: current_loss=0.02840 | best_loss=0.02788
Epoch 19/80: current_loss=0.02832 | best_loss=0.02788
Epoch 20/80: current_loss=0.02806 | best_loss=0.02788
Epoch 21/80: current_loss=0.02849 | best_loss=0.02788
Epoch 22/80: current_loss=0.02813 | best_loss=0.02788
Early Stopping at epoch 22
      explained_var=0.00207 | mse_loss=0.02830
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03348 | best_loss=0.03348
Epoch 1/80: current_loss=0.03293 | best_loss=0.03293
Epoch 2/80: current_loss=0.03293 | best_loss=0.03293
Epoch 3/80: current_loss=0.03298 | best_loss=0.03293
Epoch 4/80: current_loss=0.03308 | best_loss=0.03293
Epoch 5/80: current_loss=0.03305 | best_loss=0.03293
Epoch 6/80: current_loss=0.03297 | best_loss=0.03293
Epoch 7/80: current_loss=0.03301 | best_loss=0.03293
Epoch 8/80: current_loss=0.03294 | best_loss=0.03293
Epoch 9/80: current_loss=0.03315 | best_loss=0.03293
Epoch 10/80: current_loss=0.03314 | best_loss=0.03293
Epoch 11/80: current_loss=0.03293 | best_loss=0.03293
Epoch 12/80: current_loss=0.03294 | best_loss=0.03293
Epoch 13/80: current_loss=0.03318 | best_loss=0.03293
Epoch 14/80: current_loss=0.03297 | best_loss=0.03293
Epoch 15/80: current_loss=0.03332 | best_loss=0.03293
Epoch 16/80: current_loss=0.03297 | best_loss=0.03293
Epoch 17/80: current_loss=0.03296 | best_loss=0.03293
Epoch 18/80: current_loss=0.03295 | best_loss=0.03293
Epoch 19/80: current_loss=0.03313 | best_loss=0.03293
Epoch 20/80: current_loss=0.03296 | best_loss=0.03293
Epoch 21/80: current_loss=0.03291 | best_loss=0.03291
Epoch 22/80: current_loss=0.03338 | best_loss=0.03291
Epoch 23/80: current_loss=0.03305 | best_loss=0.03291
Epoch 24/80: current_loss=0.03292 | best_loss=0.03291
Epoch 25/80: current_loss=0.03297 | best_loss=0.03291
Epoch 26/80: current_loss=0.03302 | best_loss=0.03291
Epoch 27/80: current_loss=0.03308 | best_loss=0.03291
Epoch 28/80: current_loss=0.03293 | best_loss=0.03291
Epoch 29/80: current_loss=0.03306 | best_loss=0.03291
Epoch 30/80: current_loss=0.03309 | best_loss=0.03291
Epoch 31/80: current_loss=0.03294 | best_loss=0.03291
Epoch 32/80: current_loss=0.03307 | best_loss=0.03291
Epoch 33/80: current_loss=0.03298 | best_loss=0.03291
Epoch 34/80: current_loss=0.03304 | best_loss=0.03291
Epoch 35/80: current_loss=0.03302 | best_loss=0.03291
Epoch 36/80: current_loss=0.03344 | best_loss=0.03291
Epoch 37/80: current_loss=0.03311 | best_loss=0.03291
Epoch 38/80: current_loss=0.03297 | best_loss=0.03291
Epoch 39/80: current_loss=0.03308 | best_loss=0.03291
Epoch 40/80: current_loss=0.03296 | best_loss=0.03291
Epoch 41/80: current_loss=0.03298 | best_loss=0.03291
Early Stopping at epoch 41
      explained_var=-0.00512 | mse_loss=0.03278
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03556 | best_loss=0.03556
Epoch 1/80: current_loss=0.03551 | best_loss=0.03551
Epoch 2/80: current_loss=0.03546 | best_loss=0.03546
Epoch 3/80: current_loss=0.03546 | best_loss=0.03546
Epoch 4/80: current_loss=0.03544 | best_loss=0.03544
Epoch 5/80: current_loss=0.03554 | best_loss=0.03544
Epoch 6/80: current_loss=0.03551 | best_loss=0.03544
Epoch 7/80: current_loss=0.03551 | best_loss=0.03544
Epoch 8/80: current_loss=0.03551 | best_loss=0.03544
Epoch 9/80: current_loss=0.03553 | best_loss=0.03544
Epoch 10/80: current_loss=0.03552 | best_loss=0.03544
Epoch 11/80: current_loss=0.03551 | best_loss=0.03544
Epoch 12/80: current_loss=0.03560 | best_loss=0.03544
Epoch 13/80: current_loss=0.03552 | best_loss=0.03544
Epoch 14/80: current_loss=0.03554 | best_loss=0.03544
Epoch 15/80: current_loss=0.03552 | best_loss=0.03544
Epoch 16/80: current_loss=0.03554 | best_loss=0.03544
Epoch 17/80: current_loss=0.03554 | best_loss=0.03544
Epoch 18/80: current_loss=0.03555 | best_loss=0.03544
Epoch 19/80: current_loss=0.03555 | best_loss=0.03544
Epoch 20/80: current_loss=0.03553 | best_loss=0.03544
Epoch 21/80: current_loss=0.03558 | best_loss=0.03544
Epoch 22/80: current_loss=0.03552 | best_loss=0.03544
Epoch 23/80: current_loss=0.03556 | best_loss=0.03544
Epoch 24/80: current_loss=0.03563 | best_loss=0.03544
Early Stopping at epoch 24
      explained_var=0.01855 | mse_loss=0.03523
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=0.01194| avg_loss=0.03499
----------------------------------------------


----------------------------------------------
Params for Trial 19
{'learning_rate': 0.01, 'weight_decay': 0.0070228743096221145, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=1.00796 | best_loss=1.00796
Epoch 1/80: current_loss=0.14290 | best_loss=0.14290
Epoch 2/80: current_loss=0.08807 | best_loss=0.08807
Epoch 3/80: current_loss=0.08531 | best_loss=0.08531
Epoch 4/80: current_loss=0.04943 | best_loss=0.04943
Epoch 5/80: current_loss=0.03909 | best_loss=0.03909
Epoch 6/80: current_loss=0.04348 | best_loss=0.03909
Epoch 7/80: current_loss=0.04286 | best_loss=0.03909
Epoch 8/80: current_loss=0.03940 | best_loss=0.03909
Epoch 9/80: current_loss=0.04649 | best_loss=0.03909
Epoch 10/80: current_loss=0.05024 | best_loss=0.03909
Epoch 11/80: current_loss=0.04543 | best_loss=0.03909
Epoch 12/80: current_loss=0.04998 | best_loss=0.03909
Epoch 13/80: current_loss=0.03921 | best_loss=0.03909
Epoch 14/80: current_loss=0.05264 | best_loss=0.03909
Epoch 15/80: current_loss=0.03969 | best_loss=0.03909
Epoch 16/80: current_loss=0.04690 | best_loss=0.03909
Epoch 17/80: current_loss=0.05044 | best_loss=0.03909
Epoch 18/80: current_loss=0.04406 | best_loss=0.03909
Epoch 19/80: current_loss=0.03838 | best_loss=0.03838
Epoch 20/80: current_loss=0.04501 | best_loss=0.03838
Epoch 21/80: current_loss=0.03841 | best_loss=0.03838
Epoch 22/80: current_loss=0.03805 | best_loss=0.03805
Epoch 23/80: current_loss=0.03838 | best_loss=0.03805
Epoch 24/80: current_loss=0.03886 | best_loss=0.03805
Epoch 25/80: current_loss=0.03830 | best_loss=0.03805
Epoch 26/80: current_loss=0.03857 | best_loss=0.03805
Epoch 27/80: current_loss=0.03814 | best_loss=0.03805
Epoch 28/80: current_loss=0.03820 | best_loss=0.03805
Epoch 29/80: current_loss=0.03816 | best_loss=0.03805
Epoch 30/80: current_loss=0.03824 | best_loss=0.03805
Epoch 31/80: current_loss=0.03817 | best_loss=0.03805
Epoch 32/80: current_loss=0.03847 | best_loss=0.03805
Epoch 33/80: current_loss=0.03820 | best_loss=0.03805
Epoch 34/80: current_loss=0.03824 | best_loss=0.03805
Epoch 35/80: current_loss=0.03824 | best_loss=0.03805
Epoch 36/80: current_loss=0.03830 | best_loss=0.03805
Epoch 37/80: current_loss=0.03825 | best_loss=0.03805
Epoch 38/80: current_loss=0.03840 | best_loss=0.03805
Epoch 39/80: current_loss=0.03838 | best_loss=0.03805
Epoch 40/80: current_loss=0.03834 | best_loss=0.03805
Epoch 41/80: current_loss=0.03835 | best_loss=0.03805
Epoch 42/80: current_loss=0.03835 | best_loss=0.03805
Early Stopping at epoch 42
      explained_var=0.00847 | mse_loss=0.03878
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05782 | best_loss=0.05782
Epoch 1/80: current_loss=0.21691 | best_loss=0.05782
Epoch 2/80: current_loss=0.07196 | best_loss=0.05782
Epoch 3/80: current_loss=0.08733 | best_loss=0.05782
Epoch 4/80: current_loss=0.07376 | best_loss=0.05782
Epoch 5/80: current_loss=0.09884 | best_loss=0.05782
Epoch 6/80: current_loss=0.04999 | best_loss=0.04999
Epoch 7/80: current_loss=0.04235 | best_loss=0.04235
Epoch 8/80: current_loss=0.04404 | best_loss=0.04235
Epoch 9/80: current_loss=0.04275 | best_loss=0.04235
Epoch 10/80: current_loss=0.04268 | best_loss=0.04235
Epoch 11/80: current_loss=0.04269 | best_loss=0.04235
Epoch 12/80: current_loss=0.04263 | best_loss=0.04235
Epoch 13/80: current_loss=0.04290 | best_loss=0.04235
Epoch 14/80: current_loss=0.04265 | best_loss=0.04235
Epoch 15/80: current_loss=0.04270 | best_loss=0.04235
Epoch 16/80: current_loss=0.04301 | best_loss=0.04235
Epoch 17/80: current_loss=0.04271 | best_loss=0.04235
Epoch 18/80: current_loss=0.04297 | best_loss=0.04235
Epoch 19/80: current_loss=0.04275 | best_loss=0.04235
Epoch 20/80: current_loss=0.04264 | best_loss=0.04235
Epoch 21/80: current_loss=0.04264 | best_loss=0.04235
Epoch 22/80: current_loss=0.04263 | best_loss=0.04235
Epoch 23/80: current_loss=0.04284 | best_loss=0.04235
Epoch 24/80: current_loss=0.04284 | best_loss=0.04235
Epoch 25/80: current_loss=0.04293 | best_loss=0.04235
Epoch 26/80: current_loss=0.04265 | best_loss=0.04235
Epoch 27/80: current_loss=0.04272 | best_loss=0.04235
Early Stopping at epoch 27
      explained_var=0.01044 | mse_loss=0.04091
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.07319 | best_loss=0.07319
Epoch 1/80: current_loss=0.10822 | best_loss=0.07319
Epoch 2/80: current_loss=0.36245 | best_loss=0.07319
Epoch 3/80: current_loss=0.10694 | best_loss=0.07319
Epoch 4/80: current_loss=0.03097 | best_loss=0.03097
Epoch 5/80: current_loss=0.03764 | best_loss=0.03097
Epoch 6/80: current_loss=0.03042 | best_loss=0.03042
Epoch 7/80: current_loss=0.03087 | best_loss=0.03042
Epoch 8/80: current_loss=0.02824 | best_loss=0.02824
Epoch 9/80: current_loss=0.02870 | best_loss=0.02824
Epoch 10/80: current_loss=0.02796 | best_loss=0.02796
Epoch 11/80: current_loss=0.02815 | best_loss=0.02796
Epoch 12/80: current_loss=0.02830 | best_loss=0.02796
Epoch 13/80: current_loss=0.02799 | best_loss=0.02796
Epoch 14/80: current_loss=0.02849 | best_loss=0.02796
Epoch 15/80: current_loss=0.02801 | best_loss=0.02796
Epoch 16/80: current_loss=0.02866 | best_loss=0.02796
Epoch 17/80: current_loss=0.02838 | best_loss=0.02796
Epoch 18/80: current_loss=0.02799 | best_loss=0.02796
Epoch 19/80: current_loss=0.02822 | best_loss=0.02796
Epoch 20/80: current_loss=0.02814 | best_loss=0.02796
Epoch 21/80: current_loss=0.02812 | best_loss=0.02796
Epoch 22/80: current_loss=0.02850 | best_loss=0.02796
Epoch 23/80: current_loss=0.02851 | best_loss=0.02796
Epoch 24/80: current_loss=0.02856 | best_loss=0.02796
Epoch 25/80: current_loss=0.02813 | best_loss=0.02796
Epoch 26/80: current_loss=0.02820 | best_loss=0.02796
Epoch 27/80: current_loss=0.02808 | best_loss=0.02796
Epoch 28/80: current_loss=0.02857 | best_loss=0.02796
Epoch 29/80: current_loss=0.02808 | best_loss=0.02796
Epoch 30/80: current_loss=0.02854 | best_loss=0.02796
Early Stopping at epoch 30
      explained_var=0.00039 | mse_loss=0.02839
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.07711 | best_loss=0.07711
Epoch 1/80: current_loss=0.08774 | best_loss=0.07711
Epoch 2/80: current_loss=0.14646 | best_loss=0.07711
Epoch 3/80: current_loss=0.13419 | best_loss=0.07711
Epoch 4/80: current_loss=0.03459 | best_loss=0.03459
Epoch 5/80: current_loss=0.04676 | best_loss=0.03459
Epoch 6/80: current_loss=0.03300 | best_loss=0.03300
Epoch 7/80: current_loss=0.03288 | best_loss=0.03288
Epoch 8/80: current_loss=0.03294 | best_loss=0.03288
Epoch 9/80: current_loss=0.03305 | best_loss=0.03288
Epoch 10/80: current_loss=0.03301 | best_loss=0.03288
Epoch 11/80: current_loss=0.03337 | best_loss=0.03288
Epoch 12/80: current_loss=0.03305 | best_loss=0.03288
Epoch 13/80: current_loss=0.03297 | best_loss=0.03288
Epoch 14/80: current_loss=0.03285 | best_loss=0.03285
Epoch 15/80: current_loss=0.03308 | best_loss=0.03285
Epoch 16/80: current_loss=0.03298 | best_loss=0.03285
Epoch 17/80: current_loss=0.03308 | best_loss=0.03285
Epoch 18/80: current_loss=0.03284 | best_loss=0.03284
Epoch 19/80: current_loss=0.03289 | best_loss=0.03284
Epoch 20/80: current_loss=0.03284 | best_loss=0.03284
Epoch 21/80: current_loss=0.03283 | best_loss=0.03283
Epoch 22/80: current_loss=0.03285 | best_loss=0.03283
Epoch 23/80: current_loss=0.03398 | best_loss=0.03283
Epoch 24/80: current_loss=0.03354 | best_loss=0.03283
Epoch 25/80: current_loss=0.03306 | best_loss=0.03283
Epoch 26/80: current_loss=0.03286 | best_loss=0.03283
Epoch 27/80: current_loss=0.03320 | best_loss=0.03283
Epoch 28/80: current_loss=0.03306 | best_loss=0.03283
Epoch 29/80: current_loss=0.03303 | best_loss=0.03283
Epoch 30/80: current_loss=0.03307 | best_loss=0.03283
Epoch 31/80: current_loss=0.03290 | best_loss=0.03283
Epoch 32/80: current_loss=0.03286 | best_loss=0.03283
Epoch 33/80: current_loss=0.03283 | best_loss=0.03283
Epoch 34/80: current_loss=0.03287 | best_loss=0.03283
Epoch 35/80: current_loss=0.03289 | best_loss=0.03283
Epoch 36/80: current_loss=0.03285 | best_loss=0.03283
Epoch 37/80: current_loss=0.03285 | best_loss=0.03283
Epoch 38/80: current_loss=0.03289 | best_loss=0.03283
Epoch 39/80: current_loss=0.03303 | best_loss=0.03283
Epoch 40/80: current_loss=0.03286 | best_loss=0.03283
Epoch 41/80: current_loss=0.03285 | best_loss=0.03283
Early Stopping at epoch 41
      explained_var=0.00003 | mse_loss=0.03260
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.07980 | best_loss=0.07980
Epoch 1/80: current_loss=0.04808 | best_loss=0.04808
Epoch 2/80: current_loss=0.11575 | best_loss=0.04808
Epoch 3/80: current_loss=0.08963 | best_loss=0.04808
Epoch 4/80: current_loss=0.17724 | best_loss=0.04808
Epoch 5/80: current_loss=0.16911 | best_loss=0.04808
Epoch 6/80: current_loss=0.12022 | best_loss=0.04808
Epoch 7/80: current_loss=0.08648 | best_loss=0.04808
Epoch 8/80: current_loss=0.37965 | best_loss=0.04808
Epoch 9/80: current_loss=0.05610 | best_loss=0.04808
Epoch 10/80: current_loss=0.06990 | best_loss=0.04808
Epoch 11/80: current_loss=0.03622 | best_loss=0.03622
Epoch 12/80: current_loss=0.03617 | best_loss=0.03617
Epoch 13/80: current_loss=0.03616 | best_loss=0.03616
Epoch 14/80: current_loss=0.03634 | best_loss=0.03616
Epoch 15/80: current_loss=0.03639 | best_loss=0.03616
Epoch 16/80: current_loss=0.03629 | best_loss=0.03616
Epoch 17/80: current_loss=0.03620 | best_loss=0.03616
Epoch 18/80: current_loss=0.03619 | best_loss=0.03616
Epoch 19/80: current_loss=0.03619 | best_loss=0.03616
Epoch 20/80: current_loss=0.03620 | best_loss=0.03616
Epoch 21/80: current_loss=0.03619 | best_loss=0.03616
Epoch 22/80: current_loss=0.03632 | best_loss=0.03616
Epoch 23/80: current_loss=0.03622 | best_loss=0.03616
Epoch 24/80: current_loss=0.03622 | best_loss=0.03616
Epoch 25/80: current_loss=0.03621 | best_loss=0.03616
Epoch 26/80: current_loss=0.03630 | best_loss=0.03616
Epoch 27/80: current_loss=0.03620 | best_loss=0.03616
Epoch 28/80: current_loss=0.03619 | best_loss=0.03616
Epoch 29/80: current_loss=0.03630 | best_loss=0.03616
Epoch 30/80: current_loss=0.03619 | best_loss=0.03616
Epoch 31/80: current_loss=0.03634 | best_loss=0.03616
Epoch 32/80: current_loss=0.03623 | best_loss=0.03616
Epoch 33/80: current_loss=0.03619 | best_loss=0.03616
Early Stopping at epoch 33
      explained_var=0.00054 | mse_loss=0.03586
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.00398| avg_loss=0.03531
----------------------------------------------


----------------------------------------------
Params for Trial 20
{'learning_rate': 0.1, 'weight_decay': 0.003887858949015846, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.51317 | best_loss=0.51317
Epoch 1/80: current_loss=0.08412 | best_loss=0.08412
Epoch 2/80: current_loss=0.04355 | best_loss=0.04355
Epoch 3/80: current_loss=0.03858 | best_loss=0.03858
Epoch 4/80: current_loss=0.04529 | best_loss=0.03858
Epoch 5/80: current_loss=0.05278 | best_loss=0.03858
Epoch 6/80: current_loss=0.04237 | best_loss=0.03858
Epoch 7/80: current_loss=0.03937 | best_loss=0.03858
Epoch 8/80: current_loss=0.07803 | best_loss=0.03858
Epoch 9/80: current_loss=0.04852 | best_loss=0.03858
Epoch 10/80: current_loss=0.04187 | best_loss=0.03858
Epoch 11/80: current_loss=0.04115 | best_loss=0.03858
Epoch 12/80: current_loss=0.04252 | best_loss=0.03858
Epoch 13/80: current_loss=0.05233 | best_loss=0.03858
Epoch 14/80: current_loss=0.05253 | best_loss=0.03858
Epoch 15/80: current_loss=0.04588 | best_loss=0.03858
Epoch 16/80: current_loss=0.03854 | best_loss=0.03854
Epoch 17/80: current_loss=0.03833 | best_loss=0.03833
Epoch 18/80: current_loss=0.04026 | best_loss=0.03833
Epoch 19/80: current_loss=0.03829 | best_loss=0.03829
Epoch 20/80: current_loss=0.03777 | best_loss=0.03777
Epoch 21/80: current_loss=0.03809 | best_loss=0.03777
Epoch 22/80: current_loss=0.04016 | best_loss=0.03777
Epoch 23/80: current_loss=0.03925 | best_loss=0.03777
Epoch 24/80: current_loss=0.03910 | best_loss=0.03777
Epoch 25/80: current_loss=0.03822 | best_loss=0.03777
Epoch 26/80: current_loss=0.03828 | best_loss=0.03777
Epoch 27/80: current_loss=0.03833 | best_loss=0.03777
Epoch 28/80: current_loss=0.03911 | best_loss=0.03777
Epoch 29/80: current_loss=0.03843 | best_loss=0.03777
Epoch 30/80: current_loss=0.03845 | best_loss=0.03777
Epoch 31/80: current_loss=0.03832 | best_loss=0.03777
Epoch 32/80: current_loss=0.03857 | best_loss=0.03777
Epoch 33/80: current_loss=0.03834 | best_loss=0.03777
Epoch 34/80: current_loss=0.03901 | best_loss=0.03777
Epoch 35/80: current_loss=0.03859 | best_loss=0.03777
Epoch 36/80: current_loss=0.03846 | best_loss=0.03777
Epoch 37/80: current_loss=0.03846 | best_loss=0.03777
Epoch 38/80: current_loss=0.03851 | best_loss=0.03777
Epoch 39/80: current_loss=0.03943 | best_loss=0.03777
Epoch 40/80: current_loss=0.03835 | best_loss=0.03777
Early Stopping at epoch 40
      explained_var=0.01358 | mse_loss=0.03857
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=2.55929 | best_loss=2.55929
Epoch 1/80: current_loss=1.00716 | best_loss=1.00716
Epoch 2/80: current_loss=0.27002 | best_loss=0.27002
Epoch 3/80: current_loss=0.06428 | best_loss=0.06428
Epoch 4/80: current_loss=0.04315 | best_loss=0.04315
Epoch 5/80: current_loss=0.04314 | best_loss=0.04314
Epoch 6/80: current_loss=0.05438 | best_loss=0.04314
Epoch 7/80: current_loss=0.05067 | best_loss=0.04314
Epoch 8/80: current_loss=0.04379 | best_loss=0.04314
Epoch 9/80: current_loss=0.04661 | best_loss=0.04314
Epoch 10/80: current_loss=0.04489 | best_loss=0.04314
Epoch 11/80: current_loss=0.06282 | best_loss=0.04314
Epoch 12/80: current_loss=0.04374 | best_loss=0.04314
Epoch 13/80: current_loss=0.05926 | best_loss=0.04314
Epoch 14/80: current_loss=0.04280 | best_loss=0.04280
Epoch 15/80: current_loss=0.04260 | best_loss=0.04260
Epoch 16/80: current_loss=0.04270 | best_loss=0.04260
Epoch 17/80: current_loss=0.04265 | best_loss=0.04260
Epoch 18/80: current_loss=0.04265 | best_loss=0.04260
Epoch 19/80: current_loss=0.04268 | best_loss=0.04260
Epoch 20/80: current_loss=0.04263 | best_loss=0.04260
Epoch 21/80: current_loss=0.04298 | best_loss=0.04260
Epoch 22/80: current_loss=0.04274 | best_loss=0.04260
Epoch 23/80: current_loss=0.04264 | best_loss=0.04260
Epoch 24/80: current_loss=0.04272 | best_loss=0.04260
Epoch 25/80: current_loss=0.04265 | best_loss=0.04260
Epoch 26/80: current_loss=0.04295 | best_loss=0.04260
Epoch 27/80: current_loss=0.04269 | best_loss=0.04260
Epoch 28/80: current_loss=0.04267 | best_loss=0.04260
Epoch 29/80: current_loss=0.04270 | best_loss=0.04260
Epoch 30/80: current_loss=0.04286 | best_loss=0.04260
Epoch 31/80: current_loss=0.04275 | best_loss=0.04260
Epoch 32/80: current_loss=0.04264 | best_loss=0.04260
Epoch 33/80: current_loss=0.04270 | best_loss=0.04260
Epoch 34/80: current_loss=0.04303 | best_loss=0.04260
Epoch 35/80: current_loss=0.04264 | best_loss=0.04260
Early Stopping at epoch 35
      explained_var=0.00103 | mse_loss=0.04121
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=2.97681 | best_loss=2.97681
Epoch 1/80: current_loss=0.03235 | best_loss=0.03235
Epoch 2/80: current_loss=0.03132 | best_loss=0.03132
Epoch 3/80: current_loss=0.02950 | best_loss=0.02950
Epoch 4/80: current_loss=0.04194 | best_loss=0.02950
Epoch 5/80: current_loss=0.03012 | best_loss=0.02950
Epoch 6/80: current_loss=0.03017 | best_loss=0.02950
Epoch 7/80: current_loss=0.03033 | best_loss=0.02950
Epoch 8/80: current_loss=0.02798 | best_loss=0.02798
Epoch 9/80: current_loss=0.02864 | best_loss=0.02798
Epoch 10/80: current_loss=0.02857 | best_loss=0.02798
Epoch 11/80: current_loss=0.02816 | best_loss=0.02798
Epoch 12/80: current_loss=0.02800 | best_loss=0.02798
Epoch 13/80: current_loss=0.02839 | best_loss=0.02798
Epoch 14/80: current_loss=0.02835 | best_loss=0.02798
Epoch 15/80: current_loss=0.02917 | best_loss=0.02798
Epoch 16/80: current_loss=0.02841 | best_loss=0.02798
Epoch 17/80: current_loss=0.02823 | best_loss=0.02798
Epoch 18/80: current_loss=0.02856 | best_loss=0.02798
Epoch 19/80: current_loss=0.02814 | best_loss=0.02798
Epoch 20/80: current_loss=0.02852 | best_loss=0.02798
Epoch 21/80: current_loss=0.02830 | best_loss=0.02798
Epoch 22/80: current_loss=0.02858 | best_loss=0.02798
Epoch 23/80: current_loss=0.02796 | best_loss=0.02796
Epoch 24/80: current_loss=0.02851 | best_loss=0.02796
Epoch 25/80: current_loss=0.02807 | best_loss=0.02796
Epoch 26/80: current_loss=0.02839 | best_loss=0.02796
Epoch 27/80: current_loss=0.02825 | best_loss=0.02796
Epoch 28/80: current_loss=0.02811 | best_loss=0.02796
Epoch 29/80: current_loss=0.02832 | best_loss=0.02796
Epoch 30/80: current_loss=0.02827 | best_loss=0.02796
Epoch 31/80: current_loss=0.02818 | best_loss=0.02796
Epoch 32/80: current_loss=0.02817 | best_loss=0.02796
Epoch 33/80: current_loss=0.02835 | best_loss=0.02796
Epoch 34/80: current_loss=0.02807 | best_loss=0.02796
Epoch 35/80: current_loss=0.02821 | best_loss=0.02796
Epoch 36/80: current_loss=0.02819 | best_loss=0.02796
Epoch 37/80: current_loss=0.02828 | best_loss=0.02796
Epoch 38/80: current_loss=0.02850 | best_loss=0.02796
Epoch 39/80: current_loss=0.02808 | best_loss=0.02796
Epoch 40/80: current_loss=0.02827 | best_loss=0.02796
Epoch 41/80: current_loss=0.02866 | best_loss=0.02796
Epoch 42/80: current_loss=0.02797 | best_loss=0.02796
Epoch 43/80: current_loss=0.02876 | best_loss=0.02796
Early Stopping at epoch 43
      explained_var=-0.00000 | mse_loss=0.02836
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=3.16381 | best_loss=3.16381
Epoch 1/80: current_loss=0.04472 | best_loss=0.04472
Epoch 2/80: current_loss=0.34835 | best_loss=0.04472
Epoch 3/80: current_loss=0.03794 | best_loss=0.03794
Epoch 4/80: current_loss=0.05479 | best_loss=0.03794
Epoch 5/80: current_loss=0.12407 | best_loss=0.03794
Epoch 6/80: current_loss=0.05178 | best_loss=0.03794
Epoch 7/80: current_loss=0.05366 | best_loss=0.03794
Epoch 8/80: current_loss=0.06601 | best_loss=0.03794
Epoch 9/80: current_loss=0.07499 | best_loss=0.03794
Epoch 10/80: current_loss=0.14966 | best_loss=0.03794
Epoch 11/80: current_loss=0.03561 | best_loss=0.03561
Epoch 12/80: current_loss=0.04283 | best_loss=0.03561
Epoch 13/80: current_loss=0.05883 | best_loss=0.03561
Epoch 14/80: current_loss=0.03476 | best_loss=0.03476
Epoch 15/80: current_loss=0.03492 | best_loss=0.03476
Epoch 16/80: current_loss=0.04406 | best_loss=0.03476
Epoch 17/80: current_loss=0.03466 | best_loss=0.03466
Epoch 18/80: current_loss=0.04068 | best_loss=0.03466
Epoch 19/80: current_loss=0.03390 | best_loss=0.03390
Epoch 20/80: current_loss=0.03712 | best_loss=0.03390
Epoch 21/80: current_loss=0.03376 | best_loss=0.03376
Epoch 22/80: current_loss=0.03301 | best_loss=0.03301
Epoch 23/80: current_loss=0.03287 | best_loss=0.03287
Epoch 24/80: current_loss=0.03294 | best_loss=0.03287
Epoch 25/80: current_loss=0.03291 | best_loss=0.03287
Epoch 26/80: current_loss=0.03283 | best_loss=0.03283
Epoch 27/80: current_loss=0.03320 | best_loss=0.03283
Epoch 28/80: current_loss=0.03295 | best_loss=0.03283
Epoch 29/80: current_loss=0.03288 | best_loss=0.03283
Epoch 30/80: current_loss=0.03316 | best_loss=0.03283
Epoch 31/80: current_loss=0.03294 | best_loss=0.03283
Epoch 32/80: current_loss=0.03286 | best_loss=0.03283
Epoch 33/80: current_loss=0.03303 | best_loss=0.03283
Epoch 34/80: current_loss=0.03326 | best_loss=0.03283
Epoch 35/80: current_loss=0.03284 | best_loss=0.03283
Epoch 36/80: current_loss=0.03325 | best_loss=0.03283
Epoch 37/80: current_loss=0.03302 | best_loss=0.03283
Epoch 38/80: current_loss=0.03330 | best_loss=0.03283
Epoch 39/80: current_loss=0.03321 | best_loss=0.03283
Epoch 40/80: current_loss=0.03287 | best_loss=0.03283
Epoch 41/80: current_loss=0.03284 | best_loss=0.03283
Epoch 42/80: current_loss=0.03289 | best_loss=0.03283
Epoch 43/80: current_loss=0.03316 | best_loss=0.03283
Epoch 44/80: current_loss=0.03286 | best_loss=0.03283
Epoch 45/80: current_loss=0.03298 | best_loss=0.03283
Epoch 46/80: current_loss=0.03305 | best_loss=0.03283
Early Stopping at epoch 46
      explained_var=0.00013 | mse_loss=0.03260
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=1.86008 | best_loss=1.86008
Epoch 1/80: current_loss=1.05391 | best_loss=1.05391
Epoch 2/80: current_loss=0.04883 | best_loss=0.04883
Epoch 3/80: current_loss=0.03852 | best_loss=0.03852
Epoch 4/80: current_loss=0.03630 | best_loss=0.03630
Epoch 5/80: current_loss=0.03830 | best_loss=0.03630
Epoch 6/80: current_loss=0.03655 | best_loss=0.03630
Epoch 7/80: current_loss=0.04210 | best_loss=0.03630
Epoch 8/80: current_loss=0.28958 | best_loss=0.03630
Epoch 9/80: current_loss=0.09997 | best_loss=0.03630
Epoch 10/80: current_loss=0.17029 | best_loss=0.03630
Epoch 11/80: current_loss=0.08335 | best_loss=0.03630
Epoch 12/80: current_loss=0.07571 | best_loss=0.03630
Epoch 13/80: current_loss=0.04420 | best_loss=0.03630
Epoch 14/80: current_loss=0.09831 | best_loss=0.03630
Epoch 15/80: current_loss=0.03587 | best_loss=0.03587
Epoch 16/80: current_loss=0.03995 | best_loss=0.03587
Epoch 17/80: current_loss=0.04204 | best_loss=0.03587
Epoch 18/80: current_loss=0.04134 | best_loss=0.03587
Epoch 19/80: current_loss=0.03738 | best_loss=0.03587
Epoch 20/80: current_loss=0.03630 | best_loss=0.03587
Epoch 21/80: current_loss=0.05346 | best_loss=0.03587
Epoch 22/80: current_loss=0.04311 | best_loss=0.03587
Epoch 23/80: current_loss=0.03849 | best_loss=0.03587
Epoch 24/80: current_loss=0.03965 | best_loss=0.03587
Epoch 25/80: current_loss=0.03717 | best_loss=0.03587
Epoch 26/80: current_loss=0.03635 | best_loss=0.03587
Epoch 27/80: current_loss=0.04379 | best_loss=0.03587
Epoch 28/80: current_loss=0.03766 | best_loss=0.03587
Epoch 29/80: current_loss=0.03895 | best_loss=0.03587
Epoch 30/80: current_loss=0.03630 | best_loss=0.03587
Epoch 31/80: current_loss=0.03642 | best_loss=0.03587
Epoch 32/80: current_loss=0.03749 | best_loss=0.03587
Epoch 33/80: current_loss=0.03633 | best_loss=0.03587
Epoch 34/80: current_loss=0.03666 | best_loss=0.03587
Epoch 35/80: current_loss=0.04073 | best_loss=0.03587
Early Stopping at epoch 35
      explained_var=0.00954 | mse_loss=0.03556
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.00486| avg_loss=0.03526
----------------------------------------------


----------------------------------------------
Params for Trial 21
{'learning_rate': 0.0001, 'weight_decay': 0.0007833142441603685, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.15326 | best_loss=0.15326
Epoch 1/80: current_loss=0.05016 | best_loss=0.05016
Epoch 2/80: current_loss=0.04807 | best_loss=0.04807
Epoch 3/80: current_loss=0.04702 | best_loss=0.04702
Epoch 4/80: current_loss=0.04574 | best_loss=0.04574
Epoch 5/80: current_loss=0.04500 | best_loss=0.04500
Epoch 6/80: current_loss=0.04419 | best_loss=0.04419
Epoch 7/80: current_loss=0.04328 | best_loss=0.04328
Epoch 8/80: current_loss=0.04269 | best_loss=0.04269
Epoch 9/80: current_loss=0.04221 | best_loss=0.04221
Epoch 10/80: current_loss=0.04162 | best_loss=0.04162
Epoch 11/80: current_loss=0.04123 | best_loss=0.04123
Epoch 12/80: current_loss=0.04080 | best_loss=0.04080
Epoch 13/80: current_loss=0.04069 | best_loss=0.04069
Epoch 14/80: current_loss=0.04035 | best_loss=0.04035
Epoch 15/80: current_loss=0.04005 | best_loss=0.04005
Epoch 16/80: current_loss=0.03991 | best_loss=0.03991
Epoch 17/80: current_loss=0.03971 | best_loss=0.03971
Epoch 18/80: current_loss=0.03947 | best_loss=0.03947
Epoch 19/80: current_loss=0.03918 | best_loss=0.03918
Epoch 20/80: current_loss=0.03905 | best_loss=0.03905
Epoch 21/80: current_loss=0.03897 | best_loss=0.03897
Epoch 22/80: current_loss=0.03894 | best_loss=0.03894
Epoch 23/80: current_loss=0.03867 | best_loss=0.03867
Epoch 24/80: current_loss=0.03851 | best_loss=0.03851
Epoch 25/80: current_loss=0.03861 | best_loss=0.03851
Epoch 26/80: current_loss=0.03844 | best_loss=0.03844
Epoch 27/80: current_loss=0.03839 | best_loss=0.03839
Epoch 28/80: current_loss=0.03850 | best_loss=0.03839
Epoch 29/80: current_loss=0.03829 | best_loss=0.03829
Epoch 30/80: current_loss=0.03845 | best_loss=0.03829
Epoch 31/80: current_loss=0.03826 | best_loss=0.03826
Epoch 32/80: current_loss=0.03827 | best_loss=0.03826
Epoch 33/80: current_loss=0.03820 | best_loss=0.03820
Epoch 34/80: current_loss=0.03813 | best_loss=0.03813
Epoch 35/80: current_loss=0.03841 | best_loss=0.03813
Epoch 36/80: current_loss=0.03798 | best_loss=0.03798
Epoch 37/80: current_loss=0.03821 | best_loss=0.03798
Epoch 38/80: current_loss=0.03791 | best_loss=0.03791
Epoch 39/80: current_loss=0.03796 | best_loss=0.03791
Epoch 40/80: current_loss=0.03793 | best_loss=0.03791
Epoch 41/80: current_loss=0.03776 | best_loss=0.03776
Epoch 42/80: current_loss=0.03784 | best_loss=0.03776
Epoch 43/80: current_loss=0.03778 | best_loss=0.03776
Epoch 44/80: current_loss=0.03785 | best_loss=0.03776
Epoch 45/80: current_loss=0.03770 | best_loss=0.03770
Epoch 46/80: current_loss=0.03777 | best_loss=0.03770
Epoch 47/80: current_loss=0.03777 | best_loss=0.03770
Epoch 48/80: current_loss=0.03785 | best_loss=0.03770
Epoch 49/80: current_loss=0.03785 | best_loss=0.03770
Epoch 50/80: current_loss=0.03771 | best_loss=0.03770
Epoch 51/80: current_loss=0.03781 | best_loss=0.03770
Epoch 52/80: current_loss=0.03773 | best_loss=0.03770
Epoch 53/80: current_loss=0.03778 | best_loss=0.03770
Epoch 54/80: current_loss=0.03770 | best_loss=0.03770
Epoch 55/80: current_loss=0.03771 | best_loss=0.03770
Epoch 56/80: current_loss=0.03773 | best_loss=0.03770
Epoch 57/80: current_loss=0.03792 | best_loss=0.03770
Epoch 58/80: current_loss=0.03821 | best_loss=0.03770
Epoch 59/80: current_loss=0.03772 | best_loss=0.03770
Epoch 60/80: current_loss=0.03763 | best_loss=0.03763
Epoch 61/80: current_loss=0.03761 | best_loss=0.03761
Epoch 62/80: current_loss=0.03758 | best_loss=0.03758
Epoch 63/80: current_loss=0.03754 | best_loss=0.03754
Epoch 64/80: current_loss=0.03766 | best_loss=0.03754
Epoch 65/80: current_loss=0.03753 | best_loss=0.03753
Epoch 66/80: current_loss=0.03755 | best_loss=0.03753
Epoch 67/80: current_loss=0.03747 | best_loss=0.03747
Epoch 68/80: current_loss=0.03754 | best_loss=0.03747
Epoch 69/80: current_loss=0.03757 | best_loss=0.03747
Epoch 70/80: current_loss=0.03744 | best_loss=0.03744
Epoch 71/80: current_loss=0.03749 | best_loss=0.03744
Epoch 72/80: current_loss=0.03751 | best_loss=0.03744
Epoch 73/80: current_loss=0.03770 | best_loss=0.03744
Epoch 74/80: current_loss=0.03757 | best_loss=0.03744
Epoch 75/80: current_loss=0.03768 | best_loss=0.03744
Epoch 76/80: current_loss=0.03759 | best_loss=0.03744
Epoch 77/80: current_loss=0.03756 | best_loss=0.03744
Epoch 78/80: current_loss=0.03769 | best_loss=0.03744
Epoch 79/80: current_loss=0.03768 | best_loss=0.03744
      explained_var=0.02090 | mse_loss=0.03834
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04110 | best_loss=0.04110
Epoch 1/80: current_loss=0.04128 | best_loss=0.04110
Epoch 2/80: current_loss=0.04103 | best_loss=0.04103
Epoch 3/80: current_loss=0.04112 | best_loss=0.04103
Epoch 4/80: current_loss=0.04124 | best_loss=0.04103
Epoch 5/80: current_loss=0.04142 | best_loss=0.04103
Epoch 6/80: current_loss=0.04125 | best_loss=0.04103
Epoch 7/80: current_loss=0.04128 | best_loss=0.04103
Epoch 8/80: current_loss=0.04143 | best_loss=0.04103
Epoch 9/80: current_loss=0.04126 | best_loss=0.04103
Epoch 10/80: current_loss=0.04144 | best_loss=0.04103
Epoch 11/80: current_loss=0.04133 | best_loss=0.04103
Epoch 12/80: current_loss=0.04165 | best_loss=0.04103
Epoch 13/80: current_loss=0.04140 | best_loss=0.04103
Epoch 14/80: current_loss=0.04182 | best_loss=0.04103
Epoch 15/80: current_loss=0.04147 | best_loss=0.04103
Epoch 16/80: current_loss=0.04144 | best_loss=0.04103
Epoch 17/80: current_loss=0.04140 | best_loss=0.04103
Epoch 18/80: current_loss=0.04163 | best_loss=0.04103
Epoch 19/80: current_loss=0.04129 | best_loss=0.04103
Epoch 20/80: current_loss=0.04147 | best_loss=0.04103
Epoch 21/80: current_loss=0.04129 | best_loss=0.04103
Epoch 22/80: current_loss=0.04163 | best_loss=0.04103
Early Stopping at epoch 22
      explained_var=0.04115 | mse_loss=0.03954
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02815 | best_loss=0.02815
Epoch 1/80: current_loss=0.02848 | best_loss=0.02815
Epoch 2/80: current_loss=0.02852 | best_loss=0.02815
Epoch 3/80: current_loss=0.02830 | best_loss=0.02815
Epoch 4/80: current_loss=0.02825 | best_loss=0.02815
Epoch 5/80: current_loss=0.02860 | best_loss=0.02815
Epoch 6/80: current_loss=0.02848 | best_loss=0.02815
Epoch 7/80: current_loss=0.02836 | best_loss=0.02815
Epoch 8/80: current_loss=0.02860 | best_loss=0.02815
Epoch 9/80: current_loss=0.02832 | best_loss=0.02815
Epoch 10/80: current_loss=0.02876 | best_loss=0.02815
Epoch 11/80: current_loss=0.02840 | best_loss=0.02815
Epoch 12/80: current_loss=0.02880 | best_loss=0.02815
Epoch 13/80: current_loss=0.02855 | best_loss=0.02815
Epoch 14/80: current_loss=0.02845 | best_loss=0.02815
Epoch 15/80: current_loss=0.02860 | best_loss=0.02815
Epoch 16/80: current_loss=0.02840 | best_loss=0.02815
Epoch 17/80: current_loss=0.02843 | best_loss=0.02815
Epoch 18/80: current_loss=0.02830 | best_loss=0.02815
Epoch 19/80: current_loss=0.02847 | best_loss=0.02815
Epoch 20/80: current_loss=0.02926 | best_loss=0.02815
Early Stopping at epoch 20
      explained_var=-0.00752 | mse_loss=0.02861

----------------------------------------------
Params for Trial 22
{'learning_rate': 0.0001, 'weight_decay': 0.0007133505662011086, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05662 | best_loss=0.05662
Epoch 1/80: current_loss=0.04257 | best_loss=0.04257
Epoch 2/80: current_loss=0.04198 | best_loss=0.04198
Epoch 3/80: current_loss=0.04151 | best_loss=0.04151
Epoch 4/80: current_loss=0.04097 | best_loss=0.04097
Epoch 5/80: current_loss=0.04070 | best_loss=0.04070
Epoch 6/80: current_loss=0.04053 | best_loss=0.04053
Epoch 7/80: current_loss=0.04017 | best_loss=0.04017
Epoch 8/80: current_loss=0.03995 | best_loss=0.03995
Epoch 9/80: current_loss=0.03970 | best_loss=0.03970
Epoch 10/80: current_loss=0.03937 | best_loss=0.03937
Epoch 11/80: current_loss=0.03939 | best_loss=0.03937
Epoch 12/80: current_loss=0.03900 | best_loss=0.03900
Epoch 13/80: current_loss=0.03906 | best_loss=0.03900
Epoch 14/80: current_loss=0.03878 | best_loss=0.03878
Epoch 15/80: current_loss=0.03897 | best_loss=0.03878
Epoch 16/80: current_loss=0.03863 | best_loss=0.03863
Epoch 17/80: current_loss=0.03849 | best_loss=0.03849
Epoch 18/80: current_loss=0.03843 | best_loss=0.03843
Epoch 19/80: current_loss=0.03834 | best_loss=0.03834
Epoch 20/80: current_loss=0.03844 | best_loss=0.03834
Epoch 21/80: current_loss=0.03827 | best_loss=0.03827
Epoch 22/80: current_loss=0.03809 | best_loss=0.03809
Epoch 23/80: current_loss=0.03809 | best_loss=0.03809
Epoch 24/80: current_loss=0.03800 | best_loss=0.03800
Epoch 25/80: current_loss=0.03812 | best_loss=0.03800
Epoch 26/80: current_loss=0.03811 | best_loss=0.03800
Epoch 27/80: current_loss=0.03801 | best_loss=0.03800
Epoch 28/80: current_loss=0.03789 | best_loss=0.03789
Epoch 29/80: current_loss=0.03795 | best_loss=0.03789
Epoch 30/80: current_loss=0.03796 | best_loss=0.03789
Epoch 31/80: current_loss=0.03785 | best_loss=0.03785
Epoch 32/80: current_loss=0.03790 | best_loss=0.03785
Epoch 33/80: current_loss=0.03785 | best_loss=0.03785
Epoch 34/80: current_loss=0.03785 | best_loss=0.03785
Epoch 35/80: current_loss=0.03799 | best_loss=0.03785
Epoch 36/80: current_loss=0.03784 | best_loss=0.03784
Epoch 37/80: current_loss=0.03793 | best_loss=0.03784
Epoch 38/80: current_loss=0.03784 | best_loss=0.03784
Epoch 39/80: current_loss=0.03781 | best_loss=0.03781
Epoch 40/80: current_loss=0.03782 | best_loss=0.03781
Epoch 41/80: current_loss=0.03809 | best_loss=0.03781
Epoch 42/80: current_loss=0.03783 | best_loss=0.03781
Epoch 43/80: current_loss=0.03799 | best_loss=0.03781
Epoch 44/80: current_loss=0.03764 | best_loss=0.03764
Epoch 45/80: current_loss=0.03766 | best_loss=0.03764
Epoch 46/80: current_loss=0.03767 | best_loss=0.03764
Epoch 47/80: current_loss=0.03783 | best_loss=0.03764
Epoch 48/80: current_loss=0.03777 | best_loss=0.03764
Epoch 49/80: current_loss=0.03765 | best_loss=0.03764
Epoch 50/80: current_loss=0.03776 | best_loss=0.03764
Epoch 51/80: current_loss=0.03772 | best_loss=0.03764
Epoch 52/80: current_loss=0.03777 | best_loss=0.03764
Epoch 53/80: current_loss=0.03766 | best_loss=0.03764
Epoch 54/80: current_loss=0.03766 | best_loss=0.03764
Epoch 55/80: current_loss=0.03762 | best_loss=0.03762
Epoch 56/80: current_loss=0.03766 | best_loss=0.03762
Epoch 57/80: current_loss=0.03759 | best_loss=0.03759
Epoch 58/80: current_loss=0.03766 | best_loss=0.03759
Epoch 59/80: current_loss=0.03761 | best_loss=0.03759
Epoch 60/80: current_loss=0.03766 | best_loss=0.03759
Epoch 61/80: current_loss=0.03758 | best_loss=0.03758
Epoch 62/80: current_loss=0.03775 | best_loss=0.03758
Epoch 63/80: current_loss=0.03759 | best_loss=0.03758
Epoch 64/80: current_loss=0.03756 | best_loss=0.03756
Epoch 65/80: current_loss=0.03750 | best_loss=0.03750
Epoch 66/80: current_loss=0.03769 | best_loss=0.03750
Epoch 67/80: current_loss=0.03762 | best_loss=0.03750
Epoch 68/80: current_loss=0.03758 | best_loss=0.03750
Epoch 69/80: current_loss=0.03764 | best_loss=0.03750
Epoch 70/80: current_loss=0.03770 | best_loss=0.03750
Epoch 71/80: current_loss=0.03773 | best_loss=0.03750
Epoch 72/80: current_loss=0.03772 | best_loss=0.03750
Epoch 73/80: current_loss=0.03757 | best_loss=0.03750
Epoch 74/80: current_loss=0.03757 | best_loss=0.03750
Epoch 75/80: current_loss=0.03782 | best_loss=0.03750
Epoch 76/80: current_loss=0.03756 | best_loss=0.03750
Epoch 77/80: current_loss=0.03761 | best_loss=0.03750
Epoch 78/80: current_loss=0.03763 | best_loss=0.03750
Epoch 79/80: current_loss=0.03758 | best_loss=0.03750
      explained_var=0.01883 | mse_loss=0.03841
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04125 | best_loss=0.04125
Epoch 1/80: current_loss=0.04171 | best_loss=0.04125
Epoch 2/80: current_loss=0.04138 | best_loss=0.04125
Epoch 3/80: current_loss=0.04148 | best_loss=0.04125
Epoch 4/80: current_loss=0.04151 | best_loss=0.04125
Epoch 5/80: current_loss=0.04152 | best_loss=0.04125
Epoch 6/80: current_loss=0.04159 | best_loss=0.04125
Epoch 7/80: current_loss=0.04167 | best_loss=0.04125
Epoch 8/80: current_loss=0.04153 | best_loss=0.04125
Epoch 9/80: current_loss=0.04197 | best_loss=0.04125
Epoch 10/80: current_loss=0.04158 | best_loss=0.04125
Epoch 11/80: current_loss=0.04172 | best_loss=0.04125
Epoch 12/80: current_loss=0.04155 | best_loss=0.04125
Epoch 13/80: current_loss=0.04157 | best_loss=0.04125
Epoch 14/80: current_loss=0.04157 | best_loss=0.04125
Epoch 15/80: current_loss=0.04167 | best_loss=0.04125
Epoch 16/80: current_loss=0.04160 | best_loss=0.04125
Epoch 17/80: current_loss=0.04169 | best_loss=0.04125
Epoch 18/80: current_loss=0.04158 | best_loss=0.04125
Epoch 19/80: current_loss=0.04173 | best_loss=0.04125
Epoch 20/80: current_loss=0.04153 | best_loss=0.04125
Early Stopping at epoch 20
      explained_var=0.03572 | mse_loss=0.03977
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02848 | best_loss=0.02848
Epoch 1/80: current_loss=0.02808 | best_loss=0.02808
Epoch 2/80: current_loss=0.02825 | best_loss=0.02808
Epoch 3/80: current_loss=0.02835 | best_loss=0.02808
Epoch 4/80: current_loss=0.02845 | best_loss=0.02808
Epoch 5/80: current_loss=0.02821 | best_loss=0.02808
Epoch 6/80: current_loss=0.02829 | best_loss=0.02808
Epoch 7/80: current_loss=0.02819 | best_loss=0.02808
Epoch 8/80: current_loss=0.02806 | best_loss=0.02806
Epoch 9/80: current_loss=0.02830 | best_loss=0.02806
Epoch 10/80: current_loss=0.02823 | best_loss=0.02806
Epoch 11/80: current_loss=0.02821 | best_loss=0.02806
Epoch 12/80: current_loss=0.02823 | best_loss=0.02806
Epoch 13/80: current_loss=0.02830 | best_loss=0.02806
Epoch 14/80: current_loss=0.02838 | best_loss=0.02806
Epoch 15/80: current_loss=0.02839 | best_loss=0.02806
Epoch 16/80: current_loss=0.02841 | best_loss=0.02806
Epoch 17/80: current_loss=0.02815 | best_loss=0.02806
Epoch 18/80: current_loss=0.02847 | best_loss=0.02806
Epoch 19/80: current_loss=0.02829 | best_loss=0.02806
Epoch 20/80: current_loss=0.02824 | best_loss=0.02806
Epoch 21/80: current_loss=0.02838 | best_loss=0.02806
Epoch 22/80: current_loss=0.02840 | best_loss=0.02806
Epoch 23/80: current_loss=0.02823 | best_loss=0.02806
Epoch 24/80: current_loss=0.02836 | best_loss=0.02806
Epoch 25/80: current_loss=0.02819 | best_loss=0.02806
Epoch 26/80: current_loss=0.02818 | best_loss=0.02806
Epoch 27/80: current_loss=0.02825 | best_loss=0.02806
Epoch 28/80: current_loss=0.02830 | best_loss=0.02806
Early Stopping at epoch 28
      explained_var=-0.00385 | mse_loss=0.02849

----------------------------------------------
Params for Trial 23
{'learning_rate': 0.0001, 'weight_decay': 0.0022114961168883873, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05221 | best_loss=0.05221
Epoch 1/80: current_loss=0.04419 | best_loss=0.04419
Epoch 2/80: current_loss=0.04288 | best_loss=0.04288
Epoch 3/80: current_loss=0.04194 | best_loss=0.04194
Epoch 4/80: current_loss=0.04149 | best_loss=0.04149
Epoch 5/80: current_loss=0.04072 | best_loss=0.04072
Epoch 6/80: current_loss=0.04028 | best_loss=0.04028
Epoch 7/80: current_loss=0.04002 | best_loss=0.04002
Epoch 8/80: current_loss=0.03981 | best_loss=0.03981
Epoch 9/80: current_loss=0.03946 | best_loss=0.03946
Epoch 10/80: current_loss=0.03929 | best_loss=0.03929
Epoch 11/80: current_loss=0.03911 | best_loss=0.03911
Epoch 12/80: current_loss=0.03894 | best_loss=0.03894
Epoch 13/80: current_loss=0.03891 | best_loss=0.03891
Epoch 14/80: current_loss=0.03867 | best_loss=0.03867
Epoch 15/80: current_loss=0.03860 | best_loss=0.03860
Epoch 16/80: current_loss=0.03837 | best_loss=0.03837
Epoch 17/80: current_loss=0.03834 | best_loss=0.03834
Epoch 18/80: current_loss=0.03828 | best_loss=0.03828
Epoch 19/80: current_loss=0.03813 | best_loss=0.03813
Epoch 20/80: current_loss=0.03814 | best_loss=0.03813
Epoch 21/80: current_loss=0.03819 | best_loss=0.03813
Epoch 22/80: current_loss=0.03806 | best_loss=0.03806
Epoch 23/80: current_loss=0.03796 | best_loss=0.03796
Epoch 24/80: current_loss=0.03794 | best_loss=0.03794
Epoch 25/80: current_loss=0.03797 | best_loss=0.03794
Epoch 26/80: current_loss=0.03781 | best_loss=0.03781
Epoch 27/80: current_loss=0.03789 | best_loss=0.03781
Epoch 28/80: current_loss=0.03783 | best_loss=0.03781
Epoch 29/80: current_loss=0.03779 | best_loss=0.03779
Epoch 30/80: current_loss=0.03799 | best_loss=0.03779
Epoch 31/80: current_loss=0.03789 | best_loss=0.03779
Epoch 32/80: current_loss=0.03786 | best_loss=0.03779
Epoch 33/80: current_loss=0.03776 | best_loss=0.03776
Epoch 34/80: current_loss=0.03773 | best_loss=0.03773
Epoch 35/80: current_loss=0.03795 | best_loss=0.03773
Epoch 36/80: current_loss=0.03769 | best_loss=0.03769
Epoch 37/80: current_loss=0.03770 | best_loss=0.03769
Epoch 38/80: current_loss=0.03770 | best_loss=0.03769
Epoch 39/80: current_loss=0.03781 | best_loss=0.03769
Epoch 40/80: current_loss=0.03783 | best_loss=0.03769
Epoch 41/80: current_loss=0.03787 | best_loss=0.03769
Epoch 42/80: current_loss=0.03765 | best_loss=0.03765
Epoch 43/80: current_loss=0.03764 | best_loss=0.03764
Epoch 44/80: current_loss=0.03760 | best_loss=0.03760
Epoch 45/80: current_loss=0.03765 | best_loss=0.03760
Epoch 46/80: current_loss=0.03769 | best_loss=0.03760
Epoch 47/80: current_loss=0.03761 | best_loss=0.03760
Epoch 48/80: current_loss=0.03757 | best_loss=0.03757
Epoch 49/80: current_loss=0.03790 | best_loss=0.03757
Epoch 50/80: current_loss=0.03765 | best_loss=0.03757
Epoch 51/80: current_loss=0.03774 | best_loss=0.03757
Epoch 52/80: current_loss=0.03780 | best_loss=0.03757
Epoch 53/80: current_loss=0.03772 | best_loss=0.03757
Epoch 54/80: current_loss=0.03762 | best_loss=0.03757
Epoch 55/80: current_loss=0.03775 | best_loss=0.03757
Epoch 56/80: current_loss=0.03775 | best_loss=0.03757
Epoch 57/80: current_loss=0.03760 | best_loss=0.03757
Epoch 58/80: current_loss=0.03765 | best_loss=0.03757
Epoch 59/80: current_loss=0.03771 | best_loss=0.03757
Epoch 60/80: current_loss=0.03779 | best_loss=0.03757
Epoch 61/80: current_loss=0.03762 | best_loss=0.03757
Epoch 62/80: current_loss=0.03764 | best_loss=0.03757
Epoch 63/80: current_loss=0.03773 | best_loss=0.03757
Epoch 64/80: current_loss=0.03763 | best_loss=0.03757
Epoch 65/80: current_loss=0.03765 | best_loss=0.03757
Epoch 66/80: current_loss=0.03756 | best_loss=0.03756
Epoch 67/80: current_loss=0.03770 | best_loss=0.03756
Epoch 68/80: current_loss=0.03754 | best_loss=0.03754
Epoch 69/80: current_loss=0.03777 | best_loss=0.03754
Epoch 70/80: current_loss=0.03758 | best_loss=0.03754
Epoch 71/80: current_loss=0.03760 | best_loss=0.03754
Epoch 72/80: current_loss=0.03767 | best_loss=0.03754
Epoch 73/80: current_loss=0.03760 | best_loss=0.03754
Epoch 74/80: current_loss=0.03761 | best_loss=0.03754
Epoch 75/80: current_loss=0.03760 | best_loss=0.03754
Epoch 76/80: current_loss=0.03807 | best_loss=0.03754
Epoch 77/80: current_loss=0.03760 | best_loss=0.03754
Epoch 78/80: current_loss=0.03762 | best_loss=0.03754
Epoch 79/80: current_loss=0.03775 | best_loss=0.03754
      explained_var=0.01765 | mse_loss=0.03848
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04140 | best_loss=0.04140
Epoch 1/80: current_loss=0.04150 | best_loss=0.04140
Epoch 2/80: current_loss=0.04149 | best_loss=0.04140
Epoch 3/80: current_loss=0.04189 | best_loss=0.04140
Epoch 4/80: current_loss=0.04148 | best_loss=0.04140
Epoch 5/80: current_loss=0.04174 | best_loss=0.04140
Epoch 6/80: current_loss=0.04161 | best_loss=0.04140
Epoch 7/80: current_loss=0.04168 | best_loss=0.04140
Epoch 8/80: current_loss=0.04173 | best_loss=0.04140
Epoch 9/80: current_loss=0.04166 | best_loss=0.04140
Epoch 10/80: current_loss=0.04161 | best_loss=0.04140
Epoch 11/80: current_loss=0.04164 | best_loss=0.04140
Epoch 12/80: current_loss=0.04152 | best_loss=0.04140
Epoch 13/80: current_loss=0.04155 | best_loss=0.04140
Epoch 14/80: current_loss=0.04174 | best_loss=0.04140
Epoch 15/80: current_loss=0.04151 | best_loss=0.04140
Epoch 16/80: current_loss=0.04161 | best_loss=0.04140
Epoch 17/80: current_loss=0.04157 | best_loss=0.04140
Epoch 18/80: current_loss=0.04157 | best_loss=0.04140
Epoch 19/80: current_loss=0.04159 | best_loss=0.04140
Epoch 20/80: current_loss=0.04152 | best_loss=0.04140
Early Stopping at epoch 20
      explained_var=0.03287 | mse_loss=0.03989
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02850 | best_loss=0.02850
Epoch 1/80: current_loss=0.02807 | best_loss=0.02807
Epoch 2/80: current_loss=0.02836 | best_loss=0.02807
Epoch 3/80: current_loss=0.02827 | best_loss=0.02807
Epoch 4/80: current_loss=0.02849 | best_loss=0.02807
Epoch 5/80: current_loss=0.02826 | best_loss=0.02807
Epoch 6/80: current_loss=0.02836 | best_loss=0.02807
Epoch 7/80: current_loss=0.02821 | best_loss=0.02807
Epoch 8/80: current_loss=0.02835 | best_loss=0.02807
Epoch 9/80: current_loss=0.02819 | best_loss=0.02807
Epoch 10/80: current_loss=0.02820 | best_loss=0.02807
Epoch 11/80: current_loss=0.02822 | best_loss=0.02807
Epoch 12/80: current_loss=0.02853 | best_loss=0.02807
Epoch 13/80: current_loss=0.02846 | best_loss=0.02807
Epoch 14/80: current_loss=0.02820 | best_loss=0.02807
Epoch 15/80: current_loss=0.02855 | best_loss=0.02807
Epoch 16/80: current_loss=0.02847 | best_loss=0.02807
Epoch 17/80: current_loss=0.02835 | best_loss=0.02807
Epoch 18/80: current_loss=0.02826 | best_loss=0.02807
Epoch 19/80: current_loss=0.02842 | best_loss=0.02807
Epoch 20/80: current_loss=0.02845 | best_loss=0.02807
Epoch 21/80: current_loss=0.02815 | best_loss=0.02807
Early Stopping at epoch 21
      explained_var=-0.00241 | mse_loss=0.02848

----------------------------------------------
Params for Trial 24
{'learning_rate': 1e-05, 'weight_decay': 0.0009880601704155493, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.13140 | best_loss=0.13140
Epoch 1/80: current_loss=0.06063 | best_loss=0.06063
Epoch 2/80: current_loss=0.04906 | best_loss=0.04906
Epoch 3/80: current_loss=0.04821 | best_loss=0.04821
Epoch 4/80: current_loss=0.04758 | best_loss=0.04758
Epoch 5/80: current_loss=0.04699 | best_loss=0.04699
Epoch 6/80: current_loss=0.04630 | best_loss=0.04630
Epoch 7/80: current_loss=0.04551 | best_loss=0.04551
Epoch 8/80: current_loss=0.04497 | best_loss=0.04497
Epoch 9/80: current_loss=0.04436 | best_loss=0.04436
Epoch 10/80: current_loss=0.04395 | best_loss=0.04395
Epoch 11/80: current_loss=0.04363 | best_loss=0.04363
Epoch 12/80: current_loss=0.04317 | best_loss=0.04317
Epoch 13/80: current_loss=0.04279 | best_loss=0.04279
Epoch 14/80: current_loss=0.04256 | best_loss=0.04256
Epoch 15/80: current_loss=0.04226 | best_loss=0.04226
Epoch 16/80: current_loss=0.04195 | best_loss=0.04195
Epoch 17/80: current_loss=0.04182 | best_loss=0.04182
Epoch 18/80: current_loss=0.04161 | best_loss=0.04161
Epoch 19/80: current_loss=0.04130 | best_loss=0.04130
Epoch 20/80: current_loss=0.04118 | best_loss=0.04118
Epoch 21/80: current_loss=0.04109 | best_loss=0.04109
Epoch 22/80: current_loss=0.04079 | best_loss=0.04079
Epoch 23/80: current_loss=0.04067 | best_loss=0.04067
Epoch 24/80: current_loss=0.04058 | best_loss=0.04058
Epoch 25/80: current_loss=0.04035 | best_loss=0.04035
Epoch 26/80: current_loss=0.04019 | best_loss=0.04019
Epoch 27/80: current_loss=0.04015 | best_loss=0.04015
Epoch 28/80: current_loss=0.03997 | best_loss=0.03997
Epoch 29/80: current_loss=0.03983 | best_loss=0.03983
Epoch 30/80: current_loss=0.03989 | best_loss=0.03983
Epoch 31/80: current_loss=0.03969 | best_loss=0.03969
Epoch 32/80: current_loss=0.03975 | best_loss=0.03969
Epoch 33/80: current_loss=0.03960 | best_loss=0.03960
Epoch 34/80: current_loss=0.03958 | best_loss=0.03958
Epoch 35/80: current_loss=0.03952 | best_loss=0.03952
Epoch 36/80: current_loss=0.03943 | best_loss=0.03943
Epoch 37/80: current_loss=0.03926 | best_loss=0.03926
Epoch 38/80: current_loss=0.03935 | best_loss=0.03926
Epoch 39/80: current_loss=0.03921 | best_loss=0.03921
Epoch 40/80: current_loss=0.03911 | best_loss=0.03911
Epoch 41/80: current_loss=0.03907 | best_loss=0.03907
Epoch 42/80: current_loss=0.03896 | best_loss=0.03896
Epoch 43/80: current_loss=0.03892 | best_loss=0.03892
Epoch 44/80: current_loss=0.03906 | best_loss=0.03892
Epoch 45/80: current_loss=0.03889 | best_loss=0.03889
Epoch 46/80: current_loss=0.03892 | best_loss=0.03889
Epoch 47/80: current_loss=0.03886 | best_loss=0.03886
Epoch 48/80: current_loss=0.03881 | best_loss=0.03881
Epoch 49/80: current_loss=0.03883 | best_loss=0.03881
Epoch 50/80: current_loss=0.03880 | best_loss=0.03880
Epoch 51/80: current_loss=0.03868 | best_loss=0.03868
Epoch 52/80: current_loss=0.03856 | best_loss=0.03856
Epoch 53/80: current_loss=0.03854 | best_loss=0.03854
Epoch 54/80: current_loss=0.03857 | best_loss=0.03854
Epoch 55/80: current_loss=0.03856 | best_loss=0.03854
Epoch 56/80: current_loss=0.03848 | best_loss=0.03848
Epoch 57/80: current_loss=0.03856 | best_loss=0.03848
Epoch 58/80: current_loss=0.03843 | best_loss=0.03843
Epoch 59/80: current_loss=0.03839 | best_loss=0.03839
Epoch 60/80: current_loss=0.03837 | best_loss=0.03837
Epoch 61/80: current_loss=0.03843 | best_loss=0.03837
Epoch 62/80: current_loss=0.03830 | best_loss=0.03830
Epoch 63/80: current_loss=0.03828 | best_loss=0.03828
Epoch 64/80: current_loss=0.03827 | best_loss=0.03827
Epoch 65/80: current_loss=0.03821 | best_loss=0.03821
Epoch 66/80: current_loss=0.03827 | best_loss=0.03821
Epoch 67/80: current_loss=0.03818 | best_loss=0.03818
Epoch 68/80: current_loss=0.03814 | best_loss=0.03814
Epoch 69/80: current_loss=0.03815 | best_loss=0.03814
Epoch 70/80: current_loss=0.03814 | best_loss=0.03814
Epoch 71/80: current_loss=0.03806 | best_loss=0.03806
Epoch 72/80: current_loss=0.03809 | best_loss=0.03806
Epoch 73/80: current_loss=0.03804 | best_loss=0.03804
Epoch 74/80: current_loss=0.03809 | best_loss=0.03804
Epoch 75/80: current_loss=0.03805 | best_loss=0.03804
Epoch 76/80: current_loss=0.03808 | best_loss=0.03804
Epoch 77/80: current_loss=0.03801 | best_loss=0.03801
Epoch 78/80: current_loss=0.03796 | best_loss=0.03796
Epoch 79/80: current_loss=0.03793 | best_loss=0.03793
      explained_var=0.00754 | mse_loss=0.03897

----------------------------------------------
Params for Trial 25
{'learning_rate': 0.0001, 'weight_decay': 9.130575980119252e-05, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.12609 | best_loss=0.12609
Epoch 1/80: current_loss=0.05813 | best_loss=0.05813
Epoch 2/80: current_loss=0.05460 | best_loss=0.05460
Epoch 3/80: current_loss=0.05251 | best_loss=0.05251
Epoch 4/80: current_loss=0.05048 | best_loss=0.05048
Epoch 5/80: current_loss=0.04882 | best_loss=0.04882
Epoch 6/80: current_loss=0.04752 | best_loss=0.04752
Epoch 7/80: current_loss=0.04658 | best_loss=0.04658
Epoch 8/80: current_loss=0.04588 | best_loss=0.04588
Epoch 9/80: current_loss=0.04513 | best_loss=0.04513
Epoch 10/80: current_loss=0.04440 | best_loss=0.04440
Epoch 11/80: current_loss=0.04386 | best_loss=0.04386
Epoch 12/80: current_loss=0.04353 | best_loss=0.04353
Epoch 13/80: current_loss=0.04347 | best_loss=0.04347
Epoch 14/80: current_loss=0.04255 | best_loss=0.04255
Epoch 15/80: current_loss=0.04240 | best_loss=0.04240
Epoch 16/80: current_loss=0.04226 | best_loss=0.04226
Epoch 17/80: current_loss=0.04175 | best_loss=0.04175
Epoch 18/80: current_loss=0.04147 | best_loss=0.04147
Epoch 19/80: current_loss=0.04120 | best_loss=0.04120
Epoch 20/80: current_loss=0.04087 | best_loss=0.04087
Epoch 21/80: current_loss=0.04108 | best_loss=0.04087
Epoch 22/80: current_loss=0.04045 | best_loss=0.04045
Epoch 23/80: current_loss=0.04033 | best_loss=0.04033
Epoch 24/80: current_loss=0.04013 | best_loss=0.04013
Epoch 25/80: current_loss=0.04030 | best_loss=0.04013
Epoch 26/80: current_loss=0.03986 | best_loss=0.03986
Epoch 27/80: current_loss=0.03980 | best_loss=0.03980
Epoch 28/80: current_loss=0.03990 | best_loss=0.03980
Epoch 29/80: current_loss=0.03956 | best_loss=0.03956
Epoch 30/80: current_loss=0.03957 | best_loss=0.03956
Epoch 31/80: current_loss=0.03948 | best_loss=0.03948
Epoch 32/80: current_loss=0.03942 | best_loss=0.03942
Epoch 33/80: current_loss=0.03952 | best_loss=0.03942
Epoch 34/80: current_loss=0.03955 | best_loss=0.03942
Epoch 35/80: current_loss=0.03927 | best_loss=0.03927
Epoch 36/80: current_loss=0.03928 | best_loss=0.03927
Epoch 37/80: current_loss=0.03917 | best_loss=0.03917
Epoch 38/80: current_loss=0.03928 | best_loss=0.03917
Epoch 39/80: current_loss=0.03903 | best_loss=0.03903
Epoch 40/80: current_loss=0.03910 | best_loss=0.03903
Epoch 41/80: current_loss=0.03898 | best_loss=0.03898
Epoch 42/80: current_loss=0.03876 | best_loss=0.03876
Epoch 43/80: current_loss=0.03884 | best_loss=0.03876
Epoch 44/80: current_loss=0.03880 | best_loss=0.03876
Epoch 45/80: current_loss=0.03883 | best_loss=0.03876
Epoch 46/80: current_loss=0.03861 | best_loss=0.03861
Epoch 47/80: current_loss=0.03864 | best_loss=0.03861
Epoch 48/80: current_loss=0.03869 | best_loss=0.03861
Epoch 49/80: current_loss=0.03850 | best_loss=0.03850
Epoch 50/80: current_loss=0.03850 | best_loss=0.03850
Epoch 51/80: current_loss=0.03869 | best_loss=0.03850
Epoch 52/80: current_loss=0.03850 | best_loss=0.03850
Epoch 53/80: current_loss=0.03854 | best_loss=0.03850
Epoch 54/80: current_loss=0.03835 | best_loss=0.03835
Epoch 55/80: current_loss=0.03867 | best_loss=0.03835
Epoch 56/80: current_loss=0.03864 | best_loss=0.03835
Epoch 57/80: current_loss=0.03822 | best_loss=0.03822
Epoch 58/80: current_loss=0.03836 | best_loss=0.03822
Epoch 59/80: current_loss=0.03811 | best_loss=0.03811
Epoch 60/80: current_loss=0.03808 | best_loss=0.03808
Epoch 61/80: current_loss=0.03802 | best_loss=0.03802
Epoch 62/80: current_loss=0.03832 | best_loss=0.03802
Epoch 63/80: current_loss=0.03804 | best_loss=0.03802
Epoch 64/80: current_loss=0.03802 | best_loss=0.03802
Epoch 65/80: current_loss=0.03796 | best_loss=0.03796
Epoch 66/80: current_loss=0.03791 | best_loss=0.03791
Epoch 67/80: current_loss=0.03802 | best_loss=0.03791
Epoch 68/80: current_loss=0.03792 | best_loss=0.03791
Epoch 69/80: current_loss=0.03819 | best_loss=0.03791
Epoch 70/80: current_loss=0.03791 | best_loss=0.03791
Epoch 71/80: current_loss=0.03817 | best_loss=0.03791
Epoch 72/80: current_loss=0.03809 | best_loss=0.03791
Epoch 73/80: current_loss=0.03801 | best_loss=0.03791
Epoch 74/80: current_loss=0.03796 | best_loss=0.03791
Epoch 75/80: current_loss=0.03784 | best_loss=0.03784
Epoch 76/80: current_loss=0.03814 | best_loss=0.03784
Epoch 77/80: current_loss=0.03786 | best_loss=0.03784
Epoch 78/80: current_loss=0.03781 | best_loss=0.03781
Epoch 79/80: current_loss=0.03799 | best_loss=0.03781
      explained_var=0.01289 | mse_loss=0.03881

----------------------------------------------
Params for Trial 26
{'learning_rate': 0.0001, 'weight_decay': 0.002499292960264116, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05663 | best_loss=0.05663
Epoch 1/80: current_loss=0.04428 | best_loss=0.04428
Epoch 2/80: current_loss=0.04415 | best_loss=0.04415
Epoch 3/80: current_loss=0.04251 | best_loss=0.04251
Epoch 4/80: current_loss=0.04214 | best_loss=0.04214
Epoch 5/80: current_loss=0.04191 | best_loss=0.04191
Epoch 6/80: current_loss=0.04153 | best_loss=0.04153
Epoch 7/80: current_loss=0.04101 | best_loss=0.04101
Epoch 8/80: current_loss=0.04109 | best_loss=0.04101
Epoch 9/80: current_loss=0.04049 | best_loss=0.04049
Epoch 10/80: current_loss=0.04044 | best_loss=0.04044
Epoch 11/80: current_loss=0.04040 | best_loss=0.04040
Epoch 12/80: current_loss=0.03999 | best_loss=0.03999
Epoch 13/80: current_loss=0.03975 | best_loss=0.03975
Epoch 14/80: current_loss=0.03961 | best_loss=0.03961
Epoch 15/80: current_loss=0.03974 | best_loss=0.03961
Epoch 16/80: current_loss=0.03951 | best_loss=0.03951
Epoch 17/80: current_loss=0.03957 | best_loss=0.03951
Epoch 18/80: current_loss=0.03929 | best_loss=0.03929
Epoch 19/80: current_loss=0.03924 | best_loss=0.03924
Epoch 20/80: current_loss=0.03913 | best_loss=0.03913
Epoch 21/80: current_loss=0.03895 | best_loss=0.03895
Epoch 22/80: current_loss=0.03884 | best_loss=0.03884
Epoch 23/80: current_loss=0.03876 | best_loss=0.03876
Epoch 24/80: current_loss=0.03877 | best_loss=0.03876
Epoch 25/80: current_loss=0.03892 | best_loss=0.03876
Epoch 26/80: current_loss=0.03860 | best_loss=0.03860
Epoch 27/80: current_loss=0.03868 | best_loss=0.03860
Epoch 28/80: current_loss=0.03870 | best_loss=0.03860
Epoch 29/80: current_loss=0.03847 | best_loss=0.03847
Epoch 30/80: current_loss=0.03845 | best_loss=0.03845
Epoch 31/80: current_loss=0.03852 | best_loss=0.03845
Epoch 32/80: current_loss=0.03870 | best_loss=0.03845
Epoch 33/80: current_loss=0.03838 | best_loss=0.03838
Epoch 34/80: current_loss=0.03838 | best_loss=0.03838
Epoch 35/80: current_loss=0.03828 | best_loss=0.03828
Epoch 36/80: current_loss=0.03826 | best_loss=0.03826
Epoch 37/80: current_loss=0.03822 | best_loss=0.03822
Epoch 38/80: current_loss=0.03815 | best_loss=0.03815
Epoch 39/80: current_loss=0.03796 | best_loss=0.03796
Epoch 40/80: current_loss=0.03796 | best_loss=0.03796
Epoch 41/80: current_loss=0.03801 | best_loss=0.03796
Epoch 42/80: current_loss=0.03786 | best_loss=0.03786
Epoch 43/80: current_loss=0.03800 | best_loss=0.03786
Epoch 44/80: current_loss=0.03797 | best_loss=0.03786
Epoch 45/80: current_loss=0.03822 | best_loss=0.03786
Epoch 46/80: current_loss=0.03782 | best_loss=0.03782
Epoch 47/80: current_loss=0.03789 | best_loss=0.03782
Epoch 48/80: current_loss=0.03790 | best_loss=0.03782
Epoch 49/80: current_loss=0.03792 | best_loss=0.03782
Epoch 50/80: current_loss=0.03781 | best_loss=0.03781
Epoch 51/80: current_loss=0.03788 | best_loss=0.03781
Epoch 52/80: current_loss=0.03774 | best_loss=0.03774
Epoch 53/80: current_loss=0.03778 | best_loss=0.03774
Epoch 54/80: current_loss=0.03775 | best_loss=0.03774
Epoch 55/80: current_loss=0.03793 | best_loss=0.03774
Epoch 56/80: current_loss=0.03781 | best_loss=0.03774
Epoch 57/80: current_loss=0.03779 | best_loss=0.03774
Epoch 58/80: current_loss=0.03798 | best_loss=0.03774
Epoch 59/80: current_loss=0.03788 | best_loss=0.03774
Epoch 60/80: current_loss=0.03774 | best_loss=0.03774
Epoch 61/80: current_loss=0.03770 | best_loss=0.03770
Epoch 62/80: current_loss=0.03762 | best_loss=0.03762
Epoch 63/80: current_loss=0.03772 | best_loss=0.03762
Epoch 64/80: current_loss=0.03769 | best_loss=0.03762
Epoch 65/80: current_loss=0.03764 | best_loss=0.03762
Epoch 66/80: current_loss=0.03762 | best_loss=0.03762
Epoch 67/80: current_loss=0.03772 | best_loss=0.03762
Epoch 68/80: current_loss=0.03780 | best_loss=0.03762
Epoch 69/80: current_loss=0.03781 | best_loss=0.03762
Epoch 70/80: current_loss=0.03777 | best_loss=0.03762
Epoch 71/80: current_loss=0.03758 | best_loss=0.03758
Epoch 72/80: current_loss=0.03767 | best_loss=0.03758
Epoch 73/80: current_loss=0.03761 | best_loss=0.03758
Epoch 74/80: current_loss=0.03757 | best_loss=0.03757
Epoch 75/80: current_loss=0.03759 | best_loss=0.03757
Epoch 76/80: current_loss=0.03749 | best_loss=0.03749
Epoch 77/80: current_loss=0.03757 | best_loss=0.03749
Epoch 78/80: current_loss=0.03756 | best_loss=0.03749
Epoch 79/80: current_loss=0.03758 | best_loss=0.03749
      explained_var=0.01963 | mse_loss=0.03845
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04135 | best_loss=0.04135
Epoch 1/80: current_loss=0.04131 | best_loss=0.04131
Epoch 2/80: current_loss=0.04119 | best_loss=0.04119
Epoch 3/80: current_loss=0.04146 | best_loss=0.04119
Epoch 4/80: current_loss=0.04134 | best_loss=0.04119
Epoch 5/80: current_loss=0.04164 | best_loss=0.04119
Epoch 6/80: current_loss=0.04169 | best_loss=0.04119
Epoch 7/80: current_loss=0.04160 | best_loss=0.04119
Epoch 8/80: current_loss=0.04186 | best_loss=0.04119
Epoch 9/80: current_loss=0.04170 | best_loss=0.04119
Epoch 10/80: current_loss=0.04149 | best_loss=0.04119
Epoch 11/80: current_loss=0.04172 | best_loss=0.04119
Epoch 12/80: current_loss=0.04162 | best_loss=0.04119
Epoch 13/80: current_loss=0.04150 | best_loss=0.04119
Epoch 14/80: current_loss=0.04154 | best_loss=0.04119
Epoch 15/80: current_loss=0.04148 | best_loss=0.04119
Epoch 16/80: current_loss=0.04149 | best_loss=0.04119
Epoch 17/80: current_loss=0.04143 | best_loss=0.04119
Epoch 18/80: current_loss=0.04167 | best_loss=0.04119
Epoch 19/80: current_loss=0.04154 | best_loss=0.04119
Epoch 20/80: current_loss=0.04130 | best_loss=0.04119
Epoch 21/80: current_loss=0.04156 | best_loss=0.04119
Epoch 22/80: current_loss=0.04147 | best_loss=0.04119
Early Stopping at epoch 22
      explained_var=0.03751 | mse_loss=0.03969
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02833 | best_loss=0.02833
Epoch 1/80: current_loss=0.02827 | best_loss=0.02827
Epoch 2/80: current_loss=0.02811 | best_loss=0.02811
Epoch 3/80: current_loss=0.02876 | best_loss=0.02811
Epoch 4/80: current_loss=0.02829 | best_loss=0.02811
Epoch 5/80: current_loss=0.02829 | best_loss=0.02811
Epoch 6/80: current_loss=0.02825 | best_loss=0.02811
Epoch 7/80: current_loss=0.02833 | best_loss=0.02811
Epoch 8/80: current_loss=0.02833 | best_loss=0.02811
Epoch 9/80: current_loss=0.02831 | best_loss=0.02811
Epoch 10/80: current_loss=0.02838 | best_loss=0.02811
Epoch 11/80: current_loss=0.02857 | best_loss=0.02811
Epoch 12/80: current_loss=0.02826 | best_loss=0.02811
Epoch 13/80: current_loss=0.02861 | best_loss=0.02811
Epoch 14/80: current_loss=0.02829 | best_loss=0.02811
Epoch 15/80: current_loss=0.02842 | best_loss=0.02811
Epoch 16/80: current_loss=0.02866 | best_loss=0.02811
Epoch 17/80: current_loss=0.02813 | best_loss=0.02811
Epoch 18/80: current_loss=0.02834 | best_loss=0.02811
Epoch 19/80: current_loss=0.02822 | best_loss=0.02811
Epoch 20/80: current_loss=0.02833 | best_loss=0.02811
Epoch 21/80: current_loss=0.02838 | best_loss=0.02811
Epoch 22/80: current_loss=0.02825 | best_loss=0.02811
Early Stopping at epoch 22
      explained_var=-0.00519 | mse_loss=0.02854

----------------------------------------------
Params for Trial 27
{'learning_rate': 0.0001, 'weight_decay': 0.0010076824476769823, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.24116 | best_loss=0.24116
Epoch 1/80: current_loss=0.14929 | best_loss=0.14929
Epoch 2/80: current_loss=0.08505 | best_loss=0.08505
Epoch 3/80: current_loss=0.05344 | best_loss=0.05344
Epoch 4/80: current_loss=0.04465 | best_loss=0.04465
Epoch 5/80: current_loss=0.04366 | best_loss=0.04366
Epoch 6/80: current_loss=0.04396 | best_loss=0.04366
Epoch 7/80: current_loss=0.04344 | best_loss=0.04344
Epoch 8/80: current_loss=0.04280 | best_loss=0.04280
Epoch 9/80: current_loss=0.04253 | best_loss=0.04253
Epoch 10/80: current_loss=0.04222 | best_loss=0.04222
Epoch 11/80: current_loss=0.04208 | best_loss=0.04208
Epoch 12/80: current_loss=0.04178 | best_loss=0.04178
Epoch 13/80: current_loss=0.04171 | best_loss=0.04171
Epoch 14/80: current_loss=0.04155 | best_loss=0.04155
Epoch 15/80: current_loss=0.04146 | best_loss=0.04146
Epoch 16/80: current_loss=0.04110 | best_loss=0.04110
Epoch 17/80: current_loss=0.04093 | best_loss=0.04093
Epoch 18/80: current_loss=0.04114 | best_loss=0.04093
Epoch 19/80: current_loss=0.04067 | best_loss=0.04067
Epoch 20/80: current_loss=0.04082 | best_loss=0.04067
Epoch 21/80: current_loss=0.04094 | best_loss=0.04067
Epoch 22/80: current_loss=0.04032 | best_loss=0.04032
Epoch 23/80: current_loss=0.04011 | best_loss=0.04011
Epoch 24/80: current_loss=0.04037 | best_loss=0.04011
Epoch 25/80: current_loss=0.04004 | best_loss=0.04004
Epoch 26/80: current_loss=0.04006 | best_loss=0.04004
Epoch 27/80: current_loss=0.03980 | best_loss=0.03980
Epoch 28/80: current_loss=0.03983 | best_loss=0.03980
Epoch 29/80: current_loss=0.04004 | best_loss=0.03980
Epoch 30/80: current_loss=0.03957 | best_loss=0.03957
Epoch 31/80: current_loss=0.03939 | best_loss=0.03939
Epoch 32/80: current_loss=0.03954 | best_loss=0.03939
Epoch 33/80: current_loss=0.03945 | best_loss=0.03939
Epoch 34/80: current_loss=0.03923 | best_loss=0.03923
Epoch 35/80: current_loss=0.03936 | best_loss=0.03923
Epoch 36/80: current_loss=0.03913 | best_loss=0.03913
Epoch 37/80: current_loss=0.03894 | best_loss=0.03894
Epoch 38/80: current_loss=0.03933 | best_loss=0.03894
Epoch 39/80: current_loss=0.03886 | best_loss=0.03886
Epoch 40/80: current_loss=0.03908 | best_loss=0.03886
Epoch 41/80: current_loss=0.03914 | best_loss=0.03886
Epoch 42/80: current_loss=0.03871 | best_loss=0.03871
Epoch 43/80: current_loss=0.03902 | best_loss=0.03871
Epoch 44/80: current_loss=0.03909 | best_loss=0.03871
Epoch 45/80: current_loss=0.03874 | best_loss=0.03871
Epoch 46/80: current_loss=0.03875 | best_loss=0.03871
Epoch 47/80: current_loss=0.03855 | best_loss=0.03855
Epoch 48/80: current_loss=0.03864 | best_loss=0.03855
Epoch 49/80: current_loss=0.03848 | best_loss=0.03848
Epoch 50/80: current_loss=0.03845 | best_loss=0.03845
Epoch 51/80: current_loss=0.03880 | best_loss=0.03845
Epoch 52/80: current_loss=0.03848 | best_loss=0.03845
Epoch 53/80: current_loss=0.03863 | best_loss=0.03845
Epoch 54/80: current_loss=0.03842 | best_loss=0.03842
Epoch 55/80: current_loss=0.03848 | best_loss=0.03842
Epoch 56/80: current_loss=0.03885 | best_loss=0.03842
Epoch 57/80: current_loss=0.03844 | best_loss=0.03842
Epoch 58/80: current_loss=0.03840 | best_loss=0.03840
Epoch 59/80: current_loss=0.03850 | best_loss=0.03840
Epoch 60/80: current_loss=0.03837 | best_loss=0.03837
Epoch 61/80: current_loss=0.03818 | best_loss=0.03818
Epoch 62/80: current_loss=0.03850 | best_loss=0.03818
Epoch 63/80: current_loss=0.03821 | best_loss=0.03818
Epoch 64/80: current_loss=0.03843 | best_loss=0.03818
Epoch 65/80: current_loss=0.03832 | best_loss=0.03818
Epoch 66/80: current_loss=0.03820 | best_loss=0.03818
Epoch 67/80: current_loss=0.03818 | best_loss=0.03818
Epoch 68/80: current_loss=0.03836 | best_loss=0.03818
Epoch 69/80: current_loss=0.03810 | best_loss=0.03810
Epoch 70/80: current_loss=0.03808 | best_loss=0.03808
Epoch 71/80: current_loss=0.03813 | best_loss=0.03808
Epoch 72/80: current_loss=0.03809 | best_loss=0.03808
Epoch 73/80: current_loss=0.03816 | best_loss=0.03808
Epoch 74/80: current_loss=0.03813 | best_loss=0.03808
Epoch 75/80: current_loss=0.03809 | best_loss=0.03808
Epoch 76/80: current_loss=0.03834 | best_loss=0.03808
Epoch 77/80: current_loss=0.03807 | best_loss=0.03807
Epoch 78/80: current_loss=0.03819 | best_loss=0.03807
Epoch 79/80: current_loss=0.03827 | best_loss=0.03807
      explained_var=0.00698 | mse_loss=0.03904

----------------------------------------------
Params for Trial 28
{'learning_rate': 0.0001, 'weight_decay': 0.004169508644681581, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04331 | best_loss=0.04331
Epoch 1/80: current_loss=0.04068 | best_loss=0.04068
Epoch 2/80: current_loss=0.03986 | best_loss=0.03986
Epoch 3/80: current_loss=0.03985 | best_loss=0.03985
Epoch 4/80: current_loss=0.03911 | best_loss=0.03911
Epoch 5/80: current_loss=0.03913 | best_loss=0.03911
Epoch 6/80: current_loss=0.03887 | best_loss=0.03887
Epoch 7/80: current_loss=0.03872 | best_loss=0.03872
Epoch 8/80: current_loss=0.03843 | best_loss=0.03843
Epoch 9/80: current_loss=0.03887 | best_loss=0.03843
Epoch 10/80: current_loss=0.03831 | best_loss=0.03831
Epoch 11/80: current_loss=0.03838 | best_loss=0.03831
Epoch 12/80: current_loss=0.03826 | best_loss=0.03826
Epoch 13/80: current_loss=0.03800 | best_loss=0.03800
Epoch 14/80: current_loss=0.03839 | best_loss=0.03800
Epoch 15/80: current_loss=0.03798 | best_loss=0.03798
Epoch 16/80: current_loss=0.03822 | best_loss=0.03798
Epoch 17/80: current_loss=0.03802 | best_loss=0.03798
Epoch 18/80: current_loss=0.03803 | best_loss=0.03798
Epoch 19/80: current_loss=0.03776 | best_loss=0.03776
Epoch 20/80: current_loss=0.03796 | best_loss=0.03776
Epoch 21/80: current_loss=0.03778 | best_loss=0.03776
Epoch 22/80: current_loss=0.03787 | best_loss=0.03776
Epoch 23/80: current_loss=0.03790 | best_loss=0.03776
Epoch 24/80: current_loss=0.03793 | best_loss=0.03776
Epoch 25/80: current_loss=0.03763 | best_loss=0.03763
Epoch 26/80: current_loss=0.03782 | best_loss=0.03763
Epoch 27/80: current_loss=0.03757 | best_loss=0.03757
Epoch 28/80: current_loss=0.03757 | best_loss=0.03757
Epoch 29/80: current_loss=0.03764 | best_loss=0.03757
Epoch 30/80: current_loss=0.03772 | best_loss=0.03757
Epoch 31/80: current_loss=0.03758 | best_loss=0.03757
Epoch 32/80: current_loss=0.03774 | best_loss=0.03757
Epoch 33/80: current_loss=0.03787 | best_loss=0.03757
Epoch 34/80: current_loss=0.03768 | best_loss=0.03757
Epoch 35/80: current_loss=0.03766 | best_loss=0.03757
Epoch 36/80: current_loss=0.03790 | best_loss=0.03757
Epoch 37/80: current_loss=0.03768 | best_loss=0.03757
Epoch 38/80: current_loss=0.03778 | best_loss=0.03757
Epoch 39/80: current_loss=0.03767 | best_loss=0.03757
Epoch 40/80: current_loss=0.03761 | best_loss=0.03757
Epoch 41/80: current_loss=0.03774 | best_loss=0.03757
Epoch 42/80: current_loss=0.03780 | best_loss=0.03757
Epoch 43/80: current_loss=0.03753 | best_loss=0.03753
Epoch 44/80: current_loss=0.03765 | best_loss=0.03753
Epoch 45/80: current_loss=0.03766 | best_loss=0.03753
Epoch 46/80: current_loss=0.03758 | best_loss=0.03753
Epoch 47/80: current_loss=0.03767 | best_loss=0.03753
Epoch 48/80: current_loss=0.03778 | best_loss=0.03753
Epoch 49/80: current_loss=0.03760 | best_loss=0.03753
Epoch 50/80: current_loss=0.03786 | best_loss=0.03753
Epoch 51/80: current_loss=0.03753 | best_loss=0.03753
Epoch 52/80: current_loss=0.03763 | best_loss=0.03753
Epoch 53/80: current_loss=0.03782 | best_loss=0.03753
Epoch 54/80: current_loss=0.03761 | best_loss=0.03753
Epoch 55/80: current_loss=0.03755 | best_loss=0.03753
Epoch 56/80: current_loss=0.03769 | best_loss=0.03753
Epoch 57/80: current_loss=0.03782 | best_loss=0.03753
Epoch 58/80: current_loss=0.03758 | best_loss=0.03753
Epoch 59/80: current_loss=0.03765 | best_loss=0.03753
Epoch 60/80: current_loss=0.03765 | best_loss=0.03753
Epoch 61/80: current_loss=0.03759 | best_loss=0.03753
Epoch 62/80: current_loss=0.03752 | best_loss=0.03752
Epoch 63/80: current_loss=0.03793 | best_loss=0.03752
Epoch 64/80: current_loss=0.03752 | best_loss=0.03752
Epoch 65/80: current_loss=0.03756 | best_loss=0.03752
Epoch 66/80: current_loss=0.03795 | best_loss=0.03752
Epoch 67/80: current_loss=0.03780 | best_loss=0.03752
Epoch 68/80: current_loss=0.03758 | best_loss=0.03752
Epoch 69/80: current_loss=0.03770 | best_loss=0.03752
Epoch 70/80: current_loss=0.03768 | best_loss=0.03752
Epoch 71/80: current_loss=0.03778 | best_loss=0.03752
Epoch 72/80: current_loss=0.03766 | best_loss=0.03752
Epoch 73/80: current_loss=0.03776 | best_loss=0.03752
Epoch 74/80: current_loss=0.03781 | best_loss=0.03752
Epoch 75/80: current_loss=0.03762 | best_loss=0.03752
Epoch 76/80: current_loss=0.03768 | best_loss=0.03752
Epoch 77/80: current_loss=0.03795 | best_loss=0.03752
Epoch 78/80: current_loss=0.03770 | best_loss=0.03752
Epoch 79/80: current_loss=0.03761 | best_loss=0.03752
      explained_var=0.01816 | mse_loss=0.03842
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04138 | best_loss=0.04138
Epoch 1/80: current_loss=0.04188 | best_loss=0.04138
Epoch 2/80: current_loss=0.04145 | best_loss=0.04138
Epoch 3/80: current_loss=0.04164 | best_loss=0.04138
Epoch 4/80: current_loss=0.04161 | best_loss=0.04138
Epoch 5/80: current_loss=0.04145 | best_loss=0.04138
Epoch 6/80: current_loss=0.04235 | best_loss=0.04138
Epoch 7/80: current_loss=0.04149 | best_loss=0.04138
Epoch 8/80: current_loss=0.04149 | best_loss=0.04138
Epoch 9/80: current_loss=0.04157 | best_loss=0.04138
Epoch 10/80: current_loss=0.04160 | best_loss=0.04138
Epoch 11/80: current_loss=0.04153 | best_loss=0.04138
Epoch 12/80: current_loss=0.04161 | best_loss=0.04138
Epoch 13/80: current_loss=0.04197 | best_loss=0.04138
Epoch 14/80: current_loss=0.04169 | best_loss=0.04138
Epoch 15/80: current_loss=0.04158 | best_loss=0.04138
Epoch 16/80: current_loss=0.04163 | best_loss=0.04138
Epoch 17/80: current_loss=0.04149 | best_loss=0.04138
Epoch 18/80: current_loss=0.04193 | best_loss=0.04138
Epoch 19/80: current_loss=0.04160 | best_loss=0.04138
Epoch 20/80: current_loss=0.04161 | best_loss=0.04138
Early Stopping at epoch 20
      explained_var=0.03336 | mse_loss=0.03989
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02790 | best_loss=0.02790
Epoch 1/80: current_loss=0.02846 | best_loss=0.02790
Epoch 2/80: current_loss=0.02834 | best_loss=0.02790
Epoch 3/80: current_loss=0.02800 | best_loss=0.02790
Epoch 4/80: current_loss=0.02804 | best_loss=0.02790
Epoch 5/80: current_loss=0.02825 | best_loss=0.02790
Epoch 6/80: current_loss=0.02807 | best_loss=0.02790
Epoch 7/80: current_loss=0.02836 | best_loss=0.02790
Epoch 8/80: current_loss=0.02816 | best_loss=0.02790
Epoch 9/80: current_loss=0.02795 | best_loss=0.02790
Epoch 10/80: current_loss=0.02845 | best_loss=0.02790
Epoch 11/80: current_loss=0.02837 | best_loss=0.02790
Epoch 12/80: current_loss=0.02801 | best_loss=0.02790
Epoch 13/80: current_loss=0.02812 | best_loss=0.02790
Epoch 14/80: current_loss=0.02809 | best_loss=0.02790
Epoch 15/80: current_loss=0.02831 | best_loss=0.02790
Epoch 16/80: current_loss=0.02805 | best_loss=0.02790
Epoch 17/80: current_loss=0.02814 | best_loss=0.02790
Epoch 18/80: current_loss=0.02822 | best_loss=0.02790
Epoch 19/80: current_loss=0.02818 | best_loss=0.02790
Epoch 20/80: current_loss=0.02810 | best_loss=0.02790
Early Stopping at epoch 20
      explained_var=0.00171 | mse_loss=0.02831
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03300 | best_loss=0.03300
Epoch 1/80: current_loss=0.03296 | best_loss=0.03296
Epoch 2/80: current_loss=0.03316 | best_loss=0.03296
Epoch 3/80: current_loss=0.03298 | best_loss=0.03296
Epoch 4/80: current_loss=0.03294 | best_loss=0.03294
Epoch 5/80: current_loss=0.03298 | best_loss=0.03294
Epoch 6/80: current_loss=0.03317 | best_loss=0.03294
Epoch 7/80: current_loss=0.03300 | best_loss=0.03294
Epoch 8/80: current_loss=0.03328 | best_loss=0.03294
Epoch 9/80: current_loss=0.03297 | best_loss=0.03294
Epoch 10/80: current_loss=0.03294 | best_loss=0.03294
Epoch 11/80: current_loss=0.03300 | best_loss=0.03294
Epoch 12/80: current_loss=0.03316 | best_loss=0.03294
Epoch 13/80: current_loss=0.03301 | best_loss=0.03294
Epoch 14/80: current_loss=0.03298 | best_loss=0.03294
Epoch 15/80: current_loss=0.03313 | best_loss=0.03294
Epoch 16/80: current_loss=0.03297 | best_loss=0.03294
Epoch 17/80: current_loss=0.03297 | best_loss=0.03294
Epoch 18/80: current_loss=0.03301 | best_loss=0.03294
Epoch 19/80: current_loss=0.03299 | best_loss=0.03294
Epoch 20/80: current_loss=0.03303 | best_loss=0.03294
Epoch 21/80: current_loss=0.03323 | best_loss=0.03294
Epoch 22/80: current_loss=0.03297 | best_loss=0.03294
Epoch 23/80: current_loss=0.03298 | best_loss=0.03294
Epoch 24/80: current_loss=0.03315 | best_loss=0.03294
Early Stopping at epoch 24
      explained_var=-0.00637 | mse_loss=0.03281
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03545 | best_loss=0.03545
Epoch 1/80: current_loss=0.03543 | best_loss=0.03543
Epoch 2/80: current_loss=0.03553 | best_loss=0.03543
Epoch 3/80: current_loss=0.03542 | best_loss=0.03542
Epoch 4/80: current_loss=0.03548 | best_loss=0.03542
Epoch 5/80: current_loss=0.03542 | best_loss=0.03542
Epoch 6/80: current_loss=0.03545 | best_loss=0.03542
Epoch 7/80: current_loss=0.03545 | best_loss=0.03542
Epoch 8/80: current_loss=0.03548 | best_loss=0.03542
Epoch 9/80: current_loss=0.03549 | best_loss=0.03542
Epoch 10/80: current_loss=0.03552 | best_loss=0.03542
Epoch 11/80: current_loss=0.03549 | best_loss=0.03542
Epoch 12/80: current_loss=0.03548 | best_loss=0.03542
Epoch 13/80: current_loss=0.03552 | best_loss=0.03542
Epoch 14/80: current_loss=0.03552 | best_loss=0.03542
Epoch 15/80: current_loss=0.03555 | best_loss=0.03542
Epoch 16/80: current_loss=0.03553 | best_loss=0.03542
Epoch 17/80: current_loss=0.03563 | best_loss=0.03542
Epoch 18/80: current_loss=0.03552 | best_loss=0.03542
Epoch 19/80: current_loss=0.03551 | best_loss=0.03542
Epoch 20/80: current_loss=0.03554 | best_loss=0.03542
Epoch 21/80: current_loss=0.03553 | best_loss=0.03542
Epoch 22/80: current_loss=0.03551 | best_loss=0.03542
Epoch 23/80: current_loss=0.03552 | best_loss=0.03542
Early Stopping at epoch 23
      explained_var=0.01882 | mse_loss=0.03522
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.01314| avg_loss=0.03493
----------------------------------------------


----------------------------------------------
Params for Trial 29
{'learning_rate': 0.0001, 'weight_decay': 0.0013876994804289905, 'n_layers': 1, 'hidden_size': 32, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.07236 | best_loss=0.07236
Epoch 1/80: current_loss=0.05661 | best_loss=0.05661
Epoch 2/80: current_loss=0.04696 | best_loss=0.04696
Epoch 3/80: current_loss=0.04192 | best_loss=0.04192
Epoch 4/80: current_loss=0.04041 | best_loss=0.04041
Epoch 5/80: current_loss=0.03997 | best_loss=0.03997
Epoch 6/80: current_loss=0.03977 | best_loss=0.03977
Epoch 7/80: current_loss=0.03975 | best_loss=0.03975
Epoch 8/80: current_loss=0.03963 | best_loss=0.03963
Epoch 9/80: current_loss=0.03935 | best_loss=0.03935
Epoch 10/80: current_loss=0.03965 | best_loss=0.03935
Epoch 11/80: current_loss=0.03935 | best_loss=0.03935
Epoch 12/80: current_loss=0.03912 | best_loss=0.03912
Epoch 13/80: current_loss=0.03918 | best_loss=0.03912
Epoch 14/80: current_loss=0.03919 | best_loss=0.03912
Epoch 15/80: current_loss=0.03918 | best_loss=0.03912
Epoch 16/80: current_loss=0.03894 | best_loss=0.03894
Epoch 17/80: current_loss=0.03912 | best_loss=0.03894
Epoch 18/80: current_loss=0.03907 | best_loss=0.03894
Epoch 19/80: current_loss=0.03924 | best_loss=0.03894
Epoch 20/80: current_loss=0.03923 | best_loss=0.03894
Epoch 21/80: current_loss=0.03876 | best_loss=0.03876
Epoch 22/80: current_loss=0.03878 | best_loss=0.03876
Epoch 23/80: current_loss=0.03876 | best_loss=0.03876
Epoch 24/80: current_loss=0.03882 | best_loss=0.03876
Epoch 25/80: current_loss=0.03885 | best_loss=0.03876
Epoch 26/80: current_loss=0.03863 | best_loss=0.03863
Epoch 27/80: current_loss=0.03874 | best_loss=0.03863
Epoch 28/80: current_loss=0.03872 | best_loss=0.03863
Epoch 29/80: current_loss=0.03870 | best_loss=0.03863
Epoch 30/80: current_loss=0.03875 | best_loss=0.03863
Epoch 31/80: current_loss=0.03848 | best_loss=0.03848
Epoch 32/80: current_loss=0.03847 | best_loss=0.03847
Epoch 33/80: current_loss=0.03852 | best_loss=0.03847
Epoch 34/80: current_loss=0.03852 | best_loss=0.03847
Epoch 35/80: current_loss=0.03846 | best_loss=0.03846
Epoch 36/80: current_loss=0.03841 | best_loss=0.03841
Epoch 37/80: current_loss=0.03840 | best_loss=0.03840
Epoch 38/80: current_loss=0.03846 | best_loss=0.03840
Epoch 39/80: current_loss=0.03843 | best_loss=0.03840
Epoch 40/80: current_loss=0.03847 | best_loss=0.03840
Epoch 41/80: current_loss=0.03855 | best_loss=0.03840
Epoch 42/80: current_loss=0.03842 | best_loss=0.03840
Epoch 43/80: current_loss=0.03830 | best_loss=0.03830
Epoch 44/80: current_loss=0.03821 | best_loss=0.03821
Epoch 45/80: current_loss=0.03812 | best_loss=0.03812
Epoch 46/80: current_loss=0.03825 | best_loss=0.03812
Epoch 47/80: current_loss=0.03825 | best_loss=0.03812
Epoch 48/80: current_loss=0.03818 | best_loss=0.03812
Epoch 49/80: current_loss=0.03820 | best_loss=0.03812
Epoch 50/80: current_loss=0.03825 | best_loss=0.03812
Epoch 51/80: current_loss=0.03809 | best_loss=0.03809
Epoch 52/80: current_loss=0.03808 | best_loss=0.03808
Epoch 53/80: current_loss=0.03815 | best_loss=0.03808
Epoch 54/80: current_loss=0.03803 | best_loss=0.03803
Epoch 55/80: current_loss=0.03810 | best_loss=0.03803
Epoch 56/80: current_loss=0.03812 | best_loss=0.03803
Epoch 57/80: current_loss=0.03804 | best_loss=0.03803
Epoch 58/80: current_loss=0.03814 | best_loss=0.03803
Epoch 59/80: current_loss=0.03804 | best_loss=0.03803
Epoch 60/80: current_loss=0.03806 | best_loss=0.03803
Epoch 61/80: current_loss=0.03802 | best_loss=0.03802
Epoch 62/80: current_loss=0.03793 | best_loss=0.03793
Epoch 63/80: current_loss=0.03792 | best_loss=0.03792
Epoch 64/80: current_loss=0.03798 | best_loss=0.03792
Epoch 65/80: current_loss=0.03813 | best_loss=0.03792
Epoch 66/80: current_loss=0.03797 | best_loss=0.03792
Epoch 67/80: current_loss=0.03792 | best_loss=0.03792
Epoch 68/80: current_loss=0.03799 | best_loss=0.03792
Epoch 69/80: current_loss=0.03798 | best_loss=0.03792
Epoch 70/80: current_loss=0.03794 | best_loss=0.03792
Epoch 71/80: current_loss=0.03791 | best_loss=0.03791
Epoch 72/80: current_loss=0.03797 | best_loss=0.03791
Epoch 73/80: current_loss=0.03799 | best_loss=0.03791
Epoch 74/80: current_loss=0.03801 | best_loss=0.03791
Epoch 75/80: current_loss=0.03801 | best_loss=0.03791
Epoch 76/80: current_loss=0.03801 | best_loss=0.03791
Epoch 77/80: current_loss=0.03805 | best_loss=0.03791
Epoch 78/80: current_loss=0.03793 | best_loss=0.03791
Epoch 79/80: current_loss=0.03788 | best_loss=0.03788
      explained_var=0.01341 | mse_loss=0.03889

----------------------------------------------
Params for Trial 30
{'learning_rate': 0.01, 'weight_decay': 0.002485867808970739, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04252 | best_loss=0.04252
Epoch 1/80: current_loss=0.03811 | best_loss=0.03811
Epoch 2/80: current_loss=0.04131 | best_loss=0.03811
Epoch 3/80: current_loss=0.03891 | best_loss=0.03811
Epoch 4/80: current_loss=0.03823 | best_loss=0.03811
Epoch 5/80: current_loss=0.04151 | best_loss=0.03811
Epoch 6/80: current_loss=0.03802 | best_loss=0.03802
Epoch 7/80: current_loss=0.03923 | best_loss=0.03802
Epoch 8/80: current_loss=0.03901 | best_loss=0.03802
Epoch 9/80: current_loss=0.03870 | best_loss=0.03802
Epoch 10/80: current_loss=0.03767 | best_loss=0.03767
Epoch 11/80: current_loss=0.04195 | best_loss=0.03767
Epoch 12/80: current_loss=0.03807 | best_loss=0.03767
Epoch 13/80: current_loss=0.03766 | best_loss=0.03766
Epoch 14/80: current_loss=0.03777 | best_loss=0.03766
Epoch 15/80: current_loss=0.03778 | best_loss=0.03766
Epoch 16/80: current_loss=0.03927 | best_loss=0.03766
Epoch 17/80: current_loss=0.03836 | best_loss=0.03766
Epoch 18/80: current_loss=0.03784 | best_loss=0.03766
Epoch 19/80: current_loss=0.03783 | best_loss=0.03766
Epoch 20/80: current_loss=0.03790 | best_loss=0.03766
Epoch 21/80: current_loss=0.03794 | best_loss=0.03766
Epoch 22/80: current_loss=0.03912 | best_loss=0.03766
Epoch 23/80: current_loss=0.03794 | best_loss=0.03766
Epoch 24/80: current_loss=0.03777 | best_loss=0.03766
Epoch 25/80: current_loss=0.03898 | best_loss=0.03766
Epoch 26/80: current_loss=0.03803 | best_loss=0.03766
Epoch 27/80: current_loss=0.03780 | best_loss=0.03766
Epoch 28/80: current_loss=0.03853 | best_loss=0.03766
Epoch 29/80: current_loss=0.03832 | best_loss=0.03766
Epoch 30/80: current_loss=0.03808 | best_loss=0.03766
Epoch 31/80: current_loss=0.03782 | best_loss=0.03766
Epoch 32/80: current_loss=0.03805 | best_loss=0.03766
Epoch 33/80: current_loss=0.03824 | best_loss=0.03766
Early Stopping at epoch 33
      explained_var=0.01740 | mse_loss=0.03843
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04289 | best_loss=0.04289
Epoch 1/80: current_loss=0.04202 | best_loss=0.04202
Epoch 2/80: current_loss=0.04233 | best_loss=0.04202
Epoch 3/80: current_loss=0.04268 | best_loss=0.04202
Epoch 4/80: current_loss=0.04274 | best_loss=0.04202
Epoch 5/80: current_loss=0.04375 | best_loss=0.04202
Epoch 6/80: current_loss=0.04359 | best_loss=0.04202
Epoch 7/80: current_loss=0.04292 | best_loss=0.04202
Epoch 8/80: current_loss=0.04363 | best_loss=0.04202
Epoch 9/80: current_loss=0.04269 | best_loss=0.04202
Epoch 10/80: current_loss=0.04289 | best_loss=0.04202
Epoch 11/80: current_loss=0.04304 | best_loss=0.04202
Epoch 12/80: current_loss=0.04264 | best_loss=0.04202
Epoch 13/80: current_loss=0.04303 | best_loss=0.04202
Epoch 14/80: current_loss=0.04312 | best_loss=0.04202
Epoch 15/80: current_loss=0.04271 | best_loss=0.04202
Epoch 16/80: current_loss=0.04278 | best_loss=0.04202
Epoch 17/80: current_loss=0.04278 | best_loss=0.04202
Epoch 18/80: current_loss=0.04273 | best_loss=0.04202
Epoch 19/80: current_loss=0.04311 | best_loss=0.04202
Epoch 20/80: current_loss=0.04288 | best_loss=0.04202
Epoch 21/80: current_loss=0.04276 | best_loss=0.04202
Early Stopping at epoch 21
      explained_var=0.01644 | mse_loss=0.04059
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02956 | best_loss=0.02956
Epoch 1/80: current_loss=0.02802 | best_loss=0.02802
Epoch 2/80: current_loss=0.02807 | best_loss=0.02802
Epoch 3/80: current_loss=0.02808 | best_loss=0.02802
Epoch 4/80: current_loss=0.02832 | best_loss=0.02802
Epoch 5/80: current_loss=0.03021 | best_loss=0.02802
Epoch 6/80: current_loss=0.02840 | best_loss=0.02802
Epoch 7/80: current_loss=0.02802 | best_loss=0.02802
Epoch 8/80: current_loss=0.02801 | best_loss=0.02801
Epoch 9/80: current_loss=0.02834 | best_loss=0.02801
Epoch 10/80: current_loss=0.02914 | best_loss=0.02801
Epoch 11/80: current_loss=0.02871 | best_loss=0.02801
Epoch 12/80: current_loss=0.02812 | best_loss=0.02801
Epoch 13/80: current_loss=0.02867 | best_loss=0.02801
Epoch 14/80: current_loss=0.02989 | best_loss=0.02801
Epoch 15/80: current_loss=0.02812 | best_loss=0.02801
Epoch 16/80: current_loss=0.02799 | best_loss=0.02799
Epoch 17/80: current_loss=0.02798 | best_loss=0.02798
Epoch 18/80: current_loss=0.02809 | best_loss=0.02798
Epoch 19/80: current_loss=0.02801 | best_loss=0.02798
Epoch 20/80: current_loss=0.02853 | best_loss=0.02798
Epoch 21/80: current_loss=0.02887 | best_loss=0.02798
Epoch 22/80: current_loss=0.02863 | best_loss=0.02798
Epoch 23/80: current_loss=0.02853 | best_loss=0.02798
Epoch 24/80: current_loss=0.02800 | best_loss=0.02798
Epoch 25/80: current_loss=0.02798 | best_loss=0.02798
Epoch 26/80: current_loss=0.02886 | best_loss=0.02798
Epoch 27/80: current_loss=0.02809 | best_loss=0.02798
Epoch 28/80: current_loss=0.02844 | best_loss=0.02798
Epoch 29/80: current_loss=0.02815 | best_loss=0.02798
Epoch 30/80: current_loss=0.02819 | best_loss=0.02798
Epoch 31/80: current_loss=0.02816 | best_loss=0.02798
Epoch 32/80: current_loss=0.02830 | best_loss=0.02798
Epoch 33/80: current_loss=0.02827 | best_loss=0.02798
Epoch 34/80: current_loss=0.02833 | best_loss=0.02798
Epoch 35/80: current_loss=0.02814 | best_loss=0.02798
Epoch 36/80: current_loss=0.02852 | best_loss=0.02798
Epoch 37/80: current_loss=0.02823 | best_loss=0.02798
Early Stopping at epoch 37
      explained_var=0.00007 | mse_loss=0.02840
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03357 | best_loss=0.03357
Epoch 1/80: current_loss=0.03311 | best_loss=0.03311
Epoch 2/80: current_loss=0.03311 | best_loss=0.03311
Epoch 3/80: current_loss=0.03351 | best_loss=0.03311
Epoch 4/80: current_loss=0.03288 | best_loss=0.03288
Epoch 5/80: current_loss=0.03285 | best_loss=0.03285
Epoch 6/80: current_loss=0.03286 | best_loss=0.03285
Epoch 7/80: current_loss=0.03284 | best_loss=0.03284
Epoch 8/80: current_loss=0.03312 | best_loss=0.03284
Epoch 9/80: current_loss=0.03295 | best_loss=0.03284
Epoch 10/80: current_loss=0.03286 | best_loss=0.03284
Epoch 11/80: current_loss=0.03287 | best_loss=0.03284
Epoch 12/80: current_loss=0.03300 | best_loss=0.03284
Epoch 13/80: current_loss=0.03288 | best_loss=0.03284
Epoch 14/80: current_loss=0.03302 | best_loss=0.03284
Epoch 15/80: current_loss=0.03313 | best_loss=0.03284
Epoch 16/80: current_loss=0.03286 | best_loss=0.03284
Epoch 17/80: current_loss=0.03287 | best_loss=0.03284
Epoch 18/80: current_loss=0.03288 | best_loss=0.03284
Epoch 19/80: current_loss=0.03289 | best_loss=0.03284
Epoch 20/80: current_loss=0.03284 | best_loss=0.03284
Epoch 21/80: current_loss=0.03287 | best_loss=0.03284
Epoch 22/80: current_loss=0.03289 | best_loss=0.03284
Epoch 23/80: current_loss=0.03299 | best_loss=0.03284
Epoch 24/80: current_loss=0.03525 | best_loss=0.03284
Epoch 25/80: current_loss=0.03293 | best_loss=0.03284
Epoch 26/80: current_loss=0.03294 | best_loss=0.03284
Epoch 27/80: current_loss=0.03293 | best_loss=0.03284
Epoch 28/80: current_loss=0.03284 | best_loss=0.03284
Epoch 29/80: current_loss=0.10823 | best_loss=0.03284
Epoch 30/80: current_loss=0.10053 | best_loss=0.03284
Epoch 31/80: current_loss=0.06721 | best_loss=0.03284
Epoch 32/80: current_loss=0.03833 | best_loss=0.03284
Epoch 33/80: current_loss=0.03644 | best_loss=0.03284
Epoch 34/80: current_loss=0.03318 | best_loss=0.03284
Epoch 35/80: current_loss=0.03327 | best_loss=0.03284
Epoch 36/80: current_loss=0.03337 | best_loss=0.03284
Epoch 37/80: current_loss=0.03293 | best_loss=0.03284
Epoch 38/80: current_loss=0.03285 | best_loss=0.03284
Epoch 39/80: current_loss=0.03283 | best_loss=0.03283
Epoch 40/80: current_loss=0.03323 | best_loss=0.03283
Epoch 41/80: current_loss=0.03306 | best_loss=0.03283
Epoch 42/80: current_loss=0.03294 | best_loss=0.03283
Epoch 43/80: current_loss=0.03284 | best_loss=0.03283
Epoch 44/80: current_loss=0.03311 | best_loss=0.03283
Epoch 45/80: current_loss=0.03284 | best_loss=0.03283
Epoch 46/80: current_loss=0.03332 | best_loss=0.03283
Epoch 47/80: current_loss=0.03286 | best_loss=0.03283
Epoch 48/80: current_loss=0.03283 | best_loss=0.03283
Epoch 49/80: current_loss=0.03283 | best_loss=0.03283
Epoch 50/80: current_loss=0.03301 | best_loss=0.03283
Epoch 51/80: current_loss=0.03318 | best_loss=0.03283
Epoch 52/80: current_loss=0.03291 | best_loss=0.03283
Epoch 53/80: current_loss=0.03289 | best_loss=0.03283
Epoch 54/80: current_loss=0.03283 | best_loss=0.03283
Epoch 55/80: current_loss=0.03296 | best_loss=0.03283
Epoch 56/80: current_loss=0.03284 | best_loss=0.03283
Epoch 57/80: current_loss=0.03293 | best_loss=0.03283
Epoch 58/80: current_loss=0.03283 | best_loss=0.03283
Epoch 59/80: current_loss=0.03296 | best_loss=0.03283
Epoch 60/80: current_loss=0.03288 | best_loss=0.03283
Epoch 61/80: current_loss=0.03285 | best_loss=0.03283
Epoch 62/80: current_loss=0.03346 | best_loss=0.03283
Epoch 63/80: current_loss=0.03287 | best_loss=0.03283
Epoch 64/80: current_loss=0.03284 | best_loss=0.03283
Epoch 65/80: current_loss=0.03284 | best_loss=0.03283
Epoch 66/80: current_loss=0.03287 | best_loss=0.03283
Epoch 67/80: current_loss=0.03284 | best_loss=0.03283
Epoch 68/80: current_loss=0.03293 | best_loss=0.03283
Early Stopping at epoch 68
      explained_var=0.00001 | mse_loss=0.03261
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03619 | best_loss=0.03619
Epoch 1/80: current_loss=0.03602 | best_loss=0.03602
Epoch 2/80: current_loss=0.03619 | best_loss=0.03602
Epoch 3/80: current_loss=0.03618 | best_loss=0.03602
Epoch 4/80: current_loss=0.03627 | best_loss=0.03602
Epoch 5/80: current_loss=0.03684 | best_loss=0.03602
Epoch 6/80: current_loss=0.03619 | best_loss=0.03602
Epoch 7/80: current_loss=0.03620 | best_loss=0.03602
Epoch 8/80: current_loss=0.03633 | best_loss=0.03602
Epoch 9/80: current_loss=0.03623 | best_loss=0.03602
Epoch 10/80: current_loss=0.03633 | best_loss=0.03602
Epoch 11/80: current_loss=0.03626 | best_loss=0.03602
Epoch 12/80: current_loss=0.03667 | best_loss=0.03602
Epoch 13/80: current_loss=0.03629 | best_loss=0.03602
Epoch 14/80: current_loss=0.03619 | best_loss=0.03602
Epoch 15/80: current_loss=0.03655 | best_loss=0.03602
Epoch 16/80: current_loss=0.03619 | best_loss=0.03602
Epoch 17/80: current_loss=0.03626 | best_loss=0.03602
Epoch 18/80: current_loss=0.03625 | best_loss=0.03602
Epoch 19/80: current_loss=0.03654 | best_loss=0.03602
Epoch 20/80: current_loss=0.03621 | best_loss=0.03602
Epoch 21/80: current_loss=0.03599 | best_loss=0.03599
Epoch 22/80: current_loss=0.44857 | best_loss=0.03599
Epoch 23/80: current_loss=0.03649 | best_loss=0.03599
Epoch 24/80: current_loss=0.09842 | best_loss=0.03599
Epoch 25/80: current_loss=0.03600 | best_loss=0.03599
Epoch 26/80: current_loss=0.03616 | best_loss=0.03599
Epoch 27/80: current_loss=0.03620 | best_loss=0.03599
Epoch 28/80: current_loss=0.03617 | best_loss=0.03599
Epoch 29/80: current_loss=0.03625 | best_loss=0.03599
Epoch 30/80: current_loss=0.03620 | best_loss=0.03599
Epoch 31/80: current_loss=0.03624 | best_loss=0.03599
Epoch 32/80: current_loss=0.03619 | best_loss=0.03599
Epoch 33/80: current_loss=0.03625 | best_loss=0.03599
Epoch 34/80: current_loss=0.03621 | best_loss=0.03599
Epoch 35/80: current_loss=0.03619 | best_loss=0.03599
Epoch 36/80: current_loss=0.03624 | best_loss=0.03599
Epoch 37/80: current_loss=0.03630 | best_loss=0.03599
Epoch 38/80: current_loss=0.03620 | best_loss=0.03599
Epoch 39/80: current_loss=0.03626 | best_loss=0.03599
Epoch 40/80: current_loss=0.03619 | best_loss=0.03599
Epoch 41/80: current_loss=0.03619 | best_loss=0.03599
Early Stopping at epoch 41
      explained_var=0.00550 | mse_loss=0.03569
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.00788| avg_loss=0.03514
----------------------------------------------


----------------------------------------------
Params for Trial 31
{'learning_rate': 0.0001, 'weight_decay': 0.0001226870856730668, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04873 | best_loss=0.04873
Epoch 1/80: current_loss=0.04358 | best_loss=0.04358
Epoch 2/80: current_loss=0.04250 | best_loss=0.04250
Epoch 3/80: current_loss=0.04149 | best_loss=0.04149
Epoch 4/80: current_loss=0.04082 | best_loss=0.04082
Epoch 5/80: current_loss=0.04030 | best_loss=0.04030
Epoch 6/80: current_loss=0.04015 | best_loss=0.04015
Epoch 7/80: current_loss=0.03961 | best_loss=0.03961
Epoch 8/80: current_loss=0.03964 | best_loss=0.03961
Epoch 9/80: current_loss=0.03899 | best_loss=0.03899
Epoch 10/80: current_loss=0.03867 | best_loss=0.03867
Epoch 11/80: current_loss=0.03853 | best_loss=0.03853
Epoch 12/80: current_loss=0.03829 | best_loss=0.03829
Epoch 13/80: current_loss=0.03850 | best_loss=0.03829
Epoch 14/80: current_loss=0.03818 | best_loss=0.03818
Epoch 15/80: current_loss=0.03827 | best_loss=0.03818
Epoch 16/80: current_loss=0.03812 | best_loss=0.03812
Epoch 17/80: current_loss=0.03816 | best_loss=0.03812
Epoch 18/80: current_loss=0.03802 | best_loss=0.03802
Epoch 19/80: current_loss=0.03788 | best_loss=0.03788
Epoch 20/80: current_loss=0.03790 | best_loss=0.03788
Epoch 21/80: current_loss=0.03782 | best_loss=0.03782
Epoch 22/80: current_loss=0.03782 | best_loss=0.03782
Epoch 23/80: current_loss=0.03781 | best_loss=0.03781
Epoch 24/80: current_loss=0.03782 | best_loss=0.03781
Epoch 25/80: current_loss=0.03780 | best_loss=0.03780
Epoch 26/80: current_loss=0.03783 | best_loss=0.03780
Epoch 27/80: current_loss=0.03773 | best_loss=0.03773
Epoch 28/80: current_loss=0.03767 | best_loss=0.03767
Epoch 29/80: current_loss=0.03773 | best_loss=0.03767
Epoch 30/80: current_loss=0.03767 | best_loss=0.03767
Epoch 31/80: current_loss=0.03779 | best_loss=0.03767
Epoch 32/80: current_loss=0.03770 | best_loss=0.03767
Epoch 33/80: current_loss=0.03759 | best_loss=0.03759
Epoch 34/80: current_loss=0.03776 | best_loss=0.03759
Epoch 35/80: current_loss=0.03869 | best_loss=0.03759
Epoch 36/80: current_loss=0.03747 | best_loss=0.03747
Epoch 37/80: current_loss=0.03764 | best_loss=0.03747
Epoch 38/80: current_loss=0.03745 | best_loss=0.03745
Epoch 39/80: current_loss=0.03774 | best_loss=0.03745
Epoch 40/80: current_loss=0.03746 | best_loss=0.03745
Epoch 41/80: current_loss=0.03750 | best_loss=0.03745
Epoch 42/80: current_loss=0.03745 | best_loss=0.03745
Epoch 43/80: current_loss=0.03805 | best_loss=0.03745
Epoch 44/80: current_loss=0.03745 | best_loss=0.03745
Epoch 45/80: current_loss=0.03746 | best_loss=0.03745
Epoch 46/80: current_loss=0.03747 | best_loss=0.03745
Epoch 47/80: current_loss=0.03759 | best_loss=0.03745
Epoch 48/80: current_loss=0.03806 | best_loss=0.03745
Epoch 49/80: current_loss=0.03744 | best_loss=0.03744
Epoch 50/80: current_loss=0.03752 | best_loss=0.03744
Epoch 51/80: current_loss=0.03778 | best_loss=0.03744
Epoch 52/80: current_loss=0.03756 | best_loss=0.03744
Epoch 53/80: current_loss=0.03775 | best_loss=0.03744
Epoch 54/80: current_loss=0.03768 | best_loss=0.03744
Epoch 55/80: current_loss=0.03751 | best_loss=0.03744
Epoch 56/80: current_loss=0.03747 | best_loss=0.03744
Epoch 57/80: current_loss=0.03760 | best_loss=0.03744
Epoch 58/80: current_loss=0.03756 | best_loss=0.03744
Epoch 59/80: current_loss=0.03760 | best_loss=0.03744
Epoch 60/80: current_loss=0.03755 | best_loss=0.03744
Epoch 61/80: current_loss=0.03754 | best_loss=0.03744
Epoch 62/80: current_loss=0.03747 | best_loss=0.03744
Epoch 63/80: current_loss=0.03802 | best_loss=0.03744
Epoch 64/80: current_loss=0.03776 | best_loss=0.03744
Epoch 65/80: current_loss=0.03758 | best_loss=0.03744
Epoch 66/80: current_loss=0.03768 | best_loss=0.03744
Epoch 67/80: current_loss=0.03783 | best_loss=0.03744
Epoch 68/80: current_loss=0.03751 | best_loss=0.03744
Epoch 69/80: current_loss=0.03746 | best_loss=0.03744
Early Stopping at epoch 69
      explained_var=0.02145 | mse_loss=0.03834
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04133 | best_loss=0.04133
Epoch 1/80: current_loss=0.04158 | best_loss=0.04133
Epoch 2/80: current_loss=0.04143 | best_loss=0.04133
Epoch 3/80: current_loss=0.04135 | best_loss=0.04133
Epoch 4/80: current_loss=0.04129 | best_loss=0.04129
Epoch 5/80: current_loss=0.04238 | best_loss=0.04129
Epoch 6/80: current_loss=0.04134 | best_loss=0.04129
Epoch 7/80: current_loss=0.04169 | best_loss=0.04129
Epoch 8/80: current_loss=0.04141 | best_loss=0.04129
Epoch 9/80: current_loss=0.04164 | best_loss=0.04129
Epoch 10/80: current_loss=0.04160 | best_loss=0.04129
Epoch 11/80: current_loss=0.04179 | best_loss=0.04129
Epoch 12/80: current_loss=0.04185 | best_loss=0.04129
Epoch 13/80: current_loss=0.04157 | best_loss=0.04129
Epoch 14/80: current_loss=0.04145 | best_loss=0.04129
Epoch 15/80: current_loss=0.04199 | best_loss=0.04129
Epoch 16/80: current_loss=0.04132 | best_loss=0.04129
Epoch 17/80: current_loss=0.04132 | best_loss=0.04129
Epoch 18/80: current_loss=0.04137 | best_loss=0.04129
Epoch 19/80: current_loss=0.04132 | best_loss=0.04129
Epoch 20/80: current_loss=0.04145 | best_loss=0.04129
Epoch 21/80: current_loss=0.04133 | best_loss=0.04129
Epoch 22/80: current_loss=0.04149 | best_loss=0.04129
Epoch 23/80: current_loss=0.04137 | best_loss=0.04129
Epoch 24/80: current_loss=0.04149 | best_loss=0.04129
Early Stopping at epoch 24
      explained_var=0.03467 | mse_loss=0.03983
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02825 | best_loss=0.02825
Epoch 1/80: current_loss=0.02831 | best_loss=0.02825
Epoch 2/80: current_loss=0.02802 | best_loss=0.02802
Epoch 3/80: current_loss=0.02845 | best_loss=0.02802
Epoch 4/80: current_loss=0.02812 | best_loss=0.02802
Epoch 5/80: current_loss=0.02841 | best_loss=0.02802
Epoch 6/80: current_loss=0.02822 | best_loss=0.02802
Epoch 7/80: current_loss=0.02804 | best_loss=0.02802
Epoch 8/80: current_loss=0.02812 | best_loss=0.02802
Epoch 9/80: current_loss=0.02918 | best_loss=0.02802
Epoch 10/80: current_loss=0.02835 | best_loss=0.02802
Epoch 11/80: current_loss=0.02822 | best_loss=0.02802
Epoch 12/80: current_loss=0.02829 | best_loss=0.02802
Epoch 13/80: current_loss=0.02835 | best_loss=0.02802
Epoch 14/80: current_loss=0.02815 | best_loss=0.02802
Epoch 15/80: current_loss=0.02894 | best_loss=0.02802
Epoch 16/80: current_loss=0.02819 | best_loss=0.02802
Epoch 17/80: current_loss=0.02871 | best_loss=0.02802
Epoch 18/80: current_loss=0.02811 | best_loss=0.02802
Epoch 19/80: current_loss=0.02935 | best_loss=0.02802
Epoch 20/80: current_loss=0.02812 | best_loss=0.02802
Epoch 21/80: current_loss=0.02849 | best_loss=0.02802
Epoch 22/80: current_loss=0.02826 | best_loss=0.02802
Early Stopping at epoch 22
      explained_var=-0.00265 | mse_loss=0.02846
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03299 | best_loss=0.03299
Epoch 1/80: current_loss=0.03310 | best_loss=0.03299
Epoch 2/80: current_loss=0.03303 | best_loss=0.03299
Epoch 3/80: current_loss=0.03308 | best_loss=0.03299
Epoch 4/80: current_loss=0.03300 | best_loss=0.03299
Epoch 5/80: current_loss=0.03314 | best_loss=0.03299
Epoch 6/80: current_loss=0.03309 | best_loss=0.03299
Epoch 7/80: current_loss=0.03313 | best_loss=0.03299
Epoch 8/80: current_loss=0.03322 | best_loss=0.03299
Epoch 9/80: current_loss=0.03341 | best_loss=0.03299
Epoch 10/80: current_loss=0.03334 | best_loss=0.03299
Epoch 11/80: current_loss=0.03314 | best_loss=0.03299
Epoch 12/80: current_loss=0.03308 | best_loss=0.03299
Epoch 13/80: current_loss=0.03323 | best_loss=0.03299
Epoch 14/80: current_loss=0.03307 | best_loss=0.03299
Epoch 15/80: current_loss=0.03307 | best_loss=0.03299
Epoch 16/80: current_loss=0.03327 | best_loss=0.03299
Epoch 17/80: current_loss=0.03302 | best_loss=0.03299
Epoch 18/80: current_loss=0.03304 | best_loss=0.03299
Epoch 19/80: current_loss=0.03313 | best_loss=0.03299
Epoch 20/80: current_loss=0.03301 | best_loss=0.03299
Early Stopping at epoch 20
      explained_var=-0.00960 | mse_loss=0.03292
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03533 | best_loss=0.03533
Epoch 1/80: current_loss=0.03546 | best_loss=0.03533
Epoch 2/80: current_loss=0.03534 | best_loss=0.03533
Epoch 3/80: current_loss=0.03543 | best_loss=0.03533
Epoch 4/80: current_loss=0.03533 | best_loss=0.03533
Epoch 5/80: current_loss=0.03533 | best_loss=0.03533
Epoch 6/80: current_loss=0.03532 | best_loss=0.03532
Epoch 7/80: current_loss=0.03537 | best_loss=0.03532
Epoch 8/80: current_loss=0.03530 | best_loss=0.03530
Epoch 9/80: current_loss=0.03532 | best_loss=0.03530
Epoch 10/80: current_loss=0.03530 | best_loss=0.03530
Epoch 11/80: current_loss=0.03554 | best_loss=0.03530
Epoch 12/80: current_loss=0.03545 | best_loss=0.03530
Epoch 13/80: current_loss=0.03534 | best_loss=0.03530
Epoch 14/80: current_loss=0.03558 | best_loss=0.03530
Epoch 15/80: current_loss=0.03535 | best_loss=0.03530
Epoch 16/80: current_loss=0.03538 | best_loss=0.03530
Epoch 17/80: current_loss=0.03547 | best_loss=0.03530
Epoch 18/80: current_loss=0.03551 | best_loss=0.03530
Epoch 19/80: current_loss=0.03546 | best_loss=0.03530
Epoch 20/80: current_loss=0.03543 | best_loss=0.03530
Epoch 21/80: current_loss=0.03555 | best_loss=0.03530
Epoch 22/80: current_loss=0.03562 | best_loss=0.03530
Epoch 23/80: current_loss=0.03545 | best_loss=0.03530
Epoch 24/80: current_loss=0.03546 | best_loss=0.03530
Epoch 25/80: current_loss=0.03542 | best_loss=0.03530
Epoch 26/80: current_loss=0.03536 | best_loss=0.03530
Epoch 27/80: current_loss=0.03534 | best_loss=0.03530
Epoch 28/80: current_loss=0.03545 | best_loss=0.03530
Early Stopping at epoch 28
      explained_var=0.02142 | mse_loss=0.03512
----------------------------------------------
Average early_stopping_point: 12| avg_exp_var=0.01306| avg_loss=0.03493
----------------------------------------------


----------------------------------------------
Params for Trial 32
{'learning_rate': 0.0001, 'weight_decay': 0.0004997752360387344, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05245 | best_loss=0.05245
Epoch 1/80: current_loss=0.04518 | best_loss=0.04518
Epoch 2/80: current_loss=0.04406 | best_loss=0.04406
Epoch 3/80: current_loss=0.04306 | best_loss=0.04306
Epoch 4/80: current_loss=0.04259 | best_loss=0.04259
Epoch 5/80: current_loss=0.04192 | best_loss=0.04192
Epoch 6/80: current_loss=0.04129 | best_loss=0.04129
Epoch 7/80: current_loss=0.04114 | best_loss=0.04114
Epoch 8/80: current_loss=0.04065 | best_loss=0.04065
Epoch 9/80: current_loss=0.04034 | best_loss=0.04034
Epoch 10/80: current_loss=0.03996 | best_loss=0.03996
Epoch 11/80: current_loss=0.03977 | best_loss=0.03977
Epoch 12/80: current_loss=0.03953 | best_loss=0.03953
Epoch 13/80: current_loss=0.03935 | best_loss=0.03935
Epoch 14/80: current_loss=0.03934 | best_loss=0.03934
Epoch 15/80: current_loss=0.03882 | best_loss=0.03882
Epoch 16/80: current_loss=0.03883 | best_loss=0.03882
Epoch 17/80: current_loss=0.03852 | best_loss=0.03852
Epoch 18/80: current_loss=0.03839 | best_loss=0.03839
Epoch 19/80: current_loss=0.03824 | best_loss=0.03824
Epoch 20/80: current_loss=0.03820 | best_loss=0.03820
Epoch 21/80: current_loss=0.03831 | best_loss=0.03820
Epoch 22/80: current_loss=0.03819 | best_loss=0.03819
Epoch 23/80: current_loss=0.03804 | best_loss=0.03804
Epoch 24/80: current_loss=0.03808 | best_loss=0.03804
Epoch 25/80: current_loss=0.03788 | best_loss=0.03788
Epoch 26/80: current_loss=0.03817 | best_loss=0.03788
Epoch 27/80: current_loss=0.03790 | best_loss=0.03788
Epoch 28/80: current_loss=0.03834 | best_loss=0.03788
Epoch 29/80: current_loss=0.03781 | best_loss=0.03781
Epoch 30/80: current_loss=0.03796 | best_loss=0.03781
Epoch 31/80: current_loss=0.03782 | best_loss=0.03781
Epoch 32/80: current_loss=0.03783 | best_loss=0.03781
Epoch 33/80: current_loss=0.03793 | best_loss=0.03781
Epoch 34/80: current_loss=0.03768 | best_loss=0.03768
Epoch 35/80: current_loss=0.03791 | best_loss=0.03768
Epoch 36/80: current_loss=0.03768 | best_loss=0.03768
Epoch 37/80: current_loss=0.03824 | best_loss=0.03768
Epoch 38/80: current_loss=0.03759 | best_loss=0.03759
Epoch 39/80: current_loss=0.03765 | best_loss=0.03759
Epoch 40/80: current_loss=0.03756 | best_loss=0.03756
Epoch 41/80: current_loss=0.03770 | best_loss=0.03756
Epoch 42/80: current_loss=0.03763 | best_loss=0.03756
Epoch 43/80: current_loss=0.03769 | best_loss=0.03756
Epoch 44/80: current_loss=0.03749 | best_loss=0.03749
Epoch 45/80: current_loss=0.03759 | best_loss=0.03749
Epoch 46/80: current_loss=0.03758 | best_loss=0.03749
Epoch 47/80: current_loss=0.03752 | best_loss=0.03749
Epoch 48/80: current_loss=0.03750 | best_loss=0.03749
Epoch 49/80: current_loss=0.03747 | best_loss=0.03747
Epoch 50/80: current_loss=0.03749 | best_loss=0.03747
Epoch 51/80: current_loss=0.03762 | best_loss=0.03747
Epoch 52/80: current_loss=0.03752 | best_loss=0.03747
Epoch 53/80: current_loss=0.03773 | best_loss=0.03747
Epoch 54/80: current_loss=0.03767 | best_loss=0.03747
Epoch 55/80: current_loss=0.03763 | best_loss=0.03747
Epoch 56/80: current_loss=0.03767 | best_loss=0.03747
Epoch 57/80: current_loss=0.03780 | best_loss=0.03747
Epoch 58/80: current_loss=0.03787 | best_loss=0.03747
Epoch 59/80: current_loss=0.03762 | best_loss=0.03747
Epoch 60/80: current_loss=0.03760 | best_loss=0.03747
Epoch 61/80: current_loss=0.03792 | best_loss=0.03747
Epoch 62/80: current_loss=0.03771 | best_loss=0.03747
Epoch 63/80: current_loss=0.03761 | best_loss=0.03747
Epoch 64/80: current_loss=0.03795 | best_loss=0.03747
Epoch 65/80: current_loss=0.03763 | best_loss=0.03747
Epoch 66/80: current_loss=0.03787 | best_loss=0.03747
Epoch 67/80: current_loss=0.03781 | best_loss=0.03747
Epoch 68/80: current_loss=0.03759 | best_loss=0.03747
Epoch 69/80: current_loss=0.03762 | best_loss=0.03747
Early Stopping at epoch 69
      explained_var=0.01940 | mse_loss=0.03840
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04152 | best_loss=0.04152
Epoch 1/80: current_loss=0.04141 | best_loss=0.04141
Epoch 2/80: current_loss=0.04171 | best_loss=0.04141
Epoch 3/80: current_loss=0.04158 | best_loss=0.04141
Epoch 4/80: current_loss=0.04145 | best_loss=0.04141
Epoch 5/80: current_loss=0.04144 | best_loss=0.04141
Epoch 6/80: current_loss=0.04186 | best_loss=0.04141
Epoch 7/80: current_loss=0.04150 | best_loss=0.04141
Epoch 8/80: current_loss=0.04160 | best_loss=0.04141
Epoch 9/80: current_loss=0.04155 | best_loss=0.04141
Epoch 10/80: current_loss=0.04164 | best_loss=0.04141
Epoch 11/80: current_loss=0.04151 | best_loss=0.04141
Epoch 12/80: current_loss=0.04151 | best_loss=0.04141
Epoch 13/80: current_loss=0.04157 | best_loss=0.04141
Epoch 14/80: current_loss=0.04152 | best_loss=0.04141
Epoch 15/80: current_loss=0.04166 | best_loss=0.04141
Epoch 16/80: current_loss=0.04167 | best_loss=0.04141
Epoch 17/80: current_loss=0.04154 | best_loss=0.04141
Epoch 18/80: current_loss=0.04166 | best_loss=0.04141
Epoch 19/80: current_loss=0.04161 | best_loss=0.04141
Epoch 20/80: current_loss=0.04148 | best_loss=0.04141
Epoch 21/80: current_loss=0.04154 | best_loss=0.04141
Early Stopping at epoch 21
      explained_var=0.03338 | mse_loss=0.03993
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02815 | best_loss=0.02815
Epoch 1/80: current_loss=0.02833 | best_loss=0.02815
Epoch 2/80: current_loss=0.02810 | best_loss=0.02810
Epoch 3/80: current_loss=0.02844 | best_loss=0.02810
Epoch 4/80: current_loss=0.02802 | best_loss=0.02802
Epoch 5/80: current_loss=0.02828 | best_loss=0.02802
Epoch 6/80: current_loss=0.02845 | best_loss=0.02802
Epoch 7/80: current_loss=0.02806 | best_loss=0.02802
Epoch 8/80: current_loss=0.02849 | best_loss=0.02802
Epoch 9/80: current_loss=0.02823 | best_loss=0.02802
Epoch 10/80: current_loss=0.02814 | best_loss=0.02802
Epoch 11/80: current_loss=0.02843 | best_loss=0.02802
Epoch 12/80: current_loss=0.02824 | best_loss=0.02802
Epoch 13/80: current_loss=0.02884 | best_loss=0.02802
Epoch 14/80: current_loss=0.02820 | best_loss=0.02802
Epoch 15/80: current_loss=0.02845 | best_loss=0.02802
Epoch 16/80: current_loss=0.02844 | best_loss=0.02802
Epoch 17/80: current_loss=0.02869 | best_loss=0.02802
Epoch 18/80: current_loss=0.02818 | best_loss=0.02802
Epoch 19/80: current_loss=0.02830 | best_loss=0.02802
Epoch 20/80: current_loss=0.02820 | best_loss=0.02802
Epoch 21/80: current_loss=0.02850 | best_loss=0.02802
Epoch 22/80: current_loss=0.02867 | best_loss=0.02802
Epoch 23/80: current_loss=0.02830 | best_loss=0.02802
Epoch 24/80: current_loss=0.02854 | best_loss=0.02802
Early Stopping at epoch 24
      explained_var=-0.00417 | mse_loss=0.02848

----------------------------------------------
Params for Trial 33
{'learning_rate': 1e-05, 'weight_decay': 0.001670243925108498, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.08851 | best_loss=0.08851
Epoch 1/80: current_loss=0.07808 | best_loss=0.07808
Epoch 2/80: current_loss=0.06909 | best_loss=0.06909
Epoch 3/80: current_loss=0.06181 | best_loss=0.06181
Epoch 4/80: current_loss=0.05585 | best_loss=0.05585
Epoch 5/80: current_loss=0.05137 | best_loss=0.05137
Epoch 6/80: current_loss=0.04821 | best_loss=0.04821
Epoch 7/80: current_loss=0.04597 | best_loss=0.04597
Epoch 8/80: current_loss=0.04445 | best_loss=0.04445
Epoch 9/80: current_loss=0.04359 | best_loss=0.04359
Epoch 10/80: current_loss=0.04298 | best_loss=0.04298
Epoch 11/80: current_loss=0.04257 | best_loss=0.04257
Epoch 12/80: current_loss=0.04233 | best_loss=0.04233
Epoch 13/80: current_loss=0.04213 | best_loss=0.04213
Epoch 14/80: current_loss=0.04205 | best_loss=0.04205
Epoch 15/80: current_loss=0.04200 | best_loss=0.04200
Epoch 16/80: current_loss=0.04185 | best_loss=0.04185
Epoch 17/80: current_loss=0.04174 | best_loss=0.04174
Epoch 18/80: current_loss=0.04164 | best_loss=0.04164
Epoch 19/80: current_loss=0.04156 | best_loss=0.04156
Epoch 20/80: current_loss=0.04149 | best_loss=0.04149
Epoch 21/80: current_loss=0.04145 | best_loss=0.04145
Epoch 22/80: current_loss=0.04140 | best_loss=0.04140
Epoch 23/80: current_loss=0.04129 | best_loss=0.04129
Epoch 24/80: current_loss=0.04127 | best_loss=0.04127
Epoch 25/80: current_loss=0.04114 | best_loss=0.04114
Epoch 26/80: current_loss=0.04110 | best_loss=0.04110
Epoch 27/80: current_loss=0.04107 | best_loss=0.04107
Epoch 28/80: current_loss=0.04098 | best_loss=0.04098
Epoch 29/80: current_loss=0.04091 | best_loss=0.04091
Epoch 30/80: current_loss=0.04080 | best_loss=0.04080
Epoch 31/80: current_loss=0.04074 | best_loss=0.04074
Epoch 32/80: current_loss=0.04071 | best_loss=0.04071
Epoch 33/80: current_loss=0.04064 | best_loss=0.04064
Epoch 34/80: current_loss=0.04057 | best_loss=0.04057
Epoch 35/80: current_loss=0.04054 | best_loss=0.04054
Epoch 36/80: current_loss=0.04047 | best_loss=0.04047
Epoch 37/80: current_loss=0.04045 | best_loss=0.04045
Epoch 38/80: current_loss=0.04040 | best_loss=0.04040
Epoch 39/80: current_loss=0.04038 | best_loss=0.04038
Epoch 40/80: current_loss=0.04026 | best_loss=0.04026
Epoch 41/80: current_loss=0.04025 | best_loss=0.04025
Epoch 42/80: current_loss=0.04018 | best_loss=0.04018
Epoch 43/80: current_loss=0.04016 | best_loss=0.04016
Epoch 44/80: current_loss=0.04011 | best_loss=0.04011
Epoch 45/80: current_loss=0.04007 | best_loss=0.04007
Epoch 46/80: current_loss=0.03999 | best_loss=0.03999
Epoch 47/80: current_loss=0.04000 | best_loss=0.03999
Epoch 48/80: current_loss=0.03993 | best_loss=0.03993
Epoch 49/80: current_loss=0.03986 | best_loss=0.03986
Epoch 50/80: current_loss=0.03988 | best_loss=0.03986
Epoch 51/80: current_loss=0.03976 | best_loss=0.03976
Epoch 52/80: current_loss=0.03971 | best_loss=0.03971
Epoch 53/80: current_loss=0.03967 | best_loss=0.03967
Epoch 54/80: current_loss=0.03964 | best_loss=0.03964
Epoch 55/80: current_loss=0.03963 | best_loss=0.03963
Epoch 56/80: current_loss=0.03958 | best_loss=0.03958
Epoch 57/80: current_loss=0.03952 | best_loss=0.03952
Epoch 58/80: current_loss=0.03948 | best_loss=0.03948
Epoch 59/80: current_loss=0.03946 | best_loss=0.03946
Epoch 60/80: current_loss=0.03944 | best_loss=0.03944
Epoch 61/80: current_loss=0.03944 | best_loss=0.03944
Epoch 62/80: current_loss=0.03935 | best_loss=0.03935
Epoch 63/80: current_loss=0.03930 | best_loss=0.03930
Epoch 64/80: current_loss=0.03932 | best_loss=0.03930
Epoch 65/80: current_loss=0.03928 | best_loss=0.03928
Epoch 66/80: current_loss=0.03932 | best_loss=0.03928
Epoch 67/80: current_loss=0.03925 | best_loss=0.03925
Epoch 68/80: current_loss=0.03924 | best_loss=0.03924
Epoch 69/80: current_loss=0.03918 | best_loss=0.03918
Epoch 70/80: current_loss=0.03917 | best_loss=0.03917
Epoch 71/80: current_loss=0.03913 | best_loss=0.03913
Epoch 72/80: current_loss=0.03910 | best_loss=0.03910
Epoch 73/80: current_loss=0.03908 | best_loss=0.03908
Epoch 74/80: current_loss=0.03906 | best_loss=0.03906
Epoch 75/80: current_loss=0.03904 | best_loss=0.03904
Epoch 76/80: current_loss=0.03902 | best_loss=0.03902
Epoch 77/80: current_loss=0.03904 | best_loss=0.03902
Epoch 78/80: current_loss=0.03899 | best_loss=0.03899
Epoch 79/80: current_loss=0.03896 | best_loss=0.03896
      explained_var=-0.01422 | mse_loss=0.04000

----------------------------------------------
Params for Trial 34
{'learning_rate': 0.001, 'weight_decay': 0.000520514148969476, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03994 | best_loss=0.03994
Epoch 1/80: current_loss=0.03826 | best_loss=0.03826
Epoch 2/80: current_loss=0.03861 | best_loss=0.03826
Epoch 3/80: current_loss=0.03828 | best_loss=0.03826
Epoch 4/80: current_loss=0.03847 | best_loss=0.03826
Epoch 5/80: current_loss=0.03746 | best_loss=0.03746
Epoch 6/80: current_loss=0.03868 | best_loss=0.03746
Epoch 7/80: current_loss=0.03828 | best_loss=0.03746
Epoch 8/80: current_loss=0.03789 | best_loss=0.03746
Epoch 9/80: current_loss=0.03808 | best_loss=0.03746
Epoch 10/80: current_loss=0.03776 | best_loss=0.03746
Epoch 11/80: current_loss=0.03745 | best_loss=0.03745
Epoch 12/80: current_loss=0.03768 | best_loss=0.03745
Epoch 13/80: current_loss=0.03798 | best_loss=0.03745
Epoch 14/80: current_loss=0.03764 | best_loss=0.03745
Epoch 15/80: current_loss=0.03760 | best_loss=0.03745
Epoch 16/80: current_loss=0.03904 | best_loss=0.03745
Epoch 17/80: current_loss=0.03748 | best_loss=0.03745
Epoch 18/80: current_loss=0.03737 | best_loss=0.03737
Epoch 19/80: current_loss=0.03800 | best_loss=0.03737
Epoch 20/80: current_loss=0.03716 | best_loss=0.03716
Epoch 21/80: current_loss=0.03772 | best_loss=0.03716
Epoch 22/80: current_loss=0.03755 | best_loss=0.03716
Epoch 23/80: current_loss=0.03874 | best_loss=0.03716
Epoch 24/80: current_loss=0.03793 | best_loss=0.03716
Epoch 25/80: current_loss=0.03761 | best_loss=0.03716
Epoch 26/80: current_loss=0.03766 | best_loss=0.03716
Epoch 27/80: current_loss=0.03776 | best_loss=0.03716
Epoch 28/80: current_loss=0.03836 | best_loss=0.03716
Epoch 29/80: current_loss=0.03769 | best_loss=0.03716
Epoch 30/80: current_loss=0.03783 | best_loss=0.03716
Epoch 31/80: current_loss=0.03783 | best_loss=0.03716
Epoch 32/80: current_loss=0.03757 | best_loss=0.03716
Epoch 33/80: current_loss=0.03803 | best_loss=0.03716
Epoch 34/80: current_loss=0.03775 | best_loss=0.03716
Epoch 35/80: current_loss=0.03772 | best_loss=0.03716
Epoch 36/80: current_loss=0.03961 | best_loss=0.03716
Epoch 37/80: current_loss=0.03767 | best_loss=0.03716
Epoch 38/80: current_loss=0.03804 | best_loss=0.03716
Epoch 39/80: current_loss=0.03862 | best_loss=0.03716
Epoch 40/80: current_loss=0.03752 | best_loss=0.03716
Early Stopping at epoch 40
      explained_var=0.03020 | mse_loss=0.03799
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04181 | best_loss=0.04181
Epoch 1/80: current_loss=0.04206 | best_loss=0.04181
Epoch 2/80: current_loss=0.04228 | best_loss=0.04181
Epoch 3/80: current_loss=0.04198 | best_loss=0.04181
Epoch 4/80: current_loss=0.04184 | best_loss=0.04181
Epoch 5/80: current_loss=0.04202 | best_loss=0.04181
Epoch 6/80: current_loss=0.04301 | best_loss=0.04181
Epoch 7/80: current_loss=0.04179 | best_loss=0.04179
Epoch 8/80: current_loss=0.04279 | best_loss=0.04179
Epoch 9/80: current_loss=0.04196 | best_loss=0.04179
Epoch 10/80: current_loss=0.04191 | best_loss=0.04179
Epoch 11/80: current_loss=0.04209 | best_loss=0.04179
Epoch 12/80: current_loss=0.04205 | best_loss=0.04179
Epoch 13/80: current_loss=0.04192 | best_loss=0.04179
Epoch 14/80: current_loss=0.04216 | best_loss=0.04179
Epoch 15/80: current_loss=0.04193 | best_loss=0.04179
Epoch 16/80: current_loss=0.04182 | best_loss=0.04179
Epoch 17/80: current_loss=0.04201 | best_loss=0.04179
Epoch 18/80: current_loss=0.04260 | best_loss=0.04179
Epoch 19/80: current_loss=0.04185 | best_loss=0.04179
Epoch 20/80: current_loss=0.04188 | best_loss=0.04179
Epoch 21/80: current_loss=0.04264 | best_loss=0.04179
Epoch 22/80: current_loss=0.04262 | best_loss=0.04179
Epoch 23/80: current_loss=0.04243 | best_loss=0.04179
Epoch 24/80: current_loss=0.04210 | best_loss=0.04179
Epoch 25/80: current_loss=0.04299 | best_loss=0.04179
Epoch 26/80: current_loss=0.04241 | best_loss=0.04179
Epoch 27/80: current_loss=0.04193 | best_loss=0.04179
Early Stopping at epoch 27
      explained_var=0.02340 | mse_loss=0.04036
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02903 | best_loss=0.02903
Epoch 1/80: current_loss=0.02932 | best_loss=0.02903
Epoch 2/80: current_loss=0.02822 | best_loss=0.02822
Epoch 3/80: current_loss=0.02832 | best_loss=0.02822
Epoch 4/80: current_loss=0.02807 | best_loss=0.02807
Epoch 5/80: current_loss=0.02857 | best_loss=0.02807
Epoch 6/80: current_loss=0.02798 | best_loss=0.02798
Epoch 7/80: current_loss=0.02857 | best_loss=0.02798
Epoch 8/80: current_loss=0.02833 | best_loss=0.02798
Epoch 9/80: current_loss=0.02947 | best_loss=0.02798
Epoch 10/80: current_loss=0.02880 | best_loss=0.02798
Epoch 11/80: current_loss=0.02888 | best_loss=0.02798
Epoch 12/80: current_loss=0.02825 | best_loss=0.02798
Epoch 13/80: current_loss=0.03171 | best_loss=0.02798
Epoch 14/80: current_loss=0.02966 | best_loss=0.02798
Epoch 15/80: current_loss=0.02860 | best_loss=0.02798
Epoch 16/80: current_loss=0.02854 | best_loss=0.02798
Epoch 17/80: current_loss=0.02819 | best_loss=0.02798
Epoch 18/80: current_loss=0.02856 | best_loss=0.02798
Epoch 19/80: current_loss=0.02890 | best_loss=0.02798
Epoch 20/80: current_loss=0.02805 | best_loss=0.02798
Epoch 21/80: current_loss=0.02802 | best_loss=0.02798
Epoch 22/80: current_loss=0.02798 | best_loss=0.02798
Epoch 23/80: current_loss=0.02915 | best_loss=0.02798
Epoch 24/80: current_loss=0.02877 | best_loss=0.02798
Epoch 25/80: current_loss=0.02875 | best_loss=0.02798
Epoch 26/80: current_loss=0.02880 | best_loss=0.02798
Epoch 27/80: current_loss=0.02825 | best_loss=0.02798
Epoch 28/80: current_loss=0.02809 | best_loss=0.02798
Epoch 29/80: current_loss=0.02828 | best_loss=0.02798
Epoch 30/80: current_loss=0.02828 | best_loss=0.02798
Epoch 31/80: current_loss=0.02926 | best_loss=0.02798
Epoch 32/80: current_loss=0.02805 | best_loss=0.02798
Epoch 33/80: current_loss=0.02819 | best_loss=0.02798
Epoch 34/80: current_loss=0.02954 | best_loss=0.02798
Epoch 35/80: current_loss=0.02803 | best_loss=0.02798
Epoch 36/80: current_loss=0.02931 | best_loss=0.02798
Epoch 37/80: current_loss=0.02925 | best_loss=0.02798
Epoch 38/80: current_loss=0.02833 | best_loss=0.02798
Epoch 39/80: current_loss=0.02901 | best_loss=0.02798
Epoch 40/80: current_loss=0.02837 | best_loss=0.02798
Epoch 41/80: current_loss=0.02822 | best_loss=0.02798
Epoch 42/80: current_loss=0.02792 | best_loss=0.02792
Epoch 43/80: current_loss=0.02820 | best_loss=0.02792
Epoch 44/80: current_loss=0.02850 | best_loss=0.02792
Epoch 45/80: current_loss=0.02823 | best_loss=0.02792
Epoch 46/80: current_loss=0.02804 | best_loss=0.02792
Epoch 47/80: current_loss=0.02835 | best_loss=0.02792
Epoch 48/80: current_loss=0.02811 | best_loss=0.02792
Epoch 49/80: current_loss=0.02921 | best_loss=0.02792
Epoch 50/80: current_loss=0.02915 | best_loss=0.02792
Epoch 51/80: current_loss=0.02846 | best_loss=0.02792
Epoch 52/80: current_loss=0.02807 | best_loss=0.02792
Epoch 53/80: current_loss=0.02812 | best_loss=0.02792
Epoch 54/80: current_loss=0.02845 | best_loss=0.02792
Epoch 55/80: current_loss=0.02824 | best_loss=0.02792
Epoch 56/80: current_loss=0.02819 | best_loss=0.02792
Epoch 57/80: current_loss=0.02950 | best_loss=0.02792
Epoch 58/80: current_loss=0.02865 | best_loss=0.02792
Epoch 59/80: current_loss=0.02837 | best_loss=0.02792
Epoch 60/80: current_loss=0.02931 | best_loss=0.02792
Epoch 61/80: current_loss=0.03070 | best_loss=0.02792
Epoch 62/80: current_loss=0.03045 | best_loss=0.02792
Early Stopping at epoch 62
      explained_var=0.00169 | mse_loss=0.02832
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03316 | best_loss=0.03316
Epoch 1/80: current_loss=0.03337 | best_loss=0.03316
Epoch 2/80: current_loss=0.03291 | best_loss=0.03291
Epoch 3/80: current_loss=0.03295 | best_loss=0.03291
Epoch 4/80: current_loss=0.03322 | best_loss=0.03291
Epoch 5/80: current_loss=0.03308 | best_loss=0.03291
Epoch 6/80: current_loss=0.03297 | best_loss=0.03291
Epoch 7/80: current_loss=0.03374 | best_loss=0.03291
Epoch 8/80: current_loss=0.03514 | best_loss=0.03291
Epoch 9/80: current_loss=0.03346 | best_loss=0.03291
Epoch 10/80: current_loss=0.03279 | best_loss=0.03279
Epoch 11/80: current_loss=0.03289 | best_loss=0.03279
Epoch 12/80: current_loss=0.03291 | best_loss=0.03279
Epoch 13/80: current_loss=0.03320 | best_loss=0.03279
Epoch 14/80: current_loss=0.03292 | best_loss=0.03279
Epoch 15/80: current_loss=0.03317 | best_loss=0.03279
Epoch 16/80: current_loss=0.03310 | best_loss=0.03279
Epoch 17/80: current_loss=0.03304 | best_loss=0.03279
Epoch 18/80: current_loss=0.03297 | best_loss=0.03279
Epoch 19/80: current_loss=0.03370 | best_loss=0.03279
Epoch 20/80: current_loss=0.03318 | best_loss=0.03279
Epoch 21/80: current_loss=0.03360 | best_loss=0.03279
Epoch 22/80: current_loss=0.03340 | best_loss=0.03279
Epoch 23/80: current_loss=0.03315 | best_loss=0.03279
Epoch 24/80: current_loss=0.03300 | best_loss=0.03279
Epoch 25/80: current_loss=0.03291 | best_loss=0.03279
Epoch 26/80: current_loss=0.03295 | best_loss=0.03279
Epoch 27/80: current_loss=0.03413 | best_loss=0.03279
Epoch 28/80: current_loss=0.03304 | best_loss=0.03279
Epoch 29/80: current_loss=0.03308 | best_loss=0.03279
Epoch 30/80: current_loss=0.03355 | best_loss=0.03279
Early Stopping at epoch 30
      explained_var=-0.00254 | mse_loss=0.03268
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03540 | best_loss=0.03540
Epoch 1/80: current_loss=0.03545 | best_loss=0.03540
Epoch 2/80: current_loss=0.03542 | best_loss=0.03540
Epoch 3/80: current_loss=0.03644 | best_loss=0.03540
Epoch 4/80: current_loss=0.03593 | best_loss=0.03540
Epoch 5/80: current_loss=0.03538 | best_loss=0.03538
Epoch 6/80: current_loss=0.03576 | best_loss=0.03538
Epoch 7/80: current_loss=0.03554 | best_loss=0.03538
Epoch 8/80: current_loss=0.03543 | best_loss=0.03538
Epoch 9/80: current_loss=0.03574 | best_loss=0.03538
Epoch 10/80: current_loss=0.03566 | best_loss=0.03538
Epoch 11/80: current_loss=0.03544 | best_loss=0.03538
Epoch 12/80: current_loss=0.03555 | best_loss=0.03538
Epoch 13/80: current_loss=0.03553 | best_loss=0.03538
Epoch 14/80: current_loss=0.03551 | best_loss=0.03538
Epoch 15/80: current_loss=0.03558 | best_loss=0.03538
Epoch 16/80: current_loss=0.03568 | best_loss=0.03538
Epoch 17/80: current_loss=0.03559 | best_loss=0.03538
Epoch 18/80: current_loss=0.03608 | best_loss=0.03538
Epoch 19/80: current_loss=0.03628 | best_loss=0.03538
Epoch 20/80: current_loss=0.03558 | best_loss=0.03538
Epoch 21/80: current_loss=0.03558 | best_loss=0.03538
Epoch 22/80: current_loss=0.03534 | best_loss=0.03534
Epoch 23/80: current_loss=0.03549 | best_loss=0.03534
Epoch 24/80: current_loss=0.03552 | best_loss=0.03534
Epoch 25/80: current_loss=0.03555 | best_loss=0.03534
Epoch 26/80: current_loss=0.03560 | best_loss=0.03534
Epoch 27/80: current_loss=0.03542 | best_loss=0.03534
Epoch 28/80: current_loss=0.03543 | best_loss=0.03534
Epoch 29/80: current_loss=0.03733 | best_loss=0.03534
Epoch 30/80: current_loss=0.03554 | best_loss=0.03534
Epoch 31/80: current_loss=0.03619 | best_loss=0.03534
Epoch 32/80: current_loss=0.03560 | best_loss=0.03534
Epoch 33/80: current_loss=0.03530 | best_loss=0.03530
Epoch 34/80: current_loss=0.03548 | best_loss=0.03530
Epoch 35/80: current_loss=0.03535 | best_loss=0.03530
Epoch 36/80: current_loss=0.03533 | best_loss=0.03530
Epoch 37/80: current_loss=0.03539 | best_loss=0.03530
Epoch 38/80: current_loss=0.03599 | best_loss=0.03530
Epoch 39/80: current_loss=0.03559 | best_loss=0.03530
Epoch 40/80: current_loss=0.03542 | best_loss=0.03530
Epoch 41/80: current_loss=0.03531 | best_loss=0.03530
Epoch 42/80: current_loss=0.03539 | best_loss=0.03530
Epoch 43/80: current_loss=0.03558 | best_loss=0.03530
Epoch 44/80: current_loss=0.03589 | best_loss=0.03530
Epoch 45/80: current_loss=0.03573 | best_loss=0.03530
Epoch 46/80: current_loss=0.03554 | best_loss=0.03530
Epoch 47/80: current_loss=0.03553 | best_loss=0.03530
Epoch 48/80: current_loss=0.03544 | best_loss=0.03530
Epoch 49/80: current_loss=0.03544 | best_loss=0.03530
Epoch 50/80: current_loss=0.03547 | best_loss=0.03530
Epoch 51/80: current_loss=0.03595 | best_loss=0.03530
Epoch 52/80: current_loss=0.03553 | best_loss=0.03530
Epoch 53/80: current_loss=0.03548 | best_loss=0.03530
Early Stopping at epoch 53
      explained_var=0.02244 | mse_loss=0.03508
----------------------------------------------
Average early_stopping_point: 22| avg_exp_var=0.01504| avg_loss=0.03489
----------------------------------------------


----------------------------------------------
Params for Trial 35
{'learning_rate': 0.001, 'weight_decay': 0.0010078513890553155, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04185 | best_loss=0.04185
Epoch 1/80: current_loss=0.04020 | best_loss=0.04020
Epoch 2/80: current_loss=0.03991 | best_loss=0.03991
Epoch 3/80: current_loss=0.03955 | best_loss=0.03955
Epoch 4/80: current_loss=0.03938 | best_loss=0.03938
Epoch 5/80: current_loss=0.03870 | best_loss=0.03870
Epoch 6/80: current_loss=0.03885 | best_loss=0.03870
Epoch 7/80: current_loss=0.03811 | best_loss=0.03811
Epoch 8/80: current_loss=0.04166 | best_loss=0.03811
Epoch 9/80: current_loss=0.03802 | best_loss=0.03802
Epoch 10/80: current_loss=0.03790 | best_loss=0.03790
Epoch 11/80: current_loss=0.03778 | best_loss=0.03778
Epoch 12/80: current_loss=0.03792 | best_loss=0.03778
Epoch 13/80: current_loss=0.03795 | best_loss=0.03778
Epoch 14/80: current_loss=0.03796 | best_loss=0.03778
Epoch 15/80: current_loss=0.03767 | best_loss=0.03767
Epoch 16/80: current_loss=0.03780 | best_loss=0.03767
Epoch 17/80: current_loss=0.03783 | best_loss=0.03767
Epoch 18/80: current_loss=0.03778 | best_loss=0.03767
Epoch 19/80: current_loss=0.03810 | best_loss=0.03767
Epoch 20/80: current_loss=0.03797 | best_loss=0.03767
Epoch 21/80: current_loss=0.03837 | best_loss=0.03767
Epoch 22/80: current_loss=0.03874 | best_loss=0.03767
Epoch 23/80: current_loss=0.03857 | best_loss=0.03767
Epoch 24/80: current_loss=0.03792 | best_loss=0.03767
Epoch 25/80: current_loss=0.03767 | best_loss=0.03767
Epoch 26/80: current_loss=0.03783 | best_loss=0.03767
Epoch 27/80: current_loss=0.03809 | best_loss=0.03767
Epoch 28/80: current_loss=0.03813 | best_loss=0.03767
Epoch 29/80: current_loss=0.03817 | best_loss=0.03767
Epoch 30/80: current_loss=0.03754 | best_loss=0.03754
Epoch 31/80: current_loss=0.03738 | best_loss=0.03738
Epoch 32/80: current_loss=0.03745 | best_loss=0.03738
Epoch 33/80: current_loss=0.03755 | best_loss=0.03738
Epoch 34/80: current_loss=0.03821 | best_loss=0.03738
Epoch 35/80: current_loss=0.03817 | best_loss=0.03738
Epoch 36/80: current_loss=0.03788 | best_loss=0.03738
Epoch 37/80: current_loss=0.03756 | best_loss=0.03738
Epoch 38/80: current_loss=0.03762 | best_loss=0.03738
Epoch 39/80: current_loss=0.03801 | best_loss=0.03738
Epoch 40/80: current_loss=0.03830 | best_loss=0.03738
Epoch 41/80: current_loss=0.03761 | best_loss=0.03738
Epoch 42/80: current_loss=0.03748 | best_loss=0.03738
Epoch 43/80: current_loss=0.03799 | best_loss=0.03738
Epoch 44/80: current_loss=0.03782 | best_loss=0.03738
Epoch 45/80: current_loss=0.03777 | best_loss=0.03738
Epoch 46/80: current_loss=0.03761 | best_loss=0.03738
Epoch 47/80: current_loss=0.03778 | best_loss=0.03738
Epoch 48/80: current_loss=0.03792 | best_loss=0.03738
Epoch 49/80: current_loss=0.03749 | best_loss=0.03738
Epoch 50/80: current_loss=0.03877 | best_loss=0.03738
Epoch 51/80: current_loss=0.03778 | best_loss=0.03738
Early Stopping at epoch 51
      explained_var=0.02273 | mse_loss=0.03825
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04429 | best_loss=0.04429
Epoch 1/80: current_loss=0.04148 | best_loss=0.04148
Epoch 2/80: current_loss=0.04150 | best_loss=0.04148
Epoch 3/80: current_loss=0.04220 | best_loss=0.04148
Epoch 4/80: current_loss=0.04145 | best_loss=0.04145
Epoch 5/80: current_loss=0.04162 | best_loss=0.04145
Epoch 6/80: current_loss=0.04224 | best_loss=0.04145
Epoch 7/80: current_loss=0.04190 | best_loss=0.04145
Epoch 8/80: current_loss=0.04162 | best_loss=0.04145
Epoch 9/80: current_loss=0.04227 | best_loss=0.04145
Epoch 10/80: current_loss=0.04169 | best_loss=0.04145
Epoch 11/80: current_loss=0.04180 | best_loss=0.04145
Epoch 12/80: current_loss=0.04155 | best_loss=0.04145
Epoch 13/80: current_loss=0.04169 | best_loss=0.04145
Epoch 14/80: current_loss=0.04161 | best_loss=0.04145
Epoch 15/80: current_loss=0.04166 | best_loss=0.04145
Epoch 16/80: current_loss=0.04227 | best_loss=0.04145
Epoch 17/80: current_loss=0.04161 | best_loss=0.04145
Epoch 18/80: current_loss=0.04140 | best_loss=0.04140
Epoch 19/80: current_loss=0.04172 | best_loss=0.04140
Epoch 20/80: current_loss=0.04151 | best_loss=0.04140
Epoch 21/80: current_loss=0.04212 | best_loss=0.04140
Epoch 22/80: current_loss=0.04144 | best_loss=0.04140
Epoch 23/80: current_loss=0.04158 | best_loss=0.04140
Epoch 24/80: current_loss=0.04165 | best_loss=0.04140
Epoch 25/80: current_loss=0.04171 | best_loss=0.04140
Epoch 26/80: current_loss=0.04219 | best_loss=0.04140
Epoch 27/80: current_loss=0.04163 | best_loss=0.04140
Epoch 28/80: current_loss=0.04176 | best_loss=0.04140
Epoch 29/80: current_loss=0.04186 | best_loss=0.04140
Epoch 30/80: current_loss=0.04214 | best_loss=0.04140
Epoch 31/80: current_loss=0.04241 | best_loss=0.04140
Epoch 32/80: current_loss=0.04176 | best_loss=0.04140
Epoch 33/80: current_loss=0.04163 | best_loss=0.04140
Epoch 34/80: current_loss=0.04160 | best_loss=0.04140
Epoch 35/80: current_loss=0.04186 | best_loss=0.04140
Epoch 36/80: current_loss=0.04168 | best_loss=0.04140
Epoch 37/80: current_loss=0.04205 | best_loss=0.04140
Epoch 38/80: current_loss=0.04167 | best_loss=0.04140
Early Stopping at epoch 38
      explained_var=0.03347 | mse_loss=0.03994
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02839 | best_loss=0.02839
Epoch 1/80: current_loss=0.02812 | best_loss=0.02812
Epoch 2/80: current_loss=0.02816 | best_loss=0.02812
Epoch 3/80: current_loss=0.02813 | best_loss=0.02812
Epoch 4/80: current_loss=0.02804 | best_loss=0.02804
Epoch 5/80: current_loss=0.02815 | best_loss=0.02804
Epoch 6/80: current_loss=0.02859 | best_loss=0.02804
Epoch 7/80: current_loss=0.02822 | best_loss=0.02804
Epoch 8/80: current_loss=0.02910 | best_loss=0.02804
Epoch 9/80: current_loss=0.02911 | best_loss=0.02804
Epoch 10/80: current_loss=0.02881 | best_loss=0.02804
Epoch 11/80: current_loss=0.02801 | best_loss=0.02801
Epoch 12/80: current_loss=0.02848 | best_loss=0.02801
Epoch 13/80: current_loss=0.02833 | best_loss=0.02801
Epoch 14/80: current_loss=0.02887 | best_loss=0.02801
Epoch 15/80: current_loss=0.02894 | best_loss=0.02801
Epoch 16/80: current_loss=0.02802 | best_loss=0.02801
Epoch 17/80: current_loss=0.02801 | best_loss=0.02801
Epoch 18/80: current_loss=0.02839 | best_loss=0.02801
Epoch 19/80: current_loss=0.02809 | best_loss=0.02801
Epoch 20/80: current_loss=0.02818 | best_loss=0.02801
Epoch 21/80: current_loss=0.02852 | best_loss=0.02801
Epoch 22/80: current_loss=0.02988 | best_loss=0.02801
Epoch 23/80: current_loss=0.02827 | best_loss=0.02801
Epoch 24/80: current_loss=0.02926 | best_loss=0.02801
Epoch 25/80: current_loss=0.02881 | best_loss=0.02801
Epoch 26/80: current_loss=0.02910 | best_loss=0.02801
Epoch 27/80: current_loss=0.02837 | best_loss=0.02801
Epoch 28/80: current_loss=0.02799 | best_loss=0.02799
Epoch 29/80: current_loss=0.02837 | best_loss=0.02799
Epoch 30/80: current_loss=0.02845 | best_loss=0.02799
Epoch 31/80: current_loss=0.02913 | best_loss=0.02799
Epoch 32/80: current_loss=0.02854 | best_loss=0.02799
Epoch 33/80: current_loss=0.02802 | best_loss=0.02799
Epoch 34/80: current_loss=0.02850 | best_loss=0.02799
Epoch 35/80: current_loss=0.02808 | best_loss=0.02799
Epoch 36/80: current_loss=0.02843 | best_loss=0.02799
Epoch 37/80: current_loss=0.02910 | best_loss=0.02799
Epoch 38/80: current_loss=0.02806 | best_loss=0.02799
Epoch 39/80: current_loss=0.02910 | best_loss=0.02799
Epoch 40/80: current_loss=0.02904 | best_loss=0.02799
Epoch 41/80: current_loss=0.02803 | best_loss=0.02799
Epoch 42/80: current_loss=0.02803 | best_loss=0.02799
Epoch 43/80: current_loss=0.02838 | best_loss=0.02799
Epoch 44/80: current_loss=0.02897 | best_loss=0.02799
Epoch 45/80: current_loss=0.02902 | best_loss=0.02799
Epoch 46/80: current_loss=0.02843 | best_loss=0.02799
Epoch 47/80: current_loss=0.02815 | best_loss=0.02799
Epoch 48/80: current_loss=0.02887 | best_loss=0.02799
Early Stopping at epoch 48
      explained_var=-0.00371 | mse_loss=0.02847

----------------------------------------------
Params for Trial 36
{'learning_rate': 0.001, 'weight_decay': 0.0019235892424597772, 'n_layers': 1, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04313 | best_loss=0.04313
Epoch 1/80: current_loss=0.04008 | best_loss=0.04008
Epoch 2/80: current_loss=0.03920 | best_loss=0.03920
Epoch 3/80: current_loss=0.03875 | best_loss=0.03875
Epoch 4/80: current_loss=0.03823 | best_loss=0.03823
Epoch 5/80: current_loss=0.03834 | best_loss=0.03823
Epoch 6/80: current_loss=0.03832 | best_loss=0.03823
Epoch 7/80: current_loss=0.03823 | best_loss=0.03823
Epoch 8/80: current_loss=0.03778 | best_loss=0.03778
Epoch 9/80: current_loss=0.03766 | best_loss=0.03766
Epoch 10/80: current_loss=0.03754 | best_loss=0.03754
Epoch 11/80: current_loss=0.03893 | best_loss=0.03754
Epoch 12/80: current_loss=0.03763 | best_loss=0.03754
Epoch 13/80: current_loss=0.03749 | best_loss=0.03749
Epoch 14/80: current_loss=0.03821 | best_loss=0.03749
Epoch 15/80: current_loss=0.03818 | best_loss=0.03749
Epoch 16/80: current_loss=0.03746 | best_loss=0.03746
Epoch 17/80: current_loss=0.03800 | best_loss=0.03746
Epoch 18/80: current_loss=0.03740 | best_loss=0.03740
Epoch 19/80: current_loss=0.03809 | best_loss=0.03740
Epoch 20/80: current_loss=0.03756 | best_loss=0.03740
Epoch 21/80: current_loss=0.03767 | best_loss=0.03740
Epoch 22/80: current_loss=0.03797 | best_loss=0.03740
Epoch 23/80: current_loss=0.03756 | best_loss=0.03740
Epoch 24/80: current_loss=0.03868 | best_loss=0.03740
Epoch 25/80: current_loss=0.03745 | best_loss=0.03740
Epoch 26/80: current_loss=0.03792 | best_loss=0.03740
Epoch 27/80: current_loss=0.03820 | best_loss=0.03740
Epoch 28/80: current_loss=0.03775 | best_loss=0.03740
Epoch 29/80: current_loss=0.03743 | best_loss=0.03740
Epoch 30/80: current_loss=0.03772 | best_loss=0.03740
Epoch 31/80: current_loss=0.03742 | best_loss=0.03740
Epoch 32/80: current_loss=0.03756 | best_loss=0.03740
Epoch 33/80: current_loss=0.03753 | best_loss=0.03740
Epoch 34/80: current_loss=0.03758 | best_loss=0.03740
Epoch 35/80: current_loss=0.03759 | best_loss=0.03740
Epoch 36/80: current_loss=0.03793 | best_loss=0.03740
Epoch 37/80: current_loss=0.03757 | best_loss=0.03740
Epoch 38/80: current_loss=0.03865 | best_loss=0.03740
Early Stopping at epoch 38
      explained_var=0.02476 | mse_loss=0.03835
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04150 | best_loss=0.04150
Epoch 1/80: current_loss=0.04225 | best_loss=0.04150
Epoch 2/80: current_loss=0.04138 | best_loss=0.04138
Epoch 3/80: current_loss=0.04144 | best_loss=0.04138
Epoch 4/80: current_loss=0.04181 | best_loss=0.04138
Epoch 5/80: current_loss=0.04141 | best_loss=0.04138
Epoch 6/80: current_loss=0.04154 | best_loss=0.04138
Epoch 7/80: current_loss=0.04123 | best_loss=0.04123
Epoch 8/80: current_loss=0.04472 | best_loss=0.04123
Epoch 9/80: current_loss=0.04181 | best_loss=0.04123
Epoch 10/80: current_loss=0.04366 | best_loss=0.04123
Epoch 11/80: current_loss=0.04141 | best_loss=0.04123
Epoch 12/80: current_loss=0.04134 | best_loss=0.04123
Epoch 13/80: current_loss=0.04130 | best_loss=0.04123
Epoch 14/80: current_loss=0.04147 | best_loss=0.04123
Epoch 15/80: current_loss=0.04198 | best_loss=0.04123
Epoch 16/80: current_loss=0.04355 | best_loss=0.04123
Epoch 17/80: current_loss=0.04156 | best_loss=0.04123
Epoch 18/80: current_loss=0.04142 | best_loss=0.04123
Epoch 19/80: current_loss=0.04264 | best_loss=0.04123
Epoch 20/80: current_loss=0.04185 | best_loss=0.04123
Epoch 21/80: current_loss=0.04363 | best_loss=0.04123
Epoch 22/80: current_loss=0.04158 | best_loss=0.04123
Epoch 23/80: current_loss=0.04242 | best_loss=0.04123
Epoch 24/80: current_loss=0.04242 | best_loss=0.04123
Epoch 25/80: current_loss=0.04147 | best_loss=0.04123
Epoch 26/80: current_loss=0.04136 | best_loss=0.04123
Epoch 27/80: current_loss=0.04127 | best_loss=0.04123
Early Stopping at epoch 27
      explained_var=0.03626 | mse_loss=0.03975
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02811 | best_loss=0.02811
Epoch 1/80: current_loss=0.02856 | best_loss=0.02811
Epoch 2/80: current_loss=0.02855 | best_loss=0.02811
Epoch 3/80: current_loss=0.02962 | best_loss=0.02811
Epoch 4/80: current_loss=0.02841 | best_loss=0.02811
Epoch 5/80: current_loss=0.02828 | best_loss=0.02811
Epoch 6/80: current_loss=0.03019 | best_loss=0.02811
Epoch 7/80: current_loss=0.02858 | best_loss=0.02811
Epoch 8/80: current_loss=0.02933 | best_loss=0.02811
Epoch 9/80: current_loss=0.02874 | best_loss=0.02811
Epoch 10/80: current_loss=0.02887 | best_loss=0.02811
Epoch 11/80: current_loss=0.02827 | best_loss=0.02811
Epoch 12/80: current_loss=0.02839 | best_loss=0.02811
Epoch 13/80: current_loss=0.02873 | best_loss=0.02811
Epoch 14/80: current_loss=0.02811 | best_loss=0.02811
Epoch 15/80: current_loss=0.02868 | best_loss=0.02811
Epoch 16/80: current_loss=0.02835 | best_loss=0.02811
Epoch 17/80: current_loss=0.02818 | best_loss=0.02811
Epoch 18/80: current_loss=0.02813 | best_loss=0.02811
Epoch 19/80: current_loss=0.02812 | best_loss=0.02811
Epoch 20/80: current_loss=0.02905 | best_loss=0.02811
Epoch 21/80: current_loss=0.03025 | best_loss=0.02811
Epoch 22/80: current_loss=0.02808 | best_loss=0.02808
Epoch 23/80: current_loss=0.02860 | best_loss=0.02808
Epoch 24/80: current_loss=0.02875 | best_loss=0.02808
Epoch 25/80: current_loss=0.02826 | best_loss=0.02808
Epoch 26/80: current_loss=0.02829 | best_loss=0.02808
Epoch 27/80: current_loss=0.02809 | best_loss=0.02808
Epoch 28/80: current_loss=0.02850 | best_loss=0.02808
Epoch 29/80: current_loss=0.02820 | best_loss=0.02808
Epoch 30/80: current_loss=0.02835 | best_loss=0.02808
Epoch 31/80: current_loss=0.02861 | best_loss=0.02808
Epoch 32/80: current_loss=0.02818 | best_loss=0.02808
Epoch 33/80: current_loss=0.02983 | best_loss=0.02808
Epoch 34/80: current_loss=0.02903 | best_loss=0.02808
Epoch 35/80: current_loss=0.02943 | best_loss=0.02808
Epoch 36/80: current_loss=0.02899 | best_loss=0.02808
Epoch 37/80: current_loss=0.02909 | best_loss=0.02808
Epoch 38/80: current_loss=0.02828 | best_loss=0.02808
Epoch 39/80: current_loss=0.02813 | best_loss=0.02808
Epoch 40/80: current_loss=0.02853 | best_loss=0.02808
Epoch 41/80: current_loss=0.02902 | best_loss=0.02808
Epoch 42/80: current_loss=0.02824 | best_loss=0.02808
Early Stopping at epoch 42
      explained_var=-0.00606 | mse_loss=0.02853

----------------------------------------------
Params for Trial 37
{'learning_rate': 0.001, 'weight_decay': 0.0013148507628590455, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04382 | best_loss=0.04382
Epoch 1/80: current_loss=0.04253 | best_loss=0.04253
Epoch 2/80: current_loss=0.03851 | best_loss=0.03851
Epoch 3/80: current_loss=0.04078 | best_loss=0.03851
Epoch 4/80: current_loss=0.03774 | best_loss=0.03774
Epoch 5/80: current_loss=0.03834 | best_loss=0.03774
Epoch 6/80: current_loss=0.03820 | best_loss=0.03774
Epoch 7/80: current_loss=0.04344 | best_loss=0.03774
Epoch 8/80: current_loss=0.03775 | best_loss=0.03774
Epoch 9/80: current_loss=0.03773 | best_loss=0.03773
Epoch 10/80: current_loss=0.03766 | best_loss=0.03766
Epoch 11/80: current_loss=0.03882 | best_loss=0.03766
Epoch 12/80: current_loss=0.03944 | best_loss=0.03766
Epoch 13/80: current_loss=0.03938 | best_loss=0.03766
Epoch 14/80: current_loss=0.04011 | best_loss=0.03766
Epoch 15/80: current_loss=0.03765 | best_loss=0.03765
Epoch 16/80: current_loss=0.03799 | best_loss=0.03765
Epoch 17/80: current_loss=0.03832 | best_loss=0.03765
Epoch 18/80: current_loss=0.03813 | best_loss=0.03765
Epoch 19/80: current_loss=0.03951 | best_loss=0.03765
Epoch 20/80: current_loss=0.03764 | best_loss=0.03764
Epoch 21/80: current_loss=0.03867 | best_loss=0.03764
Epoch 22/80: current_loss=0.03887 | best_loss=0.03764
Epoch 23/80: current_loss=0.03807 | best_loss=0.03764
Epoch 24/80: current_loss=0.03772 | best_loss=0.03764
Epoch 25/80: current_loss=0.03903 | best_loss=0.03764
Epoch 26/80: current_loss=0.03814 | best_loss=0.03764
Epoch 27/80: current_loss=0.03774 | best_loss=0.03764
Epoch 28/80: current_loss=0.03774 | best_loss=0.03764
Epoch 29/80: current_loss=0.03956 | best_loss=0.03764
Epoch 30/80: current_loss=0.03921 | best_loss=0.03764
Epoch 31/80: current_loss=0.03941 | best_loss=0.03764
Epoch 32/80: current_loss=0.03769 | best_loss=0.03764
Epoch 33/80: current_loss=0.03747 | best_loss=0.03747
Epoch 34/80: current_loss=0.03835 | best_loss=0.03747
Epoch 35/80: current_loss=0.03834 | best_loss=0.03747
Epoch 36/80: current_loss=0.03779 | best_loss=0.03747
Epoch 37/80: current_loss=0.04076 | best_loss=0.03747
Epoch 38/80: current_loss=0.03761 | best_loss=0.03747
Epoch 39/80: current_loss=0.03752 | best_loss=0.03747
Epoch 40/80: current_loss=0.03759 | best_loss=0.03747
Epoch 41/80: current_loss=0.03755 | best_loss=0.03747
Epoch 42/80: current_loss=0.03793 | best_loss=0.03747
Epoch 43/80: current_loss=0.03831 | best_loss=0.03747
Epoch 44/80: current_loss=0.03763 | best_loss=0.03747
Epoch 45/80: current_loss=0.03842 | best_loss=0.03747
Epoch 46/80: current_loss=0.04057 | best_loss=0.03747
Epoch 47/80: current_loss=0.04425 | best_loss=0.03747
Epoch 48/80: current_loss=0.03886 | best_loss=0.03747
Epoch 49/80: current_loss=0.03758 | best_loss=0.03747
Epoch 50/80: current_loss=0.03852 | best_loss=0.03747
Epoch 51/80: current_loss=0.03801 | best_loss=0.03747
Epoch 52/80: current_loss=0.03865 | best_loss=0.03747
Epoch 53/80: current_loss=0.03774 | best_loss=0.03747
Early Stopping at epoch 53
      explained_var=0.02166 | mse_loss=0.03832
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04229 | best_loss=0.04229
Epoch 1/80: current_loss=0.04375 | best_loss=0.04229
Epoch 2/80: current_loss=0.04187 | best_loss=0.04187
Epoch 3/80: current_loss=0.04221 | best_loss=0.04187
Epoch 4/80: current_loss=0.04210 | best_loss=0.04187
Epoch 5/80: current_loss=0.04323 | best_loss=0.04187
Epoch 6/80: current_loss=0.04226 | best_loss=0.04187
Epoch 7/80: current_loss=0.04188 | best_loss=0.04187
Epoch 8/80: current_loss=0.04242 | best_loss=0.04187
Epoch 9/80: current_loss=0.04196 | best_loss=0.04187
Epoch 10/80: current_loss=0.04477 | best_loss=0.04187
Epoch 11/80: current_loss=0.04344 | best_loss=0.04187
Epoch 12/80: current_loss=0.04207 | best_loss=0.04187
Epoch 13/80: current_loss=0.04242 | best_loss=0.04187
Epoch 14/80: current_loss=0.04337 | best_loss=0.04187
Epoch 15/80: current_loss=0.04581 | best_loss=0.04187
Epoch 16/80: current_loss=0.04379 | best_loss=0.04187
Epoch 17/80: current_loss=0.04167 | best_loss=0.04167
Epoch 18/80: current_loss=0.04180 | best_loss=0.04167
Epoch 19/80: current_loss=0.04247 | best_loss=0.04167
Epoch 20/80: current_loss=0.04230 | best_loss=0.04167
Epoch 21/80: current_loss=0.04273 | best_loss=0.04167
Epoch 22/80: current_loss=0.04616 | best_loss=0.04167
Epoch 23/80: current_loss=0.04181 | best_loss=0.04167
Epoch 24/80: current_loss=0.04226 | best_loss=0.04167
Epoch 25/80: current_loss=0.04309 | best_loss=0.04167
Epoch 26/80: current_loss=0.04171 | best_loss=0.04167
Epoch 27/80: current_loss=0.04370 | best_loss=0.04167
Epoch 28/80: current_loss=0.04245 | best_loss=0.04167
Epoch 29/80: current_loss=0.04363 | best_loss=0.04167
Epoch 30/80: current_loss=0.04373 | best_loss=0.04167
Epoch 31/80: current_loss=0.04200 | best_loss=0.04167
Epoch 32/80: current_loss=0.04159 | best_loss=0.04159
Epoch 33/80: current_loss=0.04190 | best_loss=0.04159
Epoch 34/80: current_loss=0.04186 | best_loss=0.04159
Epoch 35/80: current_loss=0.04203 | best_loss=0.04159
Epoch 36/80: current_loss=0.04199 | best_loss=0.04159
Epoch 37/80: current_loss=0.04196 | best_loss=0.04159
Epoch 38/80: current_loss=0.04170 | best_loss=0.04159
Epoch 39/80: current_loss=0.04196 | best_loss=0.04159
Epoch 40/80: current_loss=0.04196 | best_loss=0.04159
Epoch 41/80: current_loss=0.04213 | best_loss=0.04159
Epoch 42/80: current_loss=0.04417 | best_loss=0.04159
Epoch 43/80: current_loss=0.04181 | best_loss=0.04159
Epoch 44/80: current_loss=0.04184 | best_loss=0.04159
Epoch 45/80: current_loss=0.04240 | best_loss=0.04159
Epoch 46/80: current_loss=0.04161 | best_loss=0.04159
Epoch 47/80: current_loss=0.04210 | best_loss=0.04159
Epoch 48/80: current_loss=0.04174 | best_loss=0.04159
Epoch 49/80: current_loss=0.04181 | best_loss=0.04159
Epoch 50/80: current_loss=0.04279 | best_loss=0.04159
Epoch 51/80: current_loss=0.04161 | best_loss=0.04159
Epoch 52/80: current_loss=0.04220 | best_loss=0.04159
Early Stopping at epoch 52
      explained_var=0.02764 | mse_loss=0.04015
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02857 | best_loss=0.02857
Epoch 1/80: current_loss=0.02790 | best_loss=0.02790
Epoch 2/80: current_loss=0.03162 | best_loss=0.02790
Epoch 3/80: current_loss=0.02871 | best_loss=0.02790
Epoch 4/80: current_loss=0.02945 | best_loss=0.02790
Epoch 5/80: current_loss=0.02820 | best_loss=0.02790
Epoch 6/80: current_loss=0.02811 | best_loss=0.02790
Epoch 7/80: current_loss=0.02799 | best_loss=0.02790
Epoch 8/80: current_loss=0.02798 | best_loss=0.02790
Epoch 9/80: current_loss=0.02848 | best_loss=0.02790
Epoch 10/80: current_loss=0.02801 | best_loss=0.02790
Epoch 11/80: current_loss=0.02840 | best_loss=0.02790
Epoch 12/80: current_loss=0.02909 | best_loss=0.02790
Epoch 13/80: current_loss=0.02997 | best_loss=0.02790
Epoch 14/80: current_loss=0.02781 | best_loss=0.02781
Epoch 15/80: current_loss=0.02841 | best_loss=0.02781
Epoch 16/80: current_loss=0.02812 | best_loss=0.02781
Epoch 17/80: current_loss=0.02806 | best_loss=0.02781
Epoch 18/80: current_loss=0.02793 | best_loss=0.02781
Epoch 19/80: current_loss=0.02816 | best_loss=0.02781
Epoch 20/80: current_loss=0.02807 | best_loss=0.02781
Epoch 21/80: current_loss=0.02815 | best_loss=0.02781
Epoch 22/80: current_loss=0.02797 | best_loss=0.02781
Epoch 23/80: current_loss=0.02797 | best_loss=0.02781
Epoch 24/80: current_loss=0.02818 | best_loss=0.02781
Epoch 25/80: current_loss=0.02872 | best_loss=0.02781
Epoch 26/80: current_loss=0.02887 | best_loss=0.02781
Epoch 27/80: current_loss=0.02945 | best_loss=0.02781
Epoch 28/80: current_loss=0.02802 | best_loss=0.02781
Epoch 29/80: current_loss=0.02849 | best_loss=0.02781
Epoch 30/80: current_loss=0.02879 | best_loss=0.02781
Epoch 31/80: current_loss=0.02830 | best_loss=0.02781
Epoch 32/80: current_loss=0.02836 | best_loss=0.02781
Epoch 33/80: current_loss=0.03066 | best_loss=0.02781
Epoch 34/80: current_loss=0.03177 | best_loss=0.02781
Early Stopping at epoch 34
      explained_var=0.00323 | mse_loss=0.02827
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03468 | best_loss=0.03468
Epoch 1/80: current_loss=0.03295 | best_loss=0.03295
Epoch 2/80: current_loss=0.03349 | best_loss=0.03295
Epoch 3/80: current_loss=0.03319 | best_loss=0.03295
Epoch 4/80: current_loss=0.03360 | best_loss=0.03295
Epoch 5/80: current_loss=0.03284 | best_loss=0.03284
Epoch 6/80: current_loss=0.03284 | best_loss=0.03284
Epoch 7/80: current_loss=0.03295 | best_loss=0.03284
Epoch 8/80: current_loss=0.03288 | best_loss=0.03284
Epoch 9/80: current_loss=0.03293 | best_loss=0.03284
Epoch 10/80: current_loss=0.03280 | best_loss=0.03280
Epoch 11/80: current_loss=0.03400 | best_loss=0.03280
Epoch 12/80: current_loss=0.03304 | best_loss=0.03280
Epoch 13/80: current_loss=0.03320 | best_loss=0.03280
Epoch 14/80: current_loss=0.03281 | best_loss=0.03280
Epoch 15/80: current_loss=0.03531 | best_loss=0.03280
Epoch 16/80: current_loss=0.03303 | best_loss=0.03280
Epoch 17/80: current_loss=0.03320 | best_loss=0.03280
Epoch 18/80: current_loss=0.03298 | best_loss=0.03280
Epoch 19/80: current_loss=0.03323 | best_loss=0.03280
Epoch 20/80: current_loss=0.03306 | best_loss=0.03280
Epoch 21/80: current_loss=0.03359 | best_loss=0.03280
Epoch 22/80: current_loss=0.03304 | best_loss=0.03280
Epoch 23/80: current_loss=0.03287 | best_loss=0.03280
Epoch 24/80: current_loss=0.03285 | best_loss=0.03280
Epoch 25/80: current_loss=0.03310 | best_loss=0.03280
Epoch 26/80: current_loss=0.03566 | best_loss=0.03280
Epoch 27/80: current_loss=0.03287 | best_loss=0.03280
Epoch 28/80: current_loss=0.03307 | best_loss=0.03280
Epoch 29/80: current_loss=0.03288 | best_loss=0.03280
Epoch 30/80: current_loss=0.03290 | best_loss=0.03280
Early Stopping at epoch 30
      explained_var=-0.00181 | mse_loss=0.03268
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03551 | best_loss=0.03551
Epoch 1/80: current_loss=0.03579 | best_loss=0.03551
Epoch 2/80: current_loss=0.03693 | best_loss=0.03551
Epoch 3/80: current_loss=0.03556 | best_loss=0.03551
Epoch 4/80: current_loss=0.03569 | best_loss=0.03551
Epoch 5/80: current_loss=0.03660 | best_loss=0.03551
Epoch 6/80: current_loss=0.03568 | best_loss=0.03551
Epoch 7/80: current_loss=0.03563 | best_loss=0.03551
Epoch 8/80: current_loss=0.03564 | best_loss=0.03551
Epoch 9/80: current_loss=0.03558 | best_loss=0.03551
Epoch 10/80: current_loss=0.03550 | best_loss=0.03550
Epoch 11/80: current_loss=0.03541 | best_loss=0.03541
Epoch 12/80: current_loss=0.03558 | best_loss=0.03541
Epoch 13/80: current_loss=0.03563 | best_loss=0.03541
Epoch 14/80: current_loss=0.03570 | best_loss=0.03541
Epoch 15/80: current_loss=0.03598 | best_loss=0.03541
Epoch 16/80: current_loss=0.03566 | best_loss=0.03541
Epoch 17/80: current_loss=0.03548 | best_loss=0.03541
Epoch 18/80: current_loss=0.03551 | best_loss=0.03541
Epoch 19/80: current_loss=0.03582 | best_loss=0.03541
Epoch 20/80: current_loss=0.03556 | best_loss=0.03541
Epoch 21/80: current_loss=0.03733 | best_loss=0.03541
Epoch 22/80: current_loss=0.03550 | best_loss=0.03541
Epoch 23/80: current_loss=0.03572 | best_loss=0.03541
Epoch 24/80: current_loss=0.28165 | best_loss=0.03541
Epoch 25/80: current_loss=0.06888 | best_loss=0.03541
Epoch 26/80: current_loss=0.04213 | best_loss=0.03541
Epoch 27/80: current_loss=0.04320 | best_loss=0.03541
Epoch 28/80: current_loss=0.05537 | best_loss=0.03541
Epoch 29/80: current_loss=0.03684 | best_loss=0.03541
Epoch 30/80: current_loss=0.03731 | best_loss=0.03541
Epoch 31/80: current_loss=0.03789 | best_loss=0.03541
Early Stopping at epoch 31
      explained_var=0.01886 | mse_loss=0.03521
----------------------------------------------
Average early_stopping_point: 20| avg_exp_var=0.01392| avg_loss=0.03493
----------------------------------------------


----------------------------------------------
Params for Trial 38
{'learning_rate': 0.001, 'weight_decay': 0.0005587848003536554, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04049 | best_loss=0.04049
Epoch 1/80: current_loss=0.03845 | best_loss=0.03845
Epoch 2/80: current_loss=0.04052 | best_loss=0.03845
Epoch 3/80: current_loss=0.03889 | best_loss=0.03845
Epoch 4/80: current_loss=0.03850 | best_loss=0.03845
Epoch 5/80: current_loss=0.03768 | best_loss=0.03768
Epoch 6/80: current_loss=0.03942 | best_loss=0.03768
Epoch 7/80: current_loss=0.03895 | best_loss=0.03768
Epoch 8/80: current_loss=0.03907 | best_loss=0.03768
Epoch 9/80: current_loss=0.03758 | best_loss=0.03758
Epoch 10/80: current_loss=0.03870 | best_loss=0.03758
Epoch 11/80: current_loss=0.04331 | best_loss=0.03758
Epoch 12/80: current_loss=0.03817 | best_loss=0.03758
Epoch 13/80: current_loss=0.03848 | best_loss=0.03758
Epoch 14/80: current_loss=0.03797 | best_loss=0.03758
Epoch 15/80: current_loss=0.03875 | best_loss=0.03758
Epoch 16/80: current_loss=0.03809 | best_loss=0.03758
Epoch 17/80: current_loss=0.03751 | best_loss=0.03751
Epoch 18/80: current_loss=0.03774 | best_loss=0.03751
Epoch 19/80: current_loss=0.03804 | best_loss=0.03751
Epoch 20/80: current_loss=0.03784 | best_loss=0.03751
Epoch 21/80: current_loss=0.03788 | best_loss=0.03751
Epoch 22/80: current_loss=0.03747 | best_loss=0.03747
Epoch 23/80: current_loss=0.03772 | best_loss=0.03747
Epoch 24/80: current_loss=0.03755 | best_loss=0.03747
Epoch 25/80: current_loss=0.03804 | best_loss=0.03747
Epoch 26/80: current_loss=0.03825 | best_loss=0.03747
Epoch 27/80: current_loss=0.03748 | best_loss=0.03747
Epoch 28/80: current_loss=0.03730 | best_loss=0.03730
Epoch 29/80: current_loss=0.03729 | best_loss=0.03729
Epoch 30/80: current_loss=0.03754 | best_loss=0.03729
Epoch 31/80: current_loss=0.03743 | best_loss=0.03729
Epoch 32/80: current_loss=0.03812 | best_loss=0.03729
Epoch 33/80: current_loss=0.03757 | best_loss=0.03729
Epoch 34/80: current_loss=0.03756 | best_loss=0.03729
Epoch 35/80: current_loss=0.03770 | best_loss=0.03729
Epoch 36/80: current_loss=0.03738 | best_loss=0.03729
Epoch 37/80: current_loss=0.03752 | best_loss=0.03729
Epoch 38/80: current_loss=0.03743 | best_loss=0.03729
Epoch 39/80: current_loss=0.03765 | best_loss=0.03729
Epoch 40/80: current_loss=0.03746 | best_loss=0.03729
Epoch 41/80: current_loss=0.03810 | best_loss=0.03729
Epoch 42/80: current_loss=0.03750 | best_loss=0.03729
Epoch 43/80: current_loss=0.03742 | best_loss=0.03729
Epoch 44/80: current_loss=0.03754 | best_loss=0.03729
Epoch 45/80: current_loss=0.03756 | best_loss=0.03729
Epoch 46/80: current_loss=0.03790 | best_loss=0.03729
Epoch 47/80: current_loss=0.03756 | best_loss=0.03729
Epoch 48/80: current_loss=0.03756 | best_loss=0.03729
Epoch 49/80: current_loss=0.03777 | best_loss=0.03729
Early Stopping at epoch 49
      explained_var=0.02410 | mse_loss=0.03820
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04472 | best_loss=0.04472
Epoch 1/80: current_loss=0.04274 | best_loss=0.04274
Epoch 2/80: current_loss=0.04373 | best_loss=0.04274
Epoch 3/80: current_loss=0.04168 | best_loss=0.04168
Epoch 4/80: current_loss=0.04321 | best_loss=0.04168
Epoch 5/80: current_loss=0.04222 | best_loss=0.04168
Epoch 6/80: current_loss=0.04224 | best_loss=0.04168
Epoch 7/80: current_loss=0.04168 | best_loss=0.04168
Epoch 8/80: current_loss=0.04192 | best_loss=0.04168
Epoch 9/80: current_loss=0.04166 | best_loss=0.04166
Epoch 10/80: current_loss=0.04173 | best_loss=0.04166
Epoch 11/80: current_loss=0.04163 | best_loss=0.04163
Epoch 12/80: current_loss=0.04160 | best_loss=0.04160
Epoch 13/80: current_loss=0.04177 | best_loss=0.04160
Epoch 14/80: current_loss=0.04200 | best_loss=0.04160
Epoch 15/80: current_loss=0.04193 | best_loss=0.04160
Epoch 16/80: current_loss=0.04177 | best_loss=0.04160
Epoch 17/80: current_loss=0.04182 | best_loss=0.04160
Epoch 18/80: current_loss=0.04167 | best_loss=0.04160
Epoch 19/80: current_loss=0.04188 | best_loss=0.04160
Epoch 20/80: current_loss=0.04190 | best_loss=0.04160
Epoch 21/80: current_loss=0.04179 | best_loss=0.04160
Epoch 22/80: current_loss=0.04167 | best_loss=0.04160
Epoch 23/80: current_loss=0.04181 | best_loss=0.04160
Epoch 24/80: current_loss=0.04170 | best_loss=0.04160
Epoch 25/80: current_loss=0.04165 | best_loss=0.04160
Epoch 26/80: current_loss=0.04390 | best_loss=0.04160
Epoch 27/80: current_loss=0.04275 | best_loss=0.04160
Epoch 28/80: current_loss=0.04165 | best_loss=0.04160
Epoch 29/80: current_loss=0.04156 | best_loss=0.04156
Epoch 30/80: current_loss=0.04263 | best_loss=0.04156
Epoch 31/80: current_loss=0.04224 | best_loss=0.04156
Epoch 32/80: current_loss=0.04206 | best_loss=0.04156
Epoch 33/80: current_loss=0.04181 | best_loss=0.04156
Epoch 34/80: current_loss=0.04179 | best_loss=0.04156
Epoch 35/80: current_loss=0.04176 | best_loss=0.04156
Epoch 36/80: current_loss=0.04161 | best_loss=0.04156
Epoch 37/80: current_loss=0.04208 | best_loss=0.04156
Epoch 38/80: current_loss=0.04180 | best_loss=0.04156
Epoch 39/80: current_loss=0.04128 | best_loss=0.04128
Epoch 40/80: current_loss=0.04137 | best_loss=0.04128
Epoch 41/80: current_loss=0.04157 | best_loss=0.04128
Epoch 42/80: current_loss=0.04150 | best_loss=0.04128
Epoch 43/80: current_loss=0.04147 | best_loss=0.04128
Epoch 44/80: current_loss=0.04159 | best_loss=0.04128
Epoch 45/80: current_loss=0.04149 | best_loss=0.04128
Epoch 46/80: current_loss=0.04185 | best_loss=0.04128
Epoch 47/80: current_loss=0.04144 | best_loss=0.04128
Epoch 48/80: current_loss=0.04129 | best_loss=0.04128
Epoch 49/80: current_loss=0.04174 | best_loss=0.04128
Epoch 50/80: current_loss=0.04148 | best_loss=0.04128
Epoch 51/80: current_loss=0.04152 | best_loss=0.04128
Epoch 52/80: current_loss=0.04354 | best_loss=0.04128
Epoch 53/80: current_loss=0.04341 | best_loss=0.04128
Epoch 54/80: current_loss=0.04242 | best_loss=0.04128
Epoch 55/80: current_loss=0.04163 | best_loss=0.04128
Epoch 56/80: current_loss=0.04175 | best_loss=0.04128
Epoch 57/80: current_loss=0.04199 | best_loss=0.04128
Epoch 58/80: current_loss=0.04182 | best_loss=0.04128
Epoch 59/80: current_loss=0.04200 | best_loss=0.04128
Early Stopping at epoch 59
      explained_var=0.03586 | mse_loss=0.03981
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02838 | best_loss=0.02838
Epoch 1/80: current_loss=0.02810 | best_loss=0.02810
Epoch 2/80: current_loss=0.02831 | best_loss=0.02810
Epoch 3/80: current_loss=0.02859 | best_loss=0.02810
Epoch 4/80: current_loss=0.02806 | best_loss=0.02806
Epoch 5/80: current_loss=0.02832 | best_loss=0.02806
Epoch 6/80: current_loss=0.02819 | best_loss=0.02806
Epoch 7/80: current_loss=0.02812 | best_loss=0.02806
Epoch 8/80: current_loss=0.02935 | best_loss=0.02806
Epoch 9/80: current_loss=0.02805 | best_loss=0.02805
Epoch 10/80: current_loss=0.02867 | best_loss=0.02805
Epoch 11/80: current_loss=0.02833 | best_loss=0.02805
Epoch 12/80: current_loss=0.02813 | best_loss=0.02805
Epoch 13/80: current_loss=0.02813 | best_loss=0.02805
Epoch 14/80: current_loss=0.02885 | best_loss=0.02805
Epoch 15/80: current_loss=0.02809 | best_loss=0.02805
Epoch 16/80: current_loss=0.02808 | best_loss=0.02805
Epoch 17/80: current_loss=0.02963 | best_loss=0.02805
Epoch 18/80: current_loss=0.02824 | best_loss=0.02805
Epoch 19/80: current_loss=0.02861 | best_loss=0.02805
Epoch 20/80: current_loss=0.02813 | best_loss=0.02805
Epoch 21/80: current_loss=0.02841 | best_loss=0.02805
Epoch 22/80: current_loss=0.02862 | best_loss=0.02805
Epoch 23/80: current_loss=0.02843 | best_loss=0.02805
Epoch 24/80: current_loss=0.02800 | best_loss=0.02800
Epoch 25/80: current_loss=0.02905 | best_loss=0.02800
Epoch 26/80: current_loss=0.02802 | best_loss=0.02800
Epoch 27/80: current_loss=0.02827 | best_loss=0.02800
Epoch 28/80: current_loss=0.02859 | best_loss=0.02800
Epoch 29/80: current_loss=0.02806 | best_loss=0.02800
Epoch 30/80: current_loss=0.02885 | best_loss=0.02800
Epoch 31/80: current_loss=0.02831 | best_loss=0.02800
Epoch 32/80: current_loss=0.02834 | best_loss=0.02800
Epoch 33/80: current_loss=0.02872 | best_loss=0.02800
Epoch 34/80: current_loss=0.02803 | best_loss=0.02800
Epoch 35/80: current_loss=0.02896 | best_loss=0.02800
Epoch 36/80: current_loss=0.02808 | best_loss=0.02800
Epoch 37/80: current_loss=0.02847 | best_loss=0.02800
Epoch 38/80: current_loss=0.02948 | best_loss=0.02800
Epoch 39/80: current_loss=0.02836 | best_loss=0.02800
Epoch 40/80: current_loss=0.02925 | best_loss=0.02800
Epoch 41/80: current_loss=0.02883 | best_loss=0.02800
Epoch 42/80: current_loss=0.02899 | best_loss=0.02800
Epoch 43/80: current_loss=0.02932 | best_loss=0.02800
Epoch 44/80: current_loss=0.02823 | best_loss=0.02800
Early Stopping at epoch 44
      explained_var=-0.00201 | mse_loss=0.02843
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03309 | best_loss=0.03309
Epoch 1/80: current_loss=0.03291 | best_loss=0.03291
Epoch 2/80: current_loss=0.03385 | best_loss=0.03291
Epoch 3/80: current_loss=0.03310 | best_loss=0.03291
Epoch 4/80: current_loss=0.03294 | best_loss=0.03291
Epoch 5/80: current_loss=0.03288 | best_loss=0.03288
Epoch 6/80: current_loss=0.03302 | best_loss=0.03288
Epoch 7/80: current_loss=0.03313 | best_loss=0.03288
Epoch 8/80: current_loss=0.03308 | best_loss=0.03288
Epoch 9/80: current_loss=0.03341 | best_loss=0.03288
Epoch 10/80: current_loss=0.03302 | best_loss=0.03288
Epoch 11/80: current_loss=0.03309 | best_loss=0.03288
Epoch 12/80: current_loss=0.03298 | best_loss=0.03288
Epoch 13/80: current_loss=0.03295 | best_loss=0.03288
Epoch 14/80: current_loss=0.03390 | best_loss=0.03288
Epoch 15/80: current_loss=0.03303 | best_loss=0.03288
Epoch 16/80: current_loss=0.03294 | best_loss=0.03288
Epoch 17/80: current_loss=0.03321 | best_loss=0.03288
Epoch 18/80: current_loss=0.03334 | best_loss=0.03288
Epoch 19/80: current_loss=0.03307 | best_loss=0.03288
Epoch 20/80: current_loss=0.03302 | best_loss=0.03288
Epoch 21/80: current_loss=0.03287 | best_loss=0.03287
Epoch 22/80: current_loss=0.03296 | best_loss=0.03287
Epoch 23/80: current_loss=0.03310 | best_loss=0.03287
Epoch 24/80: current_loss=0.03295 | best_loss=0.03287
Epoch 25/80: current_loss=0.03287 | best_loss=0.03287
Epoch 26/80: current_loss=0.03311 | best_loss=0.03287
Epoch 27/80: current_loss=0.03305 | best_loss=0.03287
Epoch 28/80: current_loss=0.03300 | best_loss=0.03287
Epoch 29/80: current_loss=0.03291 | best_loss=0.03287
Epoch 30/80: current_loss=0.03292 | best_loss=0.03287
Epoch 31/80: current_loss=0.03335 | best_loss=0.03287
Epoch 32/80: current_loss=0.03289 | best_loss=0.03287
Epoch 33/80: current_loss=0.03361 | best_loss=0.03287
Epoch 34/80: current_loss=0.03288 | best_loss=0.03287
Epoch 35/80: current_loss=0.03298 | best_loss=0.03287
Epoch 36/80: current_loss=0.03296 | best_loss=0.03287
Epoch 37/80: current_loss=0.03307 | best_loss=0.03287
Epoch 38/80: current_loss=0.03319 | best_loss=0.03287
Epoch 39/80: current_loss=0.03295 | best_loss=0.03287
Epoch 40/80: current_loss=0.03289 | best_loss=0.03287
Epoch 41/80: current_loss=0.03301 | best_loss=0.03287
Early Stopping at epoch 41
      explained_var=-0.00368 | mse_loss=0.03273
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03537 | best_loss=0.03537
Epoch 1/80: current_loss=0.03539 | best_loss=0.03537
Epoch 2/80: current_loss=0.03550 | best_loss=0.03537
Epoch 3/80: current_loss=0.03537 | best_loss=0.03537
Epoch 4/80: current_loss=0.03549 | best_loss=0.03537
Epoch 5/80: current_loss=0.03545 | best_loss=0.03537
Epoch 6/80: current_loss=0.03551 | best_loss=0.03537
Epoch 7/80: current_loss=0.03563 | best_loss=0.03537
Epoch 8/80: current_loss=0.03547 | best_loss=0.03537
Epoch 9/80: current_loss=0.03555 | best_loss=0.03537
Epoch 10/80: current_loss=0.03552 | best_loss=0.03537
Epoch 11/80: current_loss=0.03546 | best_loss=0.03537
Epoch 12/80: current_loss=0.03556 | best_loss=0.03537
Epoch 13/80: current_loss=0.03542 | best_loss=0.03537
Epoch 14/80: current_loss=0.03526 | best_loss=0.03526
Epoch 15/80: current_loss=0.03530 | best_loss=0.03526
Epoch 16/80: current_loss=0.03546 | best_loss=0.03526
Epoch 17/80: current_loss=0.03558 | best_loss=0.03526
Epoch 18/80: current_loss=0.03547 | best_loss=0.03526
Epoch 19/80: current_loss=0.03540 | best_loss=0.03526
Epoch 20/80: current_loss=0.03552 | best_loss=0.03526
Epoch 21/80: current_loss=0.03554 | best_loss=0.03526
Epoch 22/80: current_loss=0.03546 | best_loss=0.03526
Epoch 23/80: current_loss=0.03540 | best_loss=0.03526
Epoch 24/80: current_loss=0.03543 | best_loss=0.03526
Epoch 25/80: current_loss=0.03545 | best_loss=0.03526
Epoch 26/80: current_loss=0.03548 | best_loss=0.03526
Epoch 27/80: current_loss=0.03551 | best_loss=0.03526
Epoch 28/80: current_loss=0.03565 | best_loss=0.03526
Epoch 29/80: current_loss=0.03550 | best_loss=0.03526
Epoch 30/80: current_loss=0.03553 | best_loss=0.03526
Epoch 31/80: current_loss=0.03550 | best_loss=0.03526
Epoch 32/80: current_loss=0.03556 | best_loss=0.03526
Epoch 33/80: current_loss=0.03561 | best_loss=0.03526
Epoch 34/80: current_loss=0.03572 | best_loss=0.03526
Early Stopping at epoch 34
      explained_var=0.02303 | mse_loss=0.03506
----------------------------------------------
Average early_stopping_point: 25| avg_exp_var=0.01546| avg_loss=0.03485
----------------------------------------------


----------------------------------------------
Params for Trial 39
{'learning_rate': 0.001, 'weight_decay': 0.0005760067847031483, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04306 | best_loss=0.04306
Epoch 1/80: current_loss=0.04040 | best_loss=0.04040
Epoch 2/80: current_loss=0.03986 | best_loss=0.03986
Epoch 3/80: current_loss=0.03913 | best_loss=0.03913
Epoch 4/80: current_loss=0.03902 | best_loss=0.03902
Epoch 5/80: current_loss=0.03912 | best_loss=0.03902
Epoch 6/80: current_loss=0.03864 | best_loss=0.03864
Epoch 7/80: current_loss=0.03856 | best_loss=0.03856
Epoch 8/80: current_loss=0.03829 | best_loss=0.03829
Epoch 9/80: current_loss=0.03817 | best_loss=0.03817
Epoch 10/80: current_loss=0.04020 | best_loss=0.03817
Epoch 11/80: current_loss=0.03794 | best_loss=0.03794
Epoch 12/80: current_loss=0.03792 | best_loss=0.03792
Epoch 13/80: current_loss=0.03815 | best_loss=0.03792
Epoch 14/80: current_loss=0.03831 | best_loss=0.03792
Epoch 15/80: current_loss=0.03845 | best_loss=0.03792
Epoch 16/80: current_loss=0.03771 | best_loss=0.03771
Epoch 17/80: current_loss=0.03828 | best_loss=0.03771
Epoch 18/80: current_loss=0.03802 | best_loss=0.03771
Epoch 19/80: current_loss=0.03770 | best_loss=0.03770
Epoch 20/80: current_loss=0.03765 | best_loss=0.03765
Epoch 21/80: current_loss=0.03811 | best_loss=0.03765
Epoch 22/80: current_loss=0.03828 | best_loss=0.03765
Epoch 23/80: current_loss=0.03754 | best_loss=0.03754
Epoch 24/80: current_loss=0.03768 | best_loss=0.03754
Epoch 25/80: current_loss=0.03883 | best_loss=0.03754
Epoch 26/80: current_loss=0.03756 | best_loss=0.03754
Epoch 27/80: current_loss=0.03778 | best_loss=0.03754
Epoch 28/80: current_loss=0.03780 | best_loss=0.03754
Epoch 29/80: current_loss=0.03774 | best_loss=0.03754
Epoch 30/80: current_loss=0.03779 | best_loss=0.03754
Epoch 31/80: current_loss=0.03760 | best_loss=0.03754
Epoch 32/80: current_loss=0.03819 | best_loss=0.03754
Epoch 33/80: current_loss=0.03778 | best_loss=0.03754
Epoch 34/80: current_loss=0.03784 | best_loss=0.03754
Epoch 35/80: current_loss=0.03770 | best_loss=0.03754
Epoch 36/80: current_loss=0.03837 | best_loss=0.03754
Epoch 37/80: current_loss=0.03776 | best_loss=0.03754
Epoch 38/80: current_loss=0.03771 | best_loss=0.03754
Epoch 39/80: current_loss=0.03768 | best_loss=0.03754
Epoch 40/80: current_loss=0.03877 | best_loss=0.03754
Epoch 41/80: current_loss=0.04137 | best_loss=0.03754
Epoch 42/80: current_loss=0.03757 | best_loss=0.03754
Epoch 43/80: current_loss=0.03861 | best_loss=0.03754
Early Stopping at epoch 43
      explained_var=0.01903 | mse_loss=0.03849
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04185 | best_loss=0.04185
Epoch 1/80: current_loss=0.04265 | best_loss=0.04185
Epoch 2/80: current_loss=0.04158 | best_loss=0.04158
Epoch 3/80: current_loss=0.04163 | best_loss=0.04158
Epoch 4/80: current_loss=0.04212 | best_loss=0.04158
Epoch 5/80: current_loss=0.04140 | best_loss=0.04140
Epoch 6/80: current_loss=0.04148 | best_loss=0.04140
Epoch 7/80: current_loss=0.04150 | best_loss=0.04140
Epoch 8/80: current_loss=0.04225 | best_loss=0.04140
Epoch 9/80: current_loss=0.04155 | best_loss=0.04140
Epoch 10/80: current_loss=0.04155 | best_loss=0.04140
Epoch 11/80: current_loss=0.04157 | best_loss=0.04140
Epoch 12/80: current_loss=0.04285 | best_loss=0.04140
Epoch 13/80: current_loss=0.04174 | best_loss=0.04140
Epoch 14/80: current_loss=0.04201 | best_loss=0.04140
Epoch 15/80: current_loss=0.04175 | best_loss=0.04140
Epoch 16/80: current_loss=0.04196 | best_loss=0.04140
Epoch 17/80: current_loss=0.04164 | best_loss=0.04140
Epoch 18/80: current_loss=0.04194 | best_loss=0.04140
Epoch 19/80: current_loss=0.04191 | best_loss=0.04140
Epoch 20/80: current_loss=0.04203 | best_loss=0.04140
Epoch 21/80: current_loss=0.04175 | best_loss=0.04140
Epoch 22/80: current_loss=0.04175 | best_loss=0.04140
Epoch 23/80: current_loss=0.04335 | best_loss=0.04140
Epoch 24/80: current_loss=0.04331 | best_loss=0.04140
Epoch 25/80: current_loss=0.04155 | best_loss=0.04140
Early Stopping at epoch 25
      explained_var=0.03156 | mse_loss=0.03994
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02814 | best_loss=0.02814
Epoch 1/80: current_loss=0.02880 | best_loss=0.02814
Epoch 2/80: current_loss=0.02815 | best_loss=0.02814
Epoch 3/80: current_loss=0.02815 | best_loss=0.02814
Epoch 4/80: current_loss=0.02808 | best_loss=0.02808
Epoch 5/80: current_loss=0.02823 | best_loss=0.02808
Epoch 6/80: current_loss=0.03018 | best_loss=0.02808
Epoch 7/80: current_loss=0.02812 | best_loss=0.02808
Epoch 8/80: current_loss=0.02916 | best_loss=0.02808
Epoch 9/80: current_loss=0.02811 | best_loss=0.02808
Epoch 10/80: current_loss=0.02839 | best_loss=0.02808
Epoch 11/80: current_loss=0.02814 | best_loss=0.02808
Epoch 12/80: current_loss=0.02909 | best_loss=0.02808
Epoch 13/80: current_loss=0.02806 | best_loss=0.02806
Epoch 14/80: current_loss=0.02804 | best_loss=0.02804
Epoch 15/80: current_loss=0.02827 | best_loss=0.02804
Epoch 16/80: current_loss=0.02815 | best_loss=0.02804
Epoch 17/80: current_loss=0.02865 | best_loss=0.02804
Epoch 18/80: current_loss=0.02805 | best_loss=0.02804
Epoch 19/80: current_loss=0.02844 | best_loss=0.02804
Epoch 20/80: current_loss=0.02814 | best_loss=0.02804
Epoch 21/80: current_loss=0.02814 | best_loss=0.02804
Epoch 22/80: current_loss=0.02853 | best_loss=0.02804
Epoch 23/80: current_loss=0.02838 | best_loss=0.02804
Epoch 24/80: current_loss=0.02830 | best_loss=0.02804
Epoch 25/80: current_loss=0.02864 | best_loss=0.02804
Epoch 26/80: current_loss=0.02892 | best_loss=0.02804
Epoch 27/80: current_loss=0.02819 | best_loss=0.02804
Epoch 28/80: current_loss=0.02811 | best_loss=0.02804
Epoch 29/80: current_loss=0.02914 | best_loss=0.02804
Epoch 30/80: current_loss=0.02898 | best_loss=0.02804
Epoch 31/80: current_loss=0.02810 | best_loss=0.02804
Epoch 32/80: current_loss=0.02829 | best_loss=0.02804
Epoch 33/80: current_loss=0.02841 | best_loss=0.02804
Epoch 34/80: current_loss=0.02813 | best_loss=0.02804
Early Stopping at epoch 34
      explained_var=-0.00451 | mse_loss=0.02850

----------------------------------------------
Params for Trial 40
{'learning_rate': 0.001, 'weight_decay': 0.009764968251243388, 'n_layers': 3, 'hidden_size': 512, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03983 | best_loss=0.03983
Epoch 1/80: current_loss=0.03832 | best_loss=0.03832
Epoch 2/80: current_loss=0.03878 | best_loss=0.03832
Epoch 3/80: current_loss=0.03823 | best_loss=0.03823
Epoch 4/80: current_loss=0.03842 | best_loss=0.03823
Epoch 5/80: current_loss=0.04035 | best_loss=0.03823
Epoch 6/80: current_loss=0.03997 | best_loss=0.03823
Epoch 7/80: current_loss=0.04047 | best_loss=0.03823
Epoch 8/80: current_loss=0.03881 | best_loss=0.03823
Epoch 9/80: current_loss=0.04142 | best_loss=0.03823
Epoch 10/80: current_loss=0.04003 | best_loss=0.03823
Epoch 11/80: current_loss=0.03906 | best_loss=0.03823
Epoch 12/80: current_loss=0.03902 | best_loss=0.03823
Epoch 13/80: current_loss=0.03903 | best_loss=0.03823
Epoch 14/80: current_loss=0.03889 | best_loss=0.03823
Epoch 15/80: current_loss=0.04016 | best_loss=0.03823
Epoch 16/80: current_loss=0.04009 | best_loss=0.03823
Epoch 17/80: current_loss=0.03990 | best_loss=0.03823
Epoch 18/80: current_loss=0.04071 | best_loss=0.03823
Epoch 19/80: current_loss=0.03921 | best_loss=0.03823
Epoch 20/80: current_loss=0.04079 | best_loss=0.03823
Epoch 21/80: current_loss=0.03839 | best_loss=0.03823
Epoch 22/80: current_loss=0.03887 | best_loss=0.03823
Epoch 23/80: current_loss=0.04010 | best_loss=0.03823
Early Stopping at epoch 23
      explained_var=0.00059 | mse_loss=0.03908

----------------------------------------------
Params for Trial 41
{'learning_rate': 0.001, 'weight_decay': 0.0005277571401622937, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05113 | best_loss=0.05113
Epoch 1/80: current_loss=0.04190 | best_loss=0.04190
Epoch 2/80: current_loss=0.03906 | best_loss=0.03906
Epoch 3/80: current_loss=0.03842 | best_loss=0.03842
Epoch 4/80: current_loss=0.03785 | best_loss=0.03785
Epoch 5/80: current_loss=0.03792 | best_loss=0.03785
Epoch 6/80: current_loss=0.03830 | best_loss=0.03785
Epoch 7/80: current_loss=0.03808 | best_loss=0.03785
Epoch 8/80: current_loss=0.03835 | best_loss=0.03785
Epoch 9/80: current_loss=0.03911 | best_loss=0.03785
Epoch 10/80: current_loss=0.03998 | best_loss=0.03785
Epoch 11/80: current_loss=0.03867 | best_loss=0.03785
Epoch 12/80: current_loss=0.03820 | best_loss=0.03785
Epoch 13/80: current_loss=0.04265 | best_loss=0.03785
Epoch 14/80: current_loss=0.03755 | best_loss=0.03755
Epoch 15/80: current_loss=0.03815 | best_loss=0.03755
Epoch 16/80: current_loss=0.03896 | best_loss=0.03755
Epoch 17/80: current_loss=0.03761 | best_loss=0.03755
Epoch 18/80: current_loss=0.03843 | best_loss=0.03755
Epoch 19/80: current_loss=0.03796 | best_loss=0.03755
Epoch 20/80: current_loss=0.04005 | best_loss=0.03755
Epoch 21/80: current_loss=0.03751 | best_loss=0.03751
Epoch 22/80: current_loss=0.03770 | best_loss=0.03751
Epoch 23/80: current_loss=0.03741 | best_loss=0.03741
Epoch 24/80: current_loss=0.03884 | best_loss=0.03741
Epoch 25/80: current_loss=0.03757 | best_loss=0.03741
Epoch 26/80: current_loss=0.03772 | best_loss=0.03741
Epoch 27/80: current_loss=0.03850 | best_loss=0.03741
Epoch 28/80: current_loss=0.03735 | best_loss=0.03735
Epoch 29/80: current_loss=0.03754 | best_loss=0.03735
Epoch 30/80: current_loss=0.03829 | best_loss=0.03735
Epoch 31/80: current_loss=0.03781 | best_loss=0.03735
Epoch 32/80: current_loss=0.03785 | best_loss=0.03735
Epoch 33/80: current_loss=0.03737 | best_loss=0.03735
Epoch 34/80: current_loss=0.03766 | best_loss=0.03735
Epoch 35/80: current_loss=0.03753 | best_loss=0.03735
Epoch 36/80: current_loss=0.03842 | best_loss=0.03735
Epoch 37/80: current_loss=0.03756 | best_loss=0.03735
Epoch 38/80: current_loss=0.03779 | best_loss=0.03735
Epoch 39/80: current_loss=0.03751 | best_loss=0.03735
Epoch 40/80: current_loss=0.03736 | best_loss=0.03735
Epoch 41/80: current_loss=0.03768 | best_loss=0.03735
Epoch 42/80: current_loss=0.03756 | best_loss=0.03735
Epoch 43/80: current_loss=0.03788 | best_loss=0.03735
Epoch 44/80: current_loss=0.03813 | best_loss=0.03735
Epoch 45/80: current_loss=0.03765 | best_loss=0.03735
Epoch 46/80: current_loss=0.03746 | best_loss=0.03735
Epoch 47/80: current_loss=0.03740 | best_loss=0.03735
Epoch 48/80: current_loss=0.03769 | best_loss=0.03735
Early Stopping at epoch 48
      explained_var=0.02352 | mse_loss=0.03824
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04140 | best_loss=0.04140
Epoch 1/80: current_loss=0.04267 | best_loss=0.04140
Epoch 2/80: current_loss=0.04474 | best_loss=0.04140
Epoch 3/80: current_loss=0.04208 | best_loss=0.04140
Epoch 4/80: current_loss=0.04178 | best_loss=0.04140
Epoch 5/80: current_loss=0.04170 | best_loss=0.04140
Epoch 6/80: current_loss=0.04184 | best_loss=0.04140
Epoch 7/80: current_loss=0.04208 | best_loss=0.04140
Epoch 8/80: current_loss=0.04234 | best_loss=0.04140
Epoch 9/80: current_loss=0.04195 | best_loss=0.04140
Epoch 10/80: current_loss=0.04171 | best_loss=0.04140
Epoch 11/80: current_loss=0.04300 | best_loss=0.04140
Epoch 12/80: current_loss=0.04449 | best_loss=0.04140
Epoch 13/80: current_loss=0.04248 | best_loss=0.04140
Epoch 14/80: current_loss=0.04185 | best_loss=0.04140
Epoch 15/80: current_loss=0.04175 | best_loss=0.04140
Epoch 16/80: current_loss=0.04218 | best_loss=0.04140
Epoch 17/80: current_loss=0.04189 | best_loss=0.04140
Epoch 18/80: current_loss=0.04168 | best_loss=0.04140
Epoch 19/80: current_loss=0.04176 | best_loss=0.04140
Epoch 20/80: current_loss=0.04174 | best_loss=0.04140
Early Stopping at epoch 20
      explained_var=0.03297 | mse_loss=0.03991
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02836 | best_loss=0.02836
Epoch 1/80: current_loss=0.02831 | best_loss=0.02831
Epoch 2/80: current_loss=0.02860 | best_loss=0.02831
Epoch 3/80: current_loss=0.02822 | best_loss=0.02822
Epoch 4/80: current_loss=0.02815 | best_loss=0.02815
Epoch 5/80: current_loss=0.02807 | best_loss=0.02807
Epoch 6/80: current_loss=0.02851 | best_loss=0.02807
Epoch 7/80: current_loss=0.02903 | best_loss=0.02807
Epoch 8/80: current_loss=0.02878 | best_loss=0.02807
Epoch 9/80: current_loss=0.02831 | best_loss=0.02807
Epoch 10/80: current_loss=0.02850 | best_loss=0.02807
Epoch 11/80: current_loss=0.02843 | best_loss=0.02807
Epoch 12/80: current_loss=0.02820 | best_loss=0.02807
Epoch 13/80: current_loss=0.02853 | best_loss=0.02807
Epoch 14/80: current_loss=0.02844 | best_loss=0.02807
Epoch 15/80: current_loss=0.02929 | best_loss=0.02807
Epoch 16/80: current_loss=0.02842 | best_loss=0.02807
Epoch 17/80: current_loss=0.02824 | best_loss=0.02807
Epoch 18/80: current_loss=0.02841 | best_loss=0.02807
Epoch 19/80: current_loss=0.03007 | best_loss=0.02807
Epoch 20/80: current_loss=0.02824 | best_loss=0.02807
Epoch 21/80: current_loss=0.02902 | best_loss=0.02807
Epoch 22/80: current_loss=0.02889 | best_loss=0.02807
Epoch 23/80: current_loss=0.02800 | best_loss=0.02800
Epoch 24/80: current_loss=0.02860 | best_loss=0.02800
Epoch 25/80: current_loss=0.02891 | best_loss=0.02800
Epoch 26/80: current_loss=0.02807 | best_loss=0.02800
Epoch 27/80: current_loss=0.02818 | best_loss=0.02800
Epoch 28/80: current_loss=0.02862 | best_loss=0.02800
Epoch 29/80: current_loss=0.02813 | best_loss=0.02800
Epoch 30/80: current_loss=0.02868 | best_loss=0.02800
Epoch 31/80: current_loss=0.02890 | best_loss=0.02800
Epoch 32/80: current_loss=0.02833 | best_loss=0.02800
Epoch 33/80: current_loss=0.02853 | best_loss=0.02800
Epoch 34/80: current_loss=0.02871 | best_loss=0.02800
Epoch 35/80: current_loss=0.02812 | best_loss=0.02800
Epoch 36/80: current_loss=0.02815 | best_loss=0.02800
Epoch 37/80: current_loss=0.02821 | best_loss=0.02800
Epoch 38/80: current_loss=0.02906 | best_loss=0.02800
Epoch 39/80: current_loss=0.02809 | best_loss=0.02800
Epoch 40/80: current_loss=0.02873 | best_loss=0.02800
Epoch 41/80: current_loss=0.02811 | best_loss=0.02800
Epoch 42/80: current_loss=0.02816 | best_loss=0.02800
Epoch 43/80: current_loss=0.02832 | best_loss=0.02800
Early Stopping at epoch 43
      explained_var=-0.00336 | mse_loss=0.02846
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03281 | best_loss=0.03281
Epoch 1/80: current_loss=0.03303 | best_loss=0.03281
Epoch 2/80: current_loss=0.03346 | best_loss=0.03281
Epoch 3/80: current_loss=0.03302 | best_loss=0.03281
Epoch 4/80: current_loss=0.03301 | best_loss=0.03281
Epoch 5/80: current_loss=0.03292 | best_loss=0.03281
Epoch 6/80: current_loss=0.03311 | best_loss=0.03281
Epoch 7/80: current_loss=0.03295 | best_loss=0.03281
Epoch 8/80: current_loss=0.03308 | best_loss=0.03281
Epoch 9/80: current_loss=0.03359 | best_loss=0.03281
Epoch 10/80: current_loss=0.03345 | best_loss=0.03281
Epoch 11/80: current_loss=0.03385 | best_loss=0.03281
Epoch 12/80: current_loss=0.03303 | best_loss=0.03281
Epoch 13/80: current_loss=0.03285 | best_loss=0.03281
Epoch 14/80: current_loss=0.03286 | best_loss=0.03281
Epoch 15/80: current_loss=0.03337 | best_loss=0.03281
Epoch 16/80: current_loss=0.03287 | best_loss=0.03281
Epoch 17/80: current_loss=0.03293 | best_loss=0.03281
Epoch 18/80: current_loss=0.03306 | best_loss=0.03281
Epoch 19/80: current_loss=0.03297 | best_loss=0.03281
Epoch 20/80: current_loss=0.03295 | best_loss=0.03281
Early Stopping at epoch 20
      explained_var=-0.00192 | mse_loss=0.03266
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03556 | best_loss=0.03556
Epoch 1/80: current_loss=0.03544 | best_loss=0.03544
Epoch 2/80: current_loss=0.03545 | best_loss=0.03544
Epoch 3/80: current_loss=0.03592 | best_loss=0.03544
Epoch 4/80: current_loss=0.03566 | best_loss=0.03544
Epoch 5/80: current_loss=0.03541 | best_loss=0.03541
Epoch 6/80: current_loss=0.03541 | best_loss=0.03541
Epoch 7/80: current_loss=0.03598 | best_loss=0.03541
Epoch 8/80: current_loss=0.03590 | best_loss=0.03541
Epoch 9/80: current_loss=0.03575 | best_loss=0.03541
Epoch 10/80: current_loss=0.03547 | best_loss=0.03541
Epoch 11/80: current_loss=0.03564 | best_loss=0.03541
Epoch 12/80: current_loss=0.03560 | best_loss=0.03541
Epoch 13/80: current_loss=0.03566 | best_loss=0.03541
Epoch 14/80: current_loss=0.03549 | best_loss=0.03541
Epoch 15/80: current_loss=0.03540 | best_loss=0.03540
Epoch 16/80: current_loss=0.03554 | best_loss=0.03540
Epoch 17/80: current_loss=0.03539 | best_loss=0.03539
Epoch 18/80: current_loss=0.03546 | best_loss=0.03539
Epoch 19/80: current_loss=0.03567 | best_loss=0.03539
Epoch 20/80: current_loss=0.03530 | best_loss=0.03530
Epoch 21/80: current_loss=0.03567 | best_loss=0.03530
Epoch 22/80: current_loss=0.03549 | best_loss=0.03530
Epoch 23/80: current_loss=0.03548 | best_loss=0.03530
Epoch 24/80: current_loss=0.03553 | best_loss=0.03530
Epoch 25/80: current_loss=0.03542 | best_loss=0.03530
Epoch 26/80: current_loss=0.03544 | best_loss=0.03530
Epoch 27/80: current_loss=0.03552 | best_loss=0.03530
Epoch 28/80: current_loss=0.03535 | best_loss=0.03530
Epoch 29/80: current_loss=0.03535 | best_loss=0.03530
Epoch 30/80: current_loss=0.03557 | best_loss=0.03530
Epoch 31/80: current_loss=0.03551 | best_loss=0.03530
Epoch 32/80: current_loss=0.03572 | best_loss=0.03530
Epoch 33/80: current_loss=0.03546 | best_loss=0.03530
Epoch 34/80: current_loss=0.03552 | best_loss=0.03530
Epoch 35/80: current_loss=0.03544 | best_loss=0.03530
Epoch 36/80: current_loss=0.03547 | best_loss=0.03530
Epoch 37/80: current_loss=0.03542 | best_loss=0.03530
Epoch 38/80: current_loss=0.03567 | best_loss=0.03530
Epoch 39/80: current_loss=0.03563 | best_loss=0.03530
Epoch 40/80: current_loss=0.03556 | best_loss=0.03530
Early Stopping at epoch 40
      explained_var=0.02185 | mse_loss=0.03510
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.01461| avg_loss=0.03487
----------------------------------------------


----------------------------------------------
Params for Trial 42
{'learning_rate': 0.001, 'weight_decay': 0.000464566432343273, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04547 | best_loss=0.04547
Epoch 1/80: current_loss=0.04078 | best_loss=0.04078
Epoch 2/80: current_loss=0.03931 | best_loss=0.03931
Epoch 3/80: current_loss=0.04112 | best_loss=0.03931
Epoch 4/80: current_loss=0.03814 | best_loss=0.03814
Epoch 5/80: current_loss=0.03807 | best_loss=0.03807
Epoch 6/80: current_loss=0.03771 | best_loss=0.03771
Epoch 7/80: current_loss=0.03803 | best_loss=0.03771
Epoch 8/80: current_loss=0.03902 | best_loss=0.03771
Epoch 9/80: current_loss=0.03946 | best_loss=0.03771
Epoch 10/80: current_loss=0.03823 | best_loss=0.03771
Epoch 11/80: current_loss=0.03804 | best_loss=0.03771
Epoch 12/80: current_loss=0.03754 | best_loss=0.03754
Epoch 13/80: current_loss=0.04191 | best_loss=0.03754
Epoch 14/80: current_loss=0.03746 | best_loss=0.03746
Epoch 15/80: current_loss=0.03786 | best_loss=0.03746
Epoch 16/80: current_loss=0.03770 | best_loss=0.03746
Epoch 17/80: current_loss=0.03841 | best_loss=0.03746
Epoch 18/80: current_loss=0.03783 | best_loss=0.03746
Epoch 19/80: current_loss=0.03760 | best_loss=0.03746
Epoch 20/80: current_loss=0.03764 | best_loss=0.03746
Epoch 21/80: current_loss=0.03746 | best_loss=0.03746
Epoch 22/80: current_loss=0.03828 | best_loss=0.03746
Epoch 23/80: current_loss=0.03834 | best_loss=0.03746
Epoch 24/80: current_loss=0.03762 | best_loss=0.03746
Epoch 25/80: current_loss=0.03750 | best_loss=0.03746
Epoch 26/80: current_loss=0.03834 | best_loss=0.03746
Epoch 27/80: current_loss=0.03843 | best_loss=0.03746
Epoch 28/80: current_loss=0.03781 | best_loss=0.03746
Epoch 29/80: current_loss=0.03766 | best_loss=0.03746
Epoch 30/80: current_loss=0.03759 | best_loss=0.03746
Epoch 31/80: current_loss=0.03845 | best_loss=0.03746
Epoch 32/80: current_loss=0.03980 | best_loss=0.03746
Epoch 33/80: current_loss=0.03781 | best_loss=0.03746
Epoch 34/80: current_loss=0.03757 | best_loss=0.03746
Early Stopping at epoch 34
      explained_var=0.02318 | mse_loss=0.03843
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04386 | best_loss=0.04386
Epoch 1/80: current_loss=0.04153 | best_loss=0.04153
Epoch 2/80: current_loss=0.04277 | best_loss=0.04153
Epoch 3/80: current_loss=0.04135 | best_loss=0.04135
Epoch 4/80: current_loss=0.04214 | best_loss=0.04135
Epoch 5/80: current_loss=0.04153 | best_loss=0.04135
Epoch 6/80: current_loss=0.04147 | best_loss=0.04135
Epoch 7/80: current_loss=0.04340 | best_loss=0.04135
Epoch 8/80: current_loss=0.04159 | best_loss=0.04135
Epoch 9/80: current_loss=0.04196 | best_loss=0.04135
Epoch 10/80: current_loss=0.04206 | best_loss=0.04135
Epoch 11/80: current_loss=0.04177 | best_loss=0.04135
Epoch 12/80: current_loss=0.04200 | best_loss=0.04135
Epoch 13/80: current_loss=0.04232 | best_loss=0.04135
Epoch 14/80: current_loss=0.04231 | best_loss=0.04135
Epoch 15/80: current_loss=0.04193 | best_loss=0.04135
Epoch 16/80: current_loss=0.04150 | best_loss=0.04135
Epoch 17/80: current_loss=0.04212 | best_loss=0.04135
Epoch 18/80: current_loss=0.04175 | best_loss=0.04135
Epoch 19/80: current_loss=0.04265 | best_loss=0.04135
Epoch 20/80: current_loss=0.04164 | best_loss=0.04135
Epoch 21/80: current_loss=0.04182 | best_loss=0.04135
Epoch 22/80: current_loss=0.04163 | best_loss=0.04135
Epoch 23/80: current_loss=0.04184 | best_loss=0.04135
Early Stopping at epoch 23
      explained_var=0.03297 | mse_loss=0.03988
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02836 | best_loss=0.02836
Epoch 1/80: current_loss=0.02808 | best_loss=0.02808
Epoch 2/80: current_loss=0.02821 | best_loss=0.02808
Epoch 3/80: current_loss=0.03016 | best_loss=0.02808
Epoch 4/80: current_loss=0.02910 | best_loss=0.02808
Epoch 5/80: current_loss=0.02849 | best_loss=0.02808
Epoch 6/80: current_loss=0.02847 | best_loss=0.02808
Epoch 7/80: current_loss=0.02818 | best_loss=0.02808
Epoch 8/80: current_loss=0.02797 | best_loss=0.02797
Epoch 9/80: current_loss=0.02803 | best_loss=0.02797
Epoch 10/80: current_loss=0.02874 | best_loss=0.02797
Epoch 11/80: current_loss=0.02826 | best_loss=0.02797
Epoch 12/80: current_loss=0.02803 | best_loss=0.02797
Epoch 13/80: current_loss=0.02820 | best_loss=0.02797
Epoch 14/80: current_loss=0.02812 | best_loss=0.02797
Epoch 15/80: current_loss=0.02820 | best_loss=0.02797
Epoch 16/80: current_loss=0.02833 | best_loss=0.02797
Epoch 17/80: current_loss=0.02807 | best_loss=0.02797
Epoch 18/80: current_loss=0.02815 | best_loss=0.02797
Epoch 19/80: current_loss=0.02855 | best_loss=0.02797
Epoch 20/80: current_loss=0.02817 | best_loss=0.02797
Epoch 21/80: current_loss=0.02809 | best_loss=0.02797
Epoch 22/80: current_loss=0.02926 | best_loss=0.02797
Epoch 23/80: current_loss=0.02807 | best_loss=0.02797
Epoch 24/80: current_loss=0.02864 | best_loss=0.02797
Epoch 25/80: current_loss=0.02849 | best_loss=0.02797
Epoch 26/80: current_loss=0.02845 | best_loss=0.02797
Epoch 27/80: current_loss=0.02875 | best_loss=0.02797
Epoch 28/80: current_loss=0.02860 | best_loss=0.02797
Early Stopping at epoch 28
      explained_var=-0.00248 | mse_loss=0.02845
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03340 | best_loss=0.03340
Epoch 1/80: current_loss=0.03370 | best_loss=0.03340
Epoch 2/80: current_loss=0.03372 | best_loss=0.03340
Epoch 3/80: current_loss=0.03296 | best_loss=0.03296
Epoch 4/80: current_loss=0.03434 | best_loss=0.03296
Epoch 5/80: current_loss=0.03333 | best_loss=0.03296
Epoch 6/80: current_loss=0.03297 | best_loss=0.03296
Epoch 7/80: current_loss=0.03303 | best_loss=0.03296
Epoch 8/80: current_loss=0.03330 | best_loss=0.03296
Epoch 9/80: current_loss=0.03330 | best_loss=0.03296
Epoch 10/80: current_loss=0.03295 | best_loss=0.03295
Epoch 11/80: current_loss=0.03304 | best_loss=0.03295
Epoch 12/80: current_loss=0.03322 | best_loss=0.03295
Epoch 13/80: current_loss=0.03508 | best_loss=0.03295
Epoch 14/80: current_loss=0.03286 | best_loss=0.03286
Epoch 15/80: current_loss=0.03357 | best_loss=0.03286
Epoch 16/80: current_loss=0.03457 | best_loss=0.03286
Epoch 17/80: current_loss=0.03305 | best_loss=0.03286
Epoch 18/80: current_loss=0.03327 | best_loss=0.03286
Epoch 19/80: current_loss=0.03340 | best_loss=0.03286
Epoch 20/80: current_loss=0.03290 | best_loss=0.03286
Epoch 21/80: current_loss=0.03307 | best_loss=0.03286
Epoch 22/80: current_loss=0.03296 | best_loss=0.03286
Epoch 23/80: current_loss=0.03300 | best_loss=0.03286
Epoch 24/80: current_loss=0.03467 | best_loss=0.03286
Epoch 25/80: current_loss=0.03366 | best_loss=0.03286
Epoch 26/80: current_loss=0.03300 | best_loss=0.03286
Epoch 27/80: current_loss=0.03311 | best_loss=0.03286
Epoch 28/80: current_loss=0.03485 | best_loss=0.03286
Epoch 29/80: current_loss=0.03291 | best_loss=0.03286
Epoch 30/80: current_loss=0.03293 | best_loss=0.03286
Epoch 31/80: current_loss=0.03304 | best_loss=0.03286
Epoch 32/80: current_loss=0.03308 | best_loss=0.03286
Epoch 33/80: current_loss=0.03302 | best_loss=0.03286
Epoch 34/80: current_loss=0.03322 | best_loss=0.03286
Early Stopping at epoch 34
      explained_var=-0.00564 | mse_loss=0.03278
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03572 | best_loss=0.03572
Epoch 1/80: current_loss=0.03609 | best_loss=0.03572
Epoch 2/80: current_loss=0.03540 | best_loss=0.03540
Epoch 3/80: current_loss=0.03545 | best_loss=0.03540
Epoch 4/80: current_loss=0.03578 | best_loss=0.03540
Epoch 5/80: current_loss=0.03553 | best_loss=0.03540
Epoch 6/80: current_loss=0.03541 | best_loss=0.03540
Epoch 7/80: current_loss=0.03539 | best_loss=0.03539
Epoch 8/80: current_loss=0.03544 | best_loss=0.03539
Epoch 9/80: current_loss=0.03557 | best_loss=0.03539
Epoch 10/80: current_loss=0.03537 | best_loss=0.03537
Epoch 11/80: current_loss=0.03527 | best_loss=0.03527
Epoch 12/80: current_loss=0.03546 | best_loss=0.03527
Epoch 13/80: current_loss=0.03571 | best_loss=0.03527
Epoch 14/80: current_loss=0.03571 | best_loss=0.03527
Epoch 15/80: current_loss=0.03543 | best_loss=0.03527
Epoch 16/80: current_loss=0.03534 | best_loss=0.03527
Epoch 17/80: current_loss=0.03560 | best_loss=0.03527
Epoch 18/80: current_loss=0.03539 | best_loss=0.03527
Epoch 19/80: current_loss=0.03549 | best_loss=0.03527
Epoch 20/80: current_loss=0.03540 | best_loss=0.03527
Epoch 21/80: current_loss=0.03537 | best_loss=0.03527
Epoch 22/80: current_loss=0.03563 | best_loss=0.03527
Epoch 23/80: current_loss=0.03542 | best_loss=0.03527
Epoch 24/80: current_loss=0.03590 | best_loss=0.03527
Epoch 25/80: current_loss=0.03558 | best_loss=0.03527
Epoch 26/80: current_loss=0.03562 | best_loss=0.03527
Epoch 27/80: current_loss=0.03539 | best_loss=0.03527
Epoch 28/80: current_loss=0.03548 | best_loss=0.03527
Epoch 29/80: current_loss=0.03567 | best_loss=0.03527
Epoch 30/80: current_loss=0.03594 | best_loss=0.03527
Epoch 31/80: current_loss=0.03547 | best_loss=0.03527
Early Stopping at epoch 31
      explained_var=0.02335 | mse_loss=0.03504
----------------------------------------------
Average early_stopping_point: 10| avg_exp_var=0.01427| avg_loss=0.03492
----------------------------------------------


----------------------------------------------
Params for Trial 43
{'learning_rate': 0.001, 'weight_decay': 0.0012347884575060878, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04800 | best_loss=0.04800
Epoch 1/80: current_loss=0.03813 | best_loss=0.03813
Epoch 2/80: current_loss=0.03792 | best_loss=0.03792
Epoch 3/80: current_loss=0.03794 | best_loss=0.03792
Epoch 4/80: current_loss=0.03840 | best_loss=0.03792
Epoch 5/80: current_loss=0.04060 | best_loss=0.03792
Epoch 6/80: current_loss=0.03916 | best_loss=0.03792
Epoch 7/80: current_loss=0.04098 | best_loss=0.03792
Epoch 8/80: current_loss=0.03833 | best_loss=0.03792
Epoch 9/80: current_loss=0.03796 | best_loss=0.03792
Epoch 10/80: current_loss=0.03775 | best_loss=0.03775
Epoch 11/80: current_loss=0.03759 | best_loss=0.03759
Epoch 12/80: current_loss=0.03757 | best_loss=0.03757
Epoch 13/80: current_loss=0.04058 | best_loss=0.03757
Epoch 14/80: current_loss=0.03760 | best_loss=0.03757
Epoch 15/80: current_loss=0.03774 | best_loss=0.03757
Epoch 16/80: current_loss=0.03762 | best_loss=0.03757
Epoch 17/80: current_loss=0.03811 | best_loss=0.03757
Epoch 18/80: current_loss=0.03858 | best_loss=0.03757
Epoch 19/80: current_loss=0.03767 | best_loss=0.03757
Epoch 20/80: current_loss=0.03793 | best_loss=0.03757
Epoch 21/80: current_loss=0.03862 | best_loss=0.03757
Epoch 22/80: current_loss=0.03778 | best_loss=0.03757
Epoch 23/80: current_loss=0.03763 | best_loss=0.03757
Epoch 24/80: current_loss=0.03754 | best_loss=0.03754
Epoch 25/80: current_loss=0.03754 | best_loss=0.03754
Epoch 26/80: current_loss=0.03824 | best_loss=0.03754
Epoch 27/80: current_loss=0.03765 | best_loss=0.03754
Epoch 28/80: current_loss=0.03761 | best_loss=0.03754
Epoch 29/80: current_loss=0.03807 | best_loss=0.03754
Epoch 30/80: current_loss=0.03801 | best_loss=0.03754
Epoch 31/80: current_loss=0.03771 | best_loss=0.03754
Epoch 32/80: current_loss=0.03762 | best_loss=0.03754
Epoch 33/80: current_loss=0.03771 | best_loss=0.03754
Epoch 34/80: current_loss=0.03752 | best_loss=0.03752
Epoch 35/80: current_loss=0.03816 | best_loss=0.03752
Epoch 36/80: current_loss=0.03768 | best_loss=0.03752
Epoch 37/80: current_loss=0.03853 | best_loss=0.03752
Epoch 38/80: current_loss=0.03877 | best_loss=0.03752
Epoch 39/80: current_loss=0.03762 | best_loss=0.03752
Epoch 40/80: current_loss=0.03803 | best_loss=0.03752
Epoch 41/80: current_loss=0.03790 | best_loss=0.03752
Epoch 42/80: current_loss=0.03760 | best_loss=0.03752
Epoch 43/80: current_loss=0.03963 | best_loss=0.03752
Epoch 44/80: current_loss=0.03955 | best_loss=0.03752
Epoch 45/80: current_loss=0.03755 | best_loss=0.03752
Epoch 46/80: current_loss=0.03824 | best_loss=0.03752
Epoch 47/80: current_loss=0.03758 | best_loss=0.03752
Epoch 48/80: current_loss=0.03853 | best_loss=0.03752
Epoch 49/80: current_loss=0.03762 | best_loss=0.03752
Epoch 50/80: current_loss=0.03759 | best_loss=0.03752
Epoch 51/80: current_loss=0.03783 | best_loss=0.03752
Epoch 52/80: current_loss=0.03775 | best_loss=0.03752
Epoch 53/80: current_loss=0.03775 | best_loss=0.03752
Epoch 54/80: current_loss=0.03786 | best_loss=0.03752
Early Stopping at epoch 54
      explained_var=0.01805 | mse_loss=0.03842
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04228 | best_loss=0.04228
Epoch 1/80: current_loss=0.04192 | best_loss=0.04192
Epoch 2/80: current_loss=0.04151 | best_loss=0.04151
Epoch 3/80: current_loss=0.04208 | best_loss=0.04151
Epoch 4/80: current_loss=0.04376 | best_loss=0.04151
Epoch 5/80: current_loss=0.04273 | best_loss=0.04151
Epoch 6/80: current_loss=0.04175 | best_loss=0.04151
Epoch 7/80: current_loss=0.04171 | best_loss=0.04151
Epoch 8/80: current_loss=0.04238 | best_loss=0.04151
Epoch 9/80: current_loss=0.04219 | best_loss=0.04151
Epoch 10/80: current_loss=0.04197 | best_loss=0.04151
Epoch 11/80: current_loss=0.04177 | best_loss=0.04151
Epoch 12/80: current_loss=0.04173 | best_loss=0.04151
Epoch 13/80: current_loss=0.04160 | best_loss=0.04151
Epoch 14/80: current_loss=0.04186 | best_loss=0.04151
Epoch 15/80: current_loss=0.04216 | best_loss=0.04151
Epoch 16/80: current_loss=0.04170 | best_loss=0.04151
Epoch 17/80: current_loss=0.04176 | best_loss=0.04151
Epoch 18/80: current_loss=0.04227 | best_loss=0.04151
Epoch 19/80: current_loss=0.04416 | best_loss=0.04151
Epoch 20/80: current_loss=0.04299 | best_loss=0.04151
Epoch 21/80: current_loss=0.04179 | best_loss=0.04151
Epoch 22/80: current_loss=0.04219 | best_loss=0.04151
Early Stopping at epoch 22
      explained_var=0.02957 | mse_loss=0.04005
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03023 | best_loss=0.03023
Epoch 1/80: current_loss=0.02812 | best_loss=0.02812
Epoch 2/80: current_loss=0.02841 | best_loss=0.02812
Epoch 3/80: current_loss=0.02850 | best_loss=0.02812
Epoch 4/80: current_loss=0.02804 | best_loss=0.02804
Epoch 5/80: current_loss=0.02812 | best_loss=0.02804
Epoch 6/80: current_loss=0.02986 | best_loss=0.02804
Epoch 7/80: current_loss=0.02831 | best_loss=0.02804
Epoch 8/80: current_loss=0.02803 | best_loss=0.02803
Epoch 9/80: current_loss=0.02861 | best_loss=0.02803
Epoch 10/80: current_loss=0.02835 | best_loss=0.02803
Epoch 11/80: current_loss=0.02810 | best_loss=0.02803
Epoch 12/80: current_loss=0.02842 | best_loss=0.02803
Epoch 13/80: current_loss=0.02868 | best_loss=0.02803
Epoch 14/80: current_loss=0.02857 | best_loss=0.02803
Epoch 15/80: current_loss=0.02829 | best_loss=0.02803
Epoch 16/80: current_loss=0.02813 | best_loss=0.02803
Epoch 17/80: current_loss=0.02903 | best_loss=0.02803
Epoch 18/80: current_loss=0.02826 | best_loss=0.02803
Epoch 19/80: current_loss=0.02795 | best_loss=0.02795
Epoch 20/80: current_loss=0.02857 | best_loss=0.02795
Epoch 21/80: current_loss=0.02806 | best_loss=0.02795
Epoch 22/80: current_loss=0.02849 | best_loss=0.02795
Epoch 23/80: current_loss=0.02872 | best_loss=0.02795
Epoch 24/80: current_loss=0.02804 | best_loss=0.02795
Epoch 25/80: current_loss=0.02810 | best_loss=0.02795
Epoch 26/80: current_loss=0.02824 | best_loss=0.02795
Epoch 27/80: current_loss=0.02937 | best_loss=0.02795
Epoch 28/80: current_loss=0.02807 | best_loss=0.02795
Epoch 29/80: current_loss=0.02892 | best_loss=0.02795
Epoch 30/80: current_loss=0.02803 | best_loss=0.02795
Epoch 31/80: current_loss=0.02817 | best_loss=0.02795
Epoch 32/80: current_loss=0.02875 | best_loss=0.02795
Epoch 33/80: current_loss=0.02803 | best_loss=0.02795
Epoch 34/80: current_loss=0.02796 | best_loss=0.02795
Epoch 35/80: current_loss=0.02863 | best_loss=0.02795
Epoch 36/80: current_loss=0.02798 | best_loss=0.02795
Epoch 37/80: current_loss=0.02906 | best_loss=0.02795
Epoch 38/80: current_loss=0.02800 | best_loss=0.02795
Epoch 39/80: current_loss=0.02841 | best_loss=0.02795
Early Stopping at epoch 39
      explained_var=-0.00075 | mse_loss=0.02838
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03345 | best_loss=0.03345
Epoch 1/80: current_loss=0.03339 | best_loss=0.03339
Epoch 2/80: current_loss=0.03358 | best_loss=0.03339
Epoch 3/80: current_loss=0.03293 | best_loss=0.03293
Epoch 4/80: current_loss=0.03308 | best_loss=0.03293
Epoch 5/80: current_loss=0.03314 | best_loss=0.03293
Epoch 6/80: current_loss=0.03302 | best_loss=0.03293
Epoch 7/80: current_loss=0.03293 | best_loss=0.03293
Epoch 8/80: current_loss=0.03305 | best_loss=0.03293
Epoch 9/80: current_loss=0.03294 | best_loss=0.03293
Epoch 10/80: current_loss=0.03328 | best_loss=0.03293
Epoch 11/80: current_loss=0.03325 | best_loss=0.03293
Epoch 12/80: current_loss=0.03284 | best_loss=0.03284
Epoch 13/80: current_loss=0.03307 | best_loss=0.03284
Epoch 14/80: current_loss=0.03322 | best_loss=0.03284
Epoch 15/80: current_loss=0.03298 | best_loss=0.03284
Epoch 16/80: current_loss=0.03290 | best_loss=0.03284
Epoch 17/80: current_loss=0.03319 | best_loss=0.03284
Epoch 18/80: current_loss=0.03290 | best_loss=0.03284
Epoch 19/80: current_loss=0.03302 | best_loss=0.03284
Epoch 20/80: current_loss=0.03291 | best_loss=0.03284
Epoch 21/80: current_loss=0.03299 | best_loss=0.03284
Epoch 22/80: current_loss=0.03288 | best_loss=0.03284
Epoch 23/80: current_loss=0.03301 | best_loss=0.03284
Epoch 24/80: current_loss=0.03311 | best_loss=0.03284
Epoch 25/80: current_loss=0.03307 | best_loss=0.03284
Epoch 26/80: current_loss=0.03314 | best_loss=0.03284
Epoch 27/80: current_loss=0.03290 | best_loss=0.03284
Epoch 28/80: current_loss=0.03279 | best_loss=0.03279
Epoch 29/80: current_loss=0.03290 | best_loss=0.03279
Epoch 30/80: current_loss=0.03327 | best_loss=0.03279
Epoch 31/80: current_loss=0.03287 | best_loss=0.03279
Epoch 32/80: current_loss=0.03286 | best_loss=0.03279
Epoch 33/80: current_loss=0.03291 | best_loss=0.03279
Epoch 34/80: current_loss=0.03296 | best_loss=0.03279
Epoch 35/80: current_loss=0.03323 | best_loss=0.03279
Epoch 36/80: current_loss=0.03294 | best_loss=0.03279
Epoch 37/80: current_loss=0.03312 | best_loss=0.03279
Epoch 38/80: current_loss=0.03283 | best_loss=0.03279
Epoch 39/80: current_loss=0.03282 | best_loss=0.03279
Epoch 40/80: current_loss=0.03285 | best_loss=0.03279
Epoch 41/80: current_loss=0.03302 | best_loss=0.03279
Epoch 42/80: current_loss=0.03295 | best_loss=0.03279
Epoch 43/80: current_loss=0.03284 | best_loss=0.03279
Epoch 44/80: current_loss=0.03281 | best_loss=0.03279
Epoch 45/80: current_loss=0.03284 | best_loss=0.03279
Epoch 46/80: current_loss=0.03287 | best_loss=0.03279
Epoch 47/80: current_loss=0.03286 | best_loss=0.03279
Epoch 48/80: current_loss=0.03299 | best_loss=0.03279
Early Stopping at epoch 48
      explained_var=-0.00156 | mse_loss=0.03266
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03577 | best_loss=0.03577
Epoch 1/80: current_loss=0.03558 | best_loss=0.03558
Epoch 2/80: current_loss=0.03565 | best_loss=0.03558
Epoch 3/80: current_loss=0.03559 | best_loss=0.03558
Epoch 4/80: current_loss=0.03554 | best_loss=0.03554
Epoch 5/80: current_loss=0.03555 | best_loss=0.03554
Epoch 6/80: current_loss=0.03578 | best_loss=0.03554
Epoch 7/80: current_loss=0.03568 | best_loss=0.03554
Epoch 8/80: current_loss=0.03556 | best_loss=0.03554
Epoch 9/80: current_loss=0.03554 | best_loss=0.03554
Epoch 10/80: current_loss=0.03563 | best_loss=0.03554
Epoch 11/80: current_loss=0.03550 | best_loss=0.03550
Epoch 12/80: current_loss=0.03548 | best_loss=0.03548
Epoch 13/80: current_loss=0.03543 | best_loss=0.03543
Epoch 14/80: current_loss=0.03553 | best_loss=0.03543
Epoch 15/80: current_loss=0.03568 | best_loss=0.03543
Epoch 16/80: current_loss=0.03554 | best_loss=0.03543
Epoch 17/80: current_loss=0.03553 | best_loss=0.03543
Epoch 18/80: current_loss=0.03563 | best_loss=0.03543
Epoch 19/80: current_loss=0.03559 | best_loss=0.03543
Epoch 20/80: current_loss=0.03560 | best_loss=0.03543
Epoch 21/80: current_loss=0.03567 | best_loss=0.03543
Epoch 22/80: current_loss=0.03566 | best_loss=0.03543
Epoch 23/80: current_loss=0.03553 | best_loss=0.03543
Epoch 24/80: current_loss=0.03557 | best_loss=0.03543
Epoch 25/80: current_loss=0.03570 | best_loss=0.03543
Epoch 26/80: current_loss=0.03560 | best_loss=0.03543
Epoch 27/80: current_loss=0.03553 | best_loss=0.03543
Epoch 28/80: current_loss=0.03558 | best_loss=0.03543
Epoch 29/80: current_loss=0.03556 | best_loss=0.03543
Epoch 30/80: current_loss=0.03560 | best_loss=0.03543
Epoch 31/80: current_loss=0.03566 | best_loss=0.03543
Epoch 32/80: current_loss=0.03569 | best_loss=0.03543
Epoch 33/80: current_loss=0.03556 | best_loss=0.03543
Early Stopping at epoch 33
      explained_var=0.01886 | mse_loss=0.03520
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.01284| avg_loss=0.03494
----------------------------------------------


----------------------------------------------
Params for Trial 44
{'learning_rate': 0.001, 'weight_decay': 0.005676551554108868, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04229 | best_loss=0.04229
Epoch 1/80: current_loss=0.04027 | best_loss=0.04027
Epoch 2/80: current_loss=0.03918 | best_loss=0.03918
Epoch 3/80: current_loss=0.04005 | best_loss=0.03918
Epoch 4/80: current_loss=0.03924 | best_loss=0.03918
Epoch 5/80: current_loss=0.03798 | best_loss=0.03798
Epoch 6/80: current_loss=0.03784 | best_loss=0.03784
Epoch 7/80: current_loss=0.03778 | best_loss=0.03778
Epoch 8/80: current_loss=0.03777 | best_loss=0.03777
Epoch 9/80: current_loss=0.03841 | best_loss=0.03777
Epoch 10/80: current_loss=0.03863 | best_loss=0.03777
Epoch 11/80: current_loss=0.03795 | best_loss=0.03777
Epoch 12/80: current_loss=0.03814 | best_loss=0.03777
Epoch 13/80: current_loss=0.03832 | best_loss=0.03777
Epoch 14/80: current_loss=0.03827 | best_loss=0.03777
Epoch 15/80: current_loss=0.03843 | best_loss=0.03777
Epoch 16/80: current_loss=0.03810 | best_loss=0.03777
Epoch 17/80: current_loss=0.03783 | best_loss=0.03777
Epoch 18/80: current_loss=0.03789 | best_loss=0.03777
Epoch 19/80: current_loss=0.03806 | best_loss=0.03777
Epoch 20/80: current_loss=0.03963 | best_loss=0.03777
Epoch 21/80: current_loss=0.03790 | best_loss=0.03777
Epoch 22/80: current_loss=0.03785 | best_loss=0.03777
Epoch 23/80: current_loss=0.03875 | best_loss=0.03777
Epoch 24/80: current_loss=0.03857 | best_loss=0.03777
Epoch 25/80: current_loss=0.03804 | best_loss=0.03777
Epoch 26/80: current_loss=0.03828 | best_loss=0.03777
Epoch 27/80: current_loss=0.03793 | best_loss=0.03777
Epoch 28/80: current_loss=0.03798 | best_loss=0.03777
Early Stopping at epoch 28
      explained_var=0.01137 | mse_loss=0.03867

----------------------------------------------
Params for Trial 45
{'learning_rate': 0.001, 'weight_decay': 0.0026426647880227784, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03878 | best_loss=0.03878
Epoch 1/80: current_loss=0.03890 | best_loss=0.03878
Epoch 2/80: current_loss=0.03801 | best_loss=0.03801
Epoch 3/80: current_loss=0.03786 | best_loss=0.03786
Epoch 4/80: current_loss=0.04219 | best_loss=0.03786
Epoch 5/80: current_loss=0.03820 | best_loss=0.03786
Epoch 6/80: current_loss=0.03791 | best_loss=0.03786
Epoch 7/80: current_loss=0.03849 | best_loss=0.03786
Epoch 8/80: current_loss=0.04251 | best_loss=0.03786
Epoch 9/80: current_loss=0.03777 | best_loss=0.03777
Epoch 10/80: current_loss=0.03865 | best_loss=0.03777
Epoch 11/80: current_loss=0.03895 | best_loss=0.03777
Epoch 12/80: current_loss=0.03811 | best_loss=0.03777
Epoch 13/80: current_loss=0.03825 | best_loss=0.03777
Epoch 14/80: current_loss=0.03764 | best_loss=0.03764
Epoch 15/80: current_loss=0.03786 | best_loss=0.03764
Epoch 16/80: current_loss=0.03838 | best_loss=0.03764
Epoch 17/80: current_loss=0.03852 | best_loss=0.03764
Epoch 18/80: current_loss=0.03777 | best_loss=0.03764
Epoch 19/80: current_loss=0.03760 | best_loss=0.03760
Epoch 20/80: current_loss=0.03869 | best_loss=0.03760
Epoch 21/80: current_loss=0.03820 | best_loss=0.03760
Epoch 22/80: current_loss=0.03759 | best_loss=0.03759
Epoch 23/80: current_loss=0.03793 | best_loss=0.03759
Epoch 24/80: current_loss=0.03786 | best_loss=0.03759
Epoch 25/80: current_loss=0.03775 | best_loss=0.03759
Epoch 26/80: current_loss=0.03815 | best_loss=0.03759
Epoch 27/80: current_loss=0.03757 | best_loss=0.03757
Epoch 28/80: current_loss=0.03750 | best_loss=0.03750
Epoch 29/80: current_loss=0.03750 | best_loss=0.03750
Epoch 30/80: current_loss=0.03753 | best_loss=0.03750
Epoch 31/80: current_loss=0.03768 | best_loss=0.03750
Epoch 32/80: current_loss=0.03766 | best_loss=0.03750
Epoch 33/80: current_loss=0.03812 | best_loss=0.03750
Epoch 34/80: current_loss=0.03756 | best_loss=0.03750
Epoch 35/80: current_loss=0.03790 | best_loss=0.03750
Epoch 36/80: current_loss=0.03815 | best_loss=0.03750
Epoch 37/80: current_loss=0.03769 | best_loss=0.03750
Epoch 38/80: current_loss=0.03786 | best_loss=0.03750
Epoch 39/80: current_loss=0.03762 | best_loss=0.03750
Epoch 40/80: current_loss=0.03790 | best_loss=0.03750
Epoch 41/80: current_loss=0.03763 | best_loss=0.03750
Epoch 42/80: current_loss=0.03872 | best_loss=0.03750
Epoch 43/80: current_loss=0.03801 | best_loss=0.03750
Epoch 44/80: current_loss=0.03912 | best_loss=0.03750
Epoch 45/80: current_loss=0.03782 | best_loss=0.03750
Epoch 46/80: current_loss=0.03830 | best_loss=0.03750
Epoch 47/80: current_loss=0.03876 | best_loss=0.03750
Epoch 48/80: current_loss=0.03783 | best_loss=0.03750
Epoch 49/80: current_loss=0.03844 | best_loss=0.03750
Early Stopping at epoch 49
      explained_var=0.02031 | mse_loss=0.03831
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04164 | best_loss=0.04164
Epoch 1/80: current_loss=0.04171 | best_loss=0.04164
Epoch 2/80: current_loss=0.04204 | best_loss=0.04164
Epoch 3/80: current_loss=0.04187 | best_loss=0.04164
Epoch 4/80: current_loss=0.04219 | best_loss=0.04164
Epoch 5/80: current_loss=0.04240 | best_loss=0.04164
Epoch 6/80: current_loss=0.04238 | best_loss=0.04164
Epoch 7/80: current_loss=0.04180 | best_loss=0.04164
Epoch 8/80: current_loss=0.04183 | best_loss=0.04164
Epoch 9/80: current_loss=0.04182 | best_loss=0.04164
Epoch 10/80: current_loss=0.04177 | best_loss=0.04164
Epoch 11/80: current_loss=0.04195 | best_loss=0.04164
Epoch 12/80: current_loss=0.04200 | best_loss=0.04164
Epoch 13/80: current_loss=0.04219 | best_loss=0.04164
Epoch 14/80: current_loss=0.04179 | best_loss=0.04164
Epoch 15/80: current_loss=0.04184 | best_loss=0.04164
Epoch 16/80: current_loss=0.04176 | best_loss=0.04164
Epoch 17/80: current_loss=0.04271 | best_loss=0.04164
Epoch 18/80: current_loss=0.04191 | best_loss=0.04164
Epoch 19/80: current_loss=0.04213 | best_loss=0.04164
Epoch 20/80: current_loss=0.04327 | best_loss=0.04164
Early Stopping at epoch 20
      explained_var=0.02547 | mse_loss=0.04018
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02792 | best_loss=0.02792
Epoch 1/80: current_loss=0.02807 | best_loss=0.02792
Epoch 2/80: current_loss=0.02803 | best_loss=0.02792
Epoch 3/80: current_loss=0.02792 | best_loss=0.02792
Epoch 4/80: current_loss=0.02812 | best_loss=0.02792
Epoch 5/80: current_loss=0.02863 | best_loss=0.02792
Epoch 6/80: current_loss=0.02873 | best_loss=0.02792
Epoch 7/80: current_loss=0.02801 | best_loss=0.02792
Epoch 8/80: current_loss=0.02791 | best_loss=0.02791
Epoch 9/80: current_loss=0.02793 | best_loss=0.02791
Epoch 10/80: current_loss=0.03192 | best_loss=0.02791
Epoch 11/80: current_loss=0.02820 | best_loss=0.02791
Epoch 12/80: current_loss=0.02792 | best_loss=0.02791
Epoch 13/80: current_loss=0.02808 | best_loss=0.02791
Epoch 14/80: current_loss=0.02799 | best_loss=0.02791
Epoch 15/80: current_loss=0.02833 | best_loss=0.02791
Epoch 16/80: current_loss=0.02875 | best_loss=0.02791
Epoch 17/80: current_loss=0.02805 | best_loss=0.02791
Epoch 18/80: current_loss=0.02792 | best_loss=0.02791
Epoch 19/80: current_loss=0.02885 | best_loss=0.02791
Epoch 20/80: current_loss=0.02797 | best_loss=0.02791
Epoch 21/80: current_loss=0.02799 | best_loss=0.02791
Epoch 22/80: current_loss=0.02870 | best_loss=0.02791
Epoch 23/80: current_loss=0.02790 | best_loss=0.02790
Epoch 24/80: current_loss=0.02961 | best_loss=0.02790
Epoch 25/80: current_loss=0.03006 | best_loss=0.02790
Epoch 26/80: current_loss=0.02949 | best_loss=0.02790
Epoch 27/80: current_loss=0.02874 | best_loss=0.02790
Epoch 28/80: current_loss=0.02796 | best_loss=0.02790
Epoch 29/80: current_loss=0.02797 | best_loss=0.02790
Epoch 30/80: current_loss=0.02829 | best_loss=0.02790
Epoch 31/80: current_loss=0.02793 | best_loss=0.02790
Epoch 32/80: current_loss=0.02818 | best_loss=0.02790
Epoch 33/80: current_loss=0.02806 | best_loss=0.02790
Epoch 34/80: current_loss=0.02806 | best_loss=0.02790
Epoch 35/80: current_loss=0.02810 | best_loss=0.02790
Epoch 36/80: current_loss=0.02929 | best_loss=0.02790
Epoch 37/80: current_loss=0.02814 | best_loss=0.02790
Epoch 38/80: current_loss=0.02809 | best_loss=0.02790
Epoch 39/80: current_loss=0.02812 | best_loss=0.02790
Epoch 40/80: current_loss=0.02794 | best_loss=0.02790
Epoch 41/80: current_loss=0.02797 | best_loss=0.02790
Epoch 42/80: current_loss=0.02824 | best_loss=0.02790
Epoch 43/80: current_loss=0.02866 | best_loss=0.02790
Early Stopping at epoch 43
      explained_var=0.00004 | mse_loss=0.02837
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03301 | best_loss=0.03301
Epoch 1/80: current_loss=0.03293 | best_loss=0.03293
Epoch 2/80: current_loss=0.03298 | best_loss=0.03293
Epoch 3/80: current_loss=0.03304 | best_loss=0.03293
Epoch 4/80: current_loss=0.03311 | best_loss=0.03293
Epoch 5/80: current_loss=0.03295 | best_loss=0.03293
Epoch 6/80: current_loss=0.03302 | best_loss=0.03293
Epoch 7/80: current_loss=0.03298 | best_loss=0.03293
Epoch 8/80: current_loss=0.03289 | best_loss=0.03289
Epoch 9/80: current_loss=0.03309 | best_loss=0.03289
Epoch 10/80: current_loss=0.03335 | best_loss=0.03289
Epoch 11/80: current_loss=0.03296 | best_loss=0.03289
Epoch 12/80: current_loss=0.03311 | best_loss=0.03289
Epoch 13/80: current_loss=0.03284 | best_loss=0.03284
Epoch 14/80: current_loss=0.03319 | best_loss=0.03284
Epoch 15/80: current_loss=0.03284 | best_loss=0.03284
Epoch 16/80: current_loss=0.03332 | best_loss=0.03284
Epoch 17/80: current_loss=0.03280 | best_loss=0.03280
Epoch 18/80: current_loss=0.03350 | best_loss=0.03280
Epoch 19/80: current_loss=0.03394 | best_loss=0.03280
Epoch 20/80: current_loss=0.03289 | best_loss=0.03280
Epoch 21/80: current_loss=0.03301 | best_loss=0.03280
Epoch 22/80: current_loss=0.03283 | best_loss=0.03280
Epoch 23/80: current_loss=0.03279 | best_loss=0.03279
Epoch 24/80: current_loss=0.03303 | best_loss=0.03279
Epoch 25/80: current_loss=0.03288 | best_loss=0.03279
Epoch 26/80: current_loss=0.03291 | best_loss=0.03279
Epoch 27/80: current_loss=0.03313 | best_loss=0.03279
Epoch 28/80: current_loss=0.03291 | best_loss=0.03279
Epoch 29/80: current_loss=0.03319 | best_loss=0.03279
Epoch 30/80: current_loss=0.03285 | best_loss=0.03279
Epoch 31/80: current_loss=0.03284 | best_loss=0.03279
Epoch 32/80: current_loss=0.03354 | best_loss=0.03279
Epoch 33/80: current_loss=0.03289 | best_loss=0.03279
Epoch 34/80: current_loss=0.03283 | best_loss=0.03279
Epoch 35/80: current_loss=0.03278 | best_loss=0.03278
Epoch 36/80: current_loss=0.03283 | best_loss=0.03278
Epoch 37/80: current_loss=0.03359 | best_loss=0.03278
Epoch 38/80: current_loss=0.03279 | best_loss=0.03278
Epoch 39/80: current_loss=0.03286 | best_loss=0.03278
Epoch 40/80: current_loss=0.03303 | best_loss=0.03278
Epoch 41/80: current_loss=0.03286 | best_loss=0.03278
Epoch 42/80: current_loss=0.03309 | best_loss=0.03278
Epoch 43/80: current_loss=0.03282 | best_loss=0.03278
Epoch 44/80: current_loss=0.03280 | best_loss=0.03278
Epoch 45/80: current_loss=0.03303 | best_loss=0.03278
Epoch 46/80: current_loss=0.03285 | best_loss=0.03278
Epoch 47/80: current_loss=0.03280 | best_loss=0.03278
Epoch 48/80: current_loss=0.03291 | best_loss=0.03278
Epoch 49/80: current_loss=0.03283 | best_loss=0.03278
Epoch 50/80: current_loss=0.03306 | best_loss=0.03278
Epoch 51/80: current_loss=0.03285 | best_loss=0.03278
Epoch 52/80: current_loss=0.03282 | best_loss=0.03278
Epoch 53/80: current_loss=0.03283 | best_loss=0.03278
Epoch 54/80: current_loss=0.03282 | best_loss=0.03278
Epoch 55/80: current_loss=0.03314 | best_loss=0.03278
Early Stopping at epoch 55
      explained_var=-0.00113 | mse_loss=0.03265
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03564 | best_loss=0.03564
Epoch 1/80: current_loss=0.03565 | best_loss=0.03564
Epoch 2/80: current_loss=0.03565 | best_loss=0.03564
Epoch 3/80: current_loss=0.03574 | best_loss=0.03564
Epoch 4/80: current_loss=0.03571 | best_loss=0.03564
Epoch 5/80: current_loss=0.03592 | best_loss=0.03564
Epoch 6/80: current_loss=0.03571 | best_loss=0.03564
Epoch 7/80: current_loss=0.03566 | best_loss=0.03564
Epoch 8/80: current_loss=0.03574 | best_loss=0.03564
Epoch 9/80: current_loss=0.03597 | best_loss=0.03564
Epoch 10/80: current_loss=0.03578 | best_loss=0.03564
Epoch 11/80: current_loss=0.03573 | best_loss=0.03564
Epoch 12/80: current_loss=0.03570 | best_loss=0.03564
Epoch 13/80: current_loss=0.03576 | best_loss=0.03564
Epoch 14/80: current_loss=0.03577 | best_loss=0.03564
Epoch 15/80: current_loss=0.03564 | best_loss=0.03564
Epoch 16/80: current_loss=0.03570 | best_loss=0.03564
Epoch 17/80: current_loss=0.03573 | best_loss=0.03564
Epoch 18/80: current_loss=0.03575 | best_loss=0.03564
Epoch 19/80: current_loss=0.03573 | best_loss=0.03564
Epoch 20/80: current_loss=0.03586 | best_loss=0.03564
Early Stopping at epoch 20
      explained_var=0.01446 | mse_loss=0.03542
----------------------------------------------
Average early_stopping_point: 17| avg_exp_var=0.01183| avg_loss=0.03499
----------------------------------------------


----------------------------------------------
Params for Trial 46
{'learning_rate': 0.1, 'weight_decay': 0.0003992581731703973, 'n_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=2.44307 | best_loss=2.44307
Epoch 1/80: current_loss=0.10397 | best_loss=0.10397
Epoch 2/80: current_loss=0.07321 | best_loss=0.07321
Epoch 3/80: current_loss=0.03874 | best_loss=0.03874
Epoch 4/80: current_loss=0.04718 | best_loss=0.03874
Epoch 5/80: current_loss=0.04309 | best_loss=0.03874
Epoch 6/80: current_loss=0.03828 | best_loss=0.03828
Epoch 7/80: current_loss=0.05616 | best_loss=0.03828
Epoch 8/80: current_loss=0.04639 | best_loss=0.03828
Epoch 9/80: current_loss=0.04005 | best_loss=0.03828
Epoch 10/80: current_loss=0.04186 | best_loss=0.03828
Epoch 11/80: current_loss=0.04215 | best_loss=0.03828
Epoch 12/80: current_loss=0.04240 | best_loss=0.03828
Epoch 13/80: current_loss=0.05720 | best_loss=0.03828
Epoch 14/80: current_loss=0.05049 | best_loss=0.03828
Epoch 15/80: current_loss=0.04300 | best_loss=0.03828
Epoch 16/80: current_loss=0.05507 | best_loss=0.03828
Epoch 17/80: current_loss=0.07514 | best_loss=0.03828
Epoch 18/80: current_loss=0.08842 | best_loss=0.03828
Epoch 19/80: current_loss=0.03967 | best_loss=0.03828
Epoch 20/80: current_loss=0.04444 | best_loss=0.03828
Epoch 21/80: current_loss=0.10053 | best_loss=0.03828
Epoch 22/80: current_loss=0.05775 | best_loss=0.03828
Epoch 23/80: current_loss=0.05347 | best_loss=0.03828
Epoch 24/80: current_loss=0.07618 | best_loss=0.03828
Epoch 25/80: current_loss=0.04563 | best_loss=0.03828
Epoch 26/80: current_loss=0.04293 | best_loss=0.03828
Early Stopping at epoch 26
      explained_var=-0.00072 | mse_loss=0.03922

----------------------------------------------
Params for Trial 47
{'learning_rate': 0.001, 'weight_decay': 1.2594496663688958e-05, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04874 | best_loss=0.04874
Epoch 1/80: current_loss=0.04233 | best_loss=0.04233
Epoch 2/80: current_loss=0.03954 | best_loss=0.03954
Epoch 3/80: current_loss=0.04013 | best_loss=0.03954
Epoch 4/80: current_loss=0.03860 | best_loss=0.03860
Epoch 5/80: current_loss=0.03809 | best_loss=0.03809
Epoch 6/80: current_loss=0.03799 | best_loss=0.03799
Epoch 7/80: current_loss=0.03794 | best_loss=0.03794
Epoch 8/80: current_loss=0.03822 | best_loss=0.03794
Epoch 9/80: current_loss=0.04405 | best_loss=0.03794
Epoch 10/80: current_loss=0.03784 | best_loss=0.03784
Epoch 11/80: current_loss=0.03815 | best_loss=0.03784
Epoch 12/80: current_loss=0.03781 | best_loss=0.03781
Epoch 13/80: current_loss=0.03816 | best_loss=0.03781
Epoch 14/80: current_loss=0.03821 | best_loss=0.03781
Epoch 15/80: current_loss=0.03828 | best_loss=0.03781
Epoch 16/80: current_loss=0.03764 | best_loss=0.03764
Epoch 17/80: current_loss=0.03809 | best_loss=0.03764
Epoch 18/80: current_loss=0.03846 | best_loss=0.03764
Epoch 19/80: current_loss=0.03794 | best_loss=0.03764
Epoch 20/80: current_loss=0.03747 | best_loss=0.03747
Epoch 21/80: current_loss=0.03744 | best_loss=0.03744
Epoch 22/80: current_loss=0.03789 | best_loss=0.03744
Epoch 23/80: current_loss=0.03944 | best_loss=0.03744
Epoch 24/80: current_loss=0.03820 | best_loss=0.03744
Epoch 25/80: current_loss=0.03759 | best_loss=0.03744
Epoch 26/80: current_loss=0.03756 | best_loss=0.03744
Epoch 27/80: current_loss=0.03758 | best_loss=0.03744
Epoch 28/80: current_loss=0.03775 | best_loss=0.03744
Epoch 29/80: current_loss=0.03773 | best_loss=0.03744
Epoch 30/80: current_loss=0.03764 | best_loss=0.03744
Epoch 31/80: current_loss=0.03793 | best_loss=0.03744
Epoch 32/80: current_loss=0.03785 | best_loss=0.03744
Epoch 33/80: current_loss=0.03786 | best_loss=0.03744
Epoch 34/80: current_loss=0.03773 | best_loss=0.03744
Epoch 35/80: current_loss=0.03803 | best_loss=0.03744
Epoch 36/80: current_loss=0.03796 | best_loss=0.03744
Epoch 37/80: current_loss=0.03763 | best_loss=0.03744
Epoch 38/80: current_loss=0.03797 | best_loss=0.03744
Epoch 39/80: current_loss=0.03785 | best_loss=0.03744
Epoch 40/80: current_loss=0.03819 | best_loss=0.03744
Epoch 41/80: current_loss=0.03898 | best_loss=0.03744
Early Stopping at epoch 41
      explained_var=0.02293 | mse_loss=0.03836
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04244 | best_loss=0.04244
Epoch 1/80: current_loss=0.04306 | best_loss=0.04244
Epoch 2/80: current_loss=0.04156 | best_loss=0.04156
Epoch 3/80: current_loss=0.04162 | best_loss=0.04156
Epoch 4/80: current_loss=0.04233 | best_loss=0.04156
Epoch 5/80: current_loss=0.04138 | best_loss=0.04138
Epoch 6/80: current_loss=0.04170 | best_loss=0.04138
Epoch 7/80: current_loss=0.04250 | best_loss=0.04138
Epoch 8/80: current_loss=0.04236 | best_loss=0.04138
Epoch 9/80: current_loss=0.04168 | best_loss=0.04138
Epoch 10/80: current_loss=0.04197 | best_loss=0.04138
Epoch 11/80: current_loss=0.04176 | best_loss=0.04138
Epoch 12/80: current_loss=0.04177 | best_loss=0.04138
Epoch 13/80: current_loss=0.04256 | best_loss=0.04138
Epoch 14/80: current_loss=0.04200 | best_loss=0.04138
Epoch 15/80: current_loss=0.04167 | best_loss=0.04138
Epoch 16/80: current_loss=0.04180 | best_loss=0.04138
Epoch 17/80: current_loss=0.04182 | best_loss=0.04138
Epoch 18/80: current_loss=0.04208 | best_loss=0.04138
Epoch 19/80: current_loss=0.04152 | best_loss=0.04138
Epoch 20/80: current_loss=0.04149 | best_loss=0.04138
Epoch 21/80: current_loss=0.04304 | best_loss=0.04138
Epoch 22/80: current_loss=0.04202 | best_loss=0.04138
Epoch 23/80: current_loss=0.04258 | best_loss=0.04138
Epoch 24/80: current_loss=0.04183 | best_loss=0.04138
Epoch 25/80: current_loss=0.04171 | best_loss=0.04138
Early Stopping at epoch 25
      explained_var=0.03384 | mse_loss=0.03991
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02784 | best_loss=0.02784
Epoch 1/80: current_loss=0.02852 | best_loss=0.02784
Epoch 2/80: current_loss=0.02809 | best_loss=0.02784
Epoch 3/80: current_loss=0.02820 | best_loss=0.02784
Epoch 4/80: current_loss=0.02865 | best_loss=0.02784
Epoch 5/80: current_loss=0.02836 | best_loss=0.02784
Epoch 6/80: current_loss=0.02924 | best_loss=0.02784
Epoch 7/80: current_loss=0.02863 | best_loss=0.02784
Epoch 8/80: current_loss=0.02903 | best_loss=0.02784
Epoch 9/80: current_loss=0.02839 | best_loss=0.02784
Epoch 10/80: current_loss=0.02894 | best_loss=0.02784
Epoch 11/80: current_loss=0.02881 | best_loss=0.02784
Epoch 12/80: current_loss=0.02829 | best_loss=0.02784
Epoch 13/80: current_loss=0.02864 | best_loss=0.02784
Epoch 14/80: current_loss=0.02803 | best_loss=0.02784
Epoch 15/80: current_loss=0.02862 | best_loss=0.02784
Epoch 16/80: current_loss=0.02869 | best_loss=0.02784
Epoch 17/80: current_loss=0.02856 | best_loss=0.02784
Epoch 18/80: current_loss=0.02812 | best_loss=0.02784
Epoch 19/80: current_loss=0.02901 | best_loss=0.02784
Epoch 20/80: current_loss=0.02823 | best_loss=0.02784
Early Stopping at epoch 20
      explained_var=0.00514 | mse_loss=0.02828
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03314 | best_loss=0.03314
Epoch 1/80: current_loss=0.03325 | best_loss=0.03314
Epoch 2/80: current_loss=0.03323 | best_loss=0.03314
Epoch 3/80: current_loss=0.03325 | best_loss=0.03314
Epoch 4/80: current_loss=0.03337 | best_loss=0.03314
Epoch 5/80: current_loss=0.03305 | best_loss=0.03305
Epoch 6/80: current_loss=0.03312 | best_loss=0.03305
Epoch 7/80: current_loss=0.03347 | best_loss=0.03305
Epoch 8/80: current_loss=0.03313 | best_loss=0.03305
Epoch 9/80: current_loss=0.03294 | best_loss=0.03294
Epoch 10/80: current_loss=0.03280 | best_loss=0.03280
Epoch 11/80: current_loss=0.03299 | best_loss=0.03280
Epoch 12/80: current_loss=0.03300 | best_loss=0.03280
Epoch 13/80: current_loss=0.03364 | best_loss=0.03280
Epoch 14/80: current_loss=0.03300 | best_loss=0.03280
Epoch 15/80: current_loss=0.03302 | best_loss=0.03280
Epoch 16/80: current_loss=0.03315 | best_loss=0.03280
Epoch 17/80: current_loss=0.03305 | best_loss=0.03280
Epoch 18/80: current_loss=0.03333 | best_loss=0.03280
Epoch 19/80: current_loss=0.03305 | best_loss=0.03280
Epoch 20/80: current_loss=0.03296 | best_loss=0.03280
Epoch 21/80: current_loss=0.03281 | best_loss=0.03280
Epoch 22/80: current_loss=0.03291 | best_loss=0.03280
Epoch 23/80: current_loss=0.03276 | best_loss=0.03276
Epoch 24/80: current_loss=0.03294 | best_loss=0.03276
Epoch 25/80: current_loss=0.03317 | best_loss=0.03276
Epoch 26/80: current_loss=0.03314 | best_loss=0.03276
Epoch 27/80: current_loss=0.03312 | best_loss=0.03276
Epoch 28/80: current_loss=0.03332 | best_loss=0.03276
Epoch 29/80: current_loss=0.03303 | best_loss=0.03276
Epoch 30/80: current_loss=0.03291 | best_loss=0.03276
Epoch 31/80: current_loss=0.03292 | best_loss=0.03276
Epoch 32/80: current_loss=0.03384 | best_loss=0.03276
Epoch 33/80: current_loss=0.03284 | best_loss=0.03276
Epoch 34/80: current_loss=0.03290 | best_loss=0.03276
Epoch 35/80: current_loss=0.03320 | best_loss=0.03276
Epoch 36/80: current_loss=0.03311 | best_loss=0.03276
Epoch 37/80: current_loss=0.03306 | best_loss=0.03276
Epoch 38/80: current_loss=0.03294 | best_loss=0.03276
Epoch 39/80: current_loss=0.03306 | best_loss=0.03276
Epoch 40/80: current_loss=0.03310 | best_loss=0.03276
Epoch 41/80: current_loss=0.03325 | best_loss=0.03276
Epoch 42/80: current_loss=0.03301 | best_loss=0.03276
Epoch 43/80: current_loss=0.03310 | best_loss=0.03276
Early Stopping at epoch 43
      explained_var=-0.00257 | mse_loss=0.03268
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03536 | best_loss=0.03536
Epoch 1/80: current_loss=0.03538 | best_loss=0.03536
Epoch 2/80: current_loss=0.03546 | best_loss=0.03536
Epoch 3/80: current_loss=0.03565 | best_loss=0.03536
Epoch 4/80: current_loss=0.03563 | best_loss=0.03536
Epoch 5/80: current_loss=0.03544 | best_loss=0.03536
Epoch 6/80: current_loss=0.03551 | best_loss=0.03536
Epoch 7/80: current_loss=0.03544 | best_loss=0.03536
Epoch 8/80: current_loss=0.03549 | best_loss=0.03536
Epoch 9/80: current_loss=0.03568 | best_loss=0.03536
Epoch 10/80: current_loss=0.03586 | best_loss=0.03536
Epoch 11/80: current_loss=0.03569 | best_loss=0.03536
Epoch 12/80: current_loss=0.03549 | best_loss=0.03536
Epoch 13/80: current_loss=0.03549 | best_loss=0.03536
Epoch 14/80: current_loss=0.03550 | best_loss=0.03536
Epoch 15/80: current_loss=0.03541 | best_loss=0.03536
Epoch 16/80: current_loss=0.03544 | best_loss=0.03536
Epoch 17/80: current_loss=0.03555 | best_loss=0.03536
Epoch 18/80: current_loss=0.03556 | best_loss=0.03536
Epoch 19/80: current_loss=0.03546 | best_loss=0.03536
Epoch 20/80: current_loss=0.03547 | best_loss=0.03536
Early Stopping at epoch 20
      explained_var=0.02246 | mse_loss=0.03509
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.01636| avg_loss=0.03486
----------------------------------------------


----------------------------------------------
Params for Trial 48
{'learning_rate': 0.001, 'weight_decay': 0.007755358268918008, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03772 | best_loss=0.03772
Epoch 1/80: current_loss=0.03796 | best_loss=0.03772
Epoch 2/80: current_loss=0.03836 | best_loss=0.03772
Epoch 3/80: current_loss=0.03896 | best_loss=0.03772
Epoch 4/80: current_loss=0.03968 | best_loss=0.03772
Epoch 5/80: current_loss=0.03799 | best_loss=0.03772
Epoch 6/80: current_loss=0.03917 | best_loss=0.03772
Epoch 7/80: current_loss=0.03822 | best_loss=0.03772
Epoch 8/80: current_loss=0.03831 | best_loss=0.03772
Epoch 9/80: current_loss=0.04196 | best_loss=0.03772
Epoch 10/80: current_loss=0.03798 | best_loss=0.03772
Epoch 11/80: current_loss=0.04267 | best_loss=0.03772
Epoch 12/80: current_loss=0.03813 | best_loss=0.03772
Epoch 13/80: current_loss=0.03860 | best_loss=0.03772
Epoch 14/80: current_loss=0.04127 | best_loss=0.03772
Epoch 15/80: current_loss=0.03767 | best_loss=0.03767
Epoch 16/80: current_loss=0.03825 | best_loss=0.03767
Epoch 17/80: current_loss=0.04082 | best_loss=0.03767
Epoch 18/80: current_loss=0.03997 | best_loss=0.03767
Epoch 19/80: current_loss=0.03784 | best_loss=0.03767
Epoch 20/80: current_loss=0.03847 | best_loss=0.03767
Epoch 21/80: current_loss=0.03791 | best_loss=0.03767
Epoch 22/80: current_loss=0.03794 | best_loss=0.03767
Epoch 23/80: current_loss=0.04052 | best_loss=0.03767
Epoch 24/80: current_loss=0.03823 | best_loss=0.03767
Epoch 25/80: current_loss=0.03783 | best_loss=0.03767
Epoch 26/80: current_loss=0.03782 | best_loss=0.03767
Epoch 27/80: current_loss=0.03891 | best_loss=0.03767
Epoch 28/80: current_loss=0.03800 | best_loss=0.03767
Epoch 29/80: current_loss=0.03827 | best_loss=0.03767
Epoch 30/80: current_loss=0.03850 | best_loss=0.03767
Epoch 31/80: current_loss=0.03902 | best_loss=0.03767
Epoch 32/80: current_loss=0.03802 | best_loss=0.03767
Epoch 33/80: current_loss=0.03799 | best_loss=0.03767
Epoch 34/80: current_loss=0.03793 | best_loss=0.03767
Epoch 35/80: current_loss=0.03923 | best_loss=0.03767
Early Stopping at epoch 35
      explained_var=0.01608 | mse_loss=0.03849
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04674 | best_loss=0.04674
Epoch 1/80: current_loss=0.04288 | best_loss=0.04288
Epoch 2/80: current_loss=0.04569 | best_loss=0.04288
Epoch 3/80: current_loss=0.04285 | best_loss=0.04285
Epoch 4/80: current_loss=0.04524 | best_loss=0.04285
Epoch 5/80: current_loss=0.04563 | best_loss=0.04285
Epoch 6/80: current_loss=0.04234 | best_loss=0.04234
Epoch 7/80: current_loss=0.04229 | best_loss=0.04229
Epoch 8/80: current_loss=0.04267 | best_loss=0.04229
Epoch 9/80: current_loss=0.04215 | best_loss=0.04215
Epoch 10/80: current_loss=0.04501 | best_loss=0.04215
Epoch 11/80: current_loss=0.04288 | best_loss=0.04215
Epoch 12/80: current_loss=0.04981 | best_loss=0.04215
Epoch 13/80: current_loss=0.04268 | best_loss=0.04215
Epoch 14/80: current_loss=0.04247 | best_loss=0.04215
Epoch 15/80: current_loss=0.04247 | best_loss=0.04215
Epoch 16/80: current_loss=0.04281 | best_loss=0.04215
Epoch 17/80: current_loss=0.04377 | best_loss=0.04215
Epoch 18/80: current_loss=0.04327 | best_loss=0.04215
Epoch 19/80: current_loss=0.04273 | best_loss=0.04215
Epoch 20/80: current_loss=0.04246 | best_loss=0.04215
Epoch 21/80: current_loss=0.04413 | best_loss=0.04215
Epoch 22/80: current_loss=0.04370 | best_loss=0.04215
Epoch 23/80: current_loss=0.04243 | best_loss=0.04215
Epoch 24/80: current_loss=0.04416 | best_loss=0.04215
Epoch 25/80: current_loss=0.04242 | best_loss=0.04215
Epoch 26/80: current_loss=0.04353 | best_loss=0.04215
Epoch 27/80: current_loss=0.04275 | best_loss=0.04215
Epoch 28/80: current_loss=0.04374 | best_loss=0.04215
Epoch 29/80: current_loss=0.04243 | best_loss=0.04215
Early Stopping at epoch 29
      explained_var=0.01321 | mse_loss=0.04071
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02792 | best_loss=0.02792
Epoch 1/80: current_loss=0.02788 | best_loss=0.02788
Epoch 2/80: current_loss=0.02848 | best_loss=0.02788
Epoch 3/80: current_loss=0.02922 | best_loss=0.02788
Epoch 4/80: current_loss=0.02786 | best_loss=0.02786
Epoch 5/80: current_loss=0.02787 | best_loss=0.02786
Epoch 6/80: current_loss=0.02814 | best_loss=0.02786
Epoch 7/80: current_loss=0.02786 | best_loss=0.02786
Epoch 8/80: current_loss=0.02813 | best_loss=0.02786
Epoch 9/80: current_loss=0.02793 | best_loss=0.02786
Epoch 10/80: current_loss=0.03233 | best_loss=0.02786
Epoch 11/80: current_loss=0.02813 | best_loss=0.02786
Epoch 12/80: current_loss=0.02811 | best_loss=0.02786
Epoch 13/80: current_loss=0.02898 | best_loss=0.02786
Epoch 14/80: current_loss=0.03015 | best_loss=0.02786
Epoch 15/80: current_loss=0.02855 | best_loss=0.02786
Epoch 16/80: current_loss=0.02886 | best_loss=0.02786
Epoch 17/80: current_loss=0.02787 | best_loss=0.02786
Epoch 18/80: current_loss=0.02813 | best_loss=0.02786
Epoch 19/80: current_loss=0.03380 | best_loss=0.02786
Epoch 20/80: current_loss=0.02796 | best_loss=0.02786
Epoch 21/80: current_loss=0.02793 | best_loss=0.02786
Epoch 22/80: current_loss=0.02807 | best_loss=0.02786
Epoch 23/80: current_loss=0.02974 | best_loss=0.02786
Epoch 24/80: current_loss=0.02814 | best_loss=0.02786
Epoch 25/80: current_loss=0.02792 | best_loss=0.02786
Epoch 26/80: current_loss=0.03104 | best_loss=0.02786
Epoch 27/80: current_loss=0.02848 | best_loss=0.02786
Early Stopping at epoch 27
      explained_var=0.00267 | mse_loss=0.02831
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03360 | best_loss=0.03360
Epoch 1/80: current_loss=0.03276 | best_loss=0.03276
Epoch 2/80: current_loss=0.03398 | best_loss=0.03276
Epoch 3/80: current_loss=0.03326 | best_loss=0.03276
Epoch 4/80: current_loss=0.03307 | best_loss=0.03276
Epoch 5/80: current_loss=0.03305 | best_loss=0.03276
Epoch 6/80: current_loss=0.03291 | best_loss=0.03276
Epoch 7/80: current_loss=0.03346 | best_loss=0.03276
Epoch 8/80: current_loss=0.03325 | best_loss=0.03276
Epoch 9/80: current_loss=0.03306 | best_loss=0.03276
Epoch 10/80: current_loss=0.03324 | best_loss=0.03276
Epoch 11/80: current_loss=0.03313 | best_loss=0.03276
Epoch 12/80: current_loss=0.03369 | best_loss=0.03276
Epoch 13/80: current_loss=0.03294 | best_loss=0.03276
Epoch 14/80: current_loss=0.03281 | best_loss=0.03276
Epoch 15/80: current_loss=0.03293 | best_loss=0.03276
Epoch 16/80: current_loss=0.03290 | best_loss=0.03276
Epoch 17/80: current_loss=0.03288 | best_loss=0.03276
Epoch 18/80: current_loss=0.03305 | best_loss=0.03276
Epoch 19/80: current_loss=0.03343 | best_loss=0.03276
Epoch 20/80: current_loss=0.03289 | best_loss=0.03276
Epoch 21/80: current_loss=0.03361 | best_loss=0.03276
Early Stopping at epoch 21
      explained_var=0.00104 | mse_loss=0.03258
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03607 | best_loss=0.03607
Epoch 1/80: current_loss=0.03609 | best_loss=0.03607
Epoch 2/80: current_loss=0.03601 | best_loss=0.03601
Epoch 3/80: current_loss=0.03602 | best_loss=0.03601
Epoch 4/80: current_loss=0.03615 | best_loss=0.03601
Epoch 5/80: current_loss=0.03650 | best_loss=0.03601
Epoch 6/80: current_loss=0.03607 | best_loss=0.03601
Epoch 7/80: current_loss=0.03627 | best_loss=0.03601
Epoch 8/80: current_loss=0.03610 | best_loss=0.03601
Epoch 9/80: current_loss=0.03617 | best_loss=0.03601
Epoch 10/80: current_loss=0.03614 | best_loss=0.03601
Epoch 11/80: current_loss=0.03614 | best_loss=0.03601
Epoch 12/80: current_loss=0.03630 | best_loss=0.03601
Epoch 13/80: current_loss=0.03617 | best_loss=0.03601
Epoch 14/80: current_loss=0.03639 | best_loss=0.03601
Epoch 15/80: current_loss=0.03621 | best_loss=0.03601
Epoch 16/80: current_loss=0.03614 | best_loss=0.03601
Epoch 17/80: current_loss=0.03613 | best_loss=0.03601
Epoch 18/80: current_loss=0.03647 | best_loss=0.03601
Epoch 19/80: current_loss=0.03649 | best_loss=0.03601
Epoch 20/80: current_loss=0.03660 | best_loss=0.03601
Epoch 21/80: current_loss=0.03623 | best_loss=0.03601
Epoch 22/80: current_loss=0.03618 | best_loss=0.03601
Early Stopping at epoch 22
      explained_var=0.00452 | mse_loss=0.03572
----------------------------------------------
Average early_stopping_point: 6| avg_exp_var=0.00750| avg_loss=0.03516
----------------------------------------------


----------------------------------------------
Params for Trial 49
{'learning_rate': 1e-05, 'weight_decay': 5.420346799296649e-06, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.15342 | best_loss=0.15342
Epoch 1/80: current_loss=0.12932 | best_loss=0.12932
Epoch 2/80: current_loss=0.10855 | best_loss=0.10855
Epoch 3/80: current_loss=0.09106 | best_loss=0.09106
Epoch 4/80: current_loss=0.07652 | best_loss=0.07652
Epoch 5/80: current_loss=0.06535 | best_loss=0.06535
Epoch 6/80: current_loss=0.05726 | best_loss=0.05726
Epoch 7/80: current_loss=0.05170 | best_loss=0.05170
Epoch 8/80: current_loss=0.04847 | best_loss=0.04847
Epoch 9/80: current_loss=0.04659 | best_loss=0.04659
Epoch 10/80: current_loss=0.04570 | best_loss=0.04570
Epoch 11/80: current_loss=0.04506 | best_loss=0.04506
Epoch 12/80: current_loss=0.04478 | best_loss=0.04478
Epoch 13/80: current_loss=0.04458 | best_loss=0.04458
Epoch 14/80: current_loss=0.04437 | best_loss=0.04437
Epoch 15/80: current_loss=0.04416 | best_loss=0.04416
Epoch 16/80: current_loss=0.04402 | best_loss=0.04402
Epoch 17/80: current_loss=0.04393 | best_loss=0.04393
Epoch 18/80: current_loss=0.04381 | best_loss=0.04381
Epoch 19/80: current_loss=0.04365 | best_loss=0.04365
Epoch 20/80: current_loss=0.04352 | best_loss=0.04352
Epoch 21/80: current_loss=0.04334 | best_loss=0.04334
Epoch 22/80: current_loss=0.04321 | best_loss=0.04321
Epoch 23/80: current_loss=0.04312 | best_loss=0.04312
Epoch 24/80: current_loss=0.04301 | best_loss=0.04301
Epoch 25/80: current_loss=0.04283 | best_loss=0.04283
Epoch 26/80: current_loss=0.04276 | best_loss=0.04276
Epoch 27/80: current_loss=0.04265 | best_loss=0.04265
Epoch 28/80: current_loss=0.04254 | best_loss=0.04254
Epoch 29/80: current_loss=0.04248 | best_loss=0.04248
Epoch 30/80: current_loss=0.04244 | best_loss=0.04244
Epoch 31/80: current_loss=0.04245 | best_loss=0.04244
Epoch 32/80: current_loss=0.04232 | best_loss=0.04232
Epoch 33/80: current_loss=0.04221 | best_loss=0.04221
Epoch 34/80: current_loss=0.04192 | best_loss=0.04192
Epoch 35/80: current_loss=0.04184 | best_loss=0.04184
Epoch 36/80: current_loss=0.04165 | best_loss=0.04165
Epoch 37/80: current_loss=0.04161 | best_loss=0.04161
Epoch 38/80: current_loss=0.04154 | best_loss=0.04154
Epoch 39/80: current_loss=0.04145 | best_loss=0.04145
Epoch 40/80: current_loss=0.04135 | best_loss=0.04135
Epoch 41/80: current_loss=0.04129 | best_loss=0.04129
Epoch 42/80: current_loss=0.04124 | best_loss=0.04124
Epoch 43/80: current_loss=0.04115 | best_loss=0.04115
Epoch 44/80: current_loss=0.04105 | best_loss=0.04105
Epoch 45/80: current_loss=0.04101 | best_loss=0.04101
Epoch 46/80: current_loss=0.04094 | best_loss=0.04094
Epoch 47/80: current_loss=0.04096 | best_loss=0.04094
Epoch 48/80: current_loss=0.04085 | best_loss=0.04085
Epoch 49/80: current_loss=0.04076 | best_loss=0.04076
Epoch 50/80: current_loss=0.04077 | best_loss=0.04076
Epoch 51/80: current_loss=0.04079 | best_loss=0.04076
Epoch 52/80: current_loss=0.04067 | best_loss=0.04067
Epoch 53/80: current_loss=0.04048 | best_loss=0.04048
Epoch 54/80: current_loss=0.04038 | best_loss=0.04038
Epoch 55/80: current_loss=0.04036 | best_loss=0.04036
Epoch 56/80: current_loss=0.04037 | best_loss=0.04036
Epoch 57/80: current_loss=0.04045 | best_loss=0.04036
Epoch 58/80: current_loss=0.04034 | best_loss=0.04034
Epoch 59/80: current_loss=0.04025 | best_loss=0.04025
Epoch 60/80: current_loss=0.04023 | best_loss=0.04023
Epoch 61/80: current_loss=0.04016 | best_loss=0.04016
Epoch 62/80: current_loss=0.04009 | best_loss=0.04009
Epoch 63/80: current_loss=0.03997 | best_loss=0.03997
Epoch 64/80: current_loss=0.03991 | best_loss=0.03991
Epoch 65/80: current_loss=0.03987 | best_loss=0.03987
Epoch 66/80: current_loss=0.03989 | best_loss=0.03987
Epoch 67/80: current_loss=0.03991 | best_loss=0.03987
Epoch 68/80: current_loss=0.03991 | best_loss=0.03987
Epoch 69/80: current_loss=0.03981 | best_loss=0.03981
Epoch 70/80: current_loss=0.03978 | best_loss=0.03978
Epoch 71/80: current_loss=0.03968 | best_loss=0.03968
Epoch 72/80: current_loss=0.03962 | best_loss=0.03962
Epoch 73/80: current_loss=0.03964 | best_loss=0.03962
Epoch 74/80: current_loss=0.03958 | best_loss=0.03958
Epoch 75/80: current_loss=0.03955 | best_loss=0.03955
Epoch 76/80: current_loss=0.03951 | best_loss=0.03951
Epoch 77/80: current_loss=0.03948 | best_loss=0.03948
Epoch 78/80: current_loss=0.03949 | best_loss=0.03948
Epoch 79/80: current_loss=0.03941 | best_loss=0.03941
      explained_var=-0.02631 | mse_loss=0.04050

----------------------------------------------
Params for Trial 50
{'learning_rate': 0.001, 'weight_decay': 0.0019218232438353256, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03845 | best_loss=0.03845
Epoch 1/80: current_loss=0.03803 | best_loss=0.03803
Epoch 2/80: current_loss=0.04021 | best_loss=0.03803
Epoch 3/80: current_loss=0.03776 | best_loss=0.03776
Epoch 4/80: current_loss=0.03983 | best_loss=0.03776
Epoch 5/80: current_loss=0.03992 | best_loss=0.03776
Epoch 6/80: current_loss=0.03778 | best_loss=0.03776
Epoch 7/80: current_loss=0.03802 | best_loss=0.03776
Epoch 8/80: current_loss=0.03767 | best_loss=0.03767
Epoch 9/80: current_loss=0.03763 | best_loss=0.03763
Epoch 10/80: current_loss=0.03771 | best_loss=0.03763
Epoch 11/80: current_loss=0.03762 | best_loss=0.03762
Epoch 12/80: current_loss=0.03773 | best_loss=0.03762
Epoch 13/80: current_loss=0.03781 | best_loss=0.03762
Epoch 14/80: current_loss=0.04000 | best_loss=0.03762
Epoch 15/80: current_loss=0.03832 | best_loss=0.03762
Epoch 16/80: current_loss=0.03837 | best_loss=0.03762
Epoch 17/80: current_loss=0.03777 | best_loss=0.03762
Epoch 18/80: current_loss=0.03925 | best_loss=0.03762
Epoch 19/80: current_loss=0.03774 | best_loss=0.03762
Epoch 20/80: current_loss=0.03866 | best_loss=0.03762
Epoch 21/80: current_loss=0.03743 | best_loss=0.03743
Epoch 22/80: current_loss=0.03758 | best_loss=0.03743
Epoch 23/80: current_loss=0.03778 | best_loss=0.03743
Epoch 24/80: current_loss=0.03777 | best_loss=0.03743
Epoch 25/80: current_loss=0.03822 | best_loss=0.03743
Epoch 26/80: current_loss=0.03763 | best_loss=0.03743
Epoch 27/80: current_loss=0.03815 | best_loss=0.03743
Epoch 28/80: current_loss=0.03778 | best_loss=0.03743
Epoch 29/80: current_loss=0.03891 | best_loss=0.03743
Epoch 30/80: current_loss=0.03778 | best_loss=0.03743
Epoch 31/80: current_loss=0.03762 | best_loss=0.03743
Epoch 32/80: current_loss=0.03759 | best_loss=0.03743
Epoch 33/80: current_loss=0.03769 | best_loss=0.03743
Epoch 34/80: current_loss=0.03765 | best_loss=0.03743
Epoch 35/80: current_loss=0.03768 | best_loss=0.03743
Epoch 36/80: current_loss=0.03911 | best_loss=0.03743
Epoch 37/80: current_loss=0.03792 | best_loss=0.03743
Epoch 38/80: current_loss=0.03757 | best_loss=0.03743
Epoch 39/80: current_loss=0.03769 | best_loss=0.03743
Epoch 40/80: current_loss=0.03766 | best_loss=0.03743
Epoch 41/80: current_loss=0.03764 | best_loss=0.03743
Early Stopping at epoch 41
      explained_var=0.02060 | mse_loss=0.03830
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04173 | best_loss=0.04173
Epoch 1/80: current_loss=0.04226 | best_loss=0.04173
Epoch 2/80: current_loss=0.04181 | best_loss=0.04173
Epoch 3/80: current_loss=0.04166 | best_loss=0.04166
Epoch 4/80: current_loss=0.04243 | best_loss=0.04166
Epoch 5/80: current_loss=0.04208 | best_loss=0.04166
Epoch 6/80: current_loss=0.04196 | best_loss=0.04166
Epoch 7/80: current_loss=0.04189 | best_loss=0.04166
Epoch 8/80: current_loss=0.04185 | best_loss=0.04166
Epoch 9/80: current_loss=0.04181 | best_loss=0.04166
Epoch 10/80: current_loss=0.04229 | best_loss=0.04166
Epoch 11/80: current_loss=0.04302 | best_loss=0.04166
Epoch 12/80: current_loss=0.04179 | best_loss=0.04166
Epoch 13/80: current_loss=0.04226 | best_loss=0.04166
Epoch 14/80: current_loss=0.04189 | best_loss=0.04166
Epoch 15/80: current_loss=0.04191 | best_loss=0.04166
Epoch 16/80: current_loss=0.04213 | best_loss=0.04166
Epoch 17/80: current_loss=0.04175 | best_loss=0.04166
Epoch 18/80: current_loss=0.04197 | best_loss=0.04166
Epoch 19/80: current_loss=0.04185 | best_loss=0.04166
Epoch 20/80: current_loss=0.04234 | best_loss=0.04166
Epoch 21/80: current_loss=0.04211 | best_loss=0.04166
Epoch 22/80: current_loss=0.04186 | best_loss=0.04166
Epoch 23/80: current_loss=0.04196 | best_loss=0.04166
Early Stopping at epoch 23
      explained_var=0.02546 | mse_loss=0.04020
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02899 | best_loss=0.02899
Epoch 1/80: current_loss=0.02864 | best_loss=0.02864
Epoch 2/80: current_loss=0.02817 | best_loss=0.02817
Epoch 3/80: current_loss=0.02793 | best_loss=0.02793
Epoch 4/80: current_loss=0.02796 | best_loss=0.02793
Epoch 5/80: current_loss=0.02829 | best_loss=0.02793
Epoch 6/80: current_loss=0.02839 | best_loss=0.02793
Epoch 7/80: current_loss=0.02794 | best_loss=0.02793
Epoch 8/80: current_loss=0.02809 | best_loss=0.02793
Epoch 9/80: current_loss=0.02870 | best_loss=0.02793
Epoch 10/80: current_loss=0.02808 | best_loss=0.02793
Epoch 11/80: current_loss=0.02925 | best_loss=0.02793
Epoch 12/80: current_loss=0.02867 | best_loss=0.02793
Epoch 13/80: current_loss=0.02800 | best_loss=0.02793
Epoch 14/80: current_loss=0.02850 | best_loss=0.02793
Epoch 15/80: current_loss=0.02844 | best_loss=0.02793
Epoch 16/80: current_loss=0.02802 | best_loss=0.02793
Epoch 17/80: current_loss=0.02815 | best_loss=0.02793
Epoch 18/80: current_loss=0.02798 | best_loss=0.02793
Epoch 19/80: current_loss=0.02828 | best_loss=0.02793
Epoch 20/80: current_loss=0.02809 | best_loss=0.02793
Epoch 21/80: current_loss=0.02793 | best_loss=0.02793
Epoch 22/80: current_loss=0.02799 | best_loss=0.02793
Epoch 23/80: current_loss=0.02805 | best_loss=0.02793
Epoch 24/80: current_loss=0.02838 | best_loss=0.02793
Epoch 25/80: current_loss=0.02795 | best_loss=0.02793
Epoch 26/80: current_loss=0.02818 | best_loss=0.02793
Epoch 27/80: current_loss=0.02795 | best_loss=0.02793
Epoch 28/80: current_loss=0.02797 | best_loss=0.02793
Epoch 29/80: current_loss=0.02793 | best_loss=0.02793
Epoch 30/80: current_loss=0.02801 | best_loss=0.02793
Epoch 31/80: current_loss=0.02871 | best_loss=0.02793
Epoch 32/80: current_loss=0.02834 | best_loss=0.02793
Epoch 33/80: current_loss=0.02808 | best_loss=0.02793
Epoch 34/80: current_loss=0.02834 | best_loss=0.02793
Epoch 35/80: current_loss=0.02906 | best_loss=0.02793
Epoch 36/80: current_loss=0.02800 | best_loss=0.02793
Epoch 37/80: current_loss=0.02824 | best_loss=0.02793
Epoch 38/80: current_loss=0.02896 | best_loss=0.02793
Epoch 39/80: current_loss=0.02944 | best_loss=0.02793
Epoch 40/80: current_loss=0.02789 | best_loss=0.02789
Epoch 41/80: current_loss=0.02804 | best_loss=0.02789
Epoch 42/80: current_loss=0.02865 | best_loss=0.02789
Epoch 43/80: current_loss=0.02798 | best_loss=0.02789
Epoch 44/80: current_loss=0.02830 | best_loss=0.02789
Epoch 45/80: current_loss=0.02824 | best_loss=0.02789
Epoch 46/80: current_loss=0.02818 | best_loss=0.02789
Epoch 47/80: current_loss=0.02829 | best_loss=0.02789
Epoch 48/80: current_loss=0.02845 | best_loss=0.02789
Epoch 49/80: current_loss=0.02790 | best_loss=0.02789
Epoch 50/80: current_loss=0.02812 | best_loss=0.02789
Epoch 51/80: current_loss=0.02814 | best_loss=0.02789
Epoch 52/80: current_loss=0.02813 | best_loss=0.02789
Epoch 53/80: current_loss=0.02798 | best_loss=0.02789
Epoch 54/80: current_loss=0.02814 | best_loss=0.02789
Epoch 55/80: current_loss=0.02881 | best_loss=0.02789
Epoch 56/80: current_loss=0.02811 | best_loss=0.02789
Epoch 57/80: current_loss=0.02829 | best_loss=0.02789
Epoch 58/80: current_loss=0.02811 | best_loss=0.02789
Epoch 59/80: current_loss=0.02815 | best_loss=0.02789
Epoch 60/80: current_loss=0.02816 | best_loss=0.02789
Early Stopping at epoch 60
      explained_var=0.00093 | mse_loss=0.02833
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03334 | best_loss=0.03334
Epoch 1/80: current_loss=0.03285 | best_loss=0.03285
Epoch 2/80: current_loss=0.03286 | best_loss=0.03285
Epoch 3/80: current_loss=0.03285 | best_loss=0.03285
Epoch 4/80: current_loss=0.03290 | best_loss=0.03285
Epoch 5/80: current_loss=0.03297 | best_loss=0.03285
Epoch 6/80: current_loss=0.03288 | best_loss=0.03285
Epoch 7/80: current_loss=0.03287 | best_loss=0.03285
Epoch 8/80: current_loss=0.03298 | best_loss=0.03285
Epoch 9/80: current_loss=0.03297 | best_loss=0.03285
Epoch 10/80: current_loss=0.03296 | best_loss=0.03285
Epoch 11/80: current_loss=0.03286 | best_loss=0.03285
Epoch 12/80: current_loss=0.03288 | best_loss=0.03285
Epoch 13/80: current_loss=0.03286 | best_loss=0.03285
Epoch 14/80: current_loss=0.03289 | best_loss=0.03285
Epoch 15/80: current_loss=0.03283 | best_loss=0.03283
Epoch 16/80: current_loss=0.03296 | best_loss=0.03283
Epoch 17/80: current_loss=0.03279 | best_loss=0.03279
Epoch 18/80: current_loss=0.03280 | best_loss=0.03279
Epoch 19/80: current_loss=0.03328 | best_loss=0.03279
Epoch 20/80: current_loss=0.03286 | best_loss=0.03279
Epoch 21/80: current_loss=0.03283 | best_loss=0.03279
Epoch 22/80: current_loss=0.03283 | best_loss=0.03279
Epoch 23/80: current_loss=0.03289 | best_loss=0.03279
Epoch 24/80: current_loss=0.03284 | best_loss=0.03279
Epoch 25/80: current_loss=0.03303 | best_loss=0.03279
Epoch 26/80: current_loss=0.03296 | best_loss=0.03279
Epoch 27/80: current_loss=0.03290 | best_loss=0.03279
Epoch 28/80: current_loss=0.03295 | best_loss=0.03279
Epoch 29/80: current_loss=0.03285 | best_loss=0.03279
Epoch 30/80: current_loss=0.03278 | best_loss=0.03278
Epoch 31/80: current_loss=0.03294 | best_loss=0.03278
Epoch 32/80: current_loss=0.03282 | best_loss=0.03278
Epoch 33/80: current_loss=0.03280 | best_loss=0.03278
Epoch 34/80: current_loss=0.03337 | best_loss=0.03278
Epoch 35/80: current_loss=0.03278 | best_loss=0.03278
Epoch 36/80: current_loss=0.03281 | best_loss=0.03278
Epoch 37/80: current_loss=0.03283 | best_loss=0.03278
Epoch 38/80: current_loss=0.03287 | best_loss=0.03278
Epoch 39/80: current_loss=0.03283 | best_loss=0.03278
Epoch 40/80: current_loss=0.03282 | best_loss=0.03278
Epoch 41/80: current_loss=0.03284 | best_loss=0.03278
Epoch 42/80: current_loss=0.03280 | best_loss=0.03278
Epoch 43/80: current_loss=0.03289 | best_loss=0.03278
Epoch 44/80: current_loss=0.03289 | best_loss=0.03278
Epoch 45/80: current_loss=0.03286 | best_loss=0.03278
Epoch 46/80: current_loss=0.03290 | best_loss=0.03278
Epoch 47/80: current_loss=0.03287 | best_loss=0.03278
Epoch 48/80: current_loss=0.03283 | best_loss=0.03278
Epoch 49/80: current_loss=0.03301 | best_loss=0.03278
Epoch 50/80: current_loss=0.03281 | best_loss=0.03278
Epoch 51/80: current_loss=0.03282 | best_loss=0.03278
Epoch 52/80: current_loss=0.03279 | best_loss=0.03278
Epoch 53/80: current_loss=0.03285 | best_loss=0.03278
Epoch 54/80: current_loss=0.03294 | best_loss=0.03278
Epoch 55/80: current_loss=0.03288 | best_loss=0.03278
Early Stopping at epoch 55
      explained_var=-0.00090 | mse_loss=0.03266
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03557 | best_loss=0.03557
Epoch 1/80: current_loss=0.03562 | best_loss=0.03557
Epoch 2/80: current_loss=0.03557 | best_loss=0.03557
Epoch 3/80: current_loss=0.03565 | best_loss=0.03557
Epoch 4/80: current_loss=0.03558 | best_loss=0.03557
Epoch 5/80: current_loss=0.03568 | best_loss=0.03557
Epoch 6/80: current_loss=0.03562 | best_loss=0.03557
Epoch 7/80: current_loss=0.03557 | best_loss=0.03557
Epoch 8/80: current_loss=0.03559 | best_loss=0.03557
Epoch 9/80: current_loss=0.03555 | best_loss=0.03555
Epoch 10/80: current_loss=0.03576 | best_loss=0.03555
Epoch 11/80: current_loss=0.03565 | best_loss=0.03555
Epoch 12/80: current_loss=0.03565 | best_loss=0.03555
Epoch 13/80: current_loss=0.03568 | best_loss=0.03555
Epoch 14/80: current_loss=0.03569 | best_loss=0.03555
Epoch 15/80: current_loss=0.03559 | best_loss=0.03555
Epoch 16/80: current_loss=0.03569 | best_loss=0.03555
Epoch 17/80: current_loss=0.03569 | best_loss=0.03555
Epoch 18/80: current_loss=0.03569 | best_loss=0.03555
Epoch 19/80: current_loss=0.03568 | best_loss=0.03555
Epoch 20/80: current_loss=0.03567 | best_loss=0.03555
Epoch 21/80: current_loss=0.03559 | best_loss=0.03555
Epoch 22/80: current_loss=0.03557 | best_loss=0.03555
Epoch 23/80: current_loss=0.03563 | best_loss=0.03555
Epoch 24/80: current_loss=0.03568 | best_loss=0.03555
Epoch 25/80: current_loss=0.03562 | best_loss=0.03555
Epoch 26/80: current_loss=0.03559 | best_loss=0.03555
Epoch 27/80: current_loss=0.03565 | best_loss=0.03555
Epoch 28/80: current_loss=0.03568 | best_loss=0.03555
Epoch 29/80: current_loss=0.03564 | best_loss=0.03555
Early Stopping at epoch 29
      explained_var=0.01608 | mse_loss=0.03531
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.01244| avg_loss=0.03496
----------------------------------------------


----------------------------------------------
Params for Trial 51
{'learning_rate': 0.001, 'weight_decay': 0.0004473741407538023, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03848 | best_loss=0.03848
Epoch 1/80: current_loss=0.03786 | best_loss=0.03786
Epoch 2/80: current_loss=0.03864 | best_loss=0.03786
Epoch 3/80: current_loss=0.04021 | best_loss=0.03786
Epoch 4/80: current_loss=0.03833 | best_loss=0.03786
Epoch 5/80: current_loss=0.03761 | best_loss=0.03761
Epoch 6/80: current_loss=0.03760 | best_loss=0.03760
Epoch 7/80: current_loss=0.03775 | best_loss=0.03760
Epoch 8/80: current_loss=0.03776 | best_loss=0.03760
Epoch 9/80: current_loss=0.03760 | best_loss=0.03760
Epoch 10/80: current_loss=0.03765 | best_loss=0.03760
Epoch 11/80: current_loss=0.03771 | best_loss=0.03760
Epoch 12/80: current_loss=0.03972 | best_loss=0.03760
Epoch 13/80: current_loss=0.04021 | best_loss=0.03760
Epoch 14/80: current_loss=0.03830 | best_loss=0.03760
Epoch 15/80: current_loss=0.03768 | best_loss=0.03760
Epoch 16/80: current_loss=0.03753 | best_loss=0.03753
Epoch 17/80: current_loss=0.03761 | best_loss=0.03753
Epoch 18/80: current_loss=0.03884 | best_loss=0.03753
Epoch 19/80: current_loss=0.03772 | best_loss=0.03753
Epoch 20/80: current_loss=0.03792 | best_loss=0.03753
Epoch 21/80: current_loss=0.03862 | best_loss=0.03753
Epoch 22/80: current_loss=0.03752 | best_loss=0.03752
Epoch 23/80: current_loss=0.03753 | best_loss=0.03752
Epoch 24/80: current_loss=0.03809 | best_loss=0.03752
Epoch 25/80: current_loss=0.03774 | best_loss=0.03752
Epoch 26/80: current_loss=0.03821 | best_loss=0.03752
Epoch 27/80: current_loss=0.03769 | best_loss=0.03752
Epoch 28/80: current_loss=0.03758 | best_loss=0.03752
Epoch 29/80: current_loss=0.03761 | best_loss=0.03752
Epoch 30/80: current_loss=0.03740 | best_loss=0.03740
Epoch 31/80: current_loss=0.03757 | best_loss=0.03740
Epoch 32/80: current_loss=0.03751 | best_loss=0.03740
Epoch 33/80: current_loss=0.03814 | best_loss=0.03740
Epoch 34/80: current_loss=0.03786 | best_loss=0.03740
Epoch 35/80: current_loss=0.03822 | best_loss=0.03740
Epoch 36/80: current_loss=0.03767 | best_loss=0.03740
Epoch 37/80: current_loss=0.04002 | best_loss=0.03740
Epoch 38/80: current_loss=0.03863 | best_loss=0.03740
Epoch 39/80: current_loss=0.03748 | best_loss=0.03740
Epoch 40/80: current_loss=0.03778 | best_loss=0.03740
Epoch 41/80: current_loss=0.03762 | best_loss=0.03740
Epoch 42/80: current_loss=0.03746 | best_loss=0.03740
Epoch 43/80: current_loss=0.03825 | best_loss=0.03740
Epoch 44/80: current_loss=0.03751 | best_loss=0.03740
Epoch 45/80: current_loss=0.03791 | best_loss=0.03740
Epoch 46/80: current_loss=0.03749 | best_loss=0.03740
Epoch 47/80: current_loss=0.03758 | best_loss=0.03740
Epoch 48/80: current_loss=0.03759 | best_loss=0.03740
Epoch 49/80: current_loss=0.03769 | best_loss=0.03740
Epoch 50/80: current_loss=0.03784 | best_loss=0.03740
Early Stopping at epoch 50
      explained_var=0.02249 | mse_loss=0.03828
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04248 | best_loss=0.04248
Epoch 1/80: current_loss=0.04168 | best_loss=0.04168
Epoch 2/80: current_loss=0.04146 | best_loss=0.04146
Epoch 3/80: current_loss=0.04151 | best_loss=0.04146
Epoch 4/80: current_loss=0.04186 | best_loss=0.04146
Epoch 5/80: current_loss=0.04159 | best_loss=0.04146
Epoch 6/80: current_loss=0.04180 | best_loss=0.04146
Epoch 7/80: current_loss=0.04222 | best_loss=0.04146
Epoch 8/80: current_loss=0.04296 | best_loss=0.04146
Epoch 9/80: current_loss=0.04147 | best_loss=0.04146
Epoch 10/80: current_loss=0.04147 | best_loss=0.04146
Epoch 11/80: current_loss=0.04160 | best_loss=0.04146
Epoch 12/80: current_loss=0.04161 | best_loss=0.04146
Epoch 13/80: current_loss=0.04175 | best_loss=0.04146
Epoch 14/80: current_loss=0.04146 | best_loss=0.04146
Epoch 15/80: current_loss=0.04193 | best_loss=0.04146
Epoch 16/80: current_loss=0.04241 | best_loss=0.04146
Epoch 17/80: current_loss=0.04167 | best_loss=0.04146
Epoch 18/80: current_loss=0.04229 | best_loss=0.04146
Epoch 19/80: current_loss=0.04187 | best_loss=0.04146
Epoch 20/80: current_loss=0.04163 | best_loss=0.04146
Epoch 21/80: current_loss=0.04242 | best_loss=0.04146
Epoch 22/80: current_loss=0.04172 | best_loss=0.04146
Early Stopping at epoch 22
      explained_var=0.03001 | mse_loss=0.04001
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02814 | best_loss=0.02814
Epoch 1/80: current_loss=0.02827 | best_loss=0.02814
Epoch 2/80: current_loss=0.02950 | best_loss=0.02814
Epoch 3/80: current_loss=0.02812 | best_loss=0.02812
Epoch 4/80: current_loss=0.02826 | best_loss=0.02812
Epoch 5/80: current_loss=0.02809 | best_loss=0.02809
Epoch 6/80: current_loss=0.02843 | best_loss=0.02809
Epoch 7/80: current_loss=0.02881 | best_loss=0.02809
Epoch 8/80: current_loss=0.02830 | best_loss=0.02809
Epoch 9/80: current_loss=0.02841 | best_loss=0.02809
Epoch 10/80: current_loss=0.02805 | best_loss=0.02805
Epoch 11/80: current_loss=0.02799 | best_loss=0.02799
Epoch 12/80: current_loss=0.02823 | best_loss=0.02799
Epoch 13/80: current_loss=0.02801 | best_loss=0.02799
Epoch 14/80: current_loss=0.02855 | best_loss=0.02799
Epoch 15/80: current_loss=0.02801 | best_loss=0.02799
Epoch 16/80: current_loss=0.02829 | best_loss=0.02799
Epoch 17/80: current_loss=0.02835 | best_loss=0.02799
Epoch 18/80: current_loss=0.02847 | best_loss=0.02799
Epoch 19/80: current_loss=0.02797 | best_loss=0.02797
Epoch 20/80: current_loss=0.02854 | best_loss=0.02797
Epoch 21/80: current_loss=0.02819 | best_loss=0.02797
Epoch 22/80: current_loss=0.02799 | best_loss=0.02797
Epoch 23/80: current_loss=0.02843 | best_loss=0.02797
Epoch 24/80: current_loss=0.02854 | best_loss=0.02797
Epoch 25/80: current_loss=0.02825 | best_loss=0.02797
Epoch 26/80: current_loss=0.02864 | best_loss=0.02797
Epoch 27/80: current_loss=0.02815 | best_loss=0.02797
Epoch 28/80: current_loss=0.02816 | best_loss=0.02797
Epoch 29/80: current_loss=0.02795 | best_loss=0.02795
Epoch 30/80: current_loss=0.02853 | best_loss=0.02795
Epoch 31/80: current_loss=0.02832 | best_loss=0.02795
Epoch 32/80: current_loss=0.02811 | best_loss=0.02795
Epoch 33/80: current_loss=0.02812 | best_loss=0.02795
Epoch 34/80: current_loss=0.02851 | best_loss=0.02795
Epoch 35/80: current_loss=0.02841 | best_loss=0.02795
Epoch 36/80: current_loss=0.02820 | best_loss=0.02795
Epoch 37/80: current_loss=0.02810 | best_loss=0.02795
Epoch 38/80: current_loss=0.02824 | best_loss=0.02795
Epoch 39/80: current_loss=0.02831 | best_loss=0.02795
Epoch 40/80: current_loss=0.02843 | best_loss=0.02795
Epoch 41/80: current_loss=0.02811 | best_loss=0.02795
Epoch 42/80: current_loss=0.02844 | best_loss=0.02795
Epoch 43/80: current_loss=0.02823 | best_loss=0.02795
Epoch 44/80: current_loss=0.02827 | best_loss=0.02795
Epoch 45/80: current_loss=0.02843 | best_loss=0.02795
Epoch 46/80: current_loss=0.02840 | best_loss=0.02795
Epoch 47/80: current_loss=0.02856 | best_loss=0.02795
Epoch 48/80: current_loss=0.02796 | best_loss=0.02795
Epoch 49/80: current_loss=0.02830 | best_loss=0.02795
Early Stopping at epoch 49
      explained_var=-0.00034 | mse_loss=0.02842
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03309 | best_loss=0.03309
Epoch 1/80: current_loss=0.03298 | best_loss=0.03298
Epoch 2/80: current_loss=0.03295 | best_loss=0.03295
Epoch 3/80: current_loss=0.03322 | best_loss=0.03295
Epoch 4/80: current_loss=0.03306 | best_loss=0.03295
Epoch 5/80: current_loss=0.03310 | best_loss=0.03295
Epoch 6/80: current_loss=0.03316 | best_loss=0.03295
Epoch 7/80: current_loss=0.03293 | best_loss=0.03293
Epoch 8/80: current_loss=0.03295 | best_loss=0.03293
Epoch 9/80: current_loss=0.03310 | best_loss=0.03293
Epoch 10/80: current_loss=0.03321 | best_loss=0.03293
Epoch 11/80: current_loss=0.03281 | best_loss=0.03281
Epoch 12/80: current_loss=0.03286 | best_loss=0.03281
Epoch 13/80: current_loss=0.03300 | best_loss=0.03281
Epoch 14/80: current_loss=0.03292 | best_loss=0.03281
Epoch 15/80: current_loss=0.03305 | best_loss=0.03281
Epoch 16/80: current_loss=0.03337 | best_loss=0.03281
Epoch 17/80: current_loss=0.03304 | best_loss=0.03281
Epoch 18/80: current_loss=0.03306 | best_loss=0.03281
Epoch 19/80: current_loss=0.03292 | best_loss=0.03281
Epoch 20/80: current_loss=0.03310 | best_loss=0.03281
Epoch 21/80: current_loss=0.03292 | best_loss=0.03281
Epoch 22/80: current_loss=0.03291 | best_loss=0.03281
Epoch 23/80: current_loss=0.03288 | best_loss=0.03281
Epoch 24/80: current_loss=0.03292 | best_loss=0.03281
Epoch 25/80: current_loss=0.03293 | best_loss=0.03281
Epoch 26/80: current_loss=0.03291 | best_loss=0.03281
Epoch 27/80: current_loss=0.03286 | best_loss=0.03281
Epoch 28/80: current_loss=0.03335 | best_loss=0.03281
Epoch 29/80: current_loss=0.03293 | best_loss=0.03281
Epoch 30/80: current_loss=0.03318 | best_loss=0.03281
Epoch 31/80: current_loss=0.03291 | best_loss=0.03281
Early Stopping at epoch 31
      explained_var=-0.00234 | mse_loss=0.03267
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03536 | best_loss=0.03536
Epoch 1/80: current_loss=0.03525 | best_loss=0.03525
Epoch 2/80: current_loss=0.03555 | best_loss=0.03525
Epoch 3/80: current_loss=0.03553 | best_loss=0.03525
Epoch 4/80: current_loss=0.03552 | best_loss=0.03525
Epoch 5/80: current_loss=0.03546 | best_loss=0.03525
Epoch 6/80: current_loss=0.03547 | best_loss=0.03525
Epoch 7/80: current_loss=0.03557 | best_loss=0.03525
Epoch 8/80: current_loss=0.03559 | best_loss=0.03525
Epoch 9/80: current_loss=0.03550 | best_loss=0.03525
Epoch 10/80: current_loss=0.03554 | best_loss=0.03525
Epoch 11/80: current_loss=0.03542 | best_loss=0.03525
Epoch 12/80: current_loss=0.03549 | best_loss=0.03525
Epoch 13/80: current_loss=0.03531 | best_loss=0.03525
Epoch 14/80: current_loss=0.03546 | best_loss=0.03525
Epoch 15/80: current_loss=0.03555 | best_loss=0.03525
Epoch 16/80: current_loss=0.03554 | best_loss=0.03525
Epoch 17/80: current_loss=0.03553 | best_loss=0.03525
Epoch 18/80: current_loss=0.03544 | best_loss=0.03525
Epoch 19/80: current_loss=0.03550 | best_loss=0.03525
Epoch 20/80: current_loss=0.03547 | best_loss=0.03525
Epoch 21/80: current_loss=0.03533 | best_loss=0.03525
Early Stopping at epoch 21
      explained_var=0.02456 | mse_loss=0.03500
----------------------------------------------
Average early_stopping_point: 14| avg_exp_var=0.01488| avg_loss=0.03488
----------------------------------------------


----------------------------------------------
Params for Trial 52
{'learning_rate': 0.001, 'weight_decay': 0.0010749586743635344, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04068 | best_loss=0.04068
Epoch 1/80: current_loss=0.03842 | best_loss=0.03842
Epoch 2/80: current_loss=0.03788 | best_loss=0.03788
Epoch 3/80: current_loss=0.03911 | best_loss=0.03788
Epoch 4/80: current_loss=0.03942 | best_loss=0.03788
Epoch 5/80: current_loss=0.03810 | best_loss=0.03788
Epoch 6/80: current_loss=0.03769 | best_loss=0.03769
Epoch 7/80: current_loss=0.03794 | best_loss=0.03769
Epoch 8/80: current_loss=0.04088 | best_loss=0.03769
Epoch 9/80: current_loss=0.03811 | best_loss=0.03769
Epoch 10/80: current_loss=0.03817 | best_loss=0.03769
Epoch 11/80: current_loss=0.03769 | best_loss=0.03769
Epoch 12/80: current_loss=0.03758 | best_loss=0.03758
Epoch 13/80: current_loss=0.03801 | best_loss=0.03758
Epoch 14/80: current_loss=0.03781 | best_loss=0.03758
Epoch 15/80: current_loss=0.03793 | best_loss=0.03758
Epoch 16/80: current_loss=0.03756 | best_loss=0.03756
Epoch 17/80: current_loss=0.03771 | best_loss=0.03756
Epoch 18/80: current_loss=0.03748 | best_loss=0.03748
Epoch 19/80: current_loss=0.03777 | best_loss=0.03748
Epoch 20/80: current_loss=0.03812 | best_loss=0.03748
Epoch 21/80: current_loss=0.03764 | best_loss=0.03748
Epoch 22/80: current_loss=0.03853 | best_loss=0.03748
Epoch 23/80: current_loss=0.03736 | best_loss=0.03736
Epoch 24/80: current_loss=0.03755 | best_loss=0.03736
Epoch 25/80: current_loss=0.03772 | best_loss=0.03736
Epoch 26/80: current_loss=0.03817 | best_loss=0.03736
Epoch 27/80: current_loss=0.03755 | best_loss=0.03736
Epoch 28/80: current_loss=0.03826 | best_loss=0.03736
Epoch 29/80: current_loss=0.03784 | best_loss=0.03736
Epoch 30/80: current_loss=0.03769 | best_loss=0.03736
Epoch 31/80: current_loss=0.03796 | best_loss=0.03736
Epoch 32/80: current_loss=0.03786 | best_loss=0.03736
Epoch 33/80: current_loss=0.03799 | best_loss=0.03736
Epoch 34/80: current_loss=0.03769 | best_loss=0.03736
Epoch 35/80: current_loss=0.03752 | best_loss=0.03736
Epoch 36/80: current_loss=0.03759 | best_loss=0.03736
Epoch 37/80: current_loss=0.03841 | best_loss=0.03736
Epoch 38/80: current_loss=0.03889 | best_loss=0.03736
Epoch 39/80: current_loss=0.03738 | best_loss=0.03736
Epoch 40/80: current_loss=0.03798 | best_loss=0.03736
Epoch 41/80: current_loss=0.03754 | best_loss=0.03736
Epoch 42/80: current_loss=0.03822 | best_loss=0.03736
Epoch 43/80: current_loss=0.03788 | best_loss=0.03736
Early Stopping at epoch 43
      explained_var=0.02272 | mse_loss=0.03822
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04224 | best_loss=0.04224
Epoch 1/80: current_loss=0.04181 | best_loss=0.04181
Epoch 2/80: current_loss=0.04166 | best_loss=0.04166
Epoch 3/80: current_loss=0.04190 | best_loss=0.04166
Epoch 4/80: current_loss=0.04140 | best_loss=0.04140
Epoch 5/80: current_loss=0.04174 | best_loss=0.04140
Epoch 6/80: current_loss=0.04155 | best_loss=0.04140
Epoch 7/80: current_loss=0.04200 | best_loss=0.04140
Epoch 8/80: current_loss=0.04182 | best_loss=0.04140
Epoch 9/80: current_loss=0.04174 | best_loss=0.04140
Epoch 10/80: current_loss=0.04244 | best_loss=0.04140
Epoch 11/80: current_loss=0.04304 | best_loss=0.04140
Epoch 12/80: current_loss=0.04168 | best_loss=0.04140
Epoch 13/80: current_loss=0.04174 | best_loss=0.04140
Epoch 14/80: current_loss=0.04172 | best_loss=0.04140
Epoch 15/80: current_loss=0.04176 | best_loss=0.04140
Epoch 16/80: current_loss=0.04267 | best_loss=0.04140
Epoch 17/80: current_loss=0.04177 | best_loss=0.04140
Epoch 18/80: current_loss=0.04180 | best_loss=0.04140
Epoch 19/80: current_loss=0.04259 | best_loss=0.04140
Epoch 20/80: current_loss=0.04200 | best_loss=0.04140
Epoch 21/80: current_loss=0.04183 | best_loss=0.04140
Epoch 22/80: current_loss=0.04219 | best_loss=0.04140
Epoch 23/80: current_loss=0.04196 | best_loss=0.04140
Epoch 24/80: current_loss=0.04220 | best_loss=0.04140
Early Stopping at epoch 24
      explained_var=0.03264 | mse_loss=0.03991
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02821 | best_loss=0.02821
Epoch 1/80: current_loss=0.02823 | best_loss=0.02821
Epoch 2/80: current_loss=0.02802 | best_loss=0.02802
Epoch 3/80: current_loss=0.02830 | best_loss=0.02802
Epoch 4/80: current_loss=0.02806 | best_loss=0.02802
Epoch 5/80: current_loss=0.02863 | best_loss=0.02802
Epoch 6/80: current_loss=0.02933 | best_loss=0.02802
Epoch 7/80: current_loss=0.02802 | best_loss=0.02802
Epoch 8/80: current_loss=0.02868 | best_loss=0.02802
Epoch 9/80: current_loss=0.02806 | best_loss=0.02802
Epoch 10/80: current_loss=0.02805 | best_loss=0.02802
Epoch 11/80: current_loss=0.02823 | best_loss=0.02802
Epoch 12/80: current_loss=0.02812 | best_loss=0.02802
Epoch 13/80: current_loss=0.02882 | best_loss=0.02802
Epoch 14/80: current_loss=0.03005 | best_loss=0.02802
Epoch 15/80: current_loss=0.02831 | best_loss=0.02802
Epoch 16/80: current_loss=0.02810 | best_loss=0.02802
Epoch 17/80: current_loss=0.02925 | best_loss=0.02802
Epoch 18/80: current_loss=0.02828 | best_loss=0.02802
Epoch 19/80: current_loss=0.02863 | best_loss=0.02802
Epoch 20/80: current_loss=0.02809 | best_loss=0.02802
Epoch 21/80: current_loss=0.02829 | best_loss=0.02802
Epoch 22/80: current_loss=0.02810 | best_loss=0.02802
Early Stopping at epoch 22
      explained_var=-0.00423 | mse_loss=0.02851

----------------------------------------------
Params for Trial 53
{'learning_rate': 0.001, 'weight_decay': 0.00032763080799388933, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03885 | best_loss=0.03885
Epoch 1/80: current_loss=0.03857 | best_loss=0.03857
Epoch 2/80: current_loss=0.03815 | best_loss=0.03815
Epoch 3/80: current_loss=0.03776 | best_loss=0.03776
Epoch 4/80: current_loss=0.03800 | best_loss=0.03776
Epoch 5/80: current_loss=0.03878 | best_loss=0.03776
Epoch 6/80: current_loss=0.03821 | best_loss=0.03776
Epoch 7/80: current_loss=0.03762 | best_loss=0.03762
Epoch 8/80: current_loss=0.03967 | best_loss=0.03762
Epoch 9/80: current_loss=0.03854 | best_loss=0.03762
Epoch 10/80: current_loss=0.03767 | best_loss=0.03762
Epoch 11/80: current_loss=0.03901 | best_loss=0.03762
Epoch 12/80: current_loss=0.03823 | best_loss=0.03762
Epoch 13/80: current_loss=0.03805 | best_loss=0.03762
Epoch 14/80: current_loss=0.03765 | best_loss=0.03762
Epoch 15/80: current_loss=0.03774 | best_loss=0.03762
Epoch 16/80: current_loss=0.03776 | best_loss=0.03762
Epoch 17/80: current_loss=0.03986 | best_loss=0.03762
Epoch 18/80: current_loss=0.03815 | best_loss=0.03762
Epoch 19/80: current_loss=0.03778 | best_loss=0.03762
Epoch 20/80: current_loss=0.03759 | best_loss=0.03759
Epoch 21/80: current_loss=0.03773 | best_loss=0.03759
Epoch 22/80: current_loss=0.03753 | best_loss=0.03753
Epoch 23/80: current_loss=0.03768 | best_loss=0.03753
Epoch 24/80: current_loss=0.03769 | best_loss=0.03753
Epoch 25/80: current_loss=0.03769 | best_loss=0.03753
Epoch 26/80: current_loss=0.03749 | best_loss=0.03749
Epoch 27/80: current_loss=0.03759 | best_loss=0.03749
Epoch 28/80: current_loss=0.03903 | best_loss=0.03749
Epoch 29/80: current_loss=0.03873 | best_loss=0.03749
Epoch 30/80: current_loss=0.03760 | best_loss=0.03749
Epoch 31/80: current_loss=0.03782 | best_loss=0.03749
Epoch 32/80: current_loss=0.03758 | best_loss=0.03749
Epoch 33/80: current_loss=0.03801 | best_loss=0.03749
Epoch 34/80: current_loss=0.03799 | best_loss=0.03749
Epoch 35/80: current_loss=0.03759 | best_loss=0.03749
Epoch 36/80: current_loss=0.03751 | best_loss=0.03749
Epoch 37/80: current_loss=0.03817 | best_loss=0.03749
Epoch 38/80: current_loss=0.03806 | best_loss=0.03749
Epoch 39/80: current_loss=0.03766 | best_loss=0.03749
Epoch 40/80: current_loss=0.03863 | best_loss=0.03749
Epoch 41/80: current_loss=0.03757 | best_loss=0.03749
Epoch 42/80: current_loss=0.03765 | best_loss=0.03749
Epoch 43/80: current_loss=0.03761 | best_loss=0.03749
Epoch 44/80: current_loss=0.03770 | best_loss=0.03749
Epoch 45/80: current_loss=0.03783 | best_loss=0.03749
Epoch 46/80: current_loss=0.03855 | best_loss=0.03749
Early Stopping at epoch 46
      explained_var=0.01957 | mse_loss=0.03834
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04264 | best_loss=0.04264
Epoch 1/80: current_loss=0.04266 | best_loss=0.04264
Epoch 2/80: current_loss=0.04147 | best_loss=0.04147
Epoch 3/80: current_loss=0.04176 | best_loss=0.04147
Epoch 4/80: current_loss=0.04143 | best_loss=0.04143
Epoch 5/80: current_loss=0.04170 | best_loss=0.04143
Epoch 6/80: current_loss=0.04292 | best_loss=0.04143
Epoch 7/80: current_loss=0.04204 | best_loss=0.04143
Epoch 8/80: current_loss=0.04172 | best_loss=0.04143
Epoch 9/80: current_loss=0.04192 | best_loss=0.04143
Epoch 10/80: current_loss=0.04162 | best_loss=0.04143
Epoch 11/80: current_loss=0.04268 | best_loss=0.04143
Epoch 12/80: current_loss=0.04229 | best_loss=0.04143
Epoch 13/80: current_loss=0.04255 | best_loss=0.04143
Epoch 14/80: current_loss=0.04170 | best_loss=0.04143
Epoch 15/80: current_loss=0.04193 | best_loss=0.04143
Epoch 16/80: current_loss=0.04167 | best_loss=0.04143
Epoch 17/80: current_loss=0.04156 | best_loss=0.04143
Epoch 18/80: current_loss=0.04233 | best_loss=0.04143
Epoch 19/80: current_loss=0.04221 | best_loss=0.04143
Epoch 20/80: current_loss=0.04183 | best_loss=0.04143
Epoch 21/80: current_loss=0.04151 | best_loss=0.04143
Epoch 22/80: current_loss=0.04180 | best_loss=0.04143
Epoch 23/80: current_loss=0.04183 | best_loss=0.04143
Epoch 24/80: current_loss=0.04191 | best_loss=0.04143
Early Stopping at epoch 24
      explained_var=0.03124 | mse_loss=0.03997
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02903 | best_loss=0.02903
Epoch 1/80: current_loss=0.02880 | best_loss=0.02880
Epoch 2/80: current_loss=0.02828 | best_loss=0.02828
Epoch 3/80: current_loss=0.02820 | best_loss=0.02820
Epoch 4/80: current_loss=0.02813 | best_loss=0.02813
Epoch 5/80: current_loss=0.02853 | best_loss=0.02813
Epoch 6/80: current_loss=0.02988 | best_loss=0.02813
Epoch 7/80: current_loss=0.02895 | best_loss=0.02813
Epoch 8/80: current_loss=0.02828 | best_loss=0.02813
Epoch 9/80: current_loss=0.02848 | best_loss=0.02813
Epoch 10/80: current_loss=0.02820 | best_loss=0.02813
Epoch 11/80: current_loss=0.02831 | best_loss=0.02813
Epoch 12/80: current_loss=0.02816 | best_loss=0.02813
Epoch 13/80: current_loss=0.02812 | best_loss=0.02812
Epoch 14/80: current_loss=0.02810 | best_loss=0.02810
Epoch 15/80: current_loss=0.02811 | best_loss=0.02810
Epoch 16/80: current_loss=0.02864 | best_loss=0.02810
Epoch 17/80: current_loss=0.02847 | best_loss=0.02810
Epoch 18/80: current_loss=0.02879 | best_loss=0.02810
Epoch 19/80: current_loss=0.02914 | best_loss=0.02810
Epoch 20/80: current_loss=0.02834 | best_loss=0.02810
Epoch 21/80: current_loss=0.02876 | best_loss=0.02810
Epoch 22/80: current_loss=0.02870 | best_loss=0.02810
Epoch 23/80: current_loss=0.02812 | best_loss=0.02810
Epoch 24/80: current_loss=0.02829 | best_loss=0.02810
Epoch 25/80: current_loss=0.02841 | best_loss=0.02810
Epoch 26/80: current_loss=0.02851 | best_loss=0.02810
Epoch 27/80: current_loss=0.02817 | best_loss=0.02810
Epoch 28/80: current_loss=0.02804 | best_loss=0.02804
Epoch 29/80: current_loss=0.02844 | best_loss=0.02804
Epoch 30/80: current_loss=0.02833 | best_loss=0.02804
Epoch 31/80: current_loss=0.02800 | best_loss=0.02800
Epoch 32/80: current_loss=0.03085 | best_loss=0.02800
Epoch 33/80: current_loss=0.02839 | best_loss=0.02800
Epoch 34/80: current_loss=0.02805 | best_loss=0.02800
Epoch 35/80: current_loss=0.02810 | best_loss=0.02800
Epoch 36/80: current_loss=0.02854 | best_loss=0.02800
Epoch 37/80: current_loss=0.02830 | best_loss=0.02800
Epoch 38/80: current_loss=0.02850 | best_loss=0.02800
Epoch 39/80: current_loss=0.02828 | best_loss=0.02800
Epoch 40/80: current_loss=0.02841 | best_loss=0.02800
Epoch 41/80: current_loss=0.02873 | best_loss=0.02800
Epoch 42/80: current_loss=0.02872 | best_loss=0.02800
Epoch 43/80: current_loss=0.02835 | best_loss=0.02800
Epoch 44/80: current_loss=0.02883 | best_loss=0.02800
Epoch 45/80: current_loss=0.02820 | best_loss=0.02800
Epoch 46/80: current_loss=0.02846 | best_loss=0.02800
Epoch 47/80: current_loss=0.02842 | best_loss=0.02800
Epoch 48/80: current_loss=0.02848 | best_loss=0.02800
Epoch 49/80: current_loss=0.02825 | best_loss=0.02800
Epoch 50/80: current_loss=0.02842 | best_loss=0.02800
Epoch 51/80: current_loss=0.02860 | best_loss=0.02800
Early Stopping at epoch 51
      explained_var=-0.00449 | mse_loss=0.02849

----------------------------------------------
Params for Trial 54
{'learning_rate': 0.001, 'weight_decay': 0.0008127037762370201, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04247 | best_loss=0.04247
Epoch 1/80: current_loss=0.04258 | best_loss=0.04247
Epoch 2/80: current_loss=0.03861 | best_loss=0.03861
Epoch 3/80: current_loss=0.03908 | best_loss=0.03861
Epoch 4/80: current_loss=0.03835 | best_loss=0.03835
Epoch 5/80: current_loss=0.03788 | best_loss=0.03788
Epoch 6/80: current_loss=0.03835 | best_loss=0.03788
Epoch 7/80: current_loss=0.03808 | best_loss=0.03788
Epoch 8/80: current_loss=0.03921 | best_loss=0.03788
Epoch 9/80: current_loss=0.04241 | best_loss=0.03788
Epoch 10/80: current_loss=0.03815 | best_loss=0.03788
Epoch 11/80: current_loss=0.03832 | best_loss=0.03788
Epoch 12/80: current_loss=0.03771 | best_loss=0.03771
Epoch 13/80: current_loss=0.03747 | best_loss=0.03747
Epoch 14/80: current_loss=0.03825 | best_loss=0.03747
Epoch 15/80: current_loss=0.03851 | best_loss=0.03747
Epoch 16/80: current_loss=0.03832 | best_loss=0.03747
Epoch 17/80: current_loss=0.03871 | best_loss=0.03747
Epoch 18/80: current_loss=0.03788 | best_loss=0.03747
Epoch 19/80: current_loss=0.03765 | best_loss=0.03747
Epoch 20/80: current_loss=0.03827 | best_loss=0.03747
Epoch 21/80: current_loss=0.03773 | best_loss=0.03747
Epoch 22/80: current_loss=0.03775 | best_loss=0.03747
Epoch 23/80: current_loss=0.03755 | best_loss=0.03747
Epoch 24/80: current_loss=0.03755 | best_loss=0.03747
Epoch 25/80: current_loss=0.03766 | best_loss=0.03747
Epoch 26/80: current_loss=0.03762 | best_loss=0.03747
Epoch 27/80: current_loss=0.03798 | best_loss=0.03747
Epoch 28/80: current_loss=0.03890 | best_loss=0.03747
Epoch 29/80: current_loss=0.03746 | best_loss=0.03746
Epoch 30/80: current_loss=0.03758 | best_loss=0.03746
Epoch 31/80: current_loss=0.03756 | best_loss=0.03746
Epoch 32/80: current_loss=0.03751 | best_loss=0.03746
Epoch 33/80: current_loss=0.03752 | best_loss=0.03746
Epoch 34/80: current_loss=0.03761 | best_loss=0.03746
Epoch 35/80: current_loss=0.03803 | best_loss=0.03746
Epoch 36/80: current_loss=0.03763 | best_loss=0.03746
Epoch 37/80: current_loss=0.03771 | best_loss=0.03746
Epoch 38/80: current_loss=0.03763 | best_loss=0.03746
Epoch 39/80: current_loss=0.03818 | best_loss=0.03746
Epoch 40/80: current_loss=0.03822 | best_loss=0.03746
Epoch 41/80: current_loss=0.03859 | best_loss=0.03746
Epoch 42/80: current_loss=0.03757 | best_loss=0.03746
Epoch 43/80: current_loss=0.03766 | best_loss=0.03746
Epoch 44/80: current_loss=0.03816 | best_loss=0.03746
Epoch 45/80: current_loss=0.03758 | best_loss=0.03746
Epoch 46/80: current_loss=0.03754 | best_loss=0.03746
Epoch 47/80: current_loss=0.03759 | best_loss=0.03746
Epoch 48/80: current_loss=0.03757 | best_loss=0.03746
Epoch 49/80: current_loss=0.03773 | best_loss=0.03746
Early Stopping at epoch 49
      explained_var=0.02165 | mse_loss=0.03836
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04195 | best_loss=0.04195
Epoch 1/80: current_loss=0.04210 | best_loss=0.04195
Epoch 2/80: current_loss=0.04191 | best_loss=0.04191
Epoch 3/80: current_loss=0.04185 | best_loss=0.04185
Epoch 4/80: current_loss=0.04223 | best_loss=0.04185
Epoch 5/80: current_loss=0.04182 | best_loss=0.04182
Epoch 6/80: current_loss=0.04285 | best_loss=0.04182
Epoch 7/80: current_loss=0.04240 | best_loss=0.04182
Epoch 8/80: current_loss=0.04251 | best_loss=0.04182
Epoch 9/80: current_loss=0.04208 | best_loss=0.04182
Epoch 10/80: current_loss=0.04186 | best_loss=0.04182
Epoch 11/80: current_loss=0.04198 | best_loss=0.04182
Epoch 12/80: current_loss=0.04180 | best_loss=0.04180
Epoch 13/80: current_loss=0.04177 | best_loss=0.04177
Epoch 14/80: current_loss=0.04192 | best_loss=0.04177
Epoch 15/80: current_loss=0.04169 | best_loss=0.04169
Epoch 16/80: current_loss=0.04192 | best_loss=0.04169
Epoch 17/80: current_loss=0.04241 | best_loss=0.04169
Epoch 18/80: current_loss=0.04254 | best_loss=0.04169
Epoch 19/80: current_loss=0.04237 | best_loss=0.04169
Epoch 20/80: current_loss=0.04170 | best_loss=0.04169
Epoch 21/80: current_loss=0.04201 | best_loss=0.04169
Epoch 22/80: current_loss=0.04163 | best_loss=0.04163
Epoch 23/80: current_loss=0.04222 | best_loss=0.04163
Epoch 24/80: current_loss=0.04237 | best_loss=0.04163
Epoch 25/80: current_loss=0.04204 | best_loss=0.04163
Epoch 26/80: current_loss=0.04180 | best_loss=0.04163
Epoch 27/80: current_loss=0.04169 | best_loss=0.04163
Epoch 28/80: current_loss=0.04168 | best_loss=0.04163
Epoch 29/80: current_loss=0.04214 | best_loss=0.04163
Epoch 30/80: current_loss=0.04227 | best_loss=0.04163
Epoch 31/80: current_loss=0.04206 | best_loss=0.04163
Epoch 32/80: current_loss=0.04280 | best_loss=0.04163
Epoch 33/80: current_loss=0.04220 | best_loss=0.04163
Epoch 34/80: current_loss=0.04178 | best_loss=0.04163
Epoch 35/80: current_loss=0.04204 | best_loss=0.04163
Epoch 36/80: current_loss=0.04178 | best_loss=0.04163
Epoch 37/80: current_loss=0.04296 | best_loss=0.04163
Epoch 38/80: current_loss=0.04250 | best_loss=0.04163
Epoch 39/80: current_loss=0.04190 | best_loss=0.04163
Epoch 40/80: current_loss=0.04255 | best_loss=0.04163
Epoch 41/80: current_loss=0.04185 | best_loss=0.04163
Epoch 42/80: current_loss=0.04190 | best_loss=0.04163
Early Stopping at epoch 42
      explained_var=0.02744 | mse_loss=0.04014
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02798 | best_loss=0.02798
Epoch 1/80: current_loss=0.02812 | best_loss=0.02798
Epoch 2/80: current_loss=0.02816 | best_loss=0.02798
Epoch 3/80: current_loss=0.02810 | best_loss=0.02798
Epoch 4/80: current_loss=0.02931 | best_loss=0.02798
Epoch 5/80: current_loss=0.02811 | best_loss=0.02798
Epoch 6/80: current_loss=0.02796 | best_loss=0.02796
Epoch 7/80: current_loss=0.02818 | best_loss=0.02796
Epoch 8/80: current_loss=0.02948 | best_loss=0.02796
Epoch 9/80: current_loss=0.02796 | best_loss=0.02796
Epoch 10/80: current_loss=0.02810 | best_loss=0.02796
Epoch 11/80: current_loss=0.02813 | best_loss=0.02796
Epoch 12/80: current_loss=0.02810 | best_loss=0.02796
Epoch 13/80: current_loss=0.02828 | best_loss=0.02796
Epoch 14/80: current_loss=0.02809 | best_loss=0.02796
Epoch 15/80: current_loss=0.02823 | best_loss=0.02796
Epoch 16/80: current_loss=0.02862 | best_loss=0.02796
Epoch 17/80: current_loss=0.02793 | best_loss=0.02793
Epoch 18/80: current_loss=0.02824 | best_loss=0.02793
Epoch 19/80: current_loss=0.02929 | best_loss=0.02793
Epoch 20/80: current_loss=0.02800 | best_loss=0.02793
Epoch 21/80: current_loss=0.02809 | best_loss=0.02793
Epoch 22/80: current_loss=0.02919 | best_loss=0.02793
Epoch 23/80: current_loss=0.02803 | best_loss=0.02793
Epoch 24/80: current_loss=0.02875 | best_loss=0.02793
Epoch 25/80: current_loss=0.02801 | best_loss=0.02793
Epoch 26/80: current_loss=0.02815 | best_loss=0.02793
Epoch 27/80: current_loss=0.02797 | best_loss=0.02793
Epoch 28/80: current_loss=0.02822 | best_loss=0.02793
Epoch 29/80: current_loss=0.02864 | best_loss=0.02793
Epoch 30/80: current_loss=0.02813 | best_loss=0.02793
Epoch 31/80: current_loss=0.02827 | best_loss=0.02793
Epoch 32/80: current_loss=0.02810 | best_loss=0.02793
Epoch 33/80: current_loss=0.02874 | best_loss=0.02793
Epoch 34/80: current_loss=0.02796 | best_loss=0.02793
Epoch 35/80: current_loss=0.02805 | best_loss=0.02793
Epoch 36/80: current_loss=0.02809 | best_loss=0.02793
Epoch 37/80: current_loss=0.02807 | best_loss=0.02793
Early Stopping at epoch 37
      explained_var=-0.00086 | mse_loss=0.02838
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03297 | best_loss=0.03297
Epoch 1/80: current_loss=0.03305 | best_loss=0.03297
Epoch 2/80: current_loss=0.03313 | best_loss=0.03297
Epoch 3/80: current_loss=0.03301 | best_loss=0.03297
Epoch 4/80: current_loss=0.03292 | best_loss=0.03292
Epoch 5/80: current_loss=0.03300 | best_loss=0.03292
Epoch 6/80: current_loss=0.03285 | best_loss=0.03285
Epoch 7/80: current_loss=0.03325 | best_loss=0.03285
Epoch 8/80: current_loss=0.03342 | best_loss=0.03285
Epoch 9/80: current_loss=0.03287 | best_loss=0.03285
Epoch 10/80: current_loss=0.03295 | best_loss=0.03285
Epoch 11/80: current_loss=0.03303 | best_loss=0.03285
Epoch 12/80: current_loss=0.03296 | best_loss=0.03285
Epoch 13/80: current_loss=0.03365 | best_loss=0.03285
Epoch 14/80: current_loss=0.03284 | best_loss=0.03284
Epoch 15/80: current_loss=0.03288 | best_loss=0.03284
Epoch 16/80: current_loss=0.03298 | best_loss=0.03284
Epoch 17/80: current_loss=0.03291 | best_loss=0.03284
Epoch 18/80: current_loss=0.03290 | best_loss=0.03284
Epoch 19/80: current_loss=0.03301 | best_loss=0.03284
Epoch 20/80: current_loss=0.03289 | best_loss=0.03284
Epoch 21/80: current_loss=0.03292 | best_loss=0.03284
Epoch 22/80: current_loss=0.03285 | best_loss=0.03284
Epoch 23/80: current_loss=0.03291 | best_loss=0.03284
Epoch 24/80: current_loss=0.03307 | best_loss=0.03284
Epoch 25/80: current_loss=0.03301 | best_loss=0.03284
Epoch 26/80: current_loss=0.03318 | best_loss=0.03284
Epoch 27/80: current_loss=0.03286 | best_loss=0.03284
Epoch 28/80: current_loss=0.03296 | best_loss=0.03284
Epoch 29/80: current_loss=0.03291 | best_loss=0.03284
Epoch 30/80: current_loss=0.03298 | best_loss=0.03284
Epoch 31/80: current_loss=0.03293 | best_loss=0.03284
Epoch 32/80: current_loss=0.03304 | best_loss=0.03284
Epoch 33/80: current_loss=0.03292 | best_loss=0.03284
Epoch 34/80: current_loss=0.03289 | best_loss=0.03284
Early Stopping at epoch 34
      explained_var=-0.00337 | mse_loss=0.03271
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03550 | best_loss=0.03550
Epoch 1/80: current_loss=0.03554 | best_loss=0.03550
Epoch 2/80: current_loss=0.03543 | best_loss=0.03543
Epoch 3/80: current_loss=0.03549 | best_loss=0.03543
Epoch 4/80: current_loss=0.03565 | best_loss=0.03543
Epoch 5/80: current_loss=0.03566 | best_loss=0.03543
Epoch 6/80: current_loss=0.03552 | best_loss=0.03543
Epoch 7/80: current_loss=0.03564 | best_loss=0.03543
Epoch 8/80: current_loss=0.03550 | best_loss=0.03543
Epoch 9/80: current_loss=0.03549 | best_loss=0.03543
Epoch 10/80: current_loss=0.03558 | best_loss=0.03543
Epoch 11/80: current_loss=0.03549 | best_loss=0.03543
Epoch 12/80: current_loss=0.03546 | best_loss=0.03543
Epoch 13/80: current_loss=0.03545 | best_loss=0.03543
Epoch 14/80: current_loss=0.03550 | best_loss=0.03543
Epoch 15/80: current_loss=0.03551 | best_loss=0.03543
Epoch 16/80: current_loss=0.03554 | best_loss=0.03543
Epoch 17/80: current_loss=0.03551 | best_loss=0.03543
Epoch 18/80: current_loss=0.03554 | best_loss=0.03543
Epoch 19/80: current_loss=0.03560 | best_loss=0.03543
Epoch 20/80: current_loss=0.03559 | best_loss=0.03543
Epoch 21/80: current_loss=0.03554 | best_loss=0.03543
Epoch 22/80: current_loss=0.03554 | best_loss=0.03543
Early Stopping at epoch 22
      explained_var=0.01781 | mse_loss=0.03524
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.01253| avg_loss=0.03497
----------------------------------------------


----------------------------------------------
Params for Trial 55
{'learning_rate': 0.1, 'weight_decay': 0.0013072676944743625, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=26.02339 | best_loss=26.02339
Epoch 1/80: current_loss=2.39760 | best_loss=2.39760
Epoch 2/80: current_loss=0.79868 | best_loss=0.79868
Epoch 3/80: current_loss=0.56020 | best_loss=0.56020
Epoch 4/80: current_loss=0.08443 | best_loss=0.08443
Epoch 5/80: current_loss=0.59676 | best_loss=0.08443
Epoch 6/80: current_loss=0.34548 | best_loss=0.08443
Epoch 7/80: current_loss=0.06903 | best_loss=0.06903
Epoch 8/80: current_loss=0.04398 | best_loss=0.04398
Epoch 9/80: current_loss=0.07041 | best_loss=0.04398
Epoch 10/80: current_loss=0.04829 | best_loss=0.04398
Epoch 11/80: current_loss=0.04448 | best_loss=0.04398
Epoch 12/80: current_loss=0.08264 | best_loss=0.04398
Epoch 13/80: current_loss=0.12502 | best_loss=0.04398
Epoch 14/80: current_loss=0.05341 | best_loss=0.04398
Epoch 15/80: current_loss=0.06129 | best_loss=0.04398
Epoch 16/80: current_loss=0.15189 | best_loss=0.04398
Epoch 17/80: current_loss=0.06978 | best_loss=0.04398
Epoch 18/80: current_loss=0.06823 | best_loss=0.04398
Epoch 19/80: current_loss=0.11752 | best_loss=0.04398
Epoch 20/80: current_loss=0.07896 | best_loss=0.04398
Epoch 21/80: current_loss=0.07403 | best_loss=0.04398
Epoch 22/80: current_loss=0.14154 | best_loss=0.04398
Epoch 23/80: current_loss=0.07512 | best_loss=0.04398
Epoch 24/80: current_loss=0.17142 | best_loss=0.04398
Epoch 25/80: current_loss=0.12436 | best_loss=0.04398
Epoch 26/80: current_loss=0.08234 | best_loss=0.04398
Epoch 27/80: current_loss=0.05219 | best_loss=0.04398
Epoch 28/80: current_loss=0.28413 | best_loss=0.04398
Early Stopping at epoch 28
      explained_var=-0.14070 | mse_loss=0.04504

----------------------------------------------
Params for Trial 56
{'learning_rate': 0.01, 'weight_decay': 0.0017536502072936031, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.14413 | best_loss=0.14413
Epoch 1/80: current_loss=0.05859 | best_loss=0.05859
Epoch 2/80: current_loss=0.04391 | best_loss=0.04391
Epoch 3/80: current_loss=0.04305 | best_loss=0.04305
Epoch 4/80: current_loss=0.05086 | best_loss=0.04305
Epoch 5/80: current_loss=0.03999 | best_loss=0.03999
Epoch 6/80: current_loss=0.04577 | best_loss=0.03999
Epoch 7/80: current_loss=0.04290 | best_loss=0.03999
Epoch 8/80: current_loss=0.04671 | best_loss=0.03999
Epoch 9/80: current_loss=0.04277 | best_loss=0.03999
Epoch 10/80: current_loss=0.03871 | best_loss=0.03871
Epoch 11/80: current_loss=0.03746 | best_loss=0.03746
Epoch 12/80: current_loss=0.03858 | best_loss=0.03746
Epoch 13/80: current_loss=0.03734 | best_loss=0.03734
Epoch 14/80: current_loss=0.03781 | best_loss=0.03734
Epoch 15/80: current_loss=0.03800 | best_loss=0.03734
Epoch 16/80: current_loss=0.03842 | best_loss=0.03734
Epoch 17/80: current_loss=0.03850 | best_loss=0.03734
Epoch 18/80: current_loss=0.03775 | best_loss=0.03734
Epoch 19/80: current_loss=0.03760 | best_loss=0.03734
Epoch 20/80: current_loss=0.03779 | best_loss=0.03734
Epoch 21/80: current_loss=0.03768 | best_loss=0.03734
Epoch 22/80: current_loss=0.03852 | best_loss=0.03734
Epoch 23/80: current_loss=0.03891 | best_loss=0.03734
Epoch 24/80: current_loss=0.03774 | best_loss=0.03734
Epoch 25/80: current_loss=0.03769 | best_loss=0.03734
Epoch 26/80: current_loss=0.03789 | best_loss=0.03734
Epoch 27/80: current_loss=0.03795 | best_loss=0.03734
Epoch 28/80: current_loss=0.03853 | best_loss=0.03734
Epoch 29/80: current_loss=0.03820 | best_loss=0.03734
Epoch 30/80: current_loss=0.03773 | best_loss=0.03734
Epoch 31/80: current_loss=0.03774 | best_loss=0.03734
Epoch 32/80: current_loss=0.03814 | best_loss=0.03734
Epoch 33/80: current_loss=0.03780 | best_loss=0.03734
Early Stopping at epoch 33
      explained_var=0.02422 | mse_loss=0.03816
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04899 | best_loss=0.04899
Epoch 1/80: current_loss=0.04894 | best_loss=0.04894
Epoch 2/80: current_loss=0.05216 | best_loss=0.04894
Epoch 3/80: current_loss=0.04299 | best_loss=0.04299
Epoch 4/80: current_loss=0.04366 | best_loss=0.04299
Epoch 5/80: current_loss=0.04309 | best_loss=0.04299
Epoch 6/80: current_loss=0.04283 | best_loss=0.04283
Epoch 7/80: current_loss=0.04293 | best_loss=0.04283
Epoch 8/80: current_loss=0.04244 | best_loss=0.04244
Epoch 9/80: current_loss=0.04305 | best_loss=0.04244
Epoch 10/80: current_loss=0.04260 | best_loss=0.04244
Epoch 11/80: current_loss=0.04268 | best_loss=0.04244
Epoch 12/80: current_loss=0.04266 | best_loss=0.04244
Epoch 13/80: current_loss=0.04281 | best_loss=0.04244
Epoch 14/80: current_loss=0.04321 | best_loss=0.04244
Epoch 15/80: current_loss=0.04265 | best_loss=0.04244
Epoch 16/80: current_loss=0.04307 | best_loss=0.04244
Epoch 17/80: current_loss=0.04266 | best_loss=0.04244
Epoch 18/80: current_loss=0.04295 | best_loss=0.04244
Epoch 19/80: current_loss=0.04265 | best_loss=0.04244
Epoch 20/80: current_loss=0.04263 | best_loss=0.04244
Epoch 21/80: current_loss=0.04269 | best_loss=0.04244
Epoch 22/80: current_loss=0.04267 | best_loss=0.04244
Epoch 23/80: current_loss=0.04264 | best_loss=0.04244
Epoch 24/80: current_loss=0.04266 | best_loss=0.04244
Epoch 25/80: current_loss=0.04265 | best_loss=0.04244
Epoch 26/80: current_loss=0.04273 | best_loss=0.04244
Epoch 27/80: current_loss=0.04267 | best_loss=0.04244
Epoch 28/80: current_loss=0.04283 | best_loss=0.04244
Early Stopping at epoch 28
      explained_var=0.00517 | mse_loss=0.04103
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03007 | best_loss=0.03007
Epoch 1/80: current_loss=0.02820 | best_loss=0.02820
Epoch 2/80: current_loss=0.04202 | best_loss=0.02820
Epoch 3/80: current_loss=0.04174 | best_loss=0.02820
Epoch 4/80: current_loss=0.03582 | best_loss=0.02820
Epoch 5/80: current_loss=0.03174 | best_loss=0.02820
Epoch 6/80: current_loss=0.04518 | best_loss=0.02820
Epoch 7/80: current_loss=0.04319 | best_loss=0.02820
Epoch 8/80: current_loss=0.03515 | best_loss=0.02820
Epoch 9/80: current_loss=0.03661 | best_loss=0.02820
Epoch 10/80: current_loss=0.03583 | best_loss=0.02820
Epoch 11/80: current_loss=0.02940 | best_loss=0.02820
Epoch 12/80: current_loss=0.02798 | best_loss=0.02798
Epoch 13/80: current_loss=0.02826 | best_loss=0.02798
Epoch 14/80: current_loss=0.02803 | best_loss=0.02798
Epoch 15/80: current_loss=0.02805 | best_loss=0.02798
Epoch 16/80: current_loss=0.02842 | best_loss=0.02798
Epoch 17/80: current_loss=0.02806 | best_loss=0.02798
Epoch 18/80: current_loss=0.02841 | best_loss=0.02798
Epoch 19/80: current_loss=0.02794 | best_loss=0.02794
Epoch 20/80: current_loss=0.02894 | best_loss=0.02794
Epoch 21/80: current_loss=0.02860 | best_loss=0.02794
Epoch 22/80: current_loss=0.02862 | best_loss=0.02794
Epoch 23/80: current_loss=0.02796 | best_loss=0.02794
Epoch 24/80: current_loss=0.02798 | best_loss=0.02794
Epoch 25/80: current_loss=0.02867 | best_loss=0.02794
Epoch 26/80: current_loss=0.02839 | best_loss=0.02794
Epoch 27/80: current_loss=0.02834 | best_loss=0.02794
Epoch 28/80: current_loss=0.02850 | best_loss=0.02794
Epoch 29/80: current_loss=0.02909 | best_loss=0.02794
Epoch 30/80: current_loss=0.02798 | best_loss=0.02794
Epoch 31/80: current_loss=0.02853 | best_loss=0.02794
Epoch 32/80: current_loss=0.02798 | best_loss=0.02794
Epoch 33/80: current_loss=0.02872 | best_loss=0.02794
Epoch 34/80: current_loss=0.02808 | best_loss=0.02794
Epoch 35/80: current_loss=0.02883 | best_loss=0.02794
Epoch 36/80: current_loss=0.02817 | best_loss=0.02794
Epoch 37/80: current_loss=0.02832 | best_loss=0.02794
Epoch 38/80: current_loss=0.02818 | best_loss=0.02794
Epoch 39/80: current_loss=0.02828 | best_loss=0.02794
Early Stopping at epoch 39
      explained_var=0.00086 | mse_loss=0.02834
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04345 | best_loss=0.04345
Epoch 1/80: current_loss=0.03269 | best_loss=0.03269
Epoch 2/80: current_loss=0.04683 | best_loss=0.03269
Epoch 3/80: current_loss=0.03324 | best_loss=0.03269
Epoch 4/80: current_loss=0.03309 | best_loss=0.03269
Epoch 5/80: current_loss=0.03273 | best_loss=0.03269
Epoch 6/80: current_loss=0.03275 | best_loss=0.03269
Epoch 7/80: current_loss=0.03275 | best_loss=0.03269
Epoch 8/80: current_loss=0.03431 | best_loss=0.03269
Epoch 9/80: current_loss=0.03282 | best_loss=0.03269
Epoch 10/80: current_loss=0.03342 | best_loss=0.03269
Epoch 11/80: current_loss=0.03302 | best_loss=0.03269
Epoch 12/80: current_loss=0.03281 | best_loss=0.03269
Epoch 13/80: current_loss=0.03279 | best_loss=0.03269
Epoch 14/80: current_loss=0.03303 | best_loss=0.03269
Epoch 15/80: current_loss=0.03289 | best_loss=0.03269
Epoch 16/80: current_loss=0.03280 | best_loss=0.03269
Epoch 17/80: current_loss=0.03291 | best_loss=0.03269
Epoch 18/80: current_loss=0.03297 | best_loss=0.03269
Epoch 19/80: current_loss=0.03281 | best_loss=0.03269
Epoch 20/80: current_loss=0.03295 | best_loss=0.03269
Epoch 21/80: current_loss=0.03294 | best_loss=0.03269
Early Stopping at epoch 21
      explained_var=0.00312 | mse_loss=0.03250
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05153 | best_loss=0.05153
Epoch 1/80: current_loss=0.04733 | best_loss=0.04733
Epoch 2/80: current_loss=0.04131 | best_loss=0.04131
Epoch 3/80: current_loss=0.03646 | best_loss=0.03646
Epoch 4/80: current_loss=0.03820 | best_loss=0.03646
Epoch 5/80: current_loss=0.03953 | best_loss=0.03646
Epoch 6/80: current_loss=0.03772 | best_loss=0.03646
Epoch 7/80: current_loss=0.03584 | best_loss=0.03584
Epoch 8/80: current_loss=0.03589 | best_loss=0.03584
Epoch 9/80: current_loss=0.03616 | best_loss=0.03584
Epoch 10/80: current_loss=0.03597 | best_loss=0.03584
Epoch 11/80: current_loss=0.03645 | best_loss=0.03584
Epoch 12/80: current_loss=0.03609 | best_loss=0.03584
Epoch 13/80: current_loss=0.03622 | best_loss=0.03584
Epoch 14/80: current_loss=0.03643 | best_loss=0.03584
Epoch 15/80: current_loss=0.03610 | best_loss=0.03584
Epoch 16/80: current_loss=0.03672 | best_loss=0.03584
Epoch 17/80: current_loss=0.03616 | best_loss=0.03584
Epoch 18/80: current_loss=0.03639 | best_loss=0.03584
Epoch 19/80: current_loss=0.03623 | best_loss=0.03584
Epoch 20/80: current_loss=0.03630 | best_loss=0.03584
Epoch 21/80: current_loss=0.03620 | best_loss=0.03584
Epoch 22/80: current_loss=0.03633 | best_loss=0.03584
Epoch 23/80: current_loss=0.03626 | best_loss=0.03584
Epoch 24/80: current_loss=0.03637 | best_loss=0.03584
Epoch 25/80: current_loss=0.03641 | best_loss=0.03584
Epoch 26/80: current_loss=0.03636 | best_loss=0.03584
Epoch 27/80: current_loss=0.03626 | best_loss=0.03584
Early Stopping at epoch 27
      explained_var=0.01437 | mse_loss=0.03558
----------------------------------------------
Average early_stopping_point: 9| avg_exp_var=0.00955| avg_loss=0.03512
----------------------------------------------


----------------------------------------------
Params for Trial 57
{'learning_rate': 0.001, 'weight_decay': 0.0007257403430232907, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05143 | best_loss=0.05143
Epoch 1/80: current_loss=0.04130 | best_loss=0.04130
Epoch 2/80: current_loss=0.04020 | best_loss=0.04020
Epoch 3/80: current_loss=0.04105 | best_loss=0.04020
Epoch 4/80: current_loss=0.03969 | best_loss=0.03969
Epoch 5/80: current_loss=0.03788 | best_loss=0.03788
Epoch 6/80: current_loss=0.03811 | best_loss=0.03788
Epoch 7/80: current_loss=0.03818 | best_loss=0.03788
Epoch 8/80: current_loss=0.03817 | best_loss=0.03788
Epoch 9/80: current_loss=0.03807 | best_loss=0.03788
Epoch 10/80: current_loss=0.03773 | best_loss=0.03773
Epoch 11/80: current_loss=0.03758 | best_loss=0.03758
Epoch 12/80: current_loss=0.03766 | best_loss=0.03758
Epoch 13/80: current_loss=0.03754 | best_loss=0.03754
Epoch 14/80: current_loss=0.03770 | best_loss=0.03754
Epoch 15/80: current_loss=0.03783 | best_loss=0.03754
Epoch 16/80: current_loss=0.03923 | best_loss=0.03754
Epoch 17/80: current_loss=0.03880 | best_loss=0.03754
Epoch 18/80: current_loss=0.03776 | best_loss=0.03754
Epoch 19/80: current_loss=0.03766 | best_loss=0.03754
Epoch 20/80: current_loss=0.03834 | best_loss=0.03754
Epoch 21/80: current_loss=0.03958 | best_loss=0.03754
Epoch 22/80: current_loss=0.03776 | best_loss=0.03754
Epoch 23/80: current_loss=0.03760 | best_loss=0.03754
Epoch 24/80: current_loss=0.03747 | best_loss=0.03747
Epoch 25/80: current_loss=0.03789 | best_loss=0.03747
Epoch 26/80: current_loss=0.03773 | best_loss=0.03747
Epoch 27/80: current_loss=0.03860 | best_loss=0.03747
Epoch 28/80: current_loss=0.03752 | best_loss=0.03747
Epoch 29/80: current_loss=0.03761 | best_loss=0.03747
Epoch 30/80: current_loss=0.03814 | best_loss=0.03747
Epoch 31/80: current_loss=0.03742 | best_loss=0.03742
Epoch 32/80: current_loss=0.03992 | best_loss=0.03742
Epoch 33/80: current_loss=0.03808 | best_loss=0.03742
Epoch 34/80: current_loss=0.03833 | best_loss=0.03742
Epoch 35/80: current_loss=0.03797 | best_loss=0.03742
Epoch 36/80: current_loss=0.03797 | best_loss=0.03742
Epoch 37/80: current_loss=0.03797 | best_loss=0.03742
Epoch 38/80: current_loss=0.03823 | best_loss=0.03742
Epoch 39/80: current_loss=0.03776 | best_loss=0.03742
Epoch 40/80: current_loss=0.03766 | best_loss=0.03742
Epoch 41/80: current_loss=0.03780 | best_loss=0.03742
Epoch 42/80: current_loss=0.03779 | best_loss=0.03742
Epoch 43/80: current_loss=0.03777 | best_loss=0.03742
Epoch 44/80: current_loss=0.03956 | best_loss=0.03742
Epoch 45/80: current_loss=0.03839 | best_loss=0.03742
Epoch 46/80: current_loss=0.03773 | best_loss=0.03742
Epoch 47/80: current_loss=0.03762 | best_loss=0.03742
Epoch 48/80: current_loss=0.03804 | best_loss=0.03742
Epoch 49/80: current_loss=0.03774 | best_loss=0.03742
Epoch 50/80: current_loss=0.03831 | best_loss=0.03742
Epoch 51/80: current_loss=0.03843 | best_loss=0.03742
Early Stopping at epoch 51
      explained_var=0.02137 | mse_loss=0.03833
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04183 | best_loss=0.04183
Epoch 1/80: current_loss=0.04217 | best_loss=0.04183
Epoch 2/80: current_loss=0.04204 | best_loss=0.04183
Epoch 3/80: current_loss=0.04182 | best_loss=0.04182
Epoch 4/80: current_loss=0.04209 | best_loss=0.04182
Epoch 5/80: current_loss=0.04161 | best_loss=0.04161
Epoch 6/80: current_loss=0.04287 | best_loss=0.04161
Epoch 7/80: current_loss=0.04175 | best_loss=0.04161
Epoch 8/80: current_loss=0.04320 | best_loss=0.04161
Epoch 9/80: current_loss=0.04160 | best_loss=0.04160
Epoch 10/80: current_loss=0.04223 | best_loss=0.04160
Epoch 11/80: current_loss=0.04186 | best_loss=0.04160
Epoch 12/80: current_loss=0.04175 | best_loss=0.04160
Epoch 13/80: current_loss=0.04196 | best_loss=0.04160
Epoch 14/80: current_loss=0.04158 | best_loss=0.04158
Epoch 15/80: current_loss=0.04148 | best_loss=0.04148
Epoch 16/80: current_loss=0.04174 | best_loss=0.04148
Epoch 17/80: current_loss=0.04143 | best_loss=0.04143
Epoch 18/80: current_loss=0.04249 | best_loss=0.04143
Epoch 19/80: current_loss=0.04256 | best_loss=0.04143
Epoch 20/80: current_loss=0.04240 | best_loss=0.04143
Epoch 21/80: current_loss=0.04260 | best_loss=0.04143
Epoch 22/80: current_loss=0.04219 | best_loss=0.04143
Epoch 23/80: current_loss=0.04152 | best_loss=0.04143
Epoch 24/80: current_loss=0.04277 | best_loss=0.04143
Epoch 25/80: current_loss=0.04202 | best_loss=0.04143
Epoch 26/80: current_loss=0.04186 | best_loss=0.04143
Epoch 27/80: current_loss=0.04258 | best_loss=0.04143
Epoch 28/80: current_loss=0.04297 | best_loss=0.04143
Epoch 29/80: current_loss=0.04487 | best_loss=0.04143
Epoch 30/80: current_loss=0.04194 | best_loss=0.04143
Epoch 31/80: current_loss=0.04238 | best_loss=0.04143
Epoch 32/80: current_loss=0.04251 | best_loss=0.04143
Epoch 33/80: current_loss=0.04224 | best_loss=0.04143
Epoch 34/80: current_loss=0.04250 | best_loss=0.04143
Epoch 35/80: current_loss=0.04155 | best_loss=0.04143
Epoch 36/80: current_loss=0.04152 | best_loss=0.04143
Epoch 37/80: current_loss=0.04246 | best_loss=0.04143
Early Stopping at epoch 37
      explained_var=0.03106 | mse_loss=0.03996
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02906 | best_loss=0.02906
Epoch 1/80: current_loss=0.02823 | best_loss=0.02823
Epoch 2/80: current_loss=0.02825 | best_loss=0.02823
Epoch 3/80: current_loss=0.02804 | best_loss=0.02804
Epoch 4/80: current_loss=0.02936 | best_loss=0.02804
Epoch 5/80: current_loss=0.02918 | best_loss=0.02804
Epoch 6/80: current_loss=0.02802 | best_loss=0.02802
Epoch 7/80: current_loss=0.02819 | best_loss=0.02802
Epoch 8/80: current_loss=0.02941 | best_loss=0.02802
Epoch 9/80: current_loss=0.02840 | best_loss=0.02802
Epoch 10/80: current_loss=0.02841 | best_loss=0.02802
Epoch 11/80: current_loss=0.02818 | best_loss=0.02802
Epoch 12/80: current_loss=0.02871 | best_loss=0.02802
Epoch 13/80: current_loss=0.02838 | best_loss=0.02802
Epoch 14/80: current_loss=0.02904 | best_loss=0.02802
Epoch 15/80: current_loss=0.02908 | best_loss=0.02802
Epoch 16/80: current_loss=0.02838 | best_loss=0.02802
Epoch 17/80: current_loss=0.02894 | best_loss=0.02802
Epoch 18/80: current_loss=0.02855 | best_loss=0.02802
Epoch 19/80: current_loss=0.02847 | best_loss=0.02802
Epoch 20/80: current_loss=0.02824 | best_loss=0.02802
Epoch 21/80: current_loss=0.02815 | best_loss=0.02802
Epoch 22/80: current_loss=0.02971 | best_loss=0.02802
Epoch 23/80: current_loss=0.03040 | best_loss=0.02802
Epoch 24/80: current_loss=0.02820 | best_loss=0.02802
Epoch 25/80: current_loss=0.02832 | best_loss=0.02802
Epoch 26/80: current_loss=0.02800 | best_loss=0.02800
Epoch 27/80: current_loss=0.02854 | best_loss=0.02800
Epoch 28/80: current_loss=0.02814 | best_loss=0.02800
Epoch 29/80: current_loss=0.02819 | best_loss=0.02800
Epoch 30/80: current_loss=0.02927 | best_loss=0.02800
Epoch 31/80: current_loss=0.02841 | best_loss=0.02800
Epoch 32/80: current_loss=0.02912 | best_loss=0.02800
Epoch 33/80: current_loss=0.02827 | best_loss=0.02800
Epoch 34/80: current_loss=0.02982 | best_loss=0.02800
Epoch 35/80: current_loss=0.02802 | best_loss=0.02800
Epoch 36/80: current_loss=0.02829 | best_loss=0.02800
Epoch 37/80: current_loss=0.02832 | best_loss=0.02800
Epoch 38/80: current_loss=0.02922 | best_loss=0.02800
Epoch 39/80: current_loss=0.02797 | best_loss=0.02797
Epoch 40/80: current_loss=0.02866 | best_loss=0.02797
Epoch 41/80: current_loss=0.02839 | best_loss=0.02797
Epoch 42/80: current_loss=0.02813 | best_loss=0.02797
Epoch 43/80: current_loss=0.02808 | best_loss=0.02797
Epoch 44/80: current_loss=0.02921 | best_loss=0.02797
Epoch 45/80: current_loss=0.02965 | best_loss=0.02797
Epoch 46/80: current_loss=0.02847 | best_loss=0.02797
Epoch 47/80: current_loss=0.02824 | best_loss=0.02797
Epoch 48/80: current_loss=0.02925 | best_loss=0.02797
Epoch 49/80: current_loss=0.02828 | best_loss=0.02797
Epoch 50/80: current_loss=0.02807 | best_loss=0.02797
Epoch 51/80: current_loss=0.02848 | best_loss=0.02797
Epoch 52/80: current_loss=0.02807 | best_loss=0.02797
Epoch 53/80: current_loss=0.02828 | best_loss=0.02797
Epoch 54/80: current_loss=0.02811 | best_loss=0.02797
Epoch 55/80: current_loss=0.02843 | best_loss=0.02797
Epoch 56/80: current_loss=0.02873 | best_loss=0.02797
Epoch 57/80: current_loss=0.02895 | best_loss=0.02797
Epoch 58/80: current_loss=0.02833 | best_loss=0.02797
Epoch 59/80: current_loss=0.02857 | best_loss=0.02797
Early Stopping at epoch 59
      explained_var=-0.00340 | mse_loss=0.02846

----------------------------------------------
Params for Trial 58
{'learning_rate': 0.001, 'weight_decay': 5.274299431281701e-05, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04523 | best_loss=0.04523
Epoch 1/80: current_loss=0.04018 | best_loss=0.04018
Epoch 2/80: current_loss=0.04194 | best_loss=0.04018
Epoch 3/80: current_loss=0.03940 | best_loss=0.03940
Epoch 4/80: current_loss=0.03847 | best_loss=0.03847
Epoch 5/80: current_loss=0.03815 | best_loss=0.03815
Epoch 6/80: current_loss=0.03812 | best_loss=0.03812
Epoch 7/80: current_loss=0.04021 | best_loss=0.03812
Epoch 8/80: current_loss=0.04260 | best_loss=0.03812
Epoch 9/80: current_loss=0.03968 | best_loss=0.03812
Epoch 10/80: current_loss=0.04174 | best_loss=0.03812
Epoch 11/80: current_loss=0.03903 | best_loss=0.03812
Epoch 12/80: current_loss=0.03793 | best_loss=0.03793
Epoch 13/80: current_loss=0.03791 | best_loss=0.03791
Epoch 14/80: current_loss=0.03893 | best_loss=0.03791
Epoch 15/80: current_loss=0.03811 | best_loss=0.03791
Epoch 16/80: current_loss=0.03928 | best_loss=0.03791
Epoch 17/80: current_loss=0.03750 | best_loss=0.03750
Epoch 18/80: current_loss=0.03743 | best_loss=0.03743
Epoch 19/80: current_loss=0.04074 | best_loss=0.03743
Epoch 20/80: current_loss=0.03771 | best_loss=0.03743
Epoch 21/80: current_loss=0.03811 | best_loss=0.03743
Epoch 22/80: current_loss=0.03834 | best_loss=0.03743
Epoch 23/80: current_loss=0.03758 | best_loss=0.03743
Epoch 24/80: current_loss=0.03732 | best_loss=0.03732
Epoch 25/80: current_loss=0.03761 | best_loss=0.03732
Epoch 26/80: current_loss=0.03896 | best_loss=0.03732
Epoch 27/80: current_loss=0.03742 | best_loss=0.03732
Epoch 28/80: current_loss=0.03745 | best_loss=0.03732
Epoch 29/80: current_loss=0.03773 | best_loss=0.03732
Epoch 30/80: current_loss=0.03938 | best_loss=0.03732
Epoch 31/80: current_loss=0.03812 | best_loss=0.03732
Epoch 32/80: current_loss=0.04040 | best_loss=0.03732
Epoch 33/80: current_loss=0.03957 | best_loss=0.03732
Epoch 34/80: current_loss=0.03962 | best_loss=0.03732
Epoch 35/80: current_loss=0.03783 | best_loss=0.03732
Epoch 36/80: current_loss=0.03861 | best_loss=0.03732
Epoch 37/80: current_loss=0.03898 | best_loss=0.03732
Epoch 38/80: current_loss=0.03782 | best_loss=0.03732
Epoch 39/80: current_loss=0.03854 | best_loss=0.03732
Epoch 40/80: current_loss=0.03906 | best_loss=0.03732
Epoch 41/80: current_loss=0.04379 | best_loss=0.03732
Epoch 42/80: current_loss=0.03771 | best_loss=0.03732
Epoch 43/80: current_loss=0.03750 | best_loss=0.03732
Epoch 44/80: current_loss=0.03785 | best_loss=0.03732
Early Stopping at epoch 44
      explained_var=0.02874 | mse_loss=0.03824
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04203 | best_loss=0.04203
Epoch 1/80: current_loss=0.04208 | best_loss=0.04203
Epoch 2/80: current_loss=0.04160 | best_loss=0.04160
Epoch 3/80: current_loss=0.04286 | best_loss=0.04160
Epoch 4/80: current_loss=0.08138 | best_loss=0.04160
Epoch 5/80: current_loss=0.10763 | best_loss=0.04160
Epoch 6/80: current_loss=0.06218 | best_loss=0.04160
Epoch 7/80: current_loss=0.04520 | best_loss=0.04160
Epoch 8/80: current_loss=0.04532 | best_loss=0.04160
Epoch 9/80: current_loss=0.04520 | best_loss=0.04160
Epoch 10/80: current_loss=0.04547 | best_loss=0.04160
Epoch 11/80: current_loss=0.04597 | best_loss=0.04160
Epoch 12/80: current_loss=0.04530 | best_loss=0.04160
Epoch 13/80: current_loss=0.04324 | best_loss=0.04160
Epoch 14/80: current_loss=0.04874 | best_loss=0.04160
Epoch 15/80: current_loss=0.04351 | best_loss=0.04160
Epoch 16/80: current_loss=0.04340 | best_loss=0.04160
Epoch 17/80: current_loss=0.04743 | best_loss=0.04160
Epoch 18/80: current_loss=0.04324 | best_loss=0.04160
Epoch 19/80: current_loss=0.04344 | best_loss=0.04160
Epoch 20/80: current_loss=0.04427 | best_loss=0.04160
Epoch 21/80: current_loss=0.04236 | best_loss=0.04160
Epoch 22/80: current_loss=0.04266 | best_loss=0.04160
Early Stopping at epoch 22
      explained_var=0.02401 | mse_loss=0.04024
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03440 | best_loss=0.03440
Epoch 1/80: current_loss=0.02904 | best_loss=0.02904
Epoch 2/80: current_loss=0.02937 | best_loss=0.02904
Epoch 3/80: current_loss=0.02922 | best_loss=0.02904
Epoch 4/80: current_loss=0.02907 | best_loss=0.02904
Epoch 5/80: current_loss=0.02811 | best_loss=0.02811
Epoch 6/80: current_loss=0.02978 | best_loss=0.02811
Epoch 7/80: current_loss=0.02813 | best_loss=0.02811
Epoch 8/80: current_loss=0.03248 | best_loss=0.02811
Epoch 9/80: current_loss=0.03037 | best_loss=0.02811
Epoch 10/80: current_loss=0.02800 | best_loss=0.02800
Epoch 11/80: current_loss=0.03826 | best_loss=0.02800
Epoch 12/80: current_loss=0.02845 | best_loss=0.02800
Epoch 13/80: current_loss=0.02784 | best_loss=0.02784
Epoch 14/80: current_loss=0.02794 | best_loss=0.02784
Epoch 15/80: current_loss=0.02949 | best_loss=0.02784
Epoch 16/80: current_loss=0.02950 | best_loss=0.02784
Epoch 17/80: current_loss=0.02858 | best_loss=0.02784
Epoch 18/80: current_loss=0.02861 | best_loss=0.02784
Epoch 19/80: current_loss=0.02824 | best_loss=0.02784
Epoch 20/80: current_loss=0.03014 | best_loss=0.02784
Epoch 21/80: current_loss=0.02831 | best_loss=0.02784
Epoch 22/80: current_loss=0.02856 | best_loss=0.02784
Epoch 23/80: current_loss=0.02996 | best_loss=0.02784
Epoch 24/80: current_loss=0.02879 | best_loss=0.02784
Epoch 25/80: current_loss=0.03094 | best_loss=0.02784
Epoch 26/80: current_loss=0.03182 | best_loss=0.02784
Epoch 27/80: current_loss=0.02825 | best_loss=0.02784
Epoch 28/80: current_loss=0.02885 | best_loss=0.02784
Epoch 29/80: current_loss=0.02868 | best_loss=0.02784
Epoch 30/80: current_loss=0.02912 | best_loss=0.02784
Epoch 31/80: current_loss=0.02892 | best_loss=0.02784
Epoch 32/80: current_loss=0.02878 | best_loss=0.02784
Epoch 33/80: current_loss=0.02908 | best_loss=0.02784
Early Stopping at epoch 33
      explained_var=0.00361 | mse_loss=0.02826
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03511 | best_loss=0.03511
Epoch 1/80: current_loss=0.03293 | best_loss=0.03293
Epoch 2/80: current_loss=0.03334 | best_loss=0.03293
Epoch 3/80: current_loss=0.03299 | best_loss=0.03293
Epoch 4/80: current_loss=0.03352 | best_loss=0.03293
Epoch 5/80: current_loss=0.03313 | best_loss=0.03293
Epoch 6/80: current_loss=0.03313 | best_loss=0.03293
Epoch 7/80: current_loss=0.03437 | best_loss=0.03293
Epoch 8/80: current_loss=0.03340 | best_loss=0.03293
Epoch 9/80: current_loss=0.03330 | best_loss=0.03293
Epoch 10/80: current_loss=0.03308 | best_loss=0.03293
Epoch 11/80: current_loss=0.03318 | best_loss=0.03293
Epoch 12/80: current_loss=0.03354 | best_loss=0.03293
Epoch 13/80: current_loss=0.03348 | best_loss=0.03293
Epoch 14/80: current_loss=0.03342 | best_loss=0.03293
Epoch 15/80: current_loss=0.05058 | best_loss=0.03293
Epoch 16/80: current_loss=0.06233 | best_loss=0.03293
Epoch 17/80: current_loss=0.05954 | best_loss=0.03293
Epoch 18/80: current_loss=0.03856 | best_loss=0.03293
Epoch 19/80: current_loss=0.06617 | best_loss=0.03293
Epoch 20/80: current_loss=0.04093 | best_loss=0.03293
Epoch 21/80: current_loss=0.03465 | best_loss=0.03293
Early Stopping at epoch 21
      explained_var=-0.00459 | mse_loss=0.03275
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03805 | best_loss=0.03805
Epoch 1/80: current_loss=0.04061 | best_loss=0.03805
Epoch 2/80: current_loss=0.04114 | best_loss=0.03805
Epoch 3/80: current_loss=0.06338 | best_loss=0.03805
Epoch 4/80: current_loss=0.03910 | best_loss=0.03805
Epoch 5/80: current_loss=0.04132 | best_loss=0.03805
Epoch 6/80: current_loss=0.04123 | best_loss=0.03805
Epoch 7/80: current_loss=0.05073 | best_loss=0.03805
Epoch 8/80: current_loss=0.03823 | best_loss=0.03805
Epoch 9/80: current_loss=0.04119 | best_loss=0.03805
Epoch 10/80: current_loss=0.04108 | best_loss=0.03805
Epoch 11/80: current_loss=0.04164 | best_loss=0.03805
Epoch 12/80: current_loss=0.04038 | best_loss=0.03805
Epoch 13/80: current_loss=0.04009 | best_loss=0.03805
Epoch 14/80: current_loss=0.04014 | best_loss=0.03805
Epoch 15/80: current_loss=0.04033 | best_loss=0.03805
Epoch 16/80: current_loss=0.04886 | best_loss=0.03805
Epoch 17/80: current_loss=0.03969 | best_loss=0.03805
Epoch 18/80: current_loss=0.04520 | best_loss=0.03805
Epoch 19/80: current_loss=0.04769 | best_loss=0.03805
Epoch 20/80: current_loss=0.04654 | best_loss=0.03805
Early Stopping at epoch 20
      explained_var=-0.05458 | mse_loss=0.03788
----------------------------------------------
Average early_stopping_point: 8| avg_exp_var=-0.00056| avg_loss=0.03547
----------------------------------------------


----------------------------------------------
Params for Trial 59
{'learning_rate': 1e-05, 'weight_decay': 0.0015564042665180914, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.18555 | best_loss=0.18555
Epoch 1/80: current_loss=0.16712 | best_loss=0.16712
Epoch 2/80: current_loss=0.15051 | best_loss=0.15051
Epoch 3/80: current_loss=0.13573 | best_loss=0.13573
Epoch 4/80: current_loss=0.12219 | best_loss=0.12219
Epoch 5/80: current_loss=0.10992 | best_loss=0.10992
Epoch 6/80: current_loss=0.09923 | best_loss=0.09923
Epoch 7/80: current_loss=0.08976 | best_loss=0.08976
Epoch 8/80: current_loss=0.08143 | best_loss=0.08143
Epoch 9/80: current_loss=0.07411 | best_loss=0.07411
Epoch 10/80: current_loss=0.06808 | best_loss=0.06808
Epoch 11/80: current_loss=0.06309 | best_loss=0.06309
Epoch 12/80: current_loss=0.05894 | best_loss=0.05894
Epoch 13/80: current_loss=0.05552 | best_loss=0.05552
Epoch 14/80: current_loss=0.05288 | best_loss=0.05288
Epoch 15/80: current_loss=0.05070 | best_loss=0.05070
Epoch 16/80: current_loss=0.04914 | best_loss=0.04914
Epoch 17/80: current_loss=0.04808 | best_loss=0.04808
Epoch 18/80: current_loss=0.04703 | best_loss=0.04703
Epoch 19/80: current_loss=0.04643 | best_loss=0.04643
Epoch 20/80: current_loss=0.04598 | best_loss=0.04598
Epoch 21/80: current_loss=0.04564 | best_loss=0.04564
Epoch 22/80: current_loss=0.04543 | best_loss=0.04543
Epoch 23/80: current_loss=0.04522 | best_loss=0.04522
Epoch 24/80: current_loss=0.04504 | best_loss=0.04504
Epoch 25/80: current_loss=0.04478 | best_loss=0.04478
Epoch 26/80: current_loss=0.04462 | best_loss=0.04462
Epoch 27/80: current_loss=0.04457 | best_loss=0.04457
Epoch 28/80: current_loss=0.04451 | best_loss=0.04451
Epoch 29/80: current_loss=0.04432 | best_loss=0.04432
Epoch 30/80: current_loss=0.04425 | best_loss=0.04425
Epoch 31/80: current_loss=0.04417 | best_loss=0.04417
Epoch 32/80: current_loss=0.04405 | best_loss=0.04405
Epoch 33/80: current_loss=0.04398 | best_loss=0.04398
Epoch 34/80: current_loss=0.04393 | best_loss=0.04393
Epoch 35/80: current_loss=0.04385 | best_loss=0.04385
Epoch 36/80: current_loss=0.04373 | best_loss=0.04373
Epoch 37/80: current_loss=0.04369 | best_loss=0.04369
Epoch 38/80: current_loss=0.04357 | best_loss=0.04357
Epoch 39/80: current_loss=0.04356 | best_loss=0.04356
Epoch 40/80: current_loss=0.04349 | best_loss=0.04349
Epoch 41/80: current_loss=0.04336 | best_loss=0.04336
Epoch 42/80: current_loss=0.04333 | best_loss=0.04333
Epoch 43/80: current_loss=0.04326 | best_loss=0.04326
Epoch 44/80: current_loss=0.04325 | best_loss=0.04325
Epoch 45/80: current_loss=0.04317 | best_loss=0.04317
Epoch 46/80: current_loss=0.04310 | best_loss=0.04310
Epoch 47/80: current_loss=0.04303 | best_loss=0.04303
Epoch 48/80: current_loss=0.04297 | best_loss=0.04297
Epoch 49/80: current_loss=0.04294 | best_loss=0.04294
Epoch 50/80: current_loss=0.04283 | best_loss=0.04283
Epoch 51/80: current_loss=0.04280 | best_loss=0.04280
Epoch 52/80: current_loss=0.04266 | best_loss=0.04266
Epoch 53/80: current_loss=0.04263 | best_loss=0.04263
Epoch 54/80: current_loss=0.04253 | best_loss=0.04253
Epoch 55/80: current_loss=0.04240 | best_loss=0.04240
Epoch 56/80: current_loss=0.04236 | best_loss=0.04236
Epoch 57/80: current_loss=0.04235 | best_loss=0.04235
Epoch 58/80: current_loss=0.04233 | best_loss=0.04233
Epoch 59/80: current_loss=0.04220 | best_loss=0.04220
Epoch 60/80: current_loss=0.04210 | best_loss=0.04210
Epoch 61/80: current_loss=0.04204 | best_loss=0.04204
Epoch 62/80: current_loss=0.04203 | best_loss=0.04203
Epoch 63/80: current_loss=0.04205 | best_loss=0.04203
Epoch 64/80: current_loss=0.04210 | best_loss=0.04203
Epoch 65/80: current_loss=0.04209 | best_loss=0.04203
Epoch 66/80: current_loss=0.04196 | best_loss=0.04196
Epoch 67/80: current_loss=0.04190 | best_loss=0.04190
Epoch 68/80: current_loss=0.04191 | best_loss=0.04190
Epoch 69/80: current_loss=0.04200 | best_loss=0.04190
Epoch 70/80: current_loss=0.04193 | best_loss=0.04190
Epoch 71/80: current_loss=0.04178 | best_loss=0.04178
Epoch 72/80: current_loss=0.04171 | best_loss=0.04171
Epoch 73/80: current_loss=0.04171 | best_loss=0.04171
Epoch 74/80: current_loss=0.04162 | best_loss=0.04162
Epoch 75/80: current_loss=0.04155 | best_loss=0.04155
Epoch 76/80: current_loss=0.04149 | best_loss=0.04149
Epoch 77/80: current_loss=0.04146 | best_loss=0.04146
Epoch 78/80: current_loss=0.04142 | best_loss=0.04142
Epoch 79/80: current_loss=0.04142 | best_loss=0.04142
      explained_var=-0.06527 | mse_loss=0.04253

----------------------------------------------
Params for Trial 60
{'learning_rate': 0.1, 'weight_decay': 0.0034552258426748593, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.24738 | best_loss=0.24738
Epoch 1/80: current_loss=0.13265 | best_loss=0.13265
Epoch 2/80: current_loss=0.04463 | best_loss=0.04463
Epoch 3/80: current_loss=0.04067 | best_loss=0.04067
Epoch 4/80: current_loss=0.04519 | best_loss=0.04067
Epoch 5/80: current_loss=0.04689 | best_loss=0.04067
Epoch 6/80: current_loss=0.06071 | best_loss=0.04067
Epoch 7/80: current_loss=0.04304 | best_loss=0.04067
Epoch 8/80: current_loss=0.03842 | best_loss=0.03842
Epoch 9/80: current_loss=0.04259 | best_loss=0.03842
Epoch 10/80: current_loss=0.04477 | best_loss=0.03842
Epoch 11/80: current_loss=0.04562 | best_loss=0.03842
Epoch 12/80: current_loss=0.04495 | best_loss=0.03842
Epoch 13/80: current_loss=0.05598 | best_loss=0.03842
Epoch 14/80: current_loss=0.04421 | best_loss=0.03842
Epoch 15/80: current_loss=0.03976 | best_loss=0.03842
Epoch 16/80: current_loss=0.03885 | best_loss=0.03842
Epoch 17/80: current_loss=0.04284 | best_loss=0.03842
Epoch 18/80: current_loss=0.04246 | best_loss=0.03842
Epoch 19/80: current_loss=0.04213 | best_loss=0.03842
Epoch 20/80: current_loss=0.05363 | best_loss=0.03842
Epoch 21/80: current_loss=0.04293 | best_loss=0.03842
Epoch 22/80: current_loss=0.04947 | best_loss=0.03842
Epoch 23/80: current_loss=0.06396 | best_loss=0.03842
Epoch 24/80: current_loss=0.05440 | best_loss=0.03842
Epoch 25/80: current_loss=0.04495 | best_loss=0.03842
Epoch 26/80: current_loss=0.04661 | best_loss=0.03842
Epoch 27/80: current_loss=0.04984 | best_loss=0.03842
Epoch 28/80: current_loss=0.12690 | best_loss=0.03842
Early Stopping at epoch 28
      explained_var=0.00130 | mse_loss=0.03909

----------------------------------------------
Params for Trial 61
{'learning_rate': 0.001, 'weight_decay': 0.00038686277734171614, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04443 | best_loss=0.04443
Epoch 1/80: current_loss=0.04319 | best_loss=0.04319
Epoch 2/80: current_loss=0.03936 | best_loss=0.03936
Epoch 3/80: current_loss=0.03850 | best_loss=0.03850
Epoch 4/80: current_loss=0.03843 | best_loss=0.03843
Epoch 5/80: current_loss=0.03792 | best_loss=0.03792
Epoch 6/80: current_loss=0.03795 | best_loss=0.03792
Epoch 7/80: current_loss=0.03768 | best_loss=0.03768
Epoch 8/80: current_loss=0.03793 | best_loss=0.03768
Epoch 9/80: current_loss=0.03802 | best_loss=0.03768
Epoch 10/80: current_loss=0.03816 | best_loss=0.03768
Epoch 11/80: current_loss=0.03872 | best_loss=0.03768
Epoch 12/80: current_loss=0.03894 | best_loss=0.03768
Epoch 13/80: current_loss=0.03773 | best_loss=0.03768
Epoch 14/80: current_loss=0.03760 | best_loss=0.03760
Epoch 15/80: current_loss=0.03781 | best_loss=0.03760
Epoch 16/80: current_loss=0.03778 | best_loss=0.03760
Epoch 17/80: current_loss=0.03785 | best_loss=0.03760
Epoch 18/80: current_loss=0.03798 | best_loss=0.03760
Epoch 19/80: current_loss=0.03778 | best_loss=0.03760
Epoch 20/80: current_loss=0.03854 | best_loss=0.03760
Epoch 21/80: current_loss=0.03791 | best_loss=0.03760
Epoch 22/80: current_loss=0.03842 | best_loss=0.03760
Epoch 23/80: current_loss=0.03755 | best_loss=0.03755
Epoch 24/80: current_loss=0.03749 | best_loss=0.03749
Epoch 25/80: current_loss=0.03748 | best_loss=0.03748
Epoch 26/80: current_loss=0.03785 | best_loss=0.03748
Epoch 27/80: current_loss=0.03774 | best_loss=0.03748
Epoch 28/80: current_loss=0.03770 | best_loss=0.03748
Epoch 29/80: current_loss=0.03805 | best_loss=0.03748
Epoch 30/80: current_loss=0.03752 | best_loss=0.03748
Epoch 31/80: current_loss=0.03776 | best_loss=0.03748
Epoch 32/80: current_loss=0.03774 | best_loss=0.03748
Epoch 33/80: current_loss=0.03775 | best_loss=0.03748
Epoch 34/80: current_loss=0.03761 | best_loss=0.03748
Epoch 35/80: current_loss=0.03768 | best_loss=0.03748
Epoch 36/80: current_loss=0.03920 | best_loss=0.03748
Epoch 37/80: current_loss=0.03852 | best_loss=0.03748
Epoch 38/80: current_loss=0.03777 | best_loss=0.03748
Epoch 39/80: current_loss=0.03754 | best_loss=0.03748
Epoch 40/80: current_loss=0.03764 | best_loss=0.03748
Epoch 41/80: current_loss=0.03827 | best_loss=0.03748
Epoch 42/80: current_loss=0.03760 | best_loss=0.03748
Epoch 43/80: current_loss=0.03760 | best_loss=0.03748
Epoch 44/80: current_loss=0.03751 | best_loss=0.03748
Epoch 45/80: current_loss=0.03795 | best_loss=0.03748
Early Stopping at epoch 45
      explained_var=0.02044 | mse_loss=0.03841
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04128 | best_loss=0.04128
Epoch 1/80: current_loss=0.04180 | best_loss=0.04128
Epoch 2/80: current_loss=0.04152 | best_loss=0.04128
Epoch 3/80: current_loss=0.04179 | best_loss=0.04128
Epoch 4/80: current_loss=0.04153 | best_loss=0.04128
Epoch 5/80: current_loss=0.04202 | best_loss=0.04128
Epoch 6/80: current_loss=0.04197 | best_loss=0.04128
Epoch 7/80: current_loss=0.04259 | best_loss=0.04128
Epoch 8/80: current_loss=0.04209 | best_loss=0.04128
Epoch 9/80: current_loss=0.04169 | best_loss=0.04128
Epoch 10/80: current_loss=0.04262 | best_loss=0.04128
Epoch 11/80: current_loss=0.04388 | best_loss=0.04128
Epoch 12/80: current_loss=0.04285 | best_loss=0.04128
Epoch 13/80: current_loss=0.04152 | best_loss=0.04128
Epoch 14/80: current_loss=0.04155 | best_loss=0.04128
Epoch 15/80: current_loss=0.04164 | best_loss=0.04128
Epoch 16/80: current_loss=0.04154 | best_loss=0.04128
Epoch 17/80: current_loss=0.04160 | best_loss=0.04128
Epoch 18/80: current_loss=0.04192 | best_loss=0.04128
Epoch 19/80: current_loss=0.04166 | best_loss=0.04128
Epoch 20/80: current_loss=0.04159 | best_loss=0.04128
Early Stopping at epoch 20
      explained_var=0.03426 | mse_loss=0.03984
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02815 | best_loss=0.02815
Epoch 1/80: current_loss=0.02807 | best_loss=0.02807
Epoch 2/80: current_loss=0.02850 | best_loss=0.02807
Epoch 3/80: current_loss=0.02810 | best_loss=0.02807
Epoch 4/80: current_loss=0.02805 | best_loss=0.02805
Epoch 5/80: current_loss=0.02818 | best_loss=0.02805
Epoch 6/80: current_loss=0.02823 | best_loss=0.02805
Epoch 7/80: current_loss=0.02834 | best_loss=0.02805
Epoch 8/80: current_loss=0.02851 | best_loss=0.02805
Epoch 9/80: current_loss=0.02824 | best_loss=0.02805
Epoch 10/80: current_loss=0.02814 | best_loss=0.02805
Epoch 11/80: current_loss=0.02889 | best_loss=0.02805
Epoch 12/80: current_loss=0.02816 | best_loss=0.02805
Epoch 13/80: current_loss=0.02840 | best_loss=0.02805
Epoch 14/80: current_loss=0.02822 | best_loss=0.02805
Epoch 15/80: current_loss=0.02820 | best_loss=0.02805
Epoch 16/80: current_loss=0.02942 | best_loss=0.02805
Epoch 17/80: current_loss=0.02830 | best_loss=0.02805
Epoch 18/80: current_loss=0.02821 | best_loss=0.02805
Epoch 19/80: current_loss=0.02809 | best_loss=0.02805
Epoch 20/80: current_loss=0.02884 | best_loss=0.02805
Epoch 21/80: current_loss=0.02817 | best_loss=0.02805
Epoch 22/80: current_loss=0.02814 | best_loss=0.02805
Epoch 23/80: current_loss=0.02869 | best_loss=0.02805
Epoch 24/80: current_loss=0.02979 | best_loss=0.02805
Early Stopping at epoch 24
      explained_var=-0.00290 | mse_loss=0.02855

----------------------------------------------
Params for Trial 62
{'learning_rate': 0.001, 'weight_decay': 0.0007268491064087398, 'n_layers': 3, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03843 | best_loss=0.03843
Epoch 1/80: current_loss=0.03897 | best_loss=0.03843
Epoch 2/80: current_loss=0.03836 | best_loss=0.03836
Epoch 3/80: current_loss=0.03782 | best_loss=0.03782
Epoch 4/80: current_loss=0.03871 | best_loss=0.03782
Epoch 5/80: current_loss=0.03844 | best_loss=0.03782
Epoch 6/80: current_loss=0.03809 | best_loss=0.03782
Epoch 7/80: current_loss=0.03788 | best_loss=0.03782
Epoch 8/80: current_loss=0.03820 | best_loss=0.03782
Epoch 9/80: current_loss=0.03803 | best_loss=0.03782
Epoch 10/80: current_loss=0.03828 | best_loss=0.03782
Epoch 11/80: current_loss=0.03879 | best_loss=0.03782
Epoch 12/80: current_loss=0.03844 | best_loss=0.03782
Epoch 13/80: current_loss=0.03844 | best_loss=0.03782
Epoch 14/80: current_loss=0.03800 | best_loss=0.03782
Epoch 15/80: current_loss=0.03790 | best_loss=0.03782
Epoch 16/80: current_loss=0.03791 | best_loss=0.03782
Epoch 17/80: current_loss=0.03774 | best_loss=0.03774
Epoch 18/80: current_loss=0.03786 | best_loss=0.03774
Epoch 19/80: current_loss=0.03817 | best_loss=0.03774
Epoch 20/80: current_loss=0.03784 | best_loss=0.03774
Epoch 21/80: current_loss=0.03793 | best_loss=0.03774
Epoch 22/80: current_loss=0.03811 | best_loss=0.03774
Epoch 23/80: current_loss=0.03800 | best_loss=0.03774
Epoch 24/80: current_loss=0.03774 | best_loss=0.03774
Epoch 25/80: current_loss=0.03805 | best_loss=0.03774
Epoch 26/80: current_loss=0.03796 | best_loss=0.03774
Epoch 27/80: current_loss=0.03770 | best_loss=0.03770
Epoch 28/80: current_loss=0.03772 | best_loss=0.03770
Epoch 29/80: current_loss=0.03790 | best_loss=0.03770
Epoch 30/80: current_loss=0.03764 | best_loss=0.03764
Epoch 31/80: current_loss=0.03767 | best_loss=0.03764
Epoch 32/80: current_loss=0.03918 | best_loss=0.03764
Epoch 33/80: current_loss=0.03776 | best_loss=0.03764
Epoch 34/80: current_loss=0.03790 | best_loss=0.03764
Epoch 35/80: current_loss=0.03773 | best_loss=0.03764
Epoch 36/80: current_loss=0.03781 | best_loss=0.03764
Epoch 37/80: current_loss=0.03768 | best_loss=0.03764
Epoch 38/80: current_loss=0.03792 | best_loss=0.03764
Epoch 39/80: current_loss=0.03762 | best_loss=0.03762
Epoch 40/80: current_loss=0.03853 | best_loss=0.03762
Epoch 41/80: current_loss=0.03784 | best_loss=0.03762
Epoch 42/80: current_loss=0.03768 | best_loss=0.03762
Epoch 43/80: current_loss=0.03828 | best_loss=0.03762
Epoch 44/80: current_loss=0.03767 | best_loss=0.03762
Epoch 45/80: current_loss=0.03852 | best_loss=0.03762
Epoch 46/80: current_loss=0.03773 | best_loss=0.03762
Epoch 47/80: current_loss=0.03798 | best_loss=0.03762
Epoch 48/80: current_loss=0.03846 | best_loss=0.03762
Epoch 49/80: current_loss=0.03809 | best_loss=0.03762
Epoch 50/80: current_loss=0.03767 | best_loss=0.03762
Epoch 51/80: current_loss=0.03767 | best_loss=0.03762
Epoch 52/80: current_loss=0.03832 | best_loss=0.03762
Epoch 53/80: current_loss=0.03765 | best_loss=0.03762
Epoch 54/80: current_loss=0.03770 | best_loss=0.03762
Epoch 55/80: current_loss=0.03782 | best_loss=0.03762
Epoch 56/80: current_loss=0.03768 | best_loss=0.03762
Epoch 57/80: current_loss=0.03766 | best_loss=0.03762
Epoch 58/80: current_loss=0.03910 | best_loss=0.03762
Epoch 59/80: current_loss=0.03782 | best_loss=0.03762
Early Stopping at epoch 59
      explained_var=0.01769 | mse_loss=0.03851

----------------------------------------------
Params for Trial 63
{'learning_rate': 0.001, 'weight_decay': 0.00034611094110965234, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03933 | best_loss=0.03933
Epoch 1/80: current_loss=0.03831 | best_loss=0.03831
Epoch 2/80: current_loss=0.03847 | best_loss=0.03831
Epoch 3/80: current_loss=0.03899 | best_loss=0.03831
Epoch 4/80: current_loss=0.03854 | best_loss=0.03831
Epoch 5/80: current_loss=0.03788 | best_loss=0.03788
Epoch 6/80: current_loss=0.03808 | best_loss=0.03788
Epoch 7/80: current_loss=0.03821 | best_loss=0.03788
Epoch 8/80: current_loss=0.03761 | best_loss=0.03761
Epoch 9/80: current_loss=0.03974 | best_loss=0.03761
Epoch 10/80: current_loss=0.03743 | best_loss=0.03743
Epoch 11/80: current_loss=0.03792 | best_loss=0.03743
Epoch 12/80: current_loss=0.03791 | best_loss=0.03743
Epoch 13/80: current_loss=0.03798 | best_loss=0.03743
Epoch 14/80: current_loss=0.03908 | best_loss=0.03743
Epoch 15/80: current_loss=0.03896 | best_loss=0.03743
Epoch 16/80: current_loss=0.03761 | best_loss=0.03743
Epoch 17/80: current_loss=0.03752 | best_loss=0.03743
Epoch 18/80: current_loss=0.03902 | best_loss=0.03743
Epoch 19/80: current_loss=0.03768 | best_loss=0.03743
Epoch 20/80: current_loss=0.03778 | best_loss=0.03743
Epoch 21/80: current_loss=0.03782 | best_loss=0.03743
Epoch 22/80: current_loss=0.03991 | best_loss=0.03743
Epoch 23/80: current_loss=0.03757 | best_loss=0.03743
Epoch 24/80: current_loss=0.03757 | best_loss=0.03743
Epoch 25/80: current_loss=0.03732 | best_loss=0.03732
Epoch 26/80: current_loss=0.03798 | best_loss=0.03732
Epoch 27/80: current_loss=0.03733 | best_loss=0.03732
Epoch 28/80: current_loss=0.03728 | best_loss=0.03728
Epoch 29/80: current_loss=0.03796 | best_loss=0.03728
Epoch 30/80: current_loss=0.03796 | best_loss=0.03728
Epoch 31/80: current_loss=0.03869 | best_loss=0.03728
Epoch 32/80: current_loss=0.03757 | best_loss=0.03728
Epoch 33/80: current_loss=0.03917 | best_loss=0.03728
Epoch 34/80: current_loss=0.03873 | best_loss=0.03728
Epoch 35/80: current_loss=0.03750 | best_loss=0.03728
Epoch 36/80: current_loss=0.03753 | best_loss=0.03728
Epoch 37/80: current_loss=0.03763 | best_loss=0.03728
Epoch 38/80: current_loss=0.03760 | best_loss=0.03728
Epoch 39/80: current_loss=0.03765 | best_loss=0.03728
Epoch 40/80: current_loss=0.03850 | best_loss=0.03728
Epoch 41/80: current_loss=0.03751 | best_loss=0.03728
Epoch 42/80: current_loss=0.03744 | best_loss=0.03728
Epoch 43/80: current_loss=0.03925 | best_loss=0.03728
Epoch 44/80: current_loss=0.03830 | best_loss=0.03728
Epoch 45/80: current_loss=0.03774 | best_loss=0.03728
Epoch 46/80: current_loss=0.03796 | best_loss=0.03728
Epoch 47/80: current_loss=0.03809 | best_loss=0.03728
Epoch 48/80: current_loss=0.03762 | best_loss=0.03728
Early Stopping at epoch 48
      explained_var=0.02426 | mse_loss=0.03816
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04175 | best_loss=0.04175
Epoch 1/80: current_loss=0.04210 | best_loss=0.04175
Epoch 2/80: current_loss=0.04361 | best_loss=0.04175
Epoch 3/80: current_loss=0.04165 | best_loss=0.04165
Epoch 4/80: current_loss=0.04169 | best_loss=0.04165
Epoch 5/80: current_loss=0.04417 | best_loss=0.04165
Epoch 6/80: current_loss=0.04186 | best_loss=0.04165
Epoch 7/80: current_loss=0.04161 | best_loss=0.04161
Epoch 8/80: current_loss=0.04150 | best_loss=0.04150
Epoch 9/80: current_loss=0.04215 | best_loss=0.04150
Epoch 10/80: current_loss=0.04244 | best_loss=0.04150
Epoch 11/80: current_loss=0.04226 | best_loss=0.04150
Epoch 12/80: current_loss=0.04155 | best_loss=0.04150
Epoch 13/80: current_loss=0.04360 | best_loss=0.04150
Epoch 14/80: current_loss=0.04758 | best_loss=0.04150
Epoch 15/80: current_loss=0.04180 | best_loss=0.04150
Epoch 16/80: current_loss=0.04212 | best_loss=0.04150
Epoch 17/80: current_loss=0.04185 | best_loss=0.04150
Epoch 18/80: current_loss=0.04168 | best_loss=0.04150
Epoch 19/80: current_loss=0.04229 | best_loss=0.04150
Epoch 20/80: current_loss=0.04176 | best_loss=0.04150
Epoch 21/80: current_loss=0.04145 | best_loss=0.04145
Epoch 22/80: current_loss=0.04156 | best_loss=0.04145
Epoch 23/80: current_loss=0.04191 | best_loss=0.04145
Epoch 24/80: current_loss=0.04124 | best_loss=0.04124
Epoch 25/80: current_loss=0.04131 | best_loss=0.04124
Epoch 26/80: current_loss=0.04156 | best_loss=0.04124
Epoch 27/80: current_loss=0.04261 | best_loss=0.04124
Epoch 28/80: current_loss=0.04169 | best_loss=0.04124
Epoch 29/80: current_loss=0.04261 | best_loss=0.04124
Epoch 30/80: current_loss=0.04281 | best_loss=0.04124
Epoch 31/80: current_loss=0.04191 | best_loss=0.04124
Epoch 32/80: current_loss=0.04160 | best_loss=0.04124
Epoch 33/80: current_loss=0.04208 | best_loss=0.04124
Epoch 34/80: current_loss=0.04169 | best_loss=0.04124
Epoch 35/80: current_loss=0.04229 | best_loss=0.04124
Epoch 36/80: current_loss=0.04143 | best_loss=0.04124
Epoch 37/80: current_loss=0.04134 | best_loss=0.04124
Epoch 38/80: current_loss=0.04195 | best_loss=0.04124
Epoch 39/80: current_loss=0.04185 | best_loss=0.04124
Epoch 40/80: current_loss=0.04263 | best_loss=0.04124
Epoch 41/80: current_loss=0.04192 | best_loss=0.04124
Epoch 42/80: current_loss=0.04155 | best_loss=0.04124
Epoch 43/80: current_loss=0.04137 | best_loss=0.04124
Epoch 44/80: current_loss=0.04180 | best_loss=0.04124
Early Stopping at epoch 44
      explained_var=0.03534 | mse_loss=0.03980
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02843 | best_loss=0.02843
Epoch 1/80: current_loss=0.02896 | best_loss=0.02843
Epoch 2/80: current_loss=0.02881 | best_loss=0.02843
Epoch 3/80: current_loss=0.02845 | best_loss=0.02843
Epoch 4/80: current_loss=0.02839 | best_loss=0.02839
Epoch 5/80: current_loss=0.02864 | best_loss=0.02839
Epoch 6/80: current_loss=0.02843 | best_loss=0.02839
Epoch 7/80: current_loss=0.02827 | best_loss=0.02827
Epoch 8/80: current_loss=0.02842 | best_loss=0.02827
Epoch 9/80: current_loss=0.02852 | best_loss=0.02827
Epoch 10/80: current_loss=0.02845 | best_loss=0.02827
Epoch 11/80: current_loss=0.02829 | best_loss=0.02827
Epoch 12/80: current_loss=0.02857 | best_loss=0.02827
Epoch 13/80: current_loss=0.02851 | best_loss=0.02827
Epoch 14/80: current_loss=0.02909 | best_loss=0.02827
Epoch 15/80: current_loss=0.02913 | best_loss=0.02827
Epoch 16/80: current_loss=0.02867 | best_loss=0.02827
Epoch 17/80: current_loss=0.02996 | best_loss=0.02827
Epoch 18/80: current_loss=0.02848 | best_loss=0.02827
Epoch 19/80: current_loss=0.02827 | best_loss=0.02827
Epoch 20/80: current_loss=0.02832 | best_loss=0.02827
Epoch 21/80: current_loss=0.02830 | best_loss=0.02827
Epoch 22/80: current_loss=0.02966 | best_loss=0.02827
Epoch 23/80: current_loss=0.02837 | best_loss=0.02827
Epoch 24/80: current_loss=0.02859 | best_loss=0.02827
Epoch 25/80: current_loss=0.02834 | best_loss=0.02827
Epoch 26/80: current_loss=0.02833 | best_loss=0.02827
Epoch 27/80: current_loss=0.02894 | best_loss=0.02827
Early Stopping at epoch 27
      explained_var=-0.00906 | mse_loss=0.02871

----------------------------------------------
Params for Trial 64
{'learning_rate': 0.001, 'weight_decay': 0.000987597261822922, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04276 | best_loss=0.04276
Epoch 1/80: current_loss=0.04240 | best_loss=0.04240
Epoch 2/80: current_loss=0.04082 | best_loss=0.04082
Epoch 3/80: current_loss=0.03891 | best_loss=0.03891
Epoch 4/80: current_loss=0.03870 | best_loss=0.03870
Epoch 5/80: current_loss=0.03964 | best_loss=0.03870
Epoch 6/80: current_loss=0.03945 | best_loss=0.03870
Epoch 7/80: current_loss=0.03878 | best_loss=0.03870
Epoch 8/80: current_loss=0.03793 | best_loss=0.03793
Epoch 9/80: current_loss=0.03793 | best_loss=0.03793
Epoch 10/80: current_loss=0.03848 | best_loss=0.03793
Epoch 11/80: current_loss=0.03817 | best_loss=0.03793
Epoch 12/80: current_loss=0.03832 | best_loss=0.03793
Epoch 13/80: current_loss=0.03799 | best_loss=0.03793
Epoch 14/80: current_loss=0.04005 | best_loss=0.03793
Epoch 15/80: current_loss=0.03848 | best_loss=0.03793
Epoch 16/80: current_loss=0.03889 | best_loss=0.03793
Epoch 17/80: current_loss=0.03953 | best_loss=0.03793
Epoch 18/80: current_loss=0.03859 | best_loss=0.03793
Epoch 19/80: current_loss=0.03906 | best_loss=0.03793
Epoch 20/80: current_loss=0.03769 | best_loss=0.03769
Epoch 21/80: current_loss=0.03799 | best_loss=0.03769
Epoch 22/80: current_loss=0.03766 | best_loss=0.03766
Epoch 23/80: current_loss=0.03763 | best_loss=0.03763
Epoch 24/80: current_loss=0.03831 | best_loss=0.03763
Epoch 25/80: current_loss=0.03767 | best_loss=0.03763
Epoch 26/80: current_loss=0.03771 | best_loss=0.03763
Epoch 27/80: current_loss=0.03783 | best_loss=0.03763
Epoch 28/80: current_loss=0.03862 | best_loss=0.03763
Epoch 29/80: current_loss=0.03765 | best_loss=0.03763
Epoch 30/80: current_loss=0.03782 | best_loss=0.03763
Epoch 31/80: current_loss=0.03741 | best_loss=0.03741
Epoch 32/80: current_loss=0.03741 | best_loss=0.03741
Epoch 33/80: current_loss=0.03747 | best_loss=0.03741
Epoch 34/80: current_loss=0.03804 | best_loss=0.03741
Epoch 35/80: current_loss=0.03860 | best_loss=0.03741
Epoch 36/80: current_loss=0.03764 | best_loss=0.03741
Epoch 37/80: current_loss=0.03763 | best_loss=0.03741
Epoch 38/80: current_loss=0.03860 | best_loss=0.03741
Epoch 39/80: current_loss=0.03847 | best_loss=0.03741
Epoch 40/80: current_loss=0.03801 | best_loss=0.03741
Epoch 41/80: current_loss=0.03826 | best_loss=0.03741
Epoch 42/80: current_loss=0.03777 | best_loss=0.03741
Epoch 43/80: current_loss=0.03780 | best_loss=0.03741
Epoch 44/80: current_loss=0.03790 | best_loss=0.03741
Epoch 45/80: current_loss=0.03839 | best_loss=0.03741
Epoch 46/80: current_loss=0.03782 | best_loss=0.03741
Epoch 47/80: current_loss=0.03753 | best_loss=0.03741
Epoch 48/80: current_loss=0.03782 | best_loss=0.03741
Epoch 49/80: current_loss=0.03767 | best_loss=0.03741
Epoch 50/80: current_loss=0.03779 | best_loss=0.03741
Epoch 51/80: current_loss=0.03807 | best_loss=0.03741
Early Stopping at epoch 51
      explained_var=0.02086 | mse_loss=0.03831
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04187 | best_loss=0.04187
Epoch 1/80: current_loss=0.04251 | best_loss=0.04187
Epoch 2/80: current_loss=0.04264 | best_loss=0.04187
Epoch 3/80: current_loss=0.04175 | best_loss=0.04175
Epoch 4/80: current_loss=0.04215 | best_loss=0.04175
Epoch 5/80: current_loss=0.04310 | best_loss=0.04175
Epoch 6/80: current_loss=0.04215 | best_loss=0.04175
Epoch 7/80: current_loss=0.04182 | best_loss=0.04175
Epoch 8/80: current_loss=0.04232 | best_loss=0.04175
Epoch 9/80: current_loss=0.04176 | best_loss=0.04175
Epoch 10/80: current_loss=0.04229 | best_loss=0.04175
Epoch 11/80: current_loss=0.04443 | best_loss=0.04175
Epoch 12/80: current_loss=0.04313 | best_loss=0.04175
Epoch 13/80: current_loss=0.04211 | best_loss=0.04175
Epoch 14/80: current_loss=0.04199 | best_loss=0.04175
Epoch 15/80: current_loss=0.04186 | best_loss=0.04175
Epoch 16/80: current_loss=0.04289 | best_loss=0.04175
Epoch 17/80: current_loss=0.04193 | best_loss=0.04175
Epoch 18/80: current_loss=0.04237 | best_loss=0.04175
Epoch 19/80: current_loss=0.04238 | best_loss=0.04175
Epoch 20/80: current_loss=0.04249 | best_loss=0.04175
Epoch 21/80: current_loss=0.04186 | best_loss=0.04175
Epoch 22/80: current_loss=0.04187 | best_loss=0.04175
Epoch 23/80: current_loss=0.04177 | best_loss=0.04175
Early Stopping at epoch 23
      explained_var=0.02502 | mse_loss=0.04028
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02819 | best_loss=0.02819
Epoch 1/80: current_loss=0.02890 | best_loss=0.02819
Epoch 2/80: current_loss=0.02872 | best_loss=0.02819
Epoch 3/80: current_loss=0.02826 | best_loss=0.02819
Epoch 4/80: current_loss=0.02892 | best_loss=0.02819
Epoch 5/80: current_loss=0.02952 | best_loss=0.02819
Epoch 6/80: current_loss=0.02895 | best_loss=0.02819
Epoch 7/80: current_loss=0.02819 | best_loss=0.02819
Epoch 8/80: current_loss=0.02810 | best_loss=0.02810
Epoch 9/80: current_loss=0.02834 | best_loss=0.02810
Epoch 10/80: current_loss=0.02844 | best_loss=0.02810
Epoch 11/80: current_loss=0.02798 | best_loss=0.02798
Epoch 12/80: current_loss=0.02818 | best_loss=0.02798
Epoch 13/80: current_loss=0.02807 | best_loss=0.02798
Epoch 14/80: current_loss=0.02810 | best_loss=0.02798
Epoch 15/80: current_loss=0.02852 | best_loss=0.02798
Epoch 16/80: current_loss=0.02824 | best_loss=0.02798
Epoch 17/80: current_loss=0.02895 | best_loss=0.02798
Epoch 18/80: current_loss=0.02804 | best_loss=0.02798
Epoch 19/80: current_loss=0.02902 | best_loss=0.02798
Epoch 20/80: current_loss=0.02849 | best_loss=0.02798
Epoch 21/80: current_loss=0.02814 | best_loss=0.02798
Epoch 22/80: current_loss=0.02795 | best_loss=0.02795
Epoch 23/80: current_loss=0.02801 | best_loss=0.02795
Epoch 24/80: current_loss=0.02864 | best_loss=0.02795
Epoch 25/80: current_loss=0.02833 | best_loss=0.02795
Epoch 26/80: current_loss=0.02798 | best_loss=0.02795
Epoch 27/80: current_loss=0.02871 | best_loss=0.02795
Epoch 28/80: current_loss=0.02938 | best_loss=0.02795
Epoch 29/80: current_loss=0.02803 | best_loss=0.02795
Epoch 30/80: current_loss=0.02807 | best_loss=0.02795
Epoch 31/80: current_loss=0.02829 | best_loss=0.02795
Epoch 32/80: current_loss=0.02810 | best_loss=0.02795
Epoch 33/80: current_loss=0.02934 | best_loss=0.02795
Epoch 34/80: current_loss=0.02819 | best_loss=0.02795
Epoch 35/80: current_loss=0.02799 | best_loss=0.02795
Epoch 36/80: current_loss=0.02859 | best_loss=0.02795
Epoch 37/80: current_loss=0.02790 | best_loss=0.02790
Epoch 38/80: current_loss=0.02855 | best_loss=0.02790
Epoch 39/80: current_loss=0.02802 | best_loss=0.02790
Epoch 40/80: current_loss=0.02819 | best_loss=0.02790
Epoch 41/80: current_loss=0.02800 | best_loss=0.02790
Epoch 42/80: current_loss=0.02814 | best_loss=0.02790
Epoch 43/80: current_loss=0.02909 | best_loss=0.02790
Epoch 44/80: current_loss=0.02802 | best_loss=0.02790
Epoch 45/80: current_loss=0.02804 | best_loss=0.02790
Epoch 46/80: current_loss=0.02805 | best_loss=0.02790
Epoch 47/80: current_loss=0.02847 | best_loss=0.02790
Epoch 48/80: current_loss=0.02803 | best_loss=0.02790
Epoch 49/80: current_loss=0.02845 | best_loss=0.02790
Epoch 50/80: current_loss=0.02819 | best_loss=0.02790
Epoch 51/80: current_loss=0.02867 | best_loss=0.02790
Epoch 52/80: current_loss=0.02837 | best_loss=0.02790
Epoch 53/80: current_loss=0.02863 | best_loss=0.02790
Epoch 54/80: current_loss=0.02847 | best_loss=0.02790
Epoch 55/80: current_loss=0.02847 | best_loss=0.02790
Epoch 56/80: current_loss=0.02833 | best_loss=0.02790
Epoch 57/80: current_loss=0.02839 | best_loss=0.02790
Early Stopping at epoch 57
      explained_var=-0.00046 | mse_loss=0.02837
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03283 | best_loss=0.03283
Epoch 1/80: current_loss=0.03300 | best_loss=0.03283
Epoch 2/80: current_loss=0.03288 | best_loss=0.03283
Epoch 3/80: current_loss=0.03292 | best_loss=0.03283
Epoch 4/80: current_loss=0.03311 | best_loss=0.03283
Epoch 5/80: current_loss=0.03307 | best_loss=0.03283
Epoch 6/80: current_loss=0.03302 | best_loss=0.03283
Epoch 7/80: current_loss=0.03312 | best_loss=0.03283
Epoch 8/80: current_loss=0.03331 | best_loss=0.03283
Epoch 9/80: current_loss=0.03301 | best_loss=0.03283
Epoch 10/80: current_loss=0.03283 | best_loss=0.03283
Epoch 11/80: current_loss=0.03284 | best_loss=0.03283
Epoch 12/80: current_loss=0.03311 | best_loss=0.03283
Epoch 13/80: current_loss=0.03313 | best_loss=0.03283
Epoch 14/80: current_loss=0.03288 | best_loss=0.03283
Epoch 15/80: current_loss=0.03303 | best_loss=0.03283
Epoch 16/80: current_loss=0.03287 | best_loss=0.03283
Epoch 17/80: current_loss=0.03289 | best_loss=0.03283
Epoch 18/80: current_loss=0.03288 | best_loss=0.03283
Epoch 19/80: current_loss=0.03314 | best_loss=0.03283
Epoch 20/80: current_loss=0.03288 | best_loss=0.03283
Early Stopping at epoch 20
      explained_var=-0.00350 | mse_loss=0.03272
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03561 | best_loss=0.03561
Epoch 1/80: current_loss=0.03545 | best_loss=0.03545
Epoch 2/80: current_loss=0.03545 | best_loss=0.03545
Epoch 3/80: current_loss=0.03554 | best_loss=0.03545
Epoch 4/80: current_loss=0.03551 | best_loss=0.03545
Epoch 5/80: current_loss=0.03552 | best_loss=0.03545
Epoch 6/80: current_loss=0.03548 | best_loss=0.03545
Epoch 7/80: current_loss=0.03548 | best_loss=0.03545
Epoch 8/80: current_loss=0.03566 | best_loss=0.03545
Epoch 9/80: current_loss=0.03555 | best_loss=0.03545
Epoch 10/80: current_loss=0.03545 | best_loss=0.03545
Epoch 11/80: current_loss=0.03547 | best_loss=0.03545
Epoch 12/80: current_loss=0.03544 | best_loss=0.03544
Epoch 13/80: current_loss=0.03552 | best_loss=0.03544
Epoch 14/80: current_loss=0.03574 | best_loss=0.03544
Epoch 15/80: current_loss=0.03553 | best_loss=0.03544
Epoch 16/80: current_loss=0.03549 | best_loss=0.03544
Epoch 17/80: current_loss=0.03542 | best_loss=0.03542
Epoch 18/80: current_loss=0.03555 | best_loss=0.03542
Epoch 19/80: current_loss=0.03565 | best_loss=0.03542
Epoch 20/80: current_loss=0.03562 | best_loss=0.03542
Epoch 21/80: current_loss=0.03553 | best_loss=0.03542
Epoch 22/80: current_loss=0.03554 | best_loss=0.03542
Epoch 23/80: current_loss=0.03545 | best_loss=0.03542
Epoch 24/80: current_loss=0.03552 | best_loss=0.03542
Epoch 25/80: current_loss=0.03554 | best_loss=0.03542
Epoch 26/80: current_loss=0.03556 | best_loss=0.03542
Epoch 27/80: current_loss=0.03556 | best_loss=0.03542
Epoch 28/80: current_loss=0.03555 | best_loss=0.03542
Epoch 29/80: current_loss=0.03551 | best_loss=0.03542
Epoch 30/80: current_loss=0.03548 | best_loss=0.03542
Epoch 31/80: current_loss=0.03566 | best_loss=0.03542
Epoch 32/80: current_loss=0.03551 | best_loss=0.03542
Epoch 33/80: current_loss=0.03545 | best_loss=0.03542
Epoch 34/80: current_loss=0.03536 | best_loss=0.03536
Epoch 35/80: current_loss=0.03552 | best_loss=0.03536
Epoch 36/80: current_loss=0.03563 | best_loss=0.03536
Epoch 37/80: current_loss=0.03572 | best_loss=0.03536
Epoch 38/80: current_loss=0.03569 | best_loss=0.03536
Epoch 39/80: current_loss=0.03560 | best_loss=0.03536
Epoch 40/80: current_loss=0.03554 | best_loss=0.03536
Epoch 41/80: current_loss=0.03542 | best_loss=0.03536
Epoch 42/80: current_loss=0.03547 | best_loss=0.03536
Epoch 43/80: current_loss=0.03537 | best_loss=0.03536
Epoch 44/80: current_loss=0.03548 | best_loss=0.03536
Epoch 45/80: current_loss=0.03550 | best_loss=0.03536
Epoch 46/80: current_loss=0.03553 | best_loss=0.03536
Epoch 47/80: current_loss=0.03562 | best_loss=0.03536
Epoch 48/80: current_loss=0.03555 | best_loss=0.03536
Epoch 49/80: current_loss=0.03561 | best_loss=0.03536
Epoch 50/80: current_loss=0.03554 | best_loss=0.03536
Epoch 51/80: current_loss=0.03541 | best_loss=0.03536
Epoch 52/80: current_loss=0.03545 | best_loss=0.03536
Epoch 53/80: current_loss=0.03554 | best_loss=0.03536
Epoch 54/80: current_loss=0.03556 | best_loss=0.03536
Early Stopping at epoch 54
      explained_var=0.02065 | mse_loss=0.03514
----------------------------------------------
Average early_stopping_point: 21| avg_exp_var=0.01251| avg_loss=0.03496
----------------------------------------------


----------------------------------------------
Params for Trial 65
{'learning_rate': 0.01, 'weight_decay': 2.6427815932469033e-06, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04012 | best_loss=0.04012
Epoch 1/80: current_loss=0.03745 | best_loss=0.03745
Epoch 2/80: current_loss=0.03994 | best_loss=0.03745
Epoch 3/80: current_loss=0.04172 | best_loss=0.03745
Epoch 4/80: current_loss=0.04342 | best_loss=0.03745
Epoch 5/80: current_loss=0.03816 | best_loss=0.03745
Epoch 6/80: current_loss=0.03817 | best_loss=0.03745
Epoch 7/80: current_loss=0.03798 | best_loss=0.03745
Epoch 8/80: current_loss=0.03859 | best_loss=0.03745
Epoch 9/80: current_loss=0.03751 | best_loss=0.03745
Epoch 10/80: current_loss=0.03851 | best_loss=0.03745
Epoch 11/80: current_loss=0.03790 | best_loss=0.03745
Epoch 12/80: current_loss=0.03838 | best_loss=0.03745
Epoch 13/80: current_loss=0.03976 | best_loss=0.03745
Epoch 14/80: current_loss=0.04119 | best_loss=0.03745
Epoch 15/80: current_loss=0.03805 | best_loss=0.03745
Epoch 16/80: current_loss=0.03814 | best_loss=0.03745
Epoch 17/80: current_loss=0.03804 | best_loss=0.03745
Epoch 18/80: current_loss=0.03786 | best_loss=0.03745
Epoch 19/80: current_loss=0.03819 | best_loss=0.03745
Epoch 20/80: current_loss=0.03786 | best_loss=0.03745
Epoch 21/80: current_loss=0.03941 | best_loss=0.03745
Early Stopping at epoch 21
      explained_var=0.02136 | mse_loss=0.03842
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04225 | best_loss=0.04225
Epoch 1/80: current_loss=0.04374 | best_loss=0.04225
Epoch 2/80: current_loss=0.04279 | best_loss=0.04225
Epoch 3/80: current_loss=0.04384 | best_loss=0.04225
Epoch 4/80: current_loss=0.06519 | best_loss=0.04225
Epoch 5/80: current_loss=0.04341 | best_loss=0.04225
Epoch 6/80: current_loss=0.04286 | best_loss=0.04225
Epoch 7/80: current_loss=0.06139 | best_loss=0.04225
Epoch 8/80: current_loss=0.04711 | best_loss=0.04225
Epoch 9/80: current_loss=0.04730 | best_loss=0.04225
Epoch 10/80: current_loss=0.05452 | best_loss=0.04225
Epoch 11/80: current_loss=0.04834 | best_loss=0.04225
Epoch 12/80: current_loss=0.05018 | best_loss=0.04225
Epoch 13/80: current_loss=0.05369 | best_loss=0.04225
Epoch 14/80: current_loss=0.05998 | best_loss=0.04225
Epoch 15/80: current_loss=0.05903 | best_loss=0.04225
Epoch 16/80: current_loss=0.04529 | best_loss=0.04225
Epoch 17/80: current_loss=0.04780 | best_loss=0.04225
Epoch 18/80: current_loss=0.04772 | best_loss=0.04225
Epoch 19/80: current_loss=0.04596 | best_loss=0.04225
Epoch 20/80: current_loss=0.06526 | best_loss=0.04225
Early Stopping at epoch 20
      explained_var=0.00997 | mse_loss=0.04082
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03361 | best_loss=0.03361
Epoch 1/80: current_loss=0.04299 | best_loss=0.03361
Epoch 2/80: current_loss=0.04029 | best_loss=0.03361
Epoch 3/80: current_loss=0.02987 | best_loss=0.02987
Epoch 4/80: current_loss=0.03209 | best_loss=0.02987
Epoch 5/80: current_loss=0.03623 | best_loss=0.02987
Epoch 6/80: current_loss=0.04281 | best_loss=0.02987
Epoch 7/80: current_loss=0.04620 | best_loss=0.02987
Epoch 8/80: current_loss=0.03267 | best_loss=0.02987
Epoch 9/80: current_loss=0.02885 | best_loss=0.02885
Epoch 10/80: current_loss=0.02860 | best_loss=0.02860
Epoch 11/80: current_loss=0.03382 | best_loss=0.02860
Epoch 12/80: current_loss=0.03389 | best_loss=0.02860
Epoch 13/80: current_loss=0.03236 | best_loss=0.02860
Epoch 14/80: current_loss=0.03475 | best_loss=0.02860
Epoch 15/80: current_loss=0.02901 | best_loss=0.02860
Epoch 16/80: current_loss=0.03618 | best_loss=0.02860
Epoch 17/80: current_loss=0.04658 | best_loss=0.02860
Epoch 18/80: current_loss=0.03486 | best_loss=0.02860
Epoch 19/80: current_loss=0.03909 | best_loss=0.02860
Epoch 20/80: current_loss=0.03216 | best_loss=0.02860
Epoch 21/80: current_loss=0.02956 | best_loss=0.02860
Epoch 22/80: current_loss=0.03055 | best_loss=0.02860
Epoch 23/80: current_loss=0.03635 | best_loss=0.02860
Epoch 24/80: current_loss=0.03738 | best_loss=0.02860
Epoch 25/80: current_loss=0.03194 | best_loss=0.02860
Epoch 26/80: current_loss=0.03938 | best_loss=0.02860
Epoch 27/80: current_loss=0.03512 | best_loss=0.02860
Epoch 28/80: current_loss=0.02957 | best_loss=0.02860
Epoch 29/80: current_loss=0.04173 | best_loss=0.02860
Epoch 30/80: current_loss=0.04588 | best_loss=0.02860
Early Stopping at epoch 30
      explained_var=-0.01617 | mse_loss=0.02882

----------------------------------------------
Params for Trial 66
{'learning_rate': 0.001, 'weight_decay': 0.002275634302087203, 'n_layers': 3, 'hidden_size': 32, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04417 | best_loss=0.04417
Epoch 1/80: current_loss=0.04197 | best_loss=0.04197
Epoch 2/80: current_loss=0.04122 | best_loss=0.04122
Epoch 3/80: current_loss=0.04427 | best_loss=0.04122
Epoch 4/80: current_loss=0.04014 | best_loss=0.04014
Epoch 5/80: current_loss=0.04165 | best_loss=0.04014
Epoch 6/80: current_loss=0.03997 | best_loss=0.03997
Epoch 7/80: current_loss=0.03961 | best_loss=0.03961
Epoch 8/80: current_loss=0.03896 | best_loss=0.03896
Epoch 9/80: current_loss=0.03916 | best_loss=0.03896
Epoch 10/80: current_loss=0.04068 | best_loss=0.03896
Epoch 11/80: current_loss=0.03957 | best_loss=0.03896
Epoch 12/80: current_loss=0.03922 | best_loss=0.03896
Epoch 13/80: current_loss=0.03858 | best_loss=0.03858
Epoch 14/80: current_loss=0.03827 | best_loss=0.03827
Epoch 15/80: current_loss=0.03898 | best_loss=0.03827
Epoch 16/80: current_loss=0.03958 | best_loss=0.03827
Epoch 17/80: current_loss=0.03827 | best_loss=0.03827
Epoch 18/80: current_loss=0.03805 | best_loss=0.03805
Epoch 19/80: current_loss=0.03879 | best_loss=0.03805
Epoch 20/80: current_loss=0.03810 | best_loss=0.03805
Epoch 21/80: current_loss=0.03836 | best_loss=0.03805
Epoch 22/80: current_loss=0.03817 | best_loss=0.03805
Epoch 23/80: current_loss=0.03971 | best_loss=0.03805
Epoch 24/80: current_loss=0.03824 | best_loss=0.03805
Epoch 25/80: current_loss=0.03924 | best_loss=0.03805
Epoch 26/80: current_loss=0.03867 | best_loss=0.03805
Epoch 27/80: current_loss=0.03855 | best_loss=0.03805
Epoch 28/80: current_loss=0.03880 | best_loss=0.03805
Epoch 29/80: current_loss=0.03855 | best_loss=0.03805
Epoch 30/80: current_loss=0.03906 | best_loss=0.03805
Epoch 31/80: current_loss=0.03900 | best_loss=0.03805
Epoch 32/80: current_loss=0.03818 | best_loss=0.03805
Epoch 33/80: current_loss=0.03818 | best_loss=0.03805
Epoch 34/80: current_loss=0.03807 | best_loss=0.03805
Epoch 35/80: current_loss=0.03820 | best_loss=0.03805
Epoch 36/80: current_loss=0.03822 | best_loss=0.03805
Epoch 37/80: current_loss=0.03847 | best_loss=0.03805
Epoch 38/80: current_loss=0.03828 | best_loss=0.03805
Early Stopping at epoch 38
      explained_var=0.00504 | mse_loss=0.03897

----------------------------------------------
Params for Trial 67
{'learning_rate': 0.0001, 'weight_decay': 0.002869794754071066, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.17366 | best_loss=0.17366
Epoch 1/80: current_loss=0.05840 | best_loss=0.05840
Epoch 2/80: current_loss=0.04568 | best_loss=0.04568
Epoch 3/80: current_loss=0.04540 | best_loss=0.04540
Epoch 4/80: current_loss=0.04465 | best_loss=0.04465
Epoch 5/80: current_loss=0.04416 | best_loss=0.04416
Epoch 6/80: current_loss=0.04368 | best_loss=0.04368
Epoch 7/80: current_loss=0.04328 | best_loss=0.04328
Epoch 8/80: current_loss=0.04333 | best_loss=0.04328
Epoch 9/80: current_loss=0.04271 | best_loss=0.04271
Epoch 10/80: current_loss=0.04237 | best_loss=0.04237
Epoch 11/80: current_loss=0.04230 | best_loss=0.04230
Epoch 12/80: current_loss=0.04233 | best_loss=0.04230
Epoch 13/80: current_loss=0.04149 | best_loss=0.04149
Epoch 14/80: current_loss=0.04155 | best_loss=0.04149
Epoch 15/80: current_loss=0.04116 | best_loss=0.04116
Epoch 16/80: current_loss=0.04109 | best_loss=0.04109
Epoch 17/80: current_loss=0.04084 | best_loss=0.04084
Epoch 18/80: current_loss=0.04101 | best_loss=0.04084
Epoch 19/80: current_loss=0.04052 | best_loss=0.04052
Epoch 20/80: current_loss=0.04015 | best_loss=0.04015
Epoch 21/80: current_loss=0.04014 | best_loss=0.04014
Epoch 22/80: current_loss=0.04021 | best_loss=0.04014
Epoch 23/80: current_loss=0.03999 | best_loss=0.03999
Epoch 24/80: current_loss=0.03983 | best_loss=0.03983
Epoch 25/80: current_loss=0.04021 | best_loss=0.03983
Epoch 26/80: current_loss=0.03975 | best_loss=0.03975
Epoch 27/80: current_loss=0.03968 | best_loss=0.03968
Epoch 28/80: current_loss=0.03958 | best_loss=0.03958
Epoch 29/80: current_loss=0.03957 | best_loss=0.03957
Epoch 30/80: current_loss=0.03950 | best_loss=0.03950
Epoch 31/80: current_loss=0.03927 | best_loss=0.03927
Epoch 32/80: current_loss=0.03915 | best_loss=0.03915
Epoch 33/80: current_loss=0.03916 | best_loss=0.03915
Epoch 34/80: current_loss=0.03903 | best_loss=0.03903
Epoch 35/80: current_loss=0.03920 | best_loss=0.03903
Epoch 36/80: current_loss=0.03893 | best_loss=0.03893
Epoch 37/80: current_loss=0.03902 | best_loss=0.03893
Epoch 38/80: current_loss=0.03885 | best_loss=0.03885
Epoch 39/80: current_loss=0.03887 | best_loss=0.03885
Epoch 40/80: current_loss=0.03873 | best_loss=0.03873
Epoch 41/80: current_loss=0.03880 | best_loss=0.03873
Epoch 42/80: current_loss=0.03868 | best_loss=0.03868
Epoch 43/80: current_loss=0.03877 | best_loss=0.03868
Epoch 44/80: current_loss=0.03869 | best_loss=0.03868
Epoch 45/80: current_loss=0.03872 | best_loss=0.03868
Epoch 46/80: current_loss=0.03853 | best_loss=0.03853
Epoch 47/80: current_loss=0.03848 | best_loss=0.03848
Epoch 48/80: current_loss=0.03884 | best_loss=0.03848
Epoch 49/80: current_loss=0.03875 | best_loss=0.03848
Epoch 50/80: current_loss=0.03849 | best_loss=0.03848
Epoch 51/80: current_loss=0.03835 | best_loss=0.03835
Epoch 52/80: current_loss=0.03843 | best_loss=0.03835
Epoch 53/80: current_loss=0.03876 | best_loss=0.03835
Epoch 54/80: current_loss=0.03825 | best_loss=0.03825
Epoch 55/80: current_loss=0.03829 | best_loss=0.03825
Epoch 56/80: current_loss=0.03827 | best_loss=0.03825
Epoch 57/80: current_loss=0.03874 | best_loss=0.03825
Epoch 58/80: current_loss=0.03821 | best_loss=0.03821
Epoch 59/80: current_loss=0.03815 | best_loss=0.03815
Epoch 60/80: current_loss=0.03856 | best_loss=0.03815
Epoch 61/80: current_loss=0.03828 | best_loss=0.03815
Epoch 62/80: current_loss=0.03833 | best_loss=0.03815
Epoch 63/80: current_loss=0.03819 | best_loss=0.03815
Epoch 64/80: current_loss=0.03842 | best_loss=0.03815
Epoch 65/80: current_loss=0.03823 | best_loss=0.03815
Epoch 66/80: current_loss=0.03866 | best_loss=0.03815
Epoch 67/80: current_loss=0.03828 | best_loss=0.03815
Epoch 68/80: current_loss=0.03827 | best_loss=0.03815
Epoch 69/80: current_loss=0.03831 | best_loss=0.03815
Epoch 70/80: current_loss=0.03830 | best_loss=0.03815
Epoch 71/80: current_loss=0.03815 | best_loss=0.03815
Epoch 72/80: current_loss=0.03824 | best_loss=0.03815
Epoch 73/80: current_loss=0.03820 | best_loss=0.03815
Epoch 74/80: current_loss=0.03803 | best_loss=0.03803
Epoch 75/80: current_loss=0.03802 | best_loss=0.03802
Epoch 76/80: current_loss=0.03829 | best_loss=0.03802
Epoch 77/80: current_loss=0.03792 | best_loss=0.03792
Epoch 78/80: current_loss=0.03812 | best_loss=0.03792
Epoch 79/80: current_loss=0.03829 | best_loss=0.03792
      explained_var=0.00769 | mse_loss=0.03890

----------------------------------------------
Params for Trial 68
{'learning_rate': 0.001, 'weight_decay': 0.0007067338284013947, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04084 | best_loss=0.04084
Epoch 1/80: current_loss=0.03972 | best_loss=0.03972
Epoch 2/80: current_loss=0.03940 | best_loss=0.03940
Epoch 3/80: current_loss=0.03680 | best_loss=0.03680
Epoch 4/80: current_loss=0.03882 | best_loss=0.03680
Epoch 5/80: current_loss=0.03818 | best_loss=0.03680
Epoch 6/80: current_loss=0.03794 | best_loss=0.03680
Epoch 7/80: current_loss=0.03754 | best_loss=0.03680
Epoch 8/80: current_loss=0.03755 | best_loss=0.03680
Epoch 9/80: current_loss=0.03854 | best_loss=0.03680
Epoch 10/80: current_loss=0.03938 | best_loss=0.03680
Epoch 11/80: current_loss=0.03854 | best_loss=0.03680
Epoch 12/80: current_loss=0.04398 | best_loss=0.03680
Epoch 13/80: current_loss=0.03833 | best_loss=0.03680
Epoch 14/80: current_loss=0.03907 | best_loss=0.03680
Epoch 15/80: current_loss=0.03828 | best_loss=0.03680
Epoch 16/80: current_loss=0.04030 | best_loss=0.03680
Epoch 17/80: current_loss=0.04393 | best_loss=0.03680
Epoch 18/80: current_loss=0.03766 | best_loss=0.03680
Epoch 19/80: current_loss=0.03747 | best_loss=0.03680
Epoch 20/80: current_loss=0.04449 | best_loss=0.03680
Epoch 21/80: current_loss=0.03922 | best_loss=0.03680
Epoch 22/80: current_loss=0.03823 | best_loss=0.03680
Epoch 23/80: current_loss=0.03848 | best_loss=0.03680
Early Stopping at epoch 23
      explained_var=0.04000 | mse_loss=0.03758
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04172 | best_loss=0.04172
Epoch 1/80: current_loss=0.04304 | best_loss=0.04172
Epoch 2/80: current_loss=0.04176 | best_loss=0.04172
Epoch 3/80: current_loss=0.04169 | best_loss=0.04169
Epoch 4/80: current_loss=0.04278 | best_loss=0.04169
Epoch 5/80: current_loss=0.04580 | best_loss=0.04169
Epoch 6/80: current_loss=0.04157 | best_loss=0.04157
Epoch 7/80: current_loss=0.04157 | best_loss=0.04157
Epoch 8/80: current_loss=0.04478 | best_loss=0.04157
Epoch 9/80: current_loss=0.04282 | best_loss=0.04157
Epoch 10/80: current_loss=0.04215 | best_loss=0.04157
Epoch 11/80: current_loss=0.04220 | best_loss=0.04157
Epoch 12/80: current_loss=0.04229 | best_loss=0.04157
Epoch 13/80: current_loss=0.04383 | best_loss=0.04157
Epoch 14/80: current_loss=0.04295 | best_loss=0.04157
Epoch 15/80: current_loss=0.04206 | best_loss=0.04157
Epoch 16/80: current_loss=0.04162 | best_loss=0.04157
Epoch 17/80: current_loss=0.04761 | best_loss=0.04157
Epoch 18/80: current_loss=0.04265 | best_loss=0.04157
Epoch 19/80: current_loss=0.04230 | best_loss=0.04157
Epoch 20/80: current_loss=0.04191 | best_loss=0.04157
Epoch 21/80: current_loss=0.04184 | best_loss=0.04157
Epoch 22/80: current_loss=0.04283 | best_loss=0.04157
Epoch 23/80: current_loss=0.04181 | best_loss=0.04157
Epoch 24/80: current_loss=0.04295 | best_loss=0.04157
Epoch 25/80: current_loss=0.04271 | best_loss=0.04157
Epoch 26/80: current_loss=0.04237 | best_loss=0.04157
Epoch 27/80: current_loss=0.04186 | best_loss=0.04157
Early Stopping at epoch 27
      explained_var=0.02441 | mse_loss=0.04023
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02921 | best_loss=0.02921
Epoch 1/80: current_loss=0.02828 | best_loss=0.02828
Epoch 2/80: current_loss=0.02909 | best_loss=0.02828
Epoch 3/80: current_loss=0.02949 | best_loss=0.02828
Epoch 4/80: current_loss=0.02869 | best_loss=0.02828
Epoch 5/80: current_loss=0.03338 | best_loss=0.02828
Epoch 6/80: current_loss=0.02830 | best_loss=0.02828
Epoch 7/80: current_loss=0.02804 | best_loss=0.02804
Epoch 8/80: current_loss=0.02865 | best_loss=0.02804
Epoch 9/80: current_loss=0.02841 | best_loss=0.02804
Epoch 10/80: current_loss=0.02851 | best_loss=0.02804
Epoch 11/80: current_loss=0.02858 | best_loss=0.02804
Epoch 12/80: current_loss=0.02815 | best_loss=0.02804
Epoch 13/80: current_loss=0.03085 | best_loss=0.02804
Epoch 14/80: current_loss=0.02835 | best_loss=0.02804
Epoch 15/80: current_loss=0.02819 | best_loss=0.02804
Epoch 16/80: current_loss=0.02980 | best_loss=0.02804
Epoch 17/80: current_loss=0.02868 | best_loss=0.02804
Epoch 18/80: current_loss=0.02829 | best_loss=0.02804
Epoch 19/80: current_loss=0.03081 | best_loss=0.02804
Epoch 20/80: current_loss=0.03079 | best_loss=0.02804
Epoch 21/80: current_loss=0.02870 | best_loss=0.02804
Epoch 22/80: current_loss=0.02873 | best_loss=0.02804
Epoch 23/80: current_loss=0.02813 | best_loss=0.02804
Epoch 24/80: current_loss=0.02825 | best_loss=0.02804
Epoch 25/80: current_loss=0.03047 | best_loss=0.02804
Epoch 26/80: current_loss=0.02826 | best_loss=0.02804
Epoch 27/80: current_loss=0.02826 | best_loss=0.02804
Early Stopping at epoch 27
      explained_var=-0.00491 | mse_loss=0.02850

----------------------------------------------
Params for Trial 69
{'learning_rate': 0.0001, 'weight_decay': 0.00033641982680501174, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04221 | best_loss=0.04221
Epoch 1/80: current_loss=0.04118 | best_loss=0.04118
Epoch 2/80: current_loss=0.04040 | best_loss=0.04040
Epoch 3/80: current_loss=0.03976 | best_loss=0.03976
Epoch 4/80: current_loss=0.03941 | best_loss=0.03941
Epoch 5/80: current_loss=0.03979 | best_loss=0.03941
Epoch 6/80: current_loss=0.03900 | best_loss=0.03900
Epoch 7/80: current_loss=0.03864 | best_loss=0.03864
Epoch 8/80: current_loss=0.03872 | best_loss=0.03864
Epoch 9/80: current_loss=0.03831 | best_loss=0.03831
Epoch 10/80: current_loss=0.03834 | best_loss=0.03831
Epoch 11/80: current_loss=0.03819 | best_loss=0.03819
Epoch 12/80: current_loss=0.03831 | best_loss=0.03819
Epoch 13/80: current_loss=0.03790 | best_loss=0.03790
Epoch 14/80: current_loss=0.03788 | best_loss=0.03788
Epoch 15/80: current_loss=0.03785 | best_loss=0.03785
Epoch 16/80: current_loss=0.03774 | best_loss=0.03774
Epoch 17/80: current_loss=0.03793 | best_loss=0.03774
Epoch 18/80: current_loss=0.03787 | best_loss=0.03774
Epoch 19/80: current_loss=0.03777 | best_loss=0.03774
Epoch 20/80: current_loss=0.03784 | best_loss=0.03774
Epoch 21/80: current_loss=0.03770 | best_loss=0.03770
Epoch 22/80: current_loss=0.03795 | best_loss=0.03770
Epoch 23/80: current_loss=0.03763 | best_loss=0.03763
Epoch 24/80: current_loss=0.03758 | best_loss=0.03758
Epoch 25/80: current_loss=0.03763 | best_loss=0.03758
Epoch 26/80: current_loss=0.03751 | best_loss=0.03751
Epoch 27/80: current_loss=0.03832 | best_loss=0.03751
Epoch 28/80: current_loss=0.03753 | best_loss=0.03751
Epoch 29/80: current_loss=0.03754 | best_loss=0.03751
Epoch 30/80: current_loss=0.03755 | best_loss=0.03751
Epoch 31/80: current_loss=0.03752 | best_loss=0.03751
Epoch 32/80: current_loss=0.03763 | best_loss=0.03751
Epoch 33/80: current_loss=0.03734 | best_loss=0.03734
Epoch 34/80: current_loss=0.03748 | best_loss=0.03734
Epoch 35/80: current_loss=0.03805 | best_loss=0.03734
Epoch 36/80: current_loss=0.03749 | best_loss=0.03734
Epoch 37/80: current_loss=0.03757 | best_loss=0.03734
Epoch 38/80: current_loss=0.03768 | best_loss=0.03734
Epoch 39/80: current_loss=0.03758 | best_loss=0.03734
Epoch 40/80: current_loss=0.03754 | best_loss=0.03734
Epoch 41/80: current_loss=0.03754 | best_loss=0.03734
Epoch 42/80: current_loss=0.03740 | best_loss=0.03734
Epoch 43/80: current_loss=0.03757 | best_loss=0.03734
Epoch 44/80: current_loss=0.03746 | best_loss=0.03734
Epoch 45/80: current_loss=0.03755 | best_loss=0.03734
Epoch 46/80: current_loss=0.03769 | best_loss=0.03734
Epoch 47/80: current_loss=0.03781 | best_loss=0.03734
Epoch 48/80: current_loss=0.03758 | best_loss=0.03734
Epoch 49/80: current_loss=0.03843 | best_loss=0.03734
Epoch 50/80: current_loss=0.03736 | best_loss=0.03734
Epoch 51/80: current_loss=0.03752 | best_loss=0.03734
Epoch 52/80: current_loss=0.03744 | best_loss=0.03734
Epoch 53/80: current_loss=0.03816 | best_loss=0.03734
Early Stopping at epoch 53
      explained_var=0.02552 | mse_loss=0.03827
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04177 | best_loss=0.04177
Epoch 1/80: current_loss=0.04126 | best_loss=0.04126
Epoch 2/80: current_loss=0.04143 | best_loss=0.04126
Epoch 3/80: current_loss=0.04179 | best_loss=0.04126
Epoch 4/80: current_loss=0.04153 | best_loss=0.04126
Epoch 5/80: current_loss=0.04144 | best_loss=0.04126
Epoch 6/80: current_loss=0.04130 | best_loss=0.04126
Epoch 7/80: current_loss=0.04143 | best_loss=0.04126
Epoch 8/80: current_loss=0.04170 | best_loss=0.04126
Epoch 9/80: current_loss=0.04142 | best_loss=0.04126
Epoch 10/80: current_loss=0.04147 | best_loss=0.04126
Epoch 11/80: current_loss=0.04296 | best_loss=0.04126
Epoch 12/80: current_loss=0.04147 | best_loss=0.04126
Epoch 13/80: current_loss=0.04148 | best_loss=0.04126
Epoch 14/80: current_loss=0.04134 | best_loss=0.04126
Epoch 15/80: current_loss=0.04134 | best_loss=0.04126
Epoch 16/80: current_loss=0.04137 | best_loss=0.04126
Epoch 17/80: current_loss=0.04148 | best_loss=0.04126
Epoch 18/80: current_loss=0.04221 | best_loss=0.04126
Epoch 19/80: current_loss=0.04148 | best_loss=0.04126
Epoch 20/80: current_loss=0.04168 | best_loss=0.04126
Epoch 21/80: current_loss=0.04137 | best_loss=0.04126
Early Stopping at epoch 21
      explained_var=0.03508 | mse_loss=0.03980
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02803 | best_loss=0.02803
Epoch 1/80: current_loss=0.02805 | best_loss=0.02803
Epoch 2/80: current_loss=0.02834 | best_loss=0.02803
Epoch 3/80: current_loss=0.02830 | best_loss=0.02803
Epoch 4/80: current_loss=0.02843 | best_loss=0.02803
Epoch 5/80: current_loss=0.02871 | best_loss=0.02803
Epoch 6/80: current_loss=0.02968 | best_loss=0.02803
Epoch 7/80: current_loss=0.02822 | best_loss=0.02803
Epoch 8/80: current_loss=0.02859 | best_loss=0.02803
Epoch 9/80: current_loss=0.02843 | best_loss=0.02803
Epoch 10/80: current_loss=0.02830 | best_loss=0.02803
Epoch 11/80: current_loss=0.02837 | best_loss=0.02803
Epoch 12/80: current_loss=0.02887 | best_loss=0.02803
Epoch 13/80: current_loss=0.02823 | best_loss=0.02803
Epoch 14/80: current_loss=0.02883 | best_loss=0.02803
Epoch 15/80: current_loss=0.02825 | best_loss=0.02803
Epoch 16/80: current_loss=0.02863 | best_loss=0.02803
Epoch 17/80: current_loss=0.02818 | best_loss=0.02803
Epoch 18/80: current_loss=0.02832 | best_loss=0.02803
Epoch 19/80: current_loss=0.02840 | best_loss=0.02803
Epoch 20/80: current_loss=0.02842 | best_loss=0.02803
Early Stopping at epoch 20
      explained_var=-0.00131 | mse_loss=0.02843
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03332 | best_loss=0.03332
Epoch 1/80: current_loss=0.03311 | best_loss=0.03311
Epoch 2/80: current_loss=0.03302 | best_loss=0.03302
Epoch 3/80: current_loss=0.03313 | best_loss=0.03302
Epoch 4/80: current_loss=0.03323 | best_loss=0.03302
Epoch 5/80: current_loss=0.03311 | best_loss=0.03302
Epoch 6/80: current_loss=0.03319 | best_loss=0.03302
Epoch 7/80: current_loss=0.03305 | best_loss=0.03302
Epoch 8/80: current_loss=0.03347 | best_loss=0.03302
Epoch 9/80: current_loss=0.03307 | best_loss=0.03302
Epoch 10/80: current_loss=0.03328 | best_loss=0.03302
Epoch 11/80: current_loss=0.03322 | best_loss=0.03302
Epoch 12/80: current_loss=0.03327 | best_loss=0.03302
Epoch 13/80: current_loss=0.03327 | best_loss=0.03302
Epoch 14/80: current_loss=0.03337 | best_loss=0.03302
Epoch 15/80: current_loss=0.03322 | best_loss=0.03302
Epoch 16/80: current_loss=0.03329 | best_loss=0.03302
Epoch 17/80: current_loss=0.03327 | best_loss=0.03302
Epoch 18/80: current_loss=0.03324 | best_loss=0.03302
Epoch 19/80: current_loss=0.03322 | best_loss=0.03302
Epoch 20/80: current_loss=0.03334 | best_loss=0.03302
Epoch 21/80: current_loss=0.03327 | best_loss=0.03302
Epoch 22/80: current_loss=0.03321 | best_loss=0.03302
Early Stopping at epoch 22
      explained_var=-0.01037 | mse_loss=0.03295
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03516 | best_loss=0.03516
Epoch 1/80: current_loss=0.03514 | best_loss=0.03514
Epoch 2/80: current_loss=0.03519 | best_loss=0.03514
Epoch 3/80: current_loss=0.03522 | best_loss=0.03514
Epoch 4/80: current_loss=0.03527 | best_loss=0.03514
Epoch 5/80: current_loss=0.03533 | best_loss=0.03514
Epoch 6/80: current_loss=0.03549 | best_loss=0.03514
Epoch 7/80: current_loss=0.03543 | best_loss=0.03514
Epoch 8/80: current_loss=0.03532 | best_loss=0.03514
Epoch 9/80: current_loss=0.03553 | best_loss=0.03514
Epoch 10/80: current_loss=0.03548 | best_loss=0.03514
Epoch 11/80: current_loss=0.03543 | best_loss=0.03514
Epoch 12/80: current_loss=0.03557 | best_loss=0.03514
Epoch 13/80: current_loss=0.03540 | best_loss=0.03514
Epoch 14/80: current_loss=0.03549 | best_loss=0.03514
Epoch 15/80: current_loss=0.03548 | best_loss=0.03514
Epoch 16/80: current_loss=0.03542 | best_loss=0.03514
Epoch 17/80: current_loss=0.03544 | best_loss=0.03514
Epoch 18/80: current_loss=0.03530 | best_loss=0.03514
Epoch 19/80: current_loss=0.03525 | best_loss=0.03514
Epoch 20/80: current_loss=0.03529 | best_loss=0.03514
Epoch 21/80: current_loss=0.03529 | best_loss=0.03514
Early Stopping at epoch 21
      explained_var=0.02681 | mse_loss=0.03492
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=0.01515| avg_loss=0.03487
----------------------------------------------


----------------------------------------------
Params for Trial 70
{'learning_rate': 0.0001, 'weight_decay': 0.0010790190597535544, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04797 | best_loss=0.04797
Epoch 1/80: current_loss=0.04498 | best_loss=0.04498
Epoch 2/80: current_loss=0.04366 | best_loss=0.04366
Epoch 3/80: current_loss=0.04274 | best_loss=0.04274
Epoch 4/80: current_loss=0.04166 | best_loss=0.04166
Epoch 5/80: current_loss=0.04121 | best_loss=0.04121
Epoch 6/80: current_loss=0.04063 | best_loss=0.04063
Epoch 7/80: current_loss=0.04015 | best_loss=0.04015
Epoch 8/80: current_loss=0.03966 | best_loss=0.03966
Epoch 9/80: current_loss=0.03935 | best_loss=0.03935
Epoch 10/80: current_loss=0.03912 | best_loss=0.03912
Epoch 11/80: current_loss=0.03965 | best_loss=0.03912
Epoch 12/80: current_loss=0.03876 | best_loss=0.03876
Epoch 13/80: current_loss=0.03876 | best_loss=0.03876
Epoch 14/80: current_loss=0.03850 | best_loss=0.03850
Epoch 15/80: current_loss=0.03848 | best_loss=0.03848
Epoch 16/80: current_loss=0.03837 | best_loss=0.03837
Epoch 17/80: current_loss=0.03816 | best_loss=0.03816
Epoch 18/80: current_loss=0.03839 | best_loss=0.03816
Epoch 19/80: current_loss=0.03815 | best_loss=0.03815
Epoch 20/80: current_loss=0.03804 | best_loss=0.03804
Epoch 21/80: current_loss=0.03802 | best_loss=0.03802
Epoch 22/80: current_loss=0.03828 | best_loss=0.03802
Epoch 23/80: current_loss=0.03809 | best_loss=0.03802
Epoch 24/80: current_loss=0.03816 | best_loss=0.03802
Epoch 25/80: current_loss=0.03817 | best_loss=0.03802
Epoch 26/80: current_loss=0.03802 | best_loss=0.03802
Epoch 27/80: current_loss=0.03794 | best_loss=0.03794
Epoch 28/80: current_loss=0.03792 | best_loss=0.03792
Epoch 29/80: current_loss=0.03798 | best_loss=0.03792
Epoch 30/80: current_loss=0.03857 | best_loss=0.03792
Epoch 31/80: current_loss=0.03768 | best_loss=0.03768
Epoch 32/80: current_loss=0.03774 | best_loss=0.03768
Epoch 33/80: current_loss=0.03801 | best_loss=0.03768
Epoch 34/80: current_loss=0.03760 | best_loss=0.03760
Epoch 35/80: current_loss=0.03751 | best_loss=0.03751
Epoch 36/80: current_loss=0.03751 | best_loss=0.03751
Epoch 37/80: current_loss=0.03777 | best_loss=0.03751
Epoch 38/80: current_loss=0.03745 | best_loss=0.03745
Epoch 39/80: current_loss=0.03765 | best_loss=0.03745
Epoch 40/80: current_loss=0.03752 | best_loss=0.03745
Epoch 41/80: current_loss=0.03756 | best_loss=0.03745
Epoch 42/80: current_loss=0.03757 | best_loss=0.03745
Epoch 43/80: current_loss=0.03763 | best_loss=0.03745
Epoch 44/80: current_loss=0.03758 | best_loss=0.03745
Epoch 45/80: current_loss=0.03755 | best_loss=0.03745
Epoch 46/80: current_loss=0.03766 | best_loss=0.03745
Epoch 47/80: current_loss=0.03766 | best_loss=0.03745
Epoch 48/80: current_loss=0.03782 | best_loss=0.03745
Epoch 49/80: current_loss=0.03759 | best_loss=0.03745
Epoch 50/80: current_loss=0.03754 | best_loss=0.03745
Epoch 51/80: current_loss=0.03759 | best_loss=0.03745
Epoch 52/80: current_loss=0.03758 | best_loss=0.03745
Epoch 53/80: current_loss=0.03760 | best_loss=0.03745
Epoch 54/80: current_loss=0.03779 | best_loss=0.03745
Epoch 55/80: current_loss=0.03764 | best_loss=0.03745
Epoch 56/80: current_loss=0.03765 | best_loss=0.03745
Epoch 57/80: current_loss=0.03768 | best_loss=0.03745
Epoch 58/80: current_loss=0.03783 | best_loss=0.03745
Early Stopping at epoch 58
      explained_var=0.01946 | mse_loss=0.03838
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04125 | best_loss=0.04125
Epoch 1/80: current_loss=0.04148 | best_loss=0.04125
Epoch 2/80: current_loss=0.04149 | best_loss=0.04125
Epoch 3/80: current_loss=0.04135 | best_loss=0.04125
Epoch 4/80: current_loss=0.04135 | best_loss=0.04125
Epoch 5/80: current_loss=0.04141 | best_loss=0.04125
Epoch 6/80: current_loss=0.04135 | best_loss=0.04125
Epoch 7/80: current_loss=0.04131 | best_loss=0.04125
Epoch 8/80: current_loss=0.04125 | best_loss=0.04125
Epoch 9/80: current_loss=0.04152 | best_loss=0.04125
Epoch 10/80: current_loss=0.04139 | best_loss=0.04125
Epoch 11/80: current_loss=0.04195 | best_loss=0.04125
Epoch 12/80: current_loss=0.04161 | best_loss=0.04125
Epoch 13/80: current_loss=0.04172 | best_loss=0.04125
Epoch 14/80: current_loss=0.04143 | best_loss=0.04125
Epoch 15/80: current_loss=0.04142 | best_loss=0.04125
Epoch 16/80: current_loss=0.04154 | best_loss=0.04125
Epoch 17/80: current_loss=0.04170 | best_loss=0.04125
Epoch 18/80: current_loss=0.04216 | best_loss=0.04125
Epoch 19/80: current_loss=0.04175 | best_loss=0.04125
Epoch 20/80: current_loss=0.04173 | best_loss=0.04125
Early Stopping at epoch 20
      explained_var=0.04123 | mse_loss=0.03973
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02932 | best_loss=0.02932
Epoch 1/80: current_loss=0.02811 | best_loss=0.02811
Epoch 2/80: current_loss=0.02869 | best_loss=0.02811
Epoch 3/80: current_loss=0.02849 | best_loss=0.02811
Epoch 4/80: current_loss=0.02815 | best_loss=0.02811
Epoch 5/80: current_loss=0.02826 | best_loss=0.02811
Epoch 6/80: current_loss=0.02924 | best_loss=0.02811
Epoch 7/80: current_loss=0.02851 | best_loss=0.02811
Epoch 8/80: current_loss=0.02840 | best_loss=0.02811
Epoch 9/80: current_loss=0.02854 | best_loss=0.02811
Epoch 10/80: current_loss=0.02905 | best_loss=0.02811
Epoch 11/80: current_loss=0.02843 | best_loss=0.02811
Epoch 12/80: current_loss=0.02828 | best_loss=0.02811
Epoch 13/80: current_loss=0.02819 | best_loss=0.02811
Epoch 14/80: current_loss=0.02834 | best_loss=0.02811
Epoch 15/80: current_loss=0.02831 | best_loss=0.02811
Epoch 16/80: current_loss=0.02825 | best_loss=0.02811
Epoch 17/80: current_loss=0.02838 | best_loss=0.02811
Epoch 18/80: current_loss=0.02868 | best_loss=0.02811
Epoch 19/80: current_loss=0.02853 | best_loss=0.02811
Epoch 20/80: current_loss=0.02838 | best_loss=0.02811
Epoch 21/80: current_loss=0.02816 | best_loss=0.02811
Early Stopping at epoch 21
      explained_var=-0.00636 | mse_loss=0.02856

----------------------------------------------
Params for Trial 71
{'learning_rate': 0.0001, 'weight_decay': 0.0003020300022401741, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04375 | best_loss=0.04375
Epoch 1/80: current_loss=0.04125 | best_loss=0.04125
Epoch 2/80: current_loss=0.04026 | best_loss=0.04026
Epoch 3/80: current_loss=0.03996 | best_loss=0.03996
Epoch 4/80: current_loss=0.03951 | best_loss=0.03951
Epoch 5/80: current_loss=0.03929 | best_loss=0.03929
Epoch 6/80: current_loss=0.03944 | best_loss=0.03929
Epoch 7/80: current_loss=0.03879 | best_loss=0.03879
Epoch 8/80: current_loss=0.03850 | best_loss=0.03850
Epoch 9/80: current_loss=0.03837 | best_loss=0.03837
Epoch 10/80: current_loss=0.03826 | best_loss=0.03826
Epoch 11/80: current_loss=0.03804 | best_loss=0.03804
Epoch 12/80: current_loss=0.03802 | best_loss=0.03802
Epoch 13/80: current_loss=0.03831 | best_loss=0.03802
Epoch 14/80: current_loss=0.03803 | best_loss=0.03802
Epoch 15/80: current_loss=0.03832 | best_loss=0.03802
Epoch 16/80: current_loss=0.03771 | best_loss=0.03771
Epoch 17/80: current_loss=0.03772 | best_loss=0.03771
Epoch 18/80: current_loss=0.03762 | best_loss=0.03762
Epoch 19/80: current_loss=0.03759 | best_loss=0.03759
Epoch 20/80: current_loss=0.03778 | best_loss=0.03759
Epoch 21/80: current_loss=0.03777 | best_loss=0.03759
Epoch 22/80: current_loss=0.03807 | best_loss=0.03759
Epoch 23/80: current_loss=0.03770 | best_loss=0.03759
Epoch 24/80: current_loss=0.03779 | best_loss=0.03759
Epoch 25/80: current_loss=0.03763 | best_loss=0.03759
Epoch 26/80: current_loss=0.03787 | best_loss=0.03759
Epoch 27/80: current_loss=0.03761 | best_loss=0.03759
Epoch 28/80: current_loss=0.03755 | best_loss=0.03755
Epoch 29/80: current_loss=0.03777 | best_loss=0.03755
Epoch 30/80: current_loss=0.03772 | best_loss=0.03755
Epoch 31/80: current_loss=0.03808 | best_loss=0.03755
Epoch 32/80: current_loss=0.03794 | best_loss=0.03755
Epoch 33/80: current_loss=0.03777 | best_loss=0.03755
Epoch 34/80: current_loss=0.03783 | best_loss=0.03755
Epoch 35/80: current_loss=0.03771 | best_loss=0.03755
Epoch 36/80: current_loss=0.03873 | best_loss=0.03755
Epoch 37/80: current_loss=0.03763 | best_loss=0.03755
Epoch 38/80: current_loss=0.03811 | best_loss=0.03755
Epoch 39/80: current_loss=0.03761 | best_loss=0.03755
Epoch 40/80: current_loss=0.03755 | best_loss=0.03755
Epoch 41/80: current_loss=0.03785 | best_loss=0.03755
Epoch 42/80: current_loss=0.03812 | best_loss=0.03755
Epoch 43/80: current_loss=0.03762 | best_loss=0.03755
Epoch 44/80: current_loss=0.03778 | best_loss=0.03755
Epoch 45/80: current_loss=0.03787 | best_loss=0.03755
Epoch 46/80: current_loss=0.03770 | best_loss=0.03755
Epoch 47/80: current_loss=0.03762 | best_loss=0.03755
Epoch 48/80: current_loss=0.03760 | best_loss=0.03755
Epoch 49/80: current_loss=0.03816 | best_loss=0.03755
Epoch 50/80: current_loss=0.03760 | best_loss=0.03755
Epoch 51/80: current_loss=0.03763 | best_loss=0.03755
Epoch 52/80: current_loss=0.03760 | best_loss=0.03755
Epoch 53/80: current_loss=0.03774 | best_loss=0.03755
Epoch 54/80: current_loss=0.03760 | best_loss=0.03755
Epoch 55/80: current_loss=0.03754 | best_loss=0.03754
Epoch 56/80: current_loss=0.03759 | best_loss=0.03754
Epoch 57/80: current_loss=0.03750 | best_loss=0.03750
Epoch 58/80: current_loss=0.03776 | best_loss=0.03750
Epoch 59/80: current_loss=0.03778 | best_loss=0.03750
Epoch 60/80: current_loss=0.03771 | best_loss=0.03750
Epoch 61/80: current_loss=0.03798 | best_loss=0.03750
Epoch 62/80: current_loss=0.03822 | best_loss=0.03750
Epoch 63/80: current_loss=0.03754 | best_loss=0.03750
Epoch 64/80: current_loss=0.03763 | best_loss=0.03750
Epoch 65/80: current_loss=0.03747 | best_loss=0.03747
Epoch 66/80: current_loss=0.03752 | best_loss=0.03747
Epoch 67/80: current_loss=0.03776 | best_loss=0.03747
Epoch 68/80: current_loss=0.03754 | best_loss=0.03747
Epoch 69/80: current_loss=0.03758 | best_loss=0.03747
Epoch 70/80: current_loss=0.03758 | best_loss=0.03747
Epoch 71/80: current_loss=0.03764 | best_loss=0.03747
Epoch 72/80: current_loss=0.03762 | best_loss=0.03747
Epoch 73/80: current_loss=0.03773 | best_loss=0.03747
Epoch 74/80: current_loss=0.03767 | best_loss=0.03747
Epoch 75/80: current_loss=0.03766 | best_loss=0.03747
Epoch 76/80: current_loss=0.03779 | best_loss=0.03747
Epoch 77/80: current_loss=0.03775 | best_loss=0.03747
Epoch 78/80: current_loss=0.03788 | best_loss=0.03747
Epoch 79/80: current_loss=0.03759 | best_loss=0.03747
      explained_var=0.02000 | mse_loss=0.03832
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04135 | best_loss=0.04135
Epoch 1/80: current_loss=0.04133 | best_loss=0.04133
Epoch 2/80: current_loss=0.04136 | best_loss=0.04133
Epoch 3/80: current_loss=0.04140 | best_loss=0.04133
Epoch 4/80: current_loss=0.04142 | best_loss=0.04133
Epoch 5/80: current_loss=0.04163 | best_loss=0.04133
Epoch 6/80: current_loss=0.04142 | best_loss=0.04133
Epoch 7/80: current_loss=0.04168 | best_loss=0.04133
Epoch 8/80: current_loss=0.04127 | best_loss=0.04127
Epoch 9/80: current_loss=0.04123 | best_loss=0.04123
Epoch 10/80: current_loss=0.04174 | best_loss=0.04123
Epoch 11/80: current_loss=0.04143 | best_loss=0.04123
Epoch 12/80: current_loss=0.04134 | best_loss=0.04123
Epoch 13/80: current_loss=0.04191 | best_loss=0.04123
Epoch 14/80: current_loss=0.04130 | best_loss=0.04123
Epoch 15/80: current_loss=0.04141 | best_loss=0.04123
Epoch 16/80: current_loss=0.04130 | best_loss=0.04123
Epoch 17/80: current_loss=0.04150 | best_loss=0.04123
Epoch 18/80: current_loss=0.04152 | best_loss=0.04123
Epoch 19/80: current_loss=0.04151 | best_loss=0.04123
Epoch 20/80: current_loss=0.04160 | best_loss=0.04123
Epoch 21/80: current_loss=0.04159 | best_loss=0.04123
Epoch 22/80: current_loss=0.04151 | best_loss=0.04123
Epoch 23/80: current_loss=0.04161 | best_loss=0.04123
Epoch 24/80: current_loss=0.04234 | best_loss=0.04123
Epoch 25/80: current_loss=0.04152 | best_loss=0.04123
Epoch 26/80: current_loss=0.04157 | best_loss=0.04123
Epoch 27/80: current_loss=0.04142 | best_loss=0.04123
Epoch 28/80: current_loss=0.04158 | best_loss=0.04123
Epoch 29/80: current_loss=0.04148 | best_loss=0.04123
Early Stopping at epoch 29
      explained_var=0.03569 | mse_loss=0.03976
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02789 | best_loss=0.02789
Epoch 1/80: current_loss=0.02812 | best_loss=0.02789
Epoch 2/80: current_loss=0.02801 | best_loss=0.02789
Epoch 3/80: current_loss=0.02874 | best_loss=0.02789
Epoch 4/80: current_loss=0.02802 | best_loss=0.02789
Epoch 5/80: current_loss=0.02845 | best_loss=0.02789
Epoch 6/80: current_loss=0.02821 | best_loss=0.02789
Epoch 7/80: current_loss=0.02849 | best_loss=0.02789
Epoch 8/80: current_loss=0.02799 | best_loss=0.02789
Epoch 9/80: current_loss=0.02807 | best_loss=0.02789
Epoch 10/80: current_loss=0.02876 | best_loss=0.02789
Epoch 11/80: current_loss=0.02829 | best_loss=0.02789
Epoch 12/80: current_loss=0.02820 | best_loss=0.02789
Epoch 13/80: current_loss=0.02830 | best_loss=0.02789
Epoch 14/80: current_loss=0.02819 | best_loss=0.02789
Epoch 15/80: current_loss=0.02855 | best_loss=0.02789
Epoch 16/80: current_loss=0.02890 | best_loss=0.02789
Epoch 17/80: current_loss=0.02841 | best_loss=0.02789
Epoch 18/80: current_loss=0.02848 | best_loss=0.02789
Epoch 19/80: current_loss=0.02854 | best_loss=0.02789
Epoch 20/80: current_loss=0.02855 | best_loss=0.02789
Early Stopping at epoch 20
      explained_var=0.00008 | mse_loss=0.02836
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03302 | best_loss=0.03302
Epoch 1/80: current_loss=0.03328 | best_loss=0.03302
Epoch 2/80: current_loss=0.03303 | best_loss=0.03302
Epoch 3/80: current_loss=0.03300 | best_loss=0.03300
Epoch 4/80: current_loss=0.03298 | best_loss=0.03298
Epoch 5/80: current_loss=0.03321 | best_loss=0.03298
Epoch 6/80: current_loss=0.03309 | best_loss=0.03298
Epoch 7/80: current_loss=0.03297 | best_loss=0.03297
Epoch 8/80: current_loss=0.03294 | best_loss=0.03294
Epoch 9/80: current_loss=0.03308 | best_loss=0.03294
Epoch 10/80: current_loss=0.03308 | best_loss=0.03294
Epoch 11/80: current_loss=0.03313 | best_loss=0.03294
Epoch 12/80: current_loss=0.03312 | best_loss=0.03294
Epoch 13/80: current_loss=0.03321 | best_loss=0.03294
Epoch 14/80: current_loss=0.03337 | best_loss=0.03294
Epoch 15/80: current_loss=0.03323 | best_loss=0.03294
Epoch 16/80: current_loss=0.03328 | best_loss=0.03294
Epoch 17/80: current_loss=0.03319 | best_loss=0.03294
Epoch 18/80: current_loss=0.03333 | best_loss=0.03294
Epoch 19/80: current_loss=0.03320 | best_loss=0.03294
Epoch 20/80: current_loss=0.03345 | best_loss=0.03294
Epoch 21/80: current_loss=0.03312 | best_loss=0.03294
Epoch 22/80: current_loss=0.03330 | best_loss=0.03294
Epoch 23/80: current_loss=0.03305 | best_loss=0.03294
Epoch 24/80: current_loss=0.03306 | best_loss=0.03294
Epoch 25/80: current_loss=0.03304 | best_loss=0.03294
Epoch 26/80: current_loss=0.03359 | best_loss=0.03294
Epoch 27/80: current_loss=0.03305 | best_loss=0.03294
Epoch 28/80: current_loss=0.03320 | best_loss=0.03294
Early Stopping at epoch 28
      explained_var=-0.00827 | mse_loss=0.03287
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03538 | best_loss=0.03538
Epoch 1/80: current_loss=0.03535 | best_loss=0.03535
Epoch 2/80: current_loss=0.03541 | best_loss=0.03535
Epoch 3/80: current_loss=0.03546 | best_loss=0.03535
Epoch 4/80: current_loss=0.03546 | best_loss=0.03535
Epoch 5/80: current_loss=0.03559 | best_loss=0.03535
Epoch 6/80: current_loss=0.03549 | best_loss=0.03535
Epoch 7/80: current_loss=0.03543 | best_loss=0.03535
Epoch 8/80: current_loss=0.03537 | best_loss=0.03535
Epoch 9/80: current_loss=0.03539 | best_loss=0.03535
Epoch 10/80: current_loss=0.03546 | best_loss=0.03535
Epoch 11/80: current_loss=0.03543 | best_loss=0.03535
Epoch 12/80: current_loss=0.03540 | best_loss=0.03535
Epoch 13/80: current_loss=0.03546 | best_loss=0.03535
Epoch 14/80: current_loss=0.03550 | best_loss=0.03535
Epoch 15/80: current_loss=0.03584 | best_loss=0.03535
Epoch 16/80: current_loss=0.03544 | best_loss=0.03535
Epoch 17/80: current_loss=0.03540 | best_loss=0.03535
Epoch 18/80: current_loss=0.03540 | best_loss=0.03535
Epoch 19/80: current_loss=0.03546 | best_loss=0.03535
Epoch 20/80: current_loss=0.03545 | best_loss=0.03535
Epoch 21/80: current_loss=0.03539 | best_loss=0.03535
Early Stopping at epoch 21
      explained_var=0.01934 | mse_loss=0.03520
----------------------------------------------
Average early_stopping_point: 19| avg_exp_var=0.01337| avg_loss=0.03490
----------------------------------------------


----------------------------------------------
Params for Trial 72
{'learning_rate': 0.0001, 'weight_decay': 0.0005884352131765068, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04258 | best_loss=0.04258
Epoch 1/80: current_loss=0.04173 | best_loss=0.04173
Epoch 2/80: current_loss=0.04059 | best_loss=0.04059
Epoch 3/80: current_loss=0.04018 | best_loss=0.04018
Epoch 4/80: current_loss=0.03996 | best_loss=0.03996
Epoch 5/80: current_loss=0.03923 | best_loss=0.03923
Epoch 6/80: current_loss=0.03899 | best_loss=0.03899
Epoch 7/80: current_loss=0.03889 | best_loss=0.03889
Epoch 8/80: current_loss=0.03870 | best_loss=0.03870
Epoch 9/80: current_loss=0.03845 | best_loss=0.03845
Epoch 10/80: current_loss=0.03841 | best_loss=0.03841
Epoch 11/80: current_loss=0.03818 | best_loss=0.03818
Epoch 12/80: current_loss=0.03809 | best_loss=0.03809
Epoch 13/80: current_loss=0.03799 | best_loss=0.03799
Epoch 14/80: current_loss=0.03826 | best_loss=0.03799
Epoch 15/80: current_loss=0.03799 | best_loss=0.03799
Epoch 16/80: current_loss=0.03792 | best_loss=0.03792
Epoch 17/80: current_loss=0.03773 | best_loss=0.03773
Epoch 18/80: current_loss=0.03781 | best_loss=0.03773
Epoch 19/80: current_loss=0.03758 | best_loss=0.03758
Epoch 20/80: current_loss=0.03769 | best_loss=0.03758
Epoch 21/80: current_loss=0.03812 | best_loss=0.03758
Epoch 22/80: current_loss=0.03757 | best_loss=0.03757
Epoch 23/80: current_loss=0.03765 | best_loss=0.03757
Epoch 24/80: current_loss=0.03753 | best_loss=0.03753
Epoch 25/80: current_loss=0.03743 | best_loss=0.03743
Epoch 26/80: current_loss=0.03745 | best_loss=0.03743
Epoch 27/80: current_loss=0.03732 | best_loss=0.03732
Epoch 28/80: current_loss=0.03757 | best_loss=0.03732
Epoch 29/80: current_loss=0.03744 | best_loss=0.03732
Epoch 30/80: current_loss=0.03739 | best_loss=0.03732
Epoch 31/80: current_loss=0.03742 | best_loss=0.03732
Epoch 32/80: current_loss=0.03756 | best_loss=0.03732
Epoch 33/80: current_loss=0.03743 | best_loss=0.03732
Epoch 34/80: current_loss=0.03777 | best_loss=0.03732
Epoch 35/80: current_loss=0.03751 | best_loss=0.03732
Epoch 36/80: current_loss=0.03758 | best_loss=0.03732
Epoch 37/80: current_loss=0.03751 | best_loss=0.03732
Epoch 38/80: current_loss=0.03763 | best_loss=0.03732
Epoch 39/80: current_loss=0.03791 | best_loss=0.03732
Epoch 40/80: current_loss=0.03746 | best_loss=0.03732
Epoch 41/80: current_loss=0.03763 | best_loss=0.03732
Epoch 42/80: current_loss=0.03748 | best_loss=0.03732
Epoch 43/80: current_loss=0.03746 | best_loss=0.03732
Epoch 44/80: current_loss=0.03737 | best_loss=0.03732
Epoch 45/80: current_loss=0.03766 | best_loss=0.03732
Epoch 46/80: current_loss=0.03742 | best_loss=0.03732
Epoch 47/80: current_loss=0.03749 | best_loss=0.03732
Early Stopping at epoch 47
      explained_var=0.02255 | mse_loss=0.03826
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04181 | best_loss=0.04181
Epoch 1/80: current_loss=0.04169 | best_loss=0.04169
Epoch 2/80: current_loss=0.04156 | best_loss=0.04156
Epoch 3/80: current_loss=0.04154 | best_loss=0.04154
Epoch 4/80: current_loss=0.04205 | best_loss=0.04154
Epoch 5/80: current_loss=0.04167 | best_loss=0.04154
Epoch 6/80: current_loss=0.04227 | best_loss=0.04154
Epoch 7/80: current_loss=0.04176 | best_loss=0.04154
Epoch 8/80: current_loss=0.04196 | best_loss=0.04154
Epoch 9/80: current_loss=0.04209 | best_loss=0.04154
Epoch 10/80: current_loss=0.04159 | best_loss=0.04154
Epoch 11/80: current_loss=0.04201 | best_loss=0.04154
Epoch 12/80: current_loss=0.04151 | best_loss=0.04151
Epoch 13/80: current_loss=0.04149 | best_loss=0.04149
Epoch 14/80: current_loss=0.04180 | best_loss=0.04149
Epoch 15/80: current_loss=0.04211 | best_loss=0.04149
Epoch 16/80: current_loss=0.04178 | best_loss=0.04149
Epoch 17/80: current_loss=0.04172 | best_loss=0.04149
Epoch 18/80: current_loss=0.04169 | best_loss=0.04149
Epoch 19/80: current_loss=0.04179 | best_loss=0.04149
Epoch 20/80: current_loss=0.04166 | best_loss=0.04149
Epoch 21/80: current_loss=0.04170 | best_loss=0.04149
Epoch 22/80: current_loss=0.04174 | best_loss=0.04149
Epoch 23/80: current_loss=0.04198 | best_loss=0.04149
Epoch 24/80: current_loss=0.04175 | best_loss=0.04149
Epoch 25/80: current_loss=0.04165 | best_loss=0.04149
Epoch 26/80: current_loss=0.04194 | best_loss=0.04149
Epoch 27/80: current_loss=0.04157 | best_loss=0.04149
Epoch 28/80: current_loss=0.04162 | best_loss=0.04149
Epoch 29/80: current_loss=0.04182 | best_loss=0.04149
Epoch 30/80: current_loss=0.04154 | best_loss=0.04149
Epoch 31/80: current_loss=0.04163 | best_loss=0.04149
Epoch 32/80: current_loss=0.04145 | best_loss=0.04145
Epoch 33/80: current_loss=0.04144 | best_loss=0.04144
Epoch 34/80: current_loss=0.04155 | best_loss=0.04144
Epoch 35/80: current_loss=0.04127 | best_loss=0.04127
Epoch 36/80: current_loss=0.04171 | best_loss=0.04127
Epoch 37/80: current_loss=0.04125 | best_loss=0.04125
Epoch 38/80: current_loss=0.04155 | best_loss=0.04125
Epoch 39/80: current_loss=0.04129 | best_loss=0.04125
Epoch 40/80: current_loss=0.04121 | best_loss=0.04121
Epoch 41/80: current_loss=0.04116 | best_loss=0.04116
Epoch 42/80: current_loss=0.04126 | best_loss=0.04116
Epoch 43/80: current_loss=0.04124 | best_loss=0.04116
Epoch 44/80: current_loss=0.04143 | best_loss=0.04116
Epoch 45/80: current_loss=0.04135 | best_loss=0.04116
Epoch 46/80: current_loss=0.04173 | best_loss=0.04116
Epoch 47/80: current_loss=0.04143 | best_loss=0.04116
Epoch 48/80: current_loss=0.04211 | best_loss=0.04116
Epoch 49/80: current_loss=0.04145 | best_loss=0.04116
Epoch 50/80: current_loss=0.04171 | best_loss=0.04116
Epoch 51/80: current_loss=0.04139 | best_loss=0.04116
Epoch 52/80: current_loss=0.04142 | best_loss=0.04116
Epoch 53/80: current_loss=0.04129 | best_loss=0.04116
Epoch 54/80: current_loss=0.04126 | best_loss=0.04116
Epoch 55/80: current_loss=0.04173 | best_loss=0.04116
Epoch 56/80: current_loss=0.04119 | best_loss=0.04116
Epoch 57/80: current_loss=0.04135 | best_loss=0.04116
Epoch 58/80: current_loss=0.04135 | best_loss=0.04116
Epoch 59/80: current_loss=0.04149 | best_loss=0.04116
Epoch 60/80: current_loss=0.04129 | best_loss=0.04116
Epoch 61/80: current_loss=0.04134 | best_loss=0.04116
Early Stopping at epoch 61
      explained_var=0.03912 | mse_loss=0.03967
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02872 | best_loss=0.02872
Epoch 1/80: current_loss=0.02819 | best_loss=0.02819
Epoch 2/80: current_loss=0.02826 | best_loss=0.02819
Epoch 3/80: current_loss=0.02850 | best_loss=0.02819
Epoch 4/80: current_loss=0.02803 | best_loss=0.02803
Epoch 5/80: current_loss=0.02837 | best_loss=0.02803
Epoch 6/80: current_loss=0.02820 | best_loss=0.02803
Epoch 7/80: current_loss=0.02847 | best_loss=0.02803
Epoch 8/80: current_loss=0.02820 | best_loss=0.02803
Epoch 9/80: current_loss=0.02856 | best_loss=0.02803
Epoch 10/80: current_loss=0.02833 | best_loss=0.02803
Epoch 11/80: current_loss=0.02816 | best_loss=0.02803
Epoch 12/80: current_loss=0.02842 | best_loss=0.02803
Epoch 13/80: current_loss=0.02858 | best_loss=0.02803
Epoch 14/80: current_loss=0.02819 | best_loss=0.02803
Epoch 15/80: current_loss=0.02875 | best_loss=0.02803
Epoch 16/80: current_loss=0.02831 | best_loss=0.02803
Epoch 17/80: current_loss=0.02837 | best_loss=0.02803
Epoch 18/80: current_loss=0.02845 | best_loss=0.02803
Epoch 19/80: current_loss=0.02829 | best_loss=0.02803
Epoch 20/80: current_loss=0.02902 | best_loss=0.02803
Epoch 21/80: current_loss=0.02817 | best_loss=0.02803
Epoch 22/80: current_loss=0.02888 | best_loss=0.02803
Epoch 23/80: current_loss=0.02834 | best_loss=0.02803
Epoch 24/80: current_loss=0.02839 | best_loss=0.02803
Early Stopping at epoch 24
      explained_var=-0.00316 | mse_loss=0.02846

----------------------------------------------
Params for Trial 73
{'learning_rate': 0.0001, 'weight_decay': 0.0014988436864704496, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04227 | best_loss=0.04227
Epoch 1/80: current_loss=0.04120 | best_loss=0.04120
Epoch 2/80: current_loss=0.03874 | best_loss=0.03874
Epoch 3/80: current_loss=0.03929 | best_loss=0.03874
Epoch 4/80: current_loss=0.03828 | best_loss=0.03828
Epoch 5/80: current_loss=0.03813 | best_loss=0.03813
Epoch 6/80: current_loss=0.03828 | best_loss=0.03813
Epoch 7/80: current_loss=0.03823 | best_loss=0.03813
Epoch 8/80: current_loss=0.03848 | best_loss=0.03813
Epoch 9/80: current_loss=0.03808 | best_loss=0.03808
Epoch 10/80: current_loss=0.03782 | best_loss=0.03782
Epoch 11/80: current_loss=0.03773 | best_loss=0.03773
Epoch 12/80: current_loss=0.03773 | best_loss=0.03773
Epoch 13/80: current_loss=0.03780 | best_loss=0.03773
Epoch 14/80: current_loss=0.03801 | best_loss=0.03773
Epoch 15/80: current_loss=0.03766 | best_loss=0.03766
Epoch 16/80: current_loss=0.03780 | best_loss=0.03766
Epoch 17/80: current_loss=0.03768 | best_loss=0.03766
Epoch 18/80: current_loss=0.03765 | best_loss=0.03765
Epoch 19/80: current_loss=0.03772 | best_loss=0.03765
Epoch 20/80: current_loss=0.03797 | best_loss=0.03765
Epoch 21/80: current_loss=0.03770 | best_loss=0.03765
Epoch 22/80: current_loss=0.03794 | best_loss=0.03765
Epoch 23/80: current_loss=0.03768 | best_loss=0.03765
Epoch 24/80: current_loss=0.03784 | best_loss=0.03765
Epoch 25/80: current_loss=0.03759 | best_loss=0.03759
Epoch 26/80: current_loss=0.03759 | best_loss=0.03759
Epoch 27/80: current_loss=0.03762 | best_loss=0.03759
Epoch 28/80: current_loss=0.03759 | best_loss=0.03759
Epoch 29/80: current_loss=0.03757 | best_loss=0.03757
Epoch 30/80: current_loss=0.03763 | best_loss=0.03757
Epoch 31/80: current_loss=0.03763 | best_loss=0.03757
Epoch 32/80: current_loss=0.03765 | best_loss=0.03757
Epoch 33/80: current_loss=0.03759 | best_loss=0.03757
Epoch 34/80: current_loss=0.03765 | best_loss=0.03757
Epoch 35/80: current_loss=0.03774 | best_loss=0.03757
Epoch 36/80: current_loss=0.03756 | best_loss=0.03756
Epoch 37/80: current_loss=0.03767 | best_loss=0.03756
Epoch 38/80: current_loss=0.03765 | best_loss=0.03756
Epoch 39/80: current_loss=0.03772 | best_loss=0.03756
Epoch 40/80: current_loss=0.03761 | best_loss=0.03756
Epoch 41/80: current_loss=0.03796 | best_loss=0.03756
Epoch 42/80: current_loss=0.03757 | best_loss=0.03756
Epoch 43/80: current_loss=0.03752 | best_loss=0.03752
Epoch 44/80: current_loss=0.03747 | best_loss=0.03747
Epoch 45/80: current_loss=0.03760 | best_loss=0.03747
Epoch 46/80: current_loss=0.03760 | best_loss=0.03747
Epoch 47/80: current_loss=0.03750 | best_loss=0.03747
Epoch 48/80: current_loss=0.03789 | best_loss=0.03747
Epoch 49/80: current_loss=0.03752 | best_loss=0.03747
Epoch 50/80: current_loss=0.03752 | best_loss=0.03747
Epoch 51/80: current_loss=0.03752 | best_loss=0.03747
Epoch 52/80: current_loss=0.03775 | best_loss=0.03747
Epoch 53/80: current_loss=0.03757 | best_loss=0.03747
Epoch 54/80: current_loss=0.03756 | best_loss=0.03747
Epoch 55/80: current_loss=0.03759 | best_loss=0.03747
Epoch 56/80: current_loss=0.03772 | best_loss=0.03747
Epoch 57/80: current_loss=0.03757 | best_loss=0.03747
Epoch 58/80: current_loss=0.03755 | best_loss=0.03747
Epoch 59/80: current_loss=0.03767 | best_loss=0.03747
Epoch 60/80: current_loss=0.03767 | best_loss=0.03747
Epoch 61/80: current_loss=0.03758 | best_loss=0.03747
Epoch 62/80: current_loss=0.03756 | best_loss=0.03747
Epoch 63/80: current_loss=0.03798 | best_loss=0.03747
Epoch 64/80: current_loss=0.03760 | best_loss=0.03747
Early Stopping at epoch 64
      explained_var=0.02081 | mse_loss=0.03841
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04132 | best_loss=0.04132
Epoch 1/80: current_loss=0.04126 | best_loss=0.04126
Epoch 2/80: current_loss=0.04126 | best_loss=0.04126
Epoch 3/80: current_loss=0.04170 | best_loss=0.04126
Epoch 4/80: current_loss=0.04132 | best_loss=0.04126
Epoch 5/80: current_loss=0.04146 | best_loss=0.04126
Epoch 6/80: current_loss=0.04133 | best_loss=0.04126
Epoch 7/80: current_loss=0.04149 | best_loss=0.04126
Epoch 8/80: current_loss=0.04146 | best_loss=0.04126
Epoch 9/80: current_loss=0.04154 | best_loss=0.04126
Epoch 10/80: current_loss=0.04131 | best_loss=0.04126
Epoch 11/80: current_loss=0.04197 | best_loss=0.04126
Epoch 12/80: current_loss=0.04137 | best_loss=0.04126
Epoch 13/80: current_loss=0.04163 | best_loss=0.04126
Epoch 14/80: current_loss=0.04142 | best_loss=0.04126
Epoch 15/80: current_loss=0.04142 | best_loss=0.04126
Epoch 16/80: current_loss=0.04140 | best_loss=0.04126
Epoch 17/80: current_loss=0.04182 | best_loss=0.04126
Epoch 18/80: current_loss=0.04147 | best_loss=0.04126
Epoch 19/80: current_loss=0.04153 | best_loss=0.04126
Epoch 20/80: current_loss=0.04160 | best_loss=0.04126
Epoch 21/80: current_loss=0.04164 | best_loss=0.04126
Early Stopping at epoch 21
      explained_var=0.03754 | mse_loss=0.03974
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02816 | best_loss=0.02816
Epoch 1/80: current_loss=0.02846 | best_loss=0.02816
Epoch 2/80: current_loss=0.02815 | best_loss=0.02815
Epoch 3/80: current_loss=0.02825 | best_loss=0.02815
Epoch 4/80: current_loss=0.02826 | best_loss=0.02815
Epoch 5/80: current_loss=0.02822 | best_loss=0.02815
Epoch 6/80: current_loss=0.02842 | best_loss=0.02815
Epoch 7/80: current_loss=0.02822 | best_loss=0.02815
Epoch 8/80: current_loss=0.02838 | best_loss=0.02815
Epoch 9/80: current_loss=0.02844 | best_loss=0.02815
Epoch 10/80: current_loss=0.02868 | best_loss=0.02815
Epoch 11/80: current_loss=0.02839 | best_loss=0.02815
Epoch 12/80: current_loss=0.02824 | best_loss=0.02815
Epoch 13/80: current_loss=0.02842 | best_loss=0.02815
Epoch 14/80: current_loss=0.02822 | best_loss=0.02815
Epoch 15/80: current_loss=0.02871 | best_loss=0.02815
Epoch 16/80: current_loss=0.02807 | best_loss=0.02807
Epoch 17/80: current_loss=0.02838 | best_loss=0.02807
Epoch 18/80: current_loss=0.02816 | best_loss=0.02807
Epoch 19/80: current_loss=0.02852 | best_loss=0.02807
Epoch 20/80: current_loss=0.02820 | best_loss=0.02807
Epoch 21/80: current_loss=0.02884 | best_loss=0.02807
Epoch 22/80: current_loss=0.02824 | best_loss=0.02807
Epoch 23/80: current_loss=0.02807 | best_loss=0.02807
Epoch 24/80: current_loss=0.02853 | best_loss=0.02807
Epoch 25/80: current_loss=0.02807 | best_loss=0.02807
Epoch 26/80: current_loss=0.02872 | best_loss=0.02807
Epoch 27/80: current_loss=0.02812 | best_loss=0.02807
Epoch 28/80: current_loss=0.02811 | best_loss=0.02807
Epoch 29/80: current_loss=0.02862 | best_loss=0.02807
Epoch 30/80: current_loss=0.02813 | best_loss=0.02807
Epoch 31/80: current_loss=0.02890 | best_loss=0.02807
Epoch 32/80: current_loss=0.02803 | best_loss=0.02803
Epoch 33/80: current_loss=0.02899 | best_loss=0.02803
Epoch 34/80: current_loss=0.02812 | best_loss=0.02803
Epoch 35/80: current_loss=0.02845 | best_loss=0.02803
Epoch 36/80: current_loss=0.02837 | best_loss=0.02803
Epoch 37/80: current_loss=0.02815 | best_loss=0.02803
Epoch 38/80: current_loss=0.02831 | best_loss=0.02803
Epoch 39/80: current_loss=0.02832 | best_loss=0.02803
Epoch 40/80: current_loss=0.02827 | best_loss=0.02803
Epoch 41/80: current_loss=0.02838 | best_loss=0.02803
Epoch 42/80: current_loss=0.02823 | best_loss=0.02803
Epoch 43/80: current_loss=0.02864 | best_loss=0.02803
Epoch 44/80: current_loss=0.02811 | best_loss=0.02803
Epoch 45/80: current_loss=0.02839 | best_loss=0.02803
Epoch 46/80: current_loss=0.02830 | best_loss=0.02803
Epoch 47/80: current_loss=0.02842 | best_loss=0.02803
Epoch 48/80: current_loss=0.02834 | best_loss=0.02803
Epoch 49/80: current_loss=0.02830 | best_loss=0.02803
Epoch 50/80: current_loss=0.02864 | best_loss=0.02803
Epoch 51/80: current_loss=0.02841 | best_loss=0.02803
Epoch 52/80: current_loss=0.02860 | best_loss=0.02803
Early Stopping at epoch 52
      explained_var=-0.00401 | mse_loss=0.02847

----------------------------------------------
Params for Trial 74
{'learning_rate': 0.0001, 'weight_decay': 0.004779857367567423, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.15041 | best_loss=0.15041
Epoch 1/80: current_loss=0.05526 | best_loss=0.05526
Epoch 2/80: current_loss=0.04701 | best_loss=0.04701
Epoch 3/80: current_loss=0.04663 | best_loss=0.04663
Epoch 4/80: current_loss=0.04557 | best_loss=0.04557
Epoch 5/80: current_loss=0.04488 | best_loss=0.04488
Epoch 6/80: current_loss=0.04453 | best_loss=0.04453
Epoch 7/80: current_loss=0.04396 | best_loss=0.04396
Epoch 8/80: current_loss=0.04340 | best_loss=0.04340
Epoch 9/80: current_loss=0.04271 | best_loss=0.04271
Epoch 10/80: current_loss=0.04247 | best_loss=0.04247
Epoch 11/80: current_loss=0.04211 | best_loss=0.04211
Epoch 12/80: current_loss=0.04181 | best_loss=0.04181
Epoch 13/80: current_loss=0.04155 | best_loss=0.04155
Epoch 14/80: current_loss=0.04131 | best_loss=0.04131
Epoch 15/80: current_loss=0.04114 | best_loss=0.04114
Epoch 16/80: current_loss=0.04105 | best_loss=0.04105
Epoch 17/80: current_loss=0.04061 | best_loss=0.04061
Epoch 18/80: current_loss=0.04074 | best_loss=0.04061
Epoch 19/80: current_loss=0.04039 | best_loss=0.04039
Epoch 20/80: current_loss=0.04004 | best_loss=0.04004
Epoch 21/80: current_loss=0.03999 | best_loss=0.03999
Epoch 22/80: current_loss=0.04002 | best_loss=0.03999
Epoch 23/80: current_loss=0.04001 | best_loss=0.03999
Epoch 24/80: current_loss=0.03964 | best_loss=0.03964
Epoch 25/80: current_loss=0.03943 | best_loss=0.03943
Epoch 26/80: current_loss=0.03984 | best_loss=0.03943
Epoch 27/80: current_loss=0.03938 | best_loss=0.03938
Epoch 28/80: current_loss=0.03950 | best_loss=0.03938
Epoch 29/80: current_loss=0.03925 | best_loss=0.03925
Epoch 30/80: current_loss=0.03935 | best_loss=0.03925
Epoch 31/80: current_loss=0.03903 | best_loss=0.03903
Epoch 32/80: current_loss=0.03899 | best_loss=0.03899
Epoch 33/80: current_loss=0.03936 | best_loss=0.03899
Epoch 34/80: current_loss=0.03891 | best_loss=0.03891
Epoch 35/80: current_loss=0.03895 | best_loss=0.03891
Epoch 36/80: current_loss=0.03878 | best_loss=0.03878
Epoch 37/80: current_loss=0.03862 | best_loss=0.03862
Epoch 38/80: current_loss=0.03868 | best_loss=0.03862
Epoch 39/80: current_loss=0.03884 | best_loss=0.03862
Epoch 40/80: current_loss=0.03849 | best_loss=0.03849
Epoch 41/80: current_loss=0.03840 | best_loss=0.03840
Epoch 42/80: current_loss=0.03837 | best_loss=0.03837
Epoch 43/80: current_loss=0.03851 | best_loss=0.03837
Epoch 44/80: current_loss=0.03841 | best_loss=0.03837
Epoch 45/80: current_loss=0.03852 | best_loss=0.03837
Epoch 46/80: current_loss=0.03840 | best_loss=0.03837
Epoch 47/80: current_loss=0.03829 | best_loss=0.03829
Epoch 48/80: current_loss=0.03833 | best_loss=0.03829
Epoch 49/80: current_loss=0.03834 | best_loss=0.03829
Epoch 50/80: current_loss=0.03820 | best_loss=0.03820
Epoch 51/80: current_loss=0.03851 | best_loss=0.03820
Epoch 52/80: current_loss=0.03820 | best_loss=0.03820
Epoch 53/80: current_loss=0.03840 | best_loss=0.03820
Epoch 54/80: current_loss=0.03804 | best_loss=0.03804
Epoch 55/80: current_loss=0.03817 | best_loss=0.03804
Epoch 56/80: current_loss=0.03820 | best_loss=0.03804
Epoch 57/80: current_loss=0.03821 | best_loss=0.03804
Epoch 58/80: current_loss=0.03826 | best_loss=0.03804
Epoch 59/80: current_loss=0.03805 | best_loss=0.03804
Epoch 60/80: current_loss=0.03816 | best_loss=0.03804
Epoch 61/80: current_loss=0.03827 | best_loss=0.03804
Epoch 62/80: current_loss=0.03817 | best_loss=0.03804
Epoch 63/80: current_loss=0.03806 | best_loss=0.03804
Epoch 64/80: current_loss=0.03808 | best_loss=0.03804
Epoch 65/80: current_loss=0.03833 | best_loss=0.03804
Epoch 66/80: current_loss=0.03803 | best_loss=0.03803
Epoch 67/80: current_loss=0.03812 | best_loss=0.03803
Epoch 68/80: current_loss=0.03815 | best_loss=0.03803
Epoch 69/80: current_loss=0.03814 | best_loss=0.03803
Epoch 70/80: current_loss=0.03839 | best_loss=0.03803
Epoch 71/80: current_loss=0.03805 | best_loss=0.03803
Epoch 72/80: current_loss=0.03814 | best_loss=0.03803
Epoch 73/80: current_loss=0.03808 | best_loss=0.03803
Epoch 74/80: current_loss=0.03833 | best_loss=0.03803
Epoch 75/80: current_loss=0.03813 | best_loss=0.03803
Epoch 76/80: current_loss=0.03818 | best_loss=0.03803
Epoch 77/80: current_loss=0.03823 | best_loss=0.03803
Epoch 78/80: current_loss=0.03818 | best_loss=0.03803
Epoch 79/80: current_loss=0.03801 | best_loss=0.03801
      explained_var=0.00474 | mse_loss=0.03898

----------------------------------------------
Params for Trial 75
{'learning_rate': 0.001, 'weight_decay': 0.00025984883600589594, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03772 | best_loss=0.03772
Epoch 1/80: current_loss=0.03928 | best_loss=0.03772
Epoch 2/80: current_loss=0.03767 | best_loss=0.03767
Epoch 3/80: current_loss=0.04026 | best_loss=0.03767
Epoch 4/80: current_loss=0.03938 | best_loss=0.03767
Epoch 5/80: current_loss=0.04342 | best_loss=0.03767
Epoch 6/80: current_loss=0.03754 | best_loss=0.03754
Epoch 7/80: current_loss=0.03824 | best_loss=0.03754
Epoch 8/80: current_loss=0.03723 | best_loss=0.03723
Epoch 9/80: current_loss=0.03722 | best_loss=0.03722
Epoch 10/80: current_loss=0.03767 | best_loss=0.03722
Epoch 11/80: current_loss=0.03802 | best_loss=0.03722
Epoch 12/80: current_loss=0.03850 | best_loss=0.03722
Epoch 13/80: current_loss=0.04238 | best_loss=0.03722
Epoch 14/80: current_loss=0.03793 | best_loss=0.03722
Epoch 15/80: current_loss=0.03777 | best_loss=0.03722
Epoch 16/80: current_loss=0.03779 | best_loss=0.03722
Epoch 17/80: current_loss=0.03819 | best_loss=0.03722
Epoch 18/80: current_loss=0.03844 | best_loss=0.03722
Epoch 19/80: current_loss=0.03799 | best_loss=0.03722
Epoch 20/80: current_loss=0.03785 | best_loss=0.03722
Epoch 21/80: current_loss=0.03836 | best_loss=0.03722
Epoch 22/80: current_loss=0.03765 | best_loss=0.03722
Epoch 23/80: current_loss=0.03785 | best_loss=0.03722
Epoch 24/80: current_loss=0.03811 | best_loss=0.03722
Epoch 25/80: current_loss=0.03799 | best_loss=0.03722
Epoch 26/80: current_loss=0.03880 | best_loss=0.03722
Epoch 27/80: current_loss=0.03761 | best_loss=0.03722
Epoch 28/80: current_loss=0.03937 | best_loss=0.03722
Epoch 29/80: current_loss=0.03845 | best_loss=0.03722
Early Stopping at epoch 29
      explained_var=0.02744 | mse_loss=0.03811
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04250 | best_loss=0.04250
Epoch 1/80: current_loss=0.04135 | best_loss=0.04135
Epoch 2/80: current_loss=0.04233 | best_loss=0.04135
Epoch 3/80: current_loss=0.04184 | best_loss=0.04135
Epoch 4/80: current_loss=0.04307 | best_loss=0.04135
Epoch 5/80: current_loss=0.04148 | best_loss=0.04135
Epoch 6/80: current_loss=0.04222 | best_loss=0.04135
Epoch 7/80: current_loss=0.04148 | best_loss=0.04135
Epoch 8/80: current_loss=0.04140 | best_loss=0.04135
Epoch 9/80: current_loss=0.04178 | best_loss=0.04135
Epoch 10/80: current_loss=0.04191 | best_loss=0.04135
Epoch 11/80: current_loss=0.04183 | best_loss=0.04135
Epoch 12/80: current_loss=0.04286 | best_loss=0.04135
Epoch 13/80: current_loss=0.04177 | best_loss=0.04135
Epoch 14/80: current_loss=0.04240 | best_loss=0.04135
Epoch 15/80: current_loss=0.04185 | best_loss=0.04135
Epoch 16/80: current_loss=0.04162 | best_loss=0.04135
Epoch 17/80: current_loss=0.04165 | best_loss=0.04135
Epoch 18/80: current_loss=0.04433 | best_loss=0.04135
Epoch 19/80: current_loss=0.04339 | best_loss=0.04135
Epoch 20/80: current_loss=0.04297 | best_loss=0.04135
Epoch 21/80: current_loss=0.04163 | best_loss=0.04135
Early Stopping at epoch 21
      explained_var=0.03311 | mse_loss=0.03991
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02807 | best_loss=0.02807
Epoch 1/80: current_loss=0.03194 | best_loss=0.02807
Epoch 2/80: current_loss=0.02955 | best_loss=0.02807
Epoch 3/80: current_loss=0.02853 | best_loss=0.02807
Epoch 4/80: current_loss=0.02814 | best_loss=0.02807
Epoch 5/80: current_loss=0.02839 | best_loss=0.02807
Epoch 6/80: current_loss=0.02815 | best_loss=0.02807
Epoch 7/80: current_loss=0.02833 | best_loss=0.02807
Epoch 8/80: current_loss=0.02826 | best_loss=0.02807
Epoch 9/80: current_loss=0.02822 | best_loss=0.02807
Epoch 10/80: current_loss=0.02838 | best_loss=0.02807
Epoch 11/80: current_loss=0.02815 | best_loss=0.02807
Epoch 12/80: current_loss=0.02861 | best_loss=0.02807
Epoch 13/80: current_loss=0.02966 | best_loss=0.02807
Epoch 14/80: current_loss=0.02951 | best_loss=0.02807
Epoch 15/80: current_loss=0.02838 | best_loss=0.02807
Epoch 16/80: current_loss=0.02849 | best_loss=0.02807
Epoch 17/80: current_loss=0.02815 | best_loss=0.02807
Epoch 18/80: current_loss=0.02926 | best_loss=0.02807
Epoch 19/80: current_loss=0.02838 | best_loss=0.02807
Epoch 20/80: current_loss=0.02838 | best_loss=0.02807
Early Stopping at epoch 20
      explained_var=-0.00754 | mse_loss=0.02859

----------------------------------------------
Params for Trial 76
{'learning_rate': 0.0001, 'weight_decay': 0.0008043332125757753, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04692 | best_loss=0.04692
Epoch 1/80: current_loss=0.04320 | best_loss=0.04320
Epoch 2/80: current_loss=0.04201 | best_loss=0.04201
Epoch 3/80: current_loss=0.04130 | best_loss=0.04130
Epoch 4/80: current_loss=0.04090 | best_loss=0.04090
Epoch 5/80: current_loss=0.04034 | best_loss=0.04034
Epoch 6/80: current_loss=0.03989 | best_loss=0.03989
Epoch 7/80: current_loss=0.03959 | best_loss=0.03959
Epoch 8/80: current_loss=0.03928 | best_loss=0.03928
Epoch 9/80: current_loss=0.03903 | best_loss=0.03903
Epoch 10/80: current_loss=0.03893 | best_loss=0.03893
Epoch 11/80: current_loss=0.03863 | best_loss=0.03863
Epoch 12/80: current_loss=0.03844 | best_loss=0.03844
Epoch 13/80: current_loss=0.03853 | best_loss=0.03844
Epoch 14/80: current_loss=0.03833 | best_loss=0.03833
Epoch 15/80: current_loss=0.03818 | best_loss=0.03818
Epoch 16/80: current_loss=0.03805 | best_loss=0.03805
Epoch 17/80: current_loss=0.03799 | best_loss=0.03799
Epoch 18/80: current_loss=0.03794 | best_loss=0.03794
Epoch 19/80: current_loss=0.03791 | best_loss=0.03791
Epoch 20/80: current_loss=0.03820 | best_loss=0.03791
Epoch 21/80: current_loss=0.03790 | best_loss=0.03790
Epoch 22/80: current_loss=0.03775 | best_loss=0.03775
Epoch 23/80: current_loss=0.03772 | best_loss=0.03772
Epoch 24/80: current_loss=0.03777 | best_loss=0.03772
Epoch 25/80: current_loss=0.03778 | best_loss=0.03772
Epoch 26/80: current_loss=0.03768 | best_loss=0.03768
Epoch 27/80: current_loss=0.03767 | best_loss=0.03767
Epoch 28/80: current_loss=0.03792 | best_loss=0.03767
Epoch 29/80: current_loss=0.03774 | best_loss=0.03767
Epoch 30/80: current_loss=0.03775 | best_loss=0.03767
Epoch 31/80: current_loss=0.03771 | best_loss=0.03767
Epoch 32/80: current_loss=0.03757 | best_loss=0.03757
Epoch 33/80: current_loss=0.03750 | best_loss=0.03750
Epoch 34/80: current_loss=0.03742 | best_loss=0.03742
Epoch 35/80: current_loss=0.03745 | best_loss=0.03742
Epoch 36/80: current_loss=0.03757 | best_loss=0.03742
Epoch 37/80: current_loss=0.03750 | best_loss=0.03742
Epoch 38/80: current_loss=0.03753 | best_loss=0.03742
Epoch 39/80: current_loss=0.03750 | best_loss=0.03742
Epoch 40/80: current_loss=0.03752 | best_loss=0.03742
Epoch 41/80: current_loss=0.03759 | best_loss=0.03742
Epoch 42/80: current_loss=0.03764 | best_loss=0.03742
Epoch 43/80: current_loss=0.03760 | best_loss=0.03742
Epoch 44/80: current_loss=0.03757 | best_loss=0.03742
Epoch 45/80: current_loss=0.03754 | best_loss=0.03742
Epoch 46/80: current_loss=0.03755 | best_loss=0.03742
Epoch 47/80: current_loss=0.03770 | best_loss=0.03742
Epoch 48/80: current_loss=0.03754 | best_loss=0.03742
Epoch 49/80: current_loss=0.03758 | best_loss=0.03742
Epoch 50/80: current_loss=0.03759 | best_loss=0.03742
Epoch 51/80: current_loss=0.03756 | best_loss=0.03742
Epoch 52/80: current_loss=0.03748 | best_loss=0.03742
Epoch 53/80: current_loss=0.03743 | best_loss=0.03742
Epoch 54/80: current_loss=0.03757 | best_loss=0.03742
Early Stopping at epoch 54
      explained_var=0.02131 | mse_loss=0.03839
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04120 | best_loss=0.04120
Epoch 1/80: current_loss=0.04158 | best_loss=0.04120
Epoch 2/80: current_loss=0.04156 | best_loss=0.04120
Epoch 3/80: current_loss=0.04153 | best_loss=0.04120
Epoch 4/80: current_loss=0.04141 | best_loss=0.04120
Epoch 5/80: current_loss=0.04174 | best_loss=0.04120
Epoch 6/80: current_loss=0.04137 | best_loss=0.04120
Epoch 7/80: current_loss=0.04165 | best_loss=0.04120
Epoch 8/80: current_loss=0.04149 | best_loss=0.04120
Epoch 9/80: current_loss=0.04139 | best_loss=0.04120
Epoch 10/80: current_loss=0.04181 | best_loss=0.04120
Epoch 11/80: current_loss=0.04146 | best_loss=0.04120
Epoch 12/80: current_loss=0.04164 | best_loss=0.04120
Epoch 13/80: current_loss=0.04152 | best_loss=0.04120
Epoch 14/80: current_loss=0.04142 | best_loss=0.04120
Epoch 15/80: current_loss=0.04145 | best_loss=0.04120
Epoch 16/80: current_loss=0.04148 | best_loss=0.04120
Epoch 17/80: current_loss=0.04148 | best_loss=0.04120
Epoch 18/80: current_loss=0.04128 | best_loss=0.04120
Epoch 19/80: current_loss=0.04141 | best_loss=0.04120
Epoch 20/80: current_loss=0.04133 | best_loss=0.04120
Early Stopping at epoch 20
      explained_var=0.03652 | mse_loss=0.03974
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02824 | best_loss=0.02824
Epoch 1/80: current_loss=0.02840 | best_loss=0.02824
Epoch 2/80: current_loss=0.02833 | best_loss=0.02824
Epoch 3/80: current_loss=0.02830 | best_loss=0.02824
Epoch 4/80: current_loss=0.02872 | best_loss=0.02824
Epoch 5/80: current_loss=0.02835 | best_loss=0.02824
Epoch 6/80: current_loss=0.02847 | best_loss=0.02824
Epoch 7/80: current_loss=0.02831 | best_loss=0.02824
Epoch 8/80: current_loss=0.02830 | best_loss=0.02824
Epoch 9/80: current_loss=0.02860 | best_loss=0.02824
Epoch 10/80: current_loss=0.02837 | best_loss=0.02824
Epoch 11/80: current_loss=0.02831 | best_loss=0.02824
Epoch 12/80: current_loss=0.02833 | best_loss=0.02824
Epoch 13/80: current_loss=0.02831 | best_loss=0.02824
Epoch 14/80: current_loss=0.02830 | best_loss=0.02824
Epoch 15/80: current_loss=0.02846 | best_loss=0.02824
Epoch 16/80: current_loss=0.02837 | best_loss=0.02824
Epoch 17/80: current_loss=0.02853 | best_loss=0.02824
Epoch 18/80: current_loss=0.02845 | best_loss=0.02824
Epoch 19/80: current_loss=0.02828 | best_loss=0.02824
Epoch 20/80: current_loss=0.02841 | best_loss=0.02824
Early Stopping at epoch 20
      explained_var=-0.00613 | mse_loss=0.02863

----------------------------------------------
Params for Trial 77
{'learning_rate': 1e-05, 'weight_decay': 0.006044019709756881, 'n_layers': 1, 'hidden_size': 512, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.18348 | best_loss=0.18348
Epoch 1/80: current_loss=0.13434 | best_loss=0.13434
Epoch 2/80: current_loss=0.09332 | best_loss=0.09332
Epoch 3/80: current_loss=0.06364 | best_loss=0.06364
Epoch 4/80: current_loss=0.05127 | best_loss=0.05127
Epoch 5/80: current_loss=0.05026 | best_loss=0.05026
Epoch 6/80: current_loss=0.05009 | best_loss=0.05009
Epoch 7/80: current_loss=0.04974 | best_loss=0.04974
Epoch 8/80: current_loss=0.04936 | best_loss=0.04936
Epoch 9/80: current_loss=0.04900 | best_loss=0.04900
Epoch 10/80: current_loss=0.04886 | best_loss=0.04886
Epoch 11/80: current_loss=0.04851 | best_loss=0.04851
Epoch 12/80: current_loss=0.04819 | best_loss=0.04819
Epoch 13/80: current_loss=0.04785 | best_loss=0.04785
Epoch 14/80: current_loss=0.04757 | best_loss=0.04757
Epoch 15/80: current_loss=0.04734 | best_loss=0.04734
Epoch 16/80: current_loss=0.04702 | best_loss=0.04702
Epoch 17/80: current_loss=0.04682 | best_loss=0.04682
Epoch 18/80: current_loss=0.04659 | best_loss=0.04659
Epoch 19/80: current_loss=0.04638 | best_loss=0.04638
Epoch 20/80: current_loss=0.04621 | best_loss=0.04621
Epoch 21/80: current_loss=0.04599 | best_loss=0.04599
Epoch 22/80: current_loss=0.04577 | best_loss=0.04577
Epoch 23/80: current_loss=0.04564 | best_loss=0.04564
Epoch 24/80: current_loss=0.04553 | best_loss=0.04553
Epoch 25/80: current_loss=0.04520 | best_loss=0.04520
Epoch 26/80: current_loss=0.04514 | best_loss=0.04514
Epoch 27/80: current_loss=0.04497 | best_loss=0.04497
Epoch 28/80: current_loss=0.04466 | best_loss=0.04466
Epoch 29/80: current_loss=0.04455 | best_loss=0.04455
Epoch 30/80: current_loss=0.04437 | best_loss=0.04437
Epoch 31/80: current_loss=0.04428 | best_loss=0.04428
Epoch 32/80: current_loss=0.04405 | best_loss=0.04405
Epoch 33/80: current_loss=0.04389 | best_loss=0.04389
Epoch 34/80: current_loss=0.04378 | best_loss=0.04378
Epoch 35/80: current_loss=0.04367 | best_loss=0.04367
Epoch 36/80: current_loss=0.04347 | best_loss=0.04347
Epoch 37/80: current_loss=0.04343 | best_loss=0.04343
Epoch 38/80: current_loss=0.04329 | best_loss=0.04329
Epoch 39/80: current_loss=0.04313 | best_loss=0.04313
Epoch 40/80: current_loss=0.04305 | best_loss=0.04305
Epoch 41/80: current_loss=0.04302 | best_loss=0.04302
Epoch 42/80: current_loss=0.04292 | best_loss=0.04292
Epoch 43/80: current_loss=0.04277 | best_loss=0.04277
Epoch 44/80: current_loss=0.04264 | best_loss=0.04264
Epoch 45/80: current_loss=0.04257 | best_loss=0.04257
Epoch 46/80: current_loss=0.04250 | best_loss=0.04250
Epoch 47/80: current_loss=0.04236 | best_loss=0.04236
Epoch 48/80: current_loss=0.04227 | best_loss=0.04227
Epoch 49/80: current_loss=0.04227 | best_loss=0.04227
Epoch 50/80: current_loss=0.04231 | best_loss=0.04227
Epoch 51/80: current_loss=0.04209 | best_loss=0.04209
Epoch 52/80: current_loss=0.04208 | best_loss=0.04208
Epoch 53/80: current_loss=0.04203 | best_loss=0.04203
Epoch 54/80: current_loss=0.04194 | best_loss=0.04194
Epoch 55/80: current_loss=0.04174 | best_loss=0.04174
Epoch 56/80: current_loss=0.04163 | best_loss=0.04163
Epoch 57/80: current_loss=0.04168 | best_loss=0.04163
Epoch 58/80: current_loss=0.04161 | best_loss=0.04161
Epoch 59/80: current_loss=0.04144 | best_loss=0.04144
Epoch 60/80: current_loss=0.04140 | best_loss=0.04140
Epoch 61/80: current_loss=0.04132 | best_loss=0.04132
Epoch 62/80: current_loss=0.04131 | best_loss=0.04131
Epoch 63/80: current_loss=0.04121 | best_loss=0.04121
Epoch 64/80: current_loss=0.04110 | best_loss=0.04110
Epoch 65/80: current_loss=0.04110 | best_loss=0.04110
Epoch 66/80: current_loss=0.04095 | best_loss=0.04095
Epoch 67/80: current_loss=0.04096 | best_loss=0.04095
Epoch 68/80: current_loss=0.04086 | best_loss=0.04086
Epoch 69/80: current_loss=0.04088 | best_loss=0.04086
Epoch 70/80: current_loss=0.04082 | best_loss=0.04082
Epoch 71/80: current_loss=0.04081 | best_loss=0.04081
Epoch 72/80: current_loss=0.04073 | best_loss=0.04073
Epoch 73/80: current_loss=0.04071 | best_loss=0.04071
Epoch 74/80: current_loss=0.04067 | best_loss=0.04067
Epoch 75/80: current_loss=0.04063 | best_loss=0.04063
Epoch 76/80: current_loss=0.04050 | best_loss=0.04050
Epoch 77/80: current_loss=0.04048 | best_loss=0.04048
Epoch 78/80: current_loss=0.04040 | best_loss=0.04040
Epoch 79/80: current_loss=0.04036 | best_loss=0.04036
      explained_var=-0.04883 | mse_loss=0.04145

----------------------------------------------
Params for Trial 78
{'learning_rate': 0.001, 'weight_decay': 0.00117282122489052, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03875 | best_loss=0.03875
Epoch 1/80: current_loss=0.03940 | best_loss=0.03875
Epoch 2/80: current_loss=0.03855 | best_loss=0.03855
Epoch 3/80: current_loss=0.03792 | best_loss=0.03792
Epoch 4/80: current_loss=0.03830 | best_loss=0.03792
Epoch 5/80: current_loss=0.03836 | best_loss=0.03792
Epoch 6/80: current_loss=0.03831 | best_loss=0.03792
Epoch 7/80: current_loss=0.03857 | best_loss=0.03792
Epoch 8/80: current_loss=0.03775 | best_loss=0.03775
Epoch 9/80: current_loss=0.03815 | best_loss=0.03775
Epoch 10/80: current_loss=0.03784 | best_loss=0.03775
Epoch 11/80: current_loss=0.03766 | best_loss=0.03766
Epoch 12/80: current_loss=0.03912 | best_loss=0.03766
Epoch 13/80: current_loss=0.03779 | best_loss=0.03766
Epoch 14/80: current_loss=0.03757 | best_loss=0.03757
Epoch 15/80: current_loss=0.03749 | best_loss=0.03749
Epoch 16/80: current_loss=0.03811 | best_loss=0.03749
Epoch 17/80: current_loss=0.03765 | best_loss=0.03749
Epoch 18/80: current_loss=0.03804 | best_loss=0.03749
Epoch 19/80: current_loss=0.03755 | best_loss=0.03749
Epoch 20/80: current_loss=0.03756 | best_loss=0.03749
Epoch 21/80: current_loss=0.03807 | best_loss=0.03749
Epoch 22/80: current_loss=0.03796 | best_loss=0.03749
Epoch 23/80: current_loss=0.03761 | best_loss=0.03749
Epoch 24/80: current_loss=0.03786 | best_loss=0.03749
Epoch 25/80: current_loss=0.03799 | best_loss=0.03749
Epoch 26/80: current_loss=0.03756 | best_loss=0.03749
Epoch 27/80: current_loss=0.03763 | best_loss=0.03749
Epoch 28/80: current_loss=0.03757 | best_loss=0.03749
Epoch 29/80: current_loss=0.03813 | best_loss=0.03749
Epoch 30/80: current_loss=0.03764 | best_loss=0.03749
Epoch 31/80: current_loss=0.03820 | best_loss=0.03749
Epoch 32/80: current_loss=0.03788 | best_loss=0.03749
Epoch 33/80: current_loss=0.03811 | best_loss=0.03749
Epoch 34/80: current_loss=0.03765 | best_loss=0.03749
Epoch 35/80: current_loss=0.03770 | best_loss=0.03749
Early Stopping at epoch 35
      explained_var=0.01936 | mse_loss=0.03841
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04274 | best_loss=0.04274
Epoch 1/80: current_loss=0.04246 | best_loss=0.04246
Epoch 2/80: current_loss=0.04157 | best_loss=0.04157
Epoch 3/80: current_loss=0.04167 | best_loss=0.04157
Epoch 4/80: current_loss=0.04167 | best_loss=0.04157
Epoch 5/80: current_loss=0.04174 | best_loss=0.04157
Epoch 6/80: current_loss=0.04178 | best_loss=0.04157
Epoch 7/80: current_loss=0.04178 | best_loss=0.04157
Epoch 8/80: current_loss=0.04183 | best_loss=0.04157
Epoch 9/80: current_loss=0.04189 | best_loss=0.04157
Epoch 10/80: current_loss=0.04191 | best_loss=0.04157
Epoch 11/80: current_loss=0.04185 | best_loss=0.04157
Epoch 12/80: current_loss=0.04173 | best_loss=0.04157
Epoch 13/80: current_loss=0.04193 | best_loss=0.04157
Epoch 14/80: current_loss=0.04203 | best_loss=0.04157
Epoch 15/80: current_loss=0.04170 | best_loss=0.04157
Epoch 16/80: current_loss=0.04196 | best_loss=0.04157
Epoch 17/80: current_loss=0.04186 | best_loss=0.04157
Epoch 18/80: current_loss=0.04174 | best_loss=0.04157
Epoch 19/80: current_loss=0.04189 | best_loss=0.04157
Epoch 20/80: current_loss=0.04194 | best_loss=0.04157
Epoch 21/80: current_loss=0.04215 | best_loss=0.04157
Epoch 22/80: current_loss=0.04187 | best_loss=0.04157
Early Stopping at epoch 22
      explained_var=0.02767 | mse_loss=0.04010
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02855 | best_loss=0.02855
Epoch 1/80: current_loss=0.02832 | best_loss=0.02832
Epoch 2/80: current_loss=0.02796 | best_loss=0.02796
Epoch 3/80: current_loss=0.02829 | best_loss=0.02796
Epoch 4/80: current_loss=0.02797 | best_loss=0.02796
Epoch 5/80: current_loss=0.02823 | best_loss=0.02796
Epoch 6/80: current_loss=0.02892 | best_loss=0.02796
Epoch 7/80: current_loss=0.02791 | best_loss=0.02791
Epoch 8/80: current_loss=0.02882 | best_loss=0.02791
Epoch 9/80: current_loss=0.02795 | best_loss=0.02791
Epoch 10/80: current_loss=0.02840 | best_loss=0.02791
Epoch 11/80: current_loss=0.02831 | best_loss=0.02791
Epoch 12/80: current_loss=0.02815 | best_loss=0.02791
Epoch 13/80: current_loss=0.02829 | best_loss=0.02791
Epoch 14/80: current_loss=0.02819 | best_loss=0.02791
Epoch 15/80: current_loss=0.02828 | best_loss=0.02791
Epoch 16/80: current_loss=0.02814 | best_loss=0.02791
Epoch 17/80: current_loss=0.02839 | best_loss=0.02791
Epoch 18/80: current_loss=0.02795 | best_loss=0.02791
Epoch 19/80: current_loss=0.02825 | best_loss=0.02791
Epoch 20/80: current_loss=0.02828 | best_loss=0.02791
Epoch 21/80: current_loss=0.02816 | best_loss=0.02791
Epoch 22/80: current_loss=0.02805 | best_loss=0.02791
Epoch 23/80: current_loss=0.02846 | best_loss=0.02791
Epoch 24/80: current_loss=0.02801 | best_loss=0.02791
Epoch 25/80: current_loss=0.02861 | best_loss=0.02791
Epoch 26/80: current_loss=0.02812 | best_loss=0.02791
Epoch 27/80: current_loss=0.02846 | best_loss=0.02791
Early Stopping at epoch 27
      explained_var=0.00053 | mse_loss=0.02834
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03291 | best_loss=0.03291
Epoch 1/80: current_loss=0.03296 | best_loss=0.03291
Epoch 2/80: current_loss=0.03290 | best_loss=0.03290
Epoch 3/80: current_loss=0.03293 | best_loss=0.03290
Epoch 4/80: current_loss=0.03291 | best_loss=0.03290
Epoch 5/80: current_loss=0.03294 | best_loss=0.03290
Epoch 6/80: current_loss=0.03284 | best_loss=0.03284
Epoch 7/80: current_loss=0.03298 | best_loss=0.03284
Epoch 8/80: current_loss=0.03291 | best_loss=0.03284
Epoch 9/80: current_loss=0.03297 | best_loss=0.03284
Epoch 10/80: current_loss=0.03307 | best_loss=0.03284
Epoch 11/80: current_loss=0.03298 | best_loss=0.03284
Epoch 12/80: current_loss=0.03300 | best_loss=0.03284
Epoch 13/80: current_loss=0.03304 | best_loss=0.03284
Epoch 14/80: current_loss=0.03299 | best_loss=0.03284
Epoch 15/80: current_loss=0.03290 | best_loss=0.03284
Epoch 16/80: current_loss=0.03321 | best_loss=0.03284
Epoch 17/80: current_loss=0.03290 | best_loss=0.03284
Epoch 18/80: current_loss=0.03292 | best_loss=0.03284
Epoch 19/80: current_loss=0.03297 | best_loss=0.03284
Epoch 20/80: current_loss=0.03292 | best_loss=0.03284
Epoch 21/80: current_loss=0.03296 | best_loss=0.03284
Epoch 22/80: current_loss=0.03289 | best_loss=0.03284
Epoch 23/80: current_loss=0.03289 | best_loss=0.03284
Epoch 24/80: current_loss=0.03294 | best_loss=0.03284
Epoch 25/80: current_loss=0.03288 | best_loss=0.03284
Epoch 26/80: current_loss=0.03306 | best_loss=0.03284
Early Stopping at epoch 26
      explained_var=-0.00309 | mse_loss=0.03270
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03534 | best_loss=0.03534
Epoch 1/80: current_loss=0.03538 | best_loss=0.03534
Epoch 2/80: current_loss=0.03537 | best_loss=0.03534
Epoch 3/80: current_loss=0.03550 | best_loss=0.03534
Epoch 4/80: current_loss=0.03549 | best_loss=0.03534
Epoch 5/80: current_loss=0.03549 | best_loss=0.03534
Epoch 6/80: current_loss=0.03552 | best_loss=0.03534
Epoch 7/80: current_loss=0.03549 | best_loss=0.03534
Epoch 8/80: current_loss=0.03552 | best_loss=0.03534
Epoch 9/80: current_loss=0.03547 | best_loss=0.03534
Epoch 10/80: current_loss=0.03553 | best_loss=0.03534
Epoch 11/80: current_loss=0.03559 | best_loss=0.03534
Epoch 12/80: current_loss=0.03558 | best_loss=0.03534
Epoch 13/80: current_loss=0.03548 | best_loss=0.03534
Epoch 14/80: current_loss=0.03553 | best_loss=0.03534
Epoch 15/80: current_loss=0.03558 | best_loss=0.03534
Epoch 16/80: current_loss=0.03556 | best_loss=0.03534
Epoch 17/80: current_loss=0.03562 | best_loss=0.03534
Epoch 18/80: current_loss=0.03555 | best_loss=0.03534
Epoch 19/80: current_loss=0.03555 | best_loss=0.03534
Epoch 20/80: current_loss=0.03553 | best_loss=0.03534
Early Stopping at epoch 20
      explained_var=0.02157 | mse_loss=0.03511
----------------------------------------------
Average early_stopping_point: 6| avg_exp_var=0.01321| avg_loss=0.03493
----------------------------------------------


----------------------------------------------
Params for Trial 79
{'learning_rate': 0.0001, 'weight_decay': 0.0005303685547839058, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.15218 | best_loss=0.15218
Epoch 1/80: current_loss=0.05537 | best_loss=0.05537
Epoch 2/80: current_loss=0.04807 | best_loss=0.04807
Epoch 3/80: current_loss=0.04668 | best_loss=0.04668
Epoch 4/80: current_loss=0.04601 | best_loss=0.04601
Epoch 5/80: current_loss=0.04497 | best_loss=0.04497
Epoch 6/80: current_loss=0.04440 | best_loss=0.04440
Epoch 7/80: current_loss=0.04353 | best_loss=0.04353
Epoch 8/80: current_loss=0.04309 | best_loss=0.04309
Epoch 9/80: current_loss=0.04272 | best_loss=0.04272
Epoch 10/80: current_loss=0.04197 | best_loss=0.04197
Epoch 11/80: current_loss=0.04139 | best_loss=0.04139
Epoch 12/80: current_loss=0.04117 | best_loss=0.04117
Epoch 13/80: current_loss=0.04072 | best_loss=0.04072
Epoch 14/80: current_loss=0.04044 | best_loss=0.04044
Epoch 15/80: current_loss=0.04015 | best_loss=0.04015
Epoch 16/80: current_loss=0.04000 | best_loss=0.04000
Epoch 17/80: current_loss=0.03959 | best_loss=0.03959
Epoch 18/80: current_loss=0.03958 | best_loss=0.03958
Epoch 19/80: current_loss=0.03940 | best_loss=0.03940
Epoch 20/80: current_loss=0.03914 | best_loss=0.03914
Epoch 21/80: current_loss=0.03901 | best_loss=0.03901
Epoch 22/80: current_loss=0.03897 | best_loss=0.03897
Epoch 23/80: current_loss=0.03884 | best_loss=0.03884
Epoch 24/80: current_loss=0.03863 | best_loss=0.03863
Epoch 25/80: current_loss=0.03863 | best_loss=0.03863
Epoch 26/80: current_loss=0.03847 | best_loss=0.03847
Epoch 27/80: current_loss=0.03832 | best_loss=0.03832
Epoch 28/80: current_loss=0.03823 | best_loss=0.03823
Epoch 29/80: current_loss=0.03822 | best_loss=0.03822
Epoch 30/80: current_loss=0.03826 | best_loss=0.03822
Epoch 31/80: current_loss=0.03812 | best_loss=0.03812
Epoch 32/80: current_loss=0.03816 | best_loss=0.03812
Epoch 33/80: current_loss=0.03826 | best_loss=0.03812
Epoch 34/80: current_loss=0.03804 | best_loss=0.03804
Epoch 35/80: current_loss=0.03825 | best_loss=0.03804
Epoch 36/80: current_loss=0.03796 | best_loss=0.03796
Epoch 37/80: current_loss=0.03815 | best_loss=0.03796
Epoch 38/80: current_loss=0.03789 | best_loss=0.03789
Epoch 39/80: current_loss=0.03814 | best_loss=0.03789
Epoch 40/80: current_loss=0.03780 | best_loss=0.03780
Epoch 41/80: current_loss=0.03791 | best_loss=0.03780
Epoch 42/80: current_loss=0.03779 | best_loss=0.03779
Epoch 43/80: current_loss=0.03789 | best_loss=0.03779
Epoch 44/80: current_loss=0.03772 | best_loss=0.03772
Epoch 45/80: current_loss=0.03777 | best_loss=0.03772
Epoch 46/80: current_loss=0.03787 | best_loss=0.03772
Epoch 47/80: current_loss=0.03770 | best_loss=0.03770
Epoch 48/80: current_loss=0.03771 | best_loss=0.03770
Epoch 49/80: current_loss=0.03759 | best_loss=0.03759
Epoch 50/80: current_loss=0.03748 | best_loss=0.03748
Epoch 51/80: current_loss=0.03744 | best_loss=0.03744
Epoch 52/80: current_loss=0.03755 | best_loss=0.03744
Epoch 53/80: current_loss=0.03778 | best_loss=0.03744
Epoch 54/80: current_loss=0.03762 | best_loss=0.03744
Epoch 55/80: current_loss=0.03766 | best_loss=0.03744
Epoch 56/80: current_loss=0.03756 | best_loss=0.03744
Epoch 57/80: current_loss=0.03770 | best_loss=0.03744
Epoch 58/80: current_loss=0.03764 | best_loss=0.03744
Epoch 59/80: current_loss=0.03759 | best_loss=0.03744
Epoch 60/80: current_loss=0.03752 | best_loss=0.03744
Epoch 61/80: current_loss=0.03754 | best_loss=0.03744
Epoch 62/80: current_loss=0.03756 | best_loss=0.03744
Epoch 63/80: current_loss=0.03783 | best_loss=0.03744
Epoch 64/80: current_loss=0.03765 | best_loss=0.03744
Epoch 65/80: current_loss=0.03754 | best_loss=0.03744
Epoch 66/80: current_loss=0.03752 | best_loss=0.03744
Epoch 67/80: current_loss=0.03748 | best_loss=0.03744
Epoch 68/80: current_loss=0.03756 | best_loss=0.03744
Epoch 69/80: current_loss=0.03760 | best_loss=0.03744
Epoch 70/80: current_loss=0.03750 | best_loss=0.03744
Epoch 71/80: current_loss=0.03795 | best_loss=0.03744
Early Stopping at epoch 71
      explained_var=0.01885 | mse_loss=0.03841
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04129 | best_loss=0.04129
Epoch 1/80: current_loss=0.04131 | best_loss=0.04129
Epoch 2/80: current_loss=0.04131 | best_loss=0.04129
Epoch 3/80: current_loss=0.04146 | best_loss=0.04129
Epoch 4/80: current_loss=0.04144 | best_loss=0.04129
Epoch 5/80: current_loss=0.04163 | best_loss=0.04129
Epoch 6/80: current_loss=0.04152 | best_loss=0.04129
Epoch 7/80: current_loss=0.04157 | best_loss=0.04129
Epoch 8/80: current_loss=0.04157 | best_loss=0.04129
Epoch 9/80: current_loss=0.04161 | best_loss=0.04129
Epoch 10/80: current_loss=0.04145 | best_loss=0.04129
Epoch 11/80: current_loss=0.04146 | best_loss=0.04129
Epoch 12/80: current_loss=0.04148 | best_loss=0.04129
Epoch 13/80: current_loss=0.04141 | best_loss=0.04129
Epoch 14/80: current_loss=0.04133 | best_loss=0.04129
Epoch 15/80: current_loss=0.04146 | best_loss=0.04129
Epoch 16/80: current_loss=0.04141 | best_loss=0.04129
Epoch 17/80: current_loss=0.04136 | best_loss=0.04129
Epoch 18/80: current_loss=0.04169 | best_loss=0.04129
Epoch 19/80: current_loss=0.04143 | best_loss=0.04129
Epoch 20/80: current_loss=0.04149 | best_loss=0.04129
Early Stopping at epoch 20
      explained_var=0.03550 | mse_loss=0.03977
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02911 | best_loss=0.02911
Epoch 1/80: current_loss=0.02812 | best_loss=0.02812
Epoch 2/80: current_loss=0.02829 | best_loss=0.02812
Epoch 3/80: current_loss=0.02831 | best_loss=0.02812
Epoch 4/80: current_loss=0.02856 | best_loss=0.02812
Epoch 5/80: current_loss=0.02831 | best_loss=0.02812
Epoch 6/80: current_loss=0.02836 | best_loss=0.02812
Epoch 7/80: current_loss=0.02870 | best_loss=0.02812
Epoch 8/80: current_loss=0.02861 | best_loss=0.02812
Epoch 9/80: current_loss=0.02856 | best_loss=0.02812
Epoch 10/80: current_loss=0.02878 | best_loss=0.02812
Epoch 11/80: current_loss=0.02826 | best_loss=0.02812
Epoch 12/80: current_loss=0.02845 | best_loss=0.02812
Epoch 13/80: current_loss=0.02839 | best_loss=0.02812
Epoch 14/80: current_loss=0.02861 | best_loss=0.02812
Epoch 15/80: current_loss=0.02833 | best_loss=0.02812
Epoch 16/80: current_loss=0.02820 | best_loss=0.02812
Epoch 17/80: current_loss=0.02884 | best_loss=0.02812
Epoch 18/80: current_loss=0.02826 | best_loss=0.02812
Epoch 19/80: current_loss=0.02928 | best_loss=0.02812
Epoch 20/80: current_loss=0.02837 | best_loss=0.02812
Epoch 21/80: current_loss=0.02841 | best_loss=0.02812
Early Stopping at epoch 21
      explained_var=-0.00504 | mse_loss=0.02852

----------------------------------------------
Params for Trial 80
{'learning_rate': 0.01, 'weight_decay': 0.0043527525601359295, 'n_layers': 3, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.08296 | best_loss=0.08296
Epoch 1/80: current_loss=0.05814 | best_loss=0.05814
Epoch 2/80: current_loss=0.04014 | best_loss=0.04014
Epoch 3/80: current_loss=0.04188 | best_loss=0.04014
Epoch 4/80: current_loss=0.03943 | best_loss=0.03943
Epoch 5/80: current_loss=0.04004 | best_loss=0.03943
Epoch 6/80: current_loss=0.03869 | best_loss=0.03869
Epoch 7/80: current_loss=0.03872 | best_loss=0.03869
Epoch 8/80: current_loss=0.03984 | best_loss=0.03869
Epoch 9/80: current_loss=0.03863 | best_loss=0.03863
Epoch 10/80: current_loss=0.03862 | best_loss=0.03862
Epoch 11/80: current_loss=0.03891 | best_loss=0.03862
Epoch 12/80: current_loss=0.03837 | best_loss=0.03837
Epoch 13/80: current_loss=0.03896 | best_loss=0.03837
Epoch 14/80: current_loss=0.03841 | best_loss=0.03837
Epoch 15/80: current_loss=0.03868 | best_loss=0.03837
Epoch 16/80: current_loss=0.03869 | best_loss=0.03837
Epoch 17/80: current_loss=0.03832 | best_loss=0.03832
Epoch 18/80: current_loss=0.03988 | best_loss=0.03832
Epoch 19/80: current_loss=0.03940 | best_loss=0.03832
Epoch 20/80: current_loss=0.04085 | best_loss=0.03832
Epoch 21/80: current_loss=0.03878 | best_loss=0.03832
Epoch 22/80: current_loss=0.03886 | best_loss=0.03832
Epoch 23/80: current_loss=0.03857 | best_loss=0.03832
Epoch 24/80: current_loss=0.03836 | best_loss=0.03832
Epoch 25/80: current_loss=0.03840 | best_loss=0.03832
Epoch 26/80: current_loss=0.03870 | best_loss=0.03832
Epoch 27/80: current_loss=0.03853 | best_loss=0.03832
Epoch 28/80: current_loss=0.03867 | best_loss=0.03832
Epoch 29/80: current_loss=0.03877 | best_loss=0.03832
Epoch 30/80: current_loss=0.03834 | best_loss=0.03832
Epoch 31/80: current_loss=0.03835 | best_loss=0.03832
Epoch 32/80: current_loss=0.03849 | best_loss=0.03832
Epoch 33/80: current_loss=0.03842 | best_loss=0.03832
Epoch 34/80: current_loss=0.03838 | best_loss=0.03832
Epoch 35/80: current_loss=0.03844 | best_loss=0.03832
Epoch 36/80: current_loss=0.03852 | best_loss=0.03832
Epoch 37/80: current_loss=0.03869 | best_loss=0.03832
Early Stopping at epoch 37
      explained_var=0.00037 | mse_loss=0.03911

----------------------------------------------
Params for Trial 81
{'learning_rate': 0.0001, 'weight_decay': 0.00024710465315079554, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04981 | best_loss=0.04981
Epoch 1/80: current_loss=0.04381 | best_loss=0.04381
Epoch 2/80: current_loss=0.04143 | best_loss=0.04143
Epoch 3/80: current_loss=0.04079 | best_loss=0.04079
Epoch 4/80: current_loss=0.04031 | best_loss=0.04031
Epoch 5/80: current_loss=0.03984 | best_loss=0.03984
Epoch 6/80: current_loss=0.03943 | best_loss=0.03943
Epoch 7/80: current_loss=0.03911 | best_loss=0.03911
Epoch 8/80: current_loss=0.03882 | best_loss=0.03882
Epoch 9/80: current_loss=0.03880 | best_loss=0.03880
Epoch 10/80: current_loss=0.03869 | best_loss=0.03869
Epoch 11/80: current_loss=0.03870 | best_loss=0.03869
Epoch 12/80: current_loss=0.03830 | best_loss=0.03830
Epoch 13/80: current_loss=0.03829 | best_loss=0.03829
Epoch 14/80: current_loss=0.03823 | best_loss=0.03823
Epoch 15/80: current_loss=0.03906 | best_loss=0.03823
Epoch 16/80: current_loss=0.03829 | best_loss=0.03823
Epoch 17/80: current_loss=0.03800 | best_loss=0.03800
Epoch 18/80: current_loss=0.03798 | best_loss=0.03798
Epoch 19/80: current_loss=0.03776 | best_loss=0.03776
Epoch 20/80: current_loss=0.03785 | best_loss=0.03776
Epoch 21/80: current_loss=0.03775 | best_loss=0.03775
Epoch 22/80: current_loss=0.03876 | best_loss=0.03775
Epoch 23/80: current_loss=0.03777 | best_loss=0.03775
Epoch 24/80: current_loss=0.03781 | best_loss=0.03775
Epoch 25/80: current_loss=0.03772 | best_loss=0.03772
Epoch 26/80: current_loss=0.03783 | best_loss=0.03772
Epoch 27/80: current_loss=0.03819 | best_loss=0.03772
Epoch 28/80: current_loss=0.03774 | best_loss=0.03772
Epoch 29/80: current_loss=0.03786 | best_loss=0.03772
Epoch 30/80: current_loss=0.03770 | best_loss=0.03770
Epoch 31/80: current_loss=0.03768 | best_loss=0.03768
Epoch 32/80: current_loss=0.03761 | best_loss=0.03761
Epoch 33/80: current_loss=0.03761 | best_loss=0.03761
Epoch 34/80: current_loss=0.03779 | best_loss=0.03761
Epoch 35/80: current_loss=0.03776 | best_loss=0.03761
Epoch 36/80: current_loss=0.03775 | best_loss=0.03761
Epoch 37/80: current_loss=0.03750 | best_loss=0.03750
Epoch 38/80: current_loss=0.03771 | best_loss=0.03750
Epoch 39/80: current_loss=0.03766 | best_loss=0.03750
Epoch 40/80: current_loss=0.03765 | best_loss=0.03750
Epoch 41/80: current_loss=0.03755 | best_loss=0.03750
Epoch 42/80: current_loss=0.03785 | best_loss=0.03750
Epoch 43/80: current_loss=0.03762 | best_loss=0.03750
Epoch 44/80: current_loss=0.03761 | best_loss=0.03750
Epoch 45/80: current_loss=0.03765 | best_loss=0.03750
Epoch 46/80: current_loss=0.03753 | best_loss=0.03750
Epoch 47/80: current_loss=0.03743 | best_loss=0.03743
Epoch 48/80: current_loss=0.03740 | best_loss=0.03740
Epoch 49/80: current_loss=0.03746 | best_loss=0.03740
Epoch 50/80: current_loss=0.03748 | best_loss=0.03740
Epoch 51/80: current_loss=0.03758 | best_loss=0.03740
Epoch 52/80: current_loss=0.03769 | best_loss=0.03740
Epoch 53/80: current_loss=0.03805 | best_loss=0.03740
Epoch 54/80: current_loss=0.03756 | best_loss=0.03740
Epoch 55/80: current_loss=0.03764 | best_loss=0.03740
Epoch 56/80: current_loss=0.03775 | best_loss=0.03740
Epoch 57/80: current_loss=0.03806 | best_loss=0.03740
Epoch 58/80: current_loss=0.03759 | best_loss=0.03740
Epoch 59/80: current_loss=0.03773 | best_loss=0.03740
Epoch 60/80: current_loss=0.03789 | best_loss=0.03740
Epoch 61/80: current_loss=0.03759 | best_loss=0.03740
Epoch 62/80: current_loss=0.03766 | best_loss=0.03740
Epoch 63/80: current_loss=0.03754 | best_loss=0.03740
Epoch 64/80: current_loss=0.03756 | best_loss=0.03740
Epoch 65/80: current_loss=0.03757 | best_loss=0.03740
Epoch 66/80: current_loss=0.03741 | best_loss=0.03740
Epoch 67/80: current_loss=0.03811 | best_loss=0.03740
Epoch 68/80: current_loss=0.03743 | best_loss=0.03740
Early Stopping at epoch 68
      explained_var=0.02268 | mse_loss=0.03834
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04143 | best_loss=0.04143
Epoch 1/80: current_loss=0.04155 | best_loss=0.04143
Epoch 2/80: current_loss=0.04151 | best_loss=0.04143
Epoch 3/80: current_loss=0.04296 | best_loss=0.04143
Epoch 4/80: current_loss=0.04169 | best_loss=0.04143
Epoch 5/80: current_loss=0.04167 | best_loss=0.04143
Epoch 6/80: current_loss=0.04161 | best_loss=0.04143
Epoch 7/80: current_loss=0.04146 | best_loss=0.04143
Epoch 8/80: current_loss=0.04137 | best_loss=0.04137
Epoch 9/80: current_loss=0.04165 | best_loss=0.04137
Epoch 10/80: current_loss=0.04140 | best_loss=0.04137
Epoch 11/80: current_loss=0.04162 | best_loss=0.04137
Epoch 12/80: current_loss=0.04172 | best_loss=0.04137
Epoch 13/80: current_loss=0.04149 | best_loss=0.04137
Epoch 14/80: current_loss=0.04145 | best_loss=0.04137
Epoch 15/80: current_loss=0.04158 | best_loss=0.04137
Epoch 16/80: current_loss=0.04120 | best_loss=0.04120
Epoch 17/80: current_loss=0.04115 | best_loss=0.04115
Epoch 18/80: current_loss=0.04113 | best_loss=0.04113
Epoch 19/80: current_loss=0.04160 | best_loss=0.04113
Epoch 20/80: current_loss=0.04116 | best_loss=0.04113
Epoch 21/80: current_loss=0.04155 | best_loss=0.04113
Epoch 22/80: current_loss=0.04121 | best_loss=0.04113
Epoch 23/80: current_loss=0.04137 | best_loss=0.04113
Epoch 24/80: current_loss=0.04153 | best_loss=0.04113
Epoch 25/80: current_loss=0.04119 | best_loss=0.04113
Epoch 26/80: current_loss=0.04124 | best_loss=0.04113
Epoch 27/80: current_loss=0.04178 | best_loss=0.04113
Epoch 28/80: current_loss=0.04146 | best_loss=0.04113
Epoch 29/80: current_loss=0.04150 | best_loss=0.04113
Epoch 30/80: current_loss=0.04141 | best_loss=0.04113
Epoch 31/80: current_loss=0.04229 | best_loss=0.04113
Epoch 32/80: current_loss=0.04134 | best_loss=0.04113
Epoch 33/80: current_loss=0.04127 | best_loss=0.04113
Epoch 34/80: current_loss=0.04189 | best_loss=0.04113
Epoch 35/80: current_loss=0.04125 | best_loss=0.04113
Epoch 36/80: current_loss=0.04210 | best_loss=0.04113
Epoch 37/80: current_loss=0.04123 | best_loss=0.04113
Epoch 38/80: current_loss=0.04133 | best_loss=0.04113
Early Stopping at epoch 38
      explained_var=0.03893 | mse_loss=0.03968
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02840 | best_loss=0.02840
Epoch 1/80: current_loss=0.02824 | best_loss=0.02824
Epoch 2/80: current_loss=0.02835 | best_loss=0.02824
Epoch 3/80: current_loss=0.02830 | best_loss=0.02824
Epoch 4/80: current_loss=0.02910 | best_loss=0.02824
Epoch 5/80: current_loss=0.02832 | best_loss=0.02824
Epoch 6/80: current_loss=0.02884 | best_loss=0.02824
Epoch 7/80: current_loss=0.02828 | best_loss=0.02824
Epoch 8/80: current_loss=0.02871 | best_loss=0.02824
Epoch 9/80: current_loss=0.02845 | best_loss=0.02824
Epoch 10/80: current_loss=0.02839 | best_loss=0.02824
Epoch 11/80: current_loss=0.02836 | best_loss=0.02824
Epoch 12/80: current_loss=0.02834 | best_loss=0.02824
Epoch 13/80: current_loss=0.02919 | best_loss=0.02824
Epoch 14/80: current_loss=0.02830 | best_loss=0.02824
Epoch 15/80: current_loss=0.02863 | best_loss=0.02824
Epoch 16/80: current_loss=0.02829 | best_loss=0.02824
Epoch 17/80: current_loss=0.02836 | best_loss=0.02824
Epoch 18/80: current_loss=0.02848 | best_loss=0.02824
Epoch 19/80: current_loss=0.02852 | best_loss=0.02824
Epoch 20/80: current_loss=0.02846 | best_loss=0.02824
Epoch 21/80: current_loss=0.02840 | best_loss=0.02824
Early Stopping at epoch 21
      explained_var=-0.00764 | mse_loss=0.02867

----------------------------------------------
Params for Trial 82
{'learning_rate': 0.0001, 'weight_decay': 0.0002774661760512302, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.05242 | best_loss=0.05242
Epoch 1/80: current_loss=0.04596 | best_loss=0.04596
Epoch 2/80: current_loss=0.04459 | best_loss=0.04459
Epoch 3/80: current_loss=0.04351 | best_loss=0.04351
Epoch 4/80: current_loss=0.04287 | best_loss=0.04287
Epoch 5/80: current_loss=0.04178 | best_loss=0.04178
Epoch 6/80: current_loss=0.04118 | best_loss=0.04118
Epoch 7/80: current_loss=0.04095 | best_loss=0.04095
Epoch 8/80: current_loss=0.04025 | best_loss=0.04025
Epoch 9/80: current_loss=0.03987 | best_loss=0.03987
Epoch 10/80: current_loss=0.03957 | best_loss=0.03957
Epoch 11/80: current_loss=0.03927 | best_loss=0.03927
Epoch 12/80: current_loss=0.03919 | best_loss=0.03919
Epoch 13/80: current_loss=0.03879 | best_loss=0.03879
Epoch 14/80: current_loss=0.03872 | best_loss=0.03872
Epoch 15/80: current_loss=0.03877 | best_loss=0.03872
Epoch 16/80: current_loss=0.03881 | best_loss=0.03872
Epoch 17/80: current_loss=0.03839 | best_loss=0.03839
Epoch 18/80: current_loss=0.03877 | best_loss=0.03839
Epoch 19/80: current_loss=0.03815 | best_loss=0.03815
Epoch 20/80: current_loss=0.03856 | best_loss=0.03815
Epoch 21/80: current_loss=0.03791 | best_loss=0.03791
Epoch 22/80: current_loss=0.03869 | best_loss=0.03791
Epoch 23/80: current_loss=0.03781 | best_loss=0.03781
Epoch 24/80: current_loss=0.03773 | best_loss=0.03773
Epoch 25/80: current_loss=0.03781 | best_loss=0.03773
Epoch 26/80: current_loss=0.03793 | best_loss=0.03773
Epoch 27/80: current_loss=0.03778 | best_loss=0.03773
Epoch 28/80: current_loss=0.03831 | best_loss=0.03773
Epoch 29/80: current_loss=0.03821 | best_loss=0.03773
Epoch 30/80: current_loss=0.03786 | best_loss=0.03773
Epoch 31/80: current_loss=0.03800 | best_loss=0.03773
Epoch 32/80: current_loss=0.03797 | best_loss=0.03773
Epoch 33/80: current_loss=0.03871 | best_loss=0.03773
Epoch 34/80: current_loss=0.03774 | best_loss=0.03773
Epoch 35/80: current_loss=0.03776 | best_loss=0.03773
Epoch 36/80: current_loss=0.03783 | best_loss=0.03773
Epoch 37/80: current_loss=0.03763 | best_loss=0.03763
Epoch 38/80: current_loss=0.03786 | best_loss=0.03763
Epoch 39/80: current_loss=0.03770 | best_loss=0.03763
Epoch 40/80: current_loss=0.03785 | best_loss=0.03763
Epoch 41/80: current_loss=0.03830 | best_loss=0.03763
Epoch 42/80: current_loss=0.03797 | best_loss=0.03763
Epoch 43/80: current_loss=0.03834 | best_loss=0.03763
Epoch 44/80: current_loss=0.03781 | best_loss=0.03763
Epoch 45/80: current_loss=0.03765 | best_loss=0.03763
Epoch 46/80: current_loss=0.03760 | best_loss=0.03760
Epoch 47/80: current_loss=0.03758 | best_loss=0.03758
Epoch 48/80: current_loss=0.03752 | best_loss=0.03752
Epoch 49/80: current_loss=0.03751 | best_loss=0.03751
Epoch 50/80: current_loss=0.03780 | best_loss=0.03751
Epoch 51/80: current_loss=0.03755 | best_loss=0.03751
Epoch 52/80: current_loss=0.03753 | best_loss=0.03751
Epoch 53/80: current_loss=0.03750 | best_loss=0.03750
Epoch 54/80: current_loss=0.03767 | best_loss=0.03750
Epoch 55/80: current_loss=0.03764 | best_loss=0.03750
Epoch 56/80: current_loss=0.03745 | best_loss=0.03745
Epoch 57/80: current_loss=0.03811 | best_loss=0.03745
Epoch 58/80: current_loss=0.03747 | best_loss=0.03745
Epoch 59/80: current_loss=0.03776 | best_loss=0.03745
Epoch 60/80: current_loss=0.03772 | best_loss=0.03745
Epoch 61/80: current_loss=0.03768 | best_loss=0.03745
Epoch 62/80: current_loss=0.03797 | best_loss=0.03745
Epoch 63/80: current_loss=0.03761 | best_loss=0.03745
Epoch 64/80: current_loss=0.03775 | best_loss=0.03745
Epoch 65/80: current_loss=0.03761 | best_loss=0.03745
Epoch 66/80: current_loss=0.03782 | best_loss=0.03745
Epoch 67/80: current_loss=0.03766 | best_loss=0.03745
Epoch 68/80: current_loss=0.03788 | best_loss=0.03745
Epoch 69/80: current_loss=0.03763 | best_loss=0.03745
Epoch 70/80: current_loss=0.03804 | best_loss=0.03745
Epoch 71/80: current_loss=0.03778 | best_loss=0.03745
Epoch 72/80: current_loss=0.03762 | best_loss=0.03745
Epoch 73/80: current_loss=0.03788 | best_loss=0.03745
Epoch 74/80: current_loss=0.03761 | best_loss=0.03745
Epoch 75/80: current_loss=0.03763 | best_loss=0.03745
Epoch 76/80: current_loss=0.03825 | best_loss=0.03745
Early Stopping at epoch 76
      explained_var=0.01976 | mse_loss=0.03836
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04168 | best_loss=0.04168
Epoch 1/80: current_loss=0.04181 | best_loss=0.04168
Epoch 2/80: current_loss=0.04170 | best_loss=0.04168
Epoch 3/80: current_loss=0.04179 | best_loss=0.04168
Epoch 4/80: current_loss=0.04165 | best_loss=0.04165
Epoch 5/80: current_loss=0.04163 | best_loss=0.04163
Epoch 6/80: current_loss=0.04204 | best_loss=0.04163
Epoch 7/80: current_loss=0.04161 | best_loss=0.04161
Epoch 8/80: current_loss=0.04158 | best_loss=0.04158
Epoch 9/80: current_loss=0.04359 | best_loss=0.04158
Epoch 10/80: current_loss=0.04143 | best_loss=0.04143
Epoch 11/80: current_loss=0.04152 | best_loss=0.04143
Epoch 12/80: current_loss=0.04149 | best_loss=0.04143
Epoch 13/80: current_loss=0.04213 | best_loss=0.04143
Epoch 14/80: current_loss=0.04130 | best_loss=0.04130
Epoch 15/80: current_loss=0.04161 | best_loss=0.04130
Epoch 16/80: current_loss=0.04161 | best_loss=0.04130
Epoch 17/80: current_loss=0.04184 | best_loss=0.04130
Epoch 18/80: current_loss=0.04137 | best_loss=0.04130
Epoch 19/80: current_loss=0.04192 | best_loss=0.04130
Epoch 20/80: current_loss=0.04197 | best_loss=0.04130
Epoch 21/80: current_loss=0.04183 | best_loss=0.04130
Epoch 22/80: current_loss=0.04158 | best_loss=0.04130
Epoch 23/80: current_loss=0.04196 | best_loss=0.04130
Epoch 24/80: current_loss=0.04152 | best_loss=0.04130
Epoch 25/80: current_loss=0.04165 | best_loss=0.04130
Epoch 26/80: current_loss=0.04176 | best_loss=0.04130
Epoch 27/80: current_loss=0.04179 | best_loss=0.04130
Epoch 28/80: current_loss=0.04181 | best_loss=0.04130
Epoch 29/80: current_loss=0.04144 | best_loss=0.04130
Epoch 30/80: current_loss=0.04159 | best_loss=0.04130
Epoch 31/80: current_loss=0.04161 | best_loss=0.04130
Epoch 32/80: current_loss=0.04208 | best_loss=0.04130
Epoch 33/80: current_loss=0.04170 | best_loss=0.04130
Epoch 34/80: current_loss=0.04146 | best_loss=0.04130
Early Stopping at epoch 34
      explained_var=0.03432 | mse_loss=0.03982
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02860 | best_loss=0.02860
Epoch 1/80: current_loss=0.02820 | best_loss=0.02820
Epoch 2/80: current_loss=0.02827 | best_loss=0.02820
Epoch 3/80: current_loss=0.02815 | best_loss=0.02815
Epoch 4/80: current_loss=0.02825 | best_loss=0.02815
Epoch 5/80: current_loss=0.02830 | best_loss=0.02815
Epoch 6/80: current_loss=0.02911 | best_loss=0.02815
Epoch 7/80: current_loss=0.02851 | best_loss=0.02815
Epoch 8/80: current_loss=0.02894 | best_loss=0.02815
Epoch 9/80: current_loss=0.02861 | best_loss=0.02815
Epoch 10/80: current_loss=0.02860 | best_loss=0.02815
Epoch 11/80: current_loss=0.02870 | best_loss=0.02815
Epoch 12/80: current_loss=0.02869 | best_loss=0.02815
Epoch 13/80: current_loss=0.02835 | best_loss=0.02815
Epoch 14/80: current_loss=0.02856 | best_loss=0.02815
Epoch 15/80: current_loss=0.02827 | best_loss=0.02815
Epoch 16/80: current_loss=0.02829 | best_loss=0.02815
Epoch 17/80: current_loss=0.02882 | best_loss=0.02815
Epoch 18/80: current_loss=0.02858 | best_loss=0.02815
Epoch 19/80: current_loss=0.02876 | best_loss=0.02815
Epoch 20/80: current_loss=0.02831 | best_loss=0.02815
Epoch 21/80: current_loss=0.02908 | best_loss=0.02815
Epoch 22/80: current_loss=0.02819 | best_loss=0.02815
Epoch 23/80: current_loss=0.02864 | best_loss=0.02815
Early Stopping at epoch 23
      explained_var=-0.00913 | mse_loss=0.02862

----------------------------------------------
Params for Trial 83
{'learning_rate': 0.0001, 'weight_decay': 0.0005380429009862278, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04577 | best_loss=0.04577
Epoch 1/80: current_loss=0.04332 | best_loss=0.04332
Epoch 2/80: current_loss=0.04160 | best_loss=0.04160
Epoch 3/80: current_loss=0.04117 | best_loss=0.04117
Epoch 4/80: current_loss=0.04069 | best_loss=0.04069
Epoch 5/80: current_loss=0.03998 | best_loss=0.03998
Epoch 6/80: current_loss=0.03992 | best_loss=0.03992
Epoch 7/80: current_loss=0.03953 | best_loss=0.03953
Epoch 8/80: current_loss=0.03911 | best_loss=0.03911
Epoch 9/80: current_loss=0.03887 | best_loss=0.03887
Epoch 10/80: current_loss=0.03879 | best_loss=0.03879
Epoch 11/80: current_loss=0.03864 | best_loss=0.03864
Epoch 12/80: current_loss=0.03857 | best_loss=0.03857
Epoch 13/80: current_loss=0.03846 | best_loss=0.03846
Epoch 14/80: current_loss=0.03846 | best_loss=0.03846
Epoch 15/80: current_loss=0.03848 | best_loss=0.03846
Epoch 16/80: current_loss=0.03877 | best_loss=0.03846
Epoch 17/80: current_loss=0.03815 | best_loss=0.03815
Epoch 18/80: current_loss=0.03817 | best_loss=0.03815
Epoch 19/80: current_loss=0.03795 | best_loss=0.03795
Epoch 20/80: current_loss=0.03791 | best_loss=0.03791
Epoch 21/80: current_loss=0.03782 | best_loss=0.03782
Epoch 22/80: current_loss=0.03773 | best_loss=0.03773
Epoch 23/80: current_loss=0.03791 | best_loss=0.03773
Epoch 24/80: current_loss=0.03773 | best_loss=0.03773
Epoch 25/80: current_loss=0.03803 | best_loss=0.03773
Epoch 26/80: current_loss=0.03795 | best_loss=0.03773
Epoch 27/80: current_loss=0.03790 | best_loss=0.03773
Epoch 28/80: current_loss=0.03780 | best_loss=0.03773
Epoch 29/80: current_loss=0.03759 | best_loss=0.03759
Epoch 30/80: current_loss=0.03749 | best_loss=0.03749
Epoch 31/80: current_loss=0.03753 | best_loss=0.03749
Epoch 32/80: current_loss=0.03771 | best_loss=0.03749
Epoch 33/80: current_loss=0.03771 | best_loss=0.03749
Epoch 34/80: current_loss=0.03755 | best_loss=0.03749
Epoch 35/80: current_loss=0.03747 | best_loss=0.03747
Epoch 36/80: current_loss=0.03783 | best_loss=0.03747
Epoch 37/80: current_loss=0.03753 | best_loss=0.03747
Epoch 38/80: current_loss=0.03840 | best_loss=0.03747
Epoch 39/80: current_loss=0.03772 | best_loss=0.03747
Epoch 40/80: current_loss=0.03757 | best_loss=0.03747
Epoch 41/80: current_loss=0.03762 | best_loss=0.03747
Epoch 42/80: current_loss=0.03737 | best_loss=0.03737
Epoch 43/80: current_loss=0.03783 | best_loss=0.03737
Epoch 44/80: current_loss=0.03756 | best_loss=0.03737
Epoch 45/80: current_loss=0.03802 | best_loss=0.03737
Epoch 46/80: current_loss=0.03794 | best_loss=0.03737
Epoch 47/80: current_loss=0.03784 | best_loss=0.03737
Epoch 48/80: current_loss=0.03779 | best_loss=0.03737
Epoch 49/80: current_loss=0.03762 | best_loss=0.03737
Epoch 50/80: current_loss=0.03766 | best_loss=0.03737
Epoch 51/80: current_loss=0.03765 | best_loss=0.03737
Epoch 52/80: current_loss=0.03752 | best_loss=0.03737
Epoch 53/80: current_loss=0.03748 | best_loss=0.03737
Epoch 54/80: current_loss=0.03761 | best_loss=0.03737
Epoch 55/80: current_loss=0.03755 | best_loss=0.03737
Epoch 56/80: current_loss=0.03758 | best_loss=0.03737
Epoch 57/80: current_loss=0.03793 | best_loss=0.03737
Epoch 58/80: current_loss=0.03791 | best_loss=0.03737
Epoch 59/80: current_loss=0.03755 | best_loss=0.03737
Epoch 60/80: current_loss=0.03764 | best_loss=0.03737
Epoch 61/80: current_loss=0.03765 | best_loss=0.03737
Epoch 62/80: current_loss=0.03809 | best_loss=0.03737
Early Stopping at epoch 62
      explained_var=0.02237 | mse_loss=0.03830
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04208 | best_loss=0.04208
Epoch 1/80: current_loss=0.04158 | best_loss=0.04158
Epoch 2/80: current_loss=0.04123 | best_loss=0.04123
Epoch 3/80: current_loss=0.04144 | best_loss=0.04123
Epoch 4/80: current_loss=0.04127 | best_loss=0.04123
Epoch 5/80: current_loss=0.04244 | best_loss=0.04123
Epoch 6/80: current_loss=0.04129 | best_loss=0.04123
Epoch 7/80: current_loss=0.04125 | best_loss=0.04123
Epoch 8/80: current_loss=0.04124 | best_loss=0.04123
Epoch 9/80: current_loss=0.04201 | best_loss=0.04123
Epoch 10/80: current_loss=0.04195 | best_loss=0.04123
Epoch 11/80: current_loss=0.04149 | best_loss=0.04123
Epoch 12/80: current_loss=0.04178 | best_loss=0.04123
Epoch 13/80: current_loss=0.04157 | best_loss=0.04123
Epoch 14/80: current_loss=0.04239 | best_loss=0.04123
Epoch 15/80: current_loss=0.04151 | best_loss=0.04123
Epoch 16/80: current_loss=0.04161 | best_loss=0.04123
Epoch 17/80: current_loss=0.04160 | best_loss=0.04123
Epoch 18/80: current_loss=0.04142 | best_loss=0.04123
Epoch 19/80: current_loss=0.04180 | best_loss=0.04123
Epoch 20/80: current_loss=0.04125 | best_loss=0.04123
Epoch 21/80: current_loss=0.04123 | best_loss=0.04123
Epoch 22/80: current_loss=0.04190 | best_loss=0.04123
Epoch 23/80: current_loss=0.04158 | best_loss=0.04123
Epoch 24/80: current_loss=0.04144 | best_loss=0.04123
Epoch 25/80: current_loss=0.04161 | best_loss=0.04123
Epoch 26/80: current_loss=0.04131 | best_loss=0.04123
Epoch 27/80: current_loss=0.04148 | best_loss=0.04123
Epoch 28/80: current_loss=0.04200 | best_loss=0.04123
Epoch 29/80: current_loss=0.04165 | best_loss=0.04123
Epoch 30/80: current_loss=0.04149 | best_loss=0.04123
Epoch 31/80: current_loss=0.04146 | best_loss=0.04123
Epoch 32/80: current_loss=0.04212 | best_loss=0.04123
Epoch 33/80: current_loss=0.04185 | best_loss=0.04123
Epoch 34/80: current_loss=0.04163 | best_loss=0.04123
Epoch 35/80: current_loss=0.04136 | best_loss=0.04123
Epoch 36/80: current_loss=0.04154 | best_loss=0.04123
Epoch 37/80: current_loss=0.04128 | best_loss=0.04123
Epoch 38/80: current_loss=0.04135 | best_loss=0.04123
Epoch 39/80: current_loss=0.04148 | best_loss=0.04123
Epoch 40/80: current_loss=0.04187 | best_loss=0.04123
Epoch 41/80: current_loss=0.04176 | best_loss=0.04123
Early Stopping at epoch 41
      explained_var=0.03749 | mse_loss=0.03974
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02863 | best_loss=0.02863
Epoch 1/80: current_loss=0.02802 | best_loss=0.02802
Epoch 2/80: current_loss=0.02877 | best_loss=0.02802
Epoch 3/80: current_loss=0.02817 | best_loss=0.02802
Epoch 4/80: current_loss=0.02830 | best_loss=0.02802
Epoch 5/80: current_loss=0.02802 | best_loss=0.02802
Epoch 6/80: current_loss=0.02930 | best_loss=0.02802
Epoch 7/80: current_loss=0.02835 | best_loss=0.02802
Epoch 8/80: current_loss=0.02835 | best_loss=0.02802
Epoch 9/80: current_loss=0.02818 | best_loss=0.02802
Epoch 10/80: current_loss=0.02870 | best_loss=0.02802
Epoch 11/80: current_loss=0.02829 | best_loss=0.02802
Epoch 12/80: current_loss=0.02855 | best_loss=0.02802
Epoch 13/80: current_loss=0.02864 | best_loss=0.02802
Epoch 14/80: current_loss=0.02870 | best_loss=0.02802
Epoch 15/80: current_loss=0.02843 | best_loss=0.02802
Epoch 16/80: current_loss=0.02854 | best_loss=0.02802
Epoch 17/80: current_loss=0.02862 | best_loss=0.02802
Epoch 18/80: current_loss=0.02817 | best_loss=0.02802
Epoch 19/80: current_loss=0.02855 | best_loss=0.02802
Epoch 20/80: current_loss=0.02831 | best_loss=0.02802
Epoch 21/80: current_loss=0.02859 | best_loss=0.02802
Early Stopping at epoch 21
      explained_var=-0.00481 | mse_loss=0.02852

----------------------------------------------
Params for Trial 84
{'learning_rate': 0.0001, 'weight_decay': 0.0009992567493669643, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04523 | best_loss=0.04523
Epoch 1/80: current_loss=0.04339 | best_loss=0.04339
Epoch 2/80: current_loss=0.04272 | best_loss=0.04272
Epoch 3/80: current_loss=0.04160 | best_loss=0.04160
Epoch 4/80: current_loss=0.04098 | best_loss=0.04098
Epoch 5/80: current_loss=0.04059 | best_loss=0.04059
Epoch 6/80: current_loss=0.03992 | best_loss=0.03992
Epoch 7/80: current_loss=0.03955 | best_loss=0.03955
Epoch 8/80: current_loss=0.03929 | best_loss=0.03929
Epoch 9/80: current_loss=0.03891 | best_loss=0.03891
Epoch 10/80: current_loss=0.03987 | best_loss=0.03891
Epoch 11/80: current_loss=0.03869 | best_loss=0.03869
Epoch 12/80: current_loss=0.03842 | best_loss=0.03842
Epoch 13/80: current_loss=0.03836 | best_loss=0.03836
Epoch 14/80: current_loss=0.03842 | best_loss=0.03836
Epoch 15/80: current_loss=0.03832 | best_loss=0.03832
Epoch 16/80: current_loss=0.03843 | best_loss=0.03832
Epoch 17/80: current_loss=0.03823 | best_loss=0.03823
Epoch 18/80: current_loss=0.03872 | best_loss=0.03823
Epoch 19/80: current_loss=0.03828 | best_loss=0.03823
Epoch 20/80: current_loss=0.03798 | best_loss=0.03798
Epoch 21/80: current_loss=0.03842 | best_loss=0.03798
Epoch 22/80: current_loss=0.03796 | best_loss=0.03796
Epoch 23/80: current_loss=0.03792 | best_loss=0.03792
Epoch 24/80: current_loss=0.03796 | best_loss=0.03792
Epoch 25/80: current_loss=0.03840 | best_loss=0.03792
Epoch 26/80: current_loss=0.03815 | best_loss=0.03792
Epoch 27/80: current_loss=0.03765 | best_loss=0.03765
Epoch 28/80: current_loss=0.03763 | best_loss=0.03763
Epoch 29/80: current_loss=0.03767 | best_loss=0.03763
Epoch 30/80: current_loss=0.03780 | best_loss=0.03763
Epoch 31/80: current_loss=0.03780 | best_loss=0.03763
Epoch 32/80: current_loss=0.03766 | best_loss=0.03763
Epoch 33/80: current_loss=0.03753 | best_loss=0.03753
Epoch 34/80: current_loss=0.03770 | best_loss=0.03753
Epoch 35/80: current_loss=0.03778 | best_loss=0.03753
Epoch 36/80: current_loss=0.03761 | best_loss=0.03753
Epoch 37/80: current_loss=0.03741 | best_loss=0.03741
Epoch 38/80: current_loss=0.03764 | best_loss=0.03741
Epoch 39/80: current_loss=0.03750 | best_loss=0.03741
Epoch 40/80: current_loss=0.03755 | best_loss=0.03741
Epoch 41/80: current_loss=0.03782 | best_loss=0.03741
Epoch 42/80: current_loss=0.03747 | best_loss=0.03741
Epoch 43/80: current_loss=0.03758 | best_loss=0.03741
Epoch 44/80: current_loss=0.03754 | best_loss=0.03741
Epoch 45/80: current_loss=0.03820 | best_loss=0.03741
Epoch 46/80: current_loss=0.03747 | best_loss=0.03741
Epoch 47/80: current_loss=0.03757 | best_loss=0.03741
Epoch 48/80: current_loss=0.03765 | best_loss=0.03741
Epoch 49/80: current_loss=0.03758 | best_loss=0.03741
Epoch 50/80: current_loss=0.03777 | best_loss=0.03741
Epoch 51/80: current_loss=0.03752 | best_loss=0.03741
Epoch 52/80: current_loss=0.03742 | best_loss=0.03741
Epoch 53/80: current_loss=0.03822 | best_loss=0.03741
Epoch 54/80: current_loss=0.03751 | best_loss=0.03741
Epoch 55/80: current_loss=0.03753 | best_loss=0.03741
Epoch 56/80: current_loss=0.03737 | best_loss=0.03737
Epoch 57/80: current_loss=0.03776 | best_loss=0.03737
Epoch 58/80: current_loss=0.03753 | best_loss=0.03737
Epoch 59/80: current_loss=0.03777 | best_loss=0.03737
Epoch 60/80: current_loss=0.03770 | best_loss=0.03737
Epoch 61/80: current_loss=0.03743 | best_loss=0.03737
Epoch 62/80: current_loss=0.03751 | best_loss=0.03737
Epoch 63/80: current_loss=0.03755 | best_loss=0.03737
Epoch 64/80: current_loss=0.03820 | best_loss=0.03737
Epoch 65/80: current_loss=0.03753 | best_loss=0.03737
Epoch 66/80: current_loss=0.03784 | best_loss=0.03737
Epoch 67/80: current_loss=0.03755 | best_loss=0.03737
Epoch 68/80: current_loss=0.03761 | best_loss=0.03737
Epoch 69/80: current_loss=0.03761 | best_loss=0.03737
Epoch 70/80: current_loss=0.03769 | best_loss=0.03737
Epoch 71/80: current_loss=0.03809 | best_loss=0.03737
Epoch 72/80: current_loss=0.03759 | best_loss=0.03737
Epoch 73/80: current_loss=0.03793 | best_loss=0.03737
Epoch 74/80: current_loss=0.03757 | best_loss=0.03737
Epoch 75/80: current_loss=0.03750 | best_loss=0.03737
Epoch 76/80: current_loss=0.03757 | best_loss=0.03737
Early Stopping at epoch 76
      explained_var=0.02299 | mse_loss=0.03833
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04238 | best_loss=0.04238
Epoch 1/80: current_loss=0.04152 | best_loss=0.04152
Epoch 2/80: current_loss=0.04170 | best_loss=0.04152
Epoch 3/80: current_loss=0.04148 | best_loss=0.04148
Epoch 4/80: current_loss=0.04224 | best_loss=0.04148
Epoch 5/80: current_loss=0.04146 | best_loss=0.04146
Epoch 6/80: current_loss=0.04141 | best_loss=0.04141
Epoch 7/80: current_loss=0.04159 | best_loss=0.04141
Epoch 8/80: current_loss=0.04149 | best_loss=0.04141
Epoch 9/80: current_loss=0.04164 | best_loss=0.04141
Epoch 10/80: current_loss=0.04183 | best_loss=0.04141
Epoch 11/80: current_loss=0.04124 | best_loss=0.04124
Epoch 12/80: current_loss=0.04140 | best_loss=0.04124
Epoch 13/80: current_loss=0.04146 | best_loss=0.04124
Epoch 14/80: current_loss=0.04134 | best_loss=0.04124
Epoch 15/80: current_loss=0.04126 | best_loss=0.04124
Epoch 16/80: current_loss=0.04128 | best_loss=0.04124
Epoch 17/80: current_loss=0.04191 | best_loss=0.04124
Epoch 18/80: current_loss=0.04167 | best_loss=0.04124
Epoch 19/80: current_loss=0.04143 | best_loss=0.04124
Epoch 20/80: current_loss=0.04142 | best_loss=0.04124
Epoch 21/80: current_loss=0.04143 | best_loss=0.04124
Epoch 22/80: current_loss=0.04165 | best_loss=0.04124
Epoch 23/80: current_loss=0.04141 | best_loss=0.04124
Epoch 24/80: current_loss=0.04155 | best_loss=0.04124
Epoch 25/80: current_loss=0.04134 | best_loss=0.04124
Epoch 26/80: current_loss=0.04166 | best_loss=0.04124
Epoch 27/80: current_loss=0.04134 | best_loss=0.04124
Epoch 28/80: current_loss=0.04178 | best_loss=0.04124
Epoch 29/80: current_loss=0.04138 | best_loss=0.04124
Epoch 30/80: current_loss=0.04143 | best_loss=0.04124
Epoch 31/80: current_loss=0.04163 | best_loss=0.04124
Early Stopping at epoch 31
      explained_var=0.03623 | mse_loss=0.03975
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02817 | best_loss=0.02817
Epoch 1/80: current_loss=0.02824 | best_loss=0.02817
Epoch 2/80: current_loss=0.02867 | best_loss=0.02817
Epoch 3/80: current_loss=0.02831 | best_loss=0.02817
Epoch 4/80: current_loss=0.02843 | best_loss=0.02817
Epoch 5/80: current_loss=0.02825 | best_loss=0.02817
Epoch 6/80: current_loss=0.02908 | best_loss=0.02817
Epoch 7/80: current_loss=0.02829 | best_loss=0.02817
Epoch 8/80: current_loss=0.02850 | best_loss=0.02817
Epoch 9/80: current_loss=0.02890 | best_loss=0.02817
Epoch 10/80: current_loss=0.02831 | best_loss=0.02817
Epoch 11/80: current_loss=0.02829 | best_loss=0.02817
Epoch 12/80: current_loss=0.02879 | best_loss=0.02817
Epoch 13/80: current_loss=0.02841 | best_loss=0.02817
Epoch 14/80: current_loss=0.02870 | best_loss=0.02817
Epoch 15/80: current_loss=0.02869 | best_loss=0.02817
Epoch 16/80: current_loss=0.02835 | best_loss=0.02817
Epoch 17/80: current_loss=0.02837 | best_loss=0.02817
Epoch 18/80: current_loss=0.02890 | best_loss=0.02817
Epoch 19/80: current_loss=0.02822 | best_loss=0.02817
Epoch 20/80: current_loss=0.02833 | best_loss=0.02817
Early Stopping at epoch 20
      explained_var=-0.00275 | mse_loss=0.02855

----------------------------------------------
Params for Trial 85
{'learning_rate': 0.001, 'weight_decay': 0.0019154039069742815, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.35}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03957 | best_loss=0.03957
Epoch 1/80: current_loss=0.03818 | best_loss=0.03818
Epoch 2/80: current_loss=0.03801 | best_loss=0.03801
Epoch 3/80: current_loss=0.03864 | best_loss=0.03801
Epoch 4/80: current_loss=0.03886 | best_loss=0.03801
Epoch 5/80: current_loss=0.04128 | best_loss=0.03801
Epoch 6/80: current_loss=0.03739 | best_loss=0.03739
Epoch 7/80: current_loss=0.03760 | best_loss=0.03739
Epoch 8/80: current_loss=0.03901 | best_loss=0.03739
Epoch 9/80: current_loss=0.03780 | best_loss=0.03739
Epoch 10/80: current_loss=0.03737 | best_loss=0.03737
Epoch 11/80: current_loss=0.03730 | best_loss=0.03730
Epoch 12/80: current_loss=0.03846 | best_loss=0.03730
Epoch 13/80: current_loss=0.03854 | best_loss=0.03730
Epoch 14/80: current_loss=0.03735 | best_loss=0.03730
Epoch 15/80: current_loss=0.03752 | best_loss=0.03730
Epoch 16/80: current_loss=0.04071 | best_loss=0.03730
Epoch 17/80: current_loss=0.03753 | best_loss=0.03730
Epoch 18/80: current_loss=0.03762 | best_loss=0.03730
Epoch 19/80: current_loss=0.03813 | best_loss=0.03730
Epoch 20/80: current_loss=0.03765 | best_loss=0.03730
Epoch 21/80: current_loss=0.03755 | best_loss=0.03730
Epoch 22/80: current_loss=0.03822 | best_loss=0.03730
Epoch 23/80: current_loss=0.03765 | best_loss=0.03730
Epoch 24/80: current_loss=0.03768 | best_loss=0.03730
Epoch 25/80: current_loss=0.03810 | best_loss=0.03730
Epoch 26/80: current_loss=0.03748 | best_loss=0.03730
Epoch 27/80: current_loss=0.03749 | best_loss=0.03730
Epoch 28/80: current_loss=0.03757 | best_loss=0.03730
Epoch 29/80: current_loss=0.03760 | best_loss=0.03730
Epoch 30/80: current_loss=0.03771 | best_loss=0.03730
Epoch 31/80: current_loss=0.04005 | best_loss=0.03730
Early Stopping at epoch 31
      explained_var=0.02411 | mse_loss=0.03825
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04168 | best_loss=0.04168
Epoch 1/80: current_loss=0.04178 | best_loss=0.04168
Epoch 2/80: current_loss=0.04426 | best_loss=0.04168
Epoch 3/80: current_loss=0.04331 | best_loss=0.04168
Epoch 4/80: current_loss=0.04225 | best_loss=0.04168
Epoch 5/80: current_loss=0.04201 | best_loss=0.04168
Epoch 6/80: current_loss=0.04261 | best_loss=0.04168
Epoch 7/80: current_loss=0.04210 | best_loss=0.04168
Epoch 8/80: current_loss=0.04236 | best_loss=0.04168
Epoch 9/80: current_loss=0.04178 | best_loss=0.04168
Epoch 10/80: current_loss=0.04227 | best_loss=0.04168
Epoch 11/80: current_loss=0.04198 | best_loss=0.04168
Epoch 12/80: current_loss=0.04175 | best_loss=0.04168
Epoch 13/80: current_loss=0.04474 | best_loss=0.04168
Epoch 14/80: current_loss=0.04198 | best_loss=0.04168
Epoch 15/80: current_loss=0.04168 | best_loss=0.04168
Epoch 16/80: current_loss=0.04226 | best_loss=0.04168
Epoch 17/80: current_loss=0.04179 | best_loss=0.04168
Epoch 18/80: current_loss=0.04184 | best_loss=0.04168
Epoch 19/80: current_loss=0.04340 | best_loss=0.04168
Epoch 20/80: current_loss=0.04202 | best_loss=0.04168
Epoch 21/80: current_loss=0.04275 | best_loss=0.04168
Epoch 22/80: current_loss=0.04339 | best_loss=0.04168
Epoch 23/80: current_loss=0.04197 | best_loss=0.04168
Epoch 24/80: current_loss=0.04183 | best_loss=0.04168
Epoch 25/80: current_loss=0.04331 | best_loss=0.04168
Epoch 26/80: current_loss=0.04193 | best_loss=0.04168
Epoch 27/80: current_loss=0.04205 | best_loss=0.04168
Epoch 28/80: current_loss=0.04235 | best_loss=0.04168
Epoch 29/80: current_loss=0.04347 | best_loss=0.04168
Epoch 30/80: current_loss=0.04199 | best_loss=0.04168
Epoch 31/80: current_loss=0.04216 | best_loss=0.04168
Epoch 32/80: current_loss=0.04412 | best_loss=0.04168
Epoch 33/80: current_loss=0.04319 | best_loss=0.04168
Epoch 34/80: current_loss=0.04208 | best_loss=0.04168
Epoch 35/80: current_loss=0.04171 | best_loss=0.04168
Early Stopping at epoch 35
      explained_var=0.02464 | mse_loss=0.04022
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02907 | best_loss=0.02907
Epoch 1/80: current_loss=0.02801 | best_loss=0.02801
Epoch 2/80: current_loss=0.02876 | best_loss=0.02801
Epoch 3/80: current_loss=0.02833 | best_loss=0.02801
Epoch 4/80: current_loss=0.02938 | best_loss=0.02801
Epoch 5/80: current_loss=0.02792 | best_loss=0.02792
Epoch 6/80: current_loss=0.02813 | best_loss=0.02792
Epoch 7/80: current_loss=0.02798 | best_loss=0.02792
Epoch 8/80: current_loss=0.02836 | best_loss=0.02792
Epoch 9/80: current_loss=0.02793 | best_loss=0.02792
Epoch 10/80: current_loss=0.02826 | best_loss=0.02792
Epoch 11/80: current_loss=0.03012 | best_loss=0.02792
Epoch 12/80: current_loss=0.02817 | best_loss=0.02792
Epoch 13/80: current_loss=0.02815 | best_loss=0.02792
Epoch 14/80: current_loss=0.02797 | best_loss=0.02792
Epoch 15/80: current_loss=0.02823 | best_loss=0.02792
Epoch 16/80: current_loss=0.02910 | best_loss=0.02792
Epoch 17/80: current_loss=0.02963 | best_loss=0.02792
Epoch 18/80: current_loss=0.02797 | best_loss=0.02792
Epoch 19/80: current_loss=0.02828 | best_loss=0.02792
Epoch 20/80: current_loss=0.02793 | best_loss=0.02792
Epoch 21/80: current_loss=0.02815 | best_loss=0.02792
Epoch 22/80: current_loss=0.02807 | best_loss=0.02792
Epoch 23/80: current_loss=0.03132 | best_loss=0.02792
Epoch 24/80: current_loss=0.02812 | best_loss=0.02792
Epoch 25/80: current_loss=0.02803 | best_loss=0.02792
Early Stopping at epoch 25
      explained_var=0.00010 | mse_loss=0.02836
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03281 | best_loss=0.03281
Epoch 1/80: current_loss=0.03285 | best_loss=0.03281
Epoch 2/80: current_loss=0.03352 | best_loss=0.03281
Epoch 3/80: current_loss=0.03289 | best_loss=0.03281
Epoch 4/80: current_loss=0.03295 | best_loss=0.03281
Epoch 5/80: current_loss=0.03292 | best_loss=0.03281
Epoch 6/80: current_loss=0.03292 | best_loss=0.03281
Epoch 7/80: current_loss=0.03373 | best_loss=0.03281
Epoch 8/80: current_loss=0.03407 | best_loss=0.03281
Epoch 9/80: current_loss=0.03311 | best_loss=0.03281
Epoch 10/80: current_loss=0.03308 | best_loss=0.03281
Epoch 11/80: current_loss=0.03442 | best_loss=0.03281
Epoch 12/80: current_loss=0.03336 | best_loss=0.03281
Epoch 13/80: current_loss=0.03302 | best_loss=0.03281
Epoch 14/80: current_loss=0.03301 | best_loss=0.03281
Epoch 15/80: current_loss=0.03282 | best_loss=0.03281
Epoch 16/80: current_loss=0.03325 | best_loss=0.03281
Epoch 17/80: current_loss=0.03386 | best_loss=0.03281
Epoch 18/80: current_loss=0.03320 | best_loss=0.03281
Epoch 19/80: current_loss=0.03282 | best_loss=0.03281
Epoch 20/80: current_loss=0.03331 | best_loss=0.03281
Early Stopping at epoch 20
      explained_var=-0.00248 | mse_loss=0.03272
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03574 | best_loss=0.03574
Epoch 1/80: current_loss=0.03567 | best_loss=0.03567
Epoch 2/80: current_loss=0.03551 | best_loss=0.03551
Epoch 3/80: current_loss=0.03570 | best_loss=0.03551
Epoch 4/80: current_loss=0.03567 | best_loss=0.03551
Epoch 5/80: current_loss=0.03563 | best_loss=0.03551
Epoch 6/80: current_loss=0.03620 | best_loss=0.03551
Epoch 7/80: current_loss=0.03550 | best_loss=0.03550
Epoch 8/80: current_loss=0.03603 | best_loss=0.03550
Epoch 9/80: current_loss=0.03563 | best_loss=0.03550
Epoch 10/80: current_loss=0.03577 | best_loss=0.03550
Epoch 11/80: current_loss=0.03557 | best_loss=0.03550
Epoch 12/80: current_loss=0.03576 | best_loss=0.03550
Epoch 13/80: current_loss=0.03587 | best_loss=0.03550
Epoch 14/80: current_loss=0.03560 | best_loss=0.03550
Epoch 15/80: current_loss=0.03561 | best_loss=0.03550
Epoch 16/80: current_loss=0.03552 | best_loss=0.03550
Epoch 17/80: current_loss=0.03564 | best_loss=0.03550
Epoch 18/80: current_loss=0.03554 | best_loss=0.03550
Epoch 19/80: current_loss=0.03553 | best_loss=0.03550
Epoch 20/80: current_loss=0.03560 | best_loss=0.03550
Epoch 21/80: current_loss=0.03606 | best_loss=0.03550
Epoch 22/80: current_loss=0.03569 | best_loss=0.03550
Epoch 23/80: current_loss=0.03576 | best_loss=0.03550
Epoch 24/80: current_loss=0.03558 | best_loss=0.03550
Epoch 25/80: current_loss=0.03592 | best_loss=0.03550
Epoch 26/80: current_loss=0.03555 | best_loss=0.03550
Epoch 27/80: current_loss=0.03558 | best_loss=0.03550
Early Stopping at epoch 27
      explained_var=0.01651 | mse_loss=0.03531
----------------------------------------------
Average early_stopping_point: 7| avg_exp_var=0.01258| avg_loss=0.03497
----------------------------------------------


----------------------------------------------
Params for Trial 86
{'learning_rate': 0.0001, 'weight_decay': 0.006857820830998089, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.11829 | best_loss=0.11829
Epoch 1/80: current_loss=0.04828 | best_loss=0.04828
Epoch 2/80: current_loss=0.04305 | best_loss=0.04305
Epoch 3/80: current_loss=0.04288 | best_loss=0.04288
Epoch 4/80: current_loss=0.04265 | best_loss=0.04265
Epoch 5/80: current_loss=0.04200 | best_loss=0.04200
Epoch 6/80: current_loss=0.04179 | best_loss=0.04179
Epoch 7/80: current_loss=0.04133 | best_loss=0.04133
Epoch 8/80: current_loss=0.04094 | best_loss=0.04094
Epoch 9/80: current_loss=0.04093 | best_loss=0.04093
Epoch 10/80: current_loss=0.04040 | best_loss=0.04040
Epoch 11/80: current_loss=0.04022 | best_loss=0.04022
Epoch 12/80: current_loss=0.04004 | best_loss=0.04004
Epoch 13/80: current_loss=0.04001 | best_loss=0.04001
Epoch 14/80: current_loss=0.03969 | best_loss=0.03969
Epoch 15/80: current_loss=0.03959 | best_loss=0.03959
Epoch 16/80: current_loss=0.03952 | best_loss=0.03952
Epoch 17/80: current_loss=0.03960 | best_loss=0.03952
Epoch 18/80: current_loss=0.03932 | best_loss=0.03932
Epoch 19/80: current_loss=0.03915 | best_loss=0.03915
Epoch 20/80: current_loss=0.03917 | best_loss=0.03915
Epoch 21/80: current_loss=0.03907 | best_loss=0.03907
Epoch 22/80: current_loss=0.03912 | best_loss=0.03907
Epoch 23/80: current_loss=0.03895 | best_loss=0.03895
Epoch 24/80: current_loss=0.03902 | best_loss=0.03895
Epoch 25/80: current_loss=0.03872 | best_loss=0.03872
Epoch 26/80: current_loss=0.03887 | best_loss=0.03872
Epoch 27/80: current_loss=0.03865 | best_loss=0.03865
Epoch 28/80: current_loss=0.03853 | best_loss=0.03853
Epoch 29/80: current_loss=0.03862 | best_loss=0.03853
Epoch 30/80: current_loss=0.03888 | best_loss=0.03853
Epoch 31/80: current_loss=0.03853 | best_loss=0.03853
Epoch 32/80: current_loss=0.03850 | best_loss=0.03850
Epoch 33/80: current_loss=0.03850 | best_loss=0.03850
Epoch 34/80: current_loss=0.03853 | best_loss=0.03850
Epoch 35/80: current_loss=0.03847 | best_loss=0.03847
Epoch 36/80: current_loss=0.03835 | best_loss=0.03835
Epoch 37/80: current_loss=0.03855 | best_loss=0.03835
Epoch 38/80: current_loss=0.03841 | best_loss=0.03835
Epoch 39/80: current_loss=0.03847 | best_loss=0.03835
Epoch 40/80: current_loss=0.03838 | best_loss=0.03835
Epoch 41/80: current_loss=0.03813 | best_loss=0.03813
Epoch 42/80: current_loss=0.03829 | best_loss=0.03813
Epoch 43/80: current_loss=0.03825 | best_loss=0.03813
Epoch 44/80: current_loss=0.03834 | best_loss=0.03813
Epoch 45/80: current_loss=0.03821 | best_loss=0.03813
Epoch 46/80: current_loss=0.03828 | best_loss=0.03813
Epoch 47/80: current_loss=0.03838 | best_loss=0.03813
Epoch 48/80: current_loss=0.03817 | best_loss=0.03813
Epoch 49/80: current_loss=0.03825 | best_loss=0.03813
Epoch 50/80: current_loss=0.03824 | best_loss=0.03813
Epoch 51/80: current_loss=0.03829 | best_loss=0.03813
Epoch 52/80: current_loss=0.03819 | best_loss=0.03813
Epoch 53/80: current_loss=0.03819 | best_loss=0.03813
Epoch 54/80: current_loss=0.03821 | best_loss=0.03813
Epoch 55/80: current_loss=0.03808 | best_loss=0.03808
Epoch 56/80: current_loss=0.03813 | best_loss=0.03808
Epoch 57/80: current_loss=0.03815 | best_loss=0.03808
Epoch 58/80: current_loss=0.03801 | best_loss=0.03801
Epoch 59/80: current_loss=0.03812 | best_loss=0.03801
Epoch 60/80: current_loss=0.03805 | best_loss=0.03801
Epoch 61/80: current_loss=0.03805 | best_loss=0.03801
Epoch 62/80: current_loss=0.03803 | best_loss=0.03801
Epoch 63/80: current_loss=0.03802 | best_loss=0.03801
Epoch 64/80: current_loss=0.03808 | best_loss=0.03801
Epoch 65/80: current_loss=0.03801 | best_loss=0.03801
Epoch 66/80: current_loss=0.03829 | best_loss=0.03801
Epoch 67/80: current_loss=0.03814 | best_loss=0.03801
Epoch 68/80: current_loss=0.03809 | best_loss=0.03801
Epoch 69/80: current_loss=0.03793 | best_loss=0.03793
Epoch 70/80: current_loss=0.03812 | best_loss=0.03793
Epoch 71/80: current_loss=0.03807 | best_loss=0.03793
Epoch 72/80: current_loss=0.03798 | best_loss=0.03793
Epoch 73/80: current_loss=0.03790 | best_loss=0.03790
Epoch 74/80: current_loss=0.03800 | best_loss=0.03790
Epoch 75/80: current_loss=0.03802 | best_loss=0.03790
Epoch 76/80: current_loss=0.03791 | best_loss=0.03790
Epoch 77/80: current_loss=0.03798 | best_loss=0.03790
Epoch 78/80: current_loss=0.03794 | best_loss=0.03790
Epoch 79/80: current_loss=0.03813 | best_loss=0.03790
      explained_var=0.00997 | mse_loss=0.03888

----------------------------------------------
Params for Trial 87
{'learning_rate': 0.1, 'weight_decay': 0.0008236085157617861, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.25}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.39616 | best_loss=0.39616
Epoch 1/80: current_loss=0.24594 | best_loss=0.24594
Epoch 2/80: current_loss=0.06230 | best_loss=0.06230
Epoch 3/80: current_loss=0.05791 | best_loss=0.05791
Epoch 4/80: current_loss=0.04078 | best_loss=0.04078
Epoch 5/80: current_loss=0.05149 | best_loss=0.04078
Epoch 6/80: current_loss=0.04480 | best_loss=0.04078
Epoch 7/80: current_loss=0.05679 | best_loss=0.04078
Epoch 8/80: current_loss=0.08551 | best_loss=0.04078
Epoch 9/80: current_loss=0.04326 | best_loss=0.04078
Epoch 10/80: current_loss=0.06197 | best_loss=0.04078
Epoch 11/80: current_loss=0.05057 | best_loss=0.04078
Epoch 12/80: current_loss=0.04309 | best_loss=0.04078
Epoch 13/80: current_loss=0.04929 | best_loss=0.04078
Epoch 14/80: current_loss=0.04743 | best_loss=0.04078
Epoch 15/80: current_loss=0.03985 | best_loss=0.03985
Epoch 16/80: current_loss=0.05130 | best_loss=0.03985
Epoch 17/80: current_loss=0.04979 | best_loss=0.03985
Epoch 18/80: current_loss=0.06679 | best_loss=0.03985
Epoch 19/80: current_loss=0.04151 | best_loss=0.03985
Epoch 20/80: current_loss=0.06607 | best_loss=0.03985
Epoch 21/80: current_loss=0.07919 | best_loss=0.03985
Epoch 22/80: current_loss=0.47013 | best_loss=0.03985
Epoch 23/80: current_loss=0.05905 | best_loss=0.03985
Epoch 24/80: current_loss=0.05984 | best_loss=0.03985
Epoch 25/80: current_loss=0.03864 | best_loss=0.03864
Epoch 26/80: current_loss=0.22955 | best_loss=0.03864
Epoch 27/80: current_loss=0.22245 | best_loss=0.03864
Epoch 28/80: current_loss=0.09925 | best_loss=0.03864
Epoch 29/80: current_loss=0.13553 | best_loss=0.03864
Epoch 30/80: current_loss=0.22002 | best_loss=0.03864
Epoch 31/80: current_loss=0.04751 | best_loss=0.03864
Epoch 32/80: current_loss=0.06110 | best_loss=0.03864
Epoch 33/80: current_loss=0.05438 | best_loss=0.03864
Epoch 34/80: current_loss=0.04352 | best_loss=0.03864
Epoch 35/80: current_loss=0.07855 | best_loss=0.03864
Epoch 36/80: current_loss=0.04655 | best_loss=0.03864
Epoch 37/80: current_loss=0.05079 | best_loss=0.03864
Epoch 38/80: current_loss=0.08574 | best_loss=0.03864
Epoch 39/80: current_loss=0.04340 | best_loss=0.03864
Epoch 40/80: current_loss=0.04008 | best_loss=0.03864
Epoch 41/80: current_loss=0.03895 | best_loss=0.03864
Epoch 42/80: current_loss=0.03983 | best_loss=0.03864
Epoch 43/80: current_loss=0.04899 | best_loss=0.03864
Epoch 44/80: current_loss=0.24426 | best_loss=0.03864
Epoch 45/80: current_loss=0.05483 | best_loss=0.03864
Early Stopping at epoch 45
      explained_var=-0.00029 | mse_loss=0.03951

----------------------------------------------
Params for Trial 88
{'learning_rate': 0.001, 'weight_decay': 0.00013269083884708238, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.5}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04150 | best_loss=0.04150
Epoch 1/80: current_loss=0.04820 | best_loss=0.04150
Epoch 2/80: current_loss=0.03883 | best_loss=0.03883
Epoch 3/80: current_loss=0.04033 | best_loss=0.03883
Epoch 4/80: current_loss=0.03849 | best_loss=0.03849
Epoch 5/80: current_loss=0.03863 | best_loss=0.03849
Epoch 6/80: current_loss=0.03872 | best_loss=0.03849
Epoch 7/80: current_loss=0.03844 | best_loss=0.03844
Epoch 8/80: current_loss=0.03932 | best_loss=0.03844
Epoch 9/80: current_loss=0.03880 | best_loss=0.03844
Epoch 10/80: current_loss=0.03754 | best_loss=0.03754
Epoch 11/80: current_loss=0.03796 | best_loss=0.03754
Epoch 12/80: current_loss=0.03820 | best_loss=0.03754
Epoch 13/80: current_loss=0.03832 | best_loss=0.03754
Epoch 14/80: current_loss=0.04355 | best_loss=0.03754
Epoch 15/80: current_loss=0.03774 | best_loss=0.03754
Epoch 16/80: current_loss=0.03701 | best_loss=0.03701
Epoch 17/80: current_loss=0.03782 | best_loss=0.03701
Epoch 18/80: current_loss=0.03830 | best_loss=0.03701
Epoch 19/80: current_loss=0.03833 | best_loss=0.03701
Epoch 20/80: current_loss=0.03711 | best_loss=0.03701
Epoch 21/80: current_loss=0.03999 | best_loss=0.03701
Epoch 22/80: current_loss=0.03863 | best_loss=0.03701
Epoch 23/80: current_loss=0.04287 | best_loss=0.03701
Epoch 24/80: current_loss=0.03815 | best_loss=0.03701
Epoch 25/80: current_loss=0.03787 | best_loss=0.03701
Epoch 26/80: current_loss=0.03878 | best_loss=0.03701
Epoch 27/80: current_loss=0.04059 | best_loss=0.03701
Epoch 28/80: current_loss=0.03807 | best_loss=0.03701
Epoch 29/80: current_loss=0.03841 | best_loss=0.03701
Epoch 30/80: current_loss=0.03727 | best_loss=0.03701
Epoch 31/80: current_loss=0.03900 | best_loss=0.03701
Epoch 32/80: current_loss=0.03866 | best_loss=0.03701
Epoch 33/80: current_loss=0.03844 | best_loss=0.03701
Epoch 34/80: current_loss=0.03737 | best_loss=0.03701
Epoch 35/80: current_loss=0.03815 | best_loss=0.03701
Epoch 36/80: current_loss=0.03823 | best_loss=0.03701
Early Stopping at epoch 36
      explained_var=0.03213 | mse_loss=0.03785
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04163 | best_loss=0.04163
Epoch 1/80: current_loss=0.04289 | best_loss=0.04163
Epoch 2/80: current_loss=0.04158 | best_loss=0.04158
Epoch 3/80: current_loss=0.04195 | best_loss=0.04158
Epoch 4/80: current_loss=0.04262 | best_loss=0.04158
Epoch 5/80: current_loss=0.04427 | best_loss=0.04158
Epoch 6/80: current_loss=0.04562 | best_loss=0.04158
Epoch 7/80: current_loss=0.04366 | best_loss=0.04158
Epoch 8/80: current_loss=0.05829 | best_loss=0.04158
Epoch 9/80: current_loss=0.05022 | best_loss=0.04158
Epoch 10/80: current_loss=0.04731 | best_loss=0.04158
Epoch 11/80: current_loss=0.04344 | best_loss=0.04158
Epoch 12/80: current_loss=0.05445 | best_loss=0.04158
Epoch 13/80: current_loss=0.04588 | best_loss=0.04158
Epoch 14/80: current_loss=0.04355 | best_loss=0.04158
Epoch 15/80: current_loss=0.04528 | best_loss=0.04158
Epoch 16/80: current_loss=0.05066 | best_loss=0.04158
Epoch 17/80: current_loss=0.04708 | best_loss=0.04158
Epoch 18/80: current_loss=0.04660 | best_loss=0.04158
Epoch 19/80: current_loss=0.04426 | best_loss=0.04158
Epoch 20/80: current_loss=0.04453 | best_loss=0.04158
Epoch 21/80: current_loss=0.04425 | best_loss=0.04158
Epoch 22/80: current_loss=0.04406 | best_loss=0.04158
Early Stopping at epoch 22
      explained_var=0.02654 | mse_loss=0.04020
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02983 | best_loss=0.02983
Epoch 1/80: current_loss=0.02939 | best_loss=0.02939
Epoch 2/80: current_loss=0.03817 | best_loss=0.02939
Epoch 3/80: current_loss=0.03404 | best_loss=0.02939
Epoch 4/80: current_loss=0.02832 | best_loss=0.02832
Epoch 5/80: current_loss=0.03347 | best_loss=0.02832
Epoch 6/80: current_loss=0.02878 | best_loss=0.02832
Epoch 7/80: current_loss=0.03075 | best_loss=0.02832
Epoch 8/80: current_loss=0.03429 | best_loss=0.02832
Epoch 9/80: current_loss=0.03079 | best_loss=0.02832
Epoch 10/80: current_loss=0.03133 | best_loss=0.02832
Epoch 11/80: current_loss=0.02982 | best_loss=0.02832
Epoch 12/80: current_loss=0.03460 | best_loss=0.02832
Epoch 13/80: current_loss=0.02919 | best_loss=0.02832
Epoch 14/80: current_loss=0.04681 | best_loss=0.02832
Epoch 15/80: current_loss=0.02890 | best_loss=0.02832
Epoch 16/80: current_loss=0.03431 | best_loss=0.02832
Epoch 17/80: current_loss=0.03311 | best_loss=0.02832
Epoch 18/80: current_loss=0.05411 | best_loss=0.02832
Epoch 19/80: current_loss=0.03096 | best_loss=0.02832
Epoch 20/80: current_loss=0.03830 | best_loss=0.02832
Epoch 21/80: current_loss=0.03358 | best_loss=0.02832
Epoch 22/80: current_loss=0.03360 | best_loss=0.02832
Epoch 23/80: current_loss=0.02870 | best_loss=0.02832
Epoch 24/80: current_loss=0.06140 | best_loss=0.02832
Early Stopping at epoch 24
      explained_var=-0.00656 | mse_loss=0.02873

----------------------------------------------
Params for Trial 89
{'learning_rate': 0.0001, 'weight_decay': 0.0012202579715105796, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04803 | best_loss=0.04803
Epoch 1/80: current_loss=0.04405 | best_loss=0.04405
Epoch 2/80: current_loss=0.04317 | best_loss=0.04317
Epoch 3/80: current_loss=0.04247 | best_loss=0.04247
Epoch 4/80: current_loss=0.04195 | best_loss=0.04195
Epoch 5/80: current_loss=0.04129 | best_loss=0.04129
Epoch 6/80: current_loss=0.04113 | best_loss=0.04113
Epoch 7/80: current_loss=0.04046 | best_loss=0.04046
Epoch 8/80: current_loss=0.04069 | best_loss=0.04046
Epoch 9/80: current_loss=0.03961 | best_loss=0.03961
Epoch 10/80: current_loss=0.03969 | best_loss=0.03961
Epoch 11/80: current_loss=0.03991 | best_loss=0.03961
Epoch 12/80: current_loss=0.03888 | best_loss=0.03888
Epoch 13/80: current_loss=0.03890 | best_loss=0.03888
Epoch 14/80: current_loss=0.03884 | best_loss=0.03884
Epoch 15/80: current_loss=0.03876 | best_loss=0.03876
Epoch 16/80: current_loss=0.03843 | best_loss=0.03843
Epoch 17/80: current_loss=0.03830 | best_loss=0.03830
Epoch 18/80: current_loss=0.03851 | best_loss=0.03830
Epoch 19/80: current_loss=0.03812 | best_loss=0.03812
Epoch 20/80: current_loss=0.03831 | best_loss=0.03812
Epoch 21/80: current_loss=0.03821 | best_loss=0.03812
Epoch 22/80: current_loss=0.03820 | best_loss=0.03812
Epoch 23/80: current_loss=0.03826 | best_loss=0.03812
Epoch 24/80: current_loss=0.03816 | best_loss=0.03812
Epoch 25/80: current_loss=0.03803 | best_loss=0.03803
Epoch 26/80: current_loss=0.03816 | best_loss=0.03803
Epoch 27/80: current_loss=0.03793 | best_loss=0.03793
Epoch 28/80: current_loss=0.03820 | best_loss=0.03793
Epoch 29/80: current_loss=0.03807 | best_loss=0.03793
Epoch 30/80: current_loss=0.03777 | best_loss=0.03777
Epoch 31/80: current_loss=0.03769 | best_loss=0.03769
Epoch 32/80: current_loss=0.03775 | best_loss=0.03769
Epoch 33/80: current_loss=0.03768 | best_loss=0.03768
Epoch 34/80: current_loss=0.03777 | best_loss=0.03768
Epoch 35/80: current_loss=0.03766 | best_loss=0.03766
Epoch 36/80: current_loss=0.03771 | best_loss=0.03766
Epoch 37/80: current_loss=0.03791 | best_loss=0.03766
Epoch 38/80: current_loss=0.03760 | best_loss=0.03760
Epoch 39/80: current_loss=0.03791 | best_loss=0.03760
Epoch 40/80: current_loss=0.03767 | best_loss=0.03760
Epoch 41/80: current_loss=0.03764 | best_loss=0.03760
Epoch 42/80: current_loss=0.03789 | best_loss=0.03760
Epoch 43/80: current_loss=0.03769 | best_loss=0.03760
Epoch 44/80: current_loss=0.03775 | best_loss=0.03760
Epoch 45/80: current_loss=0.03768 | best_loss=0.03760
Epoch 46/80: current_loss=0.03788 | best_loss=0.03760
Epoch 47/80: current_loss=0.03764 | best_loss=0.03760
Epoch 48/80: current_loss=0.03793 | best_loss=0.03760
Epoch 49/80: current_loss=0.03778 | best_loss=0.03760
Epoch 50/80: current_loss=0.03767 | best_loss=0.03760
Epoch 51/80: current_loss=0.03777 | best_loss=0.03760
Epoch 52/80: current_loss=0.03778 | best_loss=0.03760
Epoch 53/80: current_loss=0.03777 | best_loss=0.03760
Epoch 54/80: current_loss=0.03771 | best_loss=0.03760
Epoch 55/80: current_loss=0.03788 | best_loss=0.03760
Epoch 56/80: current_loss=0.03774 | best_loss=0.03760
Epoch 57/80: current_loss=0.03792 | best_loss=0.03760
Epoch 58/80: current_loss=0.03753 | best_loss=0.03753
Epoch 59/80: current_loss=0.03768 | best_loss=0.03753
Epoch 60/80: current_loss=0.03743 | best_loss=0.03743
Epoch 61/80: current_loss=0.03739 | best_loss=0.03739
Epoch 62/80: current_loss=0.03744 | best_loss=0.03739
Epoch 63/80: current_loss=0.03750 | best_loss=0.03739
Epoch 64/80: current_loss=0.03772 | best_loss=0.03739
Epoch 65/80: current_loss=0.03758 | best_loss=0.03739
Epoch 66/80: current_loss=0.03764 | best_loss=0.03739
Epoch 67/80: current_loss=0.03756 | best_loss=0.03739
Epoch 68/80: current_loss=0.03748 | best_loss=0.03739
Epoch 69/80: current_loss=0.03746 | best_loss=0.03739
Epoch 70/80: current_loss=0.03760 | best_loss=0.03739
Epoch 71/80: current_loss=0.03755 | best_loss=0.03739
Epoch 72/80: current_loss=0.03752 | best_loss=0.03739
Epoch 73/80: current_loss=0.03768 | best_loss=0.03739
Epoch 74/80: current_loss=0.03754 | best_loss=0.03739
Epoch 75/80: current_loss=0.03757 | best_loss=0.03739
Epoch 76/80: current_loss=0.03760 | best_loss=0.03739
Epoch 77/80: current_loss=0.03768 | best_loss=0.03739
Epoch 78/80: current_loss=0.03768 | best_loss=0.03739
Epoch 79/80: current_loss=0.03770 | best_loss=0.03739
      explained_var=0.02169 | mse_loss=0.03831
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04183 | best_loss=0.04183
Epoch 1/80: current_loss=0.04114 | best_loss=0.04114
Epoch 2/80: current_loss=0.04151 | best_loss=0.04114
Epoch 3/80: current_loss=0.04159 | best_loss=0.04114
Epoch 4/80: current_loss=0.04148 | best_loss=0.04114
Epoch 5/80: current_loss=0.04190 | best_loss=0.04114
Epoch 6/80: current_loss=0.04139 | best_loss=0.04114
Epoch 7/80: current_loss=0.04163 | best_loss=0.04114
Epoch 8/80: current_loss=0.04121 | best_loss=0.04114
Epoch 9/80: current_loss=0.04145 | best_loss=0.04114
Epoch 10/80: current_loss=0.04173 | best_loss=0.04114
Epoch 11/80: current_loss=0.04138 | best_loss=0.04114
Epoch 12/80: current_loss=0.04141 | best_loss=0.04114
Epoch 13/80: current_loss=0.04132 | best_loss=0.04114
Epoch 14/80: current_loss=0.04133 | best_loss=0.04114
Epoch 15/80: current_loss=0.04127 | best_loss=0.04114
Epoch 16/80: current_loss=0.04155 | best_loss=0.04114
Epoch 17/80: current_loss=0.04151 | best_loss=0.04114
Epoch 18/80: current_loss=0.04139 | best_loss=0.04114
Epoch 19/80: current_loss=0.04233 | best_loss=0.04114
Epoch 20/80: current_loss=0.04140 | best_loss=0.04114
Epoch 21/80: current_loss=0.04152 | best_loss=0.04114
Early Stopping at epoch 21
      explained_var=0.03885 | mse_loss=0.03963
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02805 | best_loss=0.02805
Epoch 1/80: current_loss=0.02834 | best_loss=0.02805
Epoch 2/80: current_loss=0.02817 | best_loss=0.02805
Epoch 3/80: current_loss=0.02862 | best_loss=0.02805
Epoch 4/80: current_loss=0.02855 | best_loss=0.02805
Epoch 5/80: current_loss=0.02826 | best_loss=0.02805
Epoch 6/80: current_loss=0.02838 | best_loss=0.02805
Epoch 7/80: current_loss=0.02815 | best_loss=0.02805
Epoch 8/80: current_loss=0.02829 | best_loss=0.02805
Epoch 9/80: current_loss=0.02855 | best_loss=0.02805
Epoch 10/80: current_loss=0.02833 | best_loss=0.02805
Epoch 11/80: current_loss=0.02832 | best_loss=0.02805
Epoch 12/80: current_loss=0.02905 | best_loss=0.02805
Epoch 13/80: current_loss=0.02841 | best_loss=0.02805
Epoch 14/80: current_loss=0.02853 | best_loss=0.02805
Epoch 15/80: current_loss=0.02872 | best_loss=0.02805
Epoch 16/80: current_loss=0.02812 | best_loss=0.02805
Epoch 17/80: current_loss=0.02856 | best_loss=0.02805
Epoch 18/80: current_loss=0.02811 | best_loss=0.02805
Epoch 19/80: current_loss=0.02836 | best_loss=0.02805
Epoch 20/80: current_loss=0.02853 | best_loss=0.02805
Early Stopping at epoch 20
      explained_var=-0.00493 | mse_loss=0.02851

----------------------------------------------
Params for Trial 90
{'learning_rate': 0.001, 'weight_decay': 0.0014459985711409511, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.4}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04075 | best_loss=0.04075
Epoch 1/80: current_loss=0.03877 | best_loss=0.03877
Epoch 2/80: current_loss=0.03946 | best_loss=0.03877
Epoch 3/80: current_loss=0.03772 | best_loss=0.03772
Epoch 4/80: current_loss=0.03785 | best_loss=0.03772
Epoch 5/80: current_loss=0.03858 | best_loss=0.03772
Epoch 6/80: current_loss=0.03883 | best_loss=0.03772
Epoch 7/80: current_loss=0.03782 | best_loss=0.03772
Epoch 8/80: current_loss=0.03779 | best_loss=0.03772
Epoch 9/80: current_loss=0.03778 | best_loss=0.03772
Epoch 10/80: current_loss=0.03931 | best_loss=0.03772
Epoch 11/80: current_loss=0.03826 | best_loss=0.03772
Epoch 12/80: current_loss=0.03742 | best_loss=0.03742
Epoch 13/80: current_loss=0.03875 | best_loss=0.03742
Epoch 14/80: current_loss=0.03750 | best_loss=0.03742
Epoch 15/80: current_loss=0.03862 | best_loss=0.03742
Epoch 16/80: current_loss=0.03770 | best_loss=0.03742
Epoch 17/80: current_loss=0.03770 | best_loss=0.03742
Epoch 18/80: current_loss=0.03802 | best_loss=0.03742
Epoch 19/80: current_loss=0.03819 | best_loss=0.03742
Epoch 20/80: current_loss=0.03785 | best_loss=0.03742
Epoch 21/80: current_loss=0.03783 | best_loss=0.03742
Epoch 22/80: current_loss=0.03793 | best_loss=0.03742
Epoch 23/80: current_loss=0.03821 | best_loss=0.03742
Epoch 24/80: current_loss=0.03750 | best_loss=0.03742
Epoch 25/80: current_loss=0.03772 | best_loss=0.03742
Epoch 26/80: current_loss=0.03872 | best_loss=0.03742
Epoch 27/80: current_loss=0.03852 | best_loss=0.03742
Epoch 28/80: current_loss=0.03788 | best_loss=0.03742
Epoch 29/80: current_loss=0.03863 | best_loss=0.03742
Epoch 30/80: current_loss=0.03783 | best_loss=0.03742
Epoch 31/80: current_loss=0.03752 | best_loss=0.03742
Epoch 32/80: current_loss=0.03784 | best_loss=0.03742
Early Stopping at epoch 32
      explained_var=0.02036 | mse_loss=0.03831
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04388 | best_loss=0.04388
Epoch 1/80: current_loss=0.04209 | best_loss=0.04209
Epoch 2/80: current_loss=0.04415 | best_loss=0.04209
Epoch 3/80: current_loss=0.04182 | best_loss=0.04182
Epoch 4/80: current_loss=0.04151 | best_loss=0.04151
Epoch 5/80: current_loss=0.04232 | best_loss=0.04151
Epoch 6/80: current_loss=0.04740 | best_loss=0.04151
Epoch 7/80: current_loss=0.04240 | best_loss=0.04151
Epoch 8/80: current_loss=0.04168 | best_loss=0.04151
Epoch 9/80: current_loss=0.04161 | best_loss=0.04151
Epoch 10/80: current_loss=0.04271 | best_loss=0.04151
Epoch 11/80: current_loss=0.04188 | best_loss=0.04151
Epoch 12/80: current_loss=0.04376 | best_loss=0.04151
Epoch 13/80: current_loss=0.04297 | best_loss=0.04151
Epoch 14/80: current_loss=0.04315 | best_loss=0.04151
Epoch 15/80: current_loss=0.04174 | best_loss=0.04151
Epoch 16/80: current_loss=0.04347 | best_loss=0.04151
Epoch 17/80: current_loss=0.04178 | best_loss=0.04151
Epoch 18/80: current_loss=0.04255 | best_loss=0.04151
Epoch 19/80: current_loss=0.04294 | best_loss=0.04151
Epoch 20/80: current_loss=0.04213 | best_loss=0.04151
Epoch 21/80: current_loss=0.04188 | best_loss=0.04151
Epoch 22/80: current_loss=0.04172 | best_loss=0.04151
Epoch 23/80: current_loss=0.04199 | best_loss=0.04151
Epoch 24/80: current_loss=0.04239 | best_loss=0.04151
Early Stopping at epoch 24
      explained_var=0.02913 | mse_loss=0.04003
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02887 | best_loss=0.02887
Epoch 1/80: current_loss=0.02799 | best_loss=0.02799
Epoch 2/80: current_loss=0.02831 | best_loss=0.02799
Epoch 3/80: current_loss=0.02852 | best_loss=0.02799
Epoch 4/80: current_loss=0.02849 | best_loss=0.02799
Epoch 5/80: current_loss=0.02810 | best_loss=0.02799
Epoch 6/80: current_loss=0.02832 | best_loss=0.02799
Epoch 7/80: current_loss=0.02876 | best_loss=0.02799
Epoch 8/80: current_loss=0.02846 | best_loss=0.02799
Epoch 9/80: current_loss=0.02817 | best_loss=0.02799
Epoch 10/80: current_loss=0.02978 | best_loss=0.02799
Epoch 11/80: current_loss=0.02877 | best_loss=0.02799
Epoch 12/80: current_loss=0.02822 | best_loss=0.02799
Epoch 13/80: current_loss=0.02953 | best_loss=0.02799
Epoch 14/80: current_loss=0.02806 | best_loss=0.02799
Epoch 15/80: current_loss=0.02797 | best_loss=0.02797
Epoch 16/80: current_loss=0.02801 | best_loss=0.02797
Epoch 17/80: current_loss=0.02799 | best_loss=0.02797
Epoch 18/80: current_loss=0.02883 | best_loss=0.02797
Epoch 19/80: current_loss=0.02871 | best_loss=0.02797
Epoch 20/80: current_loss=0.02810 | best_loss=0.02797
Epoch 21/80: current_loss=0.02812 | best_loss=0.02797
Epoch 22/80: current_loss=0.02875 | best_loss=0.02797
Epoch 23/80: current_loss=0.02837 | best_loss=0.02797
Epoch 24/80: current_loss=0.02795 | best_loss=0.02795
Epoch 25/80: current_loss=0.02808 | best_loss=0.02795
Epoch 26/80: current_loss=0.02837 | best_loss=0.02795
Epoch 27/80: current_loss=0.02794 | best_loss=0.02794
Epoch 28/80: current_loss=0.02955 | best_loss=0.02794
Epoch 29/80: current_loss=0.02821 | best_loss=0.02794
Epoch 30/80: current_loss=0.02796 | best_loss=0.02794
Epoch 31/80: current_loss=0.02841 | best_loss=0.02794
Epoch 32/80: current_loss=0.02870 | best_loss=0.02794
Epoch 33/80: current_loss=0.02812 | best_loss=0.02794
Epoch 34/80: current_loss=0.02827 | best_loss=0.02794
Epoch 35/80: current_loss=0.02946 | best_loss=0.02794
Epoch 36/80: current_loss=0.02799 | best_loss=0.02794
Epoch 37/80: current_loss=0.02851 | best_loss=0.02794
Epoch 38/80: current_loss=0.02890 | best_loss=0.02794
Epoch 39/80: current_loss=0.02793 | best_loss=0.02793
Epoch 40/80: current_loss=0.02800 | best_loss=0.02793
Epoch 41/80: current_loss=0.02853 | best_loss=0.02793
Epoch 42/80: current_loss=0.03001 | best_loss=0.02793
Epoch 43/80: current_loss=0.02797 | best_loss=0.02793
Epoch 44/80: current_loss=0.02871 | best_loss=0.02793
Epoch 45/80: current_loss=0.02803 | best_loss=0.02793
Epoch 46/80: current_loss=0.02807 | best_loss=0.02793
Epoch 47/80: current_loss=0.02796 | best_loss=0.02793
Epoch 48/80: current_loss=0.02973 | best_loss=0.02793
Epoch 49/80: current_loss=0.02807 | best_loss=0.02793
Epoch 50/80: current_loss=0.02841 | best_loss=0.02793
Epoch 51/80: current_loss=0.02849 | best_loss=0.02793
Epoch 52/80: current_loss=0.02833 | best_loss=0.02793
Epoch 53/80: current_loss=0.02799 | best_loss=0.02793
Epoch 54/80: current_loss=0.02820 | best_loss=0.02793
Epoch 55/80: current_loss=0.02804 | best_loss=0.02793
Epoch 56/80: current_loss=0.02801 | best_loss=0.02793
Epoch 57/80: current_loss=0.02826 | best_loss=0.02793
Epoch 58/80: current_loss=0.02818 | best_loss=0.02793
Epoch 59/80: current_loss=0.02886 | best_loss=0.02793
Early Stopping at epoch 59
      explained_var=-0.00036 | mse_loss=0.02837
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03325 | best_loss=0.03325
Epoch 1/80: current_loss=0.03293 | best_loss=0.03293
Epoch 2/80: current_loss=0.03286 | best_loss=0.03286
Epoch 3/80: current_loss=0.03321 | best_loss=0.03286
Epoch 4/80: current_loss=0.03304 | best_loss=0.03286
Epoch 5/80: current_loss=0.03343 | best_loss=0.03286
Epoch 6/80: current_loss=0.03279 | best_loss=0.03279
Epoch 7/80: current_loss=0.03293 | best_loss=0.03279
Epoch 8/80: current_loss=0.03298 | best_loss=0.03279
Epoch 9/80: current_loss=0.03305 | best_loss=0.03279
Epoch 10/80: current_loss=0.03297 | best_loss=0.03279
Epoch 11/80: current_loss=0.03300 | best_loss=0.03279
Epoch 12/80: current_loss=0.03284 | best_loss=0.03279
Epoch 13/80: current_loss=0.03316 | best_loss=0.03279
Epoch 14/80: current_loss=0.03284 | best_loss=0.03279
Epoch 15/80: current_loss=0.03289 | best_loss=0.03279
Epoch 16/80: current_loss=0.03309 | best_loss=0.03279
Epoch 17/80: current_loss=0.03319 | best_loss=0.03279
Epoch 18/80: current_loss=0.03289 | best_loss=0.03279
Epoch 19/80: current_loss=0.03312 | best_loss=0.03279
Epoch 20/80: current_loss=0.03280 | best_loss=0.03279
Epoch 21/80: current_loss=0.03284 | best_loss=0.03279
Epoch 22/80: current_loss=0.03317 | best_loss=0.03279
Epoch 23/80: current_loss=0.03319 | best_loss=0.03279
Epoch 24/80: current_loss=0.03289 | best_loss=0.03279
Epoch 25/80: current_loss=0.03285 | best_loss=0.03279
Epoch 26/80: current_loss=0.03295 | best_loss=0.03279
Early Stopping at epoch 26
      explained_var=-0.00152 | mse_loss=0.03267
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03552 | best_loss=0.03552
Epoch 1/80: current_loss=0.03550 | best_loss=0.03550
Epoch 2/80: current_loss=0.03558 | best_loss=0.03550
Epoch 3/80: current_loss=0.03564 | best_loss=0.03550
Epoch 4/80: current_loss=0.03575 | best_loss=0.03550
Epoch 5/80: current_loss=0.03573 | best_loss=0.03550
Epoch 6/80: current_loss=0.03569 | best_loss=0.03550
Epoch 7/80: current_loss=0.03549 | best_loss=0.03549
Epoch 8/80: current_loss=0.03559 | best_loss=0.03549
Epoch 9/80: current_loss=0.03565 | best_loss=0.03549
Epoch 10/80: current_loss=0.03548 | best_loss=0.03548
Epoch 11/80: current_loss=0.03552 | best_loss=0.03548
Epoch 12/80: current_loss=0.03564 | best_loss=0.03548
Epoch 13/80: current_loss=0.03564 | best_loss=0.03548
Epoch 14/80: current_loss=0.03571 | best_loss=0.03548
Epoch 15/80: current_loss=0.03565 | best_loss=0.03548
Epoch 16/80: current_loss=0.03555 | best_loss=0.03548
Epoch 17/80: current_loss=0.03552 | best_loss=0.03548
Epoch 18/80: current_loss=0.03553 | best_loss=0.03548
Epoch 19/80: current_loss=0.03549 | best_loss=0.03548
Epoch 20/80: current_loss=0.03558 | best_loss=0.03548
Epoch 21/80: current_loss=0.03544 | best_loss=0.03544
Epoch 22/80: current_loss=0.03552 | best_loss=0.03544
Epoch 23/80: current_loss=0.03606 | best_loss=0.03544
Epoch 24/80: current_loss=0.03551 | best_loss=0.03544
Epoch 25/80: current_loss=0.03548 | best_loss=0.03544
Epoch 26/80: current_loss=0.03563 | best_loss=0.03544
Epoch 27/80: current_loss=0.03552 | best_loss=0.03544
Epoch 28/80: current_loss=0.03558 | best_loss=0.03544
Epoch 29/80: current_loss=0.03568 | best_loss=0.03544
Epoch 30/80: current_loss=0.03569 | best_loss=0.03544
Epoch 31/80: current_loss=0.03559 | best_loss=0.03544
Epoch 32/80: current_loss=0.03552 | best_loss=0.03544
Epoch 33/80: current_loss=0.03564 | best_loss=0.03544
Epoch 34/80: current_loss=0.03557 | best_loss=0.03544
Epoch 35/80: current_loss=0.03562 | best_loss=0.03544
Epoch 36/80: current_loss=0.03565 | best_loss=0.03544
Epoch 37/80: current_loss=0.03553 | best_loss=0.03544
Epoch 38/80: current_loss=0.03560 | best_loss=0.03544
Epoch 39/80: current_loss=0.03557 | best_loss=0.03544
Epoch 40/80: current_loss=0.03557 | best_loss=0.03544
Epoch 41/80: current_loss=0.03560 | best_loss=0.03544
Early Stopping at epoch 41
      explained_var=0.01921 | mse_loss=0.03521
----------------------------------------------
Average early_stopping_point: 16| avg_exp_var=0.01336| avg_loss=0.03492
----------------------------------------------


----------------------------------------------
Params for Trial 91
{'learning_rate': 0.0001, 'weight_decay': 0.0004949923341930183, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.07834 | best_loss=0.07834
Epoch 1/80: current_loss=0.04531 | best_loss=0.04531
Epoch 2/80: current_loss=0.04351 | best_loss=0.04351
Epoch 3/80: current_loss=0.04260 | best_loss=0.04260
Epoch 4/80: current_loss=0.04208 | best_loss=0.04208
Epoch 5/80: current_loss=0.04137 | best_loss=0.04137
Epoch 6/80: current_loss=0.04090 | best_loss=0.04090
Epoch 7/80: current_loss=0.04046 | best_loss=0.04046
Epoch 8/80: current_loss=0.04017 | best_loss=0.04017
Epoch 9/80: current_loss=0.03972 | best_loss=0.03972
Epoch 10/80: current_loss=0.03945 | best_loss=0.03945
Epoch 11/80: current_loss=0.03919 | best_loss=0.03919
Epoch 12/80: current_loss=0.03901 | best_loss=0.03901
Epoch 13/80: current_loss=0.03885 | best_loss=0.03885
Epoch 14/80: current_loss=0.03868 | best_loss=0.03868
Epoch 15/80: current_loss=0.03859 | best_loss=0.03859
Epoch 16/80: current_loss=0.03851 | best_loss=0.03851
Epoch 17/80: current_loss=0.03841 | best_loss=0.03841
Epoch 18/80: current_loss=0.03844 | best_loss=0.03841
Epoch 19/80: current_loss=0.03830 | best_loss=0.03830
Epoch 20/80: current_loss=0.03825 | best_loss=0.03825
Epoch 21/80: current_loss=0.03821 | best_loss=0.03821
Epoch 22/80: current_loss=0.03817 | best_loss=0.03817
Epoch 23/80: current_loss=0.03807 | best_loss=0.03807
Epoch 24/80: current_loss=0.03801 | best_loss=0.03801
Epoch 25/80: current_loss=0.03789 | best_loss=0.03789
Epoch 26/80: current_loss=0.03790 | best_loss=0.03789
Epoch 27/80: current_loss=0.03811 | best_loss=0.03789
Epoch 28/80: current_loss=0.03788 | best_loss=0.03788
Epoch 29/80: current_loss=0.03780 | best_loss=0.03780
Epoch 30/80: current_loss=0.03789 | best_loss=0.03780
Epoch 31/80: current_loss=0.03788 | best_loss=0.03780
Epoch 32/80: current_loss=0.03780 | best_loss=0.03780
Epoch 33/80: current_loss=0.03784 | best_loss=0.03780
Epoch 34/80: current_loss=0.03822 | best_loss=0.03780
Epoch 35/80: current_loss=0.03783 | best_loss=0.03780
Epoch 36/80: current_loss=0.03778 | best_loss=0.03778
Epoch 37/80: current_loss=0.03796 | best_loss=0.03778
Epoch 38/80: current_loss=0.03796 | best_loss=0.03778
Epoch 39/80: current_loss=0.03773 | best_loss=0.03773
Epoch 40/80: current_loss=0.03768 | best_loss=0.03768
Epoch 41/80: current_loss=0.03775 | best_loss=0.03768
Epoch 42/80: current_loss=0.03768 | best_loss=0.03768
Epoch 43/80: current_loss=0.03877 | best_loss=0.03768
Epoch 44/80: current_loss=0.03769 | best_loss=0.03768
Epoch 45/80: current_loss=0.03783 | best_loss=0.03768
Epoch 46/80: current_loss=0.03785 | best_loss=0.03768
Epoch 47/80: current_loss=0.03772 | best_loss=0.03768
Epoch 48/80: current_loss=0.03768 | best_loss=0.03768
Epoch 49/80: current_loss=0.03775 | best_loss=0.03768
Epoch 50/80: current_loss=0.03766 | best_loss=0.03766
Epoch 51/80: current_loss=0.03762 | best_loss=0.03762
Epoch 52/80: current_loss=0.03758 | best_loss=0.03758
Epoch 53/80: current_loss=0.03768 | best_loss=0.03758
Epoch 54/80: current_loss=0.03762 | best_loss=0.03758
Epoch 55/80: current_loss=0.03775 | best_loss=0.03758
Epoch 56/80: current_loss=0.03781 | best_loss=0.03758
Epoch 57/80: current_loss=0.03760 | best_loss=0.03758
Epoch 58/80: current_loss=0.03764 | best_loss=0.03758
Epoch 59/80: current_loss=0.03774 | best_loss=0.03758
Epoch 60/80: current_loss=0.03758 | best_loss=0.03758
Epoch 61/80: current_loss=0.03773 | best_loss=0.03758
Epoch 62/80: current_loss=0.03764 | best_loss=0.03758
Epoch 63/80: current_loss=0.03749 | best_loss=0.03749
Epoch 64/80: current_loss=0.03753 | best_loss=0.03749
Epoch 65/80: current_loss=0.03745 | best_loss=0.03745
Epoch 66/80: current_loss=0.03750 | best_loss=0.03745
Epoch 67/80: current_loss=0.03743 | best_loss=0.03743
Epoch 68/80: current_loss=0.03755 | best_loss=0.03743
Epoch 69/80: current_loss=0.03770 | best_loss=0.03743
Epoch 70/80: current_loss=0.03755 | best_loss=0.03743
Epoch 71/80: current_loss=0.03745 | best_loss=0.03743
Epoch 72/80: current_loss=0.03757 | best_loss=0.03743
Epoch 73/80: current_loss=0.03752 | best_loss=0.03743
Epoch 74/80: current_loss=0.03748 | best_loss=0.03743
Epoch 75/80: current_loss=0.03755 | best_loss=0.03743
Epoch 76/80: current_loss=0.03757 | best_loss=0.03743
Epoch 77/80: current_loss=0.03744 | best_loss=0.03743
Epoch 78/80: current_loss=0.03749 | best_loss=0.03743
Epoch 79/80: current_loss=0.03764 | best_loss=0.03743
      explained_var=0.02189 | mse_loss=0.03835
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04160 | best_loss=0.04160
Epoch 1/80: current_loss=0.04137 | best_loss=0.04137
Epoch 2/80: current_loss=0.04140 | best_loss=0.04137
Epoch 3/80: current_loss=0.04171 | best_loss=0.04137
Epoch 4/80: current_loss=0.04163 | best_loss=0.04137
Epoch 5/80: current_loss=0.04150 | best_loss=0.04137
Epoch 6/80: current_loss=0.04150 | best_loss=0.04137
Epoch 7/80: current_loss=0.04141 | best_loss=0.04137
Epoch 8/80: current_loss=0.04135 | best_loss=0.04135
Epoch 9/80: current_loss=0.04238 | best_loss=0.04135
Epoch 10/80: current_loss=0.04149 | best_loss=0.04135
Epoch 11/80: current_loss=0.04153 | best_loss=0.04135
Epoch 12/80: current_loss=0.04148 | best_loss=0.04135
Epoch 13/80: current_loss=0.04151 | best_loss=0.04135
Epoch 14/80: current_loss=0.04159 | best_loss=0.04135
Epoch 15/80: current_loss=0.04180 | best_loss=0.04135
Epoch 16/80: current_loss=0.04149 | best_loss=0.04135
Epoch 17/80: current_loss=0.04164 | best_loss=0.04135
Epoch 18/80: current_loss=0.04140 | best_loss=0.04135
Epoch 19/80: current_loss=0.04201 | best_loss=0.04135
Epoch 20/80: current_loss=0.04141 | best_loss=0.04135
Epoch 21/80: current_loss=0.04180 | best_loss=0.04135
Epoch 22/80: current_loss=0.04156 | best_loss=0.04135
Epoch 23/80: current_loss=0.04139 | best_loss=0.04135
Epoch 24/80: current_loss=0.04139 | best_loss=0.04135
Epoch 25/80: current_loss=0.04135 | best_loss=0.04135
Epoch 26/80: current_loss=0.04155 | best_loss=0.04135
Epoch 27/80: current_loss=0.04141 | best_loss=0.04135
Epoch 28/80: current_loss=0.04150 | best_loss=0.04135
Early Stopping at epoch 28
      explained_var=0.03278 | mse_loss=0.03988
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02810 | best_loss=0.02810
Epoch 1/80: current_loss=0.02835 | best_loss=0.02810
Epoch 2/80: current_loss=0.02818 | best_loss=0.02810
Epoch 3/80: current_loss=0.02845 | best_loss=0.02810
Epoch 4/80: current_loss=0.02812 | best_loss=0.02810
Epoch 5/80: current_loss=0.02844 | best_loss=0.02810
Epoch 6/80: current_loss=0.02853 | best_loss=0.02810
Epoch 7/80: current_loss=0.02825 | best_loss=0.02810
Epoch 8/80: current_loss=0.02861 | best_loss=0.02810
Epoch 9/80: current_loss=0.02821 | best_loss=0.02810
Epoch 10/80: current_loss=0.02852 | best_loss=0.02810
Epoch 11/80: current_loss=0.02851 | best_loss=0.02810
Epoch 12/80: current_loss=0.02852 | best_loss=0.02810
Epoch 13/80: current_loss=0.02816 | best_loss=0.02810
Epoch 14/80: current_loss=0.02874 | best_loss=0.02810
Epoch 15/80: current_loss=0.02857 | best_loss=0.02810
Epoch 16/80: current_loss=0.02817 | best_loss=0.02810
Epoch 17/80: current_loss=0.02932 | best_loss=0.02810
Epoch 18/80: current_loss=0.02861 | best_loss=0.02810
Epoch 19/80: current_loss=0.02838 | best_loss=0.02810
Epoch 20/80: current_loss=0.02841 | best_loss=0.02810
Early Stopping at epoch 20
      explained_var=-0.00424 | mse_loss=0.02852

----------------------------------------------
Params for Trial 92
{'learning_rate': 0.0001, 'weight_decay': 0.0008929590147016082, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.06149 | best_loss=0.06149
Epoch 1/80: current_loss=0.04109 | best_loss=0.04109
Epoch 2/80: current_loss=0.04057 | best_loss=0.04057
Epoch 3/80: current_loss=0.04037 | best_loss=0.04037
Epoch 4/80: current_loss=0.03987 | best_loss=0.03987
Epoch 5/80: current_loss=0.03962 | best_loss=0.03962
Epoch 6/80: current_loss=0.03932 | best_loss=0.03932
Epoch 7/80: current_loss=0.03908 | best_loss=0.03908
Epoch 8/80: current_loss=0.03904 | best_loss=0.03904
Epoch 9/80: current_loss=0.03869 | best_loss=0.03869
Epoch 10/80: current_loss=0.03858 | best_loss=0.03858
Epoch 11/80: current_loss=0.03846 | best_loss=0.03846
Epoch 12/80: current_loss=0.03851 | best_loss=0.03846
Epoch 13/80: current_loss=0.03828 | best_loss=0.03828
Epoch 14/80: current_loss=0.03815 | best_loss=0.03815
Epoch 15/80: current_loss=0.03808 | best_loss=0.03808
Epoch 16/80: current_loss=0.03811 | best_loss=0.03808
Epoch 17/80: current_loss=0.03800 | best_loss=0.03800
Epoch 18/80: current_loss=0.03801 | best_loss=0.03800
Epoch 19/80: current_loss=0.03799 | best_loss=0.03799
Epoch 20/80: current_loss=0.03777 | best_loss=0.03777
Epoch 21/80: current_loss=0.03775 | best_loss=0.03775
Epoch 22/80: current_loss=0.03803 | best_loss=0.03775
Epoch 23/80: current_loss=0.03780 | best_loss=0.03775
Epoch 24/80: current_loss=0.03779 | best_loss=0.03775
Epoch 25/80: current_loss=0.03791 | best_loss=0.03775
Epoch 26/80: current_loss=0.03769 | best_loss=0.03769
Epoch 27/80: current_loss=0.03771 | best_loss=0.03769
Epoch 28/80: current_loss=0.03770 | best_loss=0.03769
Epoch 29/80: current_loss=0.03765 | best_loss=0.03765
Epoch 30/80: current_loss=0.03767 | best_loss=0.03765
Epoch 31/80: current_loss=0.03784 | best_loss=0.03765
Epoch 32/80: current_loss=0.03769 | best_loss=0.03765
Epoch 33/80: current_loss=0.03768 | best_loss=0.03765
Epoch 34/80: current_loss=0.03762 | best_loss=0.03762
Epoch 35/80: current_loss=0.03763 | best_loss=0.03762
Epoch 36/80: current_loss=0.03765 | best_loss=0.03762
Epoch 37/80: current_loss=0.03778 | best_loss=0.03762
Epoch 38/80: current_loss=0.03770 | best_loss=0.03762
Epoch 39/80: current_loss=0.03777 | best_loss=0.03762
Epoch 40/80: current_loss=0.03778 | best_loss=0.03762
Epoch 41/80: current_loss=0.03772 | best_loss=0.03762
Epoch 42/80: current_loss=0.03784 | best_loss=0.03762
Epoch 43/80: current_loss=0.03768 | best_loss=0.03762
Epoch 44/80: current_loss=0.03769 | best_loss=0.03762
Epoch 45/80: current_loss=0.03766 | best_loss=0.03762
Epoch 46/80: current_loss=0.03765 | best_loss=0.03762
Epoch 47/80: current_loss=0.03766 | best_loss=0.03762
Epoch 48/80: current_loss=0.03750 | best_loss=0.03750
Epoch 49/80: current_loss=0.03773 | best_loss=0.03750
Epoch 50/80: current_loss=0.03750 | best_loss=0.03750
Epoch 51/80: current_loss=0.03760 | best_loss=0.03750
Epoch 52/80: current_loss=0.03749 | best_loss=0.03749
Epoch 53/80: current_loss=0.03748 | best_loss=0.03748
Epoch 54/80: current_loss=0.03757 | best_loss=0.03748
Epoch 55/80: current_loss=0.03763 | best_loss=0.03748
Epoch 56/80: current_loss=0.03757 | best_loss=0.03748
Epoch 57/80: current_loss=0.03762 | best_loss=0.03748
Epoch 58/80: current_loss=0.03756 | best_loss=0.03748
Epoch 59/80: current_loss=0.03752 | best_loss=0.03748
Epoch 60/80: current_loss=0.03787 | best_loss=0.03748
Epoch 61/80: current_loss=0.03766 | best_loss=0.03748
Epoch 62/80: current_loss=0.03766 | best_loss=0.03748
Epoch 63/80: current_loss=0.03750 | best_loss=0.03748
Epoch 64/80: current_loss=0.03757 | best_loss=0.03748
Epoch 65/80: current_loss=0.03758 | best_loss=0.03748
Epoch 66/80: current_loss=0.03751 | best_loss=0.03748
Epoch 67/80: current_loss=0.03775 | best_loss=0.03748
Epoch 68/80: current_loss=0.03763 | best_loss=0.03748
Epoch 69/80: current_loss=0.03751 | best_loss=0.03748
Epoch 70/80: current_loss=0.03777 | best_loss=0.03748
Epoch 71/80: current_loss=0.03747 | best_loss=0.03747
Epoch 72/80: current_loss=0.03756 | best_loss=0.03747
Epoch 73/80: current_loss=0.03753 | best_loss=0.03747
Epoch 74/80: current_loss=0.03747 | best_loss=0.03747
Epoch 75/80: current_loss=0.03754 | best_loss=0.03747
Epoch 76/80: current_loss=0.03753 | best_loss=0.03747
Epoch 77/80: current_loss=0.03749 | best_loss=0.03747
Epoch 78/80: current_loss=0.03756 | best_loss=0.03747
Epoch 79/80: current_loss=0.03749 | best_loss=0.03747
      explained_var=0.01945 | mse_loss=0.03836
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04110 | best_loss=0.04110
Epoch 1/80: current_loss=0.04126 | best_loss=0.04110
Epoch 2/80: current_loss=0.04130 | best_loss=0.04110
Epoch 3/80: current_loss=0.04139 | best_loss=0.04110
Epoch 4/80: current_loss=0.04138 | best_loss=0.04110
Epoch 5/80: current_loss=0.04138 | best_loss=0.04110
Epoch 6/80: current_loss=0.04158 | best_loss=0.04110
Epoch 7/80: current_loss=0.04138 | best_loss=0.04110
Epoch 8/80: current_loss=0.04153 | best_loss=0.04110
Epoch 9/80: current_loss=0.04147 | best_loss=0.04110
Epoch 10/80: current_loss=0.04152 | best_loss=0.04110
Epoch 11/80: current_loss=0.04150 | best_loss=0.04110
Epoch 12/80: current_loss=0.04153 | best_loss=0.04110
Epoch 13/80: current_loss=0.04140 | best_loss=0.04110
Epoch 14/80: current_loss=0.04144 | best_loss=0.04110
Epoch 15/80: current_loss=0.04143 | best_loss=0.04110
Epoch 16/80: current_loss=0.04160 | best_loss=0.04110
Epoch 17/80: current_loss=0.04146 | best_loss=0.04110
Epoch 18/80: current_loss=0.04151 | best_loss=0.04110
Epoch 19/80: current_loss=0.04155 | best_loss=0.04110
Epoch 20/80: current_loss=0.04150 | best_loss=0.04110
Early Stopping at epoch 20
      explained_var=0.03925 | mse_loss=0.03962
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02825 | best_loss=0.02825
Epoch 1/80: current_loss=0.02856 | best_loss=0.02825
Epoch 2/80: current_loss=0.02811 | best_loss=0.02811
Epoch 3/80: current_loss=0.02832 | best_loss=0.02811
Epoch 4/80: current_loss=0.02837 | best_loss=0.02811
Epoch 5/80: current_loss=0.02847 | best_loss=0.02811
Epoch 6/80: current_loss=0.02826 | best_loss=0.02811
Epoch 7/80: current_loss=0.02821 | best_loss=0.02811
Epoch 8/80: current_loss=0.02822 | best_loss=0.02811
Epoch 9/80: current_loss=0.02835 | best_loss=0.02811
Epoch 10/80: current_loss=0.02825 | best_loss=0.02811
Epoch 11/80: current_loss=0.02835 | best_loss=0.02811
Epoch 12/80: current_loss=0.02827 | best_loss=0.02811
Epoch 13/80: current_loss=0.02833 | best_loss=0.02811
Epoch 14/80: current_loss=0.02865 | best_loss=0.02811
Epoch 15/80: current_loss=0.02825 | best_loss=0.02811
Epoch 16/80: current_loss=0.02840 | best_loss=0.02811
Epoch 17/80: current_loss=0.02841 | best_loss=0.02811
Epoch 18/80: current_loss=0.02829 | best_loss=0.02811
Epoch 19/80: current_loss=0.02831 | best_loss=0.02811
Epoch 20/80: current_loss=0.02827 | best_loss=0.02811
Epoch 21/80: current_loss=0.02832 | best_loss=0.02811
Epoch 22/80: current_loss=0.02813 | best_loss=0.02811
Early Stopping at epoch 22
      explained_var=-0.00373 | mse_loss=0.02852

----------------------------------------------
Params for Trial 93
{'learning_rate': 0.0001, 'weight_decay': 0.0002372494698852749, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.15000000000000002}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.08486 | best_loss=0.08486
Epoch 1/80: current_loss=0.04044 | best_loss=0.04044
Epoch 2/80: current_loss=0.03995 | best_loss=0.03995
Epoch 3/80: current_loss=0.03983 | best_loss=0.03983
Epoch 4/80: current_loss=0.03924 | best_loss=0.03924
Epoch 5/80: current_loss=0.03908 | best_loss=0.03908
Epoch 6/80: current_loss=0.03899 | best_loss=0.03899
Epoch 7/80: current_loss=0.03867 | best_loss=0.03867
Epoch 8/80: current_loss=0.03874 | best_loss=0.03867
Epoch 9/80: current_loss=0.03838 | best_loss=0.03838
Epoch 10/80: current_loss=0.03817 | best_loss=0.03817
Epoch 11/80: current_loss=0.03819 | best_loss=0.03817
Epoch 12/80: current_loss=0.03801 | best_loss=0.03801
Epoch 13/80: current_loss=0.03803 | best_loss=0.03801
Epoch 14/80: current_loss=0.03792 | best_loss=0.03792
Epoch 15/80: current_loss=0.03782 | best_loss=0.03782
Epoch 16/80: current_loss=0.03779 | best_loss=0.03779
Epoch 17/80: current_loss=0.03769 | best_loss=0.03769
Epoch 18/80: current_loss=0.03784 | best_loss=0.03769
Epoch 19/80: current_loss=0.03789 | best_loss=0.03769
Epoch 20/80: current_loss=0.03758 | best_loss=0.03758
Epoch 21/80: current_loss=0.03762 | best_loss=0.03758
Epoch 22/80: current_loss=0.03759 | best_loss=0.03758
Epoch 23/80: current_loss=0.03758 | best_loss=0.03758
Epoch 24/80: current_loss=0.03748 | best_loss=0.03748
Epoch 25/80: current_loss=0.03736 | best_loss=0.03736
Epoch 26/80: current_loss=0.03786 | best_loss=0.03736
Epoch 27/80: current_loss=0.03741 | best_loss=0.03736
Epoch 28/80: current_loss=0.03751 | best_loss=0.03736
Epoch 29/80: current_loss=0.03762 | best_loss=0.03736
Epoch 30/80: current_loss=0.03758 | best_loss=0.03736
Epoch 31/80: current_loss=0.03778 | best_loss=0.03736
Epoch 32/80: current_loss=0.03751 | best_loss=0.03736
Epoch 33/80: current_loss=0.03755 | best_loss=0.03736
Epoch 34/80: current_loss=0.03754 | best_loss=0.03736
Epoch 35/80: current_loss=0.03749 | best_loss=0.03736
Epoch 36/80: current_loss=0.03761 | best_loss=0.03736
Epoch 37/80: current_loss=0.03750 | best_loss=0.03736
Epoch 38/80: current_loss=0.03756 | best_loss=0.03736
Epoch 39/80: current_loss=0.03759 | best_loss=0.03736
Epoch 40/80: current_loss=0.03756 | best_loss=0.03736
Epoch 41/80: current_loss=0.03785 | best_loss=0.03736
Epoch 42/80: current_loss=0.03772 | best_loss=0.03736
Epoch 43/80: current_loss=0.03758 | best_loss=0.03736
Epoch 44/80: current_loss=0.03756 | best_loss=0.03736
Epoch 45/80: current_loss=0.03760 | best_loss=0.03736
Early Stopping at epoch 45
      explained_var=0.02243 | mse_loss=0.03828
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04145 | best_loss=0.04145
Epoch 1/80: current_loss=0.04144 | best_loss=0.04144
Epoch 2/80: current_loss=0.04124 | best_loss=0.04124
Epoch 3/80: current_loss=0.04129 | best_loss=0.04124
Epoch 4/80: current_loss=0.04124 | best_loss=0.04124
Epoch 5/80: current_loss=0.04155 | best_loss=0.04124
Epoch 6/80: current_loss=0.04132 | best_loss=0.04124
Epoch 7/80: current_loss=0.04135 | best_loss=0.04124
Epoch 8/80: current_loss=0.04131 | best_loss=0.04124
Epoch 9/80: current_loss=0.04128 | best_loss=0.04124
Epoch 10/80: current_loss=0.04148 | best_loss=0.04124
Epoch 11/80: current_loss=0.04150 | best_loss=0.04124
Epoch 12/80: current_loss=0.04122 | best_loss=0.04122
Epoch 13/80: current_loss=0.04123 | best_loss=0.04122
Epoch 14/80: current_loss=0.04117 | best_loss=0.04117
Epoch 15/80: current_loss=0.04109 | best_loss=0.04109
Epoch 16/80: current_loss=0.04112 | best_loss=0.04109
Epoch 17/80: current_loss=0.04144 | best_loss=0.04109
Epoch 18/80: current_loss=0.04120 | best_loss=0.04109
Epoch 19/80: current_loss=0.04137 | best_loss=0.04109
Epoch 20/80: current_loss=0.04124 | best_loss=0.04109
Epoch 21/80: current_loss=0.04143 | best_loss=0.04109
Epoch 22/80: current_loss=0.04139 | best_loss=0.04109
Epoch 23/80: current_loss=0.04143 | best_loss=0.04109
Epoch 24/80: current_loss=0.04138 | best_loss=0.04109
Epoch 25/80: current_loss=0.04135 | best_loss=0.04109
Epoch 26/80: current_loss=0.04159 | best_loss=0.04109
Epoch 27/80: current_loss=0.04146 | best_loss=0.04109
Epoch 28/80: current_loss=0.04160 | best_loss=0.04109
Epoch 29/80: current_loss=0.04153 | best_loss=0.04109
Epoch 30/80: current_loss=0.04167 | best_loss=0.04109
Epoch 31/80: current_loss=0.04168 | best_loss=0.04109
Epoch 32/80: current_loss=0.04180 | best_loss=0.04109
Epoch 33/80: current_loss=0.04178 | best_loss=0.04109
Epoch 34/80: current_loss=0.04187 | best_loss=0.04109
Epoch 35/80: current_loss=0.04179 | best_loss=0.04109
Early Stopping at epoch 35
      explained_var=0.04038 | mse_loss=0.03957
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02883 | best_loss=0.02883
Epoch 1/80: current_loss=0.02830 | best_loss=0.02830
Epoch 2/80: current_loss=0.02855 | best_loss=0.02830
Epoch 3/80: current_loss=0.02833 | best_loss=0.02830
Epoch 4/80: current_loss=0.02866 | best_loss=0.02830
Epoch 5/80: current_loss=0.02844 | best_loss=0.02830
Epoch 6/80: current_loss=0.02861 | best_loss=0.02830
Epoch 7/80: current_loss=0.02809 | best_loss=0.02809
Epoch 8/80: current_loss=0.02857 | best_loss=0.02809
Epoch 9/80: current_loss=0.02813 | best_loss=0.02809
Epoch 10/80: current_loss=0.02842 | best_loss=0.02809
Epoch 11/80: current_loss=0.02829 | best_loss=0.02809
Epoch 12/80: current_loss=0.02835 | best_loss=0.02809
Epoch 13/80: current_loss=0.02833 | best_loss=0.02809
Epoch 14/80: current_loss=0.02830 | best_loss=0.02809
Epoch 15/80: current_loss=0.02872 | best_loss=0.02809
Epoch 16/80: current_loss=0.02839 | best_loss=0.02809
Epoch 17/80: current_loss=0.02833 | best_loss=0.02809
Epoch 18/80: current_loss=0.02857 | best_loss=0.02809
Epoch 19/80: current_loss=0.02838 | best_loss=0.02809
Epoch 20/80: current_loss=0.02822 | best_loss=0.02809
Epoch 21/80: current_loss=0.02856 | best_loss=0.02809
Epoch 22/80: current_loss=0.02850 | best_loss=0.02809
Epoch 23/80: current_loss=0.02869 | best_loss=0.02809
Epoch 24/80: current_loss=0.02837 | best_loss=0.02809
Epoch 25/80: current_loss=0.02856 | best_loss=0.02809
Epoch 26/80: current_loss=0.02833 | best_loss=0.02809
Epoch 27/80: current_loss=0.02819 | best_loss=0.02809
Early Stopping at epoch 27
      explained_var=-0.00698 | mse_loss=0.02856

----------------------------------------------
Params for Trial 94
{'learning_rate': 0.0001, 'weight_decay': 2.463319101248478e-05, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.12476 | best_loss=0.12476
Epoch 1/80: current_loss=0.05518 | best_loss=0.05518
Epoch 2/80: current_loss=0.05224 | best_loss=0.05224
Epoch 3/80: current_loss=0.05061 | best_loss=0.05061
Epoch 4/80: current_loss=0.04887 | best_loss=0.04887
Epoch 5/80: current_loss=0.04773 | best_loss=0.04773
Epoch 6/80: current_loss=0.04654 | best_loss=0.04654
Epoch 7/80: current_loss=0.04555 | best_loss=0.04555
Epoch 8/80: current_loss=0.04481 | best_loss=0.04481
Epoch 9/80: current_loss=0.04410 | best_loss=0.04410
Epoch 10/80: current_loss=0.04341 | best_loss=0.04341
Epoch 11/80: current_loss=0.04267 | best_loss=0.04267
Epoch 12/80: current_loss=0.04219 | best_loss=0.04219
Epoch 13/80: current_loss=0.04156 | best_loss=0.04156
Epoch 14/80: current_loss=0.04128 | best_loss=0.04128
Epoch 15/80: current_loss=0.04080 | best_loss=0.04080
Epoch 16/80: current_loss=0.04061 | best_loss=0.04061
Epoch 17/80: current_loss=0.04051 | best_loss=0.04051
Epoch 18/80: current_loss=0.04014 | best_loss=0.04014
Epoch 19/80: current_loss=0.03998 | best_loss=0.03998
Epoch 20/80: current_loss=0.04003 | best_loss=0.03998
Epoch 21/80: current_loss=0.03980 | best_loss=0.03980
Epoch 22/80: current_loss=0.03966 | best_loss=0.03966
Epoch 23/80: current_loss=0.03963 | best_loss=0.03963
Epoch 24/80: current_loss=0.03948 | best_loss=0.03948
Epoch 25/80: current_loss=0.03936 | best_loss=0.03936
Epoch 26/80: current_loss=0.03922 | best_loss=0.03922
Epoch 27/80: current_loss=0.03989 | best_loss=0.03922
Epoch 28/80: current_loss=0.03893 | best_loss=0.03893
Epoch 29/80: current_loss=0.03884 | best_loss=0.03884
Epoch 30/80: current_loss=0.03892 | best_loss=0.03884
Epoch 31/80: current_loss=0.03888 | best_loss=0.03884
Epoch 32/80: current_loss=0.03860 | best_loss=0.03860
Epoch 33/80: current_loss=0.03876 | best_loss=0.03860
Epoch 34/80: current_loss=0.03847 | best_loss=0.03847
Epoch 35/80: current_loss=0.03847 | best_loss=0.03847
Epoch 36/80: current_loss=0.03831 | best_loss=0.03831
Epoch 37/80: current_loss=0.03829 | best_loss=0.03829
Epoch 38/80: current_loss=0.03827 | best_loss=0.03827
Epoch 39/80: current_loss=0.03827 | best_loss=0.03827
Epoch 40/80: current_loss=0.03815 | best_loss=0.03815
Epoch 41/80: current_loss=0.03809 | best_loss=0.03809
Epoch 42/80: current_loss=0.03809 | best_loss=0.03809
Epoch 43/80: current_loss=0.03821 | best_loss=0.03809
Epoch 44/80: current_loss=0.03798 | best_loss=0.03798
Epoch 45/80: current_loss=0.03799 | best_loss=0.03798
Epoch 46/80: current_loss=0.03801 | best_loss=0.03798
Epoch 47/80: current_loss=0.03795 | best_loss=0.03795
Epoch 48/80: current_loss=0.03805 | best_loss=0.03795
Epoch 49/80: current_loss=0.03797 | best_loss=0.03795
Epoch 50/80: current_loss=0.03803 | best_loss=0.03795
Epoch 51/80: current_loss=0.03806 | best_loss=0.03795
Epoch 52/80: current_loss=0.03803 | best_loss=0.03795
Epoch 53/80: current_loss=0.03796 | best_loss=0.03795
Epoch 54/80: current_loss=0.03813 | best_loss=0.03795
Epoch 55/80: current_loss=0.03787 | best_loss=0.03787
Epoch 56/80: current_loss=0.03822 | best_loss=0.03787
Epoch 57/80: current_loss=0.03779 | best_loss=0.03779
Epoch 58/80: current_loss=0.03783 | best_loss=0.03779
Epoch 59/80: current_loss=0.03780 | best_loss=0.03779
Epoch 60/80: current_loss=0.03782 | best_loss=0.03779
Epoch 61/80: current_loss=0.03770 | best_loss=0.03770
Epoch 62/80: current_loss=0.03785 | best_loss=0.03770
Epoch 63/80: current_loss=0.03762 | best_loss=0.03762
Epoch 64/80: current_loss=0.03764 | best_loss=0.03762
Epoch 65/80: current_loss=0.03765 | best_loss=0.03762
Epoch 66/80: current_loss=0.03771 | best_loss=0.03762
Epoch 67/80: current_loss=0.03768 | best_loss=0.03762
Epoch 68/80: current_loss=0.03781 | best_loss=0.03762
Epoch 69/80: current_loss=0.03763 | best_loss=0.03762
Epoch 70/80: current_loss=0.03763 | best_loss=0.03762
Epoch 71/80: current_loss=0.03790 | best_loss=0.03762
Epoch 72/80: current_loss=0.03773 | best_loss=0.03762
Epoch 73/80: current_loss=0.03823 | best_loss=0.03762
Epoch 74/80: current_loss=0.03760 | best_loss=0.03760
Epoch 75/80: current_loss=0.03770 | best_loss=0.03760
Epoch 76/80: current_loss=0.03769 | best_loss=0.03760
Epoch 77/80: current_loss=0.03759 | best_loss=0.03759
Epoch 78/80: current_loss=0.03762 | best_loss=0.03759
Epoch 79/80: current_loss=0.03763 | best_loss=0.03759
      explained_var=0.01685 | mse_loss=0.03853

----------------------------------------------
Params for Trial 95
{'learning_rate': 0.001, 'weight_decay': 0.0005999223851701479, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.1}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03970 | best_loss=0.03970
Epoch 1/80: current_loss=0.03878 | best_loss=0.03878
Epoch 2/80: current_loss=0.03817 | best_loss=0.03817
Epoch 3/80: current_loss=0.03789 | best_loss=0.03789
Epoch 4/80: current_loss=0.03804 | best_loss=0.03789
Epoch 5/80: current_loss=0.03830 | best_loss=0.03789
Epoch 6/80: current_loss=0.03837 | best_loss=0.03789
Epoch 7/80: current_loss=0.03816 | best_loss=0.03789
Epoch 8/80: current_loss=0.03785 | best_loss=0.03785
Epoch 9/80: current_loss=0.03802 | best_loss=0.03785
Epoch 10/80: current_loss=0.03720 | best_loss=0.03720
Epoch 11/80: current_loss=0.03749 | best_loss=0.03720
Epoch 12/80: current_loss=0.03743 | best_loss=0.03720
Epoch 13/80: current_loss=0.04041 | best_loss=0.03720
Epoch 14/80: current_loss=0.03765 | best_loss=0.03720
Epoch 15/80: current_loss=0.03720 | best_loss=0.03720
Epoch 16/80: current_loss=0.03807 | best_loss=0.03720
Epoch 17/80: current_loss=0.03810 | best_loss=0.03720
Epoch 18/80: current_loss=0.03832 | best_loss=0.03720
Epoch 19/80: current_loss=0.03886 | best_loss=0.03720
Epoch 20/80: current_loss=0.03947 | best_loss=0.03720
Epoch 21/80: current_loss=0.03764 | best_loss=0.03720
Epoch 22/80: current_loss=0.03775 | best_loss=0.03720
Epoch 23/80: current_loss=0.03757 | best_loss=0.03720
Epoch 24/80: current_loss=0.03756 | best_loss=0.03720
Epoch 25/80: current_loss=0.03957 | best_loss=0.03720
Epoch 26/80: current_loss=0.03739 | best_loss=0.03720
Epoch 27/80: current_loss=0.03813 | best_loss=0.03720
Epoch 28/80: current_loss=0.03930 | best_loss=0.03720
Epoch 29/80: current_loss=0.03895 | best_loss=0.03720
Epoch 30/80: current_loss=0.03751 | best_loss=0.03720
Epoch 31/80: current_loss=0.03955 | best_loss=0.03720
Epoch 32/80: current_loss=0.03766 | best_loss=0.03720
Epoch 33/80: current_loss=0.03878 | best_loss=0.03720
Epoch 34/80: current_loss=0.03749 | best_loss=0.03720
Epoch 35/80: current_loss=0.03875 | best_loss=0.03720
Early Stopping at epoch 35
      explained_var=0.02844 | mse_loss=0.03800
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04258 | best_loss=0.04258
Epoch 1/80: current_loss=0.04549 | best_loss=0.04258
Epoch 2/80: current_loss=0.04343 | best_loss=0.04258
Epoch 3/80: current_loss=0.04195 | best_loss=0.04195
Epoch 4/80: current_loss=0.04217 | best_loss=0.04195
Epoch 5/80: current_loss=0.04174 | best_loss=0.04174
Epoch 6/80: current_loss=0.04448 | best_loss=0.04174
Epoch 7/80: current_loss=0.04170 | best_loss=0.04170
Epoch 8/80: current_loss=0.04167 | best_loss=0.04167
Epoch 9/80: current_loss=0.04198 | best_loss=0.04167
Epoch 10/80: current_loss=0.04263 | best_loss=0.04167
Epoch 11/80: current_loss=0.04188 | best_loss=0.04167
Epoch 12/80: current_loss=0.04318 | best_loss=0.04167
Epoch 13/80: current_loss=0.04327 | best_loss=0.04167
Epoch 14/80: current_loss=0.04160 | best_loss=0.04160
Epoch 15/80: current_loss=0.04152 | best_loss=0.04152
Epoch 16/80: current_loss=0.04208 | best_loss=0.04152
Epoch 17/80: current_loss=0.04152 | best_loss=0.04152
Epoch 18/80: current_loss=0.04470 | best_loss=0.04152
Epoch 19/80: current_loss=0.04141 | best_loss=0.04141
Epoch 20/80: current_loss=0.04183 | best_loss=0.04141
Epoch 21/80: current_loss=0.04223 | best_loss=0.04141
Epoch 22/80: current_loss=0.04243 | best_loss=0.04141
Epoch 23/80: current_loss=0.04181 | best_loss=0.04141
Epoch 24/80: current_loss=0.04152 | best_loss=0.04141
Epoch 25/80: current_loss=0.04211 | best_loss=0.04141
Epoch 26/80: current_loss=0.04467 | best_loss=0.04141
Epoch 27/80: current_loss=0.04171 | best_loss=0.04141
Epoch 28/80: current_loss=0.04204 | best_loss=0.04141
Epoch 29/80: current_loss=0.04190 | best_loss=0.04141
Epoch 30/80: current_loss=0.04183 | best_loss=0.04141
Epoch 31/80: current_loss=0.04176 | best_loss=0.04141
Epoch 32/80: current_loss=0.04165 | best_loss=0.04141
Epoch 33/80: current_loss=0.04205 | best_loss=0.04141
Epoch 34/80: current_loss=0.04154 | best_loss=0.04141
Epoch 35/80: current_loss=0.04237 | best_loss=0.04141
Epoch 36/80: current_loss=0.04268 | best_loss=0.04141
Epoch 37/80: current_loss=0.04152 | best_loss=0.04141
Epoch 38/80: current_loss=0.04166 | best_loss=0.04141
Epoch 39/80: current_loss=0.04270 | best_loss=0.04141
Early Stopping at epoch 39
      explained_var=0.03169 | mse_loss=0.03996
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02858 | best_loss=0.02858
Epoch 1/80: current_loss=0.02942 | best_loss=0.02858
Epoch 2/80: current_loss=0.02886 | best_loss=0.02858
Epoch 3/80: current_loss=0.02899 | best_loss=0.02858
Epoch 4/80: current_loss=0.02936 | best_loss=0.02858
Epoch 5/80: current_loss=0.02808 | best_loss=0.02808
Epoch 6/80: current_loss=0.02918 | best_loss=0.02808
Epoch 7/80: current_loss=0.02879 | best_loss=0.02808
Epoch 8/80: current_loss=0.02940 | best_loss=0.02808
Epoch 9/80: current_loss=0.02823 | best_loss=0.02808
Epoch 10/80: current_loss=0.02818 | best_loss=0.02808
Epoch 11/80: current_loss=0.02820 | best_loss=0.02808
Epoch 12/80: current_loss=0.02838 | best_loss=0.02808
Epoch 13/80: current_loss=0.02858 | best_loss=0.02808
Epoch 14/80: current_loss=0.02807 | best_loss=0.02807
Epoch 15/80: current_loss=0.02872 | best_loss=0.02807
Epoch 16/80: current_loss=0.02844 | best_loss=0.02807
Epoch 17/80: current_loss=0.02856 | best_loss=0.02807
Epoch 18/80: current_loss=0.02921 | best_loss=0.02807
Epoch 19/80: current_loss=0.03054 | best_loss=0.02807
Epoch 20/80: current_loss=0.02856 | best_loss=0.02807
Epoch 21/80: current_loss=0.02811 | best_loss=0.02807
Epoch 22/80: current_loss=0.02832 | best_loss=0.02807
Epoch 23/80: current_loss=0.02842 | best_loss=0.02807
Epoch 24/80: current_loss=0.02856 | best_loss=0.02807
Epoch 25/80: current_loss=0.02936 | best_loss=0.02807
Epoch 26/80: current_loss=0.02816 | best_loss=0.02807
Epoch 27/80: current_loss=0.02897 | best_loss=0.02807
Epoch 28/80: current_loss=0.02807 | best_loss=0.02807
Epoch 29/80: current_loss=0.02848 | best_loss=0.02807
Epoch 30/80: current_loss=0.02817 | best_loss=0.02807
Epoch 31/80: current_loss=0.02962 | best_loss=0.02807
Epoch 32/80: current_loss=0.02813 | best_loss=0.02807
Epoch 33/80: current_loss=0.02840 | best_loss=0.02807
Epoch 34/80: current_loss=0.02893 | best_loss=0.02807
Early Stopping at epoch 34
      explained_var=-0.00465 | mse_loss=0.02853

----------------------------------------------
Params for Trial 96
{'learning_rate': 1e-05, 'weight_decay': 0.0003641242674646288, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.30000000000000004}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.11176 | best_loss=0.11176
Epoch 1/80: current_loss=0.05245 | best_loss=0.05245
Epoch 2/80: current_loss=0.04875 | best_loss=0.04875
Epoch 3/80: current_loss=0.04770 | best_loss=0.04770
Epoch 4/80: current_loss=0.04697 | best_loss=0.04697
Epoch 5/80: current_loss=0.04602 | best_loss=0.04602
Epoch 6/80: current_loss=0.04539 | best_loss=0.04539
Epoch 7/80: current_loss=0.04475 | best_loss=0.04475
Epoch 8/80: current_loss=0.04417 | best_loss=0.04417
Epoch 9/80: current_loss=0.04356 | best_loss=0.04356
Epoch 10/80: current_loss=0.04316 | best_loss=0.04316
Epoch 11/80: current_loss=0.04266 | best_loss=0.04266
Epoch 12/80: current_loss=0.04232 | best_loss=0.04232
Epoch 13/80: current_loss=0.04197 | best_loss=0.04197
Epoch 14/80: current_loss=0.04172 | best_loss=0.04172
Epoch 15/80: current_loss=0.04141 | best_loss=0.04141
Epoch 16/80: current_loss=0.04125 | best_loss=0.04125
Epoch 17/80: current_loss=0.04085 | best_loss=0.04085
Epoch 18/80: current_loss=0.04066 | best_loss=0.04066
Epoch 19/80: current_loss=0.04042 | best_loss=0.04042
Epoch 20/80: current_loss=0.04026 | best_loss=0.04026
Epoch 21/80: current_loss=0.04008 | best_loss=0.04008
Epoch 22/80: current_loss=0.03997 | best_loss=0.03997
Epoch 23/80: current_loss=0.03985 | best_loss=0.03985
Epoch 24/80: current_loss=0.03979 | best_loss=0.03979
Epoch 25/80: current_loss=0.03974 | best_loss=0.03974
Epoch 26/80: current_loss=0.03955 | best_loss=0.03955
Epoch 27/80: current_loss=0.03944 | best_loss=0.03944
Epoch 28/80: current_loss=0.03933 | best_loss=0.03933
Epoch 29/80: current_loss=0.03933 | best_loss=0.03933
Epoch 30/80: current_loss=0.03925 | best_loss=0.03925
Epoch 31/80: current_loss=0.03913 | best_loss=0.03913
Epoch 32/80: current_loss=0.03901 | best_loss=0.03901
Epoch 33/80: current_loss=0.03892 | best_loss=0.03892
Epoch 34/80: current_loss=0.03890 | best_loss=0.03890
Epoch 35/80: current_loss=0.03883 | best_loss=0.03883
Epoch 36/80: current_loss=0.03872 | best_loss=0.03872
Epoch 37/80: current_loss=0.03868 | best_loss=0.03868
Epoch 38/80: current_loss=0.03862 | best_loss=0.03862
Epoch 39/80: current_loss=0.03864 | best_loss=0.03862
Epoch 40/80: current_loss=0.03866 | best_loss=0.03862
Epoch 41/80: current_loss=0.03856 | best_loss=0.03856
Epoch 42/80: current_loss=0.03849 | best_loss=0.03849
Epoch 43/80: current_loss=0.03842 | best_loss=0.03842
Epoch 44/80: current_loss=0.03845 | best_loss=0.03842
Epoch 45/80: current_loss=0.03833 | best_loss=0.03833
Epoch 46/80: current_loss=0.03828 | best_loss=0.03828
Epoch 47/80: current_loss=0.03840 | best_loss=0.03828
Epoch 48/80: current_loss=0.03823 | best_loss=0.03823
Epoch 49/80: current_loss=0.03818 | best_loss=0.03818
Epoch 50/80: current_loss=0.03809 | best_loss=0.03809
Epoch 51/80: current_loss=0.03815 | best_loss=0.03809
Epoch 52/80: current_loss=0.03813 | best_loss=0.03809
Epoch 53/80: current_loss=0.03816 | best_loss=0.03809
Epoch 54/80: current_loss=0.03812 | best_loss=0.03809
Epoch 55/80: current_loss=0.03821 | best_loss=0.03809
Epoch 56/80: current_loss=0.03801 | best_loss=0.03801
Epoch 57/80: current_loss=0.03799 | best_loss=0.03799
Epoch 58/80: current_loss=0.03792 | best_loss=0.03792
Epoch 59/80: current_loss=0.03795 | best_loss=0.03792
Epoch 60/80: current_loss=0.03795 | best_loss=0.03792
Epoch 61/80: current_loss=0.03790 | best_loss=0.03790
Epoch 62/80: current_loss=0.03795 | best_loss=0.03790
Epoch 63/80: current_loss=0.03796 | best_loss=0.03790
Epoch 64/80: current_loss=0.03796 | best_loss=0.03790
Epoch 65/80: current_loss=0.03786 | best_loss=0.03786
Epoch 66/80: current_loss=0.03785 | best_loss=0.03785
Epoch 67/80: current_loss=0.03795 | best_loss=0.03785
Epoch 68/80: current_loss=0.03796 | best_loss=0.03785
Epoch 69/80: current_loss=0.03782 | best_loss=0.03782
Epoch 70/80: current_loss=0.03787 | best_loss=0.03782
Epoch 71/80: current_loss=0.03785 | best_loss=0.03782
Epoch 72/80: current_loss=0.03796 | best_loss=0.03782
Epoch 73/80: current_loss=0.03791 | best_loss=0.03782
Epoch 74/80: current_loss=0.03786 | best_loss=0.03782
Epoch 75/80: current_loss=0.03792 | best_loss=0.03782
Epoch 76/80: current_loss=0.03790 | best_loss=0.03782
Epoch 77/80: current_loss=0.03789 | best_loss=0.03782
Epoch 78/80: current_loss=0.03779 | best_loss=0.03779
Epoch 79/80: current_loss=0.03776 | best_loss=0.03776
      explained_var=0.01285 | mse_loss=0.03878

----------------------------------------------
Params for Trial 97
{'learning_rate': 0.0001, 'weight_decay': 0.0017208063224544317, 'n_layers': 2, 'hidden_size': 32, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.11711 | best_loss=0.11711
Epoch 1/80: current_loss=0.06925 | best_loss=0.06925
Epoch 2/80: current_loss=0.04992 | best_loss=0.04992
Epoch 3/80: current_loss=0.04604 | best_loss=0.04604
Epoch 4/80: current_loss=0.04537 | best_loss=0.04537
Epoch 5/80: current_loss=0.04527 | best_loss=0.04527
Epoch 6/80: current_loss=0.04519 | best_loss=0.04519
Epoch 7/80: current_loss=0.04475 | best_loss=0.04475
Epoch 8/80: current_loss=0.04378 | best_loss=0.04378
Epoch 9/80: current_loss=0.04384 | best_loss=0.04378
Epoch 10/80: current_loss=0.04334 | best_loss=0.04334
Epoch 11/80: current_loss=0.04290 | best_loss=0.04290
Epoch 12/80: current_loss=0.04301 | best_loss=0.04290
Epoch 13/80: current_loss=0.04259 | best_loss=0.04259
Epoch 14/80: current_loss=0.04193 | best_loss=0.04193
Epoch 15/80: current_loss=0.04215 | best_loss=0.04193
Epoch 16/80: current_loss=0.04149 | best_loss=0.04149
Epoch 17/80: current_loss=0.04159 | best_loss=0.04149
Epoch 18/80: current_loss=0.04159 | best_loss=0.04149
Epoch 19/80: current_loss=0.04119 | best_loss=0.04119
Epoch 20/80: current_loss=0.04094 | best_loss=0.04094
Epoch 21/80: current_loss=0.04099 | best_loss=0.04094
Epoch 22/80: current_loss=0.04069 | best_loss=0.04069
Epoch 23/80: current_loss=0.04059 | best_loss=0.04059
Epoch 24/80: current_loss=0.04076 | best_loss=0.04059
Epoch 25/80: current_loss=0.04082 | best_loss=0.04059
Epoch 26/80: current_loss=0.04061 | best_loss=0.04059
Epoch 27/80: current_loss=0.04075 | best_loss=0.04059
Epoch 28/80: current_loss=0.04021 | best_loss=0.04021
Epoch 29/80: current_loss=0.04019 | best_loss=0.04019
Epoch 30/80: current_loss=0.04005 | best_loss=0.04005
Epoch 31/80: current_loss=0.03994 | best_loss=0.03994
Epoch 32/80: current_loss=0.03997 | best_loss=0.03994
Epoch 33/80: current_loss=0.04008 | best_loss=0.03994
Epoch 34/80: current_loss=0.03993 | best_loss=0.03993
Epoch 35/80: current_loss=0.04008 | best_loss=0.03993
Epoch 36/80: current_loss=0.04007 | best_loss=0.03993
Epoch 37/80: current_loss=0.03960 | best_loss=0.03960
Epoch 38/80: current_loss=0.04003 | best_loss=0.03960
Epoch 39/80: current_loss=0.03970 | best_loss=0.03960
Epoch 40/80: current_loss=0.03938 | best_loss=0.03938
Epoch 41/80: current_loss=0.03935 | best_loss=0.03935
Epoch 42/80: current_loss=0.03915 | best_loss=0.03915
Epoch 43/80: current_loss=0.03933 | best_loss=0.03915
Epoch 44/80: current_loss=0.03925 | best_loss=0.03915
Epoch 45/80: current_loss=0.03914 | best_loss=0.03914
Epoch 46/80: current_loss=0.03927 | best_loss=0.03914
Epoch 47/80: current_loss=0.03932 | best_loss=0.03914
Epoch 48/80: current_loss=0.03909 | best_loss=0.03909
Epoch 49/80: current_loss=0.03913 | best_loss=0.03909
Epoch 50/80: current_loss=0.03924 | best_loss=0.03909
Epoch 51/80: current_loss=0.03902 | best_loss=0.03902
Epoch 52/80: current_loss=0.03910 | best_loss=0.03902
Epoch 53/80: current_loss=0.03881 | best_loss=0.03881
Epoch 54/80: current_loss=0.03897 | best_loss=0.03881
Epoch 55/80: current_loss=0.03914 | best_loss=0.03881
Epoch 56/80: current_loss=0.03883 | best_loss=0.03881
Epoch 57/80: current_loss=0.03869 | best_loss=0.03869
Epoch 58/80: current_loss=0.03890 | best_loss=0.03869
Epoch 59/80: current_loss=0.03920 | best_loss=0.03869
Epoch 60/80: current_loss=0.03891 | best_loss=0.03869
Epoch 61/80: current_loss=0.03865 | best_loss=0.03865
Epoch 62/80: current_loss=0.03892 | best_loss=0.03865
Epoch 63/80: current_loss=0.03881 | best_loss=0.03865
Epoch 64/80: current_loss=0.03871 | best_loss=0.03865
Epoch 65/80: current_loss=0.03876 | best_loss=0.03865
Epoch 66/80: current_loss=0.03853 | best_loss=0.03853
Epoch 67/80: current_loss=0.03852 | best_loss=0.03852
Epoch 68/80: current_loss=0.03843 | best_loss=0.03843
Epoch 69/80: current_loss=0.03855 | best_loss=0.03843
Epoch 70/80: current_loss=0.03855 | best_loss=0.03843
Epoch 71/80: current_loss=0.03880 | best_loss=0.03843
Epoch 72/80: current_loss=0.03872 | best_loss=0.03843
Epoch 73/80: current_loss=0.03881 | best_loss=0.03843
Epoch 74/80: current_loss=0.03846 | best_loss=0.03843
Epoch 75/80: current_loss=0.03854 | best_loss=0.03843
Epoch 76/80: current_loss=0.03850 | best_loss=0.03843
Epoch 77/80: current_loss=0.03833 | best_loss=0.03833
Epoch 78/80: current_loss=0.03854 | best_loss=0.03833
Epoch 79/80: current_loss=0.03840 | best_loss=0.03833
      explained_var=0.00024 | mse_loss=0.03941

----------------------------------------------
Params for Trial 98
{'learning_rate': 0.001, 'weight_decay': 0.0012666824502220821, 'n_layers': 2, 'hidden_size': 64, 'dropout': 0.45000000000000007}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04223 | best_loss=0.04223
Epoch 1/80: current_loss=0.03883 | best_loss=0.03883
Epoch 2/80: current_loss=0.03874 | best_loss=0.03874
Epoch 3/80: current_loss=0.03951 | best_loss=0.03874
Epoch 4/80: current_loss=0.04016 | best_loss=0.03874
Epoch 5/80: current_loss=0.03796 | best_loss=0.03796
Epoch 6/80: current_loss=0.03763 | best_loss=0.03763
Epoch 7/80: current_loss=0.03759 | best_loss=0.03759
Epoch 8/80: current_loss=0.03842 | best_loss=0.03759
Epoch 9/80: current_loss=0.03831 | best_loss=0.03759
Epoch 10/80: current_loss=0.03773 | best_loss=0.03759
Epoch 11/80: current_loss=0.03915 | best_loss=0.03759
Epoch 12/80: current_loss=0.03784 | best_loss=0.03759
Epoch 13/80: current_loss=0.03805 | best_loss=0.03759
Epoch 14/80: current_loss=0.03762 | best_loss=0.03759
Epoch 15/80: current_loss=0.03759 | best_loss=0.03759
Epoch 16/80: current_loss=0.03760 | best_loss=0.03759
Epoch 17/80: current_loss=0.03755 | best_loss=0.03755
Epoch 18/80: current_loss=0.03747 | best_loss=0.03747
Epoch 19/80: current_loss=0.03787 | best_loss=0.03747
Epoch 20/80: current_loss=0.03773 | best_loss=0.03747
Epoch 21/80: current_loss=0.03764 | best_loss=0.03747
Epoch 22/80: current_loss=0.03848 | best_loss=0.03747
Epoch 23/80: current_loss=0.03784 | best_loss=0.03747
Epoch 24/80: current_loss=0.03877 | best_loss=0.03747
Epoch 25/80: current_loss=0.03797 | best_loss=0.03747
Epoch 26/80: current_loss=0.03775 | best_loss=0.03747
Epoch 27/80: current_loss=0.03758 | best_loss=0.03747
Epoch 28/80: current_loss=0.03979 | best_loss=0.03747
Epoch 29/80: current_loss=0.03805 | best_loss=0.03747
Epoch 30/80: current_loss=0.03814 | best_loss=0.03747
Epoch 31/80: current_loss=0.03758 | best_loss=0.03747
Epoch 32/80: current_loss=0.03769 | best_loss=0.03747
Epoch 33/80: current_loss=0.03766 | best_loss=0.03747
Epoch 34/80: current_loss=0.03807 | best_loss=0.03747
Epoch 35/80: current_loss=0.03776 | best_loss=0.03747
Epoch 36/80: current_loss=0.03835 | best_loss=0.03747
Epoch 37/80: current_loss=0.03778 | best_loss=0.03747
Epoch 38/80: current_loss=0.03792 | best_loss=0.03747
Early Stopping at epoch 38
      explained_var=0.02102 | mse_loss=0.03838
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04164 | best_loss=0.04164
Epoch 1/80: current_loss=0.04138 | best_loss=0.04138
Epoch 2/80: current_loss=0.04233 | best_loss=0.04138
Epoch 3/80: current_loss=0.04188 | best_loss=0.04138
Epoch 4/80: current_loss=0.04259 | best_loss=0.04138
Epoch 5/80: current_loss=0.04183 | best_loss=0.04138
Epoch 6/80: current_loss=0.04168 | best_loss=0.04138
Epoch 7/80: current_loss=0.04150 | best_loss=0.04138
Epoch 8/80: current_loss=0.04164 | best_loss=0.04138
Epoch 9/80: current_loss=0.04168 | best_loss=0.04138
Epoch 10/80: current_loss=0.04175 | best_loss=0.04138
Epoch 11/80: current_loss=0.04207 | best_loss=0.04138
Epoch 12/80: current_loss=0.04185 | best_loss=0.04138
Epoch 13/80: current_loss=0.04187 | best_loss=0.04138
Epoch 14/80: current_loss=0.04180 | best_loss=0.04138
Epoch 15/80: current_loss=0.04240 | best_loss=0.04138
Epoch 16/80: current_loss=0.04363 | best_loss=0.04138
Epoch 17/80: current_loss=0.04201 | best_loss=0.04138
Epoch 18/80: current_loss=0.04181 | best_loss=0.04138
Epoch 19/80: current_loss=0.04169 | best_loss=0.04138
Epoch 20/80: current_loss=0.04268 | best_loss=0.04138
Epoch 21/80: current_loss=0.04332 | best_loss=0.04138
Early Stopping at epoch 21
      explained_var=0.03237 | mse_loss=0.03991
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02798 | best_loss=0.02798
Epoch 1/80: current_loss=0.02830 | best_loss=0.02798
Epoch 2/80: current_loss=0.02798 | best_loss=0.02798
Epoch 3/80: current_loss=0.02829 | best_loss=0.02798
Epoch 4/80: current_loss=0.02851 | best_loss=0.02798
Epoch 5/80: current_loss=0.02809 | best_loss=0.02798
Epoch 6/80: current_loss=0.02800 | best_loss=0.02798
Epoch 7/80: current_loss=0.02799 | best_loss=0.02798
Epoch 8/80: current_loss=0.02903 | best_loss=0.02798
Epoch 9/80: current_loss=0.02821 | best_loss=0.02798
Epoch 10/80: current_loss=0.02842 | best_loss=0.02798
Epoch 11/80: current_loss=0.02879 | best_loss=0.02798
Epoch 12/80: current_loss=0.02812 | best_loss=0.02798
Epoch 13/80: current_loss=0.03005 | best_loss=0.02798
Epoch 14/80: current_loss=0.02837 | best_loss=0.02798
Epoch 15/80: current_loss=0.02795 | best_loss=0.02795
Epoch 16/80: current_loss=0.02869 | best_loss=0.02795
Epoch 17/80: current_loss=0.02910 | best_loss=0.02795
Epoch 18/80: current_loss=0.02833 | best_loss=0.02795
Epoch 19/80: current_loss=0.02838 | best_loss=0.02795
Epoch 20/80: current_loss=0.02826 | best_loss=0.02795
Epoch 21/80: current_loss=0.02813 | best_loss=0.02795
Epoch 22/80: current_loss=0.02804 | best_loss=0.02795
Epoch 23/80: current_loss=0.02805 | best_loss=0.02795
Epoch 24/80: current_loss=0.02955 | best_loss=0.02795
Epoch 25/80: current_loss=0.02852 | best_loss=0.02795
Epoch 26/80: current_loss=0.02794 | best_loss=0.02794
Epoch 27/80: current_loss=0.02863 | best_loss=0.02794
Epoch 28/80: current_loss=0.02809 | best_loss=0.02794
Epoch 29/80: current_loss=0.02837 | best_loss=0.02794
Epoch 30/80: current_loss=0.02814 | best_loss=0.02794
Epoch 31/80: current_loss=0.02799 | best_loss=0.02794
Epoch 32/80: current_loss=0.02903 | best_loss=0.02794
Epoch 33/80: current_loss=0.02795 | best_loss=0.02794
Epoch 34/80: current_loss=0.02828 | best_loss=0.02794
Epoch 35/80: current_loss=0.02921 | best_loss=0.02794
Epoch 36/80: current_loss=0.02817 | best_loss=0.02794
Epoch 37/80: current_loss=0.02895 | best_loss=0.02794
Epoch 38/80: current_loss=0.02819 | best_loss=0.02794
Epoch 39/80: current_loss=0.02843 | best_loss=0.02794
Epoch 40/80: current_loss=0.02841 | best_loss=0.02794
Epoch 41/80: current_loss=0.02805 | best_loss=0.02794
Epoch 42/80: current_loss=0.02891 | best_loss=0.02794
Epoch 43/80: current_loss=0.02800 | best_loss=0.02794
Epoch 44/80: current_loss=0.02845 | best_loss=0.02794
Epoch 45/80: current_loss=0.02825 | best_loss=0.02794
Epoch 46/80: current_loss=0.02817 | best_loss=0.02794
Early Stopping at epoch 46
      explained_var=-0.00158 | mse_loss=0.02840
Fold 3: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03285 | best_loss=0.03285
Epoch 1/80: current_loss=0.03288 | best_loss=0.03285
Epoch 2/80: current_loss=0.03287 | best_loss=0.03285
Epoch 3/80: current_loss=0.03300 | best_loss=0.03285
Epoch 4/80: current_loss=0.03325 | best_loss=0.03285
Epoch 5/80: current_loss=0.03293 | best_loss=0.03285
Epoch 6/80: current_loss=0.03290 | best_loss=0.03285
Epoch 7/80: current_loss=0.03287 | best_loss=0.03285
Epoch 8/80: current_loss=0.03297 | best_loss=0.03285
Epoch 9/80: current_loss=0.03315 | best_loss=0.03285
Epoch 10/80: current_loss=0.03301 | best_loss=0.03285
Epoch 11/80: current_loss=0.03346 | best_loss=0.03285
Epoch 12/80: current_loss=0.03285 | best_loss=0.03285
Epoch 13/80: current_loss=0.03286 | best_loss=0.03285
Epoch 14/80: current_loss=0.03291 | best_loss=0.03285
Epoch 15/80: current_loss=0.03282 | best_loss=0.03282
Epoch 16/80: current_loss=0.03324 | best_loss=0.03282
Epoch 17/80: current_loss=0.03281 | best_loss=0.03281
Epoch 18/80: current_loss=0.03302 | best_loss=0.03281
Epoch 19/80: current_loss=0.03285 | best_loss=0.03281
Epoch 20/80: current_loss=0.03292 | best_loss=0.03281
Epoch 21/80: current_loss=0.03286 | best_loss=0.03281
Epoch 22/80: current_loss=0.03298 | best_loss=0.03281
Epoch 23/80: current_loss=0.03296 | best_loss=0.03281
Epoch 24/80: current_loss=0.03302 | best_loss=0.03281
Epoch 25/80: current_loss=0.03316 | best_loss=0.03281
Epoch 26/80: current_loss=0.03328 | best_loss=0.03281
Epoch 27/80: current_loss=0.03283 | best_loss=0.03281
Epoch 28/80: current_loss=0.03338 | best_loss=0.03281
Epoch 29/80: current_loss=0.03286 | best_loss=0.03281
Epoch 30/80: current_loss=0.03282 | best_loss=0.03281
Epoch 31/80: current_loss=0.03289 | best_loss=0.03281
Epoch 32/80: current_loss=0.03287 | best_loss=0.03281
Epoch 33/80: current_loss=0.03281 | best_loss=0.03281
Epoch 34/80: current_loss=0.03336 | best_loss=0.03281
Epoch 35/80: current_loss=0.03286 | best_loss=0.03281
Epoch 36/80: current_loss=0.03283 | best_loss=0.03281
Epoch 37/80: current_loss=0.03294 | best_loss=0.03281
Early Stopping at epoch 37
      explained_var=-0.00186 | mse_loss=0.03266
Fold 4: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03556 | best_loss=0.03556
Epoch 1/80: current_loss=0.03559 | best_loss=0.03556
Epoch 2/80: current_loss=0.03555 | best_loss=0.03555
Epoch 3/80: current_loss=0.03566 | best_loss=0.03555
Epoch 4/80: current_loss=0.03561 | best_loss=0.03555
Epoch 5/80: current_loss=0.03547 | best_loss=0.03547
Epoch 6/80: current_loss=0.03547 | best_loss=0.03547
Epoch 7/80: current_loss=0.03554 | best_loss=0.03547
Epoch 8/80: current_loss=0.03554 | best_loss=0.03547
Epoch 9/80: current_loss=0.03555 | best_loss=0.03547
Epoch 10/80: current_loss=0.03559 | best_loss=0.03547
Epoch 11/80: current_loss=0.03558 | best_loss=0.03547
Epoch 12/80: current_loss=0.03562 | best_loss=0.03547
Epoch 13/80: current_loss=0.03555 | best_loss=0.03547
Epoch 14/80: current_loss=0.03552 | best_loss=0.03547
Epoch 15/80: current_loss=0.03560 | best_loss=0.03547
Epoch 16/80: current_loss=0.03554 | best_loss=0.03547
Epoch 17/80: current_loss=0.03559 | best_loss=0.03547
Epoch 18/80: current_loss=0.03563 | best_loss=0.03547
Epoch 19/80: current_loss=0.03561 | best_loss=0.03547
Epoch 20/80: current_loss=0.03564 | best_loss=0.03547
Epoch 21/80: current_loss=0.03550 | best_loss=0.03547
Epoch 22/80: current_loss=0.03550 | best_loss=0.03547
Epoch 23/80: current_loss=0.03548 | best_loss=0.03547
Epoch 24/80: current_loss=0.03562 | best_loss=0.03547
Epoch 25/80: current_loss=0.03555 | best_loss=0.03547
Early Stopping at epoch 25
      explained_var=0.01815 | mse_loss=0.03523
----------------------------------------------
Average early_stopping_point: 13| avg_exp_var=0.01362| avg_loss=0.03492
----------------------------------------------


----------------------------------------------
Params for Trial 99
{'learning_rate': 0.001, 'weight_decay': 0.0008379913862678563, 'n_layers': 2, 'hidden_size': 128, 'dropout': 0.2}
----------------------------------------------
Fold 0: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.03900 | best_loss=0.03900
Epoch 1/80: current_loss=0.03854 | best_loss=0.03854
Epoch 2/80: current_loss=0.03994 | best_loss=0.03854
Epoch 3/80: current_loss=0.03777 | best_loss=0.03777
Epoch 4/80: current_loss=0.03819 | best_loss=0.03777
Epoch 5/80: current_loss=0.03861 | best_loss=0.03777
Epoch 6/80: current_loss=0.03835 | best_loss=0.03777
Epoch 7/80: current_loss=0.03801 | best_loss=0.03777
Epoch 8/80: current_loss=0.03830 | best_loss=0.03777
Epoch 9/80: current_loss=0.03820 | best_loss=0.03777
Epoch 10/80: current_loss=0.03919 | best_loss=0.03777
Epoch 11/80: current_loss=0.03874 | best_loss=0.03777
Epoch 12/80: current_loss=0.03895 | best_loss=0.03777
Epoch 13/80: current_loss=0.03746 | best_loss=0.03746
Epoch 14/80: current_loss=0.03761 | best_loss=0.03746
Epoch 15/80: current_loss=0.03847 | best_loss=0.03746
Epoch 16/80: current_loss=0.03766 | best_loss=0.03746
Epoch 17/80: current_loss=0.03791 | best_loss=0.03746
Epoch 18/80: current_loss=0.04162 | best_loss=0.03746
Epoch 19/80: current_loss=0.03770 | best_loss=0.03746
Epoch 20/80: current_loss=0.03747 | best_loss=0.03746
Epoch 21/80: current_loss=0.03803 | best_loss=0.03746
Epoch 22/80: current_loss=0.03806 | best_loss=0.03746
Epoch 23/80: current_loss=0.04176 | best_loss=0.03746
Epoch 24/80: current_loss=0.03767 | best_loss=0.03746
Epoch 25/80: current_loss=0.03779 | best_loss=0.03746
Epoch 26/80: current_loss=0.03747 | best_loss=0.03746
Epoch 27/80: current_loss=0.03788 | best_loss=0.03746
Epoch 28/80: current_loss=0.03754 | best_loss=0.03746
Epoch 29/80: current_loss=0.03925 | best_loss=0.03746
Epoch 30/80: current_loss=0.03809 | best_loss=0.03746
Epoch 31/80: current_loss=0.03820 | best_loss=0.03746
Epoch 32/80: current_loss=0.04150 | best_loss=0.03746
Epoch 33/80: current_loss=0.03819 | best_loss=0.03746
Early Stopping at epoch 33
      explained_var=0.02106 | mse_loss=0.03828
Fold 1: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.04447 | best_loss=0.04447
Epoch 1/80: current_loss=0.04147 | best_loss=0.04147
Epoch 2/80: current_loss=0.04178 | best_loss=0.04147
Epoch 3/80: current_loss=0.04207 | best_loss=0.04147
Epoch 4/80: current_loss=0.04181 | best_loss=0.04147
Epoch 5/80: current_loss=0.04308 | best_loss=0.04147
Epoch 6/80: current_loss=0.04170 | best_loss=0.04147
Epoch 7/80: current_loss=0.04271 | best_loss=0.04147
Epoch 8/80: current_loss=0.04247 | best_loss=0.04147
Epoch 9/80: current_loss=0.04397 | best_loss=0.04147
Epoch 10/80: current_loss=0.04208 | best_loss=0.04147
Epoch 11/80: current_loss=0.04194 | best_loss=0.04147
Epoch 12/80: current_loss=0.04352 | best_loss=0.04147
Epoch 13/80: current_loss=0.04282 | best_loss=0.04147
Epoch 14/80: current_loss=0.04314 | best_loss=0.04147
Epoch 15/80: current_loss=0.04187 | best_loss=0.04147
Epoch 16/80: current_loss=0.04162 | best_loss=0.04147
Epoch 17/80: current_loss=0.04352 | best_loss=0.04147
Epoch 18/80: current_loss=0.04190 | best_loss=0.04147
Epoch 19/80: current_loss=0.04193 | best_loss=0.04147
Epoch 20/80: current_loss=0.04185 | best_loss=0.04147
Epoch 21/80: current_loss=0.04366 | best_loss=0.04147
Early Stopping at epoch 21
      explained_var=0.03017 | mse_loss=0.04000
Fold 2: num_train_ids=720, num_val_ids=180
Epoch 0/80: current_loss=0.02818 | best_loss=0.02818
Epoch 1/80: current_loss=0.02819 | best_loss=0.02818
Epoch 2/80: current_loss=0.02915 | best_loss=0.02818
Epoch 3/80: current_loss=0.02834 | best_loss=0.02818
Epoch 4/80: current_loss=0.02831 | best_loss=0.02818
Epoch 5/80: current_loss=0.03111 | best_loss=0.02818
Epoch 6/80: current_loss=0.02821 | best_loss=0.02818
Epoch 7/80: current_loss=0.02820 | best_loss=0.02818
Epoch 8/80: current_loss=0.02927 | best_loss=0.02818
Epoch 9/80: current_loss=0.02807 | best_loss=0.02807
Epoch 10/80: current_loss=0.02799 | best_loss=0.02799
Epoch 11/80: current_loss=0.02827 | best_loss=0.02799
Epoch 12/80: current_loss=0.02807 | best_loss=0.02799
Epoch 13/80: current_loss=0.02940 | best_loss=0.02799
Epoch 14/80: current_loss=0.02865 | best_loss=0.02799
Epoch 15/80: current_loss=0.02806 | best_loss=0.02799
Epoch 16/80: current_loss=0.02833 | best_loss=0.02799
Epoch 17/80: current_loss=0.02833 | best_loss=0.02799
Epoch 18/80: current_loss=0.02954 | best_loss=0.02799
Epoch 19/80: current_loss=0.02828 | best_loss=0.02799
Epoch 20/80: current_loss=0.02829 | best_loss=0.02799
Epoch 21/80: current_loss=0.02835 | best_loss=0.02799
Epoch 22/80: current_loss=0.02823 | best_loss=0.02799
Epoch 23/80: current_loss=0.02834 | best_loss=0.02799
Epoch 24/80: current_loss=0.02801 | best_loss=0.02799
Epoch 25/80: current_loss=0.02802 | best_loss=0.02799
Epoch 26/80: current_loss=0.02836 | best_loss=0.02799
Epoch 27/80: current_loss=0.02806 | best_loss=0.02799
Epoch 28/80: current_loss=0.03044 | best_loss=0.02799
Epoch 29/80: current_loss=0.02808 | best_loss=0.02799
Epoch 30/80: current_loss=0.02809 | best_loss=0.02799
Early Stopping at epoch 30
      explained_var=-0.00257 | mse_loss=0.02844
Optuna study finished, study statistics:
  Finished trials:  100
  Pruned trials:  55
  Completed trials:  45
  Best Trial:  2
  Value:  0.03483302574838244
  AVG stopping:  16
  Params: 
    learning_rate: 0.0001
    weight_decay: 0.000464504222554936
    n_layers: 2
    hidden_size: 512
    dropout: 0.45000000000000007
----------------------------------------------

Check best params: {'learning_rate': 0.0001, 'weight_decay': 0.000464504222554936, 'n_layers': 2, 'hidden_size': 512, 'dropout': 0.45000000000000007, 'avg_epochs': 16}
--------------------------------------------------------------
Test CNN results: avg_loss=0.0364, avg_expvar=0.0153, avg_r2score=-0.0100, avg_mae=0.1525
--------------------------------------------------------------
